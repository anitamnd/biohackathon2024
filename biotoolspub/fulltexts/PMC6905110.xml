<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Plant Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Plant Methods</journal-id>
    <journal-title-group>
      <journal-title>Plant Methods</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1746-4811</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6905110</article-id>
    <article-id pub-id-type="publisher-id">537</article-id>
    <article-id pub-id-type="doi">10.1186/s13007-019-0537-2</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TasselNetv2: in-field counting of wheat spikes with context-augmented local regression networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Xiong</surname>
          <given-names>Haipeng</given-names>
        </name>
        <address>
          <email>hpxiong@hust.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cao</surname>
          <given-names>Zhiguo</given-names>
        </name>
        <address>
          <email>zgcao@hust.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3854-8664</contrib-id>
        <name>
          <surname>Lu</surname>
          <given-names>Hao</given-names>
        </name>
        <address>
          <email>poppinace@hust.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Madec</surname>
          <given-names>Simon</given-names>
        </name>
        <address>
          <email>simon.madec@inra.fr</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Liang</given-names>
        </name>
        <address>
          <email>wings@hust.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shen</surname>
          <given-names>Chunhua</given-names>
        </name>
        <address>
          <email>chunhua.shen@adelaide.edu.au</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0368 7223</institution-id><institution-id institution-id-type="GRID">grid.33199.31</institution-id><institution>National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Artificial Intelligence and Automation, </institution><institution>Huazhong University of Science and Technology, </institution></institution-wrap>Wuhan, 430074 People’s Republic of China </aff>
      <aff id="Aff2"><label>2</label>INRA-EMMAH-CAPTE, 84914 Avignon, France </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 7304</institution-id><institution-id institution-id-type="GRID">grid.1010.0</institution-id><institution>School of Computer Science, </institution><institution>The University of Adelaide, </institution></institution-wrap>Adelaide, SA 5005 Australia </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>11</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>15</volume>
    <elocation-id>150</elocation-id>
    <history>
      <date date-type="received">
        <day>23</day>
        <month>4</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>3</day>
        <month>12</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Grain yield of wheat is greatly associated with the population of wheat spikes, i.e., <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$spike~number~\text {m}^{-2}$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mspace width="3.33333pt"/><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="3.33333pt"/><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq1.gif"/></alternatives></inline-formula>. To obtain this index in a reliable and efficient way, it is necessary to count wheat spikes accurately and automatically. Currently computer vision technologies have shown great potential to automate this task effectively in a low-end manner. In particular, counting wheat spikes is a typical visual counting problem, which is substantially studied under the name of object counting in Computer Vision. TasselNet, which represents one of the state-of-the-art counting approaches, is a convolutional neural network-based local regression model, and currently benchmarks the best record on counting maize tassels. However, when applying TasselNet to wheat spikes, it cannot predict accurate counts when spikes partially present.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we make an important observation that the counting performance of local regression networks can be significantly improved via adding visual context to the local patches. Meanwhile, such context can be treated as part of the receptive field without increasing the model capacity. We thus propose a simple yet effective contextual extension of TasselNet—TasselNetv2. If implementing TasselNetv2 in a fully convolutional form, both training and inference can be greatly sped up by reducing redundant computations. In particular, we collected and labeled a large-scale wheat spikes counting (WSC) dataset, with 1764 high-resolution images and 675,322 manually-annotated instances. Extensive experiments show that, TasselNetv2 not only achieves state-of-the-art performance on the WSC dataset (<inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$91.01\%$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mn>91.01</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq2.gif"/></alternatives></inline-formula> counting accuracy) but also is more than an order of magnitude faster than TasselNet (13.82 fps on <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$912\times 1216$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mn>912</mml:mn><mml:mo>×</mml:mo><mml:mn>1216</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq3.gif"/></alternatives></inline-formula> images). The generality of TasselNetv2 is further demonstrated by advancing the state of the art on both the Maize Tassels Counting and ShanghaiTech Crowd Counting datasets.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">This paper describes TasselNetv2 for counting wheat spikes, which simultaneously addresses two important use cases in plant counting: <italic>improving the counting accuracy without increasing model capacity</italic>, and <italic>improving efficiency without sacrificing accuracy</italic>. It is promising to be deployed in a real-time system with high-throughput demand. In particular, TasselNetv2 can achieve sufficiently accurate results when training from scratch with small networks, and adopting larger pre-trained networks can further boost accuracy. In practice, one can trade off the performance and efficiency according to certain application scenarios. Code and models are made available at: <ext-link ext-link-type="uri" xlink:href="https://tinyurl.com/TasselNetv2">https://tinyurl.com/TasselNetv2</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Wheat spikes</kwd>
      <kwd>Object counting</kwd>
      <kwd>Convolutional models</kwd>
      <kwd>Local regression networks</kwd>
      <kwd>Context fusion</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Natural Science Foundation of China</institution>
        </funding-source>
        <award-id>No. 61876211</award-id>
        <principal-award-recipient>
          <name>
            <surname>Cao</surname>
            <given-names>Zhiguo</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par4">In agricultural production, crop yield is one of the key factors when monitoring crop growth status. Wheat is one of the top three cereal crops in the world. Its grain yield is mainly associated to <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$spike~number~\text {m}^{-2}$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mspace width="3.33333pt"/><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="3.33333pt"/><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq4.gif"/></alternatives></inline-formula>, <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$grain~number~\text {m}^{-2}$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="3.33333pt"/><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="3.33333pt"/><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq5.gif"/></alternatives></inline-formula> and <italic>thousand</italic> <italic>grain</italic> <italic>weight</italic> [<xref ref-type="bibr" rid="CR1">1</xref>]. Among these traits, <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$spike~number~\text {m}^{-2}$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mspace width="3.33333pt"/><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="3.33333pt"/><mml:msup><mml:mtext>m</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq6.gif"/></alternatives></inline-formula> is the most important index [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. Conventional manual approaches to counting wheat spikes are tedious and labor-intensive. The counting results are also error-prone and unrepresentative due to small sampling areas used. To meet the need of large-scale analyses in the era of intelligent agriculture and to obtain the index of <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$spike~number~m^{-2}$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>k</mml:mi><mml:mi>e</mml:mi><mml:mspace width="3.33333pt"/><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mspace width="3.33333pt"/><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq7.gif"/></alternatives></inline-formula> accurately in real time, counting wheat spikes must be automated in a reliable way, and possibly with low cost.</p>
    <p id="Par5">With the rapid development of recent deep learning technologies, large-scale visual databases and cost-effective graphical processing units, image-based approaches appear to be promising alternatives to automate the task of wheat spikes counting.</p>
    <p id="Par6">Counting wheat spikes is a typical object counting problem in Computer Vision, and currently convolutional neural network (CNN)-based local regression models have shown remarkable performance in counting crowd [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>], vehicles [<xref ref-type="bibr" rid="CR6">6</xref>], cells [<xref ref-type="bibr" rid="CR7">7</xref>], animals [<xref ref-type="bibr" rid="CR8">8</xref>], and plants [<xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR12">12</xref>]. However, when turning to the scenario of counting wheat spikes in the wild, things are much difficult due to the non-rigid nature of spikes and substantial visual challenges. As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, these challenges are:<list list-type="bullet"><list-item><p id="Par7">Wheats planted in different regions show significant visual differences, due to differences in varieties and geographical environment (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a);</p></list-item><list-item><p id="Par8">The color, size and shape of wheat spikes vary greatly and unevenly at different growth stages of wheats (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b);</p></list-item><list-item><p id="Par9">If the imaging equipment lacks manual maintenance, or fog droplets and dust cover the lens, images will be blurred (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c);</p></list-item><list-item><p id="Par10">Dramatic illumination changes result in completely different visual characteristics of wheat (Fig. <xref rid="Fig1" ref-type="fig">1</xref>d);</p></list-item><list-item><p id="Par11">The intensive cultivation of wheats gives rise to extremely dense distributions and severe occlusions (Fig. <xref rid="Fig1" ref-type="fig">1</xref>e). In these extremely dense areas, even an expert has to count wheat spikes for multiple times to obtain a reliable measure;</p></list-item><list-item><p id="Par12">The perspective changes due to the imaging angle. Some wheats may be perpendicular to the lens and only occupy a small number of pixels in the image, which renders difficulties to distinguish wheat spikes from background. This also leads to large size variations of wheat spikes (Fig. <xref rid="Fig1" ref-type="fig">1</xref>f).</p></list-item></list>Above visual challenges make wheat spikes counting a good study case for counting non-rigid objects. Recent literatures emerge on counting wheat spikes but are mainly based on detection. [<xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR16">16</xref>] first segment the wheats using the RGB images, and then detect each object based on the segmentation result. After detection, the wheat counts can be easily inferred from the objects detected. [<xref ref-type="bibr" rid="CR17">17</xref>] fuses multi-sensor information (RGB images and multispectral images) to help segmentation. [<xref ref-type="bibr" rid="CR18">18</xref>] and [<xref ref-type="bibr" rid="CR19">19</xref>] utilize R-CNN [<xref ref-type="bibr" rid="CR20">20</xref>] to detect wheat spikes. However, the camera is close to the wheat spikes in these methods, which allows for capturing high-resolution images and obtaining accurate detection but leads to small observation areas. The efficiency of R-CNN processing high-resolution images is also an issue. [<xref ref-type="bibr" rid="CR21">21</xref>] benefits from active learning to reduce human labeling efforts and use a RetinaNet [<xref ref-type="bibr" rid="CR22">22</xref>] for detecting and counting sorghum head in UAV-based images in a large region. In order to meet the need of high-throughput plant phenotype analysis over a large area, we leverage images captured from a fixed platform (4 m/5 m above the ground) for counting. These images cover wheat spikes over around 30 <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {m}^2$$\end{document}</tex-math><mml:math id="M16"><mml:msup><mml:mtext>m</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq8.gif"/></alternatives></inline-formula>. However, wheat spikes present extremely dense distributions and severe overlaps in such images. We notice that non-maximum suppression is regularly used at the end of detection-based methods, which makes it hard to distinguish overlapping objects. Furthermore, there are more than 10,000 wheat spikes in just one image, which makes the bounding boxes annotation nearly impossible. Overall, these counting-by-detection methods render difficulties for counting dense wheat spikes within a large area.<fig id="Fig1"><label>Fig. 1</label><caption><p>Challenges of counting wheat spikes in the wild. <bold>a</bold> different planting regions, <bold>b</bold> various growth stages, <bold>c</bold> degraded image quality due to blurring, <bold>d</bold> visual differences caused by changing illumination, <bold>e</bold> extremely dense spatial distributions and severe occlusions, <bold>f</bold> size and pose variations</p></caption><graphic xlink:href="13007_2019_537_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par13">Current state-of-the-art counting approaches typically pursue the idea of local regression with CNNs. Images are often divided into small local patches, and these patches are then processed by the networks individually. Most CNN-based local regression methods adopt density maps as the regression target [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR25">25</xref>]. These methods intend to regress the per-pixel density maps, which is a dense prediction problem. But the problem is that the ground-truth density map is associated with specific choices of Gaussian kernels. This means the ground-truth density map may not be initially accurate, and the error would be introduced before learning the model. To alleviate this problem, [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR26">26</xref>] prove it is much easier to regress the local count than the density map. The benefit is that the ground truth is no longer sensitive to the exact choice of Gaussian kernels. Lu et al. [<xref ref-type="bibr" rid="CR9">9</xref>] proposed a local count regression network named TasselNet, which counts maize tassels much more accurate than other existing methods. We believe this idea should also be applicable to other non-rigid objects like wheat spikes.</p>
    <p id="Par14">Albeit successful, we found that TasselNet cannot predict correct counts when spikes partially present in local image patches. As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, it is not clear whether there are two wheat spikes or not when only looking at those visible regions. This situation is even more serious when spikes are occluded. In fact, wheats are planted far denser than maize plants, and the density of spikes typically varies between <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$200/\text {m}^2$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:mn>200</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:msup><mml:mtext>m</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq9.gif"/></alternatives></inline-formula> and <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$600/\text {m}^2$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mn>600</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:msup><mml:mtext>m</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq10.gif"/></alternatives></inline-formula>, which means partial spikes would occur frequently in cropped local image patches and thus seriously limits the applicability of TasselNet. To address this, our intuition tells that we need the help of visual contextual information. This is in consistent with the fact that, when one cannot infer the exact number of partially occluded objects within a local area, he may look further until supporting information, such as the border or other object parts, is identified. This kind of supporting information in real world refers to the visual context in images, and it is a kind of “weak context” for it only contains the local surroundings rather than all of remaining images. Therefore, a simple way to tackle above problem is to enable TasselNet to receive both local images and their surrounding pixels, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. This raises a subsequent question: <italic>how to integrate the context into CNNs in a principled way?</italic> One way is to use large convolutional kernels but at the cost of introducing extra parameters. In this paper, we show that a much clever way is to include the context as part of the receptive field so that the model can keep the same number of parameters. This idea is particularly useful for local counting models, such as TasselNet, that do not make full use of their receptive field. As a consequence, we make a simple yet effective extension to TasselNet so that contextual information could be received, leading to an extended version of TasselNet—contextual TasselNet (TasselNetv2 for short).</p>
    <p id="Par15">Another limitation of TasselNet is its low efficiency due to the need of densely sampling local image patches. This introduces many redundant computations. We wonder whether these redundant computations could be avoided in TasselNetv2. Inspired by Fast R-CNN [<xref ref-type="bibr" rid="CR27">27</xref>], we show that one actually can first extract the features maps of the whole image and then densely sample the feature maps to obtain local features, rather than processing local patches individually. Based on this idea, we implement a fully convolutional form of TasselNetv2, which is proven to be an order of magnitude faster than TasselNet. In particular, we created a large-scale Wheat Spikes Counting (WSC) dataset to validate the effectiveness of TasselNetv2.<fig id="Fig2"><label>Fig. 2</label><caption><p>Three examples of incomplete objects when only looking at the local patches. White parts are invisible contextual regions for the current visible patches. Wheat spikes annotated with black dots indicate the spike is partly within the visible area, and red dots represent spikes with severe occlusions. In both cases, accurate wheat numbers are just hard to obtain without the help of local visual context</p></caption><graphic xlink:href="13007_2019_537_Fig2_HTML" id="MO2"/></fig>
<fig id="Fig3"><label>Fig. 3</label><caption><p>A high-level overview of the approach utilizing local visual context information. The red dashed box indicates a local patch ready for counting, and the part outside the box refers to the context</p></caption><graphic xlink:href="13007_2019_537_Fig3_HTML" id="MO3"/></fig>
</p>
    <p id="Par16">Extensive experiments show that, TasselNetv2 reaches <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$91.01\%$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mn>91.01</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq11.gif"/></alternatives></inline-formula> relative counting accuracy and achieves the state-of-the-art performance on the WSC dataset, and notably, can process images 13.21 times faster than TasselNet (13.82 fps for TasselNetv2 vs. 1.05 fps for TasselNet). Further experiments demonstrate that TasselNetv2 also reports state-of-the-art counting performance on the Maize Tassels Counting (MTC) and ShanghaiTech Crowd Counting datasets [<xref ref-type="bibr" rid="CR5">5</xref>], which confirms a good generality of TasselNetv2. Several interesting ablative studies are conducted to justify the effectiveness and necessity to include the context for better counting performance.</p>
    <p id="Par17">Overall, the main contributions of this paper are:<list list-type="bullet"><list-item><p id="Par18">We introduce a principled way to supplement the local visual context into convolutional models by treating it as part of the receptive field, which can improve the counting performance without increasing extra parameters;</p></list-item><list-item><p id="Par19">We propose a simple yet effective extension of TasselNet to its contextual version TasselNetv2. TasselNetv2 not only improves the counting performance but also speeds up the computation with an order of magnitude;</p></list-item><list-item><p id="Par20">We collect and annotate a large-scale WSC dataset with 1764 high-resolution images and 675,322 manually-labeled instances;</p></list-item><list-item><p id="Par21">We report state-of-the-art counting performance on the WSC, MTC and ShanghaiTech datasets.</p></list-item></list>
</p>
  </sec>
  <sec id="Sec2">
    <title>Method</title>
    <sec id="Sec3">
      <title>Image acquisition</title>
      <p id="Par22">Field wheat images in the WSC dataset are collected from three experimental fields of Gucheng, Hebei, Zhengzhou, Henan, and Tai’an, Shandong, containing seven sequences from 2011 to 2013. Due to the different local geology and climate conditions, three cultivars were planted, respectively, including Zimai No. 24 in Taian, Jimai No. 22 in Gucheng, and Zhengmai No. 366 in Zhengzhou.</p>
      <p id="Par23">Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the image capturing device, main components include a high-resolution CCD digital camera (E450 Olympus), a low-resolution monitoring equipment, a 3G wireless data transmission system, and several solar panels for power supply. The CCD digital camera is set with a height of 5 m above the ground, and the focal length is fixed to 16 mm. From 8 a.m. to 17 p.m., images are captured from a perspective oblique to the ground once an hour. After images are acquired, wheat images are transmitted to the remote server through the 3G wireless network, and then we can access the image data. For detailed information of the image capturing equipment, readers can refer to [<xref ref-type="bibr" rid="CR28">28</xref>].<fig id="Fig4"><label>Fig. 4</label><caption><p>Imaging device in the Zhengzhou, Henan Province. The main components include a high resolution CCD digital camera (E450 Olympus) and low-resolution monitoring equipment. The camera is set 5 m high above the ground</p></caption><graphic xlink:href="13007_2019_537_Fig4_HTML" id="MO4"/></fig>
</p>
    </sec>
    <sec id="Sec4">
      <title>Wheat spikes counting dataset</title>
      <p id="Par25">There are tens of thousands of wheat spikes in the wheat images, and they present a high degree of similarity when the time interval is short, which makes the annotations for all of the captured images costly and needless. This means only a subset of images is essential to build the dataset, but this subset should be large enough to cover wheat spikes in various scenarios. We pick out this subset with a two-stage selection strategy. At the first stage, we choose images according to the date, after the heading stage of wheat. Before obvious emergence of spikes, the sampling interval is set to 3 days. After wheat spikes emerge, the number of wheat spikes changes rapidly, and thus the sampling interval is shortened to 2 days. At the second stage, 10 candidate images collected in each day (from 8 a.m. to 17 p.m.) are taken into account. Considering the illumination characteristics in one day, three images are chosen from three time periods, i.e., morning (8 a.m. to 11 a.m.), noon (12 a.m. to 14p.m.), and afternoon (15 p.m. to 17 p.m.), to maintain the diversity of the dataset.</p>
      <p id="Par26">Finally, a total of 196 images, with the resolution of <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$3648\times 2736$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mn>3648</mml:mn><mml:mo>×</mml:mo><mml:mn>2736</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq12.gif"/></alternatives></inline-formula>, were chosen. The number of wheat spikes varies from 0 to over 10, 000. Since the image resolution is very high, and wheat spikes are extremely dense (it brings tremendous difficulties for the annotation process), each original image is cropped to 9 sub-images with a resolution of <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1216\times 912$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:mn>1216</mml:mn><mml:mo>×</mml:mo><mml:mn>912</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq13.gif"/></alternatives></inline-formula>. Thus, 1764 images in all are used to construct the dataset. Table <xref rid="Tab1" ref-type="table">1</xref> presents the information of each sequence in the dataset.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Constitution of the WSC dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Sequence</th><th align="left">Images</th><th align="left">Spikes</th><th align="left">Min</th><th align="left">Max</th></tr></thead><tbody><tr><td align="left">Hebei Gucheng (2011–2012)</td><td char="." align="char">324</td><td align="left">82,578</td><td char="." align="char">0</td><td char="." align="char">661</td></tr><tr><td align="left">Henan Zhengzhou (2011–2012)</td><td char="." align="char">234</td><td align="left">118,022</td><td char="." align="char">0</td><td char="." align="char">1462</td></tr><tr><td align="left">Henan Zhengzhou (2012–2013)</td><td char="." align="char">171</td><td align="left">104,847</td><td char="." align="char">0</td><td char="." align="char">1331</td></tr><tr><td align="left">Shandong Taian (2011–2012 Camera 1)</td><td char="." align="char">279</td><td align="left">97,695</td><td char="." align="char">0</td><td char="." align="char">1010</td></tr><tr><td align="left">Shandong Taian (2011–2012 Camera 2)</td><td char="." align="char">261</td><td align="left">78,887</td><td char="." align="char">0</td><td char="." align="char">908</td></tr><tr><td align="left">Shandong Taian (2012–2013 Camera 1)</td><td char="." align="char">234</td><td align="left">94,454</td><td char="." align="char">0</td><td char="." align="char">1090</td></tr><tr><td align="left">Shandong Taian (2012–2013 Camera 2)</td><td char="." align="char">261</td><td align="left">98,839</td><td char="." align="char">0</td><td char="." align="char">971</td></tr><tr><td align="left">Total</td><td char="." align="char">1764</td><td align="left">675,322</td><td char="." align="char">0</td><td char="." align="char">1462</td></tr></tbody></table><table-wrap-foot><p><italic>Images</italic> denote the number of images in each sequence. <italic>Spikes</italic> refer to the number of wheat spikes in each sequence. <italic>Min</italic> and <italic>Max</italic> indicate the minimum and maximum number of wheat spikes per image</p></table-wrap-foot></table-wrap></p>
      <p id="Par27">With seven sequences in the WSC dataset, the training set, validation set and test set are divided, as shown in Table <xref rid="Tab2" ref-type="table">2</xref>. Images from the Shandong Taian (2012–2013 Camera 1) sequence exhibit a relatively clear distinction between spikes and background. Spikes in this sequence also appear to have a high density and are with dramatic changes caused by illumination. In the Henan Zhengzhou (2012–2013) sequence, it is hard to distinguish the spikes from the background. The presence of severe occlusions makes this task even more challenging. Evaluations on these sequences can sufficiently show the adaptability and robustness of the counting method. Local visual context may be helpful for identifying overlapped objects, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. We embed local visual context in TasselNetv2 to alleviate such a problem.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Training set (train), validation set (val) and test set (test) settings of the WSC dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Sequence</th><th align="left">Train</th><th align="left">Val</th><th align="left">Test</th></tr></thead><tbody><tr><td align="left">Hebei Gucheng (2011–2012)</td><td align="left"><inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M28"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq14.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M30"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq15.gif"/></alternatives></inline-formula></td><td align="left"/></tr><tr><td align="left">Henan Zhengzhou (2011–2012)</td><td align="left"><inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M32"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq16.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M34"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq17.gif"/></alternatives></inline-formula></td><td align="left"/></tr><tr><td align="left">Henan Zhengzhou (2012–2013)</td><td align="left"/><td align="left"/><td align="left"><inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M36"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq18.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">Shandong Taian (2011–2012 Camera 1)</td><td align="left"><inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M38"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq19.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq20"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M40"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq20.gif"/></alternatives></inline-formula></td><td align="left"/></tr><tr><td align="left">Shandong Taian (2011–2012 Camera 2)</td><td align="left"><inline-formula id="IEq21"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M42"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq21.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq22"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M44"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq22.gif"/></alternatives></inline-formula></td><td align="left"/></tr><tr><td align="left">Shandong Taian (2012–2013 Camera 1)</td><td align="left"/><td align="left"/><td align="left"><inline-formula id="IEq23"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M46"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq23.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">Shandong Taian (2012–2013 Camera 2)</td><td align="left"><inline-formula id="IEq24"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M48"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq24.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq25"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M50"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq25.gif"/></alternatives></inline-formula></td><td align="left"/></tr></tbody></table></table-wrap>
</p>
      <p id="Par28">Following [<xref ref-type="bibr" rid="CR9">9</xref>], dotted annotation is adopted where a point is marked at the location of each wheat spike. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows an example of annotated image. Six colleagues in our laboratory first participated in the annotation process. After the dataset is annotated, we double-checked the annotations and corrected some missing and wrong annotations. Especially for the second round checking, we trained a TasselNet to predict counts and identified the areas with high counting errors. With this kind of auxiliary information, particular attentions are paid to these areas for careful checking further, and other areas are also checked again.<fig id="Fig5"><label>Fig. 5</label><caption><p>An example of dotted annotation. A red dot is marked at each location of the wheat spike</p></caption><graphic xlink:href="13007_2019_537_Fig5_HTML" id="MO5"/></fig>
</p>
    </sec>
    <sec id="Sec5">
      <title>Design of TasselNetv2</title>
      <p id="Par29">We first highlight the concepts of “input image”, “input patch” and “input patch with context” in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. They are prerequisites for readers to better understand TasselNetv2.</p>
      <p id="Par34">Local patches from an image may have severe overlaps due to dense sampling, but TasselNet requires extracting the local feature from each patch first and then mapping it to the local count. In this paradigm, many redundant calculations appear during feature extraction. Inspired by Fast R-CNN [<xref ref-type="bibr" rid="CR30">30</xref>], redundant calculations can be avoided by first extracting the feature maps of the whole image, then densely sampling the feature maps to obtain local features and finally mapping them to local counts in a light-weight manner.</p>
      <p id="Par35">Notice that fully-connected layers in TasselNet can also be implemented as convolutional layers with <inline-formula id="IEq30"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 1$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq30.gif"/></alternatives></inline-formula> kernels [<xref ref-type="bibr" rid="CR31">31</xref>]. When the convolutional kernel slides over the image and manipulates a local area of pixels at a time, it performs a form of dense sampling. This inspires us to replace the explicit dense sampling with convolution.</p>
      <sec id="Sec51">
        <title>Motivation</title>
        <p id="Par30">The local visual context, in the framework of local regression, refers to the surrounding pixels of local sampling patches. In Fig. <xref rid="Fig2" ref-type="fig">2</xref>, if the visible parts belong to local sampling patches, those invisible parts represent the context. Unfortunately, since the context is not within local patches, it remains invisible to local regression networks like TasselNet. If a network can see the context, overlapping objects or part of objects may be inferred easily and counted accurately. The high-level idea is thus to enable the network to process both local patches along with the context, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.</p>
      </sec>
      <sec id="Sec52">
        <title>Adding context</title>
        <p id="Par31">The main idea of TasselNetv2 is to process local patches with the context. Notice that there is a massive waste of the receptive field in TasselNet. It is natural to think how to reduce such a waste. In this paper, we show that one can cancel zero paddings to enable the network receiving extra context and to make full use of the receptive field. The way to achieve this is simply to delete paddings in all of convolutional layers, as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig6"><label>Fig. 6</label><caption><p>The structure of TasselNet, TasselNet added context and TasselNetv2. All of the networks adopt AlexNet-like architectures. The definition of the convolutional and pooling layers is in the format: fliter size + layer name, number of channels, padding, /stride. Fully connected layers are defined in the format: layer name, number of nodes. The different settings are highlighted in red</p></caption><graphic xlink:href="13007_2019_537_Fig6_HTML" id="MO6"/></fig></p>
        <p id="Par32">We explain why this simple modification makes sense through a visualizing analysis of the receptive field in Fig. <xref rid="Fig7" ref-type="fig">7</xref>, and a brief introduction about computing the receptive field is also provided in Additional file <xref rid="MOESM1" ref-type="media">1</xref>. Assume TasselNet and TasselNetv2 regress the local count of the <inline-formula id="IEq26"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$64\times 64$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mn>64</mml:mn><mml:mo>×</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq26.gif"/></alternatives></inline-formula> local area. TasselNet (a) receives the local area without the context. It has zero paddings in all convolutional layers, and these paddings cause the zero area in the receptive field outside borders. However, if removing all the zero paddings, TasselNet (b) can leverage the wasted receptive field to receive extra context and keep the same amount of parameters.</p>
        <p id="Par33">It is worth noting that, though the network processes <inline-formula id="IEq27"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$94\times 94$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mn>94</mml:mn><mml:mo>×</mml:mo><mml:mn>94</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq27.gif"/></alternatives></inline-formula> patches, it still regresses local counts aggregated from the central <inline-formula id="IEq28"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$64\times 64$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mn>64</mml:mn><mml:mo>×</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq28.gif"/></alternatives></inline-formula> areas. Many counting approaches assume that CNNs are able to identify each object within their local receptive fields [<xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR29">29</xref>], while we argue that <italic>one should treat part of the local receptive field as additional context</italic> towards accurate counting. This is what makes TasselNetv2 quite different from existing CNN-based local regression models.<fig id="Fig7"><label>Fig. 7</label><caption><p>Feature maps and the corresponding receptive field of TasselNet and TasselNetv2. <bold>a</bold> For TasselNet, <bold>b</bold> for adding context to TasselNet via canceling zero-paddings and <bold>c</bold> for TasselNetv2. The above line are feature maps of each layer in the network, numbers below feature maps are in the format: <inline-formula id="IEq29"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$height\times width\times channel~numbers$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mspace width="3.33333pt"/><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq29.gif"/></alternatives></inline-formula>. The following line is the corresponding receptive fields, where black dotted boxes represents the target local area to be counted, the blue rectangular areas represents the input area, and the pink area represents the receptive field of the bottom left element in the feature map (the part of the receptive field beyond the input area denotes zero area). Since the last few layers have receptive fields of the same size, we use orange lines to point to the corresponding receptive fields</p></caption><graphic xlink:href="13007_2019_537_Fig7_HTML" id="MO7"/></fig>
</p>
      </sec>
      <sec id="Sec53">
        <title>Improving efficiency</title>
        <p id="Par36">Inspired by the idea of fully convolutional networks (FCNs) [<xref ref-type="bibr" rid="CR32">32</xref>], we implement TasselNetv2 into a fully convolutional form, which speeds up both training and inference significantly, as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. In what follows, we further explain in detail how TasselNetv2 works and improves efficiency.</p>
        <p id="Par37">TasselNetv2 is a composition of convolutional layers. If skipping the activation functions, the composition of convolutional layers can be view as a convolutional layer with a large kernel, and the filter size equals to the size of the receptive field. As shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>, the size of the receptive field of the output remains <inline-formula id="IEq31"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$94\times 94$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mn>94</mml:mn><mml:mo>×</mml:mo><mml:mn>94</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq31.gif"/></alternatives></inline-formula>, so TasselNetv2 can be seen as a large <inline-formula id="IEq32"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$94\times 94$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mn>94</mml:mn><mml:mo>×</mml:mo><mml:mn>94</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq32.gif"/></alternatives></inline-formula> convolutional layer and maps each <inline-formula id="IEq33"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$94\times 94$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mn>94</mml:mn><mml:mo>×</mml:mo><mml:mn>94</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq33.gif"/></alternatives></inline-formula> local area (local patch with context) to a local count. Meanwhile, since four layers are with a stride of 2, this large convolutional filter slides with a stride of <inline-formula id="IEq34"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2^4=16$$\end{document}</tex-math><mml:math id="M68"><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>4</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq34.gif"/></alternatives></inline-formula>, which is equivalent to densely sampling the input image with a stride of 16. As a consequence, TasselNetv2 adds context into TasselNet in a FCN-like manner. It is worth noting that the context is naturally exploited in FCNs by most local areas. Only the context close to image borders is partially utilized by TasselNetv2, e.g., the local area in the upper left corner only has the lower right part of the context. In order to keep the size of these local areas to be <inline-formula id="IEq35"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$94\times 94$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mn>94</mml:mn><mml:mo>×</mml:mo><mml:mn>94</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq35.gif"/></alternatives></inline-formula>, we supplement 15 zero paddings around the image borders. An elegant way to embed this pre-processing in TasselNetv2 is to use the accumulation of zero paddings from the first five layers (these zero paddings accumulate to 15 zero paddings around the input image).</p>
        <p id="Par38">The calculations performed in CNNs are mainly Floating Point Operations (FLOPs), and FLOPs are also widely adopted in evaluating the computation complexity of CNNs [<xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>] from the view of computation amount. We remark the efficiency of TasselNetv2 using FLOPs during testing in Table <xref rid="Tab3" ref-type="table">3</xref>. The first five convolution layers extract feature maps, and the following three layers map features to local counts. As mentioned in [<xref ref-type="bibr" rid="CR9">9</xref>], dense sampling is essential to generate adequate training samples for TasselNet. However, <inline-formula id="IEq36"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10\times$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq36.gif"/></alternatives></inline-formula> extra calculations are needed in this paradigm, compared to sampling non-overlapping patches. This is due to the redundant computations in both feature extraction and feature mapping. Instead, TasselNetv2 directly extracts the feature maps of the whole image, densely samples local features from the feature map and maps them to local counts simultaneously. In this way, TasselNetv2 avoids redundant calculations during feature extraction and is thus much more efficient than TasselNet. It can directly process the whole image and regress all local counts with a single forward pass.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison towards the floating point computations (FLOPs) when processing images with the resolution of <inline-formula id="IEq37"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1216\times 912$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mn>1216</mml:mn><mml:mo>×</mml:mo><mml:mn>912</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq37.gif"/></alternatives></inline-formula>. Only the single-precision floating point multiplication are taken into account</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="2">TasselNet</th><th align="left" rowspan="2">TasselNetv2</th></tr><tr><th align="left">Non-overlap</th><th align="left">Dense sample</th></tr></thead><tbody><tr><td align="left">conv1</td><td align="left"><inline-formula id="IEq38"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4.70\times 10^{8}$$\end{document}</tex-math><mml:math id="M76"><mml:mrow><mml:mn>4.70</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>8</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq38.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq39"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$6.92\times 10^{9}$$\end{document}</tex-math><mml:math id="M78"><mml:mrow><mml:mn>6.92</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq39.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq40"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4.79\times 10^{8}$$\end{document}</tex-math><mml:math id="M80"><mml:mrow><mml:mn>4.79</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>8</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq40.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">conv2</td><td align="left"><inline-formula id="IEq41"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.24\times 10^{9}$$\end{document}</tex-math><mml:math id="M82"><mml:mrow><mml:mn>1.24</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq41.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq42"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.83\times 10^{10}$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mn>1.83</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>10</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq42.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq43"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.28\times 10^{9}$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:mn>1.28</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq43.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">conv3</td><td align="left"><inline-formula id="IEq44"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.22\times 10^{9}$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mn>1.22</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq44.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq45"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.81\times 10^{10}$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mn>1.81</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>10</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq45.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq46"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.28\times 10^{9}$$\end{document}</tex-math><mml:math id="M92"><mml:mrow><mml:mn>1.28</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq46.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">conv4</td><td align="left"><inline-formula id="IEq47"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2.44\times 10^{9}$$\end{document}</tex-math><mml:math id="M94"><mml:mrow><mml:mn>2.44</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq47.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq48"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$3.61\times 10^{10}$$\end{document}</tex-math><mml:math id="M96"><mml:mrow><mml:mn>3.61</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>10</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq48.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq49"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2.56\times 10^{9}$$\end{document}</tex-math><mml:math id="M98"><mml:mrow><mml:mn>2.56</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq49.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">conv5</td><td align="left"><inline-formula id="IEq50"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2.44\times 10^{9}$$\end{document}</tex-math><mml:math id="M100"><mml:mrow><mml:mn>2.44</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq50.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq51"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$3.61\times 10^{10}$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:mn>3.61</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>10</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq51.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq52"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2.56\times 10^{9}$$\end{document}</tex-math><mml:math id="M104"><mml:mrow><mml:mn>2.56</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq52.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">conv6(fc1)</td><td align="left"><inline-formula id="IEq53"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5.17\times 10^{8}$$\end{document}</tex-math><mml:math id="M106"><mml:mrow><mml:mn>5.17</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>8</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq53.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq54"><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2.07\times 10^{9}$$\end{document}</tex-math><mml:math id="M108"><mml:mrow><mml:mn>2.07</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq54.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq55"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$2.07\times 10^{9}$$\end{document}</tex-math><mml:math id="M110"><mml:mrow><mml:mn>2.07</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq55.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">conv7(fc2)</td><td align="left"><inline-formula id="IEq56"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.75\times 10^{7}$$\end{document}</tex-math><mml:math id="M112"><mml:mrow><mml:mn>1.75</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq56.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq57"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$6.46\times 10^{7}$$\end{document}</tex-math><mml:math id="M114"><mml:mrow><mml:mn>6.46</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq57.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq58"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$6.46\times 10^{7}$$\end{document}</tex-math><mml:math id="M116"><mml:mrow><mml:mn>6.46</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq58.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">conv8(fc3)</td><td align="left"><inline-formula id="IEq59"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.26\times 10^{5}$$\end{document}</tex-math><mml:math id="M118"><mml:mrow><mml:mn>1.26</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq59.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq60"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5.05\times 10^{5}$$\end{document}</tex-math><mml:math id="M120"><mml:mrow><mml:mn>5.05</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq60.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq61"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5.05\times 10^{5}$$\end{document}</tex-math><mml:math id="M122"><mml:mrow><mml:mn>5.05</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq61.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">Total</td><td align="left"><inline-formula id="IEq62"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$8.34\times 10^{9}$$\end{document}</tex-math><mml:math id="M124"><mml:mrow><mml:mn>8.34</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq62.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq63"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.16\times 10^{11}$$\end{document}</tex-math><mml:math id="M126"><mml:mrow><mml:mn>1.16</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>11</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq63.gif"/></alternatives></inline-formula></td><td align="left"><inline-formula id="IEq64"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.03\times 10^{10}$$\end{document}</tex-math><mml:math id="M128"><mml:mrow><mml:mn>1.03</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>10</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq64.gif"/></alternatives></inline-formula></td></tr></tbody></table></table-wrap>
</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Inference of TasselNetv2</title>
      <p id="Par40">Here we formally introduce the processing pipeline of TasselNetv2 during inference, as shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. TasselNetv2 directly processes the whole image of arbitrary size (in this paper, the whole image refers to the image of size <inline-formula id="IEq65"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1216\times 912$$\end{document}</tex-math><mml:math id="M130"><mml:mrow><mml:mn>1216</mml:mn><mml:mo>×</mml:mo><mml:mn>912</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq65.gif"/></alternatives></inline-formula>) and regresses all local counts at the same time. However, since individual local areas have overlaps, the global image count cannot be acquired by summing over the whole count map directly. Following the aggregation and normalization strategy mentioned in [<xref ref-type="bibr" rid="CR9">9</xref>], all local counts are merged to obtain the normalized count map. After normalization, the global image count can then be reflected by integrating over the count map.<fig id="Fig8"><label>Fig. 8</label><caption><p>The processing pipeline of TasselNetv2 at the test stage. Unlike TasselNet, TasselNetv2 directly processes the whole input image and outputs all local counts. And the final density map can be acquired by merging and normalizing all local counts</p></caption><graphic xlink:href="13007_2019_537_Fig8_HTML" id="MO8"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Implementation details</title>
      <p id="Par41">We implement TasselNetv2 based on MatConvNet [<xref ref-type="bibr" rid="CR35">35</xref>]. During training, we use 1359 images in the training and validation sequences of the WSC dataset. <inline-formula id="IEq66"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$90\%$$\end{document}</tex-math><mml:math id="M132"><mml:mrow><mml:mn>90</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq66.gif"/></alternatives></inline-formula> images are randomly chosen for training, while the rest for validation. Before learning, mean subtraction is preprocessed (the mean is computed from the training set). It is worth mentioning that, no data augmentation is performed because the WSC dataset already contains wheat spikes under various scenarios.</p>
      <p id="Par42">We initialize networks with the <italic>improved Xavier</italic> method [<xref ref-type="bibr" rid="CR36">36</xref>]. The standard stochastic gradient descent is applied to optimize the parameters of the network. The learning rate is initially set to 0.1 and is decreased when the training error stagnates. To speed up and stabilize the error convergence process, a batch normalization layer [<xref ref-type="bibr" rid="CR37">37</xref>] is attached after each convolutional layer before ReLU.</p>
      <p id="Par43">The training time of TasselNetv2 on the WSC dataset varies from 4 h to 2 days depending on the network architecture used (4 hours for the Alex-like architecture, and 2 days when the pretrained VGG–16 is used). When training TasselNet on the WSC dataset, the training time varies between 4 days and 2 weeks according to the network capacity used (Matlab 2017a, OS: Window10 Home 64-bit, CPU: Intel i7-7700 3.60GHz, GPU: Nvidia GeForce GTX 1070 (8GB), RAM: 16 GB).</p>
    </sec>
  </sec>
  <sec id="Sec8">
    <title>Results and discussion</title>
    <p id="Par44">Extensive experiments are conducted to demonstrate the effectiveness and efficiency of TasselNetv2. First, we perform experiments on the WSC dataset to search optimal hyper parameters. After obtaining these, we verify the effect of adding context in TasselNetv2. Next, TasselNetv2 is further compared against other state-of-the-art approaches on the WSC dataset. To demonstrate the generality of TasselNetv2, we also evaluate it on the MTC [<xref ref-type="bibr" rid="CR9">9</xref>] and ShanghaiTech datasets [<xref ref-type="bibr" rid="CR5">5</xref>].</p>
    <p id="Par45">Mean absolute error (<italic>MAE</italic>) and root mean squared error (<italic>RMSE</italic>) are chosen to quantify the counting performance. They are defined as<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \small MAE=\frac{1}{N} \sum _{i=1}^{N} \left|C^{pre}_{i}-C^{gt}_{i}\right|\,, \end{aligned}$$\end{document}</tex-math><mml:math id="M134" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mstyle mathsize="0.6em"><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mfenced close="|" open="|" separators=""><mml:msubsup><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">gt</mml:mi></mml:mrow></mml:msubsup></mml:mfenced><mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13007_2019_537_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \small RMSE=\sqrt{ \frac{1}{N} \sum _{i=1}^{N} \left(C^{pre}_{i}-C^{gt}_{i}\right)^{2}}\,, \end{aligned}$$\end{document}</tex-math><mml:math id="M136" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mstyle mathsize="0.6em"><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mfenced close=")" open="(" separators=""><mml:msubsup><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">gt</mml:mi></mml:mrow></mml:msubsup></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mspace width="0.166667em"/><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="13007_2019_537_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>N</italic> denotes the number of images, <inline-formula id="IEq67"><alternatives><tex-math id="M137">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C^{pre}_{i}$$\end{document}</tex-math><mml:math id="M138"><mml:msubsup><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">pre</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq67.gif"/></alternatives></inline-formula> denotes the predicted count of the <italic>i</italic>-th image, and <inline-formula id="IEq68"><alternatives><tex-math id="M139">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C^{gt}_{i}$$\end{document}</tex-math><mml:math id="M140"><mml:msubsup><mml:mi>C</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi mathvariant="italic">gt</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq68.gif"/></alternatives></inline-formula> denotes the corresponding ground-truth count. <italic>MAE</italic> measures the accuracy of counting, and <italic>RMSE</italic> measures the stability. Lower <italic>MAE</italic> and <italic>RMSE</italic> imply better counting performance.</p>
    <sec id="Sec9">
      <title>Searching optimal parameters</title>
      <p id="Par46">Since TasselNet is the direct baseline of TasselNetv2, we set the hyper parameters of TasselNetv2 same as the TasselNet, in order to demonstrate the superiority of TasselNetv2 w.r.t. TasselNet and the benefit of embedding context information. Hence, we first search the optimal parameters on the WSC dataset using TasselNet so that TasselNet can report the optimal performance, and we then apply the same parameters to TasselNetv2.</p>
      <p id="Par47">Through extensive experiments, the optimal setting of hyper parameters for TasselNet on the WSC dataset is summarized in Table <xref rid="Tab4" ref-type="table">4</xref>. Detailed procedures of searching optimal parameters are provided in Additional file <xref rid="MOESM1" ref-type="media">1</xref>.<table-wrap id="Tab4"><label>Table 4</label><caption><p>TasselNet configurations on the WSC dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Patch size</th><th align="left"><inline-formula id="IEq69"><alternatives><tex-math id="M141">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$64\times 64$$\end{document}</tex-math><mml:math id="M142"><mml:mrow><mml:mn>64</mml:mn><mml:mo>×</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq69.gif"/></alternatives></inline-formula></th><th align="left">Gaussian size</th><th align="left">4</th></tr></thead><tbody><tr><td align="left" colspan="2">Backbone of TasselNet</td><td align="left" colspan="2">AlexNet-like in Fig. <xref rid="Fig6" ref-type="fig">6</xref></td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec10">
      <title>Why adding context?</title>
      <sec id="Sec54">
        <title>Adding context is effective</title>
        <p id="Par49">We first compare TasselNet trained with/without the context to highlight the pure effect of adding the context. Then, TasselNetv2 is evaluated to show its efficiency and accuracy beyond TasselNet.</p>
        <p id="Par50">Quantitative results are presented in Table <xref rid="Tab5" ref-type="table">5</xref>. We observe that, when forcibly adding the context into TasselNet during only inference (trained without context), the counting error increases notably, which suggests that TasselNet cannot utilize contextual information when trained without the context. This is the problem we call <italic>information asymmetry</italic>. However, after embedding contextual information since the training phase, the MAE decreases more than 10 without increasing model parameters (compared to TasselNet). Adding the context is effective. It is worth noting that this significant performance improvement comes almost at no cost.<table-wrap id="Tab5"><label>Table 5</label><caption><p>The effect of context on the test set of the WSC dataset. “train” denotes adding context into TasselNet since training phase as Fig. <xref rid="Fig7" ref-type="fig">7</xref>b, while “test” denotes only adding context into TasselNet in the testing phase</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Context</th><th align="left">MAE</th><th align="left">RMSE</th><th align="left">Train (s)</th></tr></thead><tbody><tr><td align="left">TasselNet</td><td align="left"><inline-formula id="IEq70"><alternatives><tex-math id="M143">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M144"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq70.gif"/></alternatives></inline-formula></td><td char="." align="char">61.35</td><td char="." align="char">99.27</td><td char="." align="char">3495.29</td></tr><tr><td align="left">TasselNet</td><td align="left">Test</td><td char="." align="char">79.42</td><td char="." align="char">126.18</td><td char="." align="char">3495.29</td></tr><tr><td align="left">TasselNet</td><td align="left">Train</td><td char="." align="char"><italic>50.17</italic></td><td char="." align="char">82.16</td><td char="." align="char">4026.68</td></tr><tr><td align="left">TasselNetv2</td><td align="left"><inline-formula id="IEq71"><alternatives><tex-math id="M145">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\checkmark$$\end{document}</tex-math><mml:math id="M146"><mml:mo stretchy="false">✓</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq71.gif"/></alternatives></inline-formula></td><td char="." align="char">50.79</td><td char="." align="char"><italic>80.66</italic></td><td char="." align="char">333.27</td></tr></tbody></table><table-wrap-foot><p>All networks are trained from scratch. Training time for one epoch is reported. The best performance is in italics</p></table-wrap-foot></table-wrap></p>
        <p id="Par51">It also can be observed that TasselNetv2 exhibits the same degree of improvement of adding the context. Meanwhile, TasselNetv2 is more than 10 times faster than TasselNet during the training stage. This is achieved by processing input images in a FCN manner rather than densely sampling image patches, thus avoiding redundant computations in feature extraction, as analysed in Table <xref rid="Tab3" ref-type="table">3</xref>. Now we can say that TasselNetv2 is a much more efficient implementation of adding the context into TasselNet.</p>
        <p id="Par52">We further analyze the error distributions in Fig. <xref rid="Fig9" ref-type="fig">9</xref>, and find that patch-based and image-based errors are more likely to shift towards zero with the help of context. So far, it can be concluded that lacking the context is the main drawback of TasselNet, and it is important to add the context during training.<fig id="Fig9"><label>Fig. 9</label><caption><p>The distribution of absolute errors for local patches and test images. The left is the histogram of absolute error for local patches, and the right is the histogram of absolute error for test images. All networks are trained from scratch. “TasselNet (add-c)” denotes adding the context in TasselNet as per Fig. <xref rid="Fig6" ref-type="fig">6</xref> since the training phase</p></caption><graphic xlink:href="13007_2019_537_Fig9_HTML" id="MO11"/></fig>
</p>
      </sec>
      <sec id="Sec55">
        <title>Adding context is necessary</title>
        <p id="Par53">Notice that we treat the context as part of the receptive field and regress only the local count from the central region. One may wonder what if the network simply regresses the local count accumulated from the whole receptive field. Another baseline <italic>TasselNetv2 (del-c)</italic> is used to justify this point, where we delete the context of the input patch in TasselNetv2. Specifically, we alter the regression target of TasselNetv2 to the object count within the whole <inline-formula id="IEq72"><alternatives><tex-math id="M147">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$94\times 94$$\end{document}</tex-math><mml:math id="M148"><mml:mrow><mml:mn>94</mml:mn><mml:mo>×</mml:mo><mml:mn>94</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq72.gif"/></alternatives></inline-formula> receptive field (rather than the <inline-formula id="IEq73"><alternatives><tex-math id="M149">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$64\times 64$$\end{document}</tex-math><mml:math id="M150"><mml:mrow><mml:mn>64</mml:mn><mml:mo>×</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq73.gif"/></alternatives></inline-formula> central area in our proposition).</p>
        <p id="Par54">According to the results in Table <xref rid="Tab6" ref-type="table">6</xref>, we can see that the counting performance of TasselNetv2 (del-c) drops significantly (66.96 MAE), even worse than TasselNet. This implies a network may not sense everything in its receptive field. A possible explanation may be given from some recent findings on the effective receptive field. First, the effective receptive field is much smaller than the theoretical receptive field [<xref ref-type="bibr" rid="CR38">38</xref>]. According to [<xref ref-type="bibr" rid="CR39">39</xref>], the effective receptive field empirically obeys a Gaussian distribution, which means pixels close to the center of the receptive field have much larger impact on counting than marginal pixels close to the boundary of the receptive field. A network may not capture sufficient evidence to support regressing counts at the border of the receptive field, while our empirical study shows that adding the context into part of the receptive field as auxiliary information can help to improve the counting of objects located in the center of receptive field.<table-wrap id="Tab6"><label>Table 6</label><caption><p>The necessity of adding context on the test set of the WSC dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">MAE</th><th align="left">RMSE</th></tr></thead><tbody><tr><td align="left">TasselNet</td><td char="." align="char">61.35</td><td char="." align="char">99.27</td></tr><tr><td align="left">TasselNetv2</td><td char="." align="char"><italic>50.79</italic></td><td char="." align="char"><italic>80.66</italic></td></tr><tr><td align="left">TasselNetv2(del-c)</td><td char="." align="char">66.96</td><td char="." align="char">113.20</td></tr></tbody></table><table-wrap-foot><p>All networks are trained from scratch and with the same hyper parameters. The best performance is in italics</p></table-wrap-foot></table-wrap></p>
        <p id="Par55">The above experiments justify that it is better to use a portion of the receptive field as the context, instead of counting all objects within the whole receptive field [<xref ref-type="bibr" rid="CR26">26</xref>].</p>
      </sec>
    </sec>
    <sec id="Sec11">
      <title>Comparison with state of the art</title>
      <p id="Par56">According to the above evaluations, the optimal setting on the WSC dataset is shown in Table <xref rid="Tab4" ref-type="table">4</xref>. Next, to compare TasselNetv2 with other state-of-the-art methods, several well-established baselines are chosen:<list list-type="bullet"><list-item><p id="Par57">Segmentation method in [<xref ref-type="bibr" rid="CR13">13</xref>]: This is the latest counting by segmentation method specially designed to count wheat spikes in the field. It first applies Laplacian frequency filtering to remove background, then utilizes the median filter to eliminate noise, and finally, finds the maximal to split individual wheat spikes;</p></list-item><list-item><p id="Par58">Density map regression methods: CCNN [<xref ref-type="bibr" rid="CR6">6</xref>] and MCNN [<xref ref-type="bibr" rid="CR5">5</xref>] are two typical counting-by-regression methods, which aim to regress pixel-wise density maps. Their parameters are of the same order of magnitude as TasselNetv2. CSRNet [<xref ref-type="bibr" rid="CR23">23</xref>] represents the state-of-the-art crowd counting approach and is composed of a much deeper CNN (pretrained VGG16) as the front-end used for feature extraction. For a fair comparison, we replace the feature extractor in TasselNetv2 (the first 5 convolutional layers) with all convolutional layers in VGG16 [<xref ref-type="bibr" rid="CR40">40</xref>] and mark it as TasselNetv2<inline-formula id="IEq84"><alternatives><tex-math id="M151">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M152"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq84.gif"/></alternatives></inline-formula>. More details about TasselNetv2<inline-formula id="IEq85"><alternatives><tex-math id="M153">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M154"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq85.gif"/></alternatives></inline-formula> can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref>.</p></list-item><list-item><p id="Par59">Local count regression method: TasselNet [<xref ref-type="bibr" rid="CR9">9</xref>] regresses the local counts rather than density maps. This is our direct baseline and the most closely-related approach. A brief introduction to TasselNet can also be found in Additional file.</p></list-item></list>Results are listed in Table <xref rid="Tab7" ref-type="table">7</xref>. We can make the following observations:<list list-type="bullet"><list-item><p id="Par60">Segmentation method in [<xref ref-type="bibr" rid="CR13">13</xref>] works poorly on the WSC dataset (317.19 MAE). Due to heavy dependency on the color information, this method is very sensitive to the illumination that significantly changes the color attributes. This also implies the problem of counting wheat spikes in the field-based environment cannot be addressed just by segmentation.</p></list-item><list-item><p id="Par61">Density map regression methods, such as CCNN and MCNN, perform much better than the segmentation method, with 101.39 MAE and 97.08 MAE, respectively. It seems that these two CNN-based methods can adapt to the in-field environmental variations and the morphological variations of wheat spikes to a certain degree. Nevertheless we remark that density map prediction may not be suitable for counting wheat spikes, because the ground-truth density map cannot be generated accurately. This is also true for counting other non-rigid objects.</p></list-item><list-item><p id="Par62">TasselNet outperforms CCNN and MCNN on the WSC dataset (61.35 MAE). It considerably reveals the benefit of local counts regression, which is important for object counting problems that have size variations.</p></list-item><list-item><p id="Par63">CSRNet slightly outperforms TasselNetv2 (46.32 MAE versus 50.79 MAE). However, CSRNet not only has substantial parameters, more than an order of magnitude compared to TasselNetv2, but also is greatly benefited from the pre-trained model. Though with these unfair factors, TasselNetv2 still exhibits comparable performances against CSRNet. When TasselNetv2<inline-formula id="IEq86"><alternatives><tex-math id="M155">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M156"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq86.gif"/></alternatives></inline-formula> uses the same pretrained VGG16, it outperforms CSRNet, with 44.27 MAE (<inline-formula id="IEq87"><alternatives><tex-math id="M157">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$91.01\%$$\end{document}</tex-math><mml:math id="M158"><mml:mrow><mml:mn>91.01</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq87.gif"/></alternatives></inline-formula> relative counting accuracy), reaching the state-of-the-art performance on the WSC dataset. As a consequence, for time-sensitive applications, TasselNetv2 is still our recommended choice.</p></list-item></list>
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Comparison with state-of-the-art counting approaches on the test set of WSC dataset. TasselNetv2 adopts an AlexNet-like architecture in Fig. <xref rid="Fig6" ref-type="fig">6</xref> and is trained from scratch</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method</th><th align="left" colspan="2">Henan Zhengzhou (2012–2013)</th><th align="left" colspan="2">Shandong Taian (2012–2013 Camera1)</th><th align="left" colspan="2">Overall</th><th align="left" rowspan="2">#Parameters</th></tr><tr><th align="left">MAE</th><th align="left">RMSE</th><th align="left">MAE</th><th align="left">RMSE</th><th align="left">MAE</th><th align="left">RMSE</th></tr></thead><tbody><tr><td align="left">Segmentation method in [<xref ref-type="bibr" rid="CR13">13</xref>]</td><td char="." align="char">387.09</td><td char="." align="char">436.84</td><td char="." align="char">268.03</td><td char="." align="char">345.78</td><td char="." align="char">317.19</td><td char="." align="char">386.22</td><td align="left"><inline-formula id="IEq74"><alternatives><tex-math id="M159">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M160"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq74.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">CCNN [<xref ref-type="bibr" rid="CR6">6</xref>]</td><td char="." align="char">168.41</td><td char="." align="char">214.41</td><td char="." align="char">52.40</td><td char="." align="char">72.78</td><td char="." align="char">101.39</td><td char="." align="char">149.91</td><td align="left"><inline-formula id="IEq75"><alternatives><tex-math id="M161">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5.70\times 10^5$$\end{document}</tex-math><mml:math id="M162"><mml:mrow><mml:mn>5.70</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq75.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">MCNN [<xref ref-type="bibr" rid="CR5">5</xref>]</td><td char="." align="char">149.44</td><td char="." align="char">188.34</td><td char="." align="char">58.83</td><td char="." align="char">75.50</td><td char="." align="char">97.08</td><td char="." align="char">135.17</td><td align="left"><inline-formula id="IEq76"><alternatives><tex-math id="M163">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.33\times 10^5$$\end{document}</tex-math><mml:math id="M164"><mml:mrow><mml:mn>1.33</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq76.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">CSRNet<inline-formula id="IEq77"><alternatives><tex-math id="M165">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M166"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq77.gif"/></alternatives></inline-formula> [<xref ref-type="bibr" rid="CR23">23</xref>]</td><td char="." align="char">64.19</td><td char="." align="char">88.96</td><td char="." align="char">33.26</td><td char="." align="char"><italic>46.19</italic></td><td char="." align="char">46.32</td><td char="." align="char">67.63</td><td align="left"><inline-formula id="IEq78"><alternatives><tex-math id="M167">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.63\times 10^7$$\end{document}</tex-math><mml:math id="M168"><mml:mrow><mml:mn>1.63</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq78.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">TasselNet [<xref ref-type="bibr" rid="CR9">9</xref>]</td><td char="." align="char">94.97</td><td char="." align="char">137.24</td><td char="." align="char">36.79</td><td char="." align="char">57.37</td><td char="." align="char">61.35</td><td char="." align="char">99.27</td><td align="left"><inline-formula id="IEq79"><alternatives><tex-math id="M169">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$6.38\times 10^5$$\end{document}</tex-math><mml:math id="M170"><mml:mrow><mml:mn>6.38</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq79.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">TasselNetv2</td><td char="." align="char">74.97</td><td char="." align="char">113.21</td><td char="." align="char">33.12</td><td char="." align="char">49.26</td><td char="." align="char">50.79</td><td char="." align="char">80.66</td><td align="left"><inline-formula id="IEq80"><alternatives><tex-math id="M171">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$6.38\times 10^5$$\end{document}</tex-math><mml:math id="M172"><mml:mrow><mml:mn>6.38</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq80.gif"/></alternatives></inline-formula></td></tr><tr><td align="left">TasselNetv2<inline-formula id="IEq81"><alternatives><tex-math id="M173">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M174"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq81.gif"/></alternatives></inline-formula></td><td char="." align="char"><italic>61.57</italic></td><td char="." align="char"><italic>87.67</italic></td><td char="." align="char"><italic>31.62</italic></td><td char="." align="char">47.55</td><td char="." align="char"><italic>44.27</italic></td><td char="." align="char"><italic>67.47</italic></td><td align="left"><inline-formula id="IEq82"><alternatives><tex-math id="M175">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.60\times 10^7$$\end{document}</tex-math><mml:math id="M176"><mml:mrow><mml:mn>1.60</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq82.gif"/></alternatives></inline-formula></td></tr></tbody></table><table-wrap-foot><p><inline-formula id="IEq83"><alternatives><tex-math id="M177">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M178"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq83.gif"/></alternatives></inline-formula> means the model is finetuned from the pretrained VGG16, and layer-by-layer settings can be found in Additional file. The best performance is italics</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec12">
      <title>Evaluation on the MTC dataset</title>
      <p id="Par64">To show that TasselNetv2 is a generic object counting method, particularly for the application in the agriculture scenario. We further evaluate the effectiveness of TasselNetv2 on the Maize Tassels Counting (MTC) [<xref ref-type="bibr" rid="CR9">9</xref>] dataset, following the same setting as [<xref ref-type="bibr" rid="CR9">9</xref>]. Detailed results are shown in Table <xref rid="Tab8" ref-type="table">8</xref>.<table-wrap id="Tab8"><label>Table 8</label><caption><p>Evaluations of different methods on the MTC [<xref ref-type="bibr" rid="CR9">9</xref>] dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">MAE</th><th align="left">RMSE</th></tr></thead><tbody><tr><td align="left">JointSeg [<xref ref-type="bibr" rid="CR41">41</xref>]</td><td char="." align="char">24.2</td><td char="." align="char">31.6</td></tr><tr><td align="left">mTASSEL [<xref ref-type="bibr" rid="CR42">42</xref>]</td><td char="." align="char">19.6</td><td char="." align="char">26.1</td></tr><tr><td align="left">GlobalReg [<xref ref-type="bibr" rid="CR43">43</xref>]</td><td char="." align="char">19.7</td><td char="." align="char">23.3</td></tr><tr><td align="left">DensityReg [<xref ref-type="bibr" rid="CR44">44</xref>]</td><td char="." align="char">11.9</td><td char="." align="char">14.8</td></tr><tr><td align="left">CCNN [<xref ref-type="bibr" rid="CR6">6</xref>]</td><td char="." align="char">21.0</td><td char="." align="char">25.5</td></tr><tr><td align="left">TasselNet [<xref ref-type="bibr" rid="CR9">9</xref>]</td><td char="." align="char">6.6</td><td char="." align="char">9.6</td></tr><tr><td align="left">TasselNetv2</td><td char="." align="char">5.4</td><td char="." align="char"><italic>8.8</italic></td></tr><tr><td align="left">TasselNetv2<inline-formula id="IEq91"><alternatives><tex-math id="M179">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M180"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq91.gif"/></alternatives></inline-formula></td><td char="." align="char"><italic>5.3</italic></td><td char="." align="char">9.4</td></tr></tbody></table><table-wrap-foot><p><inline-formula id="IEq92"><alternatives><tex-math id="M181">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M182"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq92.gif"/></alternatives></inline-formula> means the model is finetuned from the pretrained VGG16. The best performance is in italics</p></table-wrap-foot></table-wrap></p>
      <p id="Par65">TasselNet currently represents the state-of-the-art approach on the MTC dataset. According to the results, we found that TasselNetv2 outperforms TasselNet and further reduces the counting error by <inline-formula id="IEq88"><alternatives><tex-math id="M183">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$18.2\%$$\end{document}</tex-math><mml:math id="M184"><mml:mrow><mml:mn>18.2</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq88.gif"/></alternatives></inline-formula> (5.4 MAE versus 6.6 MAE). The context is also an important factor for maize tassels.</p>
      <p id="Par66">With a pre-trained model, TasselNetv2<inline-formula id="IEq89"><alternatives><tex-math id="M185">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M186"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq89.gif"/></alternatives></inline-formula> only performs slightly better than TasselNetv2 but increases more than an order of magnitude of parameters. We conjecture the main reason is the lack of training samples in the MTC dataset (only 186 training images). The potential of pre-trained models may not be fully exploited with such a small dataset, while a small network, such as TasselNetv2, can already produce satisfactory results. In this case, TasselNetv2 is effective and efficient, which seems to be a better choice than TasselNetv2<inline-formula id="IEq90"><alternatives><tex-math id="M187">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M188"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq90.gif"/></alternatives></inline-formula>.
</p>
    </sec>
    <sec id="Sec13">
      <title>Evaluation on the ShanghaiTech dataset</title>
      <p id="Par67">We further evaluate TasselNetv2 on the ShanghaiTech dataset [<xref ref-type="bibr" rid="CR5">5</xref>] to see its generality to crowd counting, following the same experimental setting in [<xref ref-type="bibr" rid="CR5">5</xref>]. Results are listed in Table <xref rid="Tab9" ref-type="table">9</xref>.<table-wrap id="Tab9"><label>Table 9</label><caption><p>Evaluations on the ShanghaiTech [<xref ref-type="bibr" rid="CR5">5</xref>] dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2">Method</th><th align="left" colspan="2">Part A</th><th align="left" colspan="2">Part B</th></tr><tr><th align="left">MAE</th><th align="left">RMSE</th><th align="left">MAE</th><th align="left">RMSE</th></tr></thead><tbody><tr><td align="left">MCNN [<xref ref-type="bibr" rid="CR5">5</xref>]</td><td char="." align="char">110.2</td><td char="." align="char">173.2</td><td char="." align="char">26.4</td><td char="." align="char">41.3</td></tr><tr><td align="left">CP-CNN [<xref ref-type="bibr" rid="CR25">25</xref>]</td><td char="." align="char">73.6</td><td char="." align="char">106.4</td><td char="." align="char">20.1</td><td char="." align="char">30.1</td></tr><tr><td align="left">ACSCP [<xref ref-type="bibr" rid="CR24">24</xref>]</td><td char="." align="char">75.7</td><td char="." align="char"><italic>102.7</italic></td><td char="." align="char">17.2</td><td char="." align="char">27.4</td></tr><tr><td align="left">CSRNet<inline-formula id="IEq94"><alternatives><tex-math id="M189">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M190"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq94.gif"/></alternatives></inline-formula> [<xref ref-type="bibr" rid="CR23">23</xref>]</td><td char="." align="char">68.2</td><td char="." align="char">115.0</td><td char="." align="char">10.6</td><td char="." align="char"><italic>16.0</italic></td></tr><tr><td align="left">TasselNet [<xref ref-type="bibr" rid="CR9">9</xref>]</td><td char="." align="char">87.0</td><td char="." align="char">138.9</td><td char="." align="char">16.7</td><td char="." align="char">28.1</td></tr><tr><td align="left">TasselNetv2</td><td char="." align="char">84.1</td><td char="." align="char">140.1</td><td char="." align="char">15.3</td><td char="." align="char">27.8</td></tr><tr><td align="left">TasselNetv2<inline-formula id="IEq95"><alternatives><tex-math id="M191">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M192"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq95.gif"/></alternatives></inline-formula></td><td char="." align="char"><italic>66.8</italic></td><td char="." align="char">112.1</td><td char="." align="char"><italic>9.6</italic></td><td char="." align="char">17.5</td></tr></tbody></table><table-wrap-foot><p><inline-formula id="IEq96"><alternatives><tex-math id="M193">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M194"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq96.gif"/></alternatives></inline-formula> means the model is fine-tuned from the pretrained VGG16. The best performance is in italics</p></table-wrap-foot></table-wrap></p>
      <p id="Par68">On both the part A and part B subsets, the benefit of adding the context can be reflected when comparing TasselNetv2 with TasselNet, but the improvement is marginal. When using a pre-trained VGG-16 model, TasselNetv2<inline-formula id="IEq93"><alternatives><tex-math id="M195">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^\dagger$$\end{document}</tex-math><mml:math id="M196"><mml:msup><mml:mrow/><mml:mo>†</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq93.gif"/></alternatives></inline-formula> outperforms CSRNet and reaches the state-of-the-art performance. This suggests pre-trained models is necessary to fully exploit the benefit of context on the ShanghaiTech dataset.</p>
    </sec>
    <sec id="Sec14">
      <title>Some failure cases</title>
      <p id="Par69">Figure <xref rid="Fig10" ref-type="fig">10</xref> shows some qualitative results of TasselNetv2 on the WSC dataset. In most cases, TasselNetv2 predicts accurate counts (the first four rows). However, it exposes prominent under-estimate phenomena in some cases, particularly when severe overlapping and heavy blurring occur. These visual patterns raise a huge challenge to discriminate spikes even for a human expert. Efforts still should be paid to overcome these challenges. We leave this for future explorations.<fig id="Fig10"><label>Fig. 10</label><caption><p>Some ground truth density maps overlaid on original images on the test set of the WSC dataset and count maps generated by TasselNetv2 (finetuned with pre-trained VGG16). The number above each original image denotes the ground truth count number of wheat spikes, while that above each density map denotes prediction count number. The last line shows some unsuccessful predictions, and error maps of these images are also presented. An error map denotes the difference of the ground truth and predicted density map. Over-estimate is denoted by red, under-estimate by blue, and minor difference by gray. The darker the color is, the greater the errors are. We also zoom in some local areas with high counting errors. ’GT’ denotes ground-truth counts and ’Error’ denotes the difference compared to the ground truth. Further visualizations can be found in Additional file 1.</p></caption><graphic xlink:href="13007_2019_537_Fig10_HTML" id="MO12"/></fig>
</p>
    </sec>
  </sec>
  <sec id="Sec15">
    <title>Conclusions</title>
    <p id="Par70">In this work, we addressed an important and practical problem of counting wheat spikes in the field-based environment using computer vision. We observe that, some existing CNN-based local regression models, such as TasselNet, suffer from the problem of lacking contextual information, so they usually cannot predict correct counts when objects partially present in local image patches. By integrating the context into the framework of the TasselNet, we proposed a simple but effective extension, i.e., TasselNetv2. A large-scale WSC dataset, with 1, 764 images and 675, 322 annotated wheat spikes, is also created. The dataset is very challenging due to intrinsic and extrinsic variations not only in spikes per se but also in environment, which makes it appropriate to be used as a benchmark for counting non-rigid objects.</p>
    <p id="Par71">Extensive experiments illustrate that, TasselNetv2 achieves state-of-the-art performance on the WSC dataset with <inline-formula id="IEq97"><alternatives><tex-math id="M197">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$91.01\%$$\end{document}</tex-math><mml:math id="M198"><mml:mrow><mml:mn>91.01</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="13007_2019_537_Article_IEq97.gif"/></alternatives></inline-formula> relative counting accuracy, and is also more than an order of magnitude faster than TasselNet. Further evaluations on the MTC and ShanghaiTech datasets demonstrate that TasselNetv2 can also push forward the state of the art. Sufficient analyses of potential issues effecting the practical application of TasselNetv2 are also described, including emphasizing the role of the context in object counting, searching optimal parameters for local counts regression, and analyzing potential errors. We believe TasselNetv2 shows great potentials to be applied to other object counting domains.</p>
    <p id="Par72">Albeit empirically effective, the reason why the context can improve the counting performance only stays at an intuitive level, and it remains unclear how the context interacts with the central receptive field as auxiliary information. We hope such empirical findings in this paper could inspire others to uncover the mystery of the receptive field.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec50">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="13007_2019_537_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1.</bold> More details about the WSC dataset, experiment settings and results. A brief introduction and analysis to the TasselNet [<xref ref-type="bibr" rid="CR9">9</xref>] are also included.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s13007-019-0537-2.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors would like to thank the Wuxi Institute of Radio Science and Technology for providing the facilities and equipment, and X. Xiong, C. X. Liu, H. Z. Qi, W. X. Jiang, T. D. Yu, Z. H. Zhu for their assistance in annotating the WSC dataset.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author's contributions</title>
    <p>HX proposed the idea of TasselNetv2 and implemented the experiments. Both HX and HL drafted the manuscript, while LL and MS helped design the experiments and analyse the results. ZG and CS co-supervised the study and contributed in writing the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by the Natural Science Foundation of China under Grant No. 61876211. CS’ participation was in part supported by the ARC industrial transformation research hub for driving farming productivity and disease prevention.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The WSC dataset and other supporting materials are made available online at: <ext-link ext-link-type="uri" xlink:href="https://tinyurl.com/TasselNetv2">https://tinyurl.com/TasselNetv2</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p id="Par74">Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p id="Par75">Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par76">The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Pask AJD, Pietragalla J, Mullan DM, Reynolds MP. Physiological breeding ii: a field guide to wheat phenotyping. Cimmyt. 2012;95–103.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Slafer GA, Calderini DF, Miralles DJ. Yield components and compensation in wheat: opportunities for further increasing yield potential. Increasing yield potential in wheat: breaking the barriers. 1996.</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ferrante</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cartelle</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Savin</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Slafer</surname>
            <given-names>GA</given-names>
          </name>
        </person-group>
        <article-title>Yield determination, interplay between major components and yield stability in a traditional and a contemporary wheat across a wide range of environments</article-title>
        <source>Field Crops Res</source>
        <year>2017</year>
        <volume>203</volume>
        <fpage>114</fpage>
        <lpage>127</lpage>
        <pub-id pub-id-type="doi">10.1016/j.fcr.2016.12.028</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Zhang C, Li H, Wang X, Yang X. Cross-scene crowd counting via deep convolutional neural networks. In: Proc. IEEE international conference on computer vision (ICCV), 2015. p. 833–41.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Zhang Y, Zhou D, Chen S, Gao S, Ma Y. Single-image crowd counting via multi-column convolutional neural network. In: Proc. IEEE conference on computer vision and pattern recognition (CVPR), 2016. p. 589–97.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Oñoro-Rubio</surname>
            <given-names>Daniel</given-names>
          </name>
          <name>
            <surname>López-Sastre</surname>
            <given-names>Roberto J.</given-names>
          </name>
        </person-group>
        <article-title>Towards Perspective-Free Object Counting with Deep Learning</article-title>
        <source>Computer Vision – ECCV 2016</source>
        <year>2016</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer International Publishing</publisher-name>
        <fpage>615</fpage>
        <lpage>629</lpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xie</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Microscopy cell counting and detection with fully convolutional regression networks</article-title>
        <source>Comput Methods Biomech Biomed Eng Imaging Vis</source>
        <year>2018</year>
        <volume>6</volume>
        <issue>3</issue>
        <fpage>283</fpage>
        <lpage>292</lpage>
        <pub-id pub-id-type="doi">10.1080/21681163.2016.1149104</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Arteta</surname>
            <given-names>Carlos</given-names>
          </name>
          <name>
            <surname>Lempitsky</surname>
            <given-names>Victor</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>Andrew</given-names>
          </name>
        </person-group>
        <article-title>Counting in the Wild</article-title>
        <source>Computer Vision – ECCV 2016</source>
        <year>2016</year>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer International Publishing</publisher-name>
        <fpage>483</fpage>
        <lpage>498</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhuang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>TasselNet: counting maize tassels in the wild via local counts regression network</article-title>
        <source>Plant Methods</source>
        <year>2017</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>79</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1186/s13007-017-0224-0</pub-id>
        <pub-id pub-id-type="pmid">29118821</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Aich S, Josuttes A, Ovsyannikov I, Strueby K, Ahmed I, Duddu HS, Pozniak C, Shirtliffe S, Stavness I. Deepwheat: Estimating phenotypic traits from crop images with deep learning. In: Proc. IEEE winter conference on applications of computer vision (WACV), 2018. p. 323–32.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rahnemoonfar</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sheppard</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Deep count: fruit counting based on deep simulated learning</article-title>
        <source>Sensors</source>
        <year>2017</year>
        <volume>17</volume>
        <issue>4</issue>
        <fpage>905</fpage>
        <pub-id pub-id-type="doi">10.3390/s17040905</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>SW</given-names>
          </name>
          <name>
            <surname>Skandan</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Dcunha</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Okon</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Qu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Counting apples and oranges with deep learning: a data driven approach</article-title>
        <source>IEEE Robot Autom Lett</source>
        <year>2017</year>
        <volume>2</volume>
        <issue>2</issue>
        <fpage>781</fpage>
        <lpage>788</lpage>
        <pub-id pub-id-type="doi">10.1109/LRA.2017.2651944</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fernandez-Gallego</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Kefauver</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Gutiérrez</surname>
            <given-names>NA</given-names>
          </name>
          <name>
            <surname>Nieto-Taladriz</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Araus</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Wheat ear counting in-field conditions: high throughput and low-cost approach using RGB images</article-title>
        <source>Plant Methods</source>
        <year>2018</year>
        <volume>14</volume>
        <issue>1</issue>
        <fpage>22</fpage>
        <pub-id pub-id-type="doi">10.1186/s13007-018-0289-4</pub-id>
        <pub-id pub-id-type="pmid">29568319</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Berger</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Okamoto</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Miklavcic</surname>
            <given-names>SJ</given-names>
          </name>
        </person-group>
        <article-title>Detecting spikes of wheat plants using neural networks with laws texture energy</article-title>
        <source>Plant Methods</source>
        <year>2017</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>83</fpage>
        <pub-id pub-id-type="doi">10.1186/s13007-017-0231-1</pub-id>
        <pub-id pub-id-type="pmid">29046709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Alharbi N, Zhou J, Wang W. Automatic counting of wheat spikes from wheat growth images. In: International conference on pattern recognition applications and methods. 2018. p. 346–55.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yue</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Wheat ears counting in field conditions based on multi-feature optimization and TWSVM</article-title>
        <source>Front Plant Sci</source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>1024</fpage>
        <pub-id pub-id-type="doi">10.3389/fpls.2018.01024</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Recognition of wheat spike from field based phenotype platform using multi-sensor fusion and improved maximum entropy segmentation algorithms</article-title>
        <source>Remote Sens</source>
        <year>2018</year>
        <volume>10</volume>
        <issue>2</issue>
        <fpage>246</fpage>
        <pub-id pub-id-type="doi">10.3390/rs10020246</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hasan</surname>
            <given-names>MM</given-names>
          </name>
          <name>
            <surname>Chopin</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Laga</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Miklavcic</surname>
            <given-names>SJ</given-names>
          </name>
        </person-group>
        <article-title>Detection and analysis of wheat spikes using convolutional neural networks</article-title>
        <source>Plant Methods</source>
        <year>2018</year>
        <volume>14</volume>
        <issue>1</issue>
        <fpage>100</fpage>
        <pub-id pub-id-type="doi">10.1186/s13007-018-0366-8</pub-id>
        <pub-id pub-id-type="pmid">30459822</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Madec</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>De Solan</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Duyme</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Heritier</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Baret</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Ear density estimation from high resolution RGB imagery using deep learning technique</article-title>
        <source>Agric Forest Meteorol</source>
        <year>2019</year>
        <volume>264</volume>
        <fpage>225</fpage>
        <lpage>234</lpage>
        <pub-id pub-id-type="doi">10.1016/j.agrformet.2018.10.013</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Girshick R, Donahue J, Darrell T, Malik J. Rich feature hierarchies for accurate object detection and semantic segmentation. In: Proc. IEEE conference on computer vision and pattern recognition (CVPR), 2014. p. 580–7.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ghosal</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Chapman</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Potgieter</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Jordan</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hirafuji</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ninomiya</surname>
            <given-names>S</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A weakly supervised deep learning framework for sorghum head detection and counting</article-title>
        <source>Plant Phenomics</source>
        <year>2019</year>
        <volume>2019</volume>
        <fpage>1525874</fpage>
        <pub-id pub-id-type="doi">10.34133/2019/1525874</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Lin T-Y, Goyal P, Girshick R, He K, Dollár P. Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision, 2017. p. 2980–8.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Li Y, Zhang X, Chen D. CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes. In: Proc. IEEE conference on computer vision and pattern recognition (CVPR), 2018. p. 1091–100.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Shen Z, Xu Y, Ni B, Wang M, Hu J, Yang X. Crowd counting via adversarial cross-scale consistency pursuit. In: Proc. IEEE conference on computer vision and pattern recognition (CVPR) 2018.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Sindagi VA, Patel VM. Generating high-quality crowd density maps using contextual pyramid cnns. In: Proc. IEEE international conference on computer vision (ICCV), 2017. p. 1879–88.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Cohen JP, Boucher G, Glastonbury CA, Lo HZ, Bengio Y. Count-ception: Counting by fully convolutional redundant counting. In: Proc. IEEE international conference on computer vision workshop (ICCVW), 2017. p. 18–26.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Girshick R. Fast R-CNN. In: Proc. IEEE conference on computer vision and pattern recognition (ICCV), 2015. p. 1440–8.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Toward good practices for fine-grained maize cultivar identification with filter-specific convolutional activations</article-title>
        <source>IEEE Trans Autom Sci Eng</source>
        <year>2018</year>
        <volume>15</volume>
        <issue>2</issue>
        <fpage>430</fpage>
        <lpage>442</lpage>
        <pub-id pub-id-type="doi">10.1109/TASE.2016.2616485</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Seguí S, Pujol O, Vitrià J. Learning to count with deep object features. In: Proc. IEEE conference on computer vision and pattern recognition workshops (CVPRW), 2015. p. 90–6.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Eggert C, Brehm S, Winschel A, Dan Z, Lienhart R. A closer look: Small object detection in faster r-cnn. In: Proc. IEEE international conference on multimedia and expo (ICME), 2017. p. 421–6.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Lin M, Chen Q, Yan S. Network in network. In: Proc. International conference on learning representations (ICLR) 2013.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation. In: Proc. IEEE conference on computer vision and pattern recognition (CVPR), 2015. p. 3431–40.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Howard AG, Zhu M, Chen B, Kalenichenko D, Wang W, Weyand T, Andreetto M, Adam H. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1704.04861">arXiv:1704.04861</ext-link> 2017.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Zhang X, Zhou X, Lin M, Sun J. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In: Proceedings of the IEEE conference on computer vision and pattern recognition, 2018. p. 6848–6856.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Vedaldi A, Lenc K. MatConvNet: Convolutional neural networks for MATLAB. In: Proc. ACM international conference on multimedia, 2015. p. 689–92.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In: Proc. IEEE international conference on computer vision (ICCV), 2015. p. 1026–34.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In: Proc. international conference on machine learning (ICML), 2015. p. 448–56.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Zhou B, Khosla A, Lapedriza A, Oliva A, Torralba A. Object detectors emerge in deep scene CNNs. In: Proc. international conference on learning representations (ICLR) 2014.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Luo W, Li Y, Urtasun R, Zemel R. Understanding the effective receptive field in deep convolutional neural networks. In: Advances in neural information processing systems (NIPS), 2016. p. 4898–906.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. Computer Science 2014.</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Region-based colour modelling for joint crop and maize tassel segmentation</article-title>
        <source>Biosyst Eng</source>
        <year>2016</year>
        <volume>147</volume>
        <fpage>139</fpage>
        <lpage>150</lpage>
        <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2016.04.007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xian</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Fine-grained maize tassel trait characterization with multi-view representations</article-title>
        <source>Comput Electron Agric</source>
        <year>2015</year>
        <volume>118</volume>
        <fpage>143</fpage>
        <lpage>158</lpage>
        <pub-id pub-id-type="doi">10.1016/j.compag.2015.08.027</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Tota K, Idrees H. Counting in dense crowds using deep features. CRCV 2015.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Lempitsky V, Zisserman A. Learning to count objects in images. In: Advances in neural information processing systems (NIPS), 2010. p. 1324–32.</mixed-citation>
    </ref>
  </ref-list>
</back>
