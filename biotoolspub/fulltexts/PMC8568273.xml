<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8568273</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0259448</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-21-21539</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Physics</subject>
          <subj-group>
            <subject>Thermodynamics</subject>
            <subj-group>
              <subject>Entropy</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Information Theory</subject>
          <subj-group>
            <subject>Information Entropy</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Software Tools</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Software Tools</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Programming Languages</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Programming Languages</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Computer Software</subject>
            <subj-group>
              <subject>Open Source Software</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Computer Software</subject>
            <subj-group>
              <subject>Open Source Software</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Science Policy</subject>
        <subj-group>
          <subject>Open Science</subject>
          <subj-group>
            <subject>Open Source Software</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Electronics Engineering</subject>
          <subj-group>
            <subject>Computer Engineering</subject>
            <subj-group>
              <subject>Man-Computer Interface</subject>
              <subj-group>
                <subject>Graphical User Interfaces</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Architecture</subject>
          <subj-group>
            <subject>User Interfaces</subject>
            <subj-group>
              <subject>Graphical User Interfaces</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Information Theory</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Discrete Mathematics</subject>
            <subj-group>
              <subject>Combinatorics</subject>
              <subj-group>
                <subject>Permutation</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EntropyHub: An open-source toolkit for entropic time series analysis</article-title>
      <alt-title alt-title-type="running-head">Entropy analysis in MatLab, Python and Julia</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5674-424X</contrib-id>
        <name>
          <surname>Flood</surname>
          <given-names>Matthew W.</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="cor001" ref-type="corresp">*</xref>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Grimm</surname>
          <given-names>Bernd</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff001" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <addr-line>Human Motion, Orthopaedics, Sports Medicine and Digital Methods (HOSD), Luxembourg Institute of Health (LIH), Eich, Luxembourg</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Rezakazemi</surname>
          <given-names>Mashallah</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Shahrood University of Technology, ISLAMIC REPUBLIC OF IRAN</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>matthew.flood@lih.lu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>4</day>
      <month>11</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>16</volume>
    <issue>11</issue>
    <elocation-id>e0259448</elocation-id>
    <history>
      <date date-type="received">
        <day>1</day>
        <month>7</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>10</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 Flood, Grimm</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Flood, Grimm</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0259448.pdf"/>
    <abstract>
      <p>An increasing number of studies across many research fields from biomedical engineering to finance are employing measures of entropy to quantify the regularity, variability or randomness of time series and image data. Entropy, as it relates to information theory and dynamical systems theory, can be estimated in many ways, with newly developed methods being continuously introduced in the scientific literature. Despite the growing interest in entropic time series and image analysis, there is a shortage of validated, open-source software tools that enable researchers to apply these methods. To date, packages for performing entropy analysis are often run using graphical user interfaces, lack the necessary supporting documentation, or do not include functions for more advanced entropy methods, such as cross-entropy, multiscale cross-entropy or bidimensional entropy. In light of this, this paper introduces <italic toggle="yes">EntropyHub</italic>, an open-source toolkit for performing entropic time series analysis in MATLAB, Python and Julia. EntropyHub (version 0.1) provides an extensive range of more than forty functions for estimating cross-, multiscale, multiscale cross-, and bidimensional entropy, each including a number of keyword arguments that allows the user to specify multiple parameters in the entropy calculation. Instructions for installation, descriptions of function syntax, and examples of use are fully detailed in the supporting documentation, available on the EntropyHub website– <ext-link xlink:href="http://www.entropyhub.xyz/" ext-link-type="uri">www.EntropyHub.xyz</ext-link>. Compatible with Windows, Mac and Linux operating systems, EntropyHub is hosted on GitHub, as well as the native package repository for MATLAB, Python and Julia, respectively. The goal of EntropyHub is to integrate the many established entropy methods into one complete resource, providing tools that make advanced entropic time series analysis straightforward and reproducible.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution>luxembourg institute of health</institution>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5674-424X</contrib-id>
          <name>
            <surname>Flood</surname>
            <given-names>Matthew W.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution>luxembourg insitute of health</institution>
        </funding-source>
        <principal-award-recipient>
          <name>
            <surname>Grimm</surname>
            <given-names>Bernd</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This research was funded by the Luxembourg Institute of Health (<ext-link xlink:href="https://www.lih.lu" ext-link-type="uri">https://www.lih.lu</ext-link>). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="3"/>
      <page-count count="20"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>No data is related to the present manuscript.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>No data is related to the present manuscript.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Through the lens of probability, information and uncertainty can be viewed as conversely related—the more uncertainty there is, the more information we gain by removing that uncertainty. This is the principle behind Shannon’s formulation of entropy (1) which quantifies uncertainty as it pertains to random processes [<xref rid="pone.0259448.ref001" ref-type="bibr">1</xref>]:
<disp-formula id="pone.0259448.e001"><alternatives><graphic xlink:href="pone.0259448.e001.jpg" id="pone.0259448.e001g" position="anchor"/><mml:math id="M1" display="block" overflow="scroll"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives><label>(1)</label></disp-formula>
where <italic toggle="yes">H</italic>(<italic toggle="yes">X</italic>) is the entropy (<italic toggle="yes">H</italic>) of a sequence (<italic toggle="yes">X</italic>) given the probabilities (<italic toggle="yes">p</italic>) of states (<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>). An extension of Shannon’s entropy, conditional entropy (2) measures the information gained about a process (<italic toggle="yes">X</italic>) conditional on prior information given by a process <italic toggle="yes">Y</italic>,
<disp-formula id="pone.0259448.e002"><alternatives><graphic xlink:href="pone.0259448.e002.jpg" id="pone.0259448.e002g" position="anchor"/><mml:math id="M2" display="block" overflow="scroll"><mml:mi>H</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>|</mml:mo><mml:mrow><mml:mi>Y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mspace width="0.25em"/><mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></alternatives><label>(2)</label></disp-formula>
where <italic toggle="yes">y</italic> may represent states of a separate system or previous states of the same system. Numerous variants have since been derived from conditional entropy, and to a lesser extent Shannon’s entropy, to estimate the information content of time series data across various scientific domains [<xref rid="pone.0259448.ref002" ref-type="bibr">2</xref>], resulting in what has recently been termed “the entropy universe” [<xref rid="pone.0259448.ref003" ref-type="bibr">3</xref>]. This universe of entropies continues to expand as more and more methods are derived with improved statistical properties over their precursors, such as robustness to short signal lengths [<xref rid="pone.0259448.ref004" ref-type="bibr">4</xref>–<xref rid="pone.0259448.ref007" ref-type="bibr">7</xref>], resilience to noise [<xref rid="pone.0259448.ref008" ref-type="bibr">8</xref>–<xref rid="pone.0259448.ref010" ref-type="bibr">10</xref>], insensitivity to amplitude fluctuations [<xref rid="pone.0259448.ref011" ref-type="bibr">11</xref>–<xref rid="pone.0259448.ref013" ref-type="bibr">13</xref>]. Furthermore, new entropy variants are being identified which quantify the variability of time series data in specific applications, including assessments of cardiac disease from electrocardiograms [<xref rid="pone.0259448.ref014" ref-type="bibr">14</xref>–<xref rid="pone.0259448.ref016" ref-type="bibr">16</xref>], and examinations of machine failure from vibration signals [<xref rid="pone.0259448.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0259448.ref018" ref-type="bibr">18</xref>].</p>
    <p>As the popularity of entropy spreads beyond the field of mathematics to subjects ranging from neurophysiology [<xref rid="pone.0259448.ref019" ref-type="bibr">19</xref>–<xref rid="pone.0259448.ref023" ref-type="bibr">23</xref>] to finance [<xref rid="pone.0259448.ref024" ref-type="bibr">24</xref>–<xref rid="pone.0259448.ref027" ref-type="bibr">27</xref>], there is an emerging demand for software packages with which to perform entropic time series analysis. Open-source software plays a critical role in tackling the replication crisis in science by providing validated algorithmic tools that are available to all researchers [<xref rid="pone.0259448.ref028" ref-type="bibr">28</xref>, <xref rid="pone.0259448.ref029" ref-type="bibr">29</xref>]. Without access to these software tools, researchers lacking computer programming literacy may resort to borrowing algorithms from unverified sources which could be vulnerable to coding errors. Furthermore, software packages often serve as entry points for researchers unfamiliar with a subject to develop an understanding of the most commonly used methods and how they are applied. This point is particularly relevant in the context of entropy, a concept that is often misinterpreted [<xref rid="pone.0259448.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0259448.ref030" ref-type="bibr">30</xref>, <xref rid="pone.0259448.ref031" ref-type="bibr">31</xref>], and where the name and number variant methods may be difficult to follow. For example, derivatives of the original sample entropy algorithm [<xref rid="pone.0259448.ref032" ref-type="bibr">32</xref>], already an improvement on approximate entropy [<xref rid="pone.0259448.ref033" ref-type="bibr">33</xref>], include modified sample entropy (fuzzy entropy) [<xref rid="pone.0259448.ref034" ref-type="bibr">34</xref>], multiscale (sample) entropy [<xref rid="pone.0259448.ref015" ref-type="bibr">15</xref>], composite multiscale entropy [<xref rid="pone.0259448.ref035" ref-type="bibr">35</xref>], refined multiscale entropy [<xref rid="pone.0259448.ref014" ref-type="bibr">14</xref>], and refined-composite multiscale entropy [<xref rid="pone.0259448.ref036" ref-type="bibr">36</xref>].</p>
    <p>Several packages offering entropy-related functions have been released in recent years [<xref rid="pone.0259448.ref037" ref-type="bibr">37</xref>–<xref rid="pone.0259448.ref039" ref-type="bibr">39</xref>], intended primarily for the analysis of physiological data, <xref rid="pone.0259448.t001" ref-type="table">Table 1</xref>. Although these packages offer some useful tools, they lack the capacity to perform extensive data analysis with multiple methods from the cross-entropy [<xref rid="pone.0259448.ref040" ref-type="bibr">40</xref>], bidimensional entropy [<xref rid="pone.0259448.ref041" ref-type="bibr">41</xref>], and multiscale entropy [<xref rid="pone.0259448.ref042" ref-type="bibr">42</xref>] families of algorithms. Additionally, the utility of these packages is also limited for several reasons. The <italic toggle="yes">CEPS</italic> [<xref rid="pone.0259448.ref038" ref-type="bibr">38</xref>], <italic toggle="yes">EZ Entropy</italic> [<xref rid="pone.0259448.ref037" ref-type="bibr">37</xref>] and <italic toggle="yes">PyBioS</italic> [<xref rid="pone.0259448.ref039" ref-type="bibr">39</xref>] packages all operate through graphical user interfaces (GUIs) with facilities to plot and process data interactively. The interactive nature of GUIs can be beneficial when analysing small datasets but becomes burdensome when analysing large datasets where automated processing tasks are advantageous. Both the <italic toggle="yes">CEPS</italic> [<xref rid="pone.0259448.ref038" ref-type="bibr">38</xref>] and <italic toggle="yes">EZ Entropy</italic> [<xref rid="pone.0259448.ref037" ref-type="bibr">37</xref>] are designed for the MATLAB programming environment (MathWorks, MA, USA) which requires a purchased license in order to use. This paywall prevents many users from accessing the software and consequently impedes the replication of results achieved by using these packages. Neither <italic toggle="yes">PyBioS</italic> nor <italic toggle="yes">EZ Entropy</italic> have accompanying documentation to describe how to use the software, and neither toolbox is hosted on the native package repository for MATLAB (<ext-link xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/" ext-link-type="uri">MathWorks File Exchange</ext-link>) or Python (<ext-link xlink:href="https://pypi.org/" ext-link-type="uri">PyPi</ext-link>), which facilitate direct and simplified installation and updating.</p>
    <table-wrap position="float" id="pone.0259448.t001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0259448.t001</object-id>
      <label>Table 1</label>
      <caption>
        <title>A list of resources providing entropy analysis tools.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="pone.0259448.t001" id="pone.0259448.t001g" position="float"/>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">Name</th>
              <th align="left" rowspan="1" colspan="1">Language</th>
              <th align="left" rowspan="1" colspan="1">Interface</th>
              <th align="left" rowspan="1" colspan="1">Access Links</th>
              <th align="left" rowspan="1" colspan="1">Details</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left" rowspan="3" colspan="1">
                <bold>EntropyHub</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">MATLAB</td>
              <td align="left" rowspan="3" colspan="1">Command Line</td>
              <td align="left" rowspan="3" colspan="1">• <ext-link xlink:href="http://www.mathworks.com/matlabcentral/fileexchange/94185-entropyhub" ext-link-type="uri">MATLAB Add-On Explorer</ext-link><break/>• Python Package Index (PyPi)<break/>• JuliaHub<break/>• GitHub<break/>• <ext-link xlink:href="https://mattwillflood.github.io/EntropyHub.jl/v0.1.1/" ext-link-type="uri">Julia GitHub Repo</ext-link><break/>• <ext-link xlink:href="http://www.entropyhub.xyz/" ext-link-type="uri">www.EntropyHub.xyz</ext-link></td>
              <td align="left" rowspan="3" colspan="1">See <xref rid="pone.0259448.t002" ref-type="table">Table 2</xref> for full list of functions in version 0.1. EntropyHub provides 18 <italic toggle="yes">Base</italic> entropy methods for univariate data analysis (e.g. sample entropy, fuzzy entropy, etc.), and 8 <italic toggle="yes">Cross-</italic>entropy methods (e.g. cross-permutation entropy, cross-distribution entropy). There are also 4 bidimensional entropy methods for 2D/image analysis (e.g. bidimensional dispersion entropy, bidimensional sample entropy). There are also several multiscale entropy variants available which can utilise each of the <italic toggle="yes">Base</italic> and <italic toggle="yes">Cross-</italic>entropy methods.</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Python</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Julia</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">CEPS [<xref rid="pone.0259448.ref038" ref-type="bibr">38</xref>]</td>
              <td align="left" rowspan="1" colspan="1">MATLAB</td>
              <td align="left" rowspan="1" colspan="1">GUI</td>
              <td align="left" rowspan="1" colspan="1">BitBucket</td>
              <td align="left" rowspan="1" colspan="1">Includes Shannon, Rényi, minimum, Tsallis, Kolmogorov-Sinai, conditional, corrected-conditional, approximate, sample, fuzzy, permutation, distribution, dispersion, phase, slope, bubble, spectral, differential, diffusion, and multiscale entropy methods.</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">PyBios [<xref rid="pone.0259448.ref039" ref-type="bibr">39</xref>]</td>
              <td align="left" rowspan="1" colspan="1">Python</td>
              <td align="left" rowspan="1" colspan="1">GUI</td>
              <td align="left" rowspan="1" colspan="1">
                <italic toggle="yes">Contact Author</italic>
              </td>
              <td align="left" rowspan="1" colspan="1">Includes sample, fuzzy, permutation, distribution, dispersion, phase, multiscale entropy methods.</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">EZ Entropy [<xref rid="pone.0259448.ref037" ref-type="bibr">37</xref>]</td>
              <td align="left" rowspan="1" colspan="1">MATLAB</td>
              <td align="left" rowspan="1" colspan="1">GUI</td>
              <td align="left" rowspan="1" colspan="1">GitHub</td>
              <td align="left" rowspan="1" colspan="1">Includes approximate, sample, fuzzy, permutation, distribution and conditional entropy methods.</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">PhysioNet [<xref rid="pone.0259448.ref043" ref-type="bibr">43</xref>]</td>
              <td align="left" rowspan="1" colspan="1">MATLAB C*</td>
              <td align="left" rowspan="1" colspan="1">Command Line</td>
              <td align="left" rowspan="1" colspan="1">
                <ext-link xlink:href="http://www.physionet.org/" ext-link-type="uri">www.PhysioNet.org</ext-link>
              </td>
              <td align="left" rowspan="1" colspan="1">Provides standalone functions for sample, multiscale and transfer entropies<xref rid="t001fn002" ref-type="table-fn">*</xref>.</td>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="t001fn001">
          <p>Listed next to each tool are the programming languages they support, the interface through which they operate, links to access the software, and a brief outline of the entropy analysis tools they provide.</p>
        </fn>
        <fn id="t001fn002">
          <p>* A C-programming implementation of transfer entropy is currently not available on PhysioNet.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p>Against this background, this paper introduces <italic toggle="yes">EntropyHub</italic>, an open-source toolkit for entropic time series analysis in the MATLAB, Python [<xref rid="pone.0259448.ref044" ref-type="bibr">44</xref>] and Julia [<xref rid="pone.0259448.ref045" ref-type="bibr">45</xref>] programming environments. Incorporating entropy estimators from information theory, probability theory and dynamical systems theory, EntropyHub features a wide range of functions to calculate the entropy of, and the cross-entropy between, univariate time series data. In contrast to other entropy-focused toolboxes, EntropyHub runs from the command line without the use of a GUI and provides many new benefits, including:</p>
    <list list-type="simple">
      <list-item>
        <p>■ Functions to perform refined, composite, refined-composite and hierarchical multiscale entropy analysis using more than twenty-five different entropy and cross-entropy estimators (approximate entropy, cross-sample entropy, etc).</p>
      </list-item>
      <list-item>
        <p>■ Functions to calculate bidimensional entropies from two-dimensional (image) data.</p>
      </list-item>
      <list-item>
        <p>■ An extensive range of function arguments to specify additional parameter values in the entropy calculation, including options for time-delayed state-space reconstruction and entropy value normalisation where possible.</p>
      </list-item>
      <list-item>
        <p>■ Availability in multiple programming languages–MATLAB, Python, Julia–to enable open-source access and provide cross-platform translation of methods through consistent function syntax. To the best of the Authors’ knowledge, this is the first entropy-specific toolkit for the Julia language.</p>
      </list-item>
      <list-item>
        <p>■ Compatible with both Windows, Mac and Linux operating systems.</p>
      </list-item>
      <list-item>
        <p>■ Comprehensive documentation describing installation, function syntax, examples of use, and references to source literature. Documentation is available online at <ext-link xlink:href="https://www.entropyhub.xyz/" ext-link-type="uri">www.EntropyHub.xyz</ext-link> (or at <ext-link xlink:href="https://mattwillflood.github.io/EntropyHub" ext-link-type="uri">MattWillFlood.github.io/EntropyHub</ext-link>), where it can also be downloaded as a booklet (<italic toggle="yes">EntropyHub Guide</italic>.<italic toggle="yes">pdf</italic>). Documentation specific to the MATLAB edition can also be found in the ‘supplemental software’ section of the MATLAB help browser after installation. Documentation specific to the Julia edition can also be found at <ext-link xlink:href="https://mattwillflood.github.io/EntropyHub.jl/stable" ext-link-type="uri">MattWillFlood.github.io/EntropyHub.jl/stable</ext-link>.</p>
      </list-item>
      <list-item>
        <p>■ Hosting on the native package repositories for MATLAB (<ext-link xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/94185-entropyhub" ext-link-type="uri">MathWorks File Exchange</ext-link>), Python (<ext-link xlink:href="https://pypi.org/project/EntropyHub/" ext-link-type="uri">PyPi</ext-link>) and Julia (<ext-link xlink:href="https://juliahub.com/ui/Packages/EntropyHub/npy5E/0.1.0" ext-link-type="uri">Julia General Registry</ext-link>), to facilitate straightforward downloading, installation and updating. The latest development releases can also be downloaded from the EntropyHub GitHub repository - <ext-link xlink:href="https://www.github.com/MattWillFlood/EntropyHub" ext-link-type="uri">www.github.com/MattWillFlood/EntropyHub</ext-link>.</p>
      </list-item>
    </list>
    <p>As new measures enter the ever-growing entropy universe, EntropyHub aims to incorporate these measures accordingly. EntropyHub is licensed under <ext-link xlink:href="https://www.apache.org/licenses/LICENSE-2.0.html" ext-link-type="uri">the Apache license (version 2.0)</ext-link> and is available for use by all on condition that the present paper by cited on any scientific outputs realised using the EntropyHub toolkit.</p>
    <p>The following sections of the paper outline the toolkit contents, steps for installing and accessing documentation.</p>
  </sec>
  <sec id="sec002">
    <title>Toolkit contents and functionality</title>
    <p>Functions in the EntropyHub toolkit fall into five categories. The first three categories—<italic toggle="yes">Base</italic>, <italic toggle="yes">Cross</italic> and <italic toggle="yes">Bidimensional—</italic>refer to standalone entropy estimators distinguished according to the type of input data they analyse.</p>
    <list list-type="simple">
      <list-item>
        <p>■ <italic toggle="yes">Base</italic> functions return the entropy of a single univariate time series, e.g. sample entropy (SampEn), bubble entropy (BubbEn), phase entropy (PhasEn), etc.</p>
      </list-item>
      <list-item>
        <p>■ <italic toggle="yes">Cross</italic> functions return the cross-entropy <italic toggle="yes">between</italic> two univariate time series, e.g. cross-fuzzy entropy (XFuzzEn), cross-permutation entropy (XPermEn), etc.</p>
      </list-item>
      <list-item>
        <p>■ <italic toggle="yes">Bidimensional</italic> functions return the entropy from a univariate, two-dimensional data matrix, e.g. bidimensional distribution entropy (DistEn2D), etc.</p>
      </list-item>
    </list>
    <p>The remaining two categories–<italic toggle="yes">Multiscale</italic> and <italic toggle="yes">Multiscale Cross–</italic>relate to multiscale entropy methods using the entropy estimators from the <italic toggle="yes">Base</italic> and <italic toggle="yes">Cross</italic> categories, respectively.</p>
    <list list-type="simple">
      <list-item>
        <p>■ <italic toggle="yes">Multiscale</italic> functions return the multiscale entropy of a single univariate time series, calculated using any of the <italic toggle="yes">Base</italic> entropy estimators,</p>
        <p>■ e.g. multiscale entropy (MSEn), composite multiscale entropy (cMSEn), etc.</p>
      </list-item>
      <list-item>
        <p>■ <italic toggle="yes">Multiscale Cross</italic> functions return the multiscale cross-entropy <italic toggle="yes">between</italic> two univariate time series calculated using any of the <italic toggle="yes">Cross</italic> entropy estimators,</p>
        <p>■ e.g. cross-multiscale entropy (XMSEn), refined multiscale cross-entropy (rXMSEn), etc.</p>
      </list-item>
    </list>
    <p>A list of all functions available in version 0.1 of the EntropyHub toolkit is provided in <xref rid="pone.0259448.t002" ref-type="table">Table 2</xref>. As more entropy methods are identified, these will be added to newer versions of the toolkit.</p>
    <table-wrap position="float" id="pone.0259448.t002">
      <object-id pub-id-type="doi">10.1371/journal.pone.0259448.t002</object-id>
      <label>Table 2</label>
      <caption>
        <title>List of base, cross, bidimensional, multiscale and multiscale cross-entropy functions available in version 0.1 of the EntropyHub toolkit.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="pone.0259448.t002" id="pone.0259448.t002g" position="float"/>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1"/>
              <th align="left" rowspan="1" colspan="1">Entropy Method</th>
              <th align="left" rowspan="1" colspan="1">Function Name</th>
              <th align="left" rowspan="1" colspan="1">References</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="center" rowspan="18" colspan="1">
                <bold>Base Entropy Functions</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">Approximate Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>ApEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref033" ref-type="bibr">33</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Attention Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>AttnEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref046" ref-type="bibr">46</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Bubble Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>BubbEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref047" ref-type="bibr">47</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">(corrected) Conditional Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>CondEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref048" ref-type="bibr">48</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Cosine Similarity Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>CoSiEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref049" ref-type="bibr">49</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Dispersion Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>DispEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref008" ref-type="bibr">8</xref>, <xref rid="pone.0259448.ref050" ref-type="bibr">50</xref>–<xref rid="pone.0259448.ref052" ref-type="bibr">52</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Distribution Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>DistEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref006" ref-type="bibr">6</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Entropy of Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>EnofEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref053" ref-type="bibr">53</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Fuzzy Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>FuzzEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref013" ref-type="bibr">13</xref>, <xref rid="pone.0259448.ref034" ref-type="bibr">34</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Gridded Distribution Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>GridEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref054" ref-type="bibr">54</xref>–<xref rid="pone.0259448.ref058" ref-type="bibr">58</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Increment Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>IncrEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref059" ref-type="bibr">59</xref>–<xref rid="pone.0259448.ref061" ref-type="bibr">61</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Kolmogorov Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>K2En</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref062" ref-type="bibr">62</xref>–<xref rid="pone.0259448.ref064" ref-type="bibr">64</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Permutation Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>PermEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0259448.ref012" ref-type="bibr">12</xref>, <xref rid="pone.0259448.ref065" ref-type="bibr">65</xref>–<xref rid="pone.0259448.ref071" ref-type="bibr">71</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Phase Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>PhasEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref072" ref-type="bibr">72</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Sample Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>SampEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref032" ref-type="bibr">32</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Slope Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>SlopEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref073" ref-type="bibr">73</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Spectral Entropy <xref rid="t002fn003" ref-type="table-fn"><sup>†</sup></xref></td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>SpecEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref074" ref-type="bibr">74</xref>, <xref rid="pone.0259448.ref075" ref-type="bibr">75</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Symbolic Dynamic Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>SyDyEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref076" ref-type="bibr">76</xref>–<xref rid="pone.0259448.ref078" ref-type="bibr">78</xref>]</td>
            </tr>
            <tr>
              <td align="center" rowspan="8" colspan="1">
                <bold>Cross-Entropy Functions</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">Cross-Approximate Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>XApEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref033" ref-type="bibr">33</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">(corrected) Cross-Conditional Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>XCondEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref048" ref-type="bibr">48</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Cross-Distribution Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>XDistEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0259448.ref079" ref-type="bibr">79</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Cross-Fuzzy Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>XFuzzEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref080" ref-type="bibr">80</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Cross-Kolmogorov Entropy <xref rid="t002fn004" ref-type="table-fn"><sup>§</sup></xref></td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>XK2En</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Cross-Permutation Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>XPermEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref081" ref-type="bibr">81</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Cross-Sample Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>XSampEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref032" ref-type="bibr">32</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Cross-Spectral Entropy <sup>§</sup></td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>XSpecEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="center" rowspan="4" colspan="1">
                <bold>Bidimensional Entropy Functions</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">Bidimensional Distribution Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>DistEn2D</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref082" ref-type="bibr">82</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Bidimensional Dispersion Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>DispEn2D</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref083" ref-type="bibr">83</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Bidimensional Fuzzy Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>FuzzEn2D</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref084" ref-type="bibr">84</xref>, <xref rid="pone.0259448.ref085" ref-type="bibr">85</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Bidimensional Sample Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>SampEn2D</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref086" ref-type="bibr">86</xref>]</td>
            </tr>
            <tr>
              <td align="center" rowspan="5" colspan="1">
                <bold>Multiscale Entropy Functions</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">Multiscale Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>MSEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0259448.ref022" ref-type="bibr">22</xref>, <xref rid="pone.0259448.ref087" ref-type="bibr">87</xref>–<xref rid="pone.0259448.ref094" ref-type="bibr">94</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Composite Multiscale Entropy</td>
              <td align="left" rowspan="2" colspan="1">
                <monospace>cMSEn</monospace>
              </td>
              <td align="left" rowspan="2" colspan="1">[<xref rid="pone.0259448.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0259448.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0259448.ref036" ref-type="bibr">36</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">(+ Refined-Composite Multiscale Entropy)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Refined Multiscale Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>rMSEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0259448.ref095" ref-type="bibr">95</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Hierarchical Multiscale Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>hMSEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref096" ref-type="bibr">96</xref>]</td>
            </tr>
            <tr>
              <td align="center" rowspan="5" colspan="1">
                <bold>Multiscale Cross-Entropy Functions</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">Multiscale Cross-Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>XMSEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0259448.ref040" ref-type="bibr">40</xref>, <xref rid="pone.0259448.ref097" ref-type="bibr">97</xref>–<xref rid="pone.0259448.ref100" ref-type="bibr">100</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Composite Multiscale Cross-Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>cXMSEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref101" ref-type="bibr">101</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">(+ Refined-Composite Multiscale Cross-Entropy)</td>
              <td align="left" rowspan="1" colspan="1"/>
              <td align="left" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Refined Multiscale Cross-Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>rXMSEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0259448.ref101" ref-type="bibr">101</xref>]</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Hierarchical Multiscale Cross-Entropy</td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>hXMSEn</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1">[<xref rid="pone.0259448.ref096" ref-type="bibr">96</xref>]</td>
            </tr>
            <tr>
              <td align="center" rowspan="2" colspan="1">
                <bold>Other</bold>
              </td>
              <td align="left" rowspan="1" colspan="1">Multiscale Entropy Object <xref rid="t002fn001" ref-type="table-fn">*</xref></td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>MSobject</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Example Data Importer <xref rid="t002fn002" ref-type="table-fn">**</xref></td>
              <td align="left" rowspan="1" colspan="1">
                <monospace>ExampleData</monospace>
              </td>
              <td align="left" rowspan="1" colspan="1"/>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="t002fn001">
          <p>* The multiscale entropy object returned by <monospace>MSobject</monospace> function is a required argument for <italic toggle="yes">Multiscale</italic> and <italic toggle="yes">Multiscale Cross</italic> function<italic toggle="yes">s</italic>.</p>
        </fn>
        <fn id="t002fn002">
          <p>** Sample time series and image data can be imported using the <monospace>ExampleData</monospace> function. Use of this function requires an internet connection. The imported data are the same as those used in the examples provided in the EntropyHub documentation.</p>
        </fn>
        <fn id="t002fn003">
          <p><sup>†</sup> In contrast to other <italic toggle="yes">Base</italic> entropies, spectral entropy (SpecEn) is not derived from information theory or dynamical systems theory, and instead measures the entropy of the frequency spectrum.</p>
        </fn>
        <fn id="t002fn004">
          <p><sup>§</sup> Cross-Kolmogorov and cross-spectral entropies, while included in the toolkit, have yet to be verified in the scientific literature.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p>One of the main advantages of EntropyHub is the ability to specify numerous parameters used in the entropy calculation by entering optional keyword function arguments. The default value of each keyword argument is based on the value proposed in the original source literature for that method. However, blindly analysing time series data using these arguments is strongly discouraged. Drawing conclusions about data based on entropy values is only valid when the parameters used to calculate those values accurately capture the underlying dynamics of the data.</p>
    <p>With certain <italic toggle="yes">Base</italic> and <italic toggle="yes">Cross</italic> functions, it is possible to calculate entropy using variant methods of the main estimator. For example, with the function for permutation entropy (PermEn) one can calculate the edge [<xref rid="pone.0259448.ref065" ref-type="bibr">65</xref>], weighted [<xref rid="pone.0259448.ref070" ref-type="bibr">70</xref>], amplitude-aware [<xref rid="pone.0259448.ref011" ref-type="bibr">11</xref>], modified [<xref rid="pone.0259448.ref068" ref-type="bibr">68</xref>], fine-grained [<xref rid="pone.0259448.ref067" ref-type="bibr">67</xref>], and uniform-quantization [<xref rid="pone.0259448.ref071" ref-type="bibr">71</xref>] permutation entropy variants, in addition to the original method introduced by Bandt and Pompe [<xref rid="pone.0259448.ref066" ref-type="bibr">66</xref>]. It is important to note that while the primary variable returned by each function is the estimated entropy value, most functions provide secondary and tertiary variables that may be of additional interest to the user. Some examples include the dispersion entropy function (DispEn) [<xref rid="pone.0259448.ref008" ref-type="bibr">8</xref>] which also returns the reverse dispersion entropy [<xref rid="pone.0259448.ref050" ref-type="bibr">50</xref>], the spectral entropy function (SpecEn) [<xref rid="pone.0259448.ref074" ref-type="bibr">74</xref>] which also returns the band-spectral entropy [<xref rid="pone.0259448.ref102" ref-type="bibr">102</xref>], and the Kolmogorov entropy function (K2En) [<xref rid="pone.0259448.ref063" ref-type="bibr">63</xref>] which also returns the correlation sum estimate. Furthermore, every <italic toggle="yes">Multiscale</italic> and <italic toggle="yes">Multiscale Cross</italic> function has the option to plot the multiscale (cross) entropy curve (<xref rid="pone.0259448.g001" ref-type="fig">Fig 1</xref>), as well as some <italic toggle="yes">Base</italic> functions which allow one to plot spatial representations of the original time series (Figs <xref rid="pone.0259448.g002" ref-type="fig">2</xref> and <xref rid="pone.0259448.g003" ref-type="fig">3</xref>).</p>
    <fig position="float" id="pone.0259448.g001">
      <object-id pub-id-type="doi">10.1371/journal.pone.0259448.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>Representative plot of the multiscale entropy curve returned by any <italic toggle="yes">Multiscale</italic> or <italic toggle="yes">Multiscale Cross</italic> entropy function.</title>
        <p>The curve shown corresponds to multiscale bubble entropy of a Gaussian white noise signal (N = 5000, μ = 0, σ = 1), calculated over 5 coarse-grained time scales, with estimator parameters: embedding dimension (<italic toggle="yes">m</italic>) = 2, time delay (<italic toggle="yes">τ</italic>) = 1.</p>
      </caption>
      <graphic xlink:href="pone.0259448.g001" position="float"/>
    </fig>
    <fig position="float" id="pone.0259448.g002">
      <object-id pub-id-type="doi">10.1371/journal.pone.0259448.g002</object-id>
      <label>Fig 2</label>
      <caption>
        <title>Second-order difference plot returned by the phase entropy function (PhasEn).</title>
        <p>Representative second-order difference plot of the x-component of the Henon set of equations (α = 1.4, β = 0.3), calculated with a time-delay (<italic toggle="yes">τ</italic>) = 2 and partitions (<italic toggle="yes">K</italic>) = 9.</p>
      </caption>
      <graphic xlink:href="pone.0259448.g002" position="float"/>
    </fig>
    <fig position="float" id="pone.0259448.g003">
      <object-id pub-id-type="doi">10.1371/journal.pone.0259448.g003</object-id>
      <label>Fig 3</label>
      <caption>
        <title>Poincaré plot and bivariate histogram returned by the gridded distribution entropy function (GridEn).</title>
        <p>Representative Pioncaré plot and bivariate histogram of the x-component of the Lorenz system of equations (σ = 10, β = 8/3, ρ = 28), calculated with grid partitions (<italic toggle="yes">m</italic>) = 5 and a time-delay (<italic toggle="yes">τ</italic>) = 2.</p>
      </caption>
      <graphic xlink:href="pone.0259448.g003" position="float"/>
    </fig>
  </sec>
  <sec id="sec003">
    <title>Installation and dependencies</title>
    <p>Major version releases of the EntropyHub toolkit can be directly installed through the native package repository for the <ext-link xlink:href="https://www.mathworks.com/matlabcentral/fileexchange/94185-entropyhub" ext-link-type="uri">MATLAB</ext-link>, <ext-link xlink:href="https://pypi.org/project/EntropyHub/" ext-link-type="uri">Python</ext-link> and <ext-link xlink:href="https://juliahub.com/ui/Packages/EntropyHub/npy5E/0.1.0" ext-link-type="uri">Julia</ext-link> programming environments. Beta development versions can be downloaded and installed from the directories of each programming language hosted on the EntropyHub GitHub repository– <ext-link xlink:href="https://github.com/MattWillFlood/EntropyHub" ext-link-type="uri">github.com/MattWillFlood/EntropyHub</ext-link>. EntropyHub is compatible with Windows, Mac and Linux operating systems.</p>
    <sec id="sec004">
      <title>MATLAB</title>
      <p>There are two additional toolboxes from the MATLAB product family that are required to experience the full functionality of the EntropyHub toolkit—the <italic toggle="yes">Signal Processing Toolbox</italic> and the <italic toggle="yes">Statistics and Machine Learning Toolbox</italic>. However, most functions will work without these toolboxes. EntropyHub is intended for use with MATLAB versions ≥ 2016a. In some cases, the toolkit may work on versions 2015a &amp; 2015b, although it is not recommended to install on MATLAB versions older than 2016.</p>
      <p>There are two ways to install EntropyHub in MATLAB.</p>
      <p><bold>Option 1.</bold> Note: Option 1 requires the user to be logged in to their MathWorks account.</p>
      <list list-type="order">
        <list-item>
          <p>In the MATLAB application, open the Add-Ons browser under the ‘Home’ tab by clicking ‘Get Add-Ons’ (<xref rid="pone.0259448.s001" ref-type="supplementary-material">S1A Fig</xref>).</p>
        </list-item>
        <list-item>
          <p>In the search bar, search for “EntroypHub” (S1b Fig).</p>
        </list-item>
        <list-item>
          <p>Open the resulting link and click ‘<italic toggle="yes">add</italic>’ in the top-right corner (S1c Fig).</p>
        </list-item>
        <list-item>
          <p>Follow the instructions to install the toolbox (<xref rid="pone.0259448.s001" ref-type="supplementary-material">S1D Fig</xref>).</p>
        </list-item>
      </list>
      <p><bold>Option 2</bold>.</p>
      <list list-type="order">
        <list-item>
          <p>Go to the ‘EntropyHub–MatLab’ directory in the EntropyHub repository on GitHub (<xref rid="pone.0259448.s001" ref-type="supplementary-material">S1E Fig</xref>): <ext-link xlink:href="https://github.com/MattWillFlood/EntropyHub/tree/main/EntropyHub%20-%20MatLab" ext-link-type="uri">https://github.com/MattWillFlood/EntropyHub/tree/main/EntropyHub%20-%20MatLab</ext-link></p>
        </list-item>
        <list-item>
          <p>Download the MATLAB toolbox file (<italic toggle="yes">EntropyHub</italic>.<italic toggle="yes">mltbx</italic>) file (<xref rid="pone.0259448.s001" ref-type="supplementary-material">S1F Fig</xref>).</p>
        </list-item>
        <list-item>
          <p>Open the MATLAB application and change the current folder to the directory where the <italic toggle="yes">EntropyHub</italic>.<italic toggle="yes">mltbx</italic> file is saved (<xref rid="pone.0259448.s001" ref-type="supplementary-material">S1G Fig</xref>).</p>
        </list-item>
        <list-item>
          <p>Double-click the <italic toggle="yes">EntropyHub</italic>.<italic toggle="yes">mltbx</italic> file to open it and click install (<xref rid="pone.0259448.s001" ref-type="supplementary-material">S1H Fig</xref>).</p>
        </list-item>
      </list>
      <p>To check that EntropyHub has been correctly installed, enter “<monospace>EntropyHub</monospace>” at the command line and the EntropyHub logo should be displayed (<xref rid="pone.0259448.s001" ref-type="supplementary-material">S1I Fig</xref>).</p>
    </sec>
    <sec id="sec005">
      <title>Python</title>
      <p>There are several modules required to use EntropyHub in Python—<italic toggle="yes">NumPy</italic> [<xref rid="pone.0259448.ref103" ref-type="bibr">103</xref>], <italic toggle="yes">SciPy</italic> [<xref rid="pone.0259448.ref104" ref-type="bibr">104</xref>], <italic toggle="yes">Matplotlib</italic> [<xref rid="pone.0259448.ref105" ref-type="bibr">105</xref>], <italic toggle="yes">PyEMD</italic> [<xref rid="pone.0259448.ref106" ref-type="bibr">106</xref>], and <italic toggle="yes">Requests</italic>. These modules will be automatically installed alongside EntropyHub if not already installed. EntropyHub was designed using Python3 and thus is not intended for use with Python2 or Python versions &lt; 3.6. EntropyHub Python functions are primarily built on top of the <italic toggle="yes">NumPy</italic> module for mathematical computation [<xref rid="pone.0259448.ref103" ref-type="bibr">103</xref>], so vector or matrix variables are returned as <italic toggle="yes">NumPy</italic> array objects.</p>
      <p>There are 2 ways to install EntropyHub in Python. Option 1 is strongly recommended.</p>
      <p><bold>Option 1. Note: Option 1 requires the ‘pip’ Python package installer</bold>.</p>
      <list list-type="simple">
        <list-item>
          <p>■ Using <italic toggle="yes">pip</italic>, enter the following at the command line (<xref rid="pone.0259448.s002" ref-type="supplementary-material">S2A Fig</xref>):</p>
          <p>
            <bold>        pip install EntropyHub</bold>
          </p>
          <p>*Note: this command is case sensitive</p>
        </list-item>
      </list>
      <p><bold>Option 2</bold>.</p>
      <list list-type="order">
        <list-item>
          <p>Go to the ‘EntropyHub–Python’ directory in the EntropyHub repository on GitHub (<xref rid="pone.0259448.s002" ref-type="supplementary-material">S2B Fig</xref>): <ext-link xlink:href="https://github.com/MattWillFlood/EntropyHub/tree/main/EntropyHub%20-%20Python%20" ext-link-type="uri">https://github.com/MattWillFlood/EntropyHub/tree/main/EntropyHub%20-%20Python</ext-link></p>
        </list-item>
        <list-item>
          <p>Download the <italic toggle="yes">EntropyHub</italic>.<italic toggle="yes">x</italic>.<italic toggle="yes">x</italic>.<italic toggle="yes">x</italic>.<italic toggle="yes">tar</italic>.<italic toggle="yes">gz</italic> folder and unzip it (<xref rid="pone.0259448.s002" ref-type="supplementary-material">S2C and S2D Fig</xref>).</p>
        </list-item>
        <list-item>
          <p>Open a command prompt (<monospace><bold>cmd</bold></monospace> on Windows, <bold>terminal</bold> on Mac) or the Anaconda prompt if Anaconda is the user’s python package distribution (<xref rid="pone.0259448.s002" ref-type="supplementary-material">S2E Fig</xref>).</p>
        </list-item>
        <list-item>
          <p>In the command prompt/terminal, navigate to the directory where the <italic toggle="yes">EntropyHub</italic>.<italic toggle="yes">x</italic>.<italic toggle="yes">x</italic>.<italic toggle="yes">x</italic>.<italic toggle="yes">tar</italic>.<italic toggle="yes">gz</italic> folder was saved and extracted (<xref rid="pone.0259448.s002" ref-type="supplementary-material">S2F Fig</xref>).</p>
        </list-item>
        <list-item>
          <p>Enter the following in the command line (<xref rid="pone.0259448.s002" ref-type="supplementary-material">S2G Fig</xref>):</p>
          <p>
            <bold>        python setup.py install</bold>
          </p>
        </list-item>
        <list-item>
          <p>Ensure that an up-to-date version of the <monospace>setuptools</monospace> module is installed:</p>
          <p>
            <bold>        python -m pip install—upgrade setuptools</bold>
          </p>
        </list-item>
      </list>
      <p>To use EntropyHub, import the module with the following command (<xref rid="pone.0259448.s002" ref-type="supplementary-material">S2H Fig</xref>):</p>
      <p>
        <bold>        import EntropyHub as EH</bold>
      </p>
      <p>To check that EntropyHub has been correctly installed and loaded, enter (<xref rid="pone.0259448.s002" ref-type="supplementary-material">S2H Fig</xref>):</p>
      <p>
        <bold>        EH.greet()</bold>
      </p>
    </sec>
    <sec id="sec006">
      <title>Julia</title>
      <p>There are a number of modules required to use EntropyHub in Julia—<italic toggle="yes">DSP</italic>, <italic toggle="yes">FFTW</italic>, <italic toggle="yes">HTTP</italic>, <italic toggle="yes">DelimitedFiles</italic>, <italic toggle="yes">Random</italic>, <italic toggle="yes">Plots</italic>, <italic toggle="yes">StatsBase</italic>, <italic toggle="yes">StatsFuns</italic>, <italic toggle="yes">Statistics</italic>, <italic toggle="yes">GroupSlices</italic>, <italic toggle="yes">Combinatorics</italic>, <italic toggle="yes">Clustering</italic>, <italic toggle="yes">LinearAlgebra</italic>, and <italic toggle="yes">Dierckx</italic> [<xref rid="pone.0259448.ref045" ref-type="bibr">45</xref>]. These modules will be automatically installed alongside EntropyHub if not already installed. EntropyHub was designed using Julia 1.5 and is intended for use with Julia versions ≥ 1.2.</p>
      <p>To install EntropyHub in Julia,</p>
      <list list-type="order">
        <list-item>
          <p>In the Julia programming environment, open the package REPL by typing ‘<monospace>]</monospace>’ (<xref rid="pone.0259448.s003" ref-type="supplementary-material">S3A Fig</xref>).</p>
        </list-item>
        <list-item>
          <p>At the command line, enter (<xref rid="pone.0259448.s003" ref-type="supplementary-material">S3B Fig</xref>):</p>
          <p>
            <bold>        add EntropyHub</bold>
          </p>
          <p>*Note: this command is case sensitive.</p>
          <p>Alternatively, one can install EntropyHub from the EntropyHub.jl GitHub repository:</p>
          <p>
            <bold>        add</bold>
            <ext-link xlink:href="https://github.com/MattWillFlood/EntropyHub.jl" ext-link-type="uri">
              <bold>https://github.com/MattWillFlood/EntropyHub.jl</bold>
            </ext-link>
          </p>
        </list-item>
      </list>
      <p>To use EntropyHub, import the module with the following command (<xref rid="pone.0259448.s003" ref-type="supplementary-material">S3C Fig</xref>):</p>
      <p>
        <bold>        using EntropyHub</bold>
      </p>
      <p>To check that EntropyHub has been correctly installed and loaded, type (<xref rid="pone.0259448.s003" ref-type="supplementary-material">S3D Fig</xref>):</p>
      <p>
        <bold>        EntropyHub.greet()</bold>
      </p>
    </sec>
    <sec id="sec007">
      <title>Supporting documentation and help</title>
      <p>To help users to get the most out of EntropyHub, extensive documentation has been developed to cover all aspects of the toolkit, <ext-link xlink:href="https://www.entropyhub.xyz/#documentation-help" ext-link-type="uri">www.EntropyHub.xyz/#documentation-help</ext-link>. Included in the documentation are:</p>
      <list list-type="simple">
        <list-item>
          <p>■ Instructions for installation.</p>
        </list-item>
        <list-item>
          <p>■ Thorough descriptions of the application programming interface (API) syntax–function names, keyword arguments, output values, etc.</p>
        </list-item>
        <list-item>
          <p>■ References to the original source literature for each method.</p>
        </list-item>
        <list-item>
          <p>■ Licensing and terms of use.</p>
        </list-item>
        <list-item>
          <p>■ Examples of use.</p>
        </list-item>
      </list>
      <p>Supporting documentation is available in various formats from the following sources.</p>
    </sec>
    <sec id="sec008">
      <title>
        <ext-link xlink:href="http://www.entropyhub.xyz/" ext-link-type="uri">
          <bold>www.EntropyHub.xyz</bold>
        </ext-link>
      </title>
      <p>The EntropyHub website, <ext-link xlink:href="https://www.entropyhub.xyz/" ext-link-type="uri">www.EntropyHub.xyz</ext-link> (also available at <ext-link xlink:href="https://mattwillflood.github.io/EntropyHub" ext-link-type="uri">MattWillFlood.github.io/EntropyHub</ext-link>) is the primary source of information on the toolkit with dedicated sections to MATLAB, Python and Julia, as well as release updates and links to helpful internet resources.</p>
    </sec>
    <sec id="sec009">
      <title>EntropyHub guide</title>
      <p>The <italic toggle="yes">EntropyHub Guide</italic>.<italic toggle="yes">pdf</italic> is the toolkit user manual and can be downloaded from the <ext-link xlink:href="https://www.entropyhub.xyz/Home.html#documentation-help" ext-link-type="uri">documentation section of the EntropyHub website</ext-link> or from the <ext-link xlink:href="https://github.com/MattWillFlood/EntropyHub/blob/main/EntropyHub%20Guide.pdf" ext-link-type="uri">EntropyHub GitHub repository</ext-link>. In addition to the information given on the website, the <italic toggle="yes">EntropyHub Guide</italic>.<italic toggle="yes">pdf</italic> document provides some extra material, such as plots of fuzzy functions used for fuzzy entropy (FuzzEn) calculation, or plots of symbolic mapping procedures used in dispersion (DispEn) or symbolic-dynamic entropy (SyDyEn).</p>
    </sec>
    <sec id="sec010">
      <title>MATLAB help browser</title>
      <p>Custom built documentation for the MATLAB edition of the toolkit is accessible through the MATLAB help browser after installation. Every function has its own help page featuring several examples of use ranging from basic to advanced. To access this documentation, open the help browser in the MATLAB application and at the bottom of the contents menu on the main page, under ‘Supplemental Software’, click on the link ‘EntropyHub Toolbox’.</p>
    </sec>
    <sec id="sec011">
      <title>EntropyHub.jl</title>
      <p>Custom documentation for the Julia edition of the toolkit can also be found at <ext-link xlink:href="https://mattwillflood.github.io/EntropyHub.jl/stable" ext-link-type="uri">MattWillFlood.github.io/EntropyHub.jl</ext-link> (linked to the EntropyHub website). Following Julia package convention, the Julia edition is given the suffix ‘.jl’ and is hosted in a standalone GitHub repository linked to the main EntropyHub repository.</p>
    </sec>
    <sec id="sec012">
      <title>Seeking further help</title>
      <p>Within each programming environment, information about a specific function can be displayed in the command prompt by accessing the function docstrings. For example, to display information about the approximate entropy function (ApEn), type:</p>
      <list list-type="simple">
        <list-item>
          <p>MATLAB:    <monospace>help ApEn</monospace></p>
        </list-item>
        <list-item>
          <p>Python:        <monospace>help(EntropyHub.ApEn)</monospace>        (if imported as EntropyHub)</p>
        </list-item>
        <list-item>
          <p>Julia:        <monospace>julia&gt;? </monospace>                (to open help mode in the REPL)</p>
        </list-item>
        <list-item>
          <p>                        <monospace>help?&gt; ApEn</monospace></p>
        </list-item>
      </list>
      <sec id="sec013">
        <title>Contact</title>
        <p>For help with topics not addressed in the documentation, users can seek help by contacting the toolkit developers at <email>help@entropyhub.xyz</email>. Every effort will be made to promptly respond to all queries received.</p>
        <p>To ensure that EntropyHub works as intended, with accurate and robust algorithms at its core, users are encouraged to report any potential bugs or errors discovered. The recommended way to report issues is to place an issue post under the ‘Issues’ tab on the EntropyHub GitHub repository. Doing so allows other users to find answers to common issues and contribute their own solutions. Alternatively, one can notify the package developers of any issues via email to <email>fix@entropyhub.xyz</email>.</p>
        <p>Continuous integration of new and improved entropy methods into the toolkit is a core principle of the EntropyHub project. Thus, requests and suggestions for new features are welcomed, as are contributions and offers for collaboration. EntropyHub developers will work with collaborators to ensure that contributions are valid, translated into MATLAB, Python and Julia, and follow the formatting adopted throughout the toolkit. Please contact <email>info@entropyhub.xyz</email> regarding any proposals that wish to be made.</p>
      </sec>
    </sec>
    <sec id="sec014">
      <title>Validation</title>
      <p>Included in EntropyHub are a number of sample time series and image datasets which can be used to test the validity of the toolkit functions (<xref rid="pone.0259448.g004" ref-type="fig">Fig 4</xref>). Included in these datasets are random number sequences (gaussian, uniform, random integers), chaotic attractors (Lorenz, Hénon), and matrix representations of images (Mandelbrot fractal, random numbers, etc.). Importing these datasets into the programming environment is done using the <monospace>ExampleData</monospace> function (<xref rid="pone.0259448.t002" ref-type="table">Table 2</xref>), <underline>which requires an internet connection</underline>. Every example presented in the supporting documentation on the EntropyHub website, in the MATLAB help browser, or in the <italic toggle="yes">EntropyHub Guide</italic>.<italic toggle="yes">pdf</italic>, employs the same sample datasets provided by the <monospace>ExampleData</monospace> function. Therefore, users can replicate these examples verbatim to verify that the toolkit functions properly on their computer system. The following subsections demonstrate the implementation of several <italic toggle="yes">Base</italic>, <italic toggle="yes">Cross-</italic>, <italic toggle="yes">Bidimensional</italic>, <italic toggle="yes">Multiscale</italic> and <italic toggle="yes">Multiscale Cross-</italic>entropy methods as a proof-of-principle validation. <underline>Note: the examples in the following subsections use MATLAB syntax, but the implementation of these functions and the values they return are the same when using Python and Julia.</underline></p>
      <fig position="float" id="pone.0259448.g004">
        <object-id pub-id-type="doi">10.1371/journal.pone.0259448.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Sample datasets available with the EntropyHub toolkit through the ExampleData function.</title>
          <p>(a) A gaussian white noise time series, (b) the Lorenz system of equations, (c) a Mandelbrot fractal.</p>
        </caption>
        <graphic xlink:href="pone.0259448.g004" position="float"/>
      </fig>
    </sec>
    <sec id="sec015">
      <title><italic toggle="yes">Base</italic> entropy</title>
      <p>A sequence of normally distributed random numbers (<xref rid="pone.0259448.g004" ref-type="fig">Fig 4A</xref>; <italic toggle="yes">N</italic> = 5000, mean = 0, SD = 1) is imported and approximate entropy is estimated using the default parameters (embedding dimension = 2, time delay = 1, threshold = 0.2*<italic toggle="yes">SD[X]</italic>).</p>
      <p specific-use="line">&gt;&gt; X = ExampleData(‘gaussian’);</p>
      <p specific-use="line">&gt;&gt; ApEn(X)</p>
      <p specific-use="line">2.33505 2.29926 2.10113</p>
      <p>Random number sequences produce high entropy values as such sequences possess maximum uncertainty or unpredicatbility. The high approximate entropy values (&gt; 2) returned in this example, corresponding to estimates for embedding dimensions of 0, 1 and 2, are in the expected range for such time series.</p>
    </sec>
    <sec id="sec016">
      <title><italic toggle="yes">Cross-</italic>entropy</title>
      <p>The <italic toggle="yes">x</italic>, <italic toggle="yes">y</italic> and <italic toggle="yes">z</italic> components of the Lorenz system of equations (<xref rid="pone.0259448.g004" ref-type="fig">Fig 4B</xref>; <italic toggle="yes">N</italic> = 5917, σ = 10, β = 8/3, ρ = 28, <italic toggle="yes">x</italic><sub><italic toggle="yes">0</italic></sub> = 10, <italic toggle="yes">y</italic><sub>0</sub> = 20, <italic toggle="yes">z</italic><sub><italic toggle="yes">0</italic></sub> = 10) are imported and cross-permutation entropy is estimated using the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> components with the default parameters (embedding dimension = 3, time delay = 1).</p>
      <p specific-use="line">&gt;&gt; X = ExampleData(‘lorenz’);</p>
      <p specific-use="line">&gt;&gt; XPermEn(X(:,1:2))</p>
      <p specific-use="line">0.17771</p>
      <p>The Lorenz system is commonly employed in nonlinear dynamics as its attractor exhibits chaotic behaviour. Thus, the low cross-permutation entropy estimate returned here (0.1771) reflects the high degree of deterministic structure shared between the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> components of the Lorenz system.</p>
    </sec>
    <sec id="sec017">
      <title><italic toggle="yes">Bidimensional</italic> entropy</title>
      <p>A matrix of normally distributed (Gaussian) random numbers is imported (<xref rid="pone.0259448.g004" ref-type="fig">Fig 4C</xref>; <italic toggle="yes">N</italic> = 60x120, mean = 0, SD = 1) and bidimensional dispersion entropy is estimated with a template submatrix size of 5 and all other parameters set to default values (time delay = 1, number of symbols = 3, symbolic mapping transform = normal cumulative distribution function).</p>
      <p specific-use="line">&gt;&gt; X = ExampleData(‘gaussian_Mat’);</p>
      <p specific-use="line">&gt;&gt; DispEn2D(X, ‘m’, 5)</p>
      <p specific-use="line">8.77894</p>
      <p>The high value of the bidimensional dispersion entropy estimate corresponds to those previously reported for Gaussian white noise [<xref rid="pone.0259448.ref083" ref-type="bibr">83</xref>].</p>
    </sec>
    <sec id="sec018">
      <title><italic toggle="yes">Multiscale</italic> entropy</title>
      <p>A chirp signal (<italic toggle="yes">N</italic> = 5000, t<sub>0</sub> = 1, t<sub>0</sub> = 4000, normalised instantaneous frequency at t<sub>0</sub> = 0.01Hz, instantaneous frequency at t<sub>1</sub> = 0.025Hz) is imported and multiscale sample entropy is estimated over 5 coarse-grained temporal scale using the default parameters (embedding dimension = 2, time delay = 1, threshold = 0.2*<italic toggle="yes">SD[X]</italic>). Note: a multiscale entropy object (<italic toggle="yes">Mobj</italic>) must be used with multiscale entropy functions.</p>
      <p specific-use="line">&gt;&gt; X = ExampleData(‘chirp’);</p>
      <p specific-use="line">&gt;&gt; Mobj = MSobject(‘SampEn’);</p>
      <p specific-use="line">&gt;&gt; MSEn(X, Mobj, ’Scales’, 5)</p>
      <p specific-use="line">0.2738 0.3412 0.4257 0.5452 0.6759</p>
      <p>The chirp signal imported in this example represents a swept-frequency cosine with a linearly decreasing period length. The coarse-graining procedure of multiscale entropy [<xref rid="pone.0259448.ref015" ref-type="bibr">15</xref>] functions as a low-pass filter of the original time series, with a lower cut-off frequency at each increasing time scale. Therefore, the coarse-graining procedure increasingly diminishes the localised auto-correlation of the chirp signal at each temporal scale, increasing the entropy. This reflects the increasing sample entropy values from low (0.2738) to moderate (0.6759) returned by the <italic toggle="yes">MSEn</italic> function.</p>
    </sec>
    <sec id="sec019">
      <title><italic toggle="yes">Multiscale cross</italic>-entropy</title>
      <p>Two sequences of uniformly distributed random numbers (<italic toggle="yes">N</italic> = 4096, range = [0, <xref rid="pone.0259448.ref001" ref-type="bibr">1</xref>]) are imported and multiscale cross-distribution entropy is estimated over 7 coarse-grained temporal scales with the default parameters (embedding dimension = 2, time delay = 1, histogram binning method = ‘sturges’, normalisation with respect to number of histogram bins = true).</p>
      <p specific-use="line">&gt;&gt; X = ExampleData(‘uniform2’);</p>
      <p specific-use="line">&gt;&gt; Mobj = MSobject(‘XDistEn’);</p>
      <p specific-use="line">&gt;&gt; XMSEn(X, Mobj)</p>
      <p specific-use="line">0.95735 0.86769 0.83544 0.80433 0.82617 0.77619 0.78893</p>
      <p>As expected, the <italic toggle="yes">normalised</italic> multiscale cross-distribution entropy values remain relatively constant over multiple time scales as no information can be gained about one sequence from the other at any time scale.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec020">
    <title>Discussion</title>
    <p>The growing number of entropy methods reported in the scientific literature for time series and image analysis warrants new software tools that enable researchers to apply such methods [<xref rid="pone.0259448.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0259448.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0259448.ref038" ref-type="bibr">38</xref>]. Currently, there is a dearth of validated, open-source tools that implement a comprehensive array of entropy methods at the command-line with options to modify multiple parameter values. EntropyHub is the first toolkit to provide this functionality in a package that is available in three programming languages (MATLAB, Python, and Julia) with consistent syntax, and is supported by extensive documentation (<xref rid="pone.0259448.t003" ref-type="table">Table 3</xref>). To the best of the Authors knowledge, EntropyHub is also the first toolkit to provide multiple functions for bidimensional entropy [<xref rid="pone.0259448.ref082" ref-type="bibr">82</xref>–<xref rid="pone.0259448.ref086" ref-type="bibr">86</xref>], multiscale entropy [<xref rid="pone.0259448.ref014" ref-type="bibr">14</xref>, <xref rid="pone.0259448.ref015" ref-type="bibr">15</xref>, <xref rid="pone.0259448.ref035" ref-type="bibr">35</xref>, <xref rid="pone.0259448.ref090" ref-type="bibr">90</xref>, <xref rid="pone.0259448.ref096" ref-type="bibr">96</xref>] and multiscale cross-entropy analyses [<xref rid="pone.0259448.ref040" ref-type="bibr">40</xref>, <xref rid="pone.0259448.ref097" ref-type="bibr">97</xref>, <xref rid="pone.0259448.ref098" ref-type="bibr">98</xref>] all in one package. Specific programming language editions of the EntropyHub toolkit are hosted on the native package repositories for MATLAB, Python and Julia (<xref rid="pone.0259448.t003" ref-type="table">Table 3</xref>), facilitating straightforward installation and version updates. EntropyHub is compatible with both Windows, Mac and Linux operating systems, and is open for use under the Apache License (Version 2.0) on condition that the present manuscript be cited in any outputs achieved through the use of the toolkit.</p>
    <table-wrap position="float" id="pone.0259448.t003">
      <object-id pub-id-type="doi">10.1371/journal.pone.0259448.t003</object-id>
      <label>Table 3</label>
      <caption>
        <title>List of resources for the EntropyHub toolkit.</title>
      </caption>
      <alternatives>
        <graphic xlink:href="pone.0259448.t003" id="pone.0259448.t003g" position="float"/>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" valign="middle" span="1"/>
            <col align="left" valign="middle" span="1"/>
          </colgroup>
          <tbody>
            <tr>
              <td align="center" colspan="2" rowspan="1">
                <bold>Online Resources</bold>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="2" colspan="1">EntropyHub Website</td>
              <td align="left" rowspan="1" colspan="1">
                <ext-link xlink:href="http://www.EntropyHub.xyz" ext-link-type="uri">www.EntropyHub.xyz</ext-link>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">MattWillFlood.github.io/EntropyHub</td>
            </tr>
            <tr>
              <td align="left" rowspan="2" colspan="1">GitHub Repository</td>
              <td align="left" rowspan="1" colspan="1">
                <ext-link xlink:href="http://www.github.com/MattWillFlood/EntropyHub" ext-link-type="uri">www.github.com/MattWillFlood/EntropyHub</ext-link>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1"><ext-link xlink:href="http://www.github.com/MattWillFlood/EntropyHub.jl" ext-link-type="uri">www.github.com/MattWillFlood/EntropyHub.jl</ext-link> (<italic toggle="yes">Julia only repository</italic>)</td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">MATLAB Package</td>
              <td align="left" rowspan="1" colspan="1">
                <ext-link xlink:href="http://www.mathworks.com/matlabcentral/fileexchange/94185-entropyhub" ext-link-type="uri">www.mathworks.com/matlabcentral/fileexchange/94185-entropyhub</ext-link>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Python Package</td>
              <td align="left" rowspan="1" colspan="1">
                <ext-link xlink:href="http://pypi.org/project/EntropyHub/" ext-link-type="uri">pypi.org/project/EntropyHub/</ext-link>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Julia Package</td>
              <td align="left" rowspan="1" colspan="1">
                <ext-link xlink:href="http://juliahub.com/ui/Packages/EntropyHub/npy5E/0.1.0" ext-link-type="uri">juliahub.com/ui/Packages/EntropyHub/npy5E/0.1.0</ext-link>
              </td>
            </tr>
            <tr>
              <td align="center" colspan="2" rowspan="1">
                <bold>Contact Details</bold>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">General Inquiries</td>
              <td align="left" rowspan="1" colspan="1">
                <email>info@entropyhub.xyz</email>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Help and Support</td>
              <td align="left" rowspan="1" colspan="1">
                <email>help@entropyhub.xyz</email>
              </td>
            </tr>
            <tr>
              <td align="left" rowspan="1" colspan="1">Reporting Bugs</td>
              <td align="left" rowspan="1" colspan="1">
                <email>fix@entropyhub.xyz</email>
              </td>
            </tr>
          </tbody>
        </table>
      </alternatives>
      <table-wrap-foot>
        <fn id="t003fn001">
          <p>All information about the toolkit, including installations instructions, documentation, and release updates can be found on the main EntropyHub website. Users can get in touch directly with the package developers by contacting the email addresses provided.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p>The application of entropy in the study of time series data is becoming more common in all manner of research fields such as engineering [<xref rid="pone.0259448.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0259448.ref018" ref-type="bibr">18</xref>], medicine [<xref rid="pone.0259448.ref019" ref-type="bibr">19</xref>–<xref rid="pone.0259448.ref023" ref-type="bibr">23</xref>] and finance [<xref rid="pone.0259448.ref024" ref-type="bibr">24</xref>–<xref rid="pone.0259448.ref027" ref-type="bibr">27</xref>]. The broad range of entropy functions provided by EntropyHub in multiple programming languages can serve to support researchers in these fields by characterising the uncertainty and complexity of time series data with various stochastic, time-frequency and chaotic properties. Additionally, this is the first toolkit to provide several functions for performing bidimensional (2D) entropy analysis, which can enable users to estimate the entropy of images and matrix data.</p>
    <p>The goal of EntropyHub is to continually integrate newly developed entropy methods and serve as a cohesive computing resource for all entropy-based analysis, independent of the application or research field. To achieve this goal, suggestions for new features and contributions from other researchers are welcomed.</p>
  </sec>
  <sec id="sec021" sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pone.0259448.s001" position="float" content-type="local-data">
      <label>S1 Fig</label>
      <caption>
        <title>Instructions for installing EntropyHub in MATLAB.</title>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pone.0259448.s001.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0259448.s002" position="float" content-type="local-data">
      <label>S2 Fig</label>
      <caption>
        <title>Instructions for installing EntropyHub in Python.</title>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pone.0259448.s002.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="pone.0259448.s003" position="float" content-type="local-data">
      <label>S3 Fig</label>
      <caption>
        <title>Instructions for installing EntropyHub in Julia.</title>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pone.0259448.s003.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>The Authors wish to thank Dr Lara McManus and Ben O’Callaghan for generously donating their time to test the toolkit and provide constructive feedback which substantially improved the end result. The Authors would also like to acknowledge the work of the scientific community in deriving the entropy methods that motivated the development of EntropyHub.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="pone.0259448.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Shannon</surname><given-names>CE</given-names></name>. <article-title>A Mathematical Theory of Communication.</article-title><source>Bell Syst Tech J</source>. <year>1948</year>;<volume>27</volume>: <fpage>379</fpage>–<lpage>423</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1002/j.1538-7305.1948.tb01338.x</pub-id><?supplied-pmid 30854411?><pub-id pub-id-type="pmid">30854411</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>W</given-names></name>, <name><surname>Zhao</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>Q</given-names></name>, <name><surname>Zhou</surname><given-names>J</given-names></name>. <article-title>Twenty Years of Entropy Research: A Bibliometric Overview</article-title>. <source>Entropy</source>. <year>2019</year>;<volume>21</volume>: <fpage>694</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e21070694</pub-id><?supplied-pmid 33267408?><pub-id pub-id-type="pmid">33267408</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Ribeiro</surname><given-names>M</given-names></name>, <name><surname>Henriques</surname><given-names>T</given-names></name>, <name><surname>Castro</surname><given-names>L</given-names></name>, <name><surname>Souto</surname><given-names>A</given-names></name>, <name><surname>Antunes</surname><given-names>L</given-names></name>, <name><surname>Costa-Santos</surname><given-names>C</given-names></name>, <etal>et al</etal>. <article-title>The Entropy Universe</article-title>. <source>Entropy</source>. <year>2021</year>;<volume>23</volume>: <fpage>222</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e23020222</pub-id><?supplied-pmid 33670121?><pub-id pub-id-type="pmid">33670121</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Yentes</surname><given-names>JM</given-names></name>, <name><surname>Hunt</surname><given-names>N</given-names></name>, <name><surname>Schmid</surname><given-names>KK</given-names></name>, <name><surname>Kaipust</surname><given-names>JP</given-names></name>, <name><surname>McGrath</surname><given-names>D</given-names></name>, <name><surname>Stergiou</surname><given-names>N</given-names></name>. <article-title>The appropriate use of approximate entropy and sample entropy with short data sets</article-title>. <source>Ann Biomed Eng</source>. <year>2013</year>;<volume>41</volume>: <fpage>349</fpage>–<lpage>365</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10439-012-0668-3</pub-id><?supplied-pmid 23064819?><pub-id pub-id-type="pmid">23064819</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Humeau-Heurtier</surname><given-names>A</given-names></name>, <name><surname>Wu</surname><given-names>CW</given-names></name>, <name><surname>Wu</surname><given-names>S De</given-names></name>. <article-title>Refined Composite Multiscale Permutation Entropy to Overcome Multiscale Permutation Entropy Length Dependence</article-title>. <source>IEEE Signal Process Lett</source>. <year>2015</year>;<volume>22</volume>: <fpage>2364</fpage>–<lpage>2367</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/LSP.2015.2482603</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>P</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>, <name><surname>Li</surname><given-names>K</given-names></name>, <name><surname>Zheng</surname><given-names>D</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>, <name><surname>Hou</surname><given-names>Y</given-names></name>. <article-title>Assessing the complexity of short-term heartbeat interval series by distribution entropy</article-title>. <source>Med Biol Eng Comput</source>. <year>2015</year>;<volume>53</volume>: <fpage>77</fpage>–<lpage>87</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11517-014-1216-0</pub-id><?supplied-pmid 25351477?><pub-id pub-id-type="pmid">25351477</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Cuesta-Frau</surname><given-names>D</given-names></name>, <name><surname>Murillo-Escobar</surname><given-names>JP</given-names></name>, <name><surname>Orrego</surname><given-names>DA</given-names></name>, <name><surname>Delgado-Trejos</surname><given-names>E</given-names></name>. <article-title>Embedded dimension and time series length. Practical influence on permutation entropy and its applications</article-title>. <source>Entropy</source>. <year>2019</year>;<volume>21</volume>: <fpage>385</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e21040385</pub-id><?supplied-pmid 33267099?><pub-id pub-id-type="pmid">33267099</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Rostaghi</surname><given-names>M</given-names></name>, <name><surname>Azami</surname><given-names>H</given-names></name>. <article-title>Dispersion Entropy: A Measure for Time-Series Analysis</article-title>. <source>IEEE Signal Process Lett</source>. <year>2016</year>;<volume>23</volume>: <fpage>610</fpage>–<lpage>614</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/LSP.2016.2542881</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Xiong</surname><given-names>W</given-names></name>, <name><surname>Faes</surname><given-names>L</given-names></name>, <name><surname>Ivanov</surname><given-names>PC</given-names></name>, <name><surname>Ch Ivanov</surname><given-names>P</given-names></name>. <article-title>Entropy measures, entropy estimators, and their performance in quantifying complex dynamics: Effects of artifacts, nonstationarity, and long-range correlations</article-title>. <source>Phys Rev E</source>. <year>2017</year>;<volume>95</volume>: <fpage>62114</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevE.95.062114</pub-id><?supplied-pmid 28709192?><pub-id pub-id-type="pmid">28709192</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Ramdani</surname><given-names>S</given-names></name>, <name><surname>Bouchara</surname><given-names>F</given-names></name>, <name><surname>Lagarde</surname><given-names>J</given-names></name>. <article-title>Influence of noise on the sample entropy algorithm</article-title>. <source>Chaos</source>. <year>2009</year>;<volume>19</volume>: <fpage>13123</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1063/1.3081406</pub-id><?supplied-pmid 19334987?><pub-id pub-id-type="pmid">19334987</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Azami</surname><given-names>H</given-names></name>, <name><surname>Escudero</surname><given-names>J</given-names></name>. <article-title>Amplitude-aware permutation entropy: Illustration in spike detection and signal segmentation</article-title>. <source>Comput Methods Programs Biomed</source>. <year>2016</year>;<volume>128</volume>: <fpage>40</fpage>–<lpage>51</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cmpb.2016.02.008</pub-id><?supplied-pmid 27040830?><pub-id pub-id-type="pmid">27040830</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Cuesta-Frau</surname><given-names>D.</given-names></name><article-title>Permutation entropy: Influence of amplitude information on time series classification performance</article-title>. <source>Math Biosci Eng</source>. <year>2019</year>;<volume>16</volume>: <fpage>6842</fpage>–<lpage>6857</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3934/mbe.2019342</pub-id><?supplied-pmid 31698591?><pub-id pub-id-type="pmid">31698591</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>W</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Xie</surname><given-names>H</given-names></name>, <name><surname>Yu</surname><given-names>W</given-names></name>. <article-title>Characterization of surface EMG signal based on fuzzy entropy</article-title>. <source>IEEE Trans Neural Syst Rehabil Eng</source>. <year>2007</year>;<volume>15</volume>: <fpage>266</fpage>–<lpage>272</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TNSRE.2007.897025</pub-id><?supplied-pmid 17601197?><pub-id pub-id-type="pmid">17601197</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Valencia</surname><given-names>JF</given-names></name>, <name><surname>Porta</surname><given-names>A</given-names></name>, <name><surname>Vallverdú</surname><given-names>M</given-names></name>, <name><surname>Clarià</surname><given-names>F</given-names></name>, <name><surname>Baranowski</surname><given-names>R</given-names></name>, <name><surname>Orłowska-Baranowska</surname><given-names>E</given-names></name>, <etal>et al</etal>. <article-title>Refined multiscale entropy: Application to 24-h holter recordings of heart period variability in healthy and aortic stenosis subjects</article-title>. <source>IEEE Trans Biomed Eng</source>. <year>2009</year>;<volume>56</volume>: <fpage>2202</fpage>–<lpage>2213</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2009.2021986</pub-id><?supplied-pmid 19457745?><pub-id pub-id-type="pmid">19457745</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Costa</surname><given-names>M</given-names></name>, <name><surname>Goldberger</surname><given-names>AL</given-names></name>, <name><surname>Peng</surname><given-names>CK</given-names></name>. <article-title>Multiscale Entropy Analysis of Complex Physiologic Time Series</article-title>. <source>Phys Rev Lett</source>. <year>2002</year>;<volume>89</volume>: <fpage>068102</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevLett.89.068102</pub-id><?supplied-pmid 12190613?><pub-id pub-id-type="pmid">12190613</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Hsu</surname><given-names>CF</given-names></name>, <name><surname>Lin</surname><given-names>P-YY</given-names></name>, <name><surname>Chao</surname><given-names>H-HH</given-names></name>, <name><surname>Hsu</surname><given-names>L</given-names></name>, <name><surname>Chi</surname><given-names>S</given-names></name>. <article-title>Average Entropy: Measurement of disorder for cardiac RR interval signals</article-title>. <source>Phys A Stat Mech its Appl</source>. <year>2019</year>;<volume>529</volume>: <fpage>121533</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.physa.2019.121533</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Liu</surname><given-names>Z</given-names></name>, <name><surname>Liang</surname><given-names>X</given-names></name>, <name><surname>Si</surname><given-names>S</given-names></name>. <article-title>The entropy algorithm and its variants in the fault diagnosis of rotating machinery: A review</article-title>. <source>IEEE Access</source>. <year>2018</year>;<volume>6</volume>: <fpage>66723</fpage>–<lpage>66741</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2873782</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Huo</surname><given-names>Z</given-names></name>, <name><surname>Martinez-Garcia</surname><given-names>M</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Yan</surname><given-names>R</given-names></name>, <name><surname>Shu</surname><given-names>L</given-names></name>. <article-title>Entropy Measures in Machine Fault Diagnosis: Insights and Applications</article-title>. <source>IEEE Trans Instrum Meas</source>. <year>2020</year>;<volume>69</volume>: <fpage>2607</fpage>–<lpage>2620</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TIM.2020.2981220</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Kannathal</surname><given-names>N</given-names></name>, <name><surname>Choo</surname><given-names>ML</given-names></name>, <name><surname>Acharya</surname><given-names>UR</given-names></name>, <name><surname>Sadasivan</surname><given-names>PK</given-names></name>. <article-title>Entropies for detection of epilepsy in EEG</article-title>. <source>Comput Methods Programs Biomed</source>. <year>2005</year>;<volume>80</volume>: <fpage>187</fpage>–<lpage>194</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cmpb.2005.06.012</pub-id><?supplied-pmid 16219385?><pub-id pub-id-type="pmid">16219385</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Flood</surname><given-names>MW</given-names></name>, <name><surname>Jensen</surname><given-names>BR</given-names></name>, <name><surname>Malling</surname><given-names>AS</given-names></name>, <name><surname>Lowery</surname><given-names>MM</given-names></name>. <article-title>Increased EMG intermuscular coherence and reduced signal complexity in Parkinson’s disease</article-title>. <source>Clin Neurophysiol.</source><year>2019</year>;<volume>130</volume>: <fpage>259</fpage>–<lpage>269</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.clinph.2018.10.023</pub-id><?supplied-pmid 30583273?><pub-id pub-id-type="pmid">30583273</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Abásolo</surname><given-names>D</given-names></name>, <name><surname>Hornero</surname><given-names>R</given-names></name>, <name><surname>Espino</surname><given-names>P</given-names></name>, <name><surname>Poza</surname><given-names>J</given-names></name>, <name><surname>Sánchez</surname><given-names>CI</given-names></name>, <name><surname>De La Rosa</surname><given-names>R</given-names></name>. <article-title>Analysis of regularity in the EEG background activity of Alzheimer’s disease patients with Approximate Entropy.</article-title><source>Clin Neurophysiol.</source><year>2005</year>;<volume>116</volume>: <fpage>1826</fpage>–<lpage>1834</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.clinph.2005.04.001</pub-id><?supplied-pmid 15979403?><pub-id pub-id-type="pmid">15979403</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Thuraisingham</surname><given-names>RA</given-names></name>, <name><surname>Gottwald</surname><given-names>GA</given-names></name>. <article-title>On multiscale entropy analysis for physiological data</article-title>. <source>Phys A Stat Mech its Appl</source>. <year>2006</year>;<volume>366</volume>: <fpage>323</fpage>–<lpage>332</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.physa.2005.10.008</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref023">
      <label>23</label>
      <mixed-citation publication-type="journal"><name><surname>McManus</surname><given-names>L</given-names></name>, <name><surname>Flood</surname><given-names>MW</given-names></name>, <name><surname>Lowery</surname><given-names>MM</given-names></name>. <article-title>Beta-band motor unit coherence and nonlinear surface EMG features of the first dorsal interosseous muscle vary with force</article-title>. <source>J Neurophysiol</source>. <year>2019</year>;<volume>122</volume>: <fpage>1147</fpage>–<lpage>1162</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1152/jn.00228.2019</pub-id><?supplied-pmid 31365308?><pub-id pub-id-type="pmid">31365308</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>R</given-names></name>, <name><surname>Cai</surname><given-names>R</given-names></name>, <name><surname>Tong</surname><given-names>G</given-names></name>. <article-title>Applications of entropy in finance: A review</article-title>. <source>Entropy. MDPI AG</source>; <year>2013</year>. pp. <fpage>4909</fpage>–<lpage>4931</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e15114909</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Xu</surname><given-names>M</given-names></name>, <name><surname>Shang</surname><given-names>P</given-names></name>, <name><surname>Zhang</surname><given-names>S</given-names></name>. <article-title>Multiscale analysis of financial time series by Rényi distribution entropy</article-title>. <source>Phys A Stat Mech its Appl</source>. <year>2019</year>;<volume>536</volume>: <fpage>120916</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.physa.2019.04.152</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Yin</surname><given-names>Y</given-names></name>, <name><surname>Shang</surname><given-names>P</given-names></name>. <article-title>Modified cross sample entropy and surrogate data analysis method for financial time series</article-title>. <source>Phys A Stat Mech its Appl</source>. <year>2015</year>;<volume>433</volume>: <fpage>17</fpage>–<lpage>25</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.physa.2015.03.055</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Pincus</surname><given-names>S.</given-names></name><article-title>Approximate entropy as an irregularity measure for financial data</article-title>. <source>Econom Rev</source>. <year>2008</year>;<volume>27</volume>: <fpage>329</fpage>–<lpage>362</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1080/07474930801959750</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Joppa</surname><given-names>LN</given-names></name>, <name><surname>McInerny</surname><given-names>G</given-names></name>, <name><surname>Harper</surname><given-names>R</given-names></name>, <name><surname>Salido</surname><given-names>L</given-names></name>, <name><surname>Takeda</surname><given-names>K</given-names></name>, <name><surname>O’Hara</surname><given-names>K</given-names></name>, <etal>et al</etal>. <article-title>Troubling trends in scientific software use</article-title>. <source>Science. American Association for the Advancement of Science</source>; <year>2013</year>. pp. <fpage>814</fpage>–<lpage>815</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1126/science.1231535</pub-id><?supplied-pmid 23687031?><pub-id pub-id-type="pmid">23687031</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref029">
      <label>29</label>
      <mixed-citation publication-type="journal"><name><surname>Piccolo</surname><given-names>SR</given-names></name>, <name><surname>Frampton</surname><given-names>MB</given-names></name>. <article-title>Tools and techniques for computational reproducibility</article-title>. <source>GigaScience. BioMed Central Ltd.</source>; <year>2016</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s13742-016-0135-4</pub-id><?supplied-pmid 27401684?><pub-id pub-id-type="pmid">27401684</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Kostic</surname><given-names>MM</given-names></name>. <article-title>The Elusive Nature of Entropy and Its Physical Meaning.</article-title><source>Entropy</source>. <year>2014</year>;<volume>16</volume>: <fpage>953</fpage>–<lpage>967</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e16020953</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Popovic</surname><given-names>M.</given-names></name><article-title>Researchers in an Entropy Wonderland: A Review of the Entropy Concept</article-title>. <source>Therm Sci</source>. <year>2017</year>;<volume>22</volume>: <fpage>1163</fpage>–<lpage>1178</lpage>. Available: <ext-link xlink:href="http://arxiv.org/abs/1711.07326" ext-link-type="uri">http://arxiv.org/abs/1711.07326</ext-link></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Richman</surname><given-names>JS</given-names></name>, <name><surname>Moorman</surname><given-names>JR</given-names></name>. <article-title>Physiological time-series analysis using approximate entropy and sample entropy</article-title>. <source>Am J Physiol Circ Physiol</source>. <year>2000</year>;<volume>278</volume>: <fpage>H2039</fpage>–<lpage>H2049</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1152/ajpheart.2000.278.6.H2039</pub-id><?supplied-pmid 10843903?><pub-id pub-id-type="pmid">10843903</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref033">
      <label>33</label>
      <mixed-citation publication-type="journal"><name><surname>Pincus</surname><given-names>SM</given-names></name>. <article-title>Approximate entropy as a measure of system complexity</article-title>. <source>Proc Natl Acad Sci</source>. <year>1991</year>;<volume>88</volume>: <fpage>2297</fpage>–<lpage>2301</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.88.6.2297</pub-id><?supplied-pmid 11607165?><pub-id pub-id-type="pmid">11607165</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Xie</surname><given-names>HB</given-names></name>, <name><surname>He</surname><given-names>WX</given-names></name>, <name><surname>Liu</surname><given-names>H</given-names></name>. <article-title>Measuring time series regularity using nonlinear similarity-based sample entropy</article-title>. <source>Phys Lett Sect A Gen At Solid State Phys</source>. <year>2008</year>;<volume>372</volume>: <fpage>7140</fpage>–<lpage>7146</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.physleta.2008.10.049</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>S-D</given-names></name>, <name><surname>Wu</surname><given-names>C-W</given-names></name>, <name><surname>Lin</surname><given-names>S-G</given-names></name>, <name><surname>Wang</surname><given-names>C-C</given-names></name>, <name><surname>Lee</surname><given-names>K-Y</given-names></name>. <article-title>Time Series Analysis Using Composite Multiscale Entropy</article-title>. <source>Entropy</source>. <year>2013</year>;<volume>15</volume>: <fpage>1069</fpage>–<lpage>1084</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e15031069</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Wu S De</surname><given-names>Wu CW</given-names></name>, <name><surname>Lin SG</surname><given-names>Lee KY</given-names></name>, <name><surname>Peng</surname><given-names>CK</given-names></name><article-title>Analysis of complex time series using refined composite multiscale entropy</article-title>. <source>Phys Lett Sect A Gen At Solid State Phys</source>. <year>2014</year>;<volume>378</volume>: <fpage>1369</fpage>–<lpage>1374</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.physleta.2014.03.034</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>P.</given-names></name><article-title>EZ Entropy: A software application for the entropy analysis of physiological time-series</article-title>. <source>Biomed Eng Online</source>. <year>2019</year>;<volume>18</volume>: <fpage>30</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s12938-019-0650-5</pub-id><?supplied-pmid 30894180?><pub-id pub-id-type="pmid">30894180</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Mayor</surname><given-names>D</given-names></name>, <name><surname>Panday</surname><given-names>D</given-names></name>, <name><surname>Kandel</surname><given-names>HK</given-names></name>, <name><surname>Steffert</surname><given-names>T</given-names></name>, <name><surname>Banks</surname><given-names>D</given-names></name>. <article-title>Ceps: An open access matlab graphical user interface (gui) for the analysis of complexity and entropy in physiological signals</article-title>. <source>Entropy</source>. <year>2021</year>;<volume>23</volume>: <fpage>1</fpage>–<lpage>34</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e23030321</pub-id><?supplied-pmid 33800469?><pub-id pub-id-type="pmid">33800469</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref039">
      <label>39</label>
      <mixed-citation publication-type="journal"><name><surname>Eduardo Virgilio Silva</surname><given-names>L</given-names></name>, <name><surname>Fazan</surname><given-names>R</given-names><suffix>Jr</suffix></name>, <name><surname>Antonio Marin-Neto</surname><given-names>J</given-names></name>. <article-title>PyBioS: A freeware computer software for analysis of cardiovascular signals</article-title>. <source>Comput Methods Programs Biomed</source>. <year>2020</year>;<volume>197</volume>: <fpage>105718</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105718</pub-id><?supplied-pmid 32866762?><pub-id pub-id-type="pmid">32866762</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref040">
      <label>40</label>
      <mixed-citation publication-type="journal"><name><surname>Jamin</surname><given-names>A</given-names></name>, <name><surname>Humeau-Heurtier</surname><given-names>A</given-names></name>. <article-title>(Multiscale) Cross-Entropy Methods: A Review</article-title>. <source>Entropy</source>. <year>2019</year>;<volume>22</volume>: <fpage>45</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e22010045</pub-id><?supplied-pmid 33285820?><pub-id pub-id-type="pmid">33285820</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref041">
      <label>41</label>
      <mixed-citation publication-type="book"><name><surname>Humeau-Heurtier</surname><given-names>A.</given-names></name><part-title>Texture feature extraction methods: A survey</part-title>. <publisher-name>IEEE Access. Institute of Electrical and Electronics Engineers Inc.</publisher-name>; <year>2019</year>. pp. <fpage>8975</fpage>–<lpage>9000</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2890743</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref042">
      <label>42</label>
      <mixed-citation publication-type="journal"><name><surname>Kuntzelman</surname><given-names>K</given-names></name>, <name><surname>Jack Rhodes</surname><given-names>L</given-names></name>, <name><surname>Harrington</surname><given-names>LN</given-names></name>, <name><surname>Miskovic</surname><given-names>V</given-names></name>. <article-title>A practical comparison of algorithms for the measurement of multiscale entropy in neural time series data</article-title>. <source>Brain Cogn</source>. <year>2018</year>;<volume>123</volume>: <fpage>126</fpage>–<lpage>135</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bandc.2018.03.010</pub-id><?supplied-pmid 29562207?><pub-id pub-id-type="pmid">29562207</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Moody</surname><given-names>GB</given-names></name>, <name><surname>Mark</surname><given-names>RG</given-names></name>, <name><surname>Goldberger</surname><given-names>AL</given-names></name>. <article-title>Physionet: A web-based resource for the study of physiologic signals</article-title>. <source>IEEE Eng Med Biol Mag</source>. <year>2001</year>;<volume>20</volume>: <fpage>70</fpage>–<lpage>75</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/51.932728</pub-id><?supplied-pmid 11446213?><pub-id pub-id-type="pmid">11446213</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref044">
      <label>44</label>
      <mixed-citation publication-type="journal"><name><surname>Van Rossum</surname><given-names>G</given-names></name>, <name><surname>Drake</surname><given-names>FL</given-names></name>. <source>The python reference manual.</source> 20AD. Available: <ext-link xlink:href="http://www.python.org" ext-link-type="uri">www.python.org</ext-link></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref045">
      <label>45</label>
      <mixed-citation publication-type="journal"><name><surname>Bezanson</surname><given-names>J</given-names></name>, <name><surname>Edelman</surname><given-names>A</given-names></name>, <name><surname>Karpinski</surname><given-names>S</given-names></name>, <name><surname>Shah</surname><given-names>VB</given-names></name>. <article-title>Julia: A fresh approach to numerical computing</article-title>. <source>SIAM Rev</source>. <year>2017</year>;<volume>59</volume>: <fpage>65</fpage>–<lpage>98</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1137/141000671</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref046">
      <label>46</label>
      <mixed-citation publication-type="journal"><name><surname>Yang</surname><given-names>J</given-names></name>, <name><surname>Choudhary</surname><given-names>GI</given-names></name>, <name><surname>Rahardja</surname><given-names>S</given-names></name>, <name><surname>Franti</surname><given-names>P</given-names></name>. <article-title>Classification of Interbeat Interval Time-series Using Attention Entropy</article-title>. <source>IEEE Trans Affect Comput</source>. <year>2020</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TAFFC.2017.2784832</pub-id><?supplied-pmid 32489521?><pub-id pub-id-type="pmid">32489521</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref047">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Manis</surname><given-names>G</given-names></name>, <name><surname>Aktaruzzaman</surname><given-names>M</given-names></name>, <name><surname>Sassi</surname><given-names>R</given-names></name>. <article-title>Bubble entropy: An entropy almost free of parameters</article-title>. <source>IEEE Trans Biomed Eng</source>. <year>2017</year>;<volume>64</volume>: <fpage>2711</fpage>–<lpage>2718</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2017.2664105</pub-id><?supplied-pmid 28182552?><pub-id pub-id-type="pmid">28182552</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>Porta</surname><given-names>A</given-names></name>, <name><surname>Baselli</surname><given-names>G</given-names></name>, <name><surname>Lombardi</surname><given-names>F</given-names></name>, <name><surname>Montano</surname><given-names>N</given-names></name>, <name><surname>Malliani</surname><given-names>A</given-names></name>, <name><surname>Cerutti</surname><given-names>S</given-names></name>. <article-title>Conditional entropy approach for the evaluation of the coupling strength</article-title>. <source>Biol Cybern</source>. <year>1999</year>;<volume>81</volume>: <fpage>119</fpage>–<lpage>129</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s004220050549</pub-id><?supplied-pmid 10481240?><pub-id pub-id-type="pmid">10481240</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Chanwimalueang</surname><given-names>T</given-names></name>, <name><surname>Mandic</surname><given-names>DP</given-names></name>. <article-title>Cosine Similarity Entropy: Self-Correlation-Based Complexity Analysis of Dynamical Systems</article-title>. <source>Entropy</source>. <year>2017</year>;<volume>19</volume>: <fpage>652</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e19120652</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref050">
      <label>50</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Gao</given-names></name>, <name><surname>Wang</surname></name>. <article-title>Reverse Dispersion Entropy: A New Complexity Measure for Sensor Signal</article-title>. <source>Sensors.</source><year>2019</year>;<volume>19</volume>: <fpage>5203</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/s19235203</pub-id><?supplied-pmid 31783659?><pub-id pub-id-type="pmid">31783659</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref051">
      <label>51</label>
      <mixed-citation publication-type="journal"><name><surname>Azami</surname><given-names>H</given-names></name>, <name><surname>Escudero</surname><given-names>J</given-names></name>. <article-title>Amplitude- and Fluctuation-Based Dispersion Entropy</article-title>. <source>Entropy</source>. <year>2018</year>;<volume>20</volume>: <fpage>210</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e20030210</pub-id><?supplied-pmid 33265301?><pub-id pub-id-type="pmid">33265301</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref052">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Fu</surname><given-names>W</given-names></name>, <name><surname>Tan</surname><given-names>J</given-names></name>, <name><surname>Xu</surname><given-names>Y</given-names></name>, <name><surname>Wang</surname><given-names>K</given-names></name>, <name><surname>Chen</surname><given-names>T</given-names></name>. <article-title>Fault Diagnosis for Rolling Bearings Based on Fine-Sorted Dispersion Entropy and SVM Optimized with Mutation SCA-PSO</article-title>. <source>Entropy</source>. <year>2019</year>;<volume>21</volume>: <fpage>404</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e21040404</pub-id><?supplied-pmid 33267118?><pub-id pub-id-type="pmid">33267118</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref053">
      <label>53</label>
      <mixed-citation publication-type="journal"><name><surname>Hsu</surname><given-names>C</given-names></name>, <name><surname>Wei</surname><given-names>S-Y</given-names></name>, <name><surname>Huang</surname><given-names>H-P</given-names></name>, <name><surname>Hsu</surname><given-names>L</given-names></name>, <name><surname>Chi</surname><given-names>S</given-names></name>, <name><surname>Peng</surname><given-names>C-K</given-names></name>. <article-title>Entropy of Entropy: Measurement of Dynamical Complexity for Biological Systems.</article-title><source>Entropy</source>. <year>2017</year>;<volume>19</volume>: <fpage>550</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e19100550</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref054">
      <label>54</label>
      <mixed-citation publication-type="journal"><name><surname>Yan</surname><given-names>C</given-names></name>, <name><surname>Li</surname><given-names>P</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Yin</surname><given-names>C</given-names></name>, <name><surname>Yao</surname><given-names>L</given-names></name>. <article-title>Novel gridded descriptors of poincaré plot for analyzing heartbeat interval time-series</article-title>. <source>Comput Biol Med</source>. <year>2019</year>;<volume>109</volume>: <fpage>280</fpage>–<lpage>289</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.compbiomed.2019.04.015</pub-id><?supplied-pmid 31100581?><pub-id pub-id-type="pmid">31100581</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref055">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Yan</surname><given-names>C</given-names></name>, <name><surname>Li</surname><given-names>P</given-names></name>, <name><surname>Ji</surname><given-names>L</given-names></name>, <name><surname>Yao</surname><given-names>L</given-names></name>, <name><surname>Karmakar</surname><given-names>C</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>. <article-title>Area asymmetry of heart rate variability signal</article-title>. <source>Biomed Eng Online</source>. <year>2017</year>;<volume>16</volume>: <fpage>112</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/s12938-017-0402-3</pub-id><?supplied-pmid 28934961?><pub-id pub-id-type="pmid">28934961</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref056">
      <label>56</label>
      <mixed-citation publication-type="journal"><name><surname>Porta</surname><given-names>A</given-names></name>, <name><surname>Casali</surname><given-names>KR</given-names></name>, <name><surname>Casali</surname><given-names>AG</given-names></name>, <name><surname>Gnecchi-Ruscone</surname><given-names>T</given-names></name>, <name><surname>Tobaldini</surname><given-names>E</given-names></name>, <name><surname>Montano</surname><given-names>N</given-names></name>, <etal>et al</etal>. <article-title>Temporal asymmetries of short-term heart period variability are linked to autonomic regulation</article-title>. <source>Am J Physiol—Regul Integr Comp Physiol</source>. <year>2008</year>;<fpage>295</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1152/ajpregu.00129.2008</pub-id><?supplied-pmid 18495836?><pub-id pub-id-type="pmid">18495836</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref057">
      <label>57</label>
      <mixed-citation publication-type="journal"><name><surname>Karmakar</surname><given-names>CK</given-names></name>, <name><surname>Khandoker</surname><given-names>AH</given-names></name>, <name><surname>Palaniswami</surname><given-names>M</given-names></name>. <article-title>Phase asymmetry of heart rate variability signal</article-title>. <source>Physiol Meas</source>. <year>2015</year>;<volume>36</volume>: <fpage>303</fpage>–<lpage>314</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1088/0967-3334/36/2/303</pub-id><?supplied-pmid 25585603?><pub-id pub-id-type="pmid">25585603</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref058">
      <label>58</label>
      <mixed-citation publication-type="journal"><name><surname>Guzik</surname><given-names>P</given-names></name>, <name><surname>Piskorski</surname><given-names>J</given-names></name>, <name><surname>Krauze</surname><given-names>T</given-names></name>, <name><surname>Wykretowicz</surname><given-names>A</given-names></name>, <name><surname>Wysocki</surname><given-names>H</given-names></name>. <article-title>Heart rate asymmetry by Poincaré plots of RR intervals</article-title>. <source>Biomedizinische Technik</source>. <year>2006</year>. pp. <fpage>272</fpage>–<lpage>275</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1515/BMT.2006.054</pub-id><?supplied-pmid 17061956?><pub-id pub-id-type="pmid">17061956</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref059">
      <label>59</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>X</given-names></name>, <name><surname>Jiang</surname><given-names>A</given-names></name>, <name><surname>Xu</surname><given-names>N</given-names></name>, <name><surname>Xue</surname><given-names>J</given-names></name>. <article-title>Increment Entropy as a Measure of Complexity for Time Series</article-title>. <source>Entropy</source>. <year>2016</year>;<volume>18</volume>: <fpage>22</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e18010022</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref060">
      <label>60</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>X</given-names></name>, <name><surname>Jiang</surname><given-names>A</given-names></name>, <name><surname>Xu</surname><given-names>N</given-names></name>, <name><surname>Xue</surname><given-names>J</given-names></name>. Correction on <name><surname>Liu</surname><given-names>X.</given-names></name>; <name><surname>Jiang</surname><given-names>A.</given-names></name>; <name><surname>Xu</surname><given-names>N.</given-names></name>; <name><surname>Xue</surname><given-names>J</given-names></name>. <article-title>Increment Entropy as a Measure of Complexity for Time Series</article-title>. <source>Entropy</source><year>2016</year>, <volume>18</volume>, <fpage>22</fpage>. Entropy. 2016;18: 133. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e18040133</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref061">
      <label>61</label>
      <mixed-citation publication-type="book"><name><surname>Liu</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>X</given-names></name>, <name><surname>Zhou</surname><given-names>X</given-names></name>, <name><surname>Jiang</surname><given-names>A</given-names></name>. <part-title>Appropriate use of the increment entropy for electrophysiological time series</part-title>. <source>Computers in Biology and Medicine</source>. <publisher-name>Elsevier Ltd</publisher-name>; <year>2018</year>. pp. <fpage>13</fpage>–<lpage>23</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.compbiomed.2018.01.009</pub-id><?supplied-pmid 29433037?></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref062">
      <label>62</label>
      <mixed-citation publication-type="journal"><name><surname>Dünki</surname><given-names>RM</given-names></name>. <article-title>The estimation of the Kolmogorov entropy from a time series and its limitations when performed on EEG</article-title>. <source>Bull Math Biol</source>. <year>1991</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/BF02461547</pub-id><?supplied-pmid 1933033?><pub-id pub-id-type="pmid">1933033</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref063">
      <label>63</label>
      <mixed-citation publication-type="journal"><name><surname>Grassberger</surname><given-names>P</given-names></name>, <name><surname>Procaccia</surname><given-names>I</given-names></name>. <article-title>Estimation of the Kolmogorov entropy from a chaotic signal</article-title>. <source>Phys Rev A</source>. <year>1983</year>;<volume>28</volume>: <fpage>2591</fpage>–<lpage>2593</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevA.28.2591</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref064">
      <label>64</label>
      <mixed-citation publication-type="journal"><name><surname>Gao</surname><given-names>L</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Chen</surname><given-names>L</given-names></name>. <article-title>Event-related desynchronization and synchronization quantification in motor-related EEG by Kolmogorov entropy</article-title>. <source>J Neural Eng</source>. <year>2013</year>;<volume>10</volume>: <fpage>036023</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1088/1741-2560/10/3/036023</pub-id><?supplied-pmid 23676901?><pub-id pub-id-type="pmid">23676901</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref065">
      <label>65</label>
      <mixed-citation publication-type="journal"><name><surname>Huo</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Shu</surname><given-names>L</given-names></name>, <name><surname>Liao</surname><given-names>X</given-names></name>. <article-title>Edge Permutation Entropy: An Improved Entropy Measure for Time-Series Analysis. IECON Proceedings (Industrial Electronics Conference).</article-title><source>IEEE Computer Society</source>; <year>2019</year>. pp. <fpage>5998</fpage>–<lpage>6003</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/IECON.2019.8927449</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref066">
      <label>66</label>
      <mixed-citation publication-type="journal"><name><surname>Bandt</surname><given-names>C</given-names></name>, <name><surname>Pompe</surname><given-names>B</given-names></name>. <article-title>Permutation Entropy: A Natural Complexity Measure for Time Series</article-title>. <source>Phys Rev Lett</source>. <year>2002</year>;<volume>88</volume>: <fpage>4</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevLett.88.174102</pub-id><?supplied-pmid 12005759?><pub-id pub-id-type="pmid">12005759</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref067">
      <label>67</label>
      <mixed-citation publication-type="journal"><name><surname>Xiao-Feng</surname><given-names>L</given-names></name>, <name><surname>Yue</surname><given-names>W</given-names></name>. <article-title>Fine-grained permutation entropy as a measure of natural complexity for time series</article-title>. <source>Chinese Phys B</source>. <year>2009</year>;<volume>18</volume>: <fpage>2690</fpage>–<lpage>2695</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1088/1674-1056/18/7/011</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref068">
      <label>68</label>
      <mixed-citation publication-type="journal"><name><surname>Bian</surname><given-names>C</given-names></name>, <name><surname>Qin</surname><given-names>C</given-names></name>, <name><surname>Ma</surname><given-names>QDY</given-names></name>, <name><surname>Shen</surname><given-names>Q</given-names></name>. <article-title>Modified permutation-entropy analysis of heartbeat dynamics</article-title>. <source>Phys Rev E—Stat Nonlinear, Soft Matter Phys</source>. <year>2012</year>;<volume>85</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevE.85.021906</pub-id><?supplied-pmid 22463243?><pub-id pub-id-type="pmid">22463243</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref069">
      <label>69</label>
      <mixed-citation publication-type="journal"><name><surname>Riedl</surname><given-names>M</given-names></name>, <name><surname>Müller</surname><given-names>A</given-names></name>, <name><surname>Wessel</surname><given-names>N</given-names></name>. <article-title>Practical considerations of permutation entropy: A tutorial review</article-title>. <source>European Physical Journal: Special Topics</source>. <year>2013</year>. pp. <fpage>249</fpage>–<lpage>262</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1140/epjst/e2013-01862-7</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref070">
      <label>70</label>
      <mixed-citation publication-type="journal"><name><surname>Fadlallah</surname><given-names>B</given-names></name>, <name><surname>Chen</surname><given-names>B</given-names></name>, <name><surname>Keil</surname><given-names>A</given-names></name>, <name><surname>Príncipe</surname><given-names>J</given-names></name>. <article-title>Weighted-permutation entropy: A complexity measure for time series incorporating amplitude information</article-title>. <source>Phys Rev E—Stat Nonlinear, Soft Matter Phys</source>. <year>2013</year>;<volume>87</volume>: <fpage>022911</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevE.87.022911</pub-id><?supplied-pmid 23496595?><pub-id pub-id-type="pmid">23496595</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref071">
      <label>71</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>Z</given-names></name>, <name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Liang</surname><given-names>H</given-names></name>, <name><surname>Yu</surname><given-names>J</given-names></name>. <article-title>Improved permutation entropy for measuring complexity of time series under noisy condition.</article-title><source>Complexity</source>. <year>2019</year>;<volume>2019</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1155/2019/4203158</pub-id><?supplied-pmid 31341377?><pub-id pub-id-type="pmid">31341377</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref072">
      <label>72</label>
      <mixed-citation publication-type="journal"><name><surname>Rohila</surname><given-names>A</given-names></name>, <name><surname>Sharma</surname><given-names>A</given-names></name>. <article-title>Phase entropy: A new complexity measure for heart rate variability</article-title>. <source>Physiol Meas</source>. <year>2019</year>;<volume>40</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1088/1361-6579/ab499e</pub-id><?supplied-pmid 31574498?><pub-id pub-id-type="pmid">31574498</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref073">
      <label>73</label>
      <mixed-citation publication-type="journal"><name><surname>Cuesta-Frau</surname><given-names>D.</given-names></name><article-title>Slope Entropy: A New Time Series Complexity Estimator Based on Both Symbolic Patterns and Amplitude Information</article-title>. <source>Entropy</source>. <year>2019</year>;<volume>21</volume>: <fpage>1167</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e21121167</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref074">
      <label>74</label>
      <mixed-citation publication-type="journal"><name><surname>Powell</surname><given-names>GE</given-names></name>, <name><surname>Percival</surname><given-names>IC</given-names></name>. <article-title>A spectral entropy method for distinguishing regular and irregular motion of Hamiltonian systems</article-title>. <source>J Phys A Math Gen</source>. <year>1979</year>;<volume>12</volume>: <fpage>2053</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1088/0305-4470/12/11/017</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref075">
      <label>75</label>
      <mixed-citation publication-type="journal"><name><surname>Inouye</surname><given-names>T</given-names></name>, <name><surname>Shinosaki</surname><given-names>K</given-names></name>, <name><surname>Sakamoto</surname><given-names>H</given-names></name>, <name><surname>Toi</surname><given-names>S</given-names></name>, <name><surname>Ukai</surname><given-names>S</given-names></name>, <name><surname>Iyama</surname><given-names>A</given-names></name>, <etal>et al</etal>. <article-title>Quantification of EEG irregularity by use of the entropy of the power spectrum</article-title>. <source>Electroencephalogr Clin Neurophysiol</source>. <year>1991</year>;<volume>79</volume>: <fpage>204</fpage>–<lpage>210</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/0013-4694(91)90138-t</pub-id><?supplied-pmid 1714811?><pub-id pub-id-type="pmid">1714811</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref076">
      <label>76</label>
      <mixed-citation publication-type="book"><name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Li</surname><given-names>T</given-names></name>, <name><surname>Xie</surname><given-names>R</given-names></name>, <name><surname>Wang</surname><given-names>XM</given-names></name>, <name><surname>Cao</surname><given-names>YY</given-names></name>. <part-title>Fault feature extraction for multiple electrical faults of aviation electro-mechanical actuator based on symbolic dynamics entropy</part-title>. In <source>2015 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)</source><year>2015</year><month>Sep</month><day>19</day> (pp. <fpage>1</fpage>–<lpage>6</lpage>). <publisher-name>IEEE</publisher-name>.</mixed-citation>
    </ref>
    <ref id="pone.0259448.ref077">
      <label>77</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Y</given-names></name>, <name><surname>Yang</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>G</given-names></name>, <name><surname>Xu</surname><given-names>M</given-names></name>, <name><surname>Huang</surname><given-names>W</given-names></name>. <article-title>A fault diagnosis scheme for planetary gearboxes using modified multi-scale symbolic dynamic entropy and mRMR feature selection</article-title>. <source>Mech Syst Signal Process</source>. <year>2017</year>;<volume>91</volume>: <fpage>295</fpage>–<lpage>312</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.ymssp.2016.12.040</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref078">
      <label>78</label>
      <mixed-citation publication-type="journal"><name><surname>Rajagopalan</surname><given-names>V</given-names></name>, <name><surname>Ray</surname><given-names>A</given-names></name>. <article-title>Symbolic time series analysis via wavelet-based partitioning</article-title>. <source>Signal Processing.</source><year>2006</year>;<volume>86</volume>: <fpage>3309</fpage>–<lpage>3320</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.sigpro.2006.01.014</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref079">
      <label>79</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Shang</surname><given-names>P</given-names></name>. <article-title>Analysis of financial stock markets through the multiscale cross-distribution entropy based on the Tsallis entropy</article-title>. <source>Nonlinear Dyn</source>. <year>2018</year>;<volume>94</volume>: <fpage>1361</fpage>–<lpage>1376</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11071-018-4429-1</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref080">
      <label>80</label>
      <mixed-citation publication-type="journal"><name><surname>Xie</surname><given-names>HB</given-names></name>, <name><surname>Zheng</surname><given-names>YP</given-names></name>, <name><surname>Guo</surname><given-names>JY</given-names></name>, <name><surname>Chen</surname><given-names>X</given-names></name>. <article-title>Cross-fuzzy entropy: A new method to test pattern synchrony of bivariate time series</article-title>. <source>Inf Sci (Ny).</source><year>2010</year>;<volume>180</volume>: <fpage>1715</fpage>–<lpage>1724</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.ins.2010.01.004</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref081">
      <label>81</label>
      <mixed-citation publication-type="journal"><name><surname>Shi</surname><given-names>W</given-names></name>, <name><surname>Shang</surname><given-names>P</given-names></name>, <name><surname>Lin</surname><given-names>A</given-names></name>. <article-title>The coupling analysis of stock market indices based on cross-permutation entropy</article-title>. <source>Nonlinear Dyn</source>. <year>2015</year>;<volume>79</volume>: <fpage>2439</fpage>–<lpage>2447</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11071-014-1823-1</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref082">
      <label>82</label>
      <mixed-citation publication-type="journal"><name><surname>Azami</surname><given-names>H</given-names></name>, <name><surname>Escudero</surname><given-names>J</given-names></name>, <name><surname>Humeau-Heurtier</surname><given-names>A</given-names></name>. <article-title>Bidimensional Distribution Entropy to Analyze the Irregularity of Small-Sized Textures</article-title>. <source>IEEE Signal Process Lett</source>. <year>2017</year>;<volume>24</volume>: <fpage>1338</fpage>–<lpage>1342</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/LSP.2017.2723505</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref083">
      <label>83</label>
      <mixed-citation publication-type="journal"><name><surname>Azami</surname><given-names>H</given-names></name>, <name><surname>Virgilio Da Silva</surname><given-names>E</given-names></name>, <name><surname>Omoto</surname><given-names>ACM</given-names></name>, <name><surname>Humeau-Heurtier</surname><given-names>A</given-names></name>. <article-title>Two-dimensional dispersion entropy: An information-theoretic method for irregularity analysis of images</article-title>. <source>Signal Process Image Commun</source>. <year>2019</year>;<volume>75</volume>: <fpage>178</fpage>–<lpage>187</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.image.2019.04.013</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref084">
      <label>84</label>
      <mixed-citation publication-type="book"><name><surname>Hilal</surname><given-names>M</given-names></name>, <name><surname>Gaudencio</surname><given-names>ASF</given-names></name>, <name><surname>Berthin</surname><given-names>C</given-names></name>, <name><surname>Vaz</surname><given-names>PG</given-names></name>, <name><surname>Cardoso</surname><given-names>J</given-names></name>, <name><surname>Martin</surname><given-names>L</given-names></name>, <etal>et al</etal>. <part-title>Bidimensional Colored Fuzzy Entropy Measure: A Cutaneous Microcirculation Study</part-title>. <publisher-name>International Conference on Advances in Biomedical Engineering, ICABME. Institute of Electrical and Electronics Engineers Inc.</publisher-name>; <year>2019</year>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ICABME47164.2019.8940215</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref085">
      <label>85</label>
      <mixed-citation publication-type="journal"><name><surname>Segato dos Santos</surname><given-names>LF</given-names></name>, <name><surname>Neves</surname><given-names>LA</given-names></name>, <name><surname>Rozendo</surname><given-names>GB</given-names></name>, <name><surname>Ribeiro</surname><given-names>MG</given-names></name>, <name><surname>Zanchetta do Nascimento</surname><given-names>M</given-names></name>, <name><surname>Azevedo Tosta</surname><given-names>TA</given-names></name>. <article-title>Multidimensional and fuzzy sample entropy (SampEnMF) for quantifying H&amp;E histological images of colorectal cancer</article-title>. <source>Comput Biol Med</source>. <year>2018</year>;<volume>103</volume>: <fpage>148</fpage>–<lpage>160</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.compbiomed.2018.10.013</pub-id><?supplied-pmid 30368171?><pub-id pub-id-type="pmid">30368171</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref086">
      <label>86</label>
      <mixed-citation publication-type="journal"><name><surname>Silva</surname><given-names>LE V</given-names></name>, <name><surname>Filho</surname><given-names>ACSS</given-names></name>, <name><surname>Fazan</surname><given-names>VPS</given-names></name>, <name><surname>Felipe</surname><given-names>JC</given-names></name>, <name><surname>Junior</surname><given-names>LOM</given-names></name>. <article-title>Two-dimensional sample entropy: assessing image texture through irregularity</article-title>. <source>Biomed Phys Eng Express</source>. <year>2016</year>;<volume>2</volume>: <fpage>045002</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1088/2057-1976/2/4/045002</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref087">
      <label>87</label>
      <mixed-citation publication-type="journal"><name><surname>Nikulin V</surname><given-names>V.</given-names></name>, <name><surname>Brismar</surname><given-names>T.</given-names></name><article-title>Comment on “Multiscale Entropy Analysis of Complex Physiologic Time Series</article-title>. <source>Phys Rev Lett</source>. <year>2004</year>;<volume>92</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevLett.92.089803</pub-id><?supplied-pmid 14995828?><pub-id pub-id-type="pmid">14995828</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref088">
      <label>88</label>
      <mixed-citation publication-type="journal"><name><surname>Costa</surname><given-names>M</given-names></name>, <name><surname>Goldberger</surname><given-names>AL</given-names></name>, <name><surname>Peng</surname><given-names>CK</given-names></name>. <article-title>Costa, Goldberger, and Peng Reply</article-title>. <source>Phys Rev Lett</source>. <year>2004</year>;<volume>92</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevLett.92.089804</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref089">
      <label>89</label>
      <mixed-citation publication-type="journal"><name><surname>Hu</surname><given-names>M</given-names></name>, <name><surname>Liang</surname><given-names>H</given-names></name>. <article-title>Intrinsic mode entropy based on multivariate empirical mode decomposition and its application to neural data analysis</article-title>. <source>Cogn Neurodyn</source>. <year>2011</year>;<volume>5</volume>: <fpage>277</fpage>–<lpage>284</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11571-011-9159-8</pub-id><?supplied-pmid 22942916?><pub-id pub-id-type="pmid">22942916</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref090">
      <label>90</label>
      <mixed-citation publication-type="journal"><name><surname>Humeau-Heurtier</surname><given-names>A.</given-names></name><article-title>The multiscale entropy algorithm and its variants: A review</article-title>. <source>Entropy</source>. <year>2015</year>;<volume>17</volume>: <fpage>3110</fpage>–<lpage>3123</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e17053110</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref091">
      <label>91</label>
      <mixed-citation publication-type="journal"><name><surname>Gao</surname><given-names>J</given-names></name>, <name><surname>Hu</surname><given-names>J</given-names></name>, <name><surname>Tung</surname><given-names>WW</given-names></name>. <article-title>Entropy measures for biological signal analyses</article-title>. <source>Nonlinear Dyn</source>. <year>2012</year>;<volume>68</volume>: <fpage>431</fpage>–<lpage>444</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11071-011-0281-2</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref092">
      <label>92</label>
      <mixed-citation publication-type="journal"><name><surname>Castiglioni</surname><given-names>P</given-names></name>, <name><surname>Coruzzi</surname><given-names>P</given-names></name>, <name><surname>Bini</surname><given-names>M</given-names></name>, <name><surname>Parati</surname><given-names>G</given-names></name>, <name><surname>Faini</surname><given-names>A</given-names></name>. <article-title>Multiscale Sample Entropy of Cardiovascular Signals: Does the Choice between Fixed- or Varying-Tolerance among Scales Influence Its Evaluation and Interpretation?</article-title><source>Entropy</source>. <year>2017</year>;<volume>19</volume>: <fpage>590</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e19110590</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref093">
      <label>93</label>
      <mixed-citation publication-type="journal"><name><surname>Pham</surname><given-names>TD</given-names></name>. <article-title>Time-Shift Multiscale Entropy Analysis of Physiological Signals</article-title>. <source>Entropy</source>. <year>2017</year>;<volume>19</volume>: <fpage>257</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e19060257</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref094">
      <label>94</label>
      <mixed-citation publication-type="journal"><name><surname>Azami</surname><given-names>H</given-names></name>, <name><surname>Escudero</surname><given-names>J</given-names></name>. <article-title>Coarse-graining approaches in univariate multiscale sample and dispersion entropy.</article-title><source>Entropy</source>. <year>2018</year>;<volume>20</volume>: <fpage>138</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3390/e20020138</pub-id><?supplied-pmid 33265229?><pub-id pub-id-type="pmid">33265229</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref095">
      <label>95</label>
      <mixed-citation publication-type="journal"><name><surname>Marwaha</surname><given-names>P</given-names></name>, <name><surname>Sunkaria</surname><given-names>RK</given-names></name>. <article-title>Optimal Selection of Threshold Value ‘r’ for Refined Multiscale Entropy</article-title>. <source>Cardiovasc Eng Technol</source>. <year>2015</year>;<volume>6</volume>: <fpage>557</fpage>–<lpage>576</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s13239-015-0242-x</pub-id><?supplied-pmid 26577486?><pub-id pub-id-type="pmid">26577486</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref096">
      <label>96</label>
      <mixed-citation publication-type="journal"><name><surname>Jiang</surname><given-names>Y</given-names></name>, <name><surname>Peng</surname><given-names>CK</given-names></name>, <name><surname>Xu</surname><given-names>Y</given-names></name>. <article-title>Hierarchical entropy analysis for biological signals</article-title>. <source>Journal of Computational and Applied Mathematics</source>. <year>2011</year>. pp. <fpage>728</fpage>–<lpage>742</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.cam.2011.06.007</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref097">
      <label>97</label>
      <mixed-citation publication-type="book"><name><surname>Yan</surname><given-names>R</given-names></name>, <name><surname>Yang</surname><given-names>Z</given-names></name>, <name><surname>Zhang</surname><given-names>T</given-names></name>. <source>Multiscale cross entropy: A novel algorithm for analyzing two time series</source>. <publisher-name>5th International Conference on Natural Computation, ICNC 2009.</publisher-name><year>2009</year>. pp. <fpage>411</fpage>–<lpage>413</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/ICNC.2009.118</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref098">
      <label>98</label>
      <mixed-citation publication-type="journal"><name><surname>Wu</surname><given-names>H-T</given-names></name>, <name><surname>Lee</surname><given-names>C-Y</given-names></name>, <name><surname>Liu</surname><given-names>C-C</given-names></name>, <name><surname>Liu</surname><given-names>A-B</given-names></name>. <article-title>Multiscale Cross-Approximate Entropy Analysis as a Measurement of Complexity between ECG R-R Interval and PPG Pulse Amplitude Series among the Normal and Diabetic Subjects</article-title>. <source>Comput Math Methods Med</source>. <year>2013</year>;<volume>2013</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1155/2013/231762</pub-id><?supplied-pmid 24174987?><pub-id pub-id-type="pmid">24174987</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref099">
      <label>99</label>
      <mixed-citation publication-type="journal"><name><surname>Jamin</surname><given-names>A</given-names></name>, <name><surname>Duval</surname><given-names>G</given-names></name>, <name><surname>Annweiler</surname><given-names>C</given-names></name>, <name><surname>Abraham</surname><given-names>P</given-names></name>, <name><surname>Humeau-Heurtier</surname><given-names>A</given-names></name>. <article-title>A Novel Multiscale Cross-Entropy Method Applied to Navigation Data Acquired with a Bike Simulator.</article-title><source>IEEE EMBC</source><year>2019</year>. pp. <fpage>733</fpage>–<lpage>736</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/EMBC.2019.8856815</pub-id><?supplied-pmid 31946001?><pub-id pub-id-type="pmid">31946001</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref100">
      <label>100</label>
      <mixed-citation publication-type="journal"><name><surname>Costa</surname><given-names>M</given-names></name>, <name><surname>Goldberger</surname><given-names>AL</given-names></name>, <name><surname>Peng</surname><given-names>CK</given-names></name>. <article-title>Multiscale entropy analysis of biological signals</article-title>. <source>Phys Rev E—Stat Nonlinear, Soft Matter Phys.</source><year>2005</year>;<volume>71</volume>: <fpage>021906</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevE.71.021906</pub-id><?supplied-pmid 15783351?><pub-id pub-id-type="pmid">15783351</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref101">
      <label>101</label>
      <mixed-citation publication-type="journal"><name><surname>Yin</surname><given-names>Y</given-names></name>, <name><surname>Shang</surname><given-names>P</given-names></name>, <name><surname>Feng</surname><given-names>G</given-names></name>. <article-title>Modified multiscale cross-sample entropy for complex time series</article-title>. <source>Appl Math Comput</source>. <year>2016</year>;<volume>289</volume>: <fpage>98</fpage>–<lpage>110</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.amc.2016.05.013</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref102">
      <label>102</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>XS</given-names></name>, <name><surname>Roy</surname><given-names>RJ</given-names></name>, <name><surname>Jensen</surname><given-names>EW</given-names></name>. <article-title>EEG complexity as a measure of depth of anesthesia for patients</article-title>. <source>IEEE Trans Biomed Eng</source>. <year>2001</year>;<volume>48</volume>: <fpage>1424</fpage>–<lpage>1433</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/10.966601</pub-id><?supplied-pmid 11759923?><pub-id pub-id-type="pmid">11759923</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref103">
      <label>103</label>
      <mixed-citation publication-type="journal"><name><surname>Harris</surname><given-names>CR</given-names></name>, <name><surname>Millman</surname><given-names>KJ</given-names></name>, <name><surname>van der Walt</surname><given-names>SJ</given-names></name>, <name><surname>Gommers</surname><given-names>R</given-names></name>, <name><surname>Virtanen</surname><given-names>P</given-names></name>, <name><surname>Cournapeau</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>Array programming with NumPy</article-title>. <source>Nature. Nature Research</source>; <year>2020</year>. pp. <fpage>357</fpage>–<lpage>362</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id><?supplied-pmid 32939066?><pub-id pub-id-type="pmid">32939066</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref104">
      <label>104</label>
      <mixed-citation publication-type="journal"><name><surname>Virtanen</surname><given-names>P</given-names></name>, <name><surname>Gommers</surname><given-names>R</given-names></name>, <name><surname>Oliphant</surname><given-names>TE</given-names></name>, <name><surname>Haberland</surname><given-names>M</given-names></name>, <name><surname>Reddy</surname><given-names>T</given-names></name>, <name><surname>Cournapeau</surname><given-names>D</given-names></name>, <etal>et al</etal>. <article-title>SciPy 1.0: fundamental algorithms for scientific computing</article-title> in <source>Python. Nat Methods</source>. <year>2020</year>;<volume>17</volume>: <fpage>261</fpage>–<lpage>272</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
<?supplied-pmid 32015543?><pub-id pub-id-type="pmid">32015543</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref105">
      <label>105</label>
      <mixed-citation publication-type="journal"><name><surname>Hunter</surname><given-names>JD</given-names></name>. <article-title>Matplotlib: A 2D graphics environment</article-title>. <source>Comput Sci Eng</source>. <year>2007</year>;<volume>9</volume>: <fpage>90</fpage>–<lpage>95</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0259448.ref106">
      <label>106</label>
      <mixed-citation publication-type="other">Laszuk D. PyEMD: Python implementation of Empirical Mode Decompoisition (EMD) method. [cited 10 Jun 2021]. Available: <ext-link xlink:href="https://github.com/laszukdawid/PyEMD" ext-link-type="uri">https://github.com/laszukdawid/PyEMD</ext-link>.</mixed-citation>
    </ref>
  </ref-list>
</back>
