<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7353739</article-id>
    <article-id pub-id-type="publisher-id">3635</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-020-03635-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Keras R-CNN: library for cell detection in biological images using deep neural networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Hung</surname>
          <given-names>Jane</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Goodman</surname>
          <given-names>Allen</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Ravel</surname>
          <given-names>Deepali</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Lopes</surname>
          <given-names>Stefanie C. P.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Rangel</surname>
          <given-names>Gabriel W.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nery</surname>
          <given-names>Odailton A.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Malleret</surname>
          <given-names>Benoit</given-names>
        </name>
        <xref ref-type="aff" rid="Aff7">7</xref>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nosten</surname>
          <given-names>Francois</given-names>
        </name>
        <xref ref-type="aff" rid="Aff9">9</xref>
        <xref ref-type="aff" rid="Aff10">10</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lacerda</surname>
          <given-names>Marcus V. G.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ferreira</surname>
          <given-names>Marcelo U.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Rénia</surname>
          <given-names>Laurent</given-names>
        </name>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Duraisingh</surname>
          <given-names>Manoj T.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Costa</surname>
          <given-names>Fabio T. M.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff11">11</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Marti</surname>
          <given-names>Matthias</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff12">12</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1555-8261</contrib-id>
        <name>
          <surname>Carpenter</surname>
          <given-names>Anne E.</given-names>
        </name>
        <address>
          <email>anne@broadinstitute.org</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.116068.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 2341 2786</institution-id><institution>Department of Chemical Engineering, </institution><institution>Massachusetts Institute of Technology, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.66859.34</institution-id><institution>The Broad Institute, </institution></institution-wrap>Cambridge, MA USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.38142.3c</institution-id><institution-id institution-id-type="ISNI">000000041936754X</institution-id><institution>Harvard T.H.Chan School of Public Health, </institution></institution-wrap>Boston, MA USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.418068.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 0723 0931</institution-id><institution>Instituto Leônidas e Maria Deane, </institution><institution>Fundação Oswaldo Cruz (FIOCRUZ), </institution></institution-wrap>Manaus, Amazonas Brazil </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.418153.a</institution-id><institution-id institution-id-type="ISNI">0000 0004 0486 0972</institution-id><institution>Fundação de Medicina Tropical Dr. Heitor Vieira Dourado, Gerência de Malária, </institution></institution-wrap>Manaus, Amazonas Brazil </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.11899.38</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0722</institution-id><institution>Universidade de São Paulo, </institution></institution-wrap>São Paulo, Brazil </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.4280.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2180 6431</institution-id><institution>Department of Microbiology &amp; Immunology, Yong Loo Lin School of Medicine, </institution><institution>National University of Singapore, </institution></institution-wrap>Singapore, 119077 Singapore </aff>
      <aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.430276.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 0387 2429</institution-id><institution>Singapore Immunology Network (SIgN), </institution><institution>Agency for Science Research &amp; Technology, </institution></institution-wrap>Singapore, 138632 Singapore </aff>
      <aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="GRID">grid.10223.32</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0490</institution-id><institution>Shoklo Malaria Research Unit, Mahidol-Oxford Tropical Medicine Research Unit, Faculty of Tropical Medicine, </institution><institution>Mahidol University, </institution></institution-wrap>Mae Sot, Thailand </aff>
      <aff id="Aff10"><label>10</label>Centre for Tropical Medicine and Global Health, Nuffield, Oxford, UK </aff>
      <aff id="Aff11"><label>11</label><institution-wrap><institution-id institution-id-type="GRID">grid.411087.b</institution-id><institution-id institution-id-type="ISNI">0000 0001 0723 2494</institution-id><institution>Department of Genetics, Evolution, Microbiology and Immunology, </institution><institution>University of Campinas, </institution></institution-wrap>Campinas, SP Brazil </aff>
      <aff id="Aff12"><label>12</label><institution-wrap><institution-id institution-id-type="GRID">grid.8756.c</institution-id><institution-id institution-id-type="ISNI">0000 0001 2193 314X</institution-id><institution>Wellcome Centre for Integrative Parasitology Institute of Infection, Immunity and Inflammation, College of Medical Veterinary &amp; Life Sciences, </institution><institution>University of Glasgow, </institution></institution-wrap>Glasgow, UK </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>11</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>300</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>11</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">A common yet still manual task in basic biology research, high-throughput drug screening and digital pathology is identifying the number, location, and type of individual cells in images. Object detection methods can be useful for identifying individual cells as well as their phenotype in one step. State-of-the-art deep learning for object detection is poised to improve the accuracy and efficiency of biological image analysis.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We created <italic>Keras R-CNN</italic> to bring leading computational research to the everyday practice of bioimage analysts. <italic>Keras R-CNN</italic> implements deep learning object detection techniques using Keras and Tensorflow (<ext-link ext-link-type="uri" xlink:href="https://github.com/broadinstitute/keras-rcnn">https://github.com/broadinstitute/keras-rcnn</ext-link>). We demonstrate the command line tool’s simplified Application Programming Interface on two important biological problems, nucleus detection and malaria stage classification, and show its potential for identifying and classifying a large number of cells. For malaria stage classification, we compare results with expert human annotators and find comparable performance.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3"><italic>Keras R-CNN</italic> is a Python package that performs automated cell identification for both brightfield and fluorescence images and can process large image sets. Both the package and image datasets are freely available on GitHub and the Broad Bioimage Benchmark Collection.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Deep learning</kwd>
      <kwd>Keras</kwd>
      <kwd>Convolutional networks</kwd>
      <kwd>Malaria</kwd>
      <kwd>Object detection</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000009</institution-id>
            <institution>Foundation for the National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R01 GM089652</award-id>
        <award-id>MIRA R35 GM122547</award-id>
        <principal-award-recipient>
          <name>
            <surname>Carpenter</surname>
            <given-names>Anne E.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000861</institution-id>
            <institution>Burroughs Wellcome Fund</institution>
          </institution-wrap>
        </funding-source>
        <award-id>career development award</award-id>
        <principal-award-recipient>
          <name>
            <surname>Marti</surname>
            <given-names>Matthias</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Royal Society</institution>
        </funding-source>
        <award-id>Wolfson Merit</award-id>
        <principal-award-recipient>
          <name>
            <surname>Marti</surname>
            <given-names>Matthias</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008982</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DGE1144152</award-id>
        <principal-award-recipient>
          <name>
            <surname>Ravel</surname>
            <given-names>Deepali</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001807</institution-id>
            <institution>Fundação de Amparo à Pesquisa do Estado de São Paulo</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2017/18611-7</award-id>
        <principal-award-recipient>
          <name>
            <surname>Costa</surname>
            <given-names>Fabio T. M.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003593</institution-id>
            <institution>Conselho Nacional de Desenvolvimento Científico e Tecnológico</institution>
          </institution-wrap>
        </funding-source>
        <award-id>research fellowship</award-id>
        <award-id>research fellowship</award-id>
        <award-id>research fellowship</award-id>
        <principal-award-recipient>
          <name>
            <surname>Ferreira</surname>
            <given-names>Marcelo U.</given-names>
          </name>
          <name>
            <surname>Costa</surname>
            <given-names>Fabio T. M.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id>
            <institution>Howard Hughes Medical Institute</institution>
          </institution-wrap>
        </funding-source>
        <award-id>Gilliam Fellowship for Advanced Study</award-id>
        <principal-award-recipient>
          <name>
            <surname>Rangel</surname>
            <given-names>Gabriel W.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par12">Identifying individual cells in images is often a crucial task for basic biology research, high-throughput drug screening and digital pathology. Traditional segmentation methods (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a) identify individual pixels that belong to each distinct object through a carefully designed series of image processing steps, often involving watershed, distance transform, and intensity gradients. This approach requires algorithm selection and parameter tuning (and thus time and expertise), is computationally expensive, and often fails to sufficiently handle noisy images, illumination fluctuations, and clumped cells. In cases where phenotype classification is also needed, hundreds of classical morphological features are extracted per cell, followed by a subsequent machine learning step to classify each cell. User-friendly software exists for these steps [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref>], but it does add significant effort to a typical workflow.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Overview of a traditional segmentation based pipeline and a deep learning based object detection pipeline. <bold>a</bold>. Traditional segmentation based pipelines require the selection and tuning of multiple classical image processing algorithms to produce a segmentation, where pixels associated with individual instances (e.g. nuclei, or cells) receive unique “labels”, represented here as different colors. <bold>b</bold>. Deep learning-based object detection pipelines require some example annotated images to be provided, and use neural networks to learn a model that can produce bounding boxes around each object, which can be overlapping. If multiple object classes are of interest (for example, multiple phenotypes), each bounding box is assigned a class. <bold>c</bold>. Code to train an object detection model, written using Keras R-CNN’s API. <bold>d</bold>. Graphs of cell counts of each infected type over time predicted on time course images. The time course set contains samples prepared at particular hours between 0 and 44 h and has been designed to synchronize the parasites’ growth and to show representation of all stages. The ground truth is based on Annotator 1, who annotated all images in the dataset including the training data</p></caption><graphic xlink:href="12859_2020_3635_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par13">Deep learning holds tremendous promise to overcome these challenges by simplifying workflows while also improving accuracy, at least in many contexts [<xref ref-type="bibr" rid="CR5">5</xref>]. In particular, deep learning-based object detection algorithms (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b) examine the raw pixels of images and discover which features and combinations of features best describe each phenotypic class of cells (based on examples provided by the researcher), with minimal user configuration of mathematical parameters. They can conveniently identify cells as well as their phenotype in one step. In contrast to segmentation, object detection algorithms yield a bounding box around each cell. Treating cell identification as an object detection problem rather than a pixel-level segmentation problem has several advantages, most notably that the annotation step is orders of magnitude faster. Drawing bounding boxes (in essence, marking two points in space) is much faster than the pixel level annotation that is needed for ground truth for training segmentation models, and they allow overlapping objects to be easily distinguishable. Object detection algorithms additionally do not need distinct steps to separate overlapping or touching cells and require much less storage in terms of training data and results. For cases where pixel-level segmentation is needed, object detection can be followed by post-processing steps to define precise boundaries for each object.</p>
    <p id="Par14">Here, we present an open source Keras package for cell detection called <italic>Keras R-CNN</italic>, as well as pre-trained deep learning models and new public datasets. <italic>Keras R-CNN</italic> is based on the Faster Region-based Convolutional Neural Network (<italic>Faster R-CNN</italic>) [<xref ref-type="bibr" rid="CR6">6</xref>] architecture, which is currently the basis of many best-performing models for object detection. It was among the top scoring in the 2018 Data Science Bowl public competition we co-organized [<xref ref-type="bibr" rid="CR7">7</xref>] and has already compared favorably with other deep learning architectures in performance and inference time [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR8">8</xref>] (Supplementary Figure <xref rid="MOESM1" ref-type="media">S6</xref>). <italic>Faster R-CNN</italic> takes an image as input and generates bounding boxes and bounding box classifications [<xref ref-type="bibr" rid="CR9">9</xref>]. Training involves gathering appropriate data as ground truth, namely a set of images with known bounding box coordinates and annotated labels annotated. Initially, a training procedure adapts the model to a particular application by optimizing the weights in the network. Subsequently, inference or prediction involves new images running through the trained model to produce output bounding box coordinates and class probabilities. To test the accuracy of a trained model for a given application, prediction is performed on a set of images that were not used for training (and, importantly, were collected in a completely different experimental batch than those used for training [<xref ref-type="bibr" rid="CR10">10</xref>]). The predictions are compared to the known ground truth annotations to characterize accuracy.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <p id="Par15"><italic>Keras R-CNN</italic> is distinguished from other deep learning based object detection implementations like Facebook’s Detectron [<xref ref-type="bibr" rid="CR11">11</xref>] or Tensorflow’s Object Detection API [<xref ref-type="bibr" rid="CR12">12</xref>] in several ways. First, <italic>Keras R-CNN</italic> can process an unlimited number of channels. Unlike standard consumer photos’ red, green and blue (RGB) channels, biological imaging assays often involve up to several dozen fluorescent labels in multispectral imaging. The <italic>Keras R-CNN</italic> schema is designed for users to easily provide their own datasets; its modular structure allows for flexibility and interoperability with Keras and the scientific Python ecosystems including NumPy, and it is portable across platforms (Windows, Mac, Linux) and devices. It also includes dataset augmentation through cropping, rescaling, and rotating; and efficient handling of large scale, densely annotated, and three (or more)-dimensional datasets.</p>
    <p id="Par16"><italic>Keras R-CNN</italic> can train a model in just a few lines of code as compared to the hundreds or even thousands required by other implementations (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c). While not itself a point-and-click tool, <italic>Keras R-CNN</italic> could serve as the foundation for a more accessible software tool serving biologists and pathologists [<xref ref-type="bibr" rid="CR13">13</xref>]. The use of Keras offers platform and device portability while being abstract enough for the code to be understandable and easily customizable. We also designed a human-readable schema for datasets (see Supplemental Material), which is readily understood by non-experts in computer vision so they can focus on solving biological problems.</p>
  </sec>
  <sec id="Sec3">
    <title>Results</title>
    <p id="Par17">We first demonstrated <italic>Keras R-CNN</italic> on a very common application: nucleus detection (see Supplementary Figure <xref rid="MOESM1" ref-type="media">S1</xref>). We used manually annotated fluorescent images showing nuclei stained with Hoechst 33342 to highlight their DNA. The ~ 600 training images (~ 30,000 nuclei) were from the 2018 Data Science Bowl (DSB) dataset <italic>BBBC038</italic> and the ~ 100 testing images (~ 10,000 nuclei) were from a subset of the human U2OS cell dataset <italic>BBBC022</italic> [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>]. We recently described instance segmentation of nuclei [<xref ref-type="bibr" rid="CR16">16</xref>]; here we instead address the problem of object detection, which yields bounding boxes and whose accuracy is assessed by a different metric.</p>
    <p id="Par18">We found that the trained model achieved a mean average precision score of 82% at an intersection-over-union (IoU) threshold of 0.5. For comparison, we also tested the traditional approach of segmentation, using a CellProfiler [<xref ref-type="bibr" rid="CR17">17</xref>] pipeline tuned to the data, achieving a score of 81%. The resulting scores are very similar, indicating that, on this relatively straightforward image analysis task, <italic>Keras R-CNN</italic> can perform as well as classical image processing algorithms that have been parameter-tuned by experts.</p>
    <p id="Par19">We next tested a more complex case: detection of cells in blood smears of patients infected with the human malaria parasite <italic>Plasmodium vivax</italic> in various stages, which requires phenotype classification (Supplementary Figure <xref rid="MOESM1" ref-type="media">2</xref>A) in addition to object detection. <italic>P. vivax</italic> causes a significant health burden in malaria endemic regions. Manual inspection of blood smears by trained microscopists remains the gold standard of parasite detection and stage determination because of its low cost and high flexibility. However, manual inspection and counting is tedious, requires resources to develop expertise, and is prone to significant human variability as the phenotypic changes are very subtle.</p>
    <p id="Par20">We applied <italic>Keras R-CNN</italic> to classify different stages of <italic>P. vivax</italic> development, with particular focus on trophozoites vs gametocytes. We collected 1364 images (~ 80,000 cells) from samples prepared by different groups: from Brazil, from Southeast Asia, and ex-vivo cultured samples prepared as a time course [<xref ref-type="bibr" rid="CR18">18</xref>] and made them publicly available as <italic>BBBC041</italic> (see Supplementary Table <xref rid="MOESM1" ref-type="media">S1</xref>). We created these three datasets across sites to increase reproducibility and robustness across different sample sets, preventing overfitting to a single laboratory’s imaging and staining routine (Supplementary Figure <xref rid="MOESM1" ref-type="media">2</xref>C). We used the time course images as our holdout set, and the others as training data.</p>
    <p id="Par21">We found that a trained <italic>Keras R-CNN</italic> model achieved a mean average precision score of 78% at an IoU threshold of 0.5 (Supplementary Figure <xref rid="MOESM1" ref-type="media">1</xref>), much higher than an expert-configured CellProfiler pipeline, which yielded 61%. Classical image processing struggles with this task, which is much more challenging than nucleus detection because the malaria images are brightfield rather than the high signal-to-noise fluorescent images in the nucleus detection case.</p>
    <p id="Par22">Because <italic>Keras R-CNN</italic> learns from expert-provided data, it is able to handle different image modalities without hand tuning parameters.</p>
    <p id="Par23">Mean average precision only assesses the ability to locate cells; we also evaluated phenotype classification. Classifying <italic>P. vivax</italic> stages is notoriously difficult, and often experts disagree. We quantified the level and nature of disagreement by giving two expert annotators the same set of blood smear images; there is significant confusion (disagreement) between them as well as a substantial number of cells each expert marked as low confidence (“difficult”) (Supplementary Figure <xref rid="MOESM1" ref-type="media">2</xref>B). In particular, trophozoites and gametocytes are prone to misclassification.</p>
    <p id="Par24">In light of this, in addition to comparing our predictions directly with expert annotations, we evaluated whether the results match the expected pattern for the time course images. Given the <italic>P. vivax</italic> life cycle, the number of parasites in the ring stage should decrease over time (until the parasites seed new cells and the cycle repeats), and the number in later stages should increase over time, consistent with expert counts and our results (Fig. <xref rid="Fig1" ref-type="fig">1</xref>d). We used diffusion pseudotime [<xref ref-type="bibr" rid="CR19">19</xref>] to create an unsupervised ordering of all the cells in the data set based on the features learned by the model, to see if these features can capture the known progression of the various parasite stages; previously, this sort of modeling has been used to show that deep learning features are useful in capturing other chronological progressions such as cell cycle phase (Supplementary Figure <xref rid="MOESM1" ref-type="media">S3</xref>). The ordering of the cells matches the expected cell progression, which suggests the learned features capture important and biologically meaningful underlying cell information.</p>
  </sec>
  <sec id="Sec4">
    <title>Conclusions</title>
    <p id="Par25">In summary, we find that state-of-the-art object detection with <italic>Keras R-CNN</italic> can be accomplished with just ~ 20 lines of code and little computer vision expertise required. As our aim was to implement an already-proven architecture, we did not focus here on optimizing and assessing model accuracy; performance could also improve even further given more annotated examples and better hyperparameter optimization. We emphasize that annotated example cell images are required to train the system, but many biologists may prefer marking bounding boxes rather than learning how to choose, operate and tune classical segmentation algorithms. The models trained in this work are freely available although tailored to specific applications; the open-source framework makes training for other applications and datasets relatively straightforward. We have also recently added additional architectures from the literature, such as Feature Pyramid Network (FPN) [<xref ref-type="bibr" rid="CR20">20</xref>] and Mask R-CNN [<xref ref-type="bibr" rid="CR21">21</xref>].</p>
    <p id="Par26"><italic>Keras R-CNN’</italic>s popularity was unexpected for a pre-1.0 release. It presently has more than 650 stars and forks on GitHub, making it one of the most popular codebases available on the Broad Institute GitHub organization. Machine learning based computer vision algorithms have the potential to greatly improve the accuracy of image analysis for biology and decrease reliance on human labor, by reducing manual image analysis as well as time spent configuring automated algorithms. <italic>Keras R-CNN</italic> is therefore a useful tool for performing automated cell identification for both brightfield and fluorescence images, and can serve as a foundation for future point-and-click software.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec5">
      <supplementary-material content-type="local-data" id="MOESM1">
        <media xlink:href="12859_2020_3635_MOESM1_ESM.docx">
          <caption>
            <p><bold>Additional file 1: Figure S1.</bold> Comparison of mean average precision curves for different IoU thresholds for Keras R-CNN versus CellProfiler on the nuclei and malaria datasets. For nuclei, the mean average precision is 0.99 at a threshold of 0.5 for Keras R-CNN. For malaria, the mean average precision is 0.78 at a threshold of 0.5 for Keras R-CNN. <bold>Figure S2.</bold> Overview of <italic>P. vivax</italic> data and results. The samples contain two classes of uninfected cells (red blood cells and leukocytes) and four classes of infected cells (gametocytes, rings, trophozoites, and schizonts) and have a heavy imbalance: more than 95% of all cells are uninfected, roughly the distribution in patient blood. A. Depiction of all relevant cell types found in human blood, including two types of uninfected cells and 4 types of infected cells in the <italic>P. vivax</italic> life cycle. The cycle on the left shows asexual development. Gametocytes come from sexual development and lead to transmission. B. Confusion matrix comparing annotations of two experts (colors normalized so that rows sum to 1); the significant signal off-diagonal speaks to the challenge for experts to agree upon the proper stage label for each cell. Experts were asked to identify relevant cells and label them as one of the cell types or difficult. C. Example of malaria-infected blood smear results. Red boxes are ground truth; blue boxes are predictions produced by Keras R-CNN. <bold>Table S1</bold>. Malaria datasets. <bold>Figure S3.</bold> Results for <italic>P. falciparum</italic>. <bold>Figure S4.</bold> Visualization of learned features and single-cell data. Diffusion pseudotime plots made from deep learning features with accompanying ground truth class information. The first row has plots of the first two diffusion coordinates and the next row has plots of the second and third diffusion coordinates. Note: the model used to generate these plots is slightly different than the final one run in the paper. <bold>Figure S5.</bold> Visualization of learned features and single-cell data. t-SNE plot made from deep learning features colored by ground truth class information. Note: the model used to generate these plots is slightly different than the final one run in the paper. <bold>Figure S6.</bold> Inference time comparison across common object detection methods.</p>
          </caption>
        </media>
      </supplementary-material>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>API</term>
        <def>
          <p id="Par4">Application programming interface</p>
        </def>
      </def-item>
      <def-item>
        <term>BBBC</term>
        <def>
          <p id="Par5">Broad Bioimage Benchmark Collection</p>
        </def>
      </def-item>
      <def-item>
        <term>DNA</term>
        <def>
          <p id="Par6">Deoxyribonucleic acid</p>
        </def>
      </def-item>
      <def-item>
        <term>DSB</term>
        <def>
          <p id="Par7">Data Science Bowl</p>
        </def>
      </def-item>
      <def-item>
        <term>FPN</term>
        <def>
          <p id="Par8">Feature Pyramid Network</p>
        </def>
      </def-item>
      <def-item>
        <term>IoU</term>
        <def>
          <p id="Par9">Intersection over union</p>
        </def>
      </def-item>
      <def-item>
        <term>
          <italic>P. vivax</italic>
        </term>
        <def>
          <p id="Par10">
            <italic>Plasmodium vivax</italic>
          </p>
        </def>
      </def-item>
      <def-item>
        <term>R-CNN</term>
        <def>
          <p id="Par11">Region-based convolutional neural network</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Deepali Ravel, Stefanie C. P. Lopes and Gabriel W. Rangel contributed equally to this work.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s12859-020-03635-x.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>We would like to acknowledge Dr. Shantanu Singh for providing input on the draft manuscript.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>JH designed the project, collected malaria image data, co-wrote Keras R-CNN, developed methodology, and drafted the manuscript. AG co-wrote Keras R-CNN and ran experiments. DR prepared and provided Brazilian samples and consulted on methodology and interpretation of the data. SL prepared and provided Brazilian samples and annotated. GR prepared and provided time course malaria samples and annotated. OAN prepared time course malaria samples from Brazil. BM prepared Southeast Asian malaria samples. FN prepared Southeast Asian malaria samples. MUF and ML prepared Brazilian samples. LR organised malaria slide collections from Thailand and read and corrected the manuscript. MD consulted on methodology and supervised in vitro culture work. FTMC consulted on methodology and interpretation of the data and co-supervised the project. MM consulted on methodology and interpretation of the data and supervised the project. AEC designed and supervised the project and contributed to the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Funding was provided by the National Institute of General Medical Sciences of the National Institutes of Health under R01 GM089652 and MIRA R35 GM122547 (to AEC). MM is supported by a career development award from BWF and a Wolfson Merit award from the Royal Society. DR was supported by a graduate fellowship (DGE1144152) from the U.S. National Science Foundation. FTMC is supported by FAPESP Grant 2017/18611–7. MVGL, MUF and FTMC are CNPq research fellows. GWR was supported by the HHMI Gilliam Fellowship for Advanced Study. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>Availability and requirements:</p>
    <p>Project name: Keras R-CNN.</p>
    <p>Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/broadinstitute/keras-rcnn">https://github.com/broadinstitute/keras-rcnn</ext-link></p>
    <p>Operating system: Platform independent.</p>
    <p>Programming language: Python 3.</p>
    <p>Other requirements: keras-resnet, numpy, tensorflow==1.13.1, Keras==2.2.4, scikit-image==0.15.0.</p>
    <p>License: BSD.</p>
    <p>The model weights can be found in keras_rcnn.applications. JHung2019.</p>
    <p>The datasets supporting the conclusions of this article are available in the following:</p>
    <p>Malaria data are available at <ext-link ext-link-type="uri" xlink:href="https://data.broadinstitute.org/bbbc/BBBC041/">https://data.broadinstitute.org/bbbc/BBBC041/</ext-link>.</p>
    <p>Nuclei data are available at <ext-link ext-link-type="uri" xlink:href="https://data.broadinstitute.org/bbbc/BBBC022/">https://data.broadinstitute.org/bbbc/BBBC022/</ext-link>.</p>
  </notes>
  <notes id="FPar1">
    <title>Ethics approval and consent to participate</title>
    <p id="Par27">Samples from Manaus, Brazil. All patients signed an informed consent form, and no children were included in the study. Sample collection was approved by Fundação de Medicina Tropical Ethical Board under the number CAAE 0044.0.114.000–11.</p>
    <p id="Par28">Samples from Acre, Brazil. Informed consent was obtained from all patients or, in case of children, from their parents/guardians. Study protocols for parasite sample collection were approved by the Institutional Review Board of the Institute of Biomedical Sciences, University of São Paulo, Brazil (1169/CEPSH, 2014).</p>
    <p id="Par29">Samples from Thailand. All patients signed an informed consent form, and no children were included in the study. The clinical samples were collected and tested in accordance with protocols approved by The Center for Clinical Vaccinology and Tropical Medicine at University of Oxford (OXTREC 17–11). Five mL of whole blood were collected in lithium heparin collection tubes. Samples were cryopreserved in Glycerolyte 57 (Baxter) after leukocyte depletion using cellulose columns (Sigma cat #C6288) (Sriprawat K, et al. Effective and cheap removal of leukocytes and platelets from <italic>Plasmodium vivax</italic> infected blood. Malar J 8, 115; 2009).</p>
  </notes>
  <notes id="FPar2">
    <title>Consent for publication</title>
    <p id="Par30">Not Applicable.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par31">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dao</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Fraser</surname>
            <given-names>AN</given-names>
          </name>
          <name>
            <surname>Hung</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ljosa</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Carpenter</surname>
            <given-names>AE</given-names>
          </name>
        </person-group>
        <article-title>CellProfiler Analyst: interactive data exploration, analysis and classification of large biological image sets</article-title>
        <source>Bioinformatics.</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>20</issue>
        <fpage>3210</fpage>
        <lpage>3212</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw390</pub-id>
        <pub-id pub-id-type="pmid">27354701</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Sommer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Straehle</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kothe</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Hamprecht</surname>
            <given-names>FA</given-names>
          </name>
        </person-group>
        <article-title>Ilastik: Interactive learning and segmentation toolkit</article-title>
        <source>Biomedical imaging: from Nano to Macro, 2011 IEEE International Symposium on</source>
        <year>2011</year>
        <fpage>230</fpage>
        <lpage>233</lpage>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Bankhead P, Loughrey MB, Fernández JA, Dombrowski Y, Mc Art DG, Dunne PD, et al. QuPath: Open source software for digital pathology image analysis Available from: 10.1101/099796.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Piccinini</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Balassa</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Szkalisity</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Molnar</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Paavolainen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kujala</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Advanced cell classifier: user-friendly machine-learning-based software for discovering phenotypes in high-content imaging data</article-title>
        <source>Cell Syst</source>
        <year>2017</year>
        <volume>4</volume>
        <issue>6</issue>
        <fpage>651</fpage>
        <lpage>5.e5</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cels.2017.05.012</pub-id>
        <pub-id pub-id-type="pmid">28647475</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Caicedo JC, Goodman A, Karhohs KW, Cimini BA, Ackerman J, Haghighi M, et al. Nucleus segmentation across imaging experiments: the 2018 data science bowl. Nat Methods. 2019. 10.1038/s41592-019-0612-7.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Ren S, He K, Girshick R, Sun J. Faster R-CNN: towards real-time object detection with region proposal networks [Internet]. arXiv [cs.CV]. 2015. Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1506.01497">http://arxiv.org/abs/1506.01497</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Caicedo JC, Goodman A, Karhohs KW, Cimini B, Ackerman J, Haghighi M, et al. Nuclei can be found robustly in biomedical images: results of the 2018 data science bowl. Broad Institute of MIT and Harvard.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Hollandi R, Szkalisity A, Toth T, Tasnadi E, Molnar C, Mathe B, et al. A deep learning framework for nucleus segmentation using image style transfer [Internet]. bioRxiv. 2019 [cited 2019 Mar 29]. p. 580605. Available from: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/580605v1.abstract">https://www.biorxiv.org/content/10.1101/580605v1.abstract</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Girshick R. Fast r-cnn. In: Proceedings of the IEEE International Conference on Computer Vision. 2015. p. 1440–8.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shamir</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Assessing the efficacy of low-level image content descriptors for computer-based fluorescence microscopy image analysis</article-title>
        <source>J Microsc</source>
        <year>2011</year>
        <volume>243</volume>
        <issue>3</issue>
        <fpage>284</fpage>
        <lpage>292</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1365-2818.2011.03502.x</pub-id>
        <pub-id pub-id-type="pmid">21605118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Detectron (<ext-link ext-link-type="uri" xlink:href="https://github.com/facebookresearch/Detectron">https://github.com/facebookresearch/Detectron</ext-link>) [Internet]. Github; [cited 2019 Jan 4]. Available from: <ext-link ext-link-type="uri" xlink:href="https://github.com/facebookresearch/Detectron">https://github.com/facebookresearch/Detectron</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Models (<ext-link ext-link-type="uri" xlink:href="https://github.com/tensorflow/models/tree/master/research/object_detection">https://github.com/tensorflow/models/tree/master/research/object_detection</ext-link>) [Internet]. Github; [cited 2018 Jun 15]. Available from: <ext-link ext-link-type="uri" xlink:href="https://github.com/tensorflow/models">https://github.com/tensorflow/models</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carpenter</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Kamentsky</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Eliceiri</surname>
            <given-names>KW</given-names>
          </name>
        </person-group>
        <article-title>A call for bioimaging software usability</article-title>
        <source>Nat Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <issue>7</issue>
        <fpage>666</fpage>
        <lpage>670</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2073</pub-id>
        <pub-id pub-id-type="pmid">22743771</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ljosa</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Sokolnicki</surname>
            <given-names>KL</given-names>
          </name>
          <name>
            <surname>Carpenter</surname>
            <given-names>AE</given-names>
          </name>
        </person-group>
        <article-title>Annotated high-throughput microscopy image sets for validation</article-title>
        <source>Nat Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <issue>7</issue>
        <fpage>637</fpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2083</pub-id>
        <pub-id pub-id-type="pmid">22743765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gustafsdottir</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Ljosa</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Sokolnicki</surname>
            <given-names>KL</given-names>
          </name>
          <name>
            <surname>Anthony Wilson</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Walpita</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kemp</surname>
            <given-names>MM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Multiplex cytological profiling assay to measure diverse cellular states</article-title>
        <source>PLoS One</source>
        <year>2013</year>
        <volume>8</volume>
        <issue>12</issue>
        <fpage>e80999</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0080999</pub-id>
        <pub-id pub-id-type="pmid">24312513</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">Caicedo JC, Roth J, Goodman A, Becker T, Karhohs KW, McQuin C, et al. Evaluation of deep learning strategies for nucleus segmentation in fluorescence images. bioRxiv. 2018.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McQuin</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Goodman</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chernyshev</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Kamentsky</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cimini</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Karhohs</surname>
            <given-names>KW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CellProfiler 3.0: next-generation image processing for biology</article-title>
        <source>PLoS Biol</source>
        <year>2018</year>
        <volume>16</volume>
        <issue>7</issue>
        <fpage>e2005970</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pbio.2005970</pub-id>
        <pub-id pub-id-type="pmid">29969450</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Hung J, Goodman A, Lopes S, Rangel G, Ravel D, Costa F, et al. Applying faster R-CNN for object detection on malaria images. In: 2017 IEEE Conference on computer vision and pattern recognition workshops (CVPRW). IEEE; 2017. p. 808–13.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Haghverdi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Büttner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wolf</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Buettner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>Diffusion pseudotime robustly reconstructs lineage branching</article-title>
        <source>Nat Methods</source>
        <year>2016</year>
        <volume>13</volume>
        <issue>10</issue>
        <fpage>845</fpage>
        <lpage>848</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3971</pub-id>
        <pub-id pub-id-type="pmid">27571553</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Lin T-Y, Dollár P, Girshick R, He K, Hariharan B, Belongie S. Feature pyramid networks for object detection [Internet]. arXiv [cs.CV]. 2016. Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1612.03144">http://arxiv.org/abs/1612.03144</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">He K, Gkioxari G, Dollar P, Girshick R. Mask R-CNN. IEEE Trans Pattern Anal Mach Intell [Internet]. 2018; Available from: 10.1109/TPAMI.2018.2844175.</mixed-citation>
    </ref>
  </ref-list>
</back>
