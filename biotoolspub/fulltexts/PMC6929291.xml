<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6929291</article-id>
    <article-id pub-id-type="publisher-id">3327</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3327-y</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Antimicrobial peptide identification using multi-scale convolutional network</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Su</surname>
          <given-names>Xin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Xu</surname>
          <given-names>Jing</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yin</surname>
          <given-names>Yanbin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Quan</surname>
          <given-names>Xiongwen</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8498-3451</contrib-id>
        <name>
          <surname>Zhang</surname>
          <given-names>Han</given-names>
        </name>
        <address>
          <email>zhanghan@nankai.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9878 7032</institution-id><institution-id institution-id-type="GRID">grid.216938.7</institution-id><institution>College of Artificial Intelligence, </institution><institution>Nankai University, </institution></institution-wrap>Tongyan Road, Tianjin, 300350 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9878 7032</institution-id><institution-id institution-id-type="GRID">grid.216938.7</institution-id><institution>College of Computer Science, </institution><institution>Nankai University, </institution></institution-wrap>Tongyan Road, Tianjin, 300350 China </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1937 0060</institution-id><institution-id institution-id-type="GRID">grid.24434.35</institution-id><institution>Nebraska Food for Health Center, Department of Food Science and Technology, </institution><institution>University of Nebraska-Lincoln, </institution></institution-wrap>1400 R Street, Lincoln, NE 68588 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>730</elocation-id>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>16</day>
        <month>12</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s). 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Antibiotic resistance has become an increasingly serious problem in the past decades. As an alternative choice, antimicrobial peptides (AMPs) have attracted lots of attention. To identify new AMPs, machine learning methods have been commonly used. More recently, some deep learning methods have also been applied to this problem.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we designed a deep learning model to identify AMP sequences. We employed the embedding layer and the multi-scale convolutional network in our model. The multi-scale convolutional network, which contains multiple convolutional layers of varying filter lengths, could utilize all latent features captured by the multiple convolutional layers. To further improve the performance, we also incorporated additional information into the designed model and proposed a fusion model. Results showed that our model outperforms the state-of-the-art models on two AMP datasets and the Antimicrobial Peptide Database (APD)3 benchmark dataset. The fusion model also outperforms the state-of-the-art model on an anti-inflammatory peptides (AIPs) dataset at the accuracy.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">Multi-scale convolutional network is a novel addition to existing deep neural network (DNN) models. The proposed DNN model and the modified fusion model outperform the state-of-the-art models for new AMP discovery. The source code and data are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhanglabNKU/APIN">https://github.com/zhanglabNKU/APIN</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Multi-scale convolutional network</kwd>
      <kwd>Antimicrobial peptide</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Fusion model</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id>
            <institution>National Natural Science Foundation of China</institution>
          </institution-wrap>
        </funding-source>
        <award-id>61973174</award-id>
        <principal-award-recipient>
          <name>
            <surname>Zhang</surname>
            <given-names>Han</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par31">In recent years, antimicrobial peptides (AMPs) have attracted lots of attention due to the well-known antibiotic resistance problem. AMPs are polypeptides shorter than 100 amino acids, which are an important part of host defense systems of animals and plants [<xref ref-type="bibr" rid="CR1">1</xref>]. AMPs have antimicrobial activity under specific circumstances since the difference between microbial and host cells in biochemical and biophysical provides a basis for selective toxicity of AMPs [<xref ref-type="bibr" rid="CR2">2</xref>]. AMPs exhibit many advantages including fast killing, low toxicity, and broad range of activity [<xref ref-type="bibr" rid="CR3">3</xref>]. Besides, AMPs show a lower likelihood for antimicrobial resistance compared to many antibiotics [<xref ref-type="bibr" rid="CR4">4</xref>]. Due to the advantages of AMPs, they have been a popular research area of bioinformatics.</p>
    <p id="Par32">To identify AMPs, many computational tools are proposed such as CAMP [<xref ref-type="bibr" rid="CR5">5</xref>], CAMPR3 [<xref ref-type="bibr" rid="CR6">6</xref>], ADAM [<xref ref-type="bibr" rid="CR7">7</xref>], AMPer [<xref ref-type="bibr" rid="CR8">8</xref>], AntiBP [<xref ref-type="bibr" rid="CR9">9</xref>], AntiBP2 [<xref ref-type="bibr" rid="CR10">10</xref>], AVPpred [<xref ref-type="bibr" rid="CR11">11</xref>], iAMP-2 L [<xref ref-type="bibr" rid="CR12">12</xref>], EFC-FCBF [<xref ref-type="bibr" rid="CR13">13</xref>], classAMP [<xref ref-type="bibr" rid="CR14">14</xref>] and web-based antimicrobial peptide prediction tools [<xref ref-type="bibr" rid="CR15">15</xref>]. Many of these tools applied various machine learning methods. For example, support vector machine (SVM), random forest (RF), and artificial neural network (ANN) were employed in CAMP. To apply machine learning methods, feature engineering is a necessary step. The most popular features for AMPs are amino acid composition. For example, AntiBP employed basic amino acid counts over the full peptide as the features. The pseudo-amino acid composition (PseAAC) method is also applied in some methods [<xref ref-type="bibr" rid="CR16">16</xref>].</p>
    <p id="Par33">For machine learning methods, feature construction of protein sequences relies heavily on domain knowledges. To avoid the complexity of feature engineering and remove the burden of feature construction, many deep learning models have been applied to various problems in bioinformatics [<xref ref-type="bibr" rid="CR17">17</xref>] such as protein structure prediction [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>], protein classification [<xref ref-type="bibr" rid="CR20">20</xref>], biomedical imaging recognition [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR22">22</xref>]. To apply deep learning to the problem of AMP identification, a deep neural network (DNN) model was proposed [<xref ref-type="bibr" rid="CR23">23</xref>]. This model employed a convolutional layer [<xref ref-type="bibr" rid="CR24">24</xref>] and a recurrent layer, which can capture latent features of protein sequences, so it was shown to outperform the state-of-the-art models in AMP identification. Although this model is great, there is still room for improvement. For example, a long short-term memory (LSTM) layer [<xref ref-type="bibr" rid="CR25">25</xref>] was employed due to its ability to recognize and forget gap-separated patterns in this model. However, this architecture of DNN model is usually applied in natural language processing (NLP) [<xref ref-type="bibr" rid="CR26">26</xref>, <xref ref-type="bibr" rid="CR27">27</xref>], and is not appropriate for AMP identification in our experiments which is listed in Table <xref rid="Tab3" ref-type="table">3</xref> for comparison of modified models.</p>
    <p id="Par34">In this paper, we have designed a multi-scale convolutional network which contains multiple convolutional layers of different filter lengths, and proposed a DNN model based on the multi-scale convolutional network to improve the performance of AMP identification. In the proposed model, we have employed an embedding layer and a multi-scale convolutional network. The embedding layer can capture semantic information of amino acids by converting each of them into a numerical vector. The distance between vectors can represent the relation between the corresponding amino acids. Many word embedding models, such as word2vector [<xref ref-type="bibr" rid="CR28">28</xref>] and gloves [<xref ref-type="bibr" rid="CR29">29</xref>], are widely used in text recognition tasks. The choice of a multi-scale convolutional network is due to its ability to capture latent features of motifs. Since a multi-scale convolutional network contains multiple convolutional layers, it can make use of all latent features captured by their convolutional layers. Because of the ability of the multi-scale convolutional network to capture multi-scale motifs, the proposed model outperforms the state-of-the-art DNN model [<xref ref-type="bibr" rid="CR23">23</xref>] in AMP identification. To further improve the performance, we also incorporated additional information into the proposed model and proposed a fusion model.</p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <sec id="Sec3">
      <title>Dataset</title>
      <p id="Par35">We adopt four datasets in this paper. The first dataset we used is made by Veltri et al. (2018) [<xref ref-type="bibr" rid="CR23">23</xref>], containing 1778 AMPs constructed from the APD vr.3 database [<xref ref-type="bibr" rid="CR30">30</xref>] and 1778 non-AMPs constructed from UniProt [<xref ref-type="bibr" rid="CR31">31</xref>]. The dataset is split by Veltri et al. (2018) [<xref ref-type="bibr" rid="CR23">23</xref>] into a training set, a tuning set and a test set and the number of AMP sequences are 712, 354, and 712 respectively. More detailed information of this dataset can be found in Veltri et al. (2018) [<xref ref-type="bibr" rid="CR23">23</xref>]. In the rest of the paper, this dataset is named DAMP dataset. The second dataset is taken from AntiBP2 [<xref ref-type="bibr" rid="CR10">10</xref>], which has 1998 peptide sequences. AMPs have ∼75% overlap with DAMP dataset and non-AMPs have no overlap with it. The third dataset is an anti-inflammatory peptide (AIP) dataset, which is from AIPpred [<xref ref-type="bibr" rid="CR32">32</xref>]. This dataset contains 1258 AIPs and 1887 non-AIPs in training set, 420 AIPs and 629 non-AIPs in test set. The last dataset is from the paper [<xref ref-type="bibr" rid="CR15">15</xref>], which is composed of 10,278 sequences. Table <xref rid="Tab1" ref-type="table">1</xref> summarizes the four datasets.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Dataset summary</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>DAMP dataset [<xref ref-type="bibr" rid="CR23">23</xref>]</th><th>AntiBP2 dataset</th><th>AIP dataset</th><th>APD3 dataset [<xref ref-type="bibr" rid="CR15">15</xref>]</th></tr></thead><tbody><tr><td>Positive samples</td><td>1778</td><td>999</td><td>1678</td><td>1713</td></tr><tr><td>Negative samples</td><td>1778</td><td>999</td><td>2516</td><td>8565</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec4">
      <title>Setup and runtime performance</title>
      <p id="Par37">The proposed DNN model is constructed using Keras [<xref ref-type="bibr" rid="CR33">33</xref>], a Python neural network library, with a CPU-based TensorFlow back-end [<xref ref-type="bibr" rid="CR34">34</xref>]. The weights in our model of 11 are initialized with the default value of Keras. The optimizer is RMSProp whose learning rate is set to 0.0002, and the loss function is ‘binary_crossentropy’. Besides, the batch size is set to 32. Experiments are conducted on a computer with Intel Xeon E3-1226v3 CPU and the RAM of this computer is 8GB. The training of each epoch takes about 56 s and the prediction of a peptide sequence takes 6 ms on average.</p>
    </sec>
    <sec id="Sec5">
      <title>Model tuning</title>
      <p id="Par38">First, we want to know how the model performs with only one convolutional layer. We replaced the multi-scale convolutional network with the single convolutional layer. The performance of the modified model with different filter size is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. As shown in this figure, the accuracy (ACC) [<xref ref-type="bibr" rid="CR35">35</xref>] of the modified model is under 89% when this model only contains one convolutional layer whose filter length is short. As the filter length increases, the ACC also increases very fast. The performance of the length between 6 and 20 is similar as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The results of this experiment show that any single convolutional layer whose filter length is shorter than 7 could not capture enough information of a peptide sequence in AMP identification, and the convolutional layers with filter lengths longer than 7 have similar performance in this problem.
<fig id="Fig1"><label>Fig. 1</label><caption><p>10-fold cross validation performance of the model with single convolutional layer. We replaced the multi-convolutional network with a simple convolutional layer. This figure shows how the modified model performs when the filter length of the convolutional layer changes</p></caption><graphic xlink:href="12859_2019_3327_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par39">Then we want to find the best parameter N in our multi-scale model. Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the performance of the proposed model with different parameter N. As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, when N is small, the performance of this multi-scale model is similar to the model with one convolutional layer. Conversely, when N gets larger, the multi-scale model performs better. When <italic>N</italic> = 14, ACC score is the highest with low fluctuation. We finally choose N = 14 in the proposed model.
<fig id="Fig2"><label>Fig. 2</label><caption><p>10-fold cross validation performance of the model with different parameter N</p></caption><graphic xlink:href="12859_2019_3327_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
    <sec id="Sec6">
      <title>Comparison with current main methods</title>
      <p id="Par40">To evaluate the proposed multi-scale DNN model, this model is compared with the state-of-the-art models including the traditional machine learning models and the existing DNN model. Table <xref rid="Tab2" ref-type="table">2</xref> shows comparison results of the state-of-the-art model. The results show that the proposed model outperforms the existing DNN in all evaluation metrics except sensitivity (SENS). To be specific, the accuracy of the proposed model is about 92.4%, which is 1.3% higher than the existing DNN model, and the specificity (SPEC) is about 94%, which is 1.51% higher than the existing DNN model. Although the highest SENS is achieved by the RF model, the performance of the proposed model is better than the performance of the existing DNN model. The fusion model which makes use of amino acid composition (AAC) [<xref ref-type="bibr" rid="CR32">32</xref>] and dipeptide composition (DPC) [<xref ref-type="bibr" rid="CR32">32</xref>] further improves the performance. ACC of the fusion model reaches 92.55%.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Comparison with the state-of-the-art methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>SENS (%)</th><th>SPEC (%)</th><th>ACC (%)</th><th>MCC</th><th>auROC (%)</th><th><italic>P</italic> value</th></tr></thead><tbody><tr><td>AntiBP2</td><td>87.91</td><td>90.8</td><td>89.37</td><td>0.7876</td><td>89.36</td><td>&lt; 0.001</td></tr><tr><td>CAMP-ANN</td><td>82.98</td><td>85.09</td><td>84.04</td><td>0.6809</td><td>84.06</td><td>&lt; 0.001</td></tr><tr><td>CAMP-DA</td><td>87.08</td><td>80.76</td><td>83.92</td><td>0.6797</td><td>89.97</td><td>&lt; 0.001</td></tr><tr><td>CAMP-RF</td><td>92.7</td><td>82.44</td><td>87.57</td><td>0.7554</td><td>93.63</td><td>&lt; 0.001</td></tr><tr><td>CAMP-SVM</td><td>88.9</td><td>79.92</td><td>84.41</td><td>0.691</td><td>90.63</td><td>&lt; 0.001</td></tr><tr><td>iAMP-2 L</td><td>83.99</td><td>85.86</td><td>84.9</td><td>0.6983</td><td>84.9</td><td>&lt; 0.001</td></tr><tr><td>iAMPpred</td><td>89.33</td><td>87.22</td><td>88.27</td><td>0.7656</td><td>94.44</td><td>&lt; 0.001</td></tr><tr><td>gkmSVM</td><td>88.34</td><td>90.59</td><td>89.46</td><td>0.7895</td><td>94.98</td><td>&lt; 0.001</td></tr><tr><td>DNN</td><td>89.89</td><td>92.13</td><td>91.01</td><td>0.8204</td><td>96.48</td><td>&lt; 0.001</td></tr><tr><td>proposed model</td><td>91.01</td><td>93.64</td><td>92.41</td><td>0.8486</td><td>97.23</td><td>&lt; 0.001</td></tr><tr><td>fusion model with DNN</td><td>88.48</td><td>93.26</td><td>90.87</td><td>0.8183</td><td>96.24</td><td>&lt; 0.001</td></tr><tr><td>proposed fusion model</td><td>89.89</td><td>94.96</td><td>92.55</td><td>0.8523</td><td>97.3</td><td>&lt; 0.001</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec7">
      <title>Modification comparison</title>
      <p id="Par41">We modified the propose model and conducted a modification comparison by replacing or removing some components in the proposed model in order to find out the vital elements of the success of the proposed model and discover the best architecture of DNN model in AMP identification.</p>
      <p id="Par42">To be specific, we have tested the models in which we replaced the embedding layer with one-hot encoding, or replaced multi-scale convolutional network with simple convolutional layer or replaced the pooling1 layers with LSTM layers. Besides, we also have tested models without pooling2 layer or with additional fully connected (FC) layers. The results of modification comparison are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. From the results, we find that the multi-convolutional network is the most important part in our model, and the ACC performance of the model without this component drops to 90.44%. Also, the embedding layer is significant in our model. When we run the model without embedding layer, the ACC performance drops to 91.43%. Additionally, using LSTM to replace pooling1 doesn’t improve the performance of AMP identification and increases runtime. This result implies that LSTM is not a good choice for AMP identification in the proposed model. We also tested a model in which we replaced the pooling1 layers with Gated Recurrent Unit (GRU) layers and its accuracy is 91.43%. Because the structure of GRU is similar to LSTM, the result doesn’t change obviously compared to replacing pooling1 layers with LSTM layers. In addition, the results also show that additional fully connected layer or removing pooling2 would not improve the performance.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison of modified models</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>SENS (%)</th><th>SPEC (%)</th><th>ACC (%)</th><th>MCC</th><th>auROC (%)</th></tr></thead><tbody><tr><td>Replacing embedding layer</td><td>89.61</td><td>93.26</td><td>91.43</td><td>0.8282</td><td>96.75</td></tr><tr><td>Replacing multi-scale convolutional network</td><td>89.75</td><td>91.15</td><td>90.44</td><td>0.8091</td><td>96.08</td></tr><tr><td>Replacing pooling1 with LSTM</td><td>89.75</td><td>93.25</td><td>91.5</td><td>0.8305</td><td>96.27</td></tr><tr><td>Without pooling2</td><td>91.15</td><td>92.56</td><td>91.85</td><td>0.8371</td><td>96.3</td></tr><tr><td>Additional FC layers</td><td>90.31</td><td>93.68</td><td>91.99</td><td>0.8403</td><td>97.09</td></tr><tr><td>proposed model</td><td>91.01</td><td>93.64</td><td>92.41</td><td>0.8486</td><td>97.23</td></tr></tbody></table></table-wrap></p>
      <p id="Par43">We also analyzed the training time of each modified model. The results are shown in Table <xref rid="Tab4" ref-type="table">4</xref>. The results show that replacing the embedding layer or multi-scale convolutional network reduces the training time but the accuracy decreases. Adding LSTM into the proposed model not only increases the training time but also decreases the accuracy. Besides, adding FC layers or removing pooling2 doesn’t apparently affect runtime.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Training time of modified models</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Time for training on each epoch(s)</th></tr></thead><tbody><tr><td>Replacing embedding layer</td><td>13.69</td></tr><tr><td>Replacing multi-scale convolutional network</td><td>13.95</td></tr><tr><td>Replacing pooling1 with LSTM</td><td>121.4</td></tr><tr><td>Without pooling2</td><td>56.06</td></tr><tr><td>Additional dense layers</td><td>58.45</td></tr><tr><td>proposed model</td><td>56.36</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec8">
      <title>Model performance on other datasets</title>
      <p id="Par44">To find out how the proposed model performs on other datasets, we applied our model to AntiBP2 dataset, AIP dataset and the APD3 benchmark dataset from paper [<xref ref-type="bibr" rid="CR15">15</xref>].</p>
      <p id="Par45">We used 10-fold cross validation test on AntiBP2 dataset to compare the proposed model with state-of-the-art models. Table <xref rid="Tab5" ref-type="table">5</xref> shows that the proposed DNN also outperforms other state-of-the-art models on AntiBP2 dataset. The accuracy of this dataset is 93.38%.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Comparison of the state-of-the-art methods on AntiBP2 dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>ACC (%)</th><th>MCC</th></tr></thead><tbody><tr><td>CAMP-ANN</td><td>81.03</td><td>0.624</td></tr><tr><td>CAMP-DA</td><td>84.28</td><td>0.69</td></tr><tr><td>CAMP-RF</td><td>87.09</td><td>0.752</td></tr><tr><td>CAMP-SVM</td><td>86.69</td><td>0.739</td></tr><tr><td>iAMP-2 L</td><td>86.34</td><td>0.735</td></tr><tr><td>iAMPpred</td><td>92.84</td><td>0.858</td></tr><tr><td>AntiBP2</td><td>91.64</td><td>0.831</td></tr><tr><td>DNN</td><td>92.95</td><td>0.86</td></tr><tr><td>proposed model</td><td>93.38</td><td>0.862</td></tr></tbody></table></table-wrap></p>
      <p id="Par46">We compared the proposed model with the existing DNN [<xref ref-type="bibr" rid="CR23">23</xref>] and the AIPpred model which is state-of-the-art on AIP dataset. The result is shown in Table <xref rid="Tab6" ref-type="table">6</xref>. From this table, we can see that the accuracy of the proposed model on this dataset is 73.02% (0.38% lower than AIPpred). However, the proposed model performs much better than the existing DNN [<xref ref-type="bibr" rid="CR23">23</xref>]. When using AAC, DPC and some other features, the proposed fusion model achieves a better performance than AIPpred (ACC is 0.44% higher than AIPpred). This experiment implies that the proposed model has a good applicability and could also be applied to problems of other peptide sequence identification.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Comparison of the state-of-the-art methods on AIP dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>SENS (%)</th><th>SPEC (%)</th><th>ACC (%)</th><th>MCC</th><th>auROC (%)</th><th><italic>P</italic> value</th></tr></thead><tbody><tr><td>DNN</td><td>59.05</td><td>73.61</td><td>67.78</td><td>0.3273</td><td>71.12</td><td>&lt; 0.001</td></tr><tr><td>proposed model</td><td>55.24</td><td>84.9</td><td>73.02</td><td>0.4245</td><td>76.8</td><td>&lt; 0.001</td></tr><tr><td>AIPpred</td><td>75.8</td><td>71.11</td><td>73.4</td><td>0.46</td><td>80.1</td><td>&lt; 0.001</td></tr><tr><td>fusion model with DNN</td><td>51.67</td><td>79.81</td><td>68.54</td><td>0.3285</td><td>71.23</td><td>&lt; 0.001</td></tr><tr><td>proposed fusion model</td><td>60</td><td>83.15</td><td>73.88</td><td>0.4459</td><td>78.34</td><td>&lt; 0.001</td></tr></tbody></table></table-wrap></p>
      <p id="Par47">We also tested these methods on the APD3 benchmark dataset. The prediction result is shown in Table <xref rid="Tab7" ref-type="table">7</xref>. The performance metrics indicate that our proposed method and proposed fusion method perform better than other methods. Besides, we used DeLong’s test to get differences between our two proposed methods and other methods with the area under receiver-operating curve (auROC) analysis. The result is shown in Table <xref rid="Tab8" ref-type="table">8</xref>. It also shows that our two proposed methods over-perform other methods.
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Comparison of methods on APD3 dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>SENS (%)</th><th>SPEC (%)</th><th>PREC (%)</th><th>BalACC (%)</th><th>ACC (%)</th><th>MCC</th></tr></thead><tbody><tr><td>CAMP-ANN</td><td>83.30</td><td>83.36</td><td>50.04</td><td>83.33</td><td>83.35</td><td>0.5549</td></tr><tr><td>CAMP-DA</td><td>88.09</td><td>81.25</td><td>48.44</td><td>84.67</td><td>82.39</td><td>0.5623</td></tr><tr><td>CAMP-RF</td><td>94.80</td><td>83.44</td><td>53.39</td><td>89.12</td><td>85.34</td><td>0.6388</td></tr><tr><td>CAMP-SVM</td><td>90.54</td><td>81.63</td><td>49.65</td><td>86.09</td><td>83.12</td><td>0.5848</td></tr><tr><td>gkmSVM</td><td>–</td><td>–</td><td>–</td><td>–</td><td>–</td><td>–</td></tr><tr><td>iAMP-2 L</td><td>88.32</td><td>86.12</td><td>56.00</td><td>87.22</td><td>86.49</td><td>0.6302</td></tr><tr><td>iAMPpred</td><td>93.46</td><td>79.02</td><td>47.12</td><td>86.24</td><td>81.43</td><td>0.5742</td></tr><tr><td>DNN</td><td>96.96</td><td>89.62</td><td>65.14</td><td>93.29</td><td>90.84</td><td>0.7471</td></tr><tr><td>proposed model</td><td>97.90</td><td>90.90</td><td>68.28</td><td>94.40</td><td>92.07</td><td>0.7761</td></tr><tr><td>proposed fusion model</td><td>98.25</td><td>91.00</td><td>68.58</td><td>94.62</td><td>92.21</td><td>0.7802</td></tr></tbody></table><table-wrap-foot><p>Note: the mark’—’ means that the result is not available. In this experiment, ‘gkmSVM’ method couldn’t be run successfully because the kernel requirement isn’t satisfied</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab8"><label>Table 8</label><caption><p>Comparison of auROC using DeLong’s test on APD3 dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method 1</th><th>Method 2</th><th>auROC 1</th><th>auROC 2</th><th>Difference</th><th>P value</th></tr></thead><tbody><tr><td>proposed model</td><td>CAMP-DA</td><td>0.9892</td><td>0.9069</td><td>0.0823</td><td>&lt;  0.0001</td></tr><tr><td>proposed model</td><td>CAMP-RF</td><td>0.9892</td><td>0.9528</td><td>0.0365</td><td>&lt;  0.0001</td></tr><tr><td>proposed model</td><td>CAMP-SVM</td><td>0.9892</td><td>0.9202</td><td>0.0690</td><td>&lt;  0.0001</td></tr><tr><td>proposed model</td><td>gkmSVM</td><td>0.9892</td><td>–</td><td>–</td><td>NA</td></tr><tr><td>proposed model</td><td>iAMP-2 L</td><td>0.9892</td><td>0.8722</td><td>0.1170</td><td>&lt;  0.0001</td></tr><tr><td>proposed model</td><td>iAMPpred</td><td>0.9892</td><td>0.9466</td><td>0.0427</td><td>&lt;  0.0001</td></tr><tr><td>proposed model</td><td>DNN</td><td>0.9892</td><td>0.9802</td><td>0.0091</td><td>&lt;  0.0001</td></tr><tr><td>proposed fusion model</td><td>CAMP-DA</td><td>0.9918</td><td>0.9069</td><td>0.0849</td><td>&lt;  0.0001</td></tr><tr><td>proposed fusion model</td><td>CAMP-RF</td><td>0.9918</td><td>0.9528</td><td>0.0391</td><td>&lt;  0.0001</td></tr><tr><td>proposed fusion model</td><td>CAMP-SVM</td><td>0.9918</td><td>0.9202</td><td>0.0716</td><td>&lt;  0.0001</td></tr><tr><td>proposed fusion model</td><td>gkmSVM</td><td>0.9918</td><td>–</td><td>–</td><td>NA</td></tr><tr><td>proposed fusion model</td><td>iAMP-2 L</td><td>0.9918</td><td>0.8722</td><td>0.1196</td><td>&lt;  0.0001</td></tr><tr><td>proposed fusion model</td><td>iAMPpred</td><td>0.9918</td><td>0.9466</td><td>0.0453</td><td>&lt;  0.0001</td></tr><tr><td>proposed fusion model</td><td>DNN</td><td>0.9918</td><td>0.9802</td><td>0.0117</td><td>&lt;  0.0001</td></tr><tr><td>proposed fusion model</td><td>proposed model</td><td>0.9918</td><td>0.9892</td><td>0.0026</td><td>&lt;  0.0001</td></tr></tbody></table><table-wrap-foot><p>Note: the mark’—’ means that the result is not available. In this experiment, ‘gkmSVM’ method couldn’t be run successfully because the kernel requirement isn’t satisfied</p></table-wrap-foot></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Discussion</title>
    <p id="Par48">We have designed a multi-scale convolutional DNN model to identify AMP sequences. In terms of accuracy, it overperforms other methods on three datasets. Although the proposed model and the proposed fusion model have no obvious advantage over AIPpred, the former models use less information from sequences and they’re easily to use. The propose model takes a little longer time than some modified model but the runtime is acceptable and the prediction accuracy has significant improvements.</p>
  </sec>
  <sec id="Sec10">
    <title>Conclusion</title>
    <p id="Par49">To identify AMPs, we have proposed a DNN model based on the multi-scale convolutional layers. The proposed DNN model mainly employs the embedding layer and the multi-scale convolutional network. Through the embedding layer, each amino acid in a peptide sequence is converted into an embedding vector. The multi-scale convolutional network can capture the local features, and its max pooling layers and convolutional layers of different filter lengths can help with the feature selection. This model focusing on the local context could improve the performance of AMP identification. Furthermore, we have incorporated additional information into the proposed model and developed a fusion model. Compared with the state-of-the-art models, our proposed model achieved better performance. Through the model modification comparisons, we found that the model without multi-scale convolutional network achieved the worst results, which means the multi-scale convolutional network is the most important part in our model. We also applied the proposed model and proposed fusion model to other datasets including an AMP dataset and an AIP dataset and the APD3 benchmark dataset. The results show that the fusion model could achieve a better performance and our proposed model is applicable for other peptide identification.</p>
  </sec>
  <sec id="Sec11">
    <title>Methods</title>
    <sec id="Sec12">
      <title>Structure of our proposed DNN</title>
      <p id="Par50">First, we tested and analyzed the state-of-the-art DNN model which contains a LSTM layer. The LSTM layer applied to AMP identification focuses on the whole sequence without caring about short motifs. However, it is believed that proteins with similar functions may share some short motifs [<xref ref-type="bibr" rid="CR32">32</xref>]. This means that we can predict AMPs based on these motifs shared with known AMPs.</p>
      <p id="Par51">With this mind, we designed a multi-scale convolutional network, and then proposed a new DNN model based on this network. The proposed DNN model mainly employs a multi-scale convolutional network containing many convolutional layers of different filter lengths. Since each convolutional layer can capture motifs of a fixed length, convolutional layers of different filter lengths can detect motifs of different lengths. The structure of our proposed model is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, which shows that the proposed model mainly contains an Embedding module, a Convolutional module, a Pooling module and a Fully Connection module. In the proposed model, we used dropout and set the parameter 0.2 to prevent overfitting.
<fig id="Fig3"><label>Fig. 3</label><caption><p>The structure of the proposed model. The proposed model mainly uses embedding layer and convolutional layers. All sequences are encoded into numerical vectors of length 200 and are fed into the embedding layer. Each embedding vector dimension is 128. Then the outputs of embedding layer are fed into N convolutional layers. Each convolutional layer uses 64 filter kernels. These outputs are connected to feed into a max pooling layer and outputs of the pooling layers are concatenated to fed into another max pooling layer. Finally the output will be fed into a fully connection layer and passed through a sigmoid function. The final output is in range [0,1] as the prediction of the input sequence</p></caption><graphic xlink:href="12859_2019_3327_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par52">As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, the sequence data has to be converted to be fed into the model. A peptide sequence is converted into a numerical vector of length 200, which is larger than the length of the longest sequence. We assigned an integer within 20 to each one of the 20 basic amino acids. The sequence shorter than 200 will be padded with the number 0 to obtain a fixed vector length 200. The padded 0 s will be ignored by the model during later data processing. Then the encoded data will be fed into the embedding layer that can convert the data with discrete representation into a word vector of a fixed size. That they have a dense representation and can represent an abstract symbol (e.g. a word or an amino acid) with a fixed vector can help reduce dimension. Besides, the distance between two word vectors can represent the relation between two symbols. Compared to the one-hot encoding, the word vector is more compact. As a result, the embedding layer will output a sequence matrix given an amino acid sequence. The matrix has a fixed-dimension of 128 × 200 in our model. The embedding layer will be trained with the whole model.</p>
      <p id="Par53">In the Convolutional module, we employed a multi-scale convolutional network containing N convolutional layers of different filter lengths. A filter will be activated when a matching motif is detected. An amino acid sequence embedding presentation is given as
<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ X=\left[{v}_1,{v}_2,\dots, {v}_{200}\right] $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="[" separators=",,,"><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>200</mml:mn></mml:msub></mml:mfenced></mml:math><graphic xlink:href="12859_2019_3327_Article_Equa.gif" position="anchor"/></alternatives></disp-formula>where <italic>v</italic><sub><italic>i</italic></sub>(∈<italic>R</italic><sup>128</sup>) is the embedding vector of <italic>i</italic>-th amino acid. To extract local contexts, the output of each convolutional layer is as
<disp-formula id="Equb"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {y}_i^{(f)}=\delta \left({w}^f{x}_i+{b}^{(f)}\right),f=1,2,3,\dots, 64 $$\end{document}</tex-math><mml:math id="M4" display="block"><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mfenced close=")" open="("><mml:mi>f</mml:mi></mml:mfenced></mml:msubsup><mml:mo>=</mml:mo><mml:mi>δ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>f</mml:mi></mml:msup><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mfenced close=")" open="("><mml:mi>f</mml:mi></mml:mfenced></mml:msup></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>64</mml:mn></mml:math><graphic xlink:href="12859_2019_3327_Article_Equb.gif" position="anchor"/></alternatives></disp-formula>where δ(∗) means a non-linear activation function which is Rectified Linear Unit (ReLU) [<xref ref-type="bibr" rid="CR36">36</xref>] in our model, <italic>w</italic><sup>(<italic>f</italic>)</sup> and <italic>b</italic><sup>(<italic>f</italic>)</sup> are weight and bias of <italic>f</italic>-th filter, and <italic>x</italic><sub><italic>i</italic></sub> is <italic>i</italic>-th part which is to be convolved. <italic>x</italic><sub><italic>i</italic></sub> is as [<italic>v</italic><sub><italic>i</italic></sub>, <italic>v</italic><sub><italic>i</italic> + 1</sub>, …, <italic>v</italic><sub><italic>i</italic> + <italic>l</italic></sub>] where <italic>l</italic> is the filter length of this convolutional layer. The Convolutional module takes the most important part in recognizing the AMPs by the short motifs which the convolutional layers can detect. A difference between convolutional layers in the multi-scale convolutional network is the filter lengths. Due to the filters of different lengths, each of the convolutional layers screen motifs of its length and then the results of all convolutional layers are different. To be specific, the filter lengths of all N convolutional layers are 2, 4, 6, ..., 2 N.</p>
      <p id="Par54">Each convolutional layer’s output is fed into a max pooling layer. The pooling layer helps reduce over-fitting. Besides, the max pooling is similar as feature selection, which selects the feature with max value. Next, to make use of motifs of different size, all pooling layers’ outputs are concatenated. In other words, the results of all different convolutional layers are concatenated. Then the concatenated layer’s output is fed into another max pooling layer. Finally, the output of pooling layer is fed into a fully connected layer to get the final prediction. The final dense layer uses a sigmoid function and its output is in the range [0,1]. The final output greater than 0.5 means the input sequence is an AMP, otherwise, a non-AMP.</p>
      <p id="Par55">As described above, recurrent neural network (RNN) or LSTM were not used in the proposed model. In our experiments, adding LSTM or RNN did not improve the performance of the proposed model significantly. The results of experiments are discussed in Results section. The features of motifs which convolutional layers detect are used for our identification of new AMPs.</p>
    </sec>
    <sec id="Sec13">
      <title>Model tuning and metrics</title>
      <p id="Par56">We evaluate our proposed model based on sensitivity (SENS), specificity (SPEC), precision (PREC), balanced accuracy (BalACC), accuracy (ACC) [<xref ref-type="bibr" rid="CR35">35</xref>] and Matthew’s Correlation Coefficient (MCC) [<xref ref-type="bibr" rid="CR37">37</xref>]. All of them are based on the number of true positive (TP), true negative (TN), false positive (FP), false negative (FN). They are defined as
<disp-formula id="Equc"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ SENS=\frac{TP}{\left( TP+ FN\right)}\times 100\% $$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mtext mathvariant="italic">SENS</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="italic">TP</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfenced></mml:mfrac><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:math><graphic xlink:href="12859_2019_3327_Article_Equc.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equd"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ SPEC=\frac{TN}{\left( TN+ FP\right)}\times 100\% $$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mtext mathvariant="italic">SPEC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="italic">TN</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mfenced></mml:mfrac><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:math><graphic xlink:href="12859_2019_3327_Article_Equd.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Eque"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ PREC=\frac{TP}{\left( TP+ FP\right)}\times 100\% $$\end{document}</tex-math><mml:math id="M10" display="block"><mml:mtext mathvariant="italic">PREC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="italic">TP</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mfenced></mml:mfrac><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:math><graphic xlink:href="12859_2019_3327_Article_Eque.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equf"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ BalACC=\frac{1}{2}\times \left(\frac{TP}{\left( TP+ FN\right)}+\frac{TN}{\left( TN+ FP\right)}\right)\times 100\% $$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mtext mathvariant="italic">BalACC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>×</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac><mml:mi mathvariant="italic">TP</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfenced></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mi mathvariant="italic">TN</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mfenced></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:math><graphic xlink:href="12859_2019_3327_Article_Equf.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equg"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ ACC=\frac{TP+ TN}{\left( TP+ TN+ FP+ FN\right)}\times 100\% $$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mi mathvariant="italic">ACC</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfenced></mml:mfrac><mml:mo>×</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo></mml:math><graphic xlink:href="12859_2019_3327_Article_Equg.gif" position="anchor"/></alternatives></disp-formula>
<disp-formula id="Equh"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ MCC=\frac{\left( TP\times TN\right)-\left( FP\times FN\right)}{\sqrt{\left( TP+ FN\right)\times \left( TN+ FP\right)\times \left( TP+ FP\right)\times \left( TN+ FN\right)}} $$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mi mathvariant="italic">MCC</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">FP</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:msqrt><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow></mml:mfenced><mml:mo>×</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msqrt></mml:mfrac></mml:math><graphic xlink:href="12859_2019_3327_Article_Equh.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par57">Besides, we also make use of auROC [<xref ref-type="bibr" rid="CR38">38</xref>]. The receiver operating curve (ROC) can represent the performance of a model by showing the TP rate as a function of FP rate. As the discrimination threshold changes, the TP rate and FP rate change. The auROC is the area under the ROC, which is in range [0.5,1]. 0.5 means random guess, while 1 means that the prediction is always correct.</p>
      <p id="Par58">To reflect different filter lengths bring about different prediction results, a 10-fold cross validation based on a single convolutional layer was conducted. Besides, to find out the best parameter N which is the number of convolutional layers in the multiscale convolutional network, we conducted a 10-fold cross validation to evaluate the parameter N. In this procedure, we merged the training set and tuning set and only took ACC into consideration to choose N. After N was chosen, we merged the training set and tuning set as a new training set to train the proposed model and then evaluated the proposed model and compared it with the state-of-the-art models based on the prediction results of the test set.</p>
    </sec>
    <sec id="Sec14">
      <title>Fusion model</title>
      <p id="Par59">To further improve the performance of the proposed model, redundant information [<xref ref-type="bibr" rid="CR39">39</xref>] of a peptide sequence is incorporated into the proposed model via a hybrid approach. We combined the proposed model with a fully connected network into a fusion model to capture multi-type features. Besides peptide sequences, amino acid composition (AAC) [<xref ref-type="bibr" rid="CR32">32</xref>] and dipeptide composition (DPC) [<xref ref-type="bibr" rid="CR32">32</xref>] are used in this fusion model. AAC is a vector which represents the fractions of 20 amino acid in its peptide sequence. It is defined as
<disp-formula id="Equi"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ AAC(i)=\frac{number\ of\ amino\ acid(i)}{Length\ of\ the\ peptide},i=1,2,3,\dots, 20 $$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mi mathvariant="italic">AAC</mml:mi><mml:mfenced close=")" open="("><mml:mi>i</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">number of amino acid</mml:mtext><mml:mfenced close=")" open="("><mml:mi>i</mml:mi></mml:mfenced></mml:mrow><mml:mtext mathvariant="italic">Length of the peptide</mml:mtext></mml:mfrac><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>20</mml:mn></mml:math><graphic xlink:href="12859_2019_3327_Article_Equi.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par60">DPC is a vector which represents the ratio of 400 possible dipeptides in a given sequence. It is calculated as
<disp-formula id="Equj"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ DPC(i)=\frac{\  number\ of\ dipeptide(i)}{Total\ number\ of\  all\  dipeptides},i=1,2,3,\dots, 400 $$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mi mathvariant="italic">DPC</mml:mi><mml:mfenced close=")" open="("><mml:mi>i</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mspace width="0.25em"/><mml:mtext mathvariant="italic">number of dipeptide</mml:mtext><mml:mfenced close=")" open="("><mml:mi>i</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">Total number of</mml:mtext><mml:mspace width="0.25em"/><mml:mi mathvariant="italic">all</mml:mi><mml:mspace width="0.25em"/><mml:mtext mathvariant="italic">dipeptides</mml:mtext></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>400</mml:mn></mml:math><graphic xlink:href="12859_2019_3327_Article_Equj.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par61">DPC has a fixed length of 400 which represents the 400 possible dipeptides.</p>
      <p id="Par62">Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the structure of the fusion model. There are two parts in this model. One is the proposed DNN model and another one is an additional fully connected network. The DPC and AAC are concatenated into a vector which has a length of 420. Then this vector is fed into a dense layer with 64 units and each unit use a sigmoid function. The output of this layer with the output of pooling layer in proposed model are concatenated. The concatenated vector is fed into a final dense layer with 1 unit. The final dense layer uses a sigmoid function and its output is in the range [0,1]. We only make use of DPC and AAC in this model, which are easy to obtain, and thus this model also can be applied to any sequence dataset.
<fig id="Fig4"><label>Fig. 4</label><caption><p>The structure of the proposed fusion model. There are two parts in the fusion model. The proposed structure is on the left. An additional fully connected network is on the right and this part make use of the DPC and AAC of peptide sequences. This network incorporates redundant information into the proposed model</p></caption><graphic xlink:href="12859_2019_3327_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AAC</term>
        <def>
          <p id="Par4">Amino acid composition</p>
        </def>
      </def-item>
      <def-item>
        <term>ACC</term>
        <def>
          <p id="Par5">Accuracy</p>
        </def>
      </def-item>
      <def-item>
        <term>AIPs</term>
        <def>
          <p id="Par6">Anti-inflammatory peptides</p>
        </def>
      </def-item>
      <def-item>
        <term>AMPs</term>
        <def>
          <p id="Par7">Antimicrobial peptides</p>
        </def>
      </def-item>
      <def-item>
        <term>ANN</term>
        <def>
          <p id="Par8">Artificial neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>APD</term>
        <def>
          <p id="Par9">The Antimicrobial Peptide Database</p>
        </def>
      </def-item>
      <def-item>
        <term>auROC</term>
        <def>
          <p id="Par10">The area under the ROC curve</p>
        </def>
      </def-item>
      <def-item>
        <term>BalACC</term>
        <def>
          <p id="Par11">Balanced accuracy</p>
        </def>
      </def-item>
      <def-item>
        <term>DNN</term>
        <def>
          <p id="Par12">Deep neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>DPC</term>
        <def>
          <p id="Par13">Dipeptide composition</p>
        </def>
      </def-item>
      <def-item>
        <term>FC</term>
        <def>
          <p id="Par14">Fully connected</p>
        </def>
      </def-item>
      <def-item>
        <term>FN</term>
        <def>
          <p id="Par15">False negative</p>
        </def>
      </def-item>
      <def-item>
        <term>FP</term>
        <def>
          <p id="Par16">False positive</p>
        </def>
      </def-item>
      <def-item>
        <term>GRU</term>
        <def>
          <p id="Par17">Gated recurrent unit</p>
        </def>
      </def-item>
      <def-item>
        <term>LSTM</term>
        <def>
          <p id="Par18">Long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>MCC</term>
        <def>
          <p id="Par19">Matthew’s correlation coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>NLP</term>
        <def>
          <p id="Par20">Natural language processing</p>
        </def>
      </def-item>
      <def-item>
        <term>PseAAC</term>
        <def>
          <p id="Par21">Pseudo-amino acid composition</p>
        </def>
      </def-item>
      <def-item>
        <term>ReLU</term>
        <def>
          <p id="Par22">Rectified linear unit</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p id="Par23">Random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>RNN</term>
        <def>
          <p id="Par24">Recurrent neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC</term>
        <def>
          <p id="Par25">Receiver-operating curve</p>
        </def>
      </def-item>
      <def-item>
        <term>SENS</term>
        <def>
          <p id="Par26">Sensitivity</p>
        </def>
      </def-item>
      <def-item>
        <term>SPEC</term>
        <def>
          <p id="Par27">Specificity</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p id="Par28">Support vector machine</p>
        </def>
      </def-item>
      <def-item>
        <term>TN</term>
        <def>
          <p id="Par29">True negative</p>
        </def>
      </def-item>
      <def-item>
        <term>TP</term>
        <def>
          <p id="Par30">True positive</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Xin Su and Jing Xu contributed equally to this work.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors would like to thank the editor and the anonymous reviewers for their comments and suggestions, which helped improve the manuscript greatly. This work was supported by computational facilities of College of Artificial Intelligence in Nankai University.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>HZ conceived the research. XS, JX, HZ, YY and XQ designed the research. XS and JX implemented the research. XS, HZ, YY wrote the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This research was funded by the National Natural Science Foundation of China grant No. 61973174. The funding body played no role in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The AMP dataset described in Dataset part could be downloaded from <ext-link ext-link-type="uri" xlink:href="http://www.dveltri.com/ascan/v2/ascan.html">http://www.dveltri.com/ascan/v2/ascan.html</ext-link>. The AntiBP2 dataset could be downloaded from <ext-link ext-link-type="uri" xlink:href="http://crdd.osdd.net/raghava/antibp2/">http://crdd.osdd.net/raghava/antibp2/</ext-link>. The AIP dataset could be downloaded from <ext-link ext-link-type="uri" xlink:href="http://www.thegleelab.org/AIPpred/">http://www.thegleelab.org/AIPpred/</ext-link>. The APD3 dataset could be downloaded from <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5860510/bin/btx081_supp.zip">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5860510/bin/btx081_supp.zip</ext-link>. The source code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhanglabNKU/APIN">https://github.com/zhanglabNKU/APIN</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p id="Par63">Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p id="Par64">Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par65">The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gallo</surname>
            <given-names>RL</given-names>
          </name>
          <name>
            <surname>Huttner</surname>
            <given-names>KM</given-names>
          </name>
        </person-group>
        <article-title>Antimicrobial peptides: an emerging concept in cutaneous biology</article-title>
        <source>J Invest Dermatol</source>
        <year>1998</year>
        <volume>111</volume>
        <issue>5</issue>
        <fpage>739</fpage>
        <lpage>743</lpage>
        <pub-id pub-id-type="doi">10.1046/j.1523-1747.1998.00361.x</pub-id>
        <?supplied-pmid 9804331?>
        <pub-id pub-id-type="pmid">9804331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ganz</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Defensins: antimicrobial peptides of innate immunity</article-title>
        <source>Nat Rev Immunol</source>
        <year>2003</year>
        <volume>3</volume>
        <issue>9</issue>
        <fpage>710</fpage>
        <lpage>720</lpage>
        <pub-id pub-id-type="doi">10.1038/nri1180</pub-id>
        <?supplied-pmid 12949495?>
        <pub-id pub-id-type="pmid">12949495</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fjell</surname>
            <given-names>CD</given-names>
          </name>
          <name>
            <surname>Jenssen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hilpert</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Cheung</surname>
            <given-names>WA</given-names>
          </name>
          <name>
            <surname>Panté</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hancock</surname>
            <given-names>REW</given-names>
          </name>
          <name>
            <surname>Cherkasov</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Identification of novel antibacterial peptides by Chemoinformatics and machine learning</article-title>
        <source>J Med Chem</source>
        <year>2009</year>
        <volume>52</volume>
        <issue>7</issue>
        <fpage>2006</fpage>
        <lpage>2015</lpage>
        <pub-id pub-id-type="doi">10.1021/jm8015365</pub-id>
        <?supplied-pmid 19296598?>
        <pub-id pub-id-type="pmid">19296598</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zelezetsky</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Pontillo</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Puzzi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Antcheva</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Segat</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Pacor</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Crovella</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tossi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Evolution of the primate cathelicidin. Correlation between structural variations and antimicrobial activity</article-title>
        <source>J Biol Chem</source>
        <year>2006</year>
        <volume>281</volume>
        <issue>29</issue>
        <fpage>19861</fpage>
        <lpage>19871</lpage>
        <pub-id pub-id-type="doi">10.1074/jbc.M511108200</pub-id>
        <?supplied-pmid 16720578?>
        <pub-id pub-id-type="pmid">16720578</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thomas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Karnik</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Barai</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Jayaraman</surname>
            <given-names>VK</given-names>
          </name>
          <name>
            <surname>Idicula-Thomas</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>CAMP: a useful resource for research on antimicrobial peptides</article-title>
        <source>Nucleic Acids Res</source>
        <year>2010</year>
        <volume>38</volume>
        <issue>Database issue</issue>
        <fpage>D774</fpage>
        <lpage>D780</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkp1021</pub-id>
        <?supplied-pmid 19923233?>
        <pub-id pub-id-type="pmid">19923233</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waghu</surname>
            <given-names>FH</given-names>
          </name>
          <name>
            <surname>Barai</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Gurung</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Idicula-Thomas</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>CAMPR3: a database on sequences, structures and signatures of antimicrobial peptides</article-title>
        <source>Nucleic Acids Res</source>
        <year>2016</year>
        <volume>44</volume>
        <issue>D1</issue>
        <fpage>D1094</fpage>
        <lpage>D1097</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv1051</pub-id>
        <?supplied-pmid 26467475?>
        <pub-id pub-id-type="pmid">26467475</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>HT</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Lai</surname>
            <given-names>JZ</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>KY</given-names>
          </name>
        </person-group>
        <article-title>A large-scale structural classification of antimicrobial peptides</article-title>
        <source>Biomed Res Int</source>
        <year>2015</year>
        <volume>2015</volume>
        <fpage>475062</fpage>
        <pub-id pub-id-type="doi">10.1155/2015/475062</pub-id>
        <?supplied-pmid 26000295?>
        <pub-id pub-id-type="pmid">26000295</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fjell</surname>
            <given-names>CD</given-names>
          </name>
          <name>
            <surname>Hancock</surname>
            <given-names>REW</given-names>
          </name>
          <name>
            <surname>Cherkasov</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>AMPer: a database and an automated discovery tool for antimicrobial peptides</article-title>
        <source>Bioinformatics</source>
        <year>2007</year>
        <volume>23</volume>
        <issue>9</issue>
        <fpage>1148</fpage>
        <lpage>1155</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btm068</pub-id>
        <?supplied-pmid 17341497?>
        <pub-id pub-id-type="pmid">17341497</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lata</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>BK</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>GP</given-names>
          </name>
        </person-group>
        <article-title>Analysis and prediction of antibacterial peptides</article-title>
        <source>BMC Bioinformatics</source>
        <year>2007</year>
        <volume>8</volume>
        <fpage>263</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-8-263</pub-id>
        <?supplied-pmid 17645800?>
        <pub-id pub-id-type="pmid">17645800</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lata</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mishra</surname>
            <given-names>NK</given-names>
          </name>
          <name>
            <surname>Raghava</surname>
            <given-names>GP</given-names>
          </name>
        </person-group>
        <article-title>AntiBP2: improved version of antibacterial peptide prediction</article-title>
        <source>BMC Bioinformatics</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>Suppl 1</issue>
        <fpage>S19</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-11-S1-S19</pub-id>
        <?supplied-pmid 20122190?>
        <pub-id pub-id-type="pmid">20122190</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thakur</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Qureshi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>AVPpred: collection and prediction of highly effective antiviral peptides</article-title>
        <source>Nucleic Acids Res</source>
        <year>2012</year>
        <volume>40</volume>
        <issue>Web Server issue</issue>
        <fpage>W199</fpage>
        <lpage>W204</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gks450</pub-id>
        <?supplied-pmid 22638580?>
        <pub-id pub-id-type="pmid">22638580</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>WZ</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>JH</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>KC</given-names>
          </name>
        </person-group>
        <article-title>iAMP-2L: a two-level multi-label classifier for identifying antimicrobial peptides and their functional types</article-title>
        <source>Anal Biochem</source>
        <year>2013</year>
        <volume>436</volume>
        <issue>2</issue>
        <fpage>168</fpage>
        <lpage>177</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ab.2013.01.019</pub-id>
        <?supplied-pmid 23395824?>
        <pub-id pub-id-type="pmid">23395824</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Veltri</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kamath</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Shehu</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Improving recognition of antimicrobial peptides and target selectivity through machine learning and genetic programming</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinform</source>
        <year>2017</year>
        <volume>14</volume>
        <issue>2</issue>
        <fpage>300</fpage>
        <lpage>313</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2015.2462364</pub-id>
        <?supplied-pmid 28368808?>
        <pub-id pub-id-type="pmid">28368808</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Joseph</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Karnik</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Nilawe</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Jayaraman</surname>
            <given-names>VK</given-names>
          </name>
          <name>
            <surname>Idicula-Thomas</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>ClassAMP: a prediction tool for classification of antimicrobial peptides</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinformatics</source>
        <year>2012</year>
        <volume>9</volume>
        <issue>5</issue>
        <fpage>1535</fpage>
        <lpage>1538</lpage>
        <pub-id pub-id-type="doi">10.1109/tcbb.2012.89</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gabere</surname>
            <given-names>MN</given-names>
          </name>
          <name>
            <surname>Noble</surname>
            <given-names>WS</given-names>
          </name>
        </person-group>
        <article-title>Empirical comparison of web-based antimicrobial peptide prediction tools</article-title>
        <source>Bioinformatics (Oxford, England)</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>13</issue>
        <fpage>1921</fpage>
        <lpage>1929</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx081</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meher</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Sahu</surname>
            <given-names>TK</given-names>
          </name>
          <name>
            <surname>Saini</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Rao</surname>
            <given-names>AR</given-names>
          </name>
        </person-group>
        <article-title>Predicting antimicrobial peptides with improved accuracy by incorporating the compositional, physico-chemical and structural features into Chou's general PseAAC</article-title>
        <source>Sci Rep</source>
        <year>2017</year>
        <volume>7</volume>
        <fpage>42362</fpage>
        <pub-id pub-id-type="doi">10.1038/srep42362</pub-id>
        <?supplied-pmid 28205576?>
        <pub-id pub-id-type="pmid">28205576</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in bioinformatics</article-title>
        <source>Methods</source>
        <year>2019</year>
        <volume>166</volume>
        <fpage>1</fpage>
        <lpage>3</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ymeth.2019.06.006</pub-id>
        <?supplied-pmid 31181259?>
        <pub-id pub-id-type="pmid">31181259</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heffernan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Lyons</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sattar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Improving prediction of secondary structure, local backbone angles, and solvent accessible surface area of proteins by iterative deep learning</article-title>
        <source>Sci Rep</source>
        <year>2015</year>
        <volume>5</volume>
        <fpage>11476</fpage>
        <pub-id pub-id-type="doi">10.1038/srep11476</pub-id>
        <?supplied-pmid 26098304?>
        <pub-id pub-id-type="pmid">26098304</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lyons</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Dehzangi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Heffernan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sattar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Predicting backbone Cα angles and dihedrals from protein sequences by stacked sparse auto-encoder deep neural network</article-title>
        <source>J Comput Chem</source>
        <year>2014</year>
        <volume>35</volume>
        <issue>28</issue>
        <fpage>2040</fpage>
        <lpage>2046</lpage>
        <pub-id pub-id-type="doi">10.1002/jcc.23718</pub-id>
        <?supplied-pmid 25212657?>
        <pub-id pub-id-type="pmid">25212657</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Asgari</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Mofrad</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Continuous distributed representation of biological sequences for deep proteomics and genomics</article-title>
        <source>PLoS One</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>11</issue>
        <fpage>e0141287</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0141287</pub-id>
        <?supplied-pmid 26555596?>
        <pub-id pub-id-type="pmid">26555596</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Mahjoubfar</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tai</surname>
            <given-names>LC</given-names>
          </name>
          <name>
            <surname>Blaby</surname>
            <given-names>IK</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Niazi</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Jalali</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in label-free cell classification</article-title>
        <source>Sci Rep</source>
        <year>2016</year>
        <volume>6</volume>
        <fpage>21471</fpage>
        <pub-id pub-id-type="doi">10.1038/srep21471</pub-id>
        <?supplied-pmid 26975219?>
        <pub-id pub-id-type="pmid">26975219</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Xiang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Gilmore</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Madabhushi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Stacked sparse autoencoder (SSAE) for nuclei detection on breast Cancer histopathology images</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2016</year>
        <volume>35</volume>
        <issue>1</issue>
        <fpage>119</fpage>
        <lpage>130</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2015.2458702</pub-id>
        <?supplied-pmid 26208307?>
        <pub-id pub-id-type="pmid">26208307</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Veltri</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kamath</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Shehu</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Deep learning improves antimicrobial peptide recognition</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>16</issue>
        <fpage>2740</fpage>
        <lpage>2747</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty179</pub-id>
        <?supplied-pmid 29590297?>
        <pub-id pub-id-type="pmid">29590297</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <issue>7553</issue>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <?supplied-pmid 9377276?>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Palangi</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Ward</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Deep sentence embedding using long short-term memory networks: analysis and application to information retrieval</article-title>
        <source>IEEE/ACM Trans Audio Speech Lang Proc</source>
        <year>2016</year>
        <volume>24</volume>
        <issue>4</issue>
        <fpage>694</fpage>
        <lpage>707</lpage>
        <pub-id pub-id-type="doi">10.1109/taslp.2016.2520371</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sundermeyer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ney</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Schluter</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>From feedforward to recurrent LSTM neural networks for language modeling</article-title>
        <source>Audio Speech Lang Process IEEE/ACM Trans on</source>
        <year>2015</year>
        <volume>23</volume>
        <fpage>517</fpage>
        <lpage>529</lpage>
        <pub-id pub-id-type="doi">10.1109/TASLP.2015.2400218</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mikolov</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Corrado</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Efficient estimation of word representations in vector space</article-title>
        <source>arXiv e-prints</source>
        <year>2013</year>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Pennington</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Socher</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Manning</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Glove: Global Vectors for Word Representation</article-title>
        <source>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</source>
        <year>2014</year>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>APD3: the antimicrobial peptide database as a tool for research and education</article-title>
        <source>Nucleic Acids Res</source>
        <year>2016</year>
        <volume>44</volume>
        <issue>D1</issue>
        <fpage>D1087</fpage>
        <lpage>D1093</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkv1278</pub-id>
        <?supplied-pmid 26602694?>
        <pub-id pub-id-type="pmid">26602694</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Magrane</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>UniProt</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>UniProt Knowledgebase: a hub of integrated protein data</article-title>
        <source>Database (Oxford)</source>
        <year>2011</year>
        <volume>2011</volume>
        <fpage>bar009</fpage>
        <pub-id pub-id-type="doi">10.1093/database/bar009</pub-id>
        <pub-id pub-id-type="pmid">21447597</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Manavalan</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Shin</surname>
            <given-names>TH</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>MO</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>AIPpred: sequence-based prediction of anti-inflammatory peptides using random Forest</article-title>
        <source>Front Pharmacol</source>
        <year>2018</year>
        <volume>9</volume>
        <fpage>276</fpage>
        <pub-id pub-id-type="doi">10.3389/fphar.2018.00276</pub-id>
        <?supplied-pmid 29636690?>
        <pub-id pub-id-type="pmid">29636690</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Chollet, F. Keras: The python deep learning library. In: Astrophysics Source Code Library; 2018.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Abadi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Barham</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dean</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Devin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ghemawat</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Irving</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Isard</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>TensorFlow: a system for large-scale machine learning</article-title>
        <source>arXiv e-prints</source>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Powers</surname>
            <given-names>DMW</given-names>
          </name>
        </person-group>
        <article-title>Evaluation: from precision, recall and f-measure to roc., informedness, markedness &amp; correlation</article-title>
        <source>J Mach Learn Technol</source>
        <year>2011</year>
        <volume>2</volume>
        <issue>1</issue>
        <fpage>37</fpage>
        <lpage>63</lpage>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Nair</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>GE</given-names>
          </name>
        </person-group>
        <article-title>Rectified linear units improve restricted boltzmann machines</article-title>
        <source>Proceedings of the 27th International Conference on International Conference on Machine Learning; Haifa, Israel</source>
        <year>2010</year>
        <fpage>807</fpage>
        <lpage>814</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boughorbel</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Jarray</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>El-Anbari</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Optimal classifier for imbalanced data using Matthews correlation coefficient metric</article-title>
        <source>PLoS One</source>
        <year>2017</year>
        <volume>12</volume>
        <issue>6</issue>
        <fpage>e0177678</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0177678</pub-id>
        <?supplied-pmid 28574989?>
        <pub-id pub-id-type="pmid">28574989</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brzezinski</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Stefanowski</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Prequential AUC: properties of the area under the ROC curve for data streams with concept drift</article-title>
        <source>Knowl Inf Syst</source>
        <year>2017</year>
        <volume>52</volume>
        <issue>2</issue>
        <fpage>531</fpage>
        <lpage>562</lpage>
        <pub-id pub-id-type="doi">10.1007/s10115-017-1022-8</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Chromatin accessibility prediction via a hybrid deep convolutional neural network</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>5</issue>
        <fpage>732</fpage>
        <lpage>738</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx679</pub-id>
        <?supplied-pmid 29069282?>
        <pub-id pub-id-type="pmid">29069282</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
