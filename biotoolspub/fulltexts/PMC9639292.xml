<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9639292</article-id>
    <article-id pub-id-type="publisher-id">5013</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-022-05013-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>﻿SparkEC: speeding up alignment-based DNA error correction tools</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2077-1473</contrib-id>
        <name>
          <surname>Expósito</surname>
          <given-names>Roberto R.</given-names>
        </name>
        <address>
          <email>roberto.rey.exposito@udc.es</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Martínez-Sánchez</surname>
          <given-names>Marco</given-names>
        </name>
        <address>
          <email>marco.msanchez@udc.es</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Touriño</surname>
          <given-names>Juan</given-names>
        </name>
        <address>
          <email>juan@udc.es</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.8073.c</institution-id><institution-id institution-id-type="ISNI">0000 0001 2176 8535</institution-id><institution>Universidade da Coruña, CITIC, Computer Architecture Group, </institution></institution-wrap>Campus de Elviña, 15071 A Coruña, Spain </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>7</day>
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>23</volume>
    <elocation-id>464</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>10</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">In recent years, huge improvements have been made in the context of sequencing genomic data under what is called Next Generation Sequencing (NGS). However, the DNA reads generated by current NGS platforms are not free of errors, which can affect the quality of downstream analysis. Although error correction can be performed as a preprocessing step to overcome this issue, it usually requires long computational times to analyze those large datasets generated nowadays through NGS. Therefore, new software capable of scaling out on a cluster of nodes with high performance is of great importance.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we present SparkEC, a parallel tool capable of fixing those errors produced during the sequencing process. For this purpose, the algorithms proposed by the CloudEC tool, which is already proved to perform accurate corrections, have been analyzed and optimized to improve their performance by relying on the Apache Spark framework together with the introduction of other enhancements such as the usage of memory-efficient data structures and the avoidance of any input preprocessing. The experimental results have shown significant improvements in the computational times of SparkEC when compared to CloudEC for all the representative datasets and scenarios under evaluation, providing an average and maximum speedups of 4.9<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M2"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq1.gif"/></alternatives></inline-formula> and 11.9<inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M4"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq2.gif"/></alternatives></inline-formula>, respectively, over its counterpart.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">As error correction can take excessive computational time, SparkEC provides a scalable solution for correcting large datasets. Due to its distributed implementation, SparkEC speed can increase with respect to the number of nodes in a cluster. Furthermore, the software is freely available under GPLv3 license and is compatible with different operating systems (Linux, Windows and macOS).</p>
      </sec>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s12859-022-05013-1.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Error correction</kwd>
      <kwd>Big data</kwd>
      <kwd>Distributed processing</kwd>
      <kwd>Apache Spark</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004837</institution-id>
            <institution>Ministerio de Ciencia e Innovación</institution>
          </institution-wrap>
        </funding-source>
        <award-id>PID2019-104184RB-I00/AEI/10.13039/501100011033</award-id>
        <award-id>PRE2020-093218</award-id>
        <principal-award-recipient>
          <name>
            <surname>Martínez-Sánchez</surname>
            <given-names>Marco</given-names>
          </name>
          <name>
            <surname>Touriño</surname>
            <given-names>Juan</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100008425</institution-id>
            <institution>Consellería de Cultura, Educación e Ordenación Universitaria, Xunta de Galicia</institution>
          </institution-wrap>
        </funding-source>
        <award-id>ED431G 2019/01</award-id>
        <award-id>ED431C 2021/30</award-id>
        <principal-award-recipient>
          <name>
            <surname>Touriño</surname>
            <given-names>Juan</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par13">As the need to process large amounts of DNA sequences (the so-called reads) to conduct novel research keeps growing, new technologies grouped under Next Generation Sequencing (NGS) have arisen over the last decade to solve this requirement [<xref ref-type="bibr" rid="CR1">1</xref>]. However, NGS platforms are not perfect and can introduce sequencing errors in the generated reads which can affect the quality of downstream analysis. Therefore, error correction is an important preprocessing step in many NGS pipelines (see Section 1 of Additional file <xref rid="MOESM1" ref-type="media">1</xref> for more information about this topic).</p>
    <p id="Par14">Due to its importance, multiple correction algorithms have been proposed in the literature [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. However, most of the previous solutions usually lack either accuracy in correction, performance when processing large datasets, or the capability to scale out on a computing cluster. Among them, CloudEC [<xref ref-type="bibr" rid="CR4">4</xref>] has been proved to perform precise corrections together with a scalable approach by relying on Big Data technologies, since its correction algorithms have been designed upon the MapReduce paradigm [<xref ref-type="bibr" rid="CR5">5</xref>] using its most popular open-source implementation Apache Hadoop [<xref ref-type="bibr" rid="CR6">6</xref>] (more details about Big Data and MapReduce are provided in Section 2 of Additional file <xref rid="MOESM1" ref-type="media">1</xref>). However, the usage of this tool comes at the cost of poor performance in terms of computational time when managing the huge amounts of data usually generated by NGS platforms. According to their own published results [<xref ref-type="bibr" rid="CR7">7</xref>], the fastest experiment takes more than 18 h when correcting a dataset with 200 million reads on an 80-node computing cluster, showing a limited speedup of 5<inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M6"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq3.gif"/></alternatives></inline-formula> (5 times faster execution time using 8 times the number of nodes). In order to overcome this problem, in this work we are introducing SparkEC as a new tool based on this previous approach that can tackle these scalability limitations without giving up either of its advantages in terms of correction accuracy.</p>
    <p id="Par15">The main contributions of this paper are:<list list-type="bullet"><list-item><p id="Par16">A new parallel tool based on Apache Spark [<xref ref-type="bibr" rid="CR8">8</xref>] aimed at correcting errors in genomic reads that relies on accurate algorithms based on multiple sequence alignment strategies.</p></list-item><list-item><p id="Par17">A novel split-based processing strategy with a two-step k-mers distribution that allows correcting large NGS datasets much faster than previous tools.</p></list-item><list-item><p id="Par18">A simplified workflow for scientists and researchers by directly supporting standard unaligned formats without any need for input preprocessing.</p></list-item></list></p>
    <sec id="Sec2">
      <title>Related work</title>
      <p id="Par19">According to recent literature, current state-of-the-art correction approaches can be grouped into three main categories [<xref ref-type="bibr" rid="CR9">9</xref>]: k-mer spectrum-based algorithms, suffix-tree based approaches, and strategies that rely on Multiple Sequence Alignment (MSA). The first category is based on grouping and counting subsequences of a fixed length <italic>K</italic> from the reads (i.e., the so-called k-mers). After this counting has been performed, k-mers are classified as solid or weak depending on their number of appearances. After that step, corrections on input reads are made to transform the weak k-mers into solid ones. The second category is an extension of the previous approach, where instead of keeping a hash table with all the different k-mers, the data structure to store them is based on a tree that keeps track of the different suffixes of the reads. This allows these algorithms to find low frequency strings composed by high frequency substrings, enabling them to easily spot the errors. Finally, MSA algorithms [<xref ref-type="bibr" rid="CR10">10</xref>] are based on identifing groups of similar sequences and aligning them in order to construct a reference read that has more similarity with the original one. After this step, changes on input reads are made in order to near them to the original sequence.</p>
      <p id="Par20">MSA-based approaches typically allow for higher error correction precision but at the expense of greater computational complexity due to the multiple alignments. The CloudEC tool proposes two correctors that fit into this last category, and consequently our proposal also falls into this type of algorithms.</p>
      <sec id="Sec3">
        <title>Big data and parallel correctors</title>
        <p id="Par21">Big Data technologies are increasingly being used to handle the processing of large genomic datasets in a scalable way, including aligners and assemblers, among others [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR16">16</xref>]. In the context of DNA error correction, multiple solutions have been proposed in recent years. If we delve into the literature, there exist representative works for each one of the three aforementioned correction strategies: (1) those that count the frequency of the different substrings or k-mers in order to spot misread bases (e.g., Reptile [<xref ref-type="bibr" rid="CR17">17</xref>], BLESS2 [<xref ref-type="bibr" rid="CR18">18</xref>], Musket and its Spark-based approach [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]); (2) tools that generalize the previous strategy by using trees to analyze the suffixes of the strings (e.g., Pluribus [<xref ref-type="bibr" rid="CR21">21</xref>], SHREC [<xref ref-type="bibr" rid="CR22">22</xref>]); and (3) correctors that rely on MSA strategies to make multiple alignments of the input reads to apply the corrections among them [<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR24">24</xref>].</p>
        <p id="Par22">Delving into MSA-based parallel tools, the ALLPATHS-LG assembler [<xref ref-type="bibr" rid="CR25">25</xref>] provides a built-in error corrector implemented using a set of tasks that are executed through a Makefile. Therefore, it easily allows to set up a multi-threaded execution by taking advantage of the support provided by Makefile for such goal. However, it is not possible to distribute the computation across a cluster of nodes with this approach. For this reason, CloudRS [<xref ref-type="bibr" rid="CR26">26</xref>] has been proposed by taking the corrector of ALLPATHS-LG as baseline and implementing it upon Apache Hadoop. Thanks to this change, CloudRS is able to process the sequences using multiple worker nodes, effectively allowing it to handle larger datasets than ALLPATHS-LG in less time. Finally, CloudEC [<xref ref-type="bibr" rid="CR7">7</xref>] is another Hadoop-based MSA corrector that was presented as an enhanced version of CloudRS. The major improvement of CloudEC over its counterpart was the introduction of the <italic>spread corrector</italic>, a new MSA-based algorithm which increases the reliability of the reads at the cost of reducing its performance, as this algorithm is much more computationally intensive than the one provided by CloudRS (i.e., the <italic>pinch corrector</italic>). Although providing more accurate correction algorithms represents a clear advance in the state of the art, their usage in large datasets is unaffordable in terms of computational time. This is the main challenge that our proposal tries to overcome.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec4">
    <title>Implementation</title>
    <p id="Par23">As mentioned earlier, the correction algorithms provided by our tool SparkEC are the ones originally proposed by the MSA-based reference tool CloudEC (i.e., the <italic>spread</italic> and <italic>pinch correctors</italic>). In this work, we did not take any action to enhance those correction algorithms in terms of their accuracy, since they have been extensively evaluated in previous works [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR26">26</xref>]. Instead, our objective is twofold: increasing their performance by reducing the execution time, and improving their usability by removing the need from the user to manually execute some tedious tasks. These enhancements will be presented and analyzed throughout this section.</p>
    <sec id="Sec5">
      <title>CloudEC architecture</title>
      <p id="Par24">Before getting into details about SparkEC, it is important to outline how CloudEC is structured at a high level. This tool makes use of the Pipe &amp;Filter architectural pattern, which decomposes the whole job to be undertaken into multiple phases through which the data flow gets progressively transformed into the desired final result. More specifically, CloudEC consists of six phases: two correctors (PinchCorrect and SpreadCorrect), which are responsible of implementing the MSA-based algorithms themselves; two filters (LargeKmerFilter and UniqueKmerFilter), which speed up the execution of the correctors by tagging sequences that should not be processed; and two auxiliary phases (PreProcess and PostProcess), which handle both the input and output data flow of the tool, respectively. The overall pipeline defined with these six phases, which is depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, will be kept in our implementation except for minor changes during the preprocessing step aimed at supporting additional input formats, as will be later explained.<fig id="Fig1"><label>Fig. 1</label><caption><p>Phases of the CloudEC pipeline</p></caption><graphic xlink:href="12859_2022_5013_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par25">As previously mentioned, CloudEC is implemented upon the Hadoop framework (more details in Section 2.2 of Additional file <xref rid="MOESM1" ref-type="media">1</xref>). The procedure used to set up the pipeline described earlier with Hadoop is as follows: for each phase, a set of MapReduce jobs is dispatched one after another, chaining the output of a phase with the input of the next one. This tool defines a common internal data format among all the pipeline phases that is used for this intermediate communication between them. Furthermore, the dataset provided to CloudEC as input is expected to be stored in this custom format, requiring the user to preprocess the sequencing data on his/her own to convert the input reads before being able to run the pipeline. This additional preprocessing step means that CloudEC has seven effective phases, with the first one not being parallelizable. Moreover, this tool is unable to directly process standard sequence formats such as FastQ.</p>
      <p id="Par26">The execution of multiple MapReduce jobs to implement the pipeline has also some implications in terms of performance: firstly, a Shuffle &amp; Sort task (see Section 2.1 of Additional file <xref rid="MOESM1" ref-type="media">1</xref>) has to be undertaken by the Hadoop data processing engine in most of the jobs, which degrades the overall performance of CloudEC due to the usage of network and secondary storage for temporary data (e.g., data shuffling); secondly, data processed by MapReduce jobs are typically stored in long-term storage, degrading even more the throughput of the application. With our first optimization detailed next we provide a solution to these two problems.</p>
    </sec>
    <sec id="Sec6">
      <title>Replacement of the data processing paradigm</title>
      <p id="Par27">The first step carried out was to migrate the underlying data processing engine from Hadoop to Spark. This transition implies the replacement of all the explicit MapReduce jobs executed by CloudEC to the specific paradigm defined by Spark, based on an implicit handling of these tasks via the usage of abstract data transformations.</p>
      <p id="Par28">To do so, the Resilient Distributed Datasets (RDDs) [<xref ref-type="bibr" rid="CR27">27</xref>] defined by Spark were used. These structures allow the developer to store data that are defined as a set of elements in a distributed way across a cluster of nodes. Details about RDDs are provided in Section 2.3 of Additional file <xref rid="MOESM1" ref-type="media">1</xref>. Their interesting features, such as the support of in-memory computations in a fault-tolerant manner, are specially beneficial for the performance of our tool. In fact, they can either solve or, at least, reduce the impact of the two CloudEC problems mentioned earlier: (1) most of the operations performed over RDDs are lazily computed, enabling Spark to coalesce some of them and thus minimizing the Shuffle &amp; Sort tasks that have to be undertaken; and (2) since the RDDs can be stored into main memory, the usage of secondary storage can be reduced, improving the overall performance. Moreover, the RDDs can be cached to prevent Spark from disposing them, being able to reuse the data previously generated. In SparkEC, this functionality is applied to the input reads, keeping always the most recent version of the dataset cached in memory. This configuration allows our tool to generate the k-mers that will be used by both correctors and filters and, after applying the corresponding algorithm, join those k-mers again with their original sequences.</p>
      <p id="Par29">Finally, Spark also allows developers to manage both the number of partitions and the partitioning strategy used for each RDD to determine how many pieces an RDD is decomposed into and the algorithm to assign the RDD elements to each partition, respectively. In our proposal, we have chosen to customize the default Spark behaviour when none of these values are provided, by keeping the number of partitions constant throughout all the execution. This way, we prevent the use of an extremely low number of partitions since the default behaviour of Spark is to set this value to the default parallelism level. Moreover, SparkEC relies on a hash-based partitioning strategy, which partitions the data based on the hashcode of the RDD elements. This strategy works well in our context and does not introduce the overhead of the range-based partitioning approach that is needed to guarantee that all partitions have the same size [<xref ref-type="bibr" rid="CR28">28</xref>]. Furthermore, in order to achieve a homogeneous distribution with the hash-based partitioner, SparkEC defines the hashcode of the RDD elements in such a way that they get oddly distributed.</p>
    </sec>
    <sec id="Sec7">
      <title>Input preprocessing</title>
      <p id="Par30">Another important drawback of the CloudEC tool is the need to preprocess the input dataset to transform the FastQ sequences into an internal custom format named SimpleFastQ (SFQ). This step is specially heavy, since CloudEC does not provide any parallel approach that could take advantage of multiple nodes or threads. Only after executing this preprocessing step, the user can upload the transformed dataset to a distributed file system, such as the Hadoop Distributed File System (HDFS) [<xref ref-type="bibr" rid="CR29">29</xref>], in order to be corrected in parallel by CloudEC.</p>
      <p id="Par31">To solve this issue, SparkEC relies on the the Hadoop Sequence Parser (HSP) [<xref ref-type="bibr" rid="CR30">30</xref>]. HSP is a Hadoop-based library written in Java that allows developers to read both genetic and protein sequences stored in formats commonly used in the field (i.e., FastQ/FastA) from either local or distributed file systems such as HDFS. In our proposal, this library is introduced into the PreProcess phase (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>), removing the requirement of preprocessing the input dataset and thus further optimizing performance. It is important to note that this optimization not only improves performance but also simplifies the overall pipeline that the users need to set up, enhancing the overall usability of the tool.</p>
    </sec>
    <sec id="Sec8">
      <title>Split-based system</title>
      <p id="Par32">By replacing Hadoop with Spark we can take advantage of its advanced features and extensions compared to the MapReduce model. However, simply replacing the underlying data processing engine would be a naive approach. A certain computing algorithm that works using the secondary storage might not perform adequately when constrained to use the main memory, as there is typically less memory than storage space available on disk. The default behaviour of Spark in such scenario, where it gets overwhelmed by the memory needs, is either to discard and recompute the RDD partitions as needed or instead to use local disks to store them, thus reducing the potential benefits of in-memory computations. Therefore, CloudEC is a clear example where a straightforward code migration from Hadoop to Spark may bring little to no performance advantage, since this tool provides very precise but memory-intensive correction algorithms that directly challenge the way Spark processes data in memory.</p>
      <p id="Par33">Therefore, a thorough redesign of CloudEC was mandatory in order to fully exploit Spark performance. To do so, SparkEC introduces a novel split-based processing system to keep the aggregate memory usage bounded during the computations. This optimization consists in preventing the alignment of all the sequences simultaneously. To achieve this behaviour, our tool starts by computing the total amount of memory available to Spark by multiplying the memory assigned to each Spark executor by the number of available executors (see Section 2.3.2 of Additional file <xref rid="MOESM1" ref-type="media">1</xref> for an overview of Spark cluster deployment). Next, an estimation <italic>M</italic> of the memory required to process the input dataset is calculated as shown in Equation <xref rid="Equ1" ref-type="">1</xref>, with <italic>L</italic> being an estimate of the average length of DNA reads, <italic>K</italic> the length of the k-mers, <italic>N</italic> the number of total reads, and <italic>C</italic> a constant with a default value of 5.25 obtained as a result of an experimental tuning, but such value is configurable by the user (see Section 3.4 of Additional file <xref rid="MOESM1" ref-type="media">1</xref>).<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} M = ((L - K) * K) * N * C \end{aligned}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>N</mml:mi><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2022_5013_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>Taking into account such memory estimation, a certain number of splits is recommended for each of the pipeline phases defined in SparkEC. Each phase will issue only the k-mers that are expected to be aligned in each split. After processing the different splits, the tool will aggregate the partial results that are generated, thus completing the execution of the whole phase. As an example, Fig. <xref rid="Fig2" ref-type="fig">2</xref> depicts the overall execution workflow of the SpreadCorrect phase using the split-based system, showing the join-like Spark operations needed to aggregate partial results.<fig id="Fig2"><label>Fig. 2</label><caption><p>The split-based system over SpreadCorrect</p></caption><graphic xlink:href="12859_2022_5013_Fig2_HTML" id="MO3"/></fig></p>
      <p id="Par34">Finally, it is important to mention the limitations of our current implementation. On the one hand, the maximum sequence length (<italic>L</italic>) is limited to 32,767 base pairs, inherited from the CloudEC implementation. Longer sequences are omitted in SparkEC during processing, whereas CloudEC just fails at runtime. On the other hand, <italic>L</italic> is estimated in Equation <xref rid="Equ1" ref-type="">1</xref> by taking a sample from the input dataset, so the memory estimation (<italic>M</italic>) may not be optimal for those datasets where there is a large variability in sequence length. Although the constant <italic>C</italic> can be set through configuration in order to tune the split-based system in those scenarios, it would be great to provide a heuristic to help determine a suitable value for such constant. However, the experimental results shown later, which include datasets containing fixed- and variable-length sequences, will demonstrate the effectiveness of the split-based system in its current form.</p>
      <sec id="Sec9">
        <title>Two-step k-mers distribution</title>
        <p id="Par35">There is still a challenge to be considered when introducing the split-based system. Since Spark distributes the RDDs into different partitions in order to assign tasks to the available worker nodes to process them, it is necessary to prevent a potential collision between the partitioning algorithm used by Spark and the distribution of k-mers into splits performed by our tool. Such collision may arise due to the combination of the following three facts: (1) as explained before, the RDD elements are distributed among the partitions based on their hashcode; (2) the splits where a given data record belongs to are also assigned based on their hashcode; and (3) the algorithm that determines the partition where the RDD elements belong to using the hashcode is the same used to find out the split where they should be computed (i.e., the modulus between the hashcode and the number of either partitions or splits). Taking all these facts into account, in the event that the number of partitions and the number of splits have a common divisor, some workers would not perform any computation. For example, if both the number of partitions and the number of splits are divisible by 2, only the first half of workers would have workload assigned in the first split, and the second half only during the second split.</p>
        <p id="Par36">To overcome this issue, the split-based system distributes the k-mers in two steps: in the first one, all the k-mers are distributed into <italic>P</italic> groups based on their hashcode, being <italic>P</italic> greater than the number of splits (<italic>S</italic>) and co-prime with the number of partitions of the input RDD. In the second step, data are redistributed into the different splits by computing the modulus between its group number and <italic>S</italic>. This way, two different kinds of splits are created: the first <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$( P - S )$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo>-</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq4.gif"/></alternatives></inline-formula> splits will be assigned a given workload, and the remaining ones will have to handle only half of such workload. Therefore, the problem of having idle workers is solved, although at the cost of introducing a small workload imbalance between the splits.</p>
        <p id="Par37">The split-based system together with the two-step k-mers distribution has been experimentally proven to allow SparkEC to be significantly faster than CloudEC, even in scenarios where there was no enough memory to handle the datasets. This will be experimentally shown in the Results and Discussion section, where SparkEC takes advantage of this optimization specifically in those scenarios with heavy memory constraints (i.e., those using a low number of nodes).</p>
      </sec>
    </sec>
    <sec id="Sec10">
      <title>Memory-efficient data structures</title>
      <p id="Par38">Aligned with the previous optimization, we have also modified the representation of the internal data structures used by CloudEC in order to make them more efficient in terms of memory usage.</p>
      <p id="Par39">When using the original approach proposed by CloudEC, most of the communications made between the pipeline phases are undertaken by encoding the data into plain text (typically, transforming each of the fields to text and separating them with a tab character). In SparkEC, this data encoding has been replaced by using ad hoc classes that have specific fields defined with the minimum memory usage required. As an example, whereas CloudEC would use the textual representation of the identifier for each DNA read, SparkEC relies on a single long value to store it. Although it may not seem to have a huge impact, it is worth noting that the underlying correction algorithms are based on applying multiple alignments to the data. So, the subsequences that are being aligned have to keep track of the read where they were found, which means that there is a huge number of read references stored in memory while this step is being executed.</p>
      <p id="Par40">Moreover, whenever Spark needs to perform a Shuffle (i.e., a redistribution of the data across the workers), it first needs to serialize the data, which is a costly operation in terms of CPU and disk usage. To improve data shuffling, SparkEC has been developed to take advantage of the Kryo serialization library [<xref ref-type="bibr" rid="CR31">31</xref>], which has been benchmarked against the default Java serializers used by CloudEC, proving to have better performance and being able to serialize data faster and in a more compressed way [<xref ref-type="bibr" rid="CR32">32</xref>]. This improvement, together with the aforementioned change in the representation of the data structures, is specially relevant in SparkEC since even though Kryo may also be used with CloudEC, the memory-bound approach taken by the Spark processing paradigm obtains a higher benefit from memory optimizations.</p>
      <sec id="Sec11">
        <title>Optimized encoding of DNA reads</title>
        <p id="Par41">Additional classes have also been introduced in SparkEC to optimize the encoding of the most used data structures, such as the base sequences or the reads. In this context, by read we refer to a tuple containing the bases, their qualities, an identifier, and additional fields to allow the auxiliary tagging of the read throughout the pipeline phases.</p>
        <p id="Par42">To encode the reads, a Node class is proposed as a partial replacement of the Utils class found in CloudEC. Unlike Utils, this new class, shown in Figure S4 of Additional file <xref rid="MOESM1" ref-type="media">1</xref>, contains explicit fields to encode each one of the attributes of the reads, rather than storing them in a Java HashMap that introduces memory overhead. To encode the bases and qualities of the sequences, a more complex approach is taken. Whereas CloudEC encodes the bases as text, we offer a generic interface called IDNASequence with two different implementations: EagerDNASequence and LazyDNASequence (see Figure S5 of Additional file <xref rid="MOESM1" ref-type="media">1</xref>). The first one stores the bases using an array of bytes and whenever a transformation is applied to the sequence, all the bases get recomputed. The second one applies a shared-memory, lazy-based approach to the bases by not executing the computations requested over them until the bases of the sequence are queried. This can be specially relevant, since the correction algorithms usually generate a large number of k-mers for each read, so being able to store all the k-mers and the read where they were generated from in the same location can save memory. However, our experimental results did not show a clear performance enhancement by using the shared-memory approach, and so EagerDNASequence is the default implementation used by SparkEC. This may be caused by the overhead introduced in the sequences to allow memory sharing and the need to keep the data distributed across the workers, forcing LazyDNASequence to replicate the entire reads and not only the k-mers.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Results and discussion</title>
    <p id="Par43">The experimental evaluation of SparkEC has been carried out comparatively with CloudEC on a high-performance computing cluster, both in terms of execution time and scalability. All the experiments have been conducted using the Big Data Evaluator (BDEv) tool [<xref ref-type="bibr" rid="CR33">33</xref>], which focuses on the benchmarking of Big Data processing frameworks and the applications and workloads developed on top of them. BDEv has been configured to use the YARN scheduler [<xref ref-type="bibr" rid="CR34">34</xref>] provided with Hadoop to manage the computational resources of the cluster nodes. Experiments using 5, 9 and 13 nodes have been executed to analyze the scalability of both tools, where each cluster size <italic>n</italic> can be understood as one master and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n-1$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq5.gif"/></alternatives></inline-formula> worker nodes. The main hardware characteristics shared by all the cluster nodes are summarized in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Hardware characteristics of the cluster nodes</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left">CPU model</td><td align="left">2 x Intel Xeon E5-2660 Sandy Bridge EP</td></tr><tr><td align="left">CPU clock frequency</td><td align="left">2.20 GHz</td></tr><tr><td align="left">Turbo clock frequency</td><td align="left">3 GHz</td></tr><tr><td align="left">Cores per CPU</td><td align="left">8</td></tr><tr><td align="left">Threads per core</td><td align="left">2</td></tr><tr><td align="left">L1/L2/L3 cache</td><td align="left">32 KB/256 KB/20 MB</td></tr><tr><td align="left">RAM memory</td><td align="left">64 GB DDR3 1600 MHz</td></tr><tr><td align="left">Storage</td><td align="left">HDD 1 TB SATA3 7.2K rpm</td></tr><tr><td align="left">Network interfaces</td><td align="left">InfiniBand FDR &amp; Gigabit Ethernet</td></tr></tbody></table></table-wrap></p>
    <sec id="Sec13">
      <title>Datasets and software configuration</title>
      <p id="Par44">As shown in Table <xref rid="Tab2" ref-type="table">2</xref>, six publicly available real datasets have been evaluated, named after their accession numbers in the Sequence Read Archive (SRA) [<xref ref-type="bibr" rid="CR35">35</xref>] at the National Center for Biotechnology Information (NCBI) [<xref ref-type="bibr" rid="CR36">36</xref>]. These input datasets provide a sufficiently representative sample since they have been obtained from different sequencing platforms (see third column in the table), varying both the number of total sequences from 5 to 26 million (see fourth column) and their length from 100 to several thousands of base pairs in the case of D5 and D6 (the fifth column shows their average read length and the last one the total number of bases).<table-wrap id="Tab2"><label>Table 2</label><caption><p>Public datasets used in the experimental evaluation</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Tag</th><th align="left">Accession number</th><th align="left">Instrument model</th><th align="left">#Reads</th><th align="left">Length</th><th align="left">#Bases</th></tr></thead><tbody><tr><td align="left">D1</td><td align="left">SRR352384</td><td align="left">Illumina Genome Analyzer II</td><td align="left">26.0 M</td><td align="left">152 bp</td><td align="left">4.0 G</td></tr><tr><td align="left">D2</td><td align="left">SRR022866</td><td align="left">Illumina Genome Analyzer II</td><td align="left">12.8 M</td><td align="left">152 bp</td><td align="left">1.9 G</td></tr><tr><td align="left">D3</td><td align="left">SRR034509</td><td align="left">Illumina Genome Analyzer II</td><td align="left">10.3 M</td><td align="left">202 bp</td><td align="left">2.1 G</td></tr><tr><td align="left">D4</td><td align="left">SRR4291508</td><td align="left">Illumina HiSeq 2000</td><td align="left">25.2 M</td><td align="left">100 bp</td><td align="left">2.5 G</td></tr><tr><td align="left">D5</td><td align="left">SRR21018951</td><td align="left">Oxford Nanopore MinION</td><td align="left">6.6 M</td><td align="left">285 bp<sup>a</sup></td><td align="left">1.9 G</td></tr><tr><td align="left">D6</td><td align="left">SRR2063079</td><td align="left">PacBio RS II SMRT</td><td align="left">5.1 M</td><td align="left">361 bp<sup>a</sup></td><td align="left">1.8 G</td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>Average read length</p></table-wrap-foot></table-wrap></p>
      <p id="Par45">Regarding the configuration of both tools, scenarios with two different values for <italic>K</italic> (<inline-formula id="IEq8"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K=24$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>24</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq8.gif"/></alternatives></inline-formula> and <inline-formula id="IEq9"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K=55$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>55</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq9.gif"/></alternatives></inline-formula>) have been included since those are the most widely used values according to similar studies in the literature. This parameter determines the length of the k-mers that are used to make the alignments, and has direct implications in terms of performance: the higher the value for this parameter, the lower the number of k-mers generated, and thus less computation is done. Regarding software configuration, Table <xref rid="Tab3" ref-type="table">3</xref> shows the specific versions that have been used in all the experiments. The only specific setting of Spark that was modified for the SparkEC executions was the configuration of the Kryo serializer. At the same time, there was a fine tuning of the HDFS configuration used by both tools for best performance: the replication factor (i.e., the number of replicas to store each block) was set to 2, whereas the block size was set to 64 MB.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Software configuration used in the experiments</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left">OS</td><td align="left">CentOS 7 (v7.7.1908)</td></tr><tr><td align="left">JVM</td><td align="left">OpenJDK 1.8.0_242</td></tr><tr><td align="left">Hadoop</td><td align="left">2.9.2</td></tr><tr><td align="left">Spark</td><td align="left">2.3.4</td></tr><tr><td align="left">HDFS block size</td><td align="left">64 MB</td></tr><tr><td align="left">HDFS replication factor</td><td align="left">2</td></tr></tbody></table></table-wrap></p>
      <p id="Par46">Finally, all the results shown in this section represent the arithmetic average of a minimum of 5 measurements for each experiment. The observed standard deviations were not significant since all the experiments were run with the cluster nodes in a dedicated manner (i.e., the hardware was never shared by other users’ jobs running on the cluster), which makes the average value a suitable performance metric for this work.</p>
    </sec>
    <sec id="Sec14">
      <title>Analysis of the results</title>
      <p id="Par47">Table <xref rid="Tab4" ref-type="table">4</xref> presents the experimental results of the SparkEC and CloudEC tools for each dataset and k-mer length when using 5, 9 and 13 cluster nodes. Overall, these results evidence the significant performance gains that SparkEC provides over its Hadoop-based counterpart for all the scenarios under evaluation, achieving an average speedup of 4.9<inline-formula id="IEq10"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M18"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq10.gif"/></alternatives></inline-formula>. On the one hand, this average speedup is around 4.3<inline-formula id="IEq11"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M20"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq11.gif"/></alternatives></inline-formula> for Illumina datasets (D1–D4), which contain short fixed-length reads (100–200 bp), reaching a maximum value of 11.8<inline-formula id="IEq12"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M22"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq12.gif"/></alternatives></inline-formula> when correcting the D4 dataset on 13 nodes using <inline-formula id="IEq13"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K=55$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>55</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq13.gif"/></alternatives></inline-formula> (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>). This means that SparkEC can reduce correction times for this dataset from more than 2 h when using CloudEC to just 10 min. On the other hand, the results for long-read datasets (D5–D6), which contain variable-length reads, follow a similar trend. In this case, the average speedup is even higher (6.1<inline-formula id="IEq14"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M26"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq14.gif"/></alternatives></inline-formula>), which validates the implementation of our split-based system when there is some variability in the length of the input reads. The maximum speedup is similar to that mentioned previously (11.9<inline-formula id="IEq15"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M28"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq15.gif"/></alternatives></inline-formula>), obtained when correcting the D6 dataset on 13 nodes using <inline-formula id="IEq16"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K=24$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>24</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq16.gif"/></alternatives></inline-formula> (see Fig. <xref rid="Fig4" ref-type="fig">4</xref>). It is important to remark that all the results shown for CloudEC do not include the time needed to preprocess the input datasets in order to transform them into the custom format required by this tool. Therefore, if those times were to be added to the CloudEC runtime, the benefits of using SparkEC would be even greater. For illustrative purposes, the additional time needed to preprocess the D1 dataset is around 2 min, a fact that should be taken into account only when using CloudEC, whereas SparkEC avoids such preprocessing as previously explained, thus providing an overall faster and simplified workflow for end users.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Runtimes (in seconds) and corresponding speedups of SparkEC over CloudEC for all datasets and k-mer values</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left"><italic>K</italic></th><th align="left">#Nodes</th><th align="left">CloudEC</th><th align="left">SparkEC</th><th align="left">Speedup</th></tr></thead><tbody><tr><td align="left" rowspan="6">D1</td><td char="." align="char" rowspan="3">24</td><td char="." align="char">5</td><td align="left">29,862</td><td align="left">11,951</td><td char="." align="char">2.5</td></tr><tr><td char="." align="char">9</td><td align="left">13,697</td><td align="left">4429</td><td char="." align="char">3.1</td></tr><tr><td char="." align="char">13</td><td align="left">9150</td><td align="left">2731</td><td char="." align="char">3.4</td></tr><tr><td char="." align="char" rowspan="3">55</td><td char="." align="char">5</td><td align="left">21,909</td><td align="left">9693</td><td char="." align="char">2.3</td></tr><tr><td char="." align="char">9</td><td align="left">10,135</td><td align="left">2792</td><td char="." align="char">3.6</td></tr><tr><td char="." align="char">13</td><td align="left">6216</td><td align="left">1785</td><td char="." align="char">3.5</td></tr><tr><td align="left" rowspan="6">D2</td><td char="." align="char" rowspan="3">24</td><td char="." align="char">5</td><td align="left">13,307</td><td align="left">5289</td><td char="." align="char">2.5</td></tr><tr><td char="." align="char">9</td><td align="left">5351</td><td align="left">1659</td><td char="." align="char">3.2</td></tr><tr><td char="." align="char">13</td><td align="left">3309</td><td align="left">971</td><td char="." align="char">3.4</td></tr><tr><td char="." align="char" rowspan="3">55</td><td char="." align="char">5</td><td align="left">8688</td><td align="left">4035</td><td char="." align="char">2.2</td></tr><tr><td char="." align="char">9</td><td align="left">3889</td><td align="left">1250</td><td char="." align="char">3.1</td></tr><tr><td char="." align="char">13</td><td align="left">2594</td><td align="left">700</td><td char="." align="char">3.7</td></tr><tr><td align="left" rowspan="6">D3</td><td char="." align="char" rowspan="3">24</td><td char="." align="char">5</td><td align="left">11,609</td><td align="left">4865</td><td char="." align="char">2.4</td></tr><tr><td char="." align="char">9</td><td align="left">4831</td><td align="left">1885</td><td char="." align="char">2.6</td></tr><tr><td char="." align="char">13</td><td align="left">3167</td><td align="left">1113</td><td char="." align="char">2.8</td></tr><tr><td char="." align="char" rowspan="3">55</td><td char="." align="char">5</td><td align="left">8502</td><td align="left">4892</td><td char="." align="char">1.7</td></tr><tr><td char="." align="char">9</td><td align="left">3679</td><td align="left">1348</td><td char="." align="char">2.7</td></tr><tr><td char="." align="char">13</td><td align="left">2616</td><td align="left">756</td><td char="." align="char">3.5</td></tr><tr><td align="left" rowspan="6">D4</td><td char="." align="char" rowspan="3">24</td><td char="." align="char">5</td><td align="left">43,506</td><td align="left">7473</td><td char="." align="char">5.8</td></tr><tr><td char="." align="char">9</td><td align="left">20,383</td><td align="left">2484</td><td char="." align="char">8.2</td></tr><tr><td char="." align="char">13</td><td align="left">14,334</td><td align="left">1511</td><td char="." align="char">9.5</td></tr><tr><td char="." align="char" rowspan="3">55</td><td char="." align="char">5</td><td align="left">21,723</td><td align="left">3987</td><td char="." align="char">5.4</td></tr><tr><td char="." align="char">9</td><td align="left">11,543</td><td align="left">1146</td><td char="." align="char">10.1</td></tr><tr><td char="." align="char">13</td><td align="left">7617</td><td align="left">648</td><td char="." align="char">11.8</td></tr><tr><td align="left" rowspan="6">D5</td><td char="." align="char" rowspan="3">24</td><td char="." align="char">5</td><td align="left">26,959</td><td align="left">8607</td><td char="." align="char">3.1</td></tr><tr><td char="." align="char">9</td><td align="left">12,611</td><td align="left">6486</td><td char="." align="char">1.9</td></tr><tr><td char="." align="char">13</td><td align="left">7729</td><td align="left">3927</td><td char="." align="char">2.0</td></tr><tr><td char="." align="char" rowspan="3">55</td><td char="." align="char">5</td><td align="left">14,374</td><td align="left">3698</td><td char="." align="char">3.9</td></tr><tr><td char="." align="char">9</td><td align="left">5827</td><td align="left">2233</td><td char="." align="char">2.6</td></tr><tr><td char="." align="char">13</td><td align="left">3577</td><td align="left">1437</td><td char="." align="char">2.5</td></tr><tr><td align="left" rowspan="6">D6</td><td char="." align="char" rowspan="3">24</td><td char="." align="char">5</td><td align="left">31,146</td><td align="left">3286</td><td char="." align="char">9.5</td></tr><tr><td char="." align="char">9</td><td align="left">18,511</td><td align="left">2047</td><td char="." align="char">9.0</td></tr><tr><td char="." align="char">13</td><td align="left">11,232</td><td align="left">942</td><td char="." align="char">11.9</td></tr><tr><td char="." align="char" rowspan="3">55</td><td char="." align="char">5</td><td align="left">17,806</td><td align="left">2105</td><td char="." align="char">8.5</td></tr><tr><td char="." align="char">9</td><td align="left">10,021</td><td align="left">1180</td><td char="." align="char">8.5</td></tr><tr><td char="." align="char">13</td><td align="left">6514</td><td align="left">687</td><td char="." align="char">9.5</td></tr></tbody></table></table-wrap></p>
      <p id="Par48">
        <fig id="Fig3">
          <label>Fig. 3</label>
          <caption>
            <p>Runtimes for CloudEC and SparkEC when correcting D4</p>
          </caption>
          <graphic xlink:href="12859_2022_5013_Fig3_HTML" id="MO4"/>
        </fig>
        <fig id="Fig4">
          <label>Fig. 4</label>
          <caption>
            <p>Runtimes for CloudEC and SparkEC when correcting D6</p>
          </caption>
          <graphic xlink:href="12859_2022_5013_Fig4_HTML" id="MO5"/>
        </fig>
      </p>
      <p id="Par49">It is also interesting to analyze the horizontal scalability provided by both tools, a feature which allows to further reduce the execution times by increasing the number of workers. The scalability results are shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, where the runtimes obtained for each dataset and k-mer length are grouped together using an arithmetic average. As can be observed, the scalability provided by CloudEC is not only kept by our tool, but even enhanced. Whereas CloudEC is able to obtain an average runtime reduction of 69% when increasing the number of nodes from 5 to 13, SparkEC further increases such runtime reduction to 75%. Thus, the average speedups obtained by our tool over CloudEC range from a speedup of 4.0x when using 5 nodes up to 5.6x when using 13, representing a 40% boost.<fig id="Fig5"><label>Fig. 5</label><caption><p>Runtimes for all datasets and <italic>K</italic> values grouped by number of nodes</p></caption><graphic xlink:href="12859_2022_5013_Fig5_HTML" id="MO6"/></fig><fig id="Fig6"><label>Fig. 6</label><caption><p>Runtime breakdown by phase when correcting D2 on 5 nodes</p></caption><graphic xlink:href="12859_2022_5013_Fig6_HTML" id="MO7"/></fig></p>
      <sec id="Sec15">
        <title>Runtime breakdown</title>
        <p id="Par50">Finally, a further analysis of each individual pipeline phase has been undertaken to evaluate the results in more detail, excluding the preprocessing and postprocessing steps as their impact on the total execution time is relatively low. For this assessment, the scenario correcting the D2 dataset with <inline-formula id="IEq17"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K=24$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>24</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq17.gif"/></alternatives></inline-formula> has been selected, comparing the runtimes for each phase when using 5, 9 and 13 nodes. The obtained results are shown in Figs. <xref rid="Fig6" ref-type="fig">6</xref>, <xref rid="Fig7" ref-type="fig">7</xref> and <xref rid="Fig8" ref-type="fig">8</xref>, respectively. As can be seen, SparkEC clearly outperforms CloudEC in all the phases regardless the number of nodes. Furthermore, the performance improvements provided by SparkEC are higher in those phases which are mostly compute-bound, as it is the case of PinchCorrect, LargeKmerFilter and UniqueKmerFilter. The opposite occurs in SpreadCorrect, since it is mostly an I/O-bound phase due to the large amount of data being shuffled by both Spark and Hadoop, although SparkEC keeps being able to provide significant speedups over CloudEC (up to 2.6x, see Fig. <xref rid="Fig8" ref-type="fig">8</xref>). This analysis also ensures that SparkEC would keep surpassing CloudEC even in those executions where some of the phases could be disabled, an advanced setting of both tools that can be configured by the user.<fig id="Fig7"><label>Fig. 7</label><caption><p>Runtime breakdown by phase when correcting D2 on 9 nodes</p></caption><graphic xlink:href="12859_2022_5013_Fig7_HTML" id="MO8"/></fig><fig id="Fig8"><label>Fig. 8</label><caption><p>Runtime breakdown by phase when correcting D2 on 13 nodes</p></caption><graphic xlink:href="12859_2022_5013_Fig8_HTML" id="MO9"/></fig></p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Conclusion</title>
    <p id="Par51">As the amount of genomic data generated by NGS technologies continues to grow, so does the need for more efficient ways of storing and processing them. To improve the quality of downstream analyses, there exist many tools for error correction of such sequencing data, where MSA-based algorithms represent a computational challenge when correcting large datasets.</p>
    <p id="Par52">Under the light of the results presented in this work, it is clear that our proposal represents an advance in the state of the art of MSA-based correction algorithms. SparkEC significantly outperforms its Hadoop-based counterpart in all the experiments, providing maximum speedups of around 12<inline-formula id="IEq18"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M34"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="12859_2022_5013_Article_IEq18.gif"/></alternatives></inline-formula> both for short- and long-read datasets. Our tool has also shown the ability to horizontally scale better than CloudEC and to perform well in resource-constrained scenarios and when correcting long-read datasets with variable-length sequences. Furthermore, SparkEC not only reduces the correction times to speed up subsequent biological research, but also simplifies its usage avoiding any preprocessing of the input reads. These characteristics will definitely contribute to lower the hardware requirements needed to apply MSA-based correctors after the DNA sequencing process, thus broadening the target scientists that can make use of these solutions. The result of this work is freely available under the permissive GPLv3 license and can be downloaded from the GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/UDC-GAC/SparkEC">https://github.com/UDC-GAC/SparkEC</ext-link>. Section 3 of Additional file <xref rid="MOESM1" ref-type="media">1</xref> includes a user’s guide that provides detailed instructions about downloading, executing and configuring SparkEC.</p>
    <p id="Par53">As future work, there are some minor enhancements to further boost the performance of the tool. In the short term, the memory representation of some data structures may be optimized, and the partitioning strategy could be improved to better handle scenarios with a high number of splits. In the long term, where more memory could be available, the split-based system may be revamped to process all the data simultaneously and thus further reduce the execution time.</p>
  </sec>
  <sec id="Sec17">
    <title>Availability and requirements</title>
    <p id="Par54">Project name: SparkEC.</p>
    <p id="Par55">Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/UDC-GAC/SparkEC">https://github.com/UDC-GAC/SparkEC</ext-link>.</p>
    <p id="Par56">Operating system(s): Platform independent.</p>
    <p id="Par57">Programming language: Java.</p>
    <p id="Par58">Other requirements: JRE 1.8 or higher, Apache Spark 2.0 or higher, Apache Hadoop 2.8 or higher (needed for HDFS).</p>
    <p id="Par59">License: GNU GPLv3.</p>
    <p id="Par60">Any restrictions to use by non-academics: None.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec18">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2022_5013_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1</bold>. Document including background information, additional figures related to the main text and a detailed user’s guide for SparkEC.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>NGS</term>
        <def>
          <p id="Par4">Next generation sequencing</p>
        </def>
      </def-item>
      <def-item>
        <term>MSA</term>
        <def>
          <p id="Par5">Multiple sequence alignment</p>
        </def>
      </def-item>
      <def-item>
        <term>RDD</term>
        <def>
          <p id="Par6">Resilient distributed dataset</p>
        </def>
      </def-item>
      <def-item>
        <term>SFQ</term>
        <def>
          <p id="Par7">SimpleFastQ</p>
        </def>
      </def-item>
      <def-item>
        <term>HDFS</term>
        <def>
          <p id="Par8">Hadoop distributed file system</p>
        </def>
      </def-item>
      <def-item>
        <term>HSP</term>
        <def>
          <p id="Par9">Hadoop sequence parser</p>
        </def>
      </def-item>
      <def-item>
        <term>BDEv</term>
        <def>
          <p id="Par10">Big data evaluator</p>
        </def>
      </def-item>
      <def-item>
        <term>SRA</term>
        <def>
          <p id="Par11">Sequence read archive</p>
        </def>
      </def-item>
      <def-item>
        <term>NCBI</term>
        <def>
          <p id="Par12">National Center for Biotechnology Information</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>MM and RRE conceived the software and designed the distributed implementation. MM is responsible for implementing the software. MM conducted the experiments and performed the data analysis. RRE and JT proposed and supervised the project. MM drafted the manuscript with contributions from all authors. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was funded by the Ministry of Science and Innovation of Spain (PID2019-104184RB-I00 / AEI / 10.13039 / 501100011033 and predoctoral grant PRE2020-093218), and by Xunta de Galicia and FEDER funds of the European Union (Centro de Investigación de Galicia accreditation 2019-2022, ref. ED431G 2019/01; Consolidation Program of Competitive Reference Groups, ref. ED431C 2021/30). The funding agencies did not participate in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The software, documentation and source code of SparkEC are publicly available at the GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/UDC-GAC/SparkEC">https://github.com/UDC-GAC/SparkEC</ext-link>. The real datasets analyzed during this study are also publicly available at the NCBI SRA repository (<ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/sra">https://www.ncbi.nlm.nih.gov/sra</ext-link>) using the accession numbers: SRR352384, SRR022866, SRR034509, SRR4291508, SRR21018951 and SRR2063079.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1">
      <title>Ethics approval and consent to participate</title>
      <p id="Par61">Not applicable.</p>
    </notes>
    <notes id="FPar2">
      <title>Consent for publication</title>
      <p id="Par62">Not applicable.</p>
    </notes>
    <notes id="FPar3" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par63">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van Dijk</surname>
            <given-names>EL</given-names>
          </name>
          <name>
            <surname>Auger</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jaszczyszyn</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Thermes</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Ten years of next-generation sequencing technology</article-title>
        <source>Trends Genet</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>9</issue>
        <fpage>418</fpage>
        <lpage>426</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tig.2014.07.001</pub-id>
        <?supplied-pmid 25108476?>
        <pub-id pub-id-type="pmid">25108476</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alic</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Ruzafa</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Dopazo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Blanquer</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Objective review of de novo stand-alone error correction methods for NGS data</article-title>
        <source>WIREs Comput Mol Sci</source>
        <year>2016</year>
        <volume>6</volume>
        <issue>2</issue>
        <fpage>111</fpage>
        <lpage>146</lpage>
        <pub-id pub-id-type="doi">10.1002/wcms.1239</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heydari</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Miclotte</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Demeester</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Van de Peer</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Fostier</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of the impact of Illumina error correction tools on de novo genome assembly</article-title>
        <source>BMC Bioinform</source>
        <year>2017</year>
        <volume>18</volume>
        <issue>1</issue>
        <fpage>374</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1784-8</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Chung W, Ho J, Lin C, Lee DT. CloudEC: a MapReduce-based algorithm for correcting errors in NGS data. [Online]. <ext-link ext-link-type="uri" xlink:href="https://github.com/CSCLabTW/CloudEC">https://github.com/CSCLabTW/CloudEC</ext-link>. Accessed 15 Sept 2022.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lämmel</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Google’s MapReduce programming model-Revisited</article-title>
        <source>Sci Comput Program</source>
        <year>2008</year>
        <volume>70</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="doi">10.1016/j.scico.2007.07.001</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Manikandan SG, Ravi S. Big data analysis using apache hadoop. In: Proceedings international conference on it convergence and security (ICITCS 2014), 2014;1–4 . Beijing, China.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Chung W, Ho J, Lin C, Lee DT. CloudEC: a MapReduce-based algorithm for correcting errors in next-generation sequencing Big Data. In: Proceedings IEEE international conference on big data (IEEE BigData 2017), 2017;2836–2842. Boston, MA, USA.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zaharia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Xin</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Wendell</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Armbrust</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dave</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Apache spark: a unified engine for big data processing</article-title>
        <source>Commun ACM</source>
        <year>2016</year>
        <volume>59</volume>
        <issue>11</issue>
        <fpage>56</fpage>
        <lpage>65</lpage>
        <pub-id pub-id-type="doi">10.1145/2934664</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chockalingam</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Aluru</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A survey of error-correction methods for next-generation sequencing</article-title>
        <source>Brief Bioinform</source>
        <year>2013</year>
        <volume>14</volume>
        <issue>1</issue>
        <fpage>56</fpage>
        <lpage>66</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbs015</pub-id>
        <?supplied-pmid 22492192?>
        <pub-id pub-id-type="pmid">22492192</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Edgar</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Batzoglou</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Multiple sequence alignment</article-title>
        <source>Curr Opin Struct Biol</source>
        <year>2006</year>
        <volume>16</volume>
        <issue>3</issue>
        <fpage>368</fpage>
        <lpage>373</lpage>
        <pub-id pub-id-type="doi">10.1016/j.sbi.2006.04.004</pub-id>
        <?supplied-pmid 16679011?>
        <pub-id pub-id-type="pmid">16679011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Abu-Doleh A, Çatalyürek Ü V. Spaler: spark and GraphX based de novo genome assembler. In: Proceedings IEEE international conference on big data (IEEE BigData 2015), 2015;1013–1018 . Santa Clara, CA, USA.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abuín</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Pichel</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Pena</surname>
            <given-names>TF</given-names>
          </name>
          <name>
            <surname>Amigo</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>BigBWA: approaching the burrows-wheeler aligner to big data technologies</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <issue>24</issue>
        <fpage>4003</fpage>
        <lpage>4005</lpage>
        <?supplied-pmid 26323715?>
        <pub-id pub-id-type="pmid">26323715</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abuín</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Pichel</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Pena</surname>
            <given-names>TF</given-names>
          </name>
          <name>
            <surname>Amigo</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>SparkBWA: speeding up the alignment of high-throughput DNA sequencing data</article-title>
        <source>PLoS ONE</source>
        <year>2016</year>
        <volume>11</volume>
        <issue>5</issue>
        <fpage>1</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0155461</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Expósito</surname>
            <given-names>RR</given-names>
          </name>
          <name>
            <surname>Veiga</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>González-Domínguez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Touriño</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>MarDRe: efficient MapReduce-based removal of duplicate DNA reads in the cloud</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>17</issue>
        <fpage>2762</fpage>
        <lpage>2764</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx307</pub-id>
        <?supplied-pmid 28475668?>
        <pub-id pub-id-type="pmid">28475668</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Expósito</surname>
            <given-names>RR</given-names>
          </name>
          <name>
            <surname>González-Domínguez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Touriño</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>HSRA: hadoop-based spliced read aligner for RNA sequencing data</article-title>
        <source>PLoS ONE</source>
        <year>2018</year>
        <volume>13</volume>
        <issue>7</issue>
        <fpage>1</fpage>
        <lpage>25</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0201483</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yousefi Hadadian Nejad</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Goudarzi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Motahari</surname>
            <given-names>SA</given-names>
          </name>
        </person-group>
        <article-title>IMOS: improved meta-aligner and Minimap2 on spark</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>51</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-018-2592-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Dorman</surname>
            <given-names>KS</given-names>
          </name>
          <name>
            <surname>Aluru</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Reptile: representative tiling for short read error correction</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <issue>20</issue>
        <fpage>2526</fpage>
        <lpage>2533</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btq468</pub-id>
        <?supplied-pmid 20834037?>
        <pub-id pub-id-type="pmid">20834037</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ramachandran</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hwu</surname>
            <given-names>W-M</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>BLESS 2: accurate, memory-efficient and fast error correction method</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>15</issue>
        <fpage>2369</fpage>
        <lpage>2371</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw146</pub-id>
        <?supplied-pmid 27153708?>
        <pub-id pub-id-type="pmid">27153708</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Schröder</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Musket: a multistage k-mer spectrum-based error corrector for illumina sequence data</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>3</issue>
        <fpage>308</fpage>
        <lpage>315</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts690</pub-id>
        <?supplied-pmid 23202746?>
        <pub-id pub-id-type="pmid">23202746</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Expósito</surname>
            <given-names>RR</given-names>
          </name>
          <name>
            <surname>González-Domínguez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Touriño</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>SMusket: spark-based DNA error correction on distributed-memory systems</article-title>
        <source>Futur Gener Comput Syst</source>
        <year>2020</year>
        <volume>111</volume>
        <fpage>698</fpage>
        <lpage>713</lpage>
        <pub-id pub-id-type="doi">10.1016/j.future.2019.10.038</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Savel</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>LaFramboise</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Grama</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Koyutürk</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Pluribus-exploring the limits of error correction using a suffix tree</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinf</source>
        <year>2017</year>
        <volume>14</volume>
        <issue>6</issue>
        <fpage>1378</fpage>
        <lpage>1388</lpage>
        <pub-id pub-id-type="doi">10.1109/TCBB.2016.2586060</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schröder</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Schröder</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Puglisi</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Sinha</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>SHREC: a short-read error correction method</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>17</issue>
        <fpage>2157</fpage>
        <lpage>2163</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp379</pub-id>
        <?supplied-pmid 19542152?>
        <pub-id pub-id-type="pmid">19542152</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heydari</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Miclotte</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Van de Peer</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Fostier</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Illumina error correction near highly repetitive DNA regions improves de novo genome assembly</article-title>
        <source>BMC Bioinform</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>298</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-019-2906-2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kallenborn</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hildebrandt</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Care: context-aware sequencing read error correction</article-title>
        <source>Bioinformatics</source>
        <year>2021</year>
        <volume>37</volume>
        <issue>7</issue>
        <fpage>889</fpage>
        <lpage>895</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa738</pub-id>
        <?supplied-pmid 32818262?>
        <pub-id pub-id-type="pmid">32818262</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gnerre</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>MacCallum</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Przybylski</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ribeiro</surname>
            <given-names>FJ</given-names>
          </name>
          <name>
            <surname>Burton</surname>
            <given-names>JN</given-names>
          </name>
          <name>
            <surname>Walker</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>High-quality draft assemblies of mammalian genomes from massively parallel sequence data</article-title>
        <source>Proc Natl Acad Sci</source>
        <year>2011</year>
        <volume>108</volume>
        <issue>4</issue>
        <fpage>1513</fpage>
        <lpage>1518</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1017351108</pub-id>
        <?supplied-pmid 21187386?>
        <pub-id pub-id-type="pmid">21187386</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Chen C, Chang Y, Chung W, Lee D, Ho J. CloudRS: an error correction algorithm of high-throughput sequencing data based on scalable framework. In: Proceedings IEEE international conference on big data (IEEE BigData 2013), 2013;717–722. Santa Clara, CA, USA.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Zaharia M, Chowdhury M, Das T, Dave A, Ma J, McCauly M. Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing. In: Proceedings 9th USENIX symposium on networked systems design and implementation (NSDI’12), 2012;15–28. San Jose, CA, USA.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Geetha J, Harshit NG. Implementation and performance comparison of partitioning techniques in Apache Spark. In: Proceedings 10th international conference on computing, communication and networking technologies (ICCCNT’19), 2019;1–5. Kanpur, India.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Shvachko K, Kuang H, Radia S, Chansler R. The hadoop distributed file system. In: Proceedings IEEE 26th symposium on mass storage systems and technologies (MSST’10), 2010;1–10. Incline Village, NV, USA.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Expósito RR, Mosquera LL, González-Domínguez J. Hadoop sequence parser library. [Online]. <ext-link ext-link-type="uri" xlink:href="https://github.com/UDC-GAC/hsp">https://github.com/UDC-GAC/hsp</ext-link>. Accessed 15 Sept 2022.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Kryo serialization framework for Java. [Online]. <ext-link ext-link-type="uri" xlink:href="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</ext-link>. Accessed 15 Sept 2022.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Smith E. Benchmarking JVM serializers. [Online]. <ext-link ext-link-type="uri" xlink:href="https://github.com/eishay/jvm-serializers/wiki">https://github.com/eishay/jvm-serializers/wiki</ext-link>. Accessed 15 Sept 2022.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Veiga</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Enes</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Expósito</surname>
            <given-names>RR</given-names>
          </name>
          <name>
            <surname>Touriño</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>BDEv 3.0: energy efficiency and microarchitectural characterization of big data processing frameworks</article-title>
        <source>Futur Gener Comput Syst</source>
        <year>2018</year>
        <volume>86</volume>
        <fpage>565</fpage>
        <lpage>581</lpage>
        <pub-id pub-id-type="doi">10.1016/j.future.2018.04.030</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Vavilapalli VK, Murthy AC, Douglas C, Agarwal S, Konar M, Evans R. Apache hadoop YARN: yet another resource negotiator. In: Proceedings 4th annual symposium on cloud computing (SCC’13), 2013;1–16. Santa Clara, CA, USA.</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leinonen</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sugawara</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shumway</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The sequence read archive</article-title>
        <source>Nucleic Acids Res</source>
        <year>2010</year>
        <volume>39</volume>
        <issue>1</issue>
        <fpage>19</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="pmid">20805242</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">NCBI: National Center for Biotechnology Information. [Online]. <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov">https://www.ncbi.nlm.nih.gov</ext-link>. Accessed 15 Sept 2022.</mixed-citation>
    </ref>
  </ref-list>
</back>
