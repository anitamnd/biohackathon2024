<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6612897</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz319</article-id>
    <article-id pub-id-type="publisher-id">btz319</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2019 Conference Proceedings</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>General Computational Biology</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ADAPTIVE: leArning DAta-dePendenT, concIse molecular VEctors for fast, accurate metabolite identification from tandem mass spectra</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Nguyen</surname>
          <given-names>Dai Hai</given-names>
        </name>
        <xref ref-type="aff" rid="btz319-aff1">1</xref>
        <xref ref-type="corresp" rid="btz319-cor1"/>
        <!--<email>hai@kuicr.kyoto-u.ac.jp</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nguyen</surname>
          <given-names>Canh Hao</given-names>
        </name>
        <xref ref-type="aff" rid="btz319-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mamitsuka</surname>
          <given-names>Hiroshi</given-names>
        </name>
        <xref ref-type="aff" rid="btz319-aff1">1</xref>
        <xref ref-type="aff" rid="btz319-aff2">2</xref>
      </contrib>
    </contrib-group>
    <aff id="btz319-aff1"><label>1</label>Bioinformatics Center, Institute for Chemical Research, Kyoto University, Uji, Japan</aff>
    <aff id="btz319-aff2"><label>2</label>Department of Computer Science, Aalto University, Espoo, Finland</aff>
    <author-notes>
      <corresp id="btz319-cor1">To whom correspondence should be addressed. <email>hai@kuicr.kyoto-u.ac.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-07-05">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>14</issue>
    <fpage>i164</fpage>
    <lpage>i172</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz319.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Metabolite identification is an important task in metabolomics to enhance the knowledge of biological systems. There have been a number of machine learning-based methods proposed for this task, which predict a chemical structure of a given spectrum through an intermediate (chemical structure) representation called molecular fingerprints. They usually have two steps: (i) predicting fingerprints from spectra; (ii) searching chemical compounds (in database) corresponding to the predicted fingerprints. Fingerprints are feature vectors, which are usually very large to cover all possible substructures and chemical properties, and therefore heavily redundant, in the sense of having many molecular (sub)structures irrelevant to the task, causing limited predictive performance and slow prediction.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We propose ADAPTIVE, which has two parts: learning two mappings (i) from structures to molecular vectors and (ii) from spectra to molecular vectors. The first part learns molecular vectors for metabolites from given data, to be consistent with both spectra and chemical structures of metabolites. In more detail, molecular vectors are generated by a model, being parameterized by a message passing neural network, and parameters are estimated by maximizing the correlation between molecular vectors and the corresponding spectra in terms of Hilbert-Schmidt Independence Criterion. Molecular vectors generated by this model are compact and importantly adaptive (specific) to both given data and task of metabolite identification. The second part uses input output kernel regression (IOKR), the current cutting-edge method of metabolite identification. We empirically confirmed the effectiveness of ADAPTIVE by using a benchmark data, where ADAPTIVE outperformed the original IOKR in both predictive performance and computational efficiency.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The code will be accessed through <ext-link ext-link-type="uri" xlink:href="http://www.bic.kyoto-u.ac.jp/pathway/tools/ADAPTIVE">http://www.bic.kyoto-u.ac.jp/pathway/tools/ADAPTIVE</ext-link> after the acceptance of this article.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">JSPS</named-content>
          <named-content content-type="funder-identifier">10.13039/501100001691</named-content>
        </funding-source>
        <award-id>19J14714</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">MEXT Kakenhi</named-content>
        </funding-source>
        <award-id>18K11434</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">JST ACCEL</named-content>
        </funding-source>
        <award-id>JPMJAC1503</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">MEXT Kakenhi</named-content>
        </funding-source>
        <award-id>16H02868</award-id>
        <award-id>19H04169</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Metabolites are small molecules, having many important functions in living cells such as energy transport, signaling, building blocks of cells and so on (<xref rid="btz319-B18" ref-type="bibr">Wishart, 2007</xref>). Identifying their biochemical characteristics or so-called metabolite identification is an essential task in metabolomics to increase the knowledge of biological systems. Yet, it is still a challenging task due to the size or coverage of spectra libraries.</p>
    <p>Mass spectrometry (MS) is one of the most common techniques in analytical chemistry for dealing with metabolite identification (<xref rid="btz319-B2" ref-type="bibr">de Hoffmann and Stroobant, 2007</xref>). In more detail, a chemical compound is decomposed into fragments, of which mass-to-charge ratios (m/z) are continuously measured to obtain a mass spectrum. One MS spectrum can be represented by a list of peaks, each of which corresponds to a fragment captured by MS. <xref ref-type="fig" rid="btz319-F1">Figure 1</xref> shows a real example of a MS spectrum. In practice, tandem MS (also known as MS/MS or MS2) is widely used, in which precursor ions of specific m/z values from MS spectra are selected and further fragmented to produce other groups of product ions
[see, e.g. <xref rid="btz319-B17" ref-type="bibr">Vaniya and Fiehn (2015)</xref> for more details]. The MS/MS spectra provide structural information about the measured compound, which makes MS/MS more useful for tackling metabolite identification.</p>
    <fig id="btz319-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>Example MS spectrum from Human Metabolome Database (<xref rid="btz319-B19" ref-type="bibr">Wishart <italic>et al.</italic> 2013</xref>) for 1-Methylhistidine (HMBD00001), with the corresponding chemical structure (top-left) and peak list (top-right)</p>
      </caption>
      <graphic xlink:href="btz319f1"/>
    </fig>
    <p>A number of computational methods have been proposed for identifying unknown metabolites from MS/MS spectra data. In general, they are classified into three main categories: (i) spectral library search; (ii) <italic>in silico</italic> fragmentation; and (iii) machine learning (<xref rid="btz319-B13" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018a</xref>). Recent advances in metabolite identification have been led by the machine learning category (e.g. <xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>; <xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>; <xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>). This category can be further divided into two key groups: supervised learning for substructure prediction and unsupervised learning for substructure annotation. While the former is to find a mapping from inputs (e.g. spectra) to outputs (e.g. fingerprints), the latter extracts underlying substructures of metabolites. Our research focuses on supervised learning, where the common scheme is to learn a mapping from spectra to structures.</p>
    <p>The prediction can be divided into two steps: (i) fingerprint prediction: predicting fingerprints of a given test spectrum with supervised learning; (ii) candidate retrieval: retrieving chemical compound (from database) which is closest to the predicted fingerprints (<xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>).</p>
    <p>Kernel methods have been shown to be effective for fingerprint prediction, such as methods include FingerID (<xref rid="btz319-B8" ref-type="bibr">Heinonen <italic>et al.</italic>, 2012</xref>), CSI:FingerID (<xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>) and input output kernel regression (IOKR, <xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>). In particular, IOKR is recognized as the current cutting-edge method for metabolite identification due to the following advantages: (i) structures (e.g. feature interaction in the molecular fingerprint vectors) in the output can be incorporated into the learning model by the kernel defined in the output space, leading to accuracy improvement; (ii) fingerprints are simultaneously predicted by the learned model, rather than being considered as a set of separate tasks, resulting in faster computation. One can take structures of the metabolites into account by using graph kernels (path, shortest-path and graphlet kernels) or kernels defined on molecular fingerprints. It is also known that kernels based on fingerprint vectors obtained the best performance (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>). However, a limitation of using molecular fingerprints as the intermediate representation vectors is that they are general-purpose and very large in size to encode all possible substructures and chemical properties related with metabolites. Consequently, such vectors are neither necessarily specific to any task nor data, and therefore redundant in the sense that these vectors might contain information irrelevant to the task, resulting in limited predictive performance. Moreover, the large size of fingerprints causes slow prediction in the first step of the above two steps.</p>
    <p>Generally, in machine learning, deep learning has been proven successful recently in many application domains. Deep learning is useful for regular data, say a table, in which rows are instances and columns are features, and vice versa. However, semistructured data, particularly graphs, for example, chemical (or biological) molecules, which are irregular types of data, are difficult to be used with deep learning. A number of research efforts have been devoted to applying deep learning to semistructured data, proposing models to learn representations of graphs, such as <xref rid="btz319-B4" ref-type="bibr">Duvenaud <italic>et al.</italic> (2015)</xref>, <xref rid="btz319-B11" ref-type="bibr">Li <italic>et al.</italic> (2015)</xref> and <xref rid="btz319-B15" ref-type="bibr">Nguyen <italic>et al.</italic> (2017)</xref>. Importantly, <xref rid="btz319-B5" ref-type="bibr">Gilmer <italic>et al.</italic> (2017)</xref> showed that a lot of research on graphs can be formulated in a unified model, namely message passing neural network (MPNN), with the following three components: <italic>message passing</italic>, <italic>update</italic> and <italic>readout</italic> functions. In other words, one way of defining such functions results in a different model for learning graphs. Furthermore, another attractive property of MPNN is that it allows to learn meaningful representations specific to each task for graphs in an end-to-end manner.</p>
    <p>We propose a powerful machine learning framework for metabolite identification, named ADAPTIVE, which has two subtasks: (i) learning a mapping from structures to molecular vectors and (ii) learning a mapping from spectra to molecular vectors. <xref ref-type="fig" rid="btz319-F2">Figure 2</xref> shows a schematic picture of ADAPTIVE, where the left and right blue boxes correspond to the first and second subtasks, respectively. In Subtask 1, ADAPTIVE learns a model to generate molecular vectors for metabolites using their chemical structures, where these vectors are specific to both data and the task of metabolite identification, and therefore nonredundant. The model in Subtask 1 is parameterized by MPNN for mapping metabolite structures to the molecular vectors. The <italic>main contribution</italic> of this article is in the Subtask 1, that is, to learn the correspondence between given pairs of spectra and structures for metabolites.
</p>
    <fig id="btz319-F2" orientation="portrait" position="float">
      <label>Fig. 2.</label>
      <caption>
        <p>Overview of ADAPTIVE for metabolite identification. ADAPTIVE has two components: (i) Subtask 1: estimates parameters of a function mapping metabolites from structures to molecular vectors, given a set of spectra-structure pairs; (ii) Subtask 2: learns a function mapping from spectra to molecular vectors (generated by Subtask 1), given a set of spectrum-vector pairs</p>
      </caption>
      <graphic xlink:href="btz319f2"/>
    </fig>
    <p>Thus, the parameters of MPNN are trained so that the correlation between the spectra and the vectors mapped from the structures is maximized. We use Hilbert-Schmidt Independence Criterion (HSIC, <xref rid="btz319-B7" ref-type="bibr">Gretton <italic>et al.</italic>, 2005</xref>) for evaluating the correlation, due to its theoretically nice properties and kernel-based calculation. Specifically, we formulate an objective function for the maximization problem through HSIC and solve this problem to have the best molecular vectors adapted to given data. For Subtask 2, ADAPTIVE uses IOKR to learn a mapping from spectra to molecular vectors generated by the Subtask 1.</p>
    <p>We emphasize that the key difference between ADAPTIVE and the original IOKR is that IOKR uses ‘manually designed’ fingerprints, which are large in size, possibly redundant and nonspecific to metabolite identification (and given data), while ADAPTIVE learns representations for metabolites from given data, as molecular vectors, resulting in that the molecular vectors generated by ADAPTIVE are data-driven and concise.</p>
    <p>In order to validate the performance of ADAPTIVE, we conducted extensive experiments using a benchmark data. Experimental results showed the following two main advantages of ADAPTIVE over existing methods, including the original IOKR:
<list list-type="bullet"><list-item><p>Predictive performance</p></list-item></list></p>
    <p>ADAPTIVE achieved the best performance, followed by IOKR, CSI:FingerID and FingerID. For example, the top-20 accuracy of ADAPTIVE was 78.52% with the parameters of Gaussian kernel, ALIGNF and molecular vector size of 300. On the other hand, IOKR, CSI:FingerID and FingerID achieved 74.79%, 73.07% (or 68.20%) and 58.17%, respectively, using Gaussian kernel (for IOKR) and ALIGNF. The top-<italic>k</italic> accuracy was computed by the average over all trials of 10-fold cross-validation (CV), and so the performance advantage of ADAPTIVE was significant and very clear.
<list list-type="bullet"><list-item><p>Computational efficiency for prediction</p></list-item></list></p>
    <p>Under the same experimental setting, ADAPTIVE was four to seven times faster than IOKR, which was already known as the fastest method. We can then say that ADAPTIVE is the current fastest method while keeping the highest predictive performance for metabolite identification.</p>
  </sec>
  <sec>
    <title>2 Related work</title>
    <p>As mentioned in the Introduction section, fingerprint prediction is important in supervised learning for metabolite identification, because we can retrieve metabolite candidates more reliably if fingerprints are predicted more accurately. For fingerprint prediction, kernel learning has been shown to be the most powerful approach. For example, a typical approach, FingerID (<xref rid="btz319-B8" ref-type="bibr">Heinonen <italic>et al.</italic>, 2012</xref>) uses probability product kernel (PPK, <xref rid="btz319-B9" ref-type="bibr">Jebara <italic>et al.</italic>, 2004</xref>), which can be directly computed from spectra and runs support vector machine with this kernel for solving fingerprint prediction as a classification problem. CSI:FingerID (<xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>), an extension of FingerID, uses not only spectra but also fragmentation trees (FTs, <xref rid="btz319-B16" ref-type="bibr">Rasche <italic>et al.</italic>, 2011</xref>) as input to generate kernels over spectra and FTs, which are then combined via multiple kernel learning (MKL, <xref rid="btz319-B6" ref-type="bibr">Gönen and Alpaydin, 2011</xref>). FTs may capture structural information behind spectra which is missing in the approach of FingerID. This is the motivation of CSI:FingerID. However, the computational cost for converting FTs from MS/MS spectra is very expensive, leading to heavy computational load, which causes a problem particularly in prediction. Thus, we can say that kernel-based supervised learning, particularly complex kernels, have a computation issue, regardless of high performance in prediction. On the other hand, a sparse learning model, namely SIMPLE (<xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>), considers a simpler function than kernels for fingerprint, while interactions of peaks in spectra can be incorporated into learning models explicitly. SIMPLE achieved a comparable performance against kernel-based learning, reducing the computational cost drastically. A key point of SIMPLE is to take advantage of sparsity of spectra, which results in faster prediction and interpretability, showing clear advantages over kernel-based methods.</p>
    <p>Among the series of kernel-based approaches, IOKR (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>) has been shown to outperform the previous methods, in terms of both predictive performance and computational speed.</p>
    <p>It learns a mapping from spectra, i.e. input <inline-formula id="IE1"><mml:math id="IM1"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula>, to molecular fingerprints (or structures behind fingerprints), i.e. output <inline-formula id="IE2"><mml:math id="IM2"><mml:mi mathvariant="script">Y</mml:mi></mml:math></inline-formula>. In order to do this mapping, IOKR defines kernels to encode similarities in the input space (e.g. spectra and/or FTs) and the output space (molecular fingerprints or structures). Then, the advantage of IOKR comes from the following two points: (i) unlike previous kernel-based methods, IOKR handles the structured output space by the kernel defined for the output, which improves the predictive performance; (ii) IOKR simultaneously predicts fingerprints rather than considering fingerprint prediction as a set of separate tasks, leading to an efficient computation in prediction. Some part (mapping from spectra to feature vectors) of IOKR is a part of ADAPTIVE, and so further technical details of the corresponding part of IOKR is described more in Section 3.</p>
    <p>Conventionally, molecular fingerprints for fingerprint prediction have been manually designed feature vectors to encode a predefined set of substructures or chemical properties, which are possibly found in metabolites. However, recently, machine learning-based (or data-driven) algorithms for generating fingerprints have been proposed. A typical approach is neural fingerprint (NFP, <xref rid="btz319-B4" ref-type="bibr">Duvenaud <italic>et al.</italic>, 2015</xref>), which takes graphs with arbitrary sizes and shapes as inputs. NFP uses the idea of <italic>graph convolution</italic>, an extension of convolution operation from multidimensional arrays, like images or texts, to graph structures. NFP is then trained in a supervised manner by using available labels, such as log mol/L for solubility, <italic>EC</italic><sub>50</sub> for drug efficacy. Finally, NFP results in fingerprint vectors (for molecules) specific to given task and data. An extension of NFP is for unsupervised (as well as semisupervised) settings to learn representations of molecular graph without labels (<xref rid="btz319-B15" ref-type="bibr">Nguyen <italic>et al.</italic>, 2017</xref>), since label information can be experimentally obtained and precious.</p>
    <p>More recently, <xref rid="btz319-B5" ref-type="bibr">Gilmer <italic>et al.</italic> (2017)</xref> showed that several graph convolution-based models, including NFP, Gated Graph Neural Networks (<xref rid="btz319-B11" ref-type="bibr">Li <italic>et al.</italic>, 2015</xref>), spectral graph convolutional network (<xref rid="btz319-B10" ref-type="bibr">Kipf and Welling, 2016</xref>), etc., can be formulated in an unified model, namely MPNN, with the following three functions: <italic>message passing</italic>, <italic>update</italic> and <italic>readout</italic>. A key advantage of MPNN is that defining the above components generates a proper model for learning graphs, depending on a given task. Also, another advantage of MPNN as well as other neural network-based methods in this paragraph is that they adopt differentiable operations, and thus their parameters can be effectively trained by using a stochastic gradient descent algorithm.</p>
  </sec>
  <sec>
    <title>3 Materials and methods</title>
    <sec>
      <title>3.1 ADAPTIVE: overview</title>
      <p>We first introduce the framework of ADAPTIVE for metabolite identification. This is also the general framework of approaches using machine learning for metabolite identification. It has two subtasks. <italic>Subtask 1</italic>: learning a function which maps metabolites from their structures to molecular vectors and <italic>Subtask 2</italic>: learning a function which maps metabolites from spectra to the vectors generated in Subtask 1. <xref ref-type="fig" rid="btz319-F2">Figure 2</xref> shows an illustration of the entire framework of ADAPTIVE. In this figure, the left and right blue boxes correspond to Subtasks 1 and 2, respectively.</p>
      <p>For Subtask 1, given pairs of metabolite structure-spectrum, we estimate parameters of a function which maps metabolites from their structures to molecular vectors by maximizing the correlation between the vectors mapped from the structures and also the corresponding spectra. In more detail, we model the mapping function by MPNN and evaluate the correlation between the vectors and spectra by using HSIC due to the computational simplicity and provably theoretical properties of HSIC. For Subtask 2, we simply borrow the corresponding part of IOKR to learn a function mapping metabolites from spectra to vectors generated by Subtask 1.</p>
      <p>We explain these two subtasks in the following subsections, being followed by the subsection on kernels we used in ADAPTIVE.</p>
    </sec>
    <sec>
      <title>3.2 Subtask 1: learning molecular vectors for metabolites via HSIC</title>
      <p>For this subtask, we need to estimate a function to map metabolites from structures to molecular vectors, given spectrum-structure pairs. For this problem, we use MPNN as the mapping function, which can extract meaningful representation for graphs (molecules for our problem) by supervised learning from training data. That is, MPNN requires labeled training data, which are, however, unavailable for this subtask. Then we manage this problem by taking advantage of given spectrum-structure pairs. We estimate parameters of MPNN by using the idea of maximizing the correlation between the given spectra and vectors (mapped from structures). The correlation is evaluated by HSIC. We describe the detail of MPNN, HSIC and related optimization procedures in the following subsections.</p>
      <sec>
        <title>3.2.1 Message passing neural network</title>
        <p>MPNN is a framework, which takes graphs of arbitrary sizes and structures as inputs, to learn their representation vectors at different levels (i.e. nodes, subgraphs and the whole graph) in a supervised manner (<xref rid="btz319-B5" ref-type="bibr">Gilmer <italic>et al.</italic>, 2017</xref>). A key advantage is that MPNN allows to learn features specific to the given task from the given data. Below, we explain the procedure of MPNN.</p>
        <p>First let <italic>G</italic> be an undirected graph, and <italic>v</italic> and <italic>vw</italic> be a node (atom in molecules) and an edge (bond in molecules), respectively. Each node <italic>v</italic> is assigned with <italic>state vectors</italic> at different levels, where each level represents a substructure (or subgraph) rooted at the corresponding node, denoted by <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <italic>r</italic> shows a level. We can compute state vector <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as well as <italic>message</italic><inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> in a hierarchical manner, by using the following two functions: <italic>message passing</italic> (1) and <italic>update</italic> (2):
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">vw</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>(</mml:mi><mml:mi>v</mml:mi><mml:mi>)</mml:mi></mml:mrow></mml:math></inline-formula> denotes the set of neighbors of node <italic>v</italic> in graph <italic>G</italic>; <italic>e</italic>(<italic>v</italic>, <italic>w</italic>) indicates the type of edge between two nodes <italic>v</italic> and <italic>w</italic> (this edge type is like a single, double, triple or aromatic bond); <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">vw</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is a (square) weight matrix to be learned, specific to the edge type <italic>e</italic>(<italic>vw</italic>) at the <italic>r</italic>th level; <italic>g</italic> is a nonlinear activation function (e.g. ReLU or sigmoid).</p>
        <p>Intuitively, the <italic>message passing</italic> function (1) on node <italic>v</italic> plays the role of collecting information from the neighbors of node <italic>v</italic> and <italic>update</italic> function (2) on node <italic>v</italic> is to update the state of node <italic>v</italic> based on the collected information and the former state of node <italic>v</italic>. Thus, by applying two functions (1) and (2) multiple times, the updated features at node <italic>v</italic> (e.g. <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>) can be used to represent a certain number of substructures with the root of node <italic>v</italic>. Then, the values for these series of substructures can be used to generate a vector at node <italic>v</italic> with different levels (sizes) of substructures. <xref ref-type="fig" rid="btz319-F3">Figure 3</xref> shows a schematic and illustrative picture of this procedure [<xref ref-type="fig" rid="btz319-F3">Fig. 3</xref> is from <xref rid="btz319-B15" ref-type="bibr">Nguyen <italic>et al.</italic> (2017)</xref>].
</p>
        <fig id="btz319-F3" orientation="portrait" position="float">
          <label>Fig. 3.</label>
          <caption>
            <p>Message passing and update functions are used to represent rooted substructures in a hierarchical manner. At the first level (left-most graph), each node is represented by feature vector, with only information of the node itself. We note that by repeatedly applying message passing and update functions (from left to right), more neighboring information are incorporated. For example, the updated feature (second level) has information on nodes 3 and 5, and then third level has that on nodes 2 to 5. Finally, the whole graph is covered</p>
          </caption>
          <graphic xlink:href="btz319f3"/>
        </fig>
        <p>After obtaining the state vectors of substructures rooted at node <italic>v</italic>, i.e. <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, we have the <italic>readout</italic> phase to combine all vectors at different levels into a single representation vector of the whole molecule (namely, NFPs). <xref ref-type="fig" rid="btz319-F4">Figure 4</xref> shows a schematic picture of summing up the state vectors at different levels. As in <xref rid="btz319-B4" ref-type="bibr">Duvenaud <italic>et al.</italic> (2015)</xref>, we adopt the softmax operation on the states and then perform linear projections (parameterized by different weight matrices <italic>W<sub>r</sub></italic>) and finally sum them up to obtain a single vector over different levels which represents the whole graph. In short, the molecular vector for the entire molecule can be written as following:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>r</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>v</mml:mi></mml:munder><mml:mrow><mml:mtext>softmax</mml:mtext></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
We note that operations are all differentiable with respect to parameters, which makes learning the parameters possible, given an objective function, by a stochastic or minibatch gradient descent algorithm. <xref ref-type="boxed-text" rid="btz319-BOX1">Algorithm 1</xref> shows a pseudocode of the procedure of repeating the <italic>message passing</italic> and <italic>update</italic> functions.</p>
        <fig id="btz319-F4" orientation="portrait" position="float">
          <label>Fig. 4.</label>
          <caption>
            <p>Representation vectors of substructures, which are rooted at nodes, are computed from the input graph by the message passing and update functions. These functions contribute to computing the molecular representation vector of the whole molecule</p>
          </caption>
          <graphic xlink:href="btz319f4"/>
        </fig>
      </sec>
      <sec>
        <title>3.2.2 HSIC-based objective function</title>
        <p>We estimate parameters of MPNN by maximizing the correlation (dependency) between given spectra and molecular vectors. A lot of measures can be used to evaluate and estimate the correlation, while we use HSIC due to its theoretically sound properties. More importantly, estimation of HSIC is based on kernel calculation, which can effectively deal with the uncertainty of peaks in spectra caused by measurement errors.</p>
        <p>Formally, we are given dataset <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are spectrum and molecular structure, respectively, of the <italic>i</italic>th metabolite. First, for the spectra, i.e. <italic>x</italic>, we consider kernels which combine spectra with FTs, namely <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We describe the detail of the kernels for spectra in Section 3.4. Then, given the kernel over <inline-formula id="IE13"><mml:math id="IM13"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula> is fixed, the goal is to learn the function <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>↦</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula id="IE15"><mml:math id="IM15"><mml:mi mathvariant="script">D</mml:mi></mml:math></inline-formula> such that the correlation between the input and output is maximized. The <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the output of MPNN (or molecular vectors) which belongs to space <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The linear kernel function induced by this space can be written as follows:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>〈</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>To evaluate the correlation between spectra and molecular vectors (output of MPNN), we use an unbiased empirical estimate of HSIC (<xref rid="btz319-B7" ref-type="bibr">Gretton <italic>et al.</italic>, 2005</xref>), which can be given as follows:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>uHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:mtext>trace</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>L</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>L</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>L</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the kernel matrix for the set of <italic>n</italic> spectra <inline-formula id="IE19"><mml:math id="IM19"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula> with diagonal elements set to zero; <bold>1</bold><sub><italic>n</italic></sub> is a vector of 1 s of <italic>n</italic> dimensions. Likewise <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where L<sub><italic>n</italic></sub> is the kernel matrix of <italic>n</italic> molecular vectors output by MPNN. By arranging terms in (5), we can rewrite (5) as the objective function to learn parameters as follows:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mtext>uHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>trace</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>where
<disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>
          <boxed-text id="btz319-BOX1" position="float" orientation="portrait">
            <label>Algorithm 1</label>
            <caption>
              <p>Message Passing Neural Network (MPNN).</p>
            </caption>
            <p>1: <bold>Inputs</bold>:</p>
            <p>  minibatch of molecular structures <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, radius <italic>R</italic> weight matrices of edges: <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mn>4</mml:mn><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>,</p>
            <p>  weight matrices of <italic>readout</italic> function: W<sub>1</sub>, W<sub>2</sub>,…, W<sub><italic>R</italic></sub></p>
            <p>2: <bold>Outputs:</bold></p>
            <p>  molecular vectors <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p>3: <bold>for</bold><inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:mi>i</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>B</italic><bold>do</bold></p>
            <p>4:  <bold>for</bold> each atom <italic>v</italic> in <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula><bold>do</bold></p>
            <p>5:   <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>←</mml:mo></mml:mrow></mml:math></inline-formula>initial hidden rep. vector of <italic>v</italic> ▹ atom feature</p>
            <p>6:  <bold>end for</bold></p>
            <p>7:   <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> ▹ Initialize each molecular vector with a zero vector</p>
            <p>8:   <bold>for</bold><inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:mi>r</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>R</italic><bold>do</bold></p>
            <p>9:    <bold>for</bold> each node <italic>v</italic> in <inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula><bold>do</bold></p>
            <p>10:    <inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">vw</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> ▹ <italic>message</italic> function</p>
            <p>11:    <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>   ▹ <italic>update</italic> function</p>
            <p>12:    <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mtext>softmax</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> ▹ <italic>readout</italic> function</p>
            <p>13:    <bold>end for</bold></p>
            <p>14:   <bold>end for</bold></p>
            <p>15: <bold>end for</bold></p>
            <p>16: <inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>B</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></p>
          </boxed-text>
        </p>
        <p>However, directly optimizing (6) is prohibitively expensive in computation, particularly for large-scale data, since the complexity reaches O(<italic>n</italic><sup>2</sup>), both in space and time. In order to overcome this limitation, following <xref rid="btz319-B21" ref-type="bibr">Zhang <italic>et al.</italic> (2018)</xref>, we disjointly divide samples <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> into <italic>n</italic>/<italic>B</italic> blocks with the size of <italic>B</italic>, <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and then apply HSIC on each block independently. An empirical estimate of the unbiased block HSIC can be defined by:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:mtext>ubHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mtext>trace</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where S<sub><italic>b</italic></sub> can be defined by a similar manner to (7), and <inline-formula id="IE36"><mml:math id="IM36"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the kernel matrix for the <italic>b</italic>th block.</p>
        <p>Furthermore, in order to avoid the effect by biased partition of the dataset, following <xref rid="btz319-B20" ref-type="bibr">Yamada <italic>et al.</italic> (2018)</xref>, we repeat shuffling dataset <italic>T</italic> times, compute ubHSIC on each permutation and take the average over them. HSIC by this procedure is known as bagging block HSIC, which can be written as follows:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:mtext>ubHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mtext>trace</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
We use (9) as objective function <italic>J</italic> to learn parameters.</p>
      </sec>
      <sec>
        <title>3.2.3 Optimization algorithm</title>
        <p>An advantage of objective function (9) is that we can use the gradient descent (minibatch gradient descent) for estimating parameters of MPNN. We here explain details on how to conduct the minibatch gradient descent procedure for the HSIC-based loss, which has three steps.</p>
        <p>
          <italic>Step 1: Feed forward and loss calculation.</italic>
        </p>
        <p>For samples of size <italic>n</italic>, at each iteration, we perform random permutation and then split all samples into batches, where the size of each batch is <italic>B</italic>. Batches are sequentially fed into MPNN. The output of MPNN for the <italic>b</italic>th batch at the <italic>t</italic>th iteration is denoted by <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>B</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Then using these outputs, the objective function on the whole samples can be calculated as in (9).</p>
        <p>
          <italic>Step 2: Gradient calculation of the loss layer.</italic>
        </p>
        <p>As we can compute the loss directly with the output of MPNN (i.e. <inline-formula id="IE38"><mml:math id="IM38"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>), we need to compute the gradient of <italic>J</italic> with respect to <inline-formula id="IE39"><mml:math id="IM39"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Suppose that the output of MPNN is already normalized, i.e. <inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:mo>ϕ</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for all <inline-formula id="IE41"><mml:math id="IM41"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">Y</mml:mi></mml:mrow></mml:math></inline-formula>, the gradient can be obtained by the following:
<disp-formula id="E10"><label>(10)</label><mml:math id="M10"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mi>B</mml:mi><mml:mrow><mml:mi mathvariant="italic">Tn</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>
          <italic>Step 3: Gradient calculation of the MPN and weight update.</italic>
        </p>
        <p>Having calculated the gradient of <italic>J</italic>, i.e. (10), the next step is to compute the gradient of <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with respect to model parameters <italic>θ</italic>, namely <inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>θ</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, to update the whole parameters for each batch at each step.</p>
        <p><xref ref-type="boxed-text" rid="btz319-BOX2">Algorithm 2</xref> is a pseudocode of the entire algorithm of learning parameters of MPNN.
</p>
        <p>
          <boxed-text id="btz319-BOX2" position="float" orientation="portrait">
            <label>Algorithm 2</label>
            <caption>
              <p>Learning molecular representation vectors via HSIC.</p>
            </caption>
            <p>1: <bold>Inputs:</bold></p>
            <p>  set <inline-formula id="IE44"><mml:math id="IM44"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> of spectra-structure pairs,</p>
            <p>  <italic>T</italic>: number of iterations, <italic>B</italic>: size of minibatch</p>
            <p>2: <bold>Outputs:</bold></p>
            <p> <inline-formula id="IE45"><mml:math id="IM45"><mml:mrow><mml:mo>θ</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mn>4</mml:mn><mml:mi>R</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo>}</mml:mo><mml:mo>∪</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mtext>set</mml:mtext><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mtext>atoms</mml:mtext><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p>3: <bold>for</bold><inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:mi>t</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>T</italic><bold>do</bold></p>
            <p>4:  <inline-formula id="IE47"><mml:math id="IM47"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>  ▹ shuffled and split</p>
            <p>5: <bold> for</bold><inline-formula id="IE48"><mml:math id="IM48"><mml:mrow><mml:mi>b</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>B</italic><bold>do</bold></p>
            <p>6:   <inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></p>
            <p>7:   <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> ▹ Call Algorithm 1</p>
            <p>8:   <inline-formula id="IE51"><mml:math id="IM51"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is calculated from <inline-formula id="IE52"><mml:math id="IM52"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> by (7)</p>
            <p>9:   <inline-formula id="IE53"><mml:math id="IM53"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>trace</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p>10:   gradient of loss layer <inline-formula id="IE54"><mml:math id="IM54"><mml:mrow><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p>
            <p>11:   <inline-formula id="IE55"><mml:math id="IM55"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>Grad</mml:mtext></mml:mrow></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is calculated by chain rule</p>
            <p>12:   <inline-formula id="IE56"><mml:math id="IM56"><mml:mrow><mml:mo>θ</mml:mo><mml:mo>←</mml:mo><mml:mo>θ</mml:mo><mml:mo>−</mml:mo><mml:mo>γ</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>Grad</mml:mtext></mml:mrow></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> ▹ Update the whole parameters</p>
            <p>13:  <bold>end for</bold></p>
            <p>14: <bold>end for</bold></p>
          </boxed-text>
        </p>
      </sec>
    </sec>
    <sec>
      <title>3.3 Subtask 2: learning a mapping from spectra to molecular vectors by IOKR</title>
      <p>For Subtask 2, we use IOKR. That is, we learn a mapping from spectra to molecular vectors generated in Subtask 1 by using IOKR. Again, we explain two technical reasons why we use IOKR for this mapping below: (i) IOKR allows to incorporate the structures behind outputs, such as feature interactions in molecular vectors, into the learning model, by which the prediction accuracy can be improved. (ii) Furthermore, all features in molecular vectors are predicted simultaneously, which is not like separate tasks in prediction. This leads to faster computation.</p>
      <p>We now present the technical detail of IOKR below, which has two consecutive steps.</p>
      <sec>
        <title>3.3.1 Step 1: Learning spectra-vectors mapping</title>
        <p>Once parameters, i.e. function <italic>ϕ</italic>, are learned, we convert the structures of metabolites into their molecular vectors to obtain a new set of pairs, <inline-formula id="IE57"><mml:math id="IM57"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Now the goal is to find the optimal function <inline-formula id="IE58"><mml:math id="IM58"><mml:mrow><mml:mi>h</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by minimizing the following objective function:
<disp-formula id="E11"><label>(11)</label><mml:math id="M11"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>h</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>λ</italic> (&gt; 0) is a regularization parameter to prevent overfitting and <inline-formula id="IE59"><mml:math id="IM59"><mml:mi mathvariant="script">H</mml:mi></mml:math></inline-formula> is an approximate functional space that contains <italic>h</italic>; <inline-formula id="IE60"><mml:math id="IM60"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a space of molecular vectors of dimension <italic>d</italic>.</p>
        <p>By using the representer theorem in <xref rid="btz319-B12" ref-type="bibr">Micchelli and Pontil (2005)</xref>, optimal solution <inline-formula id="IE61"><mml:math id="IM61"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of (11) can be represented by a linear combination of vector-valued kernels on training set <inline-formula id="IE62"><mml:math id="IM62"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula>:
<disp-formula id="E12"><label>(12)</label><mml:math id="M12"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE63"><mml:math id="IM63"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are vectors in <inline-formula id="IE64"><mml:math id="IM64"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>; <inline-formula id="IE65"><mml:math id="IM65"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is an operator-valued kernel, defined on spectra <inline-formula id="IE66"><mml:math id="IM66"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula>, satisfying certain constraints (see <xref rid="btz319-B12" ref-type="bibr">Micchelli and Pontil, 2005</xref>). As dimensionality <italic>d</italic> of space <inline-formula id="IE67"><mml:math id="IM67"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is finite, the kernel is a matrix with the size of <italic>d </italic>×<italic> d</italic>.</p>
        <p>By replacing <inline-formula id="IE68"><mml:math id="IM68"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in (11) with (12), <inline-formula id="IE69"><mml:math id="IM69"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be estimated in the following:
<disp-formula id="E13"><label>(13)</label><mml:math id="M13"><mml:mrow><mml:mtext>vec</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>λ</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nd</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext>vec</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE70"><mml:math id="IM70"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE71"><mml:math id="IM71"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are both matrices with the size of <italic>d </italic>×<italic> n</italic>, and vec(.) is the vectorization of the input matrix, where the output is a vector obtained by repeatedly stacking each column of the input matrix on the top of the next column.</p>
      </sec>
      <sec>
        <title>3.3.2 Step 2: Candidate retrieval</title>
        <p>Given mapping <inline-formula id="IE72"><mml:math id="IM72"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> learned in Step 1, we now turn to the problem of finding the output metabolite in the database which corresponds to the query spectrum <bold>x</bold>. To this end, we search metabolite <bold>y</bold> in the list of given candidates <inline-formula id="IE73"><mml:math id="IM73"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, such that the squared distance between <inline-formula id="IE74"><mml:math id="IM74"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE75"><mml:math id="IM75"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be minimized:
<disp-formula id="E14"><label>(14)</label><mml:math id="M14"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula></p>
        <p>Considering that the output kernel is normalized and the operator-valued kernel keeps <inline-formula id="IE76"><mml:math id="IM76"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the optimal solution of <inline-formula id="IE77"><mml:math id="IM77"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be estimated as the following:
<disp-formula id="E15"><label>(15)</label><mml:math id="M15"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mi>l</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>λ</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE78"><mml:math id="IM78"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE79"><mml:math id="IM79"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are column vectors.</p>
        <p>Practically, the values given by objective function (15) are used as scores for ranking candidate metabolites in Step 2: candidate retrieval.</p>
      </sec>
    </sec>
    <sec>
      <title>3.4 Kernels</title>
      <p>ADAPTIVE uses kernels for the input and output.</p>
      <sec>
        <title>3.4.1 Kernels for input</title>
        <p>A various types of kernels are already defined and used for the input from MS/MS spectra. These kernels are typically divided into the following two groups: (i) kernels defined for spectra such as PPK (<xref rid="btz319-B9" ref-type="bibr">Jebara <italic>et al.</italic>, 2004</xref>) and (ii) kernels defined for FTs (<xref rid="btz319-B16" ref-type="bibr">Rasche <italic>et al.</italic>, 2011</xref>). Details on these kernels can be found in <xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic> (2015)</xref>.</p>
        <p>In fact, <xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic> (2015)</xref> suggested 24 different input kernels. ADAPTIVE combines these input kernels into a single kernel through MKL (<xref rid="btz319-B6" ref-type="bibr">Gönen and Alpaydin, 2011</xref>). ADAPTIVE uses two options for MKL: (i) UNIMKL (uniform MKL): assigns the same weights to all component kernels, and (ii) ALIGNF: uses weights over kernels to combine. That is, in ALIGNF, weights over component kernels are optimized (trained) by maximizing the centered kernel alignment between the combined kernel and the target kernel defined on the molecular vectors, which generate trained parameters (model).</p>
      </sec>
      <sec>
        <title>3.4.2 Kernels for output</title>
        <p>After learning parameters (model) to generate the molecular vectors for structures, we define kernels for output <inline-formula id="IE80"><mml:math id="IM80"><mml:mi mathvariant="script">Y</mml:mi></mml:math></inline-formula> by directly computing kernels on the corresponding molecular vectors. In our experiments, we consider the following two typical kernels:
<list list-type="bullet"><list-item><p>Linear kernel: <inline-formula id="IE81"><mml:math id="IM81"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>ϕ</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Gaussian kernel: <inline-formula id="IE82"><mml:math id="IM82"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo>γ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>,</p></list-item></list></p>
        <p>where <bold>y</bold> and <inline-formula id="IE83"><mml:math id="IM83"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are molecular structures in <inline-formula id="IE84"><mml:math id="IM84"><mml:mi mathvariant="script">Y</mml:mi></mml:math></inline-formula>.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Experimental results</title>
    <sec>
      <title>4.1 Dataset and evaluation measures</title>
      <p>We used a benchmark dataset in <xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic> (2016)</xref> to evaluate ADAPTIVE and compare with existing methods. The dataset consists of 4138 MS/MS spectra extracted from the GNPS (Global Natural Products Social) public spectra library (<ext-link ext-link-type="uri" xlink:href="https://gnps.ucsd.edu/ProteoSAFe/libraries.jsp">https://gnps.ucsd.edu/ProteoSAFe/libraries.jsp</ext-link>).</p>
      <p>To compare ADAPTIVE with existing methods, we used the same setting for all competing methods. Specifically we used 10-fold CV, and the results are averaged over all 10-folds. The performance was checked by the top-<italic>k</italic> accuracies (where <italic>k </italic>=<italic> </italic>1, 10, 20), which is the ratio of the number of the cases that the true structures are ranked at lower than or equal to <italic>k</italic> to the number of all cases. Also the speed was checked by computation time for prediction, measured by milliseconds per example (ms/example).</p>
      <p>Hyperparameters, such as regularization parameter <italic>λ</italic> and parameter <italic>γ</italic> of the output kernel, were chosen by using leave-one-out CV on each training fold. For prediction in ADAPTIVE, at the retrieval stage, given test example <bold>x</bold>, we computed the molecular vectors of <bold>x</bold>, <inline-formula id="IE85"><mml:math id="IM85"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> [see (11)] and those of all candidates <italic>ϕ</italic>(<bold>y</bold>) (see <xref ref-type="boxed-text" rid="btz319-BOX1">Algorithm 1</xref>). These candidates including the correct molecular structure of test example <bold>x</bold> were ranked, according to their distances to <inline-formula id="IE86"><mml:math id="IM86"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (from the smallest to the highest). These ranked candidates were used for computing the top-<italic>k</italic> accuracy. <xref rid="btz319-T1" ref-type="table">Table 1</xref> shows a set of parameter values, which were used to train MPNN of generating molecular vectors. <italic>State vectors</italic> of identical atoms at the lowest (atomic) level were initialized with the same random vector sampled from the standard normal distribution and updated during the training stage.</p>
      <table-wrap id="btz319-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Parameter values used for experiments</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Notations</th>
              <th align="left" rowspan="1" colspan="1">Parameter</th>
              <th align="left" rowspan="1" colspan="1">Values</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>T</italic>
              </td>
              <td rowspan="1" colspan="1">#epoch</td>
              <td rowspan="1" colspan="1">100</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>B</italic>
              </td>
              <td rowspan="1" colspan="1">Batchsize</td>
              <td rowspan="1" colspan="1">100</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>R</italic>
              </td>
              <td rowspan="1" colspan="1">#updates</td>
              <td rowspan="1" colspan="1">6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>d</italic>
              </td>
              <td rowspan="1" colspan="1">#dim of molecular vectors</td>
              <td rowspan="1" colspan="1">100,200,300</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>m</italic>
              </td>
              <td rowspan="1" colspan="1">#dim of atom feature</td>
              <td rowspan="1" colspan="1">50</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">#atom types</td>
              <td rowspan="1" colspan="1">12 (C, O, N, P, S, etc.)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">#bond types</td>
              <td rowspan="1" colspan="1">4 (single, double, triple, acromatic)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>All experiments were performed on a server with 2.7 GHz Intel Core i5 CPU and 8GB memory. The code was written in Python and Matlab with the support of the Chainer framework (<xref rid="btz319-B114" ref-type="bibr">Tokui <italic>et al.</italic>, 2015</xref>).</p>
    </sec>
    <sec>
      <title>4.2 Performance results</title>
      <sec>
        <title>4.2.1 Predictive performance</title>
        <p>We compared the predictive performances of ADAPTIVE with three existing methods: FingerID (<xref rid="btz319-B8" ref-type="bibr">Heinonen <italic>et al.</italic>, 2012</xref>), CSI:FingerID (<xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>) and IOKR (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>) in terms of the top-<italic>k</italic> accuracy (<italic>k </italic>=<italic> </italic>1, 10 and 20). <xref rid="btz319-T2" ref-type="table">Table 2</xref> shows the top-<italic>k</italic> accuracies of the competing methods with UNIMKL and ALIGNF for MKL and linear and Gaussian kernels for the output kernel, changing <italic>k</italic> from 1 to 20 and also changing the size of fingerprints from 100 to 300 (for ADAPTIVE only). This table first shows that ADAPTIVE achieved the best performance, being followed by IOKR, CSI:FingerID and FingerID. For example, ADAPTIVE with ALIGNF, Gaussian kernel and the fingerprint size of 300 achieved 31.03% for <italic>k </italic>=<italic> </italic>1, while IOKR with ALIGNF and Gaussian kernel was 29.59% and CSI:FingerID with ALIGNF was 28.84% or 24.82%. That of Finger: ID was only 17.74%. Interestingly, for <italic>k </italic>=<italic> </italic>1, the performance advantage of ADAPTIVE against IOKR was rather slight, while <italic>k </italic>=<italic> </italic>10 and 20, ADAPTIVE outperformed IOKR much more clearly, with the difference of around 3–5% under the same condition for the two methods.</p>
        <table-wrap id="btz319-T2" orientation="portrait" position="float">
          <label>Table 2.</label>
          <caption>
            <p>Comparison of the top-<italic>k</italic> accuracy (<italic>k </italic>=<italic> </italic>1, 10 and 20) of FingerID, CSI:FingerID, IOKR and ADAPTIVE</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Method</th>
                <th align="left" rowspan="1" colspan="1">Vec. size</th>
                <th align="left" rowspan="1" colspan="1">MKL</th>
                <th align="left" colspan="3" rowspan="1">Accuracies (mean/SD %)<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">Top 1</th>
                <th align="left" rowspan="1" colspan="1">Top 10</th>
                <th align="left" rowspan="1" colspan="1">Top 20</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">FingerID</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">None</td>
                <td rowspan="1" colspan="1">17.74</td>
                <td rowspan="1" colspan="1">49.59</td>
                <td rowspan="1" colspan="1">58.17</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CSI:FingerID unit</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">24.82</td>
                <td rowspan="1" colspan="1">60.47</td>
                <td rowspan="1" colspan="1">68.20</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CSI:FingerID mod</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">28.84</td>
                <td rowspan="1" colspan="1">66.07</td>
                <td rowspan="1" colspan="1">73.07</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Platt</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">IOKR linear</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.58/2.23</td>
                <td rowspan="1" colspan="1">65.99/2.46</td>
                <td rowspan="1" colspan="1">73.53/2.47</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">28.54/2.54</td>
                <td rowspan="1" colspan="1">65.77/2.39</td>
                <td rowspan="1" colspan="1">73.19/3.11</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ADAPTIVE</td>
                <td rowspan="1" colspan="1">100</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.42/2.83</td>
                <td rowspan="1" colspan="1">70.01/2.79</td>
                <td rowspan="1" colspan="1">77.48/2.98</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">linear</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.19/3.21</td>
                <td rowspan="1" colspan="1">69.52/2.89</td>
                <td rowspan="1" colspan="1">77.64/3.23</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">200</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.57/3.96</td>
                <td rowspan="1" colspan="1">69.38/3.05</td>
                <td rowspan="1" colspan="1">76.95/2.98</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.11/3.45</td>
                <td rowspan="1" colspan="1">69.53/2.52</td>
                <td rowspan="1" colspan="1">77.56/2.43</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">300</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.22/3.47</td>
                <td rowspan="1" colspan="1">70.48/2.72</td>
                <td rowspan="1" colspan="1">78.18/2.67</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">
                  <bold>30.61/3.23</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>70.51/2.52</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>78.23/2.75</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">IOKR Gaussian</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.66/2.34</td>
                <td rowspan="1" colspan="1">66.51/2.87</td>
                <td rowspan="1" colspan="1">73.94/2.54</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.59/2.58</td>
                <td rowspan="1" colspan="1">66.13/2.09</td>
                <td rowspan="1" colspan="1">73.62/1.85</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ADAPTIVE</td>
                <td rowspan="1" colspan="1">100</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.47/3.21</td>
                <td rowspan="1" colspan="1">70.01/2.83</td>
                <td rowspan="1" colspan="1">77.51/2.11</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Gaussian</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.37/3.21</td>
                <td rowspan="1" colspan="1">69.91/2.64</td>
                <td rowspan="1" colspan="1">77.48/2.33</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">200</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.44/3.86</td>
                <td rowspan="1" colspan="1">69.84/2.78</td>
                <td rowspan="1" colspan="1">77.08/2.95</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">28.98/3.32</td>
                <td rowspan="1" colspan="1">69.65/2.71</td>
                <td rowspan="1" colspan="1">77.15/2.74</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">300</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.31/3.48</td>
                <td rowspan="1" colspan="1"><bold>71.10</bold>/2.73</td>
                <td rowspan="1" colspan="1">78.51/2.65</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1"><bold>31.03</bold>/3.40</td>
                <td rowspan="1" colspan="1">70.89/2.74</td>
                <td rowspan="1" colspan="1"><bold>78.52</bold>/2.52</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic>Note</italic>: The highest value (indicating the most accurate prediction) are in boldface for each <italic>k</italic>.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>We used one-sided paired <italic>t</italic>-test to verify if the differences between ADAPTIVE and IOKR are statistically significant. For example, considering the top 10 accuracy with Gaussian kernel and ALIGNF, the calculated <italic>P</italic>-value was <italic>P </italic>=<italic> </italic>0.0012. Since it is less than the significance level of <italic>α</italic>  =  0.01, we can claim the statistical significance of the advantage of ADAPTIVE in terms of the top 10 accuracy over IOKR under Gaussian kernel and ALIGNF. We conclude that the performance advantage of ADAPTIVE was confirmed by checking a larger number of top candidates. Another finding is the performance difference between linear and Gaussian kernels was very slight (almost nothing) for ADAPTIVE under the same other conditions. This is also true with the settings of UNIMKL and ALIGNF, the performance for them was rather the same. However, the size of fingerprints strongly affected the performance in the sense that a larger size of fingerprints achieved a higher performance. In summary, ADAPTIVE clearly outperformed competing methods with, for example, for <italic>k </italic>=<italic> </italic>20, the difference of 3–5%, which is very sizable.</p>
      </sec>
      <sec>
        <title>4.2.2 Computation time for prediction</title>
        <p>IOKR was already shown to be faster than previous kernel-based methods in prediction (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>). Thus, we consider only IOKR as a competing method for examining computational efficiency. <xref rid="btz319-T3" ref-type="table">Table 3</xref> shows the computation time of ADAPTIVE and IOKR with linear and Gaussian kernels for prediction. The computation time was averaged over the 10-fold CV. This table shows that ADAPTIVE was significantly faster than IOKR. Specifically, under both linear and Gaussian kernels, ADAPTIVE with the fingerprint size of 100 was four to seven times faster than IOKR. This is because molecular vectors by ADAPTIVE are much more precise and adaptive to given data than those used in IOKR.</p>
        <table-wrap id="btz319-T3" orientation="portrait" position="float">
          <label>Table 3.</label>
          <caption>
            <p>Computation time for prediction by ADAPTIVE and IOKR</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Method</th>
                <th rowspan="1" colspan="1">Mol. vec. size</th>
                <th colspan="2" rowspan="1">prediction time (ms/example)<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1">Linear</th>
                <th rowspan="1" colspan="1">Gaussian</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">IOKR</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">140.22</td>
                <td rowspan="1" colspan="1">3352.4</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ADAPTIVE</td>
                <td rowspan="1" colspan="1">100</td>
                <td rowspan="1" colspan="1">
                  <bold>20.32</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>802.6</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">200</td>
                <td rowspan="1" colspan="1">39.88</td>
                <td rowspan="1" colspan="1">844.33</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">300</td>
                <td rowspan="1" colspan="1">54.14</td>
                <td rowspan="1" colspan="1">1071.8</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn2">
              <p><italic>Note</italic>: The smallest values (indicating the fastest) were in boldface for linear and Gaussian kernels.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
    </sec>
    <sec>
      <title>4.3 Case study</title>
      <p>To understand the results obtained by ADAPTIVE more, in the obtained molecular vectors, we examined substructures rooted at atoms, which activated several example features most. As shown in Section 3.2.1, each substructure rooted at an atom is represented by a state vector and contributes to computing the molecular vector of the whole molecule. Then, given a feature, we can estimate the contribution of each substructure by simply computing the softmax value from the corresponding state vector. We use these values of substructures as scores to rank substructures to activate the given feature.</p>
      <p><xref ref-type="fig" rid="btz319-F5">Figure 5</xref> shows three example features (#2, #39 and #83). For each feature, we show three substructures with the highest scores (each score is shown above the substructure). The first row shows three substructures which activated feature #2 most. Interestingly, we can see that these substructures share a further smaller, similar group of atoms: O, P and S (highlighted in blue). Similarly, the second row shows three substructures sharing a group of atoms: O and N, where these substructures activated feature #39 most. Also the third row shows substructures which activated feature #83, all having atom: Cl. Thus, <xref ref-type="fig" rid="btz319-F5">Figure 5</xref> shows that each feature of ADAPTIVE is activated by multiple different substructures sharing some similar properties, which must be important in data and probably for prediction. In contrast, each feature in regular molecular fingerprints is activated by only one predefined substructure. In summary, from this case study, learned features of ADAPTIVE are more concise and specific to the task of metabolite identification than regular molecular fingerprints, leading to the advantage of predictive performance and computation time.
</p>
      <fig id="btz319-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Example features (#2, #39 and #83) and their three substructures (and their scores) which activated the corresponding feature most. Note that three substructures of each feature share a similar group (set) of atoms which are shown in blue</p>
        </caption>
        <graphic xlink:href="btz319f5"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>5 Discussion and conclusion</title>
    <p>Supervised learning for metabolite identification uses fingerprints as intermediate representation vectors between spectra and metabolites, while such fixed vectors are too redundant to cover all possible substructures and chemical properties in metabolites, causing limitations in predictive performance and high computational costs. To overcome this problem, we have proposed ADAPTIVE, which generates representations of metabolites specific to given spectrum-structure pairs. ADAPTIVE learns a model to generate molecular vectors for metabolites, which is parameterized by a MPNN over given molecular structures and trained through optimizing the objective function to maximize the correlation between molecular vectors and corresponding spectra. Our empirical validation of ADAPTIVE with the benchmark dataset showed the advantage of ADAPTIVE over existing methods including IOKR, the current cutting-edge method, both in predictive performance and computation time for prediction.</p>
    <p>A drawback of ADAPTIVE would be interpretability, because structural information is implicitly encoded in compact vectors in ADAPTIVE and cannot be made explicit easily. In metabolite identification, it would be desirable to connect the set of peaks to the corresponding substructures/chemical properties of metabolites (<xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>). Developing a model with such interpretability would be interesting future work.</p>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Funding</title>
    <p>D.H.N. has been supported in part by Otsuka Toshimi scholarship and JSPS KAKENHI [grant number 19J14714]. C.H.N. has been supported in part by MEXT Kakenhi 18K11434. H.M. has been supported in part by JST ACCEL [grant number JPMJAC1503], MEXT Kakenhi [grant numbers 16H02868 and 19H04169], FiDiPro by Tekes (currently Business Finland) and AIPSE program by Academy of Finland.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz319-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brouard</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Fast metabolite identification with input output kernel regression</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>i28</fpage>–<lpage>i36</lpage>.<pub-id pub-id-type="pmid">27307628</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B2">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>de Hoffmann</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Stroobant</surname><given-names>V.</given-names></name></person-group> (<year>2007</year>). <source>Mass Spectrometry, Principles and Applications</source>. <edition>3</edition>rd edn. 
<publisher-name>John Wiley &amp; Sons</publisher-name>, Hoboken, New York.</mixed-citation>
    </ref>
    <ref id="btz319-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dührkop</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Searching molecular structure databases with tandem mass spectra using CSI:FingerID</article-title>. <source>Proc. Natl. Acad. Sci</source>., <volume>112</volume>, <fpage>12580</fpage>–<lpage>12585</lpage>.<pub-id pub-id-type="pmid">26392543</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Duvenaud</surname><given-names>D.K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>). <chapter-title>Convolutional networks on graphs for learning molecular fingerprints</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Cortes</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Lawrence</surname><given-names>N.D.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>D.D.</given-names></name>, <name name-style="western"><surname>Sugiyama</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Garnett</surname><given-names>R.</given-names></name></person-group> (eds.) <source>Proceedings of the 28th International Conference on Neural Information Processing Systems</source>, Vol. 2. 
<publisher-name>Curran Associates, Inc</publisher-name>, Montreal, Canada, pp. <fpage>2224</fpage>–<lpage>2232</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gilmer</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>). <chapter-title>Neural message passing for quantum chemistry</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Precup</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Teh</surname><given-names>Y.W.</given-names></name></person-group> (eds.) <source>Proceedings of the 34th International Conference on Machine Learning, Volume 70 of Proceedings of Machine Learning Research</source>. 
<publisher-name>International Convention Centre, PMLR</publisher-name>, 
<publisher-loc>Sydney, Australia</publisher-loc>, pp. <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gönen</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Alpaydin</surname><given-names>E.</given-names></name></person-group> (<year>2011</year>) 
<article-title>Multiple kernel learning algorithms</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2211</fpage>–<lpage>2268</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gretton</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>). <chapter-title>Measuring statistical dependence with Hilbert-Schmidt norms</chapter-title> In: <source>Proceedings of the 16th International Conference on Algorithmic Learning Theory, ALT’05</source>. 
<publisher-name>Springer-Verlag</publisher-name>, 
<publisher-loc>Berlin, Heidelberg</publisher-loc>, pp. <fpage>63</fpage>–<lpage>77</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heinonen</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Metabolite identification and molecular fingerprint prediction through machine learning</article-title>. <source>Bioinformatics</source>, <volume>28</volume>, <fpage>2333</fpage>–<lpage>2341</lpage>.<pub-id pub-id-type="pmid">22815355</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jebara</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2004</year>) 
<article-title>Probability product kernels</article-title>. <source>J. Mach. Learn. Res</source>., <volume>5</volume>, <fpage>819</fpage>–<lpage>844</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kipf</surname><given-names>T.N.</given-names></name>, <name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). Semi-supervised classification with graph convolutional networks. <italic>arXiv preprint arXiv: 1609.02907</italic>.</mixed-citation>
    </ref>
    <ref id="btz319-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Gated graph sequence neural networks</article-title>. <source>CoRR</source>, abs/1511.05493.</mixed-citation>
    </ref>
    <ref id="btz319-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Micchelli</surname><given-names>C.A.</given-names></name>, <name name-style="western"><surname>Pontil</surname><given-names>M.A.</given-names></name></person-group> (<year>2005</year>) 
<article-title>On learning vector-valued functions</article-title>. <source>Neural Comput</source>., <volume>17</volume>, <fpage>177</fpage>–<lpage>204</lpage>.<pub-id pub-id-type="pmid">15563752</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B13">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>D.H.</given-names></name></person-group><etal>et al</etal> (<year>2018a</year>). Recent advances and prospects of computational methods for metabolite identification: a review with emphasis on machine learning approaches. <italic>Brief. Bioinf.</italic> doi: 10.1093/bib/bby066. [Epub ahead of print].</mixed-citation>
    </ref>
    <ref id="btz319-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>D.H.</given-names></name></person-group><etal>et al</etal> (<year>2018b</year>) 
<article-title>Simple: sparse interaction model over peaks of molecules for fast, interpretable metabolite identification from tandem mass spectra</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i323</fpage>–<lpage>i332</lpage>.<pub-id pub-id-type="pmid">29950009</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>). Semi-supervised learning of hierarchical representations of molecules using neural message passing. <italic>CoRR</italic>, abs/1711.10168.</mixed-citation>
    </ref>
    <ref id="btz319-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rasche</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Computing fragmentation trees from tandem mass spectrometry data</article-title>. <source>Anal. Chem</source>., <volume>83</volume>, <fpage>1243</fpage>–<lpage>1251</lpage>. PMID: 21182243.<pub-id pub-id-type="pmid">21182243</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B114">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tokui</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Chainer: a next-generation open source framework for deep learning</article-title>. In: <source>Proceedings of Workshop on Machine Learning Systems (LearningSys) in the Twenty-Ninth Annual Conference on Neural Information Processing Systems (NIPS)</source>, Vol. 5, pp. 1–6.</mixed-citation>
    </ref>
    <ref id="btz319-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vaniya</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Fiehn</surname><given-names>O.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Using fragmentation trees and mass spectral trees for identifying unknown compounds in metabolomics</article-title>. <source>Trends Analyt. Chem</source>., <volume>69</volume>, <fpage>52</fpage>–<lpage>61</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wishart</surname><given-names>D.S.</given-names></name></person-group> (<year>2007</year>) 
<article-title>Current progress in computational metabolomics</article-title>. <source>Brief. Bioinf</source>., <volume>8</volume>, <fpage>279</fpage>–<lpage>293</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wishart</surname><given-names>D.S.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>HMDB 3.0—the human metabolome database in 2013</article-title>. <source>Nucleic Acids Res</source>., <volume>41</volume>, <fpage>D801</fpage>–<lpage>D807</lpage>.<pub-id pub-id-type="pmid">23161693</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Yamada</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>). Post selection inference with kernels. In: Storkey,A. and Perez-Cruz,F. (eds.) <italic>Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research</italic>. Playa Blanca, PMLR, Lanzarote, Canary Islands, pp. 152–160.</mixed-citation>
    </ref>
    <ref id="btz319-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Large-scale kernel methods for independence testing</article-title>. <source>Stat. Comput</source>., <volume>28</volume>, <fpage>113</fpage>–<lpage>130</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6612897</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz319</article-id>
    <article-id pub-id-type="publisher-id">btz319</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2019 Conference Proceedings</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>General Computational Biology</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ADAPTIVE: leArning DAta-dePendenT, concIse molecular VEctors for fast, accurate metabolite identification from tandem mass spectra</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Nguyen</surname>
          <given-names>Dai Hai</given-names>
        </name>
        <xref ref-type="aff" rid="btz319-aff1">1</xref>
        <xref ref-type="corresp" rid="btz319-cor1"/>
        <!--<email>hai@kuicr.kyoto-u.ac.jp</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nguyen</surname>
          <given-names>Canh Hao</given-names>
        </name>
        <xref ref-type="aff" rid="btz319-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mamitsuka</surname>
          <given-names>Hiroshi</given-names>
        </name>
        <xref ref-type="aff" rid="btz319-aff1">1</xref>
        <xref ref-type="aff" rid="btz319-aff2">2</xref>
      </contrib>
    </contrib-group>
    <aff id="btz319-aff1"><label>1</label>Bioinformatics Center, Institute for Chemical Research, Kyoto University, Uji, Japan</aff>
    <aff id="btz319-aff2"><label>2</label>Department of Computer Science, Aalto University, Espoo, Finland</aff>
    <author-notes>
      <corresp id="btz319-cor1">To whom correspondence should be addressed. <email>hai@kuicr.kyoto-u.ac.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-07-05">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>14</issue>
    <fpage>i164</fpage>
    <lpage>i172</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz319.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Metabolite identification is an important task in metabolomics to enhance the knowledge of biological systems. There have been a number of machine learning-based methods proposed for this task, which predict a chemical structure of a given spectrum through an intermediate (chemical structure) representation called molecular fingerprints. They usually have two steps: (i) predicting fingerprints from spectra; (ii) searching chemical compounds (in database) corresponding to the predicted fingerprints. Fingerprints are feature vectors, which are usually very large to cover all possible substructures and chemical properties, and therefore heavily redundant, in the sense of having many molecular (sub)structures irrelevant to the task, causing limited predictive performance and slow prediction.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We propose ADAPTIVE, which has two parts: learning two mappings (i) from structures to molecular vectors and (ii) from spectra to molecular vectors. The first part learns molecular vectors for metabolites from given data, to be consistent with both spectra and chemical structures of metabolites. In more detail, molecular vectors are generated by a model, being parameterized by a message passing neural network, and parameters are estimated by maximizing the correlation between molecular vectors and the corresponding spectra in terms of Hilbert-Schmidt Independence Criterion. Molecular vectors generated by this model are compact and importantly adaptive (specific) to both given data and task of metabolite identification. The second part uses input output kernel regression (IOKR), the current cutting-edge method of metabolite identification. We empirically confirmed the effectiveness of ADAPTIVE by using a benchmark data, where ADAPTIVE outperformed the original IOKR in both predictive performance and computational efficiency.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The code will be accessed through <ext-link ext-link-type="uri" xlink:href="http://www.bic.kyoto-u.ac.jp/pathway/tools/ADAPTIVE">http://www.bic.kyoto-u.ac.jp/pathway/tools/ADAPTIVE</ext-link> after the acceptance of this article.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">JSPS</named-content>
          <named-content content-type="funder-identifier">10.13039/501100001691</named-content>
        </funding-source>
        <award-id>19J14714</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">MEXT Kakenhi</named-content>
        </funding-source>
        <award-id>18K11434</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">JST ACCEL</named-content>
        </funding-source>
        <award-id>JPMJAC1503</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">MEXT Kakenhi</named-content>
        </funding-source>
        <award-id>16H02868</award-id>
        <award-id>19H04169</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Metabolites are small molecules, having many important functions in living cells such as energy transport, signaling, building blocks of cells and so on (<xref rid="btz319-B18" ref-type="bibr">Wishart, 2007</xref>). Identifying their biochemical characteristics or so-called metabolite identification is an essential task in metabolomics to increase the knowledge of biological systems. Yet, it is still a challenging task due to the size or coverage of spectra libraries.</p>
    <p>Mass spectrometry (MS) is one of the most common techniques in analytical chemistry for dealing with metabolite identification (<xref rid="btz319-B2" ref-type="bibr">de Hoffmann and Stroobant, 2007</xref>). In more detail, a chemical compound is decomposed into fragments, of which mass-to-charge ratios (m/z) are continuously measured to obtain a mass spectrum. One MS spectrum can be represented by a list of peaks, each of which corresponds to a fragment captured by MS. <xref ref-type="fig" rid="btz319-F1">Figure 1</xref> shows a real example of a MS spectrum. In practice, tandem MS (also known as MS/MS or MS2) is widely used, in which precursor ions of specific m/z values from MS spectra are selected and further fragmented to produce other groups of product ions
[see, e.g. <xref rid="btz319-B17" ref-type="bibr">Vaniya and Fiehn (2015)</xref> for more details]. The MS/MS spectra provide structural information about the measured compound, which makes MS/MS more useful for tackling metabolite identification.</p>
    <fig id="btz319-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>Example MS spectrum from Human Metabolome Database (<xref rid="btz319-B19" ref-type="bibr">Wishart <italic>et al.</italic> 2013</xref>) for 1-Methylhistidine (HMBD00001), with the corresponding chemical structure (top-left) and peak list (top-right)</p>
      </caption>
      <graphic xlink:href="btz319f1"/>
    </fig>
    <p>A number of computational methods have been proposed for identifying unknown metabolites from MS/MS spectra data. In general, they are classified into three main categories: (i) spectral library search; (ii) <italic>in silico</italic> fragmentation; and (iii) machine learning (<xref rid="btz319-B13" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018a</xref>). Recent advances in metabolite identification have been led by the machine learning category (e.g. <xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>; <xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>; <xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>). This category can be further divided into two key groups: supervised learning for substructure prediction and unsupervised learning for substructure annotation. While the former is to find a mapping from inputs (e.g. spectra) to outputs (e.g. fingerprints), the latter extracts underlying substructures of metabolites. Our research focuses on supervised learning, where the common scheme is to learn a mapping from spectra to structures.</p>
    <p>The prediction can be divided into two steps: (i) fingerprint prediction: predicting fingerprints of a given test spectrum with supervised learning; (ii) candidate retrieval: retrieving chemical compound (from database) which is closest to the predicted fingerprints (<xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>).</p>
    <p>Kernel methods have been shown to be effective for fingerprint prediction, such as methods include FingerID (<xref rid="btz319-B8" ref-type="bibr">Heinonen <italic>et al.</italic>, 2012</xref>), CSI:FingerID (<xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>) and input output kernel regression (IOKR, <xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>). In particular, IOKR is recognized as the current cutting-edge method for metabolite identification due to the following advantages: (i) structures (e.g. feature interaction in the molecular fingerprint vectors) in the output can be incorporated into the learning model by the kernel defined in the output space, leading to accuracy improvement; (ii) fingerprints are simultaneously predicted by the learned model, rather than being considered as a set of separate tasks, resulting in faster computation. One can take structures of the metabolites into account by using graph kernels (path, shortest-path and graphlet kernels) or kernels defined on molecular fingerprints. It is also known that kernels based on fingerprint vectors obtained the best performance (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>). However, a limitation of using molecular fingerprints as the intermediate representation vectors is that they are general-purpose and very large in size to encode all possible substructures and chemical properties related with metabolites. Consequently, such vectors are neither necessarily specific to any task nor data, and therefore redundant in the sense that these vectors might contain information irrelevant to the task, resulting in limited predictive performance. Moreover, the large size of fingerprints causes slow prediction in the first step of the above two steps.</p>
    <p>Generally, in machine learning, deep learning has been proven successful recently in many application domains. Deep learning is useful for regular data, say a table, in which rows are instances and columns are features, and vice versa. However, semistructured data, particularly graphs, for example, chemical (or biological) molecules, which are irregular types of data, are difficult to be used with deep learning. A number of research efforts have been devoted to applying deep learning to semistructured data, proposing models to learn representations of graphs, such as <xref rid="btz319-B4" ref-type="bibr">Duvenaud <italic>et al.</italic> (2015)</xref>, <xref rid="btz319-B11" ref-type="bibr">Li <italic>et al.</italic> (2015)</xref> and <xref rid="btz319-B15" ref-type="bibr">Nguyen <italic>et al.</italic> (2017)</xref>. Importantly, <xref rid="btz319-B5" ref-type="bibr">Gilmer <italic>et al.</italic> (2017)</xref> showed that a lot of research on graphs can be formulated in a unified model, namely message passing neural network (MPNN), with the following three components: <italic>message passing</italic>, <italic>update</italic> and <italic>readout</italic> functions. In other words, one way of defining such functions results in a different model for learning graphs. Furthermore, another attractive property of MPNN is that it allows to learn meaningful representations specific to each task for graphs in an end-to-end manner.</p>
    <p>We propose a powerful machine learning framework for metabolite identification, named ADAPTIVE, which has two subtasks: (i) learning a mapping from structures to molecular vectors and (ii) learning a mapping from spectra to molecular vectors. <xref ref-type="fig" rid="btz319-F2">Figure 2</xref> shows a schematic picture of ADAPTIVE, where the left and right blue boxes correspond to the first and second subtasks, respectively. In Subtask 1, ADAPTIVE learns a model to generate molecular vectors for metabolites using their chemical structures, where these vectors are specific to both data and the task of metabolite identification, and therefore nonredundant. The model in Subtask 1 is parameterized by MPNN for mapping metabolite structures to the molecular vectors. The <italic>main contribution</italic> of this article is in the Subtask 1, that is, to learn the correspondence between given pairs of spectra and structures for metabolites.
</p>
    <fig id="btz319-F2" orientation="portrait" position="float">
      <label>Fig. 2.</label>
      <caption>
        <p>Overview of ADAPTIVE for metabolite identification. ADAPTIVE has two components: (i) Subtask 1: estimates parameters of a function mapping metabolites from structures to molecular vectors, given a set of spectra-structure pairs; (ii) Subtask 2: learns a function mapping from spectra to molecular vectors (generated by Subtask 1), given a set of spectrum-vector pairs</p>
      </caption>
      <graphic xlink:href="btz319f2"/>
    </fig>
    <p>Thus, the parameters of MPNN are trained so that the correlation between the spectra and the vectors mapped from the structures is maximized. We use Hilbert-Schmidt Independence Criterion (HSIC, <xref rid="btz319-B7" ref-type="bibr">Gretton <italic>et al.</italic>, 2005</xref>) for evaluating the correlation, due to its theoretically nice properties and kernel-based calculation. Specifically, we formulate an objective function for the maximization problem through HSIC and solve this problem to have the best molecular vectors adapted to given data. For Subtask 2, ADAPTIVE uses IOKR to learn a mapping from spectra to molecular vectors generated by the Subtask 1.</p>
    <p>We emphasize that the key difference between ADAPTIVE and the original IOKR is that IOKR uses ‘manually designed’ fingerprints, which are large in size, possibly redundant and nonspecific to metabolite identification (and given data), while ADAPTIVE learns representations for metabolites from given data, as molecular vectors, resulting in that the molecular vectors generated by ADAPTIVE are data-driven and concise.</p>
    <p>In order to validate the performance of ADAPTIVE, we conducted extensive experiments using a benchmark data. Experimental results showed the following two main advantages of ADAPTIVE over existing methods, including the original IOKR:
<list list-type="bullet"><list-item><p>Predictive performance</p></list-item></list></p>
    <p>ADAPTIVE achieved the best performance, followed by IOKR, CSI:FingerID and FingerID. For example, the top-20 accuracy of ADAPTIVE was 78.52% with the parameters of Gaussian kernel, ALIGNF and molecular vector size of 300. On the other hand, IOKR, CSI:FingerID and FingerID achieved 74.79%, 73.07% (or 68.20%) and 58.17%, respectively, using Gaussian kernel (for IOKR) and ALIGNF. The top-<italic>k</italic> accuracy was computed by the average over all trials of 10-fold cross-validation (CV), and so the performance advantage of ADAPTIVE was significant and very clear.
<list list-type="bullet"><list-item><p>Computational efficiency for prediction</p></list-item></list></p>
    <p>Under the same experimental setting, ADAPTIVE was four to seven times faster than IOKR, which was already known as the fastest method. We can then say that ADAPTIVE is the current fastest method while keeping the highest predictive performance for metabolite identification.</p>
  </sec>
  <sec>
    <title>2 Related work</title>
    <p>As mentioned in the Introduction section, fingerprint prediction is important in supervised learning for metabolite identification, because we can retrieve metabolite candidates more reliably if fingerprints are predicted more accurately. For fingerprint prediction, kernel learning has been shown to be the most powerful approach. For example, a typical approach, FingerID (<xref rid="btz319-B8" ref-type="bibr">Heinonen <italic>et al.</italic>, 2012</xref>) uses probability product kernel (PPK, <xref rid="btz319-B9" ref-type="bibr">Jebara <italic>et al.</italic>, 2004</xref>), which can be directly computed from spectra and runs support vector machine with this kernel for solving fingerprint prediction as a classification problem. CSI:FingerID (<xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>), an extension of FingerID, uses not only spectra but also fragmentation trees (FTs, <xref rid="btz319-B16" ref-type="bibr">Rasche <italic>et al.</italic>, 2011</xref>) as input to generate kernels over spectra and FTs, which are then combined via multiple kernel learning (MKL, <xref rid="btz319-B6" ref-type="bibr">Gönen and Alpaydin, 2011</xref>). FTs may capture structural information behind spectra which is missing in the approach of FingerID. This is the motivation of CSI:FingerID. However, the computational cost for converting FTs from MS/MS spectra is very expensive, leading to heavy computational load, which causes a problem particularly in prediction. Thus, we can say that kernel-based supervised learning, particularly complex kernels, have a computation issue, regardless of high performance in prediction. On the other hand, a sparse learning model, namely SIMPLE (<xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>), considers a simpler function than kernels for fingerprint, while interactions of peaks in spectra can be incorporated into learning models explicitly. SIMPLE achieved a comparable performance against kernel-based learning, reducing the computational cost drastically. A key point of SIMPLE is to take advantage of sparsity of spectra, which results in faster prediction and interpretability, showing clear advantages over kernel-based methods.</p>
    <p>Among the series of kernel-based approaches, IOKR (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>) has been shown to outperform the previous methods, in terms of both predictive performance and computational speed.</p>
    <p>It learns a mapping from spectra, i.e. input <inline-formula id="IE1"><mml:math id="IM1"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula>, to molecular fingerprints (or structures behind fingerprints), i.e. output <inline-formula id="IE2"><mml:math id="IM2"><mml:mi mathvariant="script">Y</mml:mi></mml:math></inline-formula>. In order to do this mapping, IOKR defines kernels to encode similarities in the input space (e.g. spectra and/or FTs) and the output space (molecular fingerprints or structures). Then, the advantage of IOKR comes from the following two points: (i) unlike previous kernel-based methods, IOKR handles the structured output space by the kernel defined for the output, which improves the predictive performance; (ii) IOKR simultaneously predicts fingerprints rather than considering fingerprint prediction as a set of separate tasks, leading to an efficient computation in prediction. Some part (mapping from spectra to feature vectors) of IOKR is a part of ADAPTIVE, and so further technical details of the corresponding part of IOKR is described more in Section 3.</p>
    <p>Conventionally, molecular fingerprints for fingerprint prediction have been manually designed feature vectors to encode a predefined set of substructures or chemical properties, which are possibly found in metabolites. However, recently, machine learning-based (or data-driven) algorithms for generating fingerprints have been proposed. A typical approach is neural fingerprint (NFP, <xref rid="btz319-B4" ref-type="bibr">Duvenaud <italic>et al.</italic>, 2015</xref>), which takes graphs with arbitrary sizes and shapes as inputs. NFP uses the idea of <italic>graph convolution</italic>, an extension of convolution operation from multidimensional arrays, like images or texts, to graph structures. NFP is then trained in a supervised manner by using available labels, such as log mol/L for solubility, <italic>EC</italic><sub>50</sub> for drug efficacy. Finally, NFP results in fingerprint vectors (for molecules) specific to given task and data. An extension of NFP is for unsupervised (as well as semisupervised) settings to learn representations of molecular graph without labels (<xref rid="btz319-B15" ref-type="bibr">Nguyen <italic>et al.</italic>, 2017</xref>), since label information can be experimentally obtained and precious.</p>
    <p>More recently, <xref rid="btz319-B5" ref-type="bibr">Gilmer <italic>et al.</italic> (2017)</xref> showed that several graph convolution-based models, including NFP, Gated Graph Neural Networks (<xref rid="btz319-B11" ref-type="bibr">Li <italic>et al.</italic>, 2015</xref>), spectral graph convolutional network (<xref rid="btz319-B10" ref-type="bibr">Kipf and Welling, 2016</xref>), etc., can be formulated in an unified model, namely MPNN, with the following three functions: <italic>message passing</italic>, <italic>update</italic> and <italic>readout</italic>. A key advantage of MPNN is that defining the above components generates a proper model for learning graphs, depending on a given task. Also, another advantage of MPNN as well as other neural network-based methods in this paragraph is that they adopt differentiable operations, and thus their parameters can be effectively trained by using a stochastic gradient descent algorithm.</p>
  </sec>
  <sec>
    <title>3 Materials and methods</title>
    <sec>
      <title>3.1 ADAPTIVE: overview</title>
      <p>We first introduce the framework of ADAPTIVE for metabolite identification. This is also the general framework of approaches using machine learning for metabolite identification. It has two subtasks. <italic>Subtask 1</italic>: learning a function which maps metabolites from their structures to molecular vectors and <italic>Subtask 2</italic>: learning a function which maps metabolites from spectra to the vectors generated in Subtask 1. <xref ref-type="fig" rid="btz319-F2">Figure 2</xref> shows an illustration of the entire framework of ADAPTIVE. In this figure, the left and right blue boxes correspond to Subtasks 1 and 2, respectively.</p>
      <p>For Subtask 1, given pairs of metabolite structure-spectrum, we estimate parameters of a function which maps metabolites from their structures to molecular vectors by maximizing the correlation between the vectors mapped from the structures and also the corresponding spectra. In more detail, we model the mapping function by MPNN and evaluate the correlation between the vectors and spectra by using HSIC due to the computational simplicity and provably theoretical properties of HSIC. For Subtask 2, we simply borrow the corresponding part of IOKR to learn a function mapping metabolites from spectra to vectors generated by Subtask 1.</p>
      <p>We explain these two subtasks in the following subsections, being followed by the subsection on kernels we used in ADAPTIVE.</p>
    </sec>
    <sec>
      <title>3.2 Subtask 1: learning molecular vectors for metabolites via HSIC</title>
      <p>For this subtask, we need to estimate a function to map metabolites from structures to molecular vectors, given spectrum-structure pairs. For this problem, we use MPNN as the mapping function, which can extract meaningful representation for graphs (molecules for our problem) by supervised learning from training data. That is, MPNN requires labeled training data, which are, however, unavailable for this subtask. Then we manage this problem by taking advantage of given spectrum-structure pairs. We estimate parameters of MPNN by using the idea of maximizing the correlation between the given spectra and vectors (mapped from structures). The correlation is evaluated by HSIC. We describe the detail of MPNN, HSIC and related optimization procedures in the following subsections.</p>
      <sec>
        <title>3.2.1 Message passing neural network</title>
        <p>MPNN is a framework, which takes graphs of arbitrary sizes and structures as inputs, to learn their representation vectors at different levels (i.e. nodes, subgraphs and the whole graph) in a supervised manner (<xref rid="btz319-B5" ref-type="bibr">Gilmer <italic>et al.</italic>, 2017</xref>). A key advantage is that MPNN allows to learn features specific to the given task from the given data. Below, we explain the procedure of MPNN.</p>
        <p>First let <italic>G</italic> be an undirected graph, and <italic>v</italic> and <italic>vw</italic> be a node (atom in molecules) and an edge (bond in molecules), respectively. Each node <italic>v</italic> is assigned with <italic>state vectors</italic> at different levels, where each level represents a substructure (or subgraph) rooted at the corresponding node, denoted by <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <italic>r</italic> shows a level. We can compute state vector <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as well as <italic>message</italic><inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> in a hierarchical manner, by using the following two functions: <italic>message passing</italic> (1) and <italic>update</italic> (2):
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">vw</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>(</mml:mi><mml:mi>v</mml:mi><mml:mi>)</mml:mi></mml:mrow></mml:math></inline-formula> denotes the set of neighbors of node <italic>v</italic> in graph <italic>G</italic>; <italic>e</italic>(<italic>v</italic>, <italic>w</italic>) indicates the type of edge between two nodes <italic>v</italic> and <italic>w</italic> (this edge type is like a single, double, triple or aromatic bond); <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">vw</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is a (square) weight matrix to be learned, specific to the edge type <italic>e</italic>(<italic>vw</italic>) at the <italic>r</italic>th level; <italic>g</italic> is a nonlinear activation function (e.g. ReLU or sigmoid).</p>
        <p>Intuitively, the <italic>message passing</italic> function (1) on node <italic>v</italic> plays the role of collecting information from the neighbors of node <italic>v</italic> and <italic>update</italic> function (2) on node <italic>v</italic> is to update the state of node <italic>v</italic> based on the collected information and the former state of node <italic>v</italic>. Thus, by applying two functions (1) and (2) multiple times, the updated features at node <italic>v</italic> (e.g. <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>) can be used to represent a certain number of substructures with the root of node <italic>v</italic>. Then, the values for these series of substructures can be used to generate a vector at node <italic>v</italic> with different levels (sizes) of substructures. <xref ref-type="fig" rid="btz319-F3">Figure 3</xref> shows a schematic and illustrative picture of this procedure [<xref ref-type="fig" rid="btz319-F3">Fig. 3</xref> is from <xref rid="btz319-B15" ref-type="bibr">Nguyen <italic>et al.</italic> (2017)</xref>].
</p>
        <fig id="btz319-F3" orientation="portrait" position="float">
          <label>Fig. 3.</label>
          <caption>
            <p>Message passing and update functions are used to represent rooted substructures in a hierarchical manner. At the first level (left-most graph), each node is represented by feature vector, with only information of the node itself. We note that by repeatedly applying message passing and update functions (from left to right), more neighboring information are incorporated. For example, the updated feature (second level) has information on nodes 3 and 5, and then third level has that on nodes 2 to 5. Finally, the whole graph is covered</p>
          </caption>
          <graphic xlink:href="btz319f3"/>
        </fig>
        <p>After obtaining the state vectors of substructures rooted at node <italic>v</italic>, i.e. <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, we have the <italic>readout</italic> phase to combine all vectors at different levels into a single representation vector of the whole molecule (namely, NFPs). <xref ref-type="fig" rid="btz319-F4">Figure 4</xref> shows a schematic picture of summing up the state vectors at different levels. As in <xref rid="btz319-B4" ref-type="bibr">Duvenaud <italic>et al.</italic> (2015)</xref>, we adopt the softmax operation on the states and then perform linear projections (parameterized by different weight matrices <italic>W<sub>r</sub></italic>) and finally sum them up to obtain a single vector over different levels which represents the whole graph. In short, the molecular vector for the entire molecule can be written as following:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>r</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>v</mml:mi></mml:munder><mml:mrow><mml:mtext>softmax</mml:mtext></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
We note that operations are all differentiable with respect to parameters, which makes learning the parameters possible, given an objective function, by a stochastic or minibatch gradient descent algorithm. <xref ref-type="boxed-text" rid="btz319-BOX1">Algorithm 1</xref> shows a pseudocode of the procedure of repeating the <italic>message passing</italic> and <italic>update</italic> functions.</p>
        <fig id="btz319-F4" orientation="portrait" position="float">
          <label>Fig. 4.</label>
          <caption>
            <p>Representation vectors of substructures, which are rooted at nodes, are computed from the input graph by the message passing and update functions. These functions contribute to computing the molecular representation vector of the whole molecule</p>
          </caption>
          <graphic xlink:href="btz319f4"/>
        </fig>
      </sec>
      <sec>
        <title>3.2.2 HSIC-based objective function</title>
        <p>We estimate parameters of MPNN by maximizing the correlation (dependency) between given spectra and molecular vectors. A lot of measures can be used to evaluate and estimate the correlation, while we use HSIC due to its theoretically sound properties. More importantly, estimation of HSIC is based on kernel calculation, which can effectively deal with the uncertainty of peaks in spectra caused by measurement errors.</p>
        <p>Formally, we are given dataset <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are spectrum and molecular structure, respectively, of the <italic>i</italic>th metabolite. First, for the spectra, i.e. <italic>x</italic>, we consider kernels which combine spectra with FTs, namely <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We describe the detail of the kernels for spectra in Section 3.4. Then, given the kernel over <inline-formula id="IE13"><mml:math id="IM13"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula> is fixed, the goal is to learn the function <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>↦</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula id="IE15"><mml:math id="IM15"><mml:mi mathvariant="script">D</mml:mi></mml:math></inline-formula> such that the correlation between the input and output is maximized. The <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the output of MPNN (or molecular vectors) which belongs to space <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The linear kernel function induced by this space can be written as follows:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>〈</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>To evaluate the correlation between spectra and molecular vectors (output of MPNN), we use an unbiased empirical estimate of HSIC (<xref rid="btz319-B7" ref-type="bibr">Gretton <italic>et al.</italic>, 2005</xref>), which can be given as follows:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>uHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:mtext>trace</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>L</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>L</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>L</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the kernel matrix for the set of <italic>n</italic> spectra <inline-formula id="IE19"><mml:math id="IM19"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula> with diagonal elements set to zero; <bold>1</bold><sub><italic>n</italic></sub> is a vector of 1 s of <italic>n</italic> dimensions. Likewise <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where L<sub><italic>n</italic></sub> is the kernel matrix of <italic>n</italic> molecular vectors output by MPNN. By arranging terms in (5), we can rewrite (5) as the objective function to learn parameters as follows:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mtext>uHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>trace</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>where
<disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>
          <boxed-text id="btz319-BOX1" position="float" orientation="portrait">
            <label>Algorithm 1</label>
            <caption>
              <p>Message Passing Neural Network (MPNN).</p>
            </caption>
            <p>1: <bold>Inputs</bold>:</p>
            <p>  minibatch of molecular structures <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, radius <italic>R</italic> weight matrices of edges: <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mn>4</mml:mn><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>,</p>
            <p>  weight matrices of <italic>readout</italic> function: W<sub>1</sub>, W<sub>2</sub>,…, W<sub><italic>R</italic></sub></p>
            <p>2: <bold>Outputs:</bold></p>
            <p>  molecular vectors <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p>3: <bold>for</bold><inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:mi>i</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>B</italic><bold>do</bold></p>
            <p>4:  <bold>for</bold> each atom <italic>v</italic> in <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula><bold>do</bold></p>
            <p>5:   <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>←</mml:mo></mml:mrow></mml:math></inline-formula>initial hidden rep. vector of <italic>v</italic> ▹ atom feature</p>
            <p>6:  <bold>end for</bold></p>
            <p>7:   <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> ▹ Initialize each molecular vector with a zero vector</p>
            <p>8:   <bold>for</bold><inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:mi>r</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>R</italic><bold>do</bold></p>
            <p>9:    <bold>for</bold> each node <italic>v</italic> in <inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula><bold>do</bold></p>
            <p>10:    <inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">vw</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> ▹ <italic>message</italic> function</p>
            <p>11:    <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>   ▹ <italic>update</italic> function</p>
            <p>12:    <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mtext>softmax</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> ▹ <italic>readout</italic> function</p>
            <p>13:    <bold>end for</bold></p>
            <p>14:   <bold>end for</bold></p>
            <p>15: <bold>end for</bold></p>
            <p>16: <inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>B</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></p>
          </boxed-text>
        </p>
        <p>However, directly optimizing (6) is prohibitively expensive in computation, particularly for large-scale data, since the complexity reaches O(<italic>n</italic><sup>2</sup>), both in space and time. In order to overcome this limitation, following <xref rid="btz319-B21" ref-type="bibr">Zhang <italic>et al.</italic> (2018)</xref>, we disjointly divide samples <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> into <italic>n</italic>/<italic>B</italic> blocks with the size of <italic>B</italic>, <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and then apply HSIC on each block independently. An empirical estimate of the unbiased block HSIC can be defined by:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:mtext>ubHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mtext>trace</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where S<sub><italic>b</italic></sub> can be defined by a similar manner to (7), and <inline-formula id="IE36"><mml:math id="IM36"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the kernel matrix for the <italic>b</italic>th block.</p>
        <p>Furthermore, in order to avoid the effect by biased partition of the dataset, following <xref rid="btz319-B20" ref-type="bibr">Yamada <italic>et al.</italic> (2018)</xref>, we repeat shuffling dataset <italic>T</italic> times, compute ubHSIC on each permutation and take the average over them. HSIC by this procedure is known as bagging block HSIC, which can be written as follows:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:mtext>ubHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mtext>trace</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
We use (9) as objective function <italic>J</italic> to learn parameters.</p>
      </sec>
      <sec>
        <title>3.2.3 Optimization algorithm</title>
        <p>An advantage of objective function (9) is that we can use the gradient descent (minibatch gradient descent) for estimating parameters of MPNN. We here explain details on how to conduct the minibatch gradient descent procedure for the HSIC-based loss, which has three steps.</p>
        <p>
          <italic>Step 1: Feed forward and loss calculation.</italic>
        </p>
        <p>For samples of size <italic>n</italic>, at each iteration, we perform random permutation and then split all samples into batches, where the size of each batch is <italic>B</italic>. Batches are sequentially fed into MPNN. The output of MPNN for the <italic>b</italic>th batch at the <italic>t</italic>th iteration is denoted by <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>B</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Then using these outputs, the objective function on the whole samples can be calculated as in (9).</p>
        <p>
          <italic>Step 2: Gradient calculation of the loss layer.</italic>
        </p>
        <p>As we can compute the loss directly with the output of MPNN (i.e. <inline-formula id="IE38"><mml:math id="IM38"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>), we need to compute the gradient of <italic>J</italic> with respect to <inline-formula id="IE39"><mml:math id="IM39"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Suppose that the output of MPNN is already normalized, i.e. <inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:mo>ϕ</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for all <inline-formula id="IE41"><mml:math id="IM41"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">Y</mml:mi></mml:mrow></mml:math></inline-formula>, the gradient can be obtained by the following:
<disp-formula id="E10"><label>(10)</label><mml:math id="M10"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mi>B</mml:mi><mml:mrow><mml:mi mathvariant="italic">Tn</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>
          <italic>Step 3: Gradient calculation of the MPN and weight update.</italic>
        </p>
        <p>Having calculated the gradient of <italic>J</italic>, i.e. (10), the next step is to compute the gradient of <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with respect to model parameters <italic>θ</italic>, namely <inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>θ</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, to update the whole parameters for each batch at each step.</p>
        <p><xref ref-type="boxed-text" rid="btz319-BOX2">Algorithm 2</xref> is a pseudocode of the entire algorithm of learning parameters of MPNN.
</p>
        <p>
          <boxed-text id="btz319-BOX2" position="float" orientation="portrait">
            <label>Algorithm 2</label>
            <caption>
              <p>Learning molecular representation vectors via HSIC.</p>
            </caption>
            <p>1: <bold>Inputs:</bold></p>
            <p>  set <inline-formula id="IE44"><mml:math id="IM44"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> of spectra-structure pairs,</p>
            <p>  <italic>T</italic>: number of iterations, <italic>B</italic>: size of minibatch</p>
            <p>2: <bold>Outputs:</bold></p>
            <p> <inline-formula id="IE45"><mml:math id="IM45"><mml:mrow><mml:mo>θ</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mn>4</mml:mn><mml:mi>R</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo>}</mml:mo><mml:mo>∪</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mtext>set</mml:mtext><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mtext>atoms</mml:mtext><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p>3: <bold>for</bold><inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:mi>t</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>T</italic><bold>do</bold></p>
            <p>4:  <inline-formula id="IE47"><mml:math id="IM47"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>  ▹ shuffled and split</p>
            <p>5: <bold> for</bold><inline-formula id="IE48"><mml:math id="IM48"><mml:mrow><mml:mi>b</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>B</italic><bold>do</bold></p>
            <p>6:   <inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></p>
            <p>7:   <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> ▹ Call Algorithm 1</p>
            <p>8:   <inline-formula id="IE51"><mml:math id="IM51"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is calculated from <inline-formula id="IE52"><mml:math id="IM52"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> by (7)</p>
            <p>9:   <inline-formula id="IE53"><mml:math id="IM53"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>trace</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p>10:   gradient of loss layer <inline-formula id="IE54"><mml:math id="IM54"><mml:mrow><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p>
            <p>11:   <inline-formula id="IE55"><mml:math id="IM55"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>Grad</mml:mtext></mml:mrow></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is calculated by chain rule</p>
            <p>12:   <inline-formula id="IE56"><mml:math id="IM56"><mml:mrow><mml:mo>θ</mml:mo><mml:mo>←</mml:mo><mml:mo>θ</mml:mo><mml:mo>−</mml:mo><mml:mo>γ</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>Grad</mml:mtext></mml:mrow></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> ▹ Update the whole parameters</p>
            <p>13:  <bold>end for</bold></p>
            <p>14: <bold>end for</bold></p>
          </boxed-text>
        </p>
      </sec>
    </sec>
    <sec>
      <title>3.3 Subtask 2: learning a mapping from spectra to molecular vectors by IOKR</title>
      <p>For Subtask 2, we use IOKR. That is, we learn a mapping from spectra to molecular vectors generated in Subtask 1 by using IOKR. Again, we explain two technical reasons why we use IOKR for this mapping below: (i) IOKR allows to incorporate the structures behind outputs, such as feature interactions in molecular vectors, into the learning model, by which the prediction accuracy can be improved. (ii) Furthermore, all features in molecular vectors are predicted simultaneously, which is not like separate tasks in prediction. This leads to faster computation.</p>
      <p>We now present the technical detail of IOKR below, which has two consecutive steps.</p>
      <sec>
        <title>3.3.1 Step 1: Learning spectra-vectors mapping</title>
        <p>Once parameters, i.e. function <italic>ϕ</italic>, are learned, we convert the structures of metabolites into their molecular vectors to obtain a new set of pairs, <inline-formula id="IE57"><mml:math id="IM57"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Now the goal is to find the optimal function <inline-formula id="IE58"><mml:math id="IM58"><mml:mrow><mml:mi>h</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by minimizing the following objective function:
<disp-formula id="E11"><label>(11)</label><mml:math id="M11"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>h</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>λ</italic> (&gt; 0) is a regularization parameter to prevent overfitting and <inline-formula id="IE59"><mml:math id="IM59"><mml:mi mathvariant="script">H</mml:mi></mml:math></inline-formula> is an approximate functional space that contains <italic>h</italic>; <inline-formula id="IE60"><mml:math id="IM60"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a space of molecular vectors of dimension <italic>d</italic>.</p>
        <p>By using the representer theorem in <xref rid="btz319-B12" ref-type="bibr">Micchelli and Pontil (2005)</xref>, optimal solution <inline-formula id="IE61"><mml:math id="IM61"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of (11) can be represented by a linear combination of vector-valued kernels on training set <inline-formula id="IE62"><mml:math id="IM62"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula>:
<disp-formula id="E12"><label>(12)</label><mml:math id="M12"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE63"><mml:math id="IM63"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are vectors in <inline-formula id="IE64"><mml:math id="IM64"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>; <inline-formula id="IE65"><mml:math id="IM65"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is an operator-valued kernel, defined on spectra <inline-formula id="IE66"><mml:math id="IM66"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula>, satisfying certain constraints (see <xref rid="btz319-B12" ref-type="bibr">Micchelli and Pontil, 2005</xref>). As dimensionality <italic>d</italic> of space <inline-formula id="IE67"><mml:math id="IM67"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is finite, the kernel is a matrix with the size of <italic>d </italic>×<italic> d</italic>.</p>
        <p>By replacing <inline-formula id="IE68"><mml:math id="IM68"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in (11) with (12), <inline-formula id="IE69"><mml:math id="IM69"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be estimated in the following:
<disp-formula id="E13"><label>(13)</label><mml:math id="M13"><mml:mrow><mml:mtext>vec</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>λ</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nd</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext>vec</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE70"><mml:math id="IM70"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE71"><mml:math id="IM71"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are both matrices with the size of <italic>d </italic>×<italic> n</italic>, and vec(.) is the vectorization of the input matrix, where the output is a vector obtained by repeatedly stacking each column of the input matrix on the top of the next column.</p>
      </sec>
      <sec>
        <title>3.3.2 Step 2: Candidate retrieval</title>
        <p>Given mapping <inline-formula id="IE72"><mml:math id="IM72"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> learned in Step 1, we now turn to the problem of finding the output metabolite in the database which corresponds to the query spectrum <bold>x</bold>. To this end, we search metabolite <bold>y</bold> in the list of given candidates <inline-formula id="IE73"><mml:math id="IM73"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, such that the squared distance between <inline-formula id="IE74"><mml:math id="IM74"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE75"><mml:math id="IM75"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be minimized:
<disp-formula id="E14"><label>(14)</label><mml:math id="M14"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula></p>
        <p>Considering that the output kernel is normalized and the operator-valued kernel keeps <inline-formula id="IE76"><mml:math id="IM76"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the optimal solution of <inline-formula id="IE77"><mml:math id="IM77"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be estimated as the following:
<disp-formula id="E15"><label>(15)</label><mml:math id="M15"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mi>l</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>λ</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE78"><mml:math id="IM78"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE79"><mml:math id="IM79"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are column vectors.</p>
        <p>Practically, the values given by objective function (15) are used as scores for ranking candidate metabolites in Step 2: candidate retrieval.</p>
      </sec>
    </sec>
    <sec>
      <title>3.4 Kernels</title>
      <p>ADAPTIVE uses kernels for the input and output.</p>
      <sec>
        <title>3.4.1 Kernels for input</title>
        <p>A various types of kernels are already defined and used for the input from MS/MS spectra. These kernels are typically divided into the following two groups: (i) kernels defined for spectra such as PPK (<xref rid="btz319-B9" ref-type="bibr">Jebara <italic>et al.</italic>, 2004</xref>) and (ii) kernels defined for FTs (<xref rid="btz319-B16" ref-type="bibr">Rasche <italic>et al.</italic>, 2011</xref>). Details on these kernels can be found in <xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic> (2015)</xref>.</p>
        <p>In fact, <xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic> (2015)</xref> suggested 24 different input kernels. ADAPTIVE combines these input kernels into a single kernel through MKL (<xref rid="btz319-B6" ref-type="bibr">Gönen and Alpaydin, 2011</xref>). ADAPTIVE uses two options for MKL: (i) UNIMKL (uniform MKL): assigns the same weights to all component kernels, and (ii) ALIGNF: uses weights over kernels to combine. That is, in ALIGNF, weights over component kernels are optimized (trained) by maximizing the centered kernel alignment between the combined kernel and the target kernel defined on the molecular vectors, which generate trained parameters (model).</p>
      </sec>
      <sec>
        <title>3.4.2 Kernels for output</title>
        <p>After learning parameters (model) to generate the molecular vectors for structures, we define kernels for output <inline-formula id="IE80"><mml:math id="IM80"><mml:mi mathvariant="script">Y</mml:mi></mml:math></inline-formula> by directly computing kernels on the corresponding molecular vectors. In our experiments, we consider the following two typical kernels:
<list list-type="bullet"><list-item><p>Linear kernel: <inline-formula id="IE81"><mml:math id="IM81"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>ϕ</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Gaussian kernel: <inline-formula id="IE82"><mml:math id="IM82"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo>γ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>,</p></list-item></list></p>
        <p>where <bold>y</bold> and <inline-formula id="IE83"><mml:math id="IM83"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are molecular structures in <inline-formula id="IE84"><mml:math id="IM84"><mml:mi mathvariant="script">Y</mml:mi></mml:math></inline-formula>.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Experimental results</title>
    <sec>
      <title>4.1 Dataset and evaluation measures</title>
      <p>We used a benchmark dataset in <xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic> (2016)</xref> to evaluate ADAPTIVE and compare with existing methods. The dataset consists of 4138 MS/MS spectra extracted from the GNPS (Global Natural Products Social) public spectra library (<ext-link ext-link-type="uri" xlink:href="https://gnps.ucsd.edu/ProteoSAFe/libraries.jsp">https://gnps.ucsd.edu/ProteoSAFe/libraries.jsp</ext-link>).</p>
      <p>To compare ADAPTIVE with existing methods, we used the same setting for all competing methods. Specifically we used 10-fold CV, and the results are averaged over all 10-folds. The performance was checked by the top-<italic>k</italic> accuracies (where <italic>k </italic>=<italic> </italic>1, 10, 20), which is the ratio of the number of the cases that the true structures are ranked at lower than or equal to <italic>k</italic> to the number of all cases. Also the speed was checked by computation time for prediction, measured by milliseconds per example (ms/example).</p>
      <p>Hyperparameters, such as regularization parameter <italic>λ</italic> and parameter <italic>γ</italic> of the output kernel, were chosen by using leave-one-out CV on each training fold. For prediction in ADAPTIVE, at the retrieval stage, given test example <bold>x</bold>, we computed the molecular vectors of <bold>x</bold>, <inline-formula id="IE85"><mml:math id="IM85"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> [see (11)] and those of all candidates <italic>ϕ</italic>(<bold>y</bold>) (see <xref ref-type="boxed-text" rid="btz319-BOX1">Algorithm 1</xref>). These candidates including the correct molecular structure of test example <bold>x</bold> were ranked, according to their distances to <inline-formula id="IE86"><mml:math id="IM86"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (from the smallest to the highest). These ranked candidates were used for computing the top-<italic>k</italic> accuracy. <xref rid="btz319-T1" ref-type="table">Table 1</xref> shows a set of parameter values, which were used to train MPNN of generating molecular vectors. <italic>State vectors</italic> of identical atoms at the lowest (atomic) level were initialized with the same random vector sampled from the standard normal distribution and updated during the training stage.</p>
      <table-wrap id="btz319-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Parameter values used for experiments</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Notations</th>
              <th align="left" rowspan="1" colspan="1">Parameter</th>
              <th align="left" rowspan="1" colspan="1">Values</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>T</italic>
              </td>
              <td rowspan="1" colspan="1">#epoch</td>
              <td rowspan="1" colspan="1">100</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>B</italic>
              </td>
              <td rowspan="1" colspan="1">Batchsize</td>
              <td rowspan="1" colspan="1">100</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>R</italic>
              </td>
              <td rowspan="1" colspan="1">#updates</td>
              <td rowspan="1" colspan="1">6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>d</italic>
              </td>
              <td rowspan="1" colspan="1">#dim of molecular vectors</td>
              <td rowspan="1" colspan="1">100,200,300</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>m</italic>
              </td>
              <td rowspan="1" colspan="1">#dim of atom feature</td>
              <td rowspan="1" colspan="1">50</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">#atom types</td>
              <td rowspan="1" colspan="1">12 (C, O, N, P, S, etc.)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">#bond types</td>
              <td rowspan="1" colspan="1">4 (single, double, triple, acromatic)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>All experiments were performed on a server with 2.7 GHz Intel Core i5 CPU and 8GB memory. The code was written in Python and Matlab with the support of the Chainer framework (<xref rid="btz319-B114" ref-type="bibr">Tokui <italic>et al.</italic>, 2015</xref>).</p>
    </sec>
    <sec>
      <title>4.2 Performance results</title>
      <sec>
        <title>4.2.1 Predictive performance</title>
        <p>We compared the predictive performances of ADAPTIVE with three existing methods: FingerID (<xref rid="btz319-B8" ref-type="bibr">Heinonen <italic>et al.</italic>, 2012</xref>), CSI:FingerID (<xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>) and IOKR (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>) in terms of the top-<italic>k</italic> accuracy (<italic>k </italic>=<italic> </italic>1, 10 and 20). <xref rid="btz319-T2" ref-type="table">Table 2</xref> shows the top-<italic>k</italic> accuracies of the competing methods with UNIMKL and ALIGNF for MKL and linear and Gaussian kernels for the output kernel, changing <italic>k</italic> from 1 to 20 and also changing the size of fingerprints from 100 to 300 (for ADAPTIVE only). This table first shows that ADAPTIVE achieved the best performance, being followed by IOKR, CSI:FingerID and FingerID. For example, ADAPTIVE with ALIGNF, Gaussian kernel and the fingerprint size of 300 achieved 31.03% for <italic>k </italic>=<italic> </italic>1, while IOKR with ALIGNF and Gaussian kernel was 29.59% and CSI:FingerID with ALIGNF was 28.84% or 24.82%. That of Finger: ID was only 17.74%. Interestingly, for <italic>k </italic>=<italic> </italic>1, the performance advantage of ADAPTIVE against IOKR was rather slight, while <italic>k </italic>=<italic> </italic>10 and 20, ADAPTIVE outperformed IOKR much more clearly, with the difference of around 3–5% under the same condition for the two methods.</p>
        <table-wrap id="btz319-T2" orientation="portrait" position="float">
          <label>Table 2.</label>
          <caption>
            <p>Comparison of the top-<italic>k</italic> accuracy (<italic>k </italic>=<italic> </italic>1, 10 and 20) of FingerID, CSI:FingerID, IOKR and ADAPTIVE</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Method</th>
                <th align="left" rowspan="1" colspan="1">Vec. size</th>
                <th align="left" rowspan="1" colspan="1">MKL</th>
                <th align="left" colspan="3" rowspan="1">Accuracies (mean/SD %)<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">Top 1</th>
                <th align="left" rowspan="1" colspan="1">Top 10</th>
                <th align="left" rowspan="1" colspan="1">Top 20</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">FingerID</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">None</td>
                <td rowspan="1" colspan="1">17.74</td>
                <td rowspan="1" colspan="1">49.59</td>
                <td rowspan="1" colspan="1">58.17</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CSI:FingerID unit</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">24.82</td>
                <td rowspan="1" colspan="1">60.47</td>
                <td rowspan="1" colspan="1">68.20</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CSI:FingerID mod</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">28.84</td>
                <td rowspan="1" colspan="1">66.07</td>
                <td rowspan="1" colspan="1">73.07</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Platt</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">IOKR linear</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.58/2.23</td>
                <td rowspan="1" colspan="1">65.99/2.46</td>
                <td rowspan="1" colspan="1">73.53/2.47</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">28.54/2.54</td>
                <td rowspan="1" colspan="1">65.77/2.39</td>
                <td rowspan="1" colspan="1">73.19/3.11</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ADAPTIVE</td>
                <td rowspan="1" colspan="1">100</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.42/2.83</td>
                <td rowspan="1" colspan="1">70.01/2.79</td>
                <td rowspan="1" colspan="1">77.48/2.98</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">linear</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.19/3.21</td>
                <td rowspan="1" colspan="1">69.52/2.89</td>
                <td rowspan="1" colspan="1">77.64/3.23</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">200</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.57/3.96</td>
                <td rowspan="1" colspan="1">69.38/3.05</td>
                <td rowspan="1" colspan="1">76.95/2.98</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.11/3.45</td>
                <td rowspan="1" colspan="1">69.53/2.52</td>
                <td rowspan="1" colspan="1">77.56/2.43</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">300</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.22/3.47</td>
                <td rowspan="1" colspan="1">70.48/2.72</td>
                <td rowspan="1" colspan="1">78.18/2.67</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">
                  <bold>30.61/3.23</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>70.51/2.52</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>78.23/2.75</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">IOKR Gaussian</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.66/2.34</td>
                <td rowspan="1" colspan="1">66.51/2.87</td>
                <td rowspan="1" colspan="1">73.94/2.54</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.59/2.58</td>
                <td rowspan="1" colspan="1">66.13/2.09</td>
                <td rowspan="1" colspan="1">73.62/1.85</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ADAPTIVE</td>
                <td rowspan="1" colspan="1">100</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.47/3.21</td>
                <td rowspan="1" colspan="1">70.01/2.83</td>
                <td rowspan="1" colspan="1">77.51/2.11</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Gaussian</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.37/3.21</td>
                <td rowspan="1" colspan="1">69.91/2.64</td>
                <td rowspan="1" colspan="1">77.48/2.33</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">200</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.44/3.86</td>
                <td rowspan="1" colspan="1">69.84/2.78</td>
                <td rowspan="1" colspan="1">77.08/2.95</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">28.98/3.32</td>
                <td rowspan="1" colspan="1">69.65/2.71</td>
                <td rowspan="1" colspan="1">77.15/2.74</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">300</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.31/3.48</td>
                <td rowspan="1" colspan="1"><bold>71.10</bold>/2.73</td>
                <td rowspan="1" colspan="1">78.51/2.65</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1"><bold>31.03</bold>/3.40</td>
                <td rowspan="1" colspan="1">70.89/2.74</td>
                <td rowspan="1" colspan="1"><bold>78.52</bold>/2.52</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic>Note</italic>: The highest value (indicating the most accurate prediction) are in boldface for each <italic>k</italic>.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>We used one-sided paired <italic>t</italic>-test to verify if the differences between ADAPTIVE and IOKR are statistically significant. For example, considering the top 10 accuracy with Gaussian kernel and ALIGNF, the calculated <italic>P</italic>-value was <italic>P </italic>=<italic> </italic>0.0012. Since it is less than the significance level of <italic>α</italic>  =  0.01, we can claim the statistical significance of the advantage of ADAPTIVE in terms of the top 10 accuracy over IOKR under Gaussian kernel and ALIGNF. We conclude that the performance advantage of ADAPTIVE was confirmed by checking a larger number of top candidates. Another finding is the performance difference between linear and Gaussian kernels was very slight (almost nothing) for ADAPTIVE under the same other conditions. This is also true with the settings of UNIMKL and ALIGNF, the performance for them was rather the same. However, the size of fingerprints strongly affected the performance in the sense that a larger size of fingerprints achieved a higher performance. In summary, ADAPTIVE clearly outperformed competing methods with, for example, for <italic>k </italic>=<italic> </italic>20, the difference of 3–5%, which is very sizable.</p>
      </sec>
      <sec>
        <title>4.2.2 Computation time for prediction</title>
        <p>IOKR was already shown to be faster than previous kernel-based methods in prediction (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>). Thus, we consider only IOKR as a competing method for examining computational efficiency. <xref rid="btz319-T3" ref-type="table">Table 3</xref> shows the computation time of ADAPTIVE and IOKR with linear and Gaussian kernels for prediction. The computation time was averaged over the 10-fold CV. This table shows that ADAPTIVE was significantly faster than IOKR. Specifically, under both linear and Gaussian kernels, ADAPTIVE with the fingerprint size of 100 was four to seven times faster than IOKR. This is because molecular vectors by ADAPTIVE are much more precise and adaptive to given data than those used in IOKR.</p>
        <table-wrap id="btz319-T3" orientation="portrait" position="float">
          <label>Table 3.</label>
          <caption>
            <p>Computation time for prediction by ADAPTIVE and IOKR</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Method</th>
                <th rowspan="1" colspan="1">Mol. vec. size</th>
                <th colspan="2" rowspan="1">prediction time (ms/example)<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1">Linear</th>
                <th rowspan="1" colspan="1">Gaussian</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">IOKR</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">140.22</td>
                <td rowspan="1" colspan="1">3352.4</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ADAPTIVE</td>
                <td rowspan="1" colspan="1">100</td>
                <td rowspan="1" colspan="1">
                  <bold>20.32</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>802.6</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">200</td>
                <td rowspan="1" colspan="1">39.88</td>
                <td rowspan="1" colspan="1">844.33</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">300</td>
                <td rowspan="1" colspan="1">54.14</td>
                <td rowspan="1" colspan="1">1071.8</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn2">
              <p><italic>Note</italic>: The smallest values (indicating the fastest) were in boldface for linear and Gaussian kernels.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
    </sec>
    <sec>
      <title>4.3 Case study</title>
      <p>To understand the results obtained by ADAPTIVE more, in the obtained molecular vectors, we examined substructures rooted at atoms, which activated several example features most. As shown in Section 3.2.1, each substructure rooted at an atom is represented by a state vector and contributes to computing the molecular vector of the whole molecule. Then, given a feature, we can estimate the contribution of each substructure by simply computing the softmax value from the corresponding state vector. We use these values of substructures as scores to rank substructures to activate the given feature.</p>
      <p><xref ref-type="fig" rid="btz319-F5">Figure 5</xref> shows three example features (#2, #39 and #83). For each feature, we show three substructures with the highest scores (each score is shown above the substructure). The first row shows three substructures which activated feature #2 most. Interestingly, we can see that these substructures share a further smaller, similar group of atoms: O, P and S (highlighted in blue). Similarly, the second row shows three substructures sharing a group of atoms: O and N, where these substructures activated feature #39 most. Also the third row shows substructures which activated feature #83, all having atom: Cl. Thus, <xref ref-type="fig" rid="btz319-F5">Figure 5</xref> shows that each feature of ADAPTIVE is activated by multiple different substructures sharing some similar properties, which must be important in data and probably for prediction. In contrast, each feature in regular molecular fingerprints is activated by only one predefined substructure. In summary, from this case study, learned features of ADAPTIVE are more concise and specific to the task of metabolite identification than regular molecular fingerprints, leading to the advantage of predictive performance and computation time.
</p>
      <fig id="btz319-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Example features (#2, #39 and #83) and their three substructures (and their scores) which activated the corresponding feature most. Note that three substructures of each feature share a similar group (set) of atoms which are shown in blue</p>
        </caption>
        <graphic xlink:href="btz319f5"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>5 Discussion and conclusion</title>
    <p>Supervised learning for metabolite identification uses fingerprints as intermediate representation vectors between spectra and metabolites, while such fixed vectors are too redundant to cover all possible substructures and chemical properties in metabolites, causing limitations in predictive performance and high computational costs. To overcome this problem, we have proposed ADAPTIVE, which generates representations of metabolites specific to given spectrum-structure pairs. ADAPTIVE learns a model to generate molecular vectors for metabolites, which is parameterized by a MPNN over given molecular structures and trained through optimizing the objective function to maximize the correlation between molecular vectors and corresponding spectra. Our empirical validation of ADAPTIVE with the benchmark dataset showed the advantage of ADAPTIVE over existing methods including IOKR, the current cutting-edge method, both in predictive performance and computation time for prediction.</p>
    <p>A drawback of ADAPTIVE would be interpretability, because structural information is implicitly encoded in compact vectors in ADAPTIVE and cannot be made explicit easily. In metabolite identification, it would be desirable to connect the set of peaks to the corresponding substructures/chemical properties of metabolites (<xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>). Developing a model with such interpretability would be interesting future work.</p>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Funding</title>
    <p>D.H.N. has been supported in part by Otsuka Toshimi scholarship and JSPS KAKENHI [grant number 19J14714]. C.H.N. has been supported in part by MEXT Kakenhi 18K11434. H.M. has been supported in part by JST ACCEL [grant number JPMJAC1503], MEXT Kakenhi [grant numbers 16H02868 and 19H04169], FiDiPro by Tekes (currently Business Finland) and AIPSE program by Academy of Finland.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz319-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brouard</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Fast metabolite identification with input output kernel regression</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>i28</fpage>–<lpage>i36</lpage>.<pub-id pub-id-type="pmid">27307628</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B2">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>de Hoffmann</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Stroobant</surname><given-names>V.</given-names></name></person-group> (<year>2007</year>). <source>Mass Spectrometry, Principles and Applications</source>. <edition>3</edition>rd edn. 
<publisher-name>John Wiley &amp; Sons</publisher-name>, Hoboken, New York.</mixed-citation>
    </ref>
    <ref id="btz319-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dührkop</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Searching molecular structure databases with tandem mass spectra using CSI:FingerID</article-title>. <source>Proc. Natl. Acad. Sci</source>., <volume>112</volume>, <fpage>12580</fpage>–<lpage>12585</lpage>.<pub-id pub-id-type="pmid">26392543</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Duvenaud</surname><given-names>D.K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>). <chapter-title>Convolutional networks on graphs for learning molecular fingerprints</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Cortes</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Lawrence</surname><given-names>N.D.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>D.D.</given-names></name>, <name name-style="western"><surname>Sugiyama</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Garnett</surname><given-names>R.</given-names></name></person-group> (eds.) <source>Proceedings of the 28th International Conference on Neural Information Processing Systems</source>, Vol. 2. 
<publisher-name>Curran Associates, Inc</publisher-name>, Montreal, Canada, pp. <fpage>2224</fpage>–<lpage>2232</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gilmer</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>). <chapter-title>Neural message passing for quantum chemistry</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Precup</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Teh</surname><given-names>Y.W.</given-names></name></person-group> (eds.) <source>Proceedings of the 34th International Conference on Machine Learning, Volume 70 of Proceedings of Machine Learning Research</source>. 
<publisher-name>International Convention Centre, PMLR</publisher-name>, 
<publisher-loc>Sydney, Australia</publisher-loc>, pp. <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gönen</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Alpaydin</surname><given-names>E.</given-names></name></person-group> (<year>2011</year>) 
<article-title>Multiple kernel learning algorithms</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2211</fpage>–<lpage>2268</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gretton</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>). <chapter-title>Measuring statistical dependence with Hilbert-Schmidt norms</chapter-title> In: <source>Proceedings of the 16th International Conference on Algorithmic Learning Theory, ALT’05</source>. 
<publisher-name>Springer-Verlag</publisher-name>, 
<publisher-loc>Berlin, Heidelberg</publisher-loc>, pp. <fpage>63</fpage>–<lpage>77</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heinonen</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Metabolite identification and molecular fingerprint prediction through machine learning</article-title>. <source>Bioinformatics</source>, <volume>28</volume>, <fpage>2333</fpage>–<lpage>2341</lpage>.<pub-id pub-id-type="pmid">22815355</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jebara</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2004</year>) 
<article-title>Probability product kernels</article-title>. <source>J. Mach. Learn. Res</source>., <volume>5</volume>, <fpage>819</fpage>–<lpage>844</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kipf</surname><given-names>T.N.</given-names></name>, <name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). Semi-supervised classification with graph convolutional networks. <italic>arXiv preprint arXiv: 1609.02907</italic>.</mixed-citation>
    </ref>
    <ref id="btz319-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Gated graph sequence neural networks</article-title>. <source>CoRR</source>, abs/1511.05493.</mixed-citation>
    </ref>
    <ref id="btz319-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Micchelli</surname><given-names>C.A.</given-names></name>, <name name-style="western"><surname>Pontil</surname><given-names>M.A.</given-names></name></person-group> (<year>2005</year>) 
<article-title>On learning vector-valued functions</article-title>. <source>Neural Comput</source>., <volume>17</volume>, <fpage>177</fpage>–<lpage>204</lpage>.<pub-id pub-id-type="pmid">15563752</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B13">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>D.H.</given-names></name></person-group><etal>et al</etal> (<year>2018a</year>). Recent advances and prospects of computational methods for metabolite identification: a review with emphasis on machine learning approaches. <italic>Brief. Bioinf.</italic> doi: 10.1093/bib/bby066. [Epub ahead of print].</mixed-citation>
    </ref>
    <ref id="btz319-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>D.H.</given-names></name></person-group><etal>et al</etal> (<year>2018b</year>) 
<article-title>Simple: sparse interaction model over peaks of molecules for fast, interpretable metabolite identification from tandem mass spectra</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i323</fpage>–<lpage>i332</lpage>.<pub-id pub-id-type="pmid">29950009</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>). Semi-supervised learning of hierarchical representations of molecules using neural message passing. <italic>CoRR</italic>, abs/1711.10168.</mixed-citation>
    </ref>
    <ref id="btz319-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rasche</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Computing fragmentation trees from tandem mass spectrometry data</article-title>. <source>Anal. Chem</source>., <volume>83</volume>, <fpage>1243</fpage>–<lpage>1251</lpage>. PMID: 21182243.<pub-id pub-id-type="pmid">21182243</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B114">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tokui</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Chainer: a next-generation open source framework for deep learning</article-title>. In: <source>Proceedings of Workshop on Machine Learning Systems (LearningSys) in the Twenty-Ninth Annual Conference on Neural Information Processing Systems (NIPS)</source>, Vol. 5, pp. 1–6.</mixed-citation>
    </ref>
    <ref id="btz319-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vaniya</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Fiehn</surname><given-names>O.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Using fragmentation trees and mass spectral trees for identifying unknown compounds in metabolomics</article-title>. <source>Trends Analyt. Chem</source>., <volume>69</volume>, <fpage>52</fpage>–<lpage>61</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wishart</surname><given-names>D.S.</given-names></name></person-group> (<year>2007</year>) 
<article-title>Current progress in computational metabolomics</article-title>. <source>Brief. Bioinf</source>., <volume>8</volume>, <fpage>279</fpage>–<lpage>293</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wishart</surname><given-names>D.S.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>HMDB 3.0—the human metabolome database in 2013</article-title>. <source>Nucleic Acids Res</source>., <volume>41</volume>, <fpage>D801</fpage>–<lpage>D807</lpage>.<pub-id pub-id-type="pmid">23161693</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Yamada</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>). Post selection inference with kernels. In: Storkey,A. and Perez-Cruz,F. (eds.) <italic>Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research</italic>. Playa Blanca, PMLR, Lanzarote, Canary Islands, pp. 152–160.</mixed-citation>
    </ref>
    <ref id="btz319-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Large-scale kernel methods for independence testing</article-title>. <source>Stat. Comput</source>., <volume>28</volume>, <fpage>113</fpage>–<lpage>130</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6612897</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz319</article-id>
    <article-id pub-id-type="publisher-id">btz319</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2019 Conference Proceedings</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>General Computational Biology</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ADAPTIVE: leArning DAta-dePendenT, concIse molecular VEctors for fast, accurate metabolite identification from tandem mass spectra</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Nguyen</surname>
          <given-names>Dai Hai</given-names>
        </name>
        <xref ref-type="aff" rid="btz319-aff1">1</xref>
        <xref ref-type="corresp" rid="btz319-cor1"/>
        <!--<email>hai@kuicr.kyoto-u.ac.jp</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nguyen</surname>
          <given-names>Canh Hao</given-names>
        </name>
        <xref ref-type="aff" rid="btz319-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mamitsuka</surname>
          <given-names>Hiroshi</given-names>
        </name>
        <xref ref-type="aff" rid="btz319-aff1">1</xref>
        <xref ref-type="aff" rid="btz319-aff2">2</xref>
      </contrib>
    </contrib-group>
    <aff id="btz319-aff1"><label>1</label>Bioinformatics Center, Institute for Chemical Research, Kyoto University, Uji, Japan</aff>
    <aff id="btz319-aff2"><label>2</label>Department of Computer Science, Aalto University, Espoo, Finland</aff>
    <author-notes>
      <corresp id="btz319-cor1">To whom correspondence should be addressed. <email>hai@kuicr.kyoto-u.ac.jp</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-07-05">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>14</issue>
    <fpage>i164</fpage>
    <lpage>i172</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz319.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Metabolite identification is an important task in metabolomics to enhance the knowledge of biological systems. There have been a number of machine learning-based methods proposed for this task, which predict a chemical structure of a given spectrum through an intermediate (chemical structure) representation called molecular fingerprints. They usually have two steps: (i) predicting fingerprints from spectra; (ii) searching chemical compounds (in database) corresponding to the predicted fingerprints. Fingerprints are feature vectors, which are usually very large to cover all possible substructures and chemical properties, and therefore heavily redundant, in the sense of having many molecular (sub)structures irrelevant to the task, causing limited predictive performance and slow prediction.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We propose ADAPTIVE, which has two parts: learning two mappings (i) from structures to molecular vectors and (ii) from spectra to molecular vectors. The first part learns molecular vectors for metabolites from given data, to be consistent with both spectra and chemical structures of metabolites. In more detail, molecular vectors are generated by a model, being parameterized by a message passing neural network, and parameters are estimated by maximizing the correlation between molecular vectors and the corresponding spectra in terms of Hilbert-Schmidt Independence Criterion. Molecular vectors generated by this model are compact and importantly adaptive (specific) to both given data and task of metabolite identification. The second part uses input output kernel regression (IOKR), the current cutting-edge method of metabolite identification. We empirically confirmed the effectiveness of ADAPTIVE by using a benchmark data, where ADAPTIVE outperformed the original IOKR in both predictive performance and computational efficiency.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The code will be accessed through <ext-link ext-link-type="uri" xlink:href="http://www.bic.kyoto-u.ac.jp/pathway/tools/ADAPTIVE">http://www.bic.kyoto-u.ac.jp/pathway/tools/ADAPTIVE</ext-link> after the acceptance of this article.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">JSPS</named-content>
          <named-content content-type="funder-identifier">10.13039/501100001691</named-content>
        </funding-source>
        <award-id>19J14714</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">MEXT Kakenhi</named-content>
        </funding-source>
        <award-id>18K11434</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">JST ACCEL</named-content>
        </funding-source>
        <award-id>JPMJAC1503</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">MEXT Kakenhi</named-content>
        </funding-source>
        <award-id>16H02868</award-id>
        <award-id>19H04169</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Metabolites are small molecules, having many important functions in living cells such as energy transport, signaling, building blocks of cells and so on (<xref rid="btz319-B18" ref-type="bibr">Wishart, 2007</xref>). Identifying their biochemical characteristics or so-called metabolite identification is an essential task in metabolomics to increase the knowledge of biological systems. Yet, it is still a challenging task due to the size or coverage of spectra libraries.</p>
    <p>Mass spectrometry (MS) is one of the most common techniques in analytical chemistry for dealing with metabolite identification (<xref rid="btz319-B2" ref-type="bibr">de Hoffmann and Stroobant, 2007</xref>). In more detail, a chemical compound is decomposed into fragments, of which mass-to-charge ratios (m/z) are continuously measured to obtain a mass spectrum. One MS spectrum can be represented by a list of peaks, each of which corresponds to a fragment captured by MS. <xref ref-type="fig" rid="btz319-F1">Figure 1</xref> shows a real example of a MS spectrum. In practice, tandem MS (also known as MS/MS or MS2) is widely used, in which precursor ions of specific m/z values from MS spectra are selected and further fragmented to produce other groups of product ions
[see, e.g. <xref rid="btz319-B17" ref-type="bibr">Vaniya and Fiehn (2015)</xref> for more details]. The MS/MS spectra provide structural information about the measured compound, which makes MS/MS more useful for tackling metabolite identification.</p>
    <fig id="btz319-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>Example MS spectrum from Human Metabolome Database (<xref rid="btz319-B19" ref-type="bibr">Wishart <italic>et al.</italic> 2013</xref>) for 1-Methylhistidine (HMBD00001), with the corresponding chemical structure (top-left) and peak list (top-right)</p>
      </caption>
      <graphic xlink:href="btz319f1"/>
    </fig>
    <p>A number of computational methods have been proposed for identifying unknown metabolites from MS/MS spectra data. In general, they are classified into three main categories: (i) spectral library search; (ii) <italic>in silico</italic> fragmentation; and (iii) machine learning (<xref rid="btz319-B13" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018a</xref>). Recent advances in metabolite identification have been led by the machine learning category (e.g. <xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>; <xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>; <xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>). This category can be further divided into two key groups: supervised learning for substructure prediction and unsupervised learning for substructure annotation. While the former is to find a mapping from inputs (e.g. spectra) to outputs (e.g. fingerprints), the latter extracts underlying substructures of metabolites. Our research focuses on supervised learning, where the common scheme is to learn a mapping from spectra to structures.</p>
    <p>The prediction can be divided into two steps: (i) fingerprint prediction: predicting fingerprints of a given test spectrum with supervised learning; (ii) candidate retrieval: retrieving chemical compound (from database) which is closest to the predicted fingerprints (<xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>).</p>
    <p>Kernel methods have been shown to be effective for fingerprint prediction, such as methods include FingerID (<xref rid="btz319-B8" ref-type="bibr">Heinonen <italic>et al.</italic>, 2012</xref>), CSI:FingerID (<xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>) and input output kernel regression (IOKR, <xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>). In particular, IOKR is recognized as the current cutting-edge method for metabolite identification due to the following advantages: (i) structures (e.g. feature interaction in the molecular fingerprint vectors) in the output can be incorporated into the learning model by the kernel defined in the output space, leading to accuracy improvement; (ii) fingerprints are simultaneously predicted by the learned model, rather than being considered as a set of separate tasks, resulting in faster computation. One can take structures of the metabolites into account by using graph kernels (path, shortest-path and graphlet kernels) or kernels defined on molecular fingerprints. It is also known that kernels based on fingerprint vectors obtained the best performance (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>). However, a limitation of using molecular fingerprints as the intermediate representation vectors is that they are general-purpose and very large in size to encode all possible substructures and chemical properties related with metabolites. Consequently, such vectors are neither necessarily specific to any task nor data, and therefore redundant in the sense that these vectors might contain information irrelevant to the task, resulting in limited predictive performance. Moreover, the large size of fingerprints causes slow prediction in the first step of the above two steps.</p>
    <p>Generally, in machine learning, deep learning has been proven successful recently in many application domains. Deep learning is useful for regular data, say a table, in which rows are instances and columns are features, and vice versa. However, semistructured data, particularly graphs, for example, chemical (or biological) molecules, which are irregular types of data, are difficult to be used with deep learning. A number of research efforts have been devoted to applying deep learning to semistructured data, proposing models to learn representations of graphs, such as <xref rid="btz319-B4" ref-type="bibr">Duvenaud <italic>et al.</italic> (2015)</xref>, <xref rid="btz319-B11" ref-type="bibr">Li <italic>et al.</italic> (2015)</xref> and <xref rid="btz319-B15" ref-type="bibr">Nguyen <italic>et al.</italic> (2017)</xref>. Importantly, <xref rid="btz319-B5" ref-type="bibr">Gilmer <italic>et al.</italic> (2017)</xref> showed that a lot of research on graphs can be formulated in a unified model, namely message passing neural network (MPNN), with the following three components: <italic>message passing</italic>, <italic>update</italic> and <italic>readout</italic> functions. In other words, one way of defining such functions results in a different model for learning graphs. Furthermore, another attractive property of MPNN is that it allows to learn meaningful representations specific to each task for graphs in an end-to-end manner.</p>
    <p>We propose a powerful machine learning framework for metabolite identification, named ADAPTIVE, which has two subtasks: (i) learning a mapping from structures to molecular vectors and (ii) learning a mapping from spectra to molecular vectors. <xref ref-type="fig" rid="btz319-F2">Figure 2</xref> shows a schematic picture of ADAPTIVE, where the left and right blue boxes correspond to the first and second subtasks, respectively. In Subtask 1, ADAPTIVE learns a model to generate molecular vectors for metabolites using their chemical structures, where these vectors are specific to both data and the task of metabolite identification, and therefore nonredundant. The model in Subtask 1 is parameterized by MPNN for mapping metabolite structures to the molecular vectors. The <italic>main contribution</italic> of this article is in the Subtask 1, that is, to learn the correspondence between given pairs of spectra and structures for metabolites.
</p>
    <fig id="btz319-F2" orientation="portrait" position="float">
      <label>Fig. 2.</label>
      <caption>
        <p>Overview of ADAPTIVE for metabolite identification. ADAPTIVE has two components: (i) Subtask 1: estimates parameters of a function mapping metabolites from structures to molecular vectors, given a set of spectra-structure pairs; (ii) Subtask 2: learns a function mapping from spectra to molecular vectors (generated by Subtask 1), given a set of spectrum-vector pairs</p>
      </caption>
      <graphic xlink:href="btz319f2"/>
    </fig>
    <p>Thus, the parameters of MPNN are trained so that the correlation between the spectra and the vectors mapped from the structures is maximized. We use Hilbert-Schmidt Independence Criterion (HSIC, <xref rid="btz319-B7" ref-type="bibr">Gretton <italic>et al.</italic>, 2005</xref>) for evaluating the correlation, due to its theoretically nice properties and kernel-based calculation. Specifically, we formulate an objective function for the maximization problem through HSIC and solve this problem to have the best molecular vectors adapted to given data. For Subtask 2, ADAPTIVE uses IOKR to learn a mapping from spectra to molecular vectors generated by the Subtask 1.</p>
    <p>We emphasize that the key difference between ADAPTIVE and the original IOKR is that IOKR uses ‘manually designed’ fingerprints, which are large in size, possibly redundant and nonspecific to metabolite identification (and given data), while ADAPTIVE learns representations for metabolites from given data, as molecular vectors, resulting in that the molecular vectors generated by ADAPTIVE are data-driven and concise.</p>
    <p>In order to validate the performance of ADAPTIVE, we conducted extensive experiments using a benchmark data. Experimental results showed the following two main advantages of ADAPTIVE over existing methods, including the original IOKR:
<list list-type="bullet"><list-item><p>Predictive performance</p></list-item></list></p>
    <p>ADAPTIVE achieved the best performance, followed by IOKR, CSI:FingerID and FingerID. For example, the top-20 accuracy of ADAPTIVE was 78.52% with the parameters of Gaussian kernel, ALIGNF and molecular vector size of 300. On the other hand, IOKR, CSI:FingerID and FingerID achieved 74.79%, 73.07% (or 68.20%) and 58.17%, respectively, using Gaussian kernel (for IOKR) and ALIGNF. The top-<italic>k</italic> accuracy was computed by the average over all trials of 10-fold cross-validation (CV), and so the performance advantage of ADAPTIVE was significant and very clear.
<list list-type="bullet"><list-item><p>Computational efficiency for prediction</p></list-item></list></p>
    <p>Under the same experimental setting, ADAPTIVE was four to seven times faster than IOKR, which was already known as the fastest method. We can then say that ADAPTIVE is the current fastest method while keeping the highest predictive performance for metabolite identification.</p>
  </sec>
  <sec>
    <title>2 Related work</title>
    <p>As mentioned in the Introduction section, fingerprint prediction is important in supervised learning for metabolite identification, because we can retrieve metabolite candidates more reliably if fingerprints are predicted more accurately. For fingerprint prediction, kernel learning has been shown to be the most powerful approach. For example, a typical approach, FingerID (<xref rid="btz319-B8" ref-type="bibr">Heinonen <italic>et al.</italic>, 2012</xref>) uses probability product kernel (PPK, <xref rid="btz319-B9" ref-type="bibr">Jebara <italic>et al.</italic>, 2004</xref>), which can be directly computed from spectra and runs support vector machine with this kernel for solving fingerprint prediction as a classification problem. CSI:FingerID (<xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>), an extension of FingerID, uses not only spectra but also fragmentation trees (FTs, <xref rid="btz319-B16" ref-type="bibr">Rasche <italic>et al.</italic>, 2011</xref>) as input to generate kernels over spectra and FTs, which are then combined via multiple kernel learning (MKL, <xref rid="btz319-B6" ref-type="bibr">Gönen and Alpaydin, 2011</xref>). FTs may capture structural information behind spectra which is missing in the approach of FingerID. This is the motivation of CSI:FingerID. However, the computational cost for converting FTs from MS/MS spectra is very expensive, leading to heavy computational load, which causes a problem particularly in prediction. Thus, we can say that kernel-based supervised learning, particularly complex kernels, have a computation issue, regardless of high performance in prediction. On the other hand, a sparse learning model, namely SIMPLE (<xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>), considers a simpler function than kernels for fingerprint, while interactions of peaks in spectra can be incorporated into learning models explicitly. SIMPLE achieved a comparable performance against kernel-based learning, reducing the computational cost drastically. A key point of SIMPLE is to take advantage of sparsity of spectra, which results in faster prediction and interpretability, showing clear advantages over kernel-based methods.</p>
    <p>Among the series of kernel-based approaches, IOKR (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>) has been shown to outperform the previous methods, in terms of both predictive performance and computational speed.</p>
    <p>It learns a mapping from spectra, i.e. input <inline-formula id="IE1"><mml:math id="IM1"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula>, to molecular fingerprints (or structures behind fingerprints), i.e. output <inline-formula id="IE2"><mml:math id="IM2"><mml:mi mathvariant="script">Y</mml:mi></mml:math></inline-formula>. In order to do this mapping, IOKR defines kernels to encode similarities in the input space (e.g. spectra and/or FTs) and the output space (molecular fingerprints or structures). Then, the advantage of IOKR comes from the following two points: (i) unlike previous kernel-based methods, IOKR handles the structured output space by the kernel defined for the output, which improves the predictive performance; (ii) IOKR simultaneously predicts fingerprints rather than considering fingerprint prediction as a set of separate tasks, leading to an efficient computation in prediction. Some part (mapping from spectra to feature vectors) of IOKR is a part of ADAPTIVE, and so further technical details of the corresponding part of IOKR is described more in Section 3.</p>
    <p>Conventionally, molecular fingerprints for fingerprint prediction have been manually designed feature vectors to encode a predefined set of substructures or chemical properties, which are possibly found in metabolites. However, recently, machine learning-based (or data-driven) algorithms for generating fingerprints have been proposed. A typical approach is neural fingerprint (NFP, <xref rid="btz319-B4" ref-type="bibr">Duvenaud <italic>et al.</italic>, 2015</xref>), which takes graphs with arbitrary sizes and shapes as inputs. NFP uses the idea of <italic>graph convolution</italic>, an extension of convolution operation from multidimensional arrays, like images or texts, to graph structures. NFP is then trained in a supervised manner by using available labels, such as log mol/L for solubility, <italic>EC</italic><sub>50</sub> for drug efficacy. Finally, NFP results in fingerprint vectors (for molecules) specific to given task and data. An extension of NFP is for unsupervised (as well as semisupervised) settings to learn representations of molecular graph without labels (<xref rid="btz319-B15" ref-type="bibr">Nguyen <italic>et al.</italic>, 2017</xref>), since label information can be experimentally obtained and precious.</p>
    <p>More recently, <xref rid="btz319-B5" ref-type="bibr">Gilmer <italic>et al.</italic> (2017)</xref> showed that several graph convolution-based models, including NFP, Gated Graph Neural Networks (<xref rid="btz319-B11" ref-type="bibr">Li <italic>et al.</italic>, 2015</xref>), spectral graph convolutional network (<xref rid="btz319-B10" ref-type="bibr">Kipf and Welling, 2016</xref>), etc., can be formulated in an unified model, namely MPNN, with the following three functions: <italic>message passing</italic>, <italic>update</italic> and <italic>readout</italic>. A key advantage of MPNN is that defining the above components generates a proper model for learning graphs, depending on a given task. Also, another advantage of MPNN as well as other neural network-based methods in this paragraph is that they adopt differentiable operations, and thus their parameters can be effectively trained by using a stochastic gradient descent algorithm.</p>
  </sec>
  <sec>
    <title>3 Materials and methods</title>
    <sec>
      <title>3.1 ADAPTIVE: overview</title>
      <p>We first introduce the framework of ADAPTIVE for metabolite identification. This is also the general framework of approaches using machine learning for metabolite identification. It has two subtasks. <italic>Subtask 1</italic>: learning a function which maps metabolites from their structures to molecular vectors and <italic>Subtask 2</italic>: learning a function which maps metabolites from spectra to the vectors generated in Subtask 1. <xref ref-type="fig" rid="btz319-F2">Figure 2</xref> shows an illustration of the entire framework of ADAPTIVE. In this figure, the left and right blue boxes correspond to Subtasks 1 and 2, respectively.</p>
      <p>For Subtask 1, given pairs of metabolite structure-spectrum, we estimate parameters of a function which maps metabolites from their structures to molecular vectors by maximizing the correlation between the vectors mapped from the structures and also the corresponding spectra. In more detail, we model the mapping function by MPNN and evaluate the correlation between the vectors and spectra by using HSIC due to the computational simplicity and provably theoretical properties of HSIC. For Subtask 2, we simply borrow the corresponding part of IOKR to learn a function mapping metabolites from spectra to vectors generated by Subtask 1.</p>
      <p>We explain these two subtasks in the following subsections, being followed by the subsection on kernels we used in ADAPTIVE.</p>
    </sec>
    <sec>
      <title>3.2 Subtask 1: learning molecular vectors for metabolites via HSIC</title>
      <p>For this subtask, we need to estimate a function to map metabolites from structures to molecular vectors, given spectrum-structure pairs. For this problem, we use MPNN as the mapping function, which can extract meaningful representation for graphs (molecules for our problem) by supervised learning from training data. That is, MPNN requires labeled training data, which are, however, unavailable for this subtask. Then we manage this problem by taking advantage of given spectrum-structure pairs. We estimate parameters of MPNN by using the idea of maximizing the correlation between the given spectra and vectors (mapped from structures). The correlation is evaluated by HSIC. We describe the detail of MPNN, HSIC and related optimization procedures in the following subsections.</p>
      <sec>
        <title>3.2.1 Message passing neural network</title>
        <p>MPNN is a framework, which takes graphs of arbitrary sizes and structures as inputs, to learn their representation vectors at different levels (i.e. nodes, subgraphs and the whole graph) in a supervised manner (<xref rid="btz319-B5" ref-type="bibr">Gilmer <italic>et al.</italic>, 2017</xref>). A key advantage is that MPNN allows to learn features specific to the given task from the given data. Below, we explain the procedure of MPNN.</p>
        <p>First let <italic>G</italic> be an undirected graph, and <italic>v</italic> and <italic>vw</italic> be a node (atom in molecules) and an edge (bond in molecules), respectively. Each node <italic>v</italic> is assigned with <italic>state vectors</italic> at different levels, where each level represents a substructure (or subgraph) rooted at the corresponding node, denoted by <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <italic>r</italic> shows a level. We can compute state vector <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> as well as <italic>message</italic><inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> in a hierarchical manner, by using the following two functions: <italic>message passing</italic> (1) and <italic>update</italic> (2):
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">vw</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mi>(</mml:mi><mml:mi>v</mml:mi><mml:mi>)</mml:mi></mml:mrow></mml:math></inline-formula> denotes the set of neighbors of node <italic>v</italic> in graph <italic>G</italic>; <italic>e</italic>(<italic>v</italic>, <italic>w</italic>) indicates the type of edge between two nodes <italic>v</italic> and <italic>w</italic> (this edge type is like a single, double, triple or aromatic bond); <inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">vw</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is a (square) weight matrix to be learned, specific to the edge type <italic>e</italic>(<italic>vw</italic>) at the <italic>r</italic>th level; <italic>g</italic> is a nonlinear activation function (e.g. ReLU or sigmoid).</p>
        <p>Intuitively, the <italic>message passing</italic> function (1) on node <italic>v</italic> plays the role of collecting information from the neighbors of node <italic>v</italic> and <italic>update</italic> function (2) on node <italic>v</italic> is to update the state of node <italic>v</italic> based on the collected information and the former state of node <italic>v</italic>. Thus, by applying two functions (1) and (2) multiple times, the updated features at node <italic>v</italic> (e.g. <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>) can be used to represent a certain number of substructures with the root of node <italic>v</italic>. Then, the values for these series of substructures can be used to generate a vector at node <italic>v</italic> with different levels (sizes) of substructures. <xref ref-type="fig" rid="btz319-F3">Figure 3</xref> shows a schematic and illustrative picture of this procedure [<xref ref-type="fig" rid="btz319-F3">Fig. 3</xref> is from <xref rid="btz319-B15" ref-type="bibr">Nguyen <italic>et al.</italic> (2017)</xref>].
</p>
        <fig id="btz319-F3" orientation="portrait" position="float">
          <label>Fig. 3.</label>
          <caption>
            <p>Message passing and update functions are used to represent rooted substructures in a hierarchical manner. At the first level (left-most graph), each node is represented by feature vector, with only information of the node itself. We note that by repeatedly applying message passing and update functions (from left to right), more neighboring information are incorporated. For example, the updated feature (second level) has information on nodes 3 and 5, and then third level has that on nodes 2 to 5. Finally, the whole graph is covered</p>
          </caption>
          <graphic xlink:href="btz319f3"/>
        </fig>
        <p>After obtaining the state vectors of substructures rooted at node <italic>v</italic>, i.e. <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, we have the <italic>readout</italic> phase to combine all vectors at different levels into a single representation vector of the whole molecule (namely, NFPs). <xref ref-type="fig" rid="btz319-F4">Figure 4</xref> shows a schematic picture of summing up the state vectors at different levels. As in <xref rid="btz319-B4" ref-type="bibr">Duvenaud <italic>et al.</italic> (2015)</xref>, we adopt the softmax operation on the states and then perform linear projections (parameterized by different weight matrices <italic>W<sub>r</sub></italic>) and finally sum them up to obtain a single vector over different levels which represents the whole graph. In short, the molecular vector for the entire molecule can be written as following:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>r</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>v</mml:mi></mml:munder><mml:mrow><mml:mtext>softmax</mml:mtext></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
We note that operations are all differentiable with respect to parameters, which makes learning the parameters possible, given an objective function, by a stochastic or minibatch gradient descent algorithm. <xref ref-type="boxed-text" rid="btz319-BOX1">Algorithm 1</xref> shows a pseudocode of the procedure of repeating the <italic>message passing</italic> and <italic>update</italic> functions.</p>
        <fig id="btz319-F4" orientation="portrait" position="float">
          <label>Fig. 4.</label>
          <caption>
            <p>Representation vectors of substructures, which are rooted at nodes, are computed from the input graph by the message passing and update functions. These functions contribute to computing the molecular representation vector of the whole molecule</p>
          </caption>
          <graphic xlink:href="btz319f4"/>
        </fig>
      </sec>
      <sec>
        <title>3.2.2 HSIC-based objective function</title>
        <p>We estimate parameters of MPNN by maximizing the correlation (dependency) between given spectra and molecular vectors. A lot of measures can be used to evaluate and estimate the correlation, while we use HSIC due to its theoretically sound properties. More importantly, estimation of HSIC is based on kernel calculation, which can effectively deal with the uncertainty of peaks in spectra caused by measurement errors.</p>
        <p>Formally, we are given dataset <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are spectrum and molecular structure, respectively, of the <italic>i</italic>th metabolite. First, for the spectra, i.e. <italic>x</italic>, we consider kernels which combine spectra with FTs, namely <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We describe the detail of the kernels for spectra in Section 3.4. Then, given the kernel over <inline-formula id="IE13"><mml:math id="IM13"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula> is fixed, the goal is to learn the function <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo>↦</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> from <inline-formula id="IE15"><mml:math id="IM15"><mml:mi mathvariant="script">D</mml:mi></mml:math></inline-formula> such that the correlation between the input and output is maximized. The <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the output of MPNN (or molecular vectors) which belongs to space <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The linear kernel function induced by this space can be written as follows:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>〈</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>To evaluate the correlation between spectra and molecular vectors (output of MPNN), we use an unbiased empirical estimate of HSIC (<xref rid="btz319-B7" ref-type="bibr">Gretton <italic>et al.</italic>, 2005</xref>), which can be given as follows:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mtext>uHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">[</mml:mo><mml:mtext>trace</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>L</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>L</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mtext>L</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msub><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>1</mml:mn></mml:mstyle></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes the kernel matrix for the set of <italic>n</italic> spectra <inline-formula id="IE19"><mml:math id="IM19"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula> with diagonal elements set to zero; <bold>1</bold><sub><italic>n</italic></sub> is a vector of 1 s of <italic>n</italic> dimensions. Likewise <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where L<sub><italic>n</italic></sub> is the kernel matrix of <italic>n</italic> molecular vectors output by MPNN. By arranging terms in (5), we can rewrite (5) as the objective function to learn parameters as follows:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mtext>uHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>trace</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">L</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>where
<disp-formula id="E7"><label>(7)</label><mml:math id="M7"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>
          <boxed-text id="btz319-BOX1" position="float" orientation="portrait">
            <label>Algorithm 1</label>
            <caption>
              <p>Message Passing Neural Network (MPNN).</p>
            </caption>
            <p>1: <bold>Inputs</bold>:</p>
            <p>  minibatch of molecular structures <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, radius <italic>R</italic> weight matrices of edges: <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mn>4</mml:mn><mml:mi>R</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>,</p>
            <p>  weight matrices of <italic>readout</italic> function: W<sub>1</sub>, W<sub>2</sub>,…, W<sub><italic>R</italic></sub></p>
            <p>2: <bold>Outputs:</bold></p>
            <p>  molecular vectors <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p>3: <bold>for</bold><inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:mi>i</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>B</italic><bold>do</bold></p>
            <p>4:  <bold>for</bold> each atom <italic>v</italic> in <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula><bold>do</bold></p>
            <p>5:   <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>←</mml:mo></mml:mrow></mml:math></inline-formula>initial hidden rep. vector of <italic>v</italic> ▹ atom feature</p>
            <p>6:  <bold>end for</bold></p>
            <p>7:   <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> ▹ Initialize each molecular vector with a zero vector</p>
            <p>8:   <bold>for</bold><inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:mi>r</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>R</italic><bold>do</bold></p>
            <p>9:    <bold>for</bold> each node <italic>v</italic> in <inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula><bold>do</bold></p>
            <p>10:    <inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">vw</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msubsup></mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> ▹ <italic>message</italic> function</p>
            <p>11:    <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mi>r</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>   ▹ <italic>update</italic> function</p>
            <p>12:    <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mtext>softmax</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> ▹ <italic>readout</italic> function</p>
            <p>13:    <bold>end for</bold></p>
            <p>14:   <bold>end for</bold></p>
            <p>15: <bold>end for</bold></p>
            <p>16: <inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>B</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></p>
          </boxed-text>
        </p>
        <p>However, directly optimizing (6) is prohibitively expensive in computation, particularly for large-scale data, since the complexity reaches O(<italic>n</italic><sup>2</sup>), both in space and time. In order to overcome this limitation, following <xref rid="btz319-B21" ref-type="bibr">Zhang <italic>et al.</italic> (2018)</xref>, we disjointly divide samples <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> into <italic>n</italic>/<italic>B</italic> blocks with the size of <italic>B</italic>, <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and then apply HSIC on each block independently. An empirical estimate of the unbiased block HSIC can be defined by:
<disp-formula id="E8"><label>(8)</label><mml:math id="M8"><mml:mrow><mml:mtext>ubHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mtext>trace</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where S<sub><italic>b</italic></sub> can be defined by a similar manner to (7), and <inline-formula id="IE36"><mml:math id="IM36"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the kernel matrix for the <italic>b</italic>th block.</p>
        <p>Furthermore, in order to avoid the effect by biased partition of the dataset, following <xref rid="btz319-B20" ref-type="bibr">Yamada <italic>et al.</italic> (2018)</xref>, we repeat shuffling dataset <italic>T</italic> times, compute ubHSIC on each permutation and take the average over them. HSIC by this procedure is known as bagging block HSIC, which can be written as follows:
<disp-formula id="E9"><label>(9)</label><mml:math id="M9"><mml:mrow><mml:mtext>ubHSIC</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mtext>trace</mml:mtext></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="normal">L</mml:mi><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
We use (9) as objective function <italic>J</italic> to learn parameters.</p>
      </sec>
      <sec>
        <title>3.2.3 Optimization algorithm</title>
        <p>An advantage of objective function (9) is that we can use the gradient descent (minibatch gradient descent) for estimating parameters of MPNN. We here explain details on how to conduct the minibatch gradient descent procedure for the HSIC-based loss, which has three steps.</p>
        <p>
          <italic>Step 1: Feed forward and loss calculation.</italic>
        </p>
        <p>For samples of size <italic>n</italic>, at each iteration, we perform random permutation and then split all samples into batches, where the size of each batch is <italic>B</italic>. Batches are sequentially fed into MPNN. The output of MPNN for the <italic>b</italic>th batch at the <italic>t</italic>th iteration is denoted by <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>B</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Then using these outputs, the objective function on the whole samples can be calculated as in (9).</p>
        <p>
          <italic>Step 2: Gradient calculation of the loss layer.</italic>
        </p>
        <p>As we can compute the loss directly with the output of MPNN (i.e. <inline-formula id="IE38"><mml:math id="IM38"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>), we need to compute the gradient of <italic>J</italic> with respect to <inline-formula id="IE39"><mml:math id="IM39"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Suppose that the output of MPNN is already normalized, i.e. <inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:mo>ϕ</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for all <inline-formula id="IE41"><mml:math id="IM41"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">Y</mml:mi></mml:mrow></mml:math></inline-formula>, the gradient can be obtained by the following:
<disp-formula id="E10"><label>(10)</label><mml:math id="M10"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mi>B</mml:mi><mml:mrow><mml:mi mathvariant="italic">Tn</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>
          <italic>Step 3: Gradient calculation of the MPN and weight update.</italic>
        </p>
        <p>Having calculated the gradient of <italic>J</italic>, i.e. (10), the next step is to compute the gradient of <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with respect to model parameters <italic>θ</italic>, namely <inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>θ</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, to update the whole parameters for each batch at each step.</p>
        <p><xref ref-type="boxed-text" rid="btz319-BOX2">Algorithm 2</xref> is a pseudocode of the entire algorithm of learning parameters of MPNN.
</p>
        <p>
          <boxed-text id="btz319-BOX2" position="float" orientation="portrait">
            <label>Algorithm 2</label>
            <caption>
              <p>Learning molecular representation vectors via HSIC.</p>
            </caption>
            <p>1: <bold>Inputs:</bold></p>
            <p>  set <inline-formula id="IE44"><mml:math id="IM44"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> of spectra-structure pairs,</p>
            <p>  <italic>T</italic>: number of iterations, <italic>B</italic>: size of minibatch</p>
            <p>2: <bold>Outputs:</bold></p>
            <p> <inline-formula id="IE45"><mml:math id="IM45"><mml:mrow><mml:mo>θ</mml:mo><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">H</mml:mi></mml:mrow><mml:mn>4</mml:mn><mml:mi>R</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">W</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo>}</mml:mo><mml:mo>∪</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mtext>set</mml:mtext><mml:mo> </mml:mo><mml:mtext>of</mml:mtext><mml:mo> </mml:mo><mml:mtext>atoms</mml:mtext><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p>3: <bold>for</bold><inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:mi>t</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>T</italic><bold>do</bold></p>
            <p>4:  <inline-formula id="IE47"><mml:math id="IM47"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>  ▹ shuffled and split</p>
            <p>5: <bold> for</bold><inline-formula id="IE48"><mml:math id="IM48"><mml:mrow><mml:mi>b</mml:mi><mml:mo>←</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to <italic>B</italic><bold>do</bold></p>
            <p>6:   <inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula></p>
            <p>7:   <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> ▹ Call Algorithm 1</p>
            <p>8:   <inline-formula id="IE51"><mml:math id="IM51"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is calculated from <inline-formula id="IE52"><mml:math id="IM52"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> by (7)</p>
            <p>9:   <inline-formula id="IE53"><mml:math id="IM53"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">J</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>trace</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p>
            <p>10:   gradient of loss layer <inline-formula id="IE54"><mml:math id="IM54"><mml:mrow><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">F</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p>
            <p>11:   <inline-formula id="IE55"><mml:math id="IM55"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>Grad</mml:mtext></mml:mrow></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is calculated by chain rule</p>
            <p>12:   <inline-formula id="IE56"><mml:math id="IM56"><mml:mrow><mml:mo>θ</mml:mo><mml:mo>←</mml:mo><mml:mo>θ</mml:mo><mml:mo>−</mml:mo><mml:mo>γ</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>Grad</mml:mtext></mml:mrow></mml:mrow><mml:mo>θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> ▹ Update the whole parameters</p>
            <p>13:  <bold>end for</bold></p>
            <p>14: <bold>end for</bold></p>
          </boxed-text>
        </p>
      </sec>
    </sec>
    <sec>
      <title>3.3 Subtask 2: learning a mapping from spectra to molecular vectors by IOKR</title>
      <p>For Subtask 2, we use IOKR. That is, we learn a mapping from spectra to molecular vectors generated in Subtask 1 by using IOKR. Again, we explain two technical reasons why we use IOKR for this mapping below: (i) IOKR allows to incorporate the structures behind outputs, such as feature interactions in molecular vectors, into the learning model, by which the prediction accuracy can be improved. (ii) Furthermore, all features in molecular vectors are predicted simultaneously, which is not like separate tasks in prediction. This leads to faster computation.</p>
      <p>We now present the technical detail of IOKR below, which has two consecutive steps.</p>
      <sec>
        <title>3.3.1 Step 1: Learning spectra-vectors mapping</title>
        <p>Once parameters, i.e. function <italic>ϕ</italic>, are learned, we convert the structures of metabolites into their molecular vectors to obtain a new set of pairs, <inline-formula id="IE57"><mml:math id="IM57"><mml:mrow><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Now the goal is to find the optimal function <inline-formula id="IE58"><mml:math id="IM58"><mml:mrow><mml:mi>h</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> by minimizing the following objective function:
<disp-formula id="E11"><label>(11)</label><mml:math id="M11"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>h</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo>λ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>h</mml:mi><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mi mathvariant="script">H</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>λ</italic> (&gt; 0) is a regularization parameter to prevent overfitting and <inline-formula id="IE59"><mml:math id="IM59"><mml:mi mathvariant="script">H</mml:mi></mml:math></inline-formula> is an approximate functional space that contains <italic>h</italic>; <inline-formula id="IE60"><mml:math id="IM60"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a space of molecular vectors of dimension <italic>d</italic>.</p>
        <p>By using the representer theorem in <xref rid="btz319-B12" ref-type="bibr">Micchelli and Pontil (2005)</xref>, optimal solution <inline-formula id="IE61"><mml:math id="IM61"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> of (11) can be represented by a linear combination of vector-valued kernels on training set <inline-formula id="IE62"><mml:math id="IM62"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula>:
<disp-formula id="E12"><label>(12)</label><mml:math id="M12"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE63"><mml:math id="IM63"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are vectors in <inline-formula id="IE64"><mml:math id="IM64"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>; <inline-formula id="IE65"><mml:math id="IM65"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is an operator-valued kernel, defined on spectra <inline-formula id="IE66"><mml:math id="IM66"><mml:mi mathvariant="script">X</mml:mi></mml:math></inline-formula>, satisfying certain constraints (see <xref rid="btz319-B12" ref-type="bibr">Micchelli and Pontil, 2005</xref>). As dimensionality <italic>d</italic> of space <inline-formula id="IE67"><mml:math id="IM67"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is finite, the kernel is a matrix with the size of <italic>d </italic>×<italic> d</italic>.</p>
        <p>By replacing <inline-formula id="IE68"><mml:math id="IM68"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in (11) with (12), <inline-formula id="IE69"><mml:math id="IM69"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be estimated in the following:
<disp-formula id="E13"><label>(13)</label><mml:math id="M13"><mml:mrow><mml:mtext>vec</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>λ</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">nd</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mtext>vec</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE70"><mml:math id="IM70"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">C</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE71"><mml:math id="IM71"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> are both matrices with the size of <italic>d </italic>×<italic> n</italic>, and vec(.) is the vectorization of the input matrix, where the output is a vector obtained by repeatedly stacking each column of the input matrix on the top of the next column.</p>
      </sec>
      <sec>
        <title>3.3.2 Step 2: Candidate retrieval</title>
        <p>Given mapping <inline-formula id="IE72"><mml:math id="IM72"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> learned in Step 1, we now turn to the problem of finding the output metabolite in the database which corresponds to the query spectrum <bold>x</bold>. To this end, we search metabolite <bold>y</bold> in the list of given candidates <inline-formula id="IE73"><mml:math id="IM73"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, such that the squared distance between <inline-formula id="IE74"><mml:math id="IM74"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE75"><mml:math id="IM75"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be minimized:
<disp-formula id="E14"><label>(14)</label><mml:math id="M14"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">F</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula></p>
        <p>Considering that the output kernel is normalized and the operator-valued kernel keeps <inline-formula id="IE76"><mml:math id="IM76"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the optimal solution of <inline-formula id="IE77"><mml:math id="IM77"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be estimated as the following:
<disp-formula id="E15"><label>(15)</label><mml:math id="M15"><mml:mrow><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>arg</mml:mi><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mi>l</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>λ</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">I</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">K</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE78"><mml:math id="IM78"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE79"><mml:math id="IM79"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are column vectors.</p>
        <p>Practically, the values given by objective function (15) are used as scores for ranking candidate metabolites in Step 2: candidate retrieval.</p>
      </sec>
    </sec>
    <sec>
      <title>3.4 Kernels</title>
      <p>ADAPTIVE uses kernels for the input and output.</p>
      <sec>
        <title>3.4.1 Kernels for input</title>
        <p>A various types of kernels are already defined and used for the input from MS/MS spectra. These kernels are typically divided into the following two groups: (i) kernels defined for spectra such as PPK (<xref rid="btz319-B9" ref-type="bibr">Jebara <italic>et al.</italic>, 2004</xref>) and (ii) kernels defined for FTs (<xref rid="btz319-B16" ref-type="bibr">Rasche <italic>et al.</italic>, 2011</xref>). Details on these kernels can be found in <xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic> (2015)</xref>.</p>
        <p>In fact, <xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic> (2015)</xref> suggested 24 different input kernels. ADAPTIVE combines these input kernels into a single kernel through MKL (<xref rid="btz319-B6" ref-type="bibr">Gönen and Alpaydin, 2011</xref>). ADAPTIVE uses two options for MKL: (i) UNIMKL (uniform MKL): assigns the same weights to all component kernels, and (ii) ALIGNF: uses weights over kernels to combine. That is, in ALIGNF, weights over component kernels are optimized (trained) by maximizing the centered kernel alignment between the combined kernel and the target kernel defined on the molecular vectors, which generate trained parameters (model).</p>
      </sec>
      <sec>
        <title>3.4.2 Kernels for output</title>
        <p>After learning parameters (model) to generate the molecular vectors for structures, we define kernels for output <inline-formula id="IE80"><mml:math id="IM80"><mml:mi mathvariant="script">Y</mml:mi></mml:math></inline-formula> by directly computing kernels on the corresponding molecular vectors. In our experiments, we consider the following two typical kernels:
<list list-type="bullet"><list-item><p>Linear kernel: <inline-formula id="IE81"><mml:math id="IM81"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>ϕ</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Gaussian kernel: <inline-formula id="IE82"><mml:math id="IM82"><mml:mrow><mml:mi>l</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo>γ</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>,</p></list-item></list></p>
        <p>where <bold>y</bold> and <inline-formula id="IE83"><mml:math id="IM83"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are molecular structures in <inline-formula id="IE84"><mml:math id="IM84"><mml:mi mathvariant="script">Y</mml:mi></mml:math></inline-formula>.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Experimental results</title>
    <sec>
      <title>4.1 Dataset and evaluation measures</title>
      <p>We used a benchmark dataset in <xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic> (2016)</xref> to evaluate ADAPTIVE and compare with existing methods. The dataset consists of 4138 MS/MS spectra extracted from the GNPS (Global Natural Products Social) public spectra library (<ext-link ext-link-type="uri" xlink:href="https://gnps.ucsd.edu/ProteoSAFe/libraries.jsp">https://gnps.ucsd.edu/ProteoSAFe/libraries.jsp</ext-link>).</p>
      <p>To compare ADAPTIVE with existing methods, we used the same setting for all competing methods. Specifically we used 10-fold CV, and the results are averaged over all 10-folds. The performance was checked by the top-<italic>k</italic> accuracies (where <italic>k </italic>=<italic> </italic>1, 10, 20), which is the ratio of the number of the cases that the true structures are ranked at lower than or equal to <italic>k</italic> to the number of all cases. Also the speed was checked by computation time for prediction, measured by milliseconds per example (ms/example).</p>
      <p>Hyperparameters, such as regularization parameter <italic>λ</italic> and parameter <italic>γ</italic> of the output kernel, were chosen by using leave-one-out CV on each training fold. For prediction in ADAPTIVE, at the retrieval stage, given test example <bold>x</bold>, we computed the molecular vectors of <bold>x</bold>, <inline-formula id="IE85"><mml:math id="IM85"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> [see (11)] and those of all candidates <italic>ϕ</italic>(<bold>y</bold>) (see <xref ref-type="boxed-text" rid="btz319-BOX1">Algorithm 1</xref>). These candidates including the correct molecular structure of test example <bold>x</bold> were ranked, according to their distances to <inline-formula id="IE86"><mml:math id="IM86"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> (from the smallest to the highest). These ranked candidates were used for computing the top-<italic>k</italic> accuracy. <xref rid="btz319-T1" ref-type="table">Table 1</xref> shows a set of parameter values, which were used to train MPNN of generating molecular vectors. <italic>State vectors</italic> of identical atoms at the lowest (atomic) level were initialized with the same random vector sampled from the standard normal distribution and updated during the training stage.</p>
      <table-wrap id="btz319-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Parameter values used for experiments</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Notations</th>
              <th align="left" rowspan="1" colspan="1">Parameter</th>
              <th align="left" rowspan="1" colspan="1">Values</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>T</italic>
              </td>
              <td rowspan="1" colspan="1">#epoch</td>
              <td rowspan="1" colspan="1">100</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>B</italic>
              </td>
              <td rowspan="1" colspan="1">Batchsize</td>
              <td rowspan="1" colspan="1">100</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>R</italic>
              </td>
              <td rowspan="1" colspan="1">#updates</td>
              <td rowspan="1" colspan="1">6</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>d</italic>
              </td>
              <td rowspan="1" colspan="1">#dim of molecular vectors</td>
              <td rowspan="1" colspan="1">100,200,300</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>m</italic>
              </td>
              <td rowspan="1" colspan="1">#dim of atom feature</td>
              <td rowspan="1" colspan="1">50</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">#atom types</td>
              <td rowspan="1" colspan="1">12 (C, O, N, P, S, etc.)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">#bond types</td>
              <td rowspan="1" colspan="1">4 (single, double, triple, acromatic)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>All experiments were performed on a server with 2.7 GHz Intel Core i5 CPU and 8GB memory. The code was written in Python and Matlab with the support of the Chainer framework (<xref rid="btz319-B114" ref-type="bibr">Tokui <italic>et al.</italic>, 2015</xref>).</p>
    </sec>
    <sec>
      <title>4.2 Performance results</title>
      <sec>
        <title>4.2.1 Predictive performance</title>
        <p>We compared the predictive performances of ADAPTIVE with three existing methods: FingerID (<xref rid="btz319-B8" ref-type="bibr">Heinonen <italic>et al.</italic>, 2012</xref>), CSI:FingerID (<xref rid="btz319-B3" ref-type="bibr">Dührkop <italic>et al.</italic>, 2015</xref>) and IOKR (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>) in terms of the top-<italic>k</italic> accuracy (<italic>k </italic>=<italic> </italic>1, 10 and 20). <xref rid="btz319-T2" ref-type="table">Table 2</xref> shows the top-<italic>k</italic> accuracies of the competing methods with UNIMKL and ALIGNF for MKL and linear and Gaussian kernels for the output kernel, changing <italic>k</italic> from 1 to 20 and also changing the size of fingerprints from 100 to 300 (for ADAPTIVE only). This table first shows that ADAPTIVE achieved the best performance, being followed by IOKR, CSI:FingerID and FingerID. For example, ADAPTIVE with ALIGNF, Gaussian kernel and the fingerprint size of 300 achieved 31.03% for <italic>k </italic>=<italic> </italic>1, while IOKR with ALIGNF and Gaussian kernel was 29.59% and CSI:FingerID with ALIGNF was 28.84% or 24.82%. That of Finger: ID was only 17.74%. Interestingly, for <italic>k </italic>=<italic> </italic>1, the performance advantage of ADAPTIVE against IOKR was rather slight, while <italic>k </italic>=<italic> </italic>10 and 20, ADAPTIVE outperformed IOKR much more clearly, with the difference of around 3–5% under the same condition for the two methods.</p>
        <table-wrap id="btz319-T2" orientation="portrait" position="float">
          <label>Table 2.</label>
          <caption>
            <p>Comparison of the top-<italic>k</italic> accuracy (<italic>k </italic>=<italic> </italic>1, 10 and 20) of FingerID, CSI:FingerID, IOKR and ADAPTIVE</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Method</th>
                <th align="left" rowspan="1" colspan="1">Vec. size</th>
                <th align="left" rowspan="1" colspan="1">MKL</th>
                <th align="left" colspan="3" rowspan="1">Accuracies (mean/SD %)<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">Top 1</th>
                <th align="left" rowspan="1" colspan="1">Top 10</th>
                <th align="left" rowspan="1" colspan="1">Top 20</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">FingerID</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">None</td>
                <td rowspan="1" colspan="1">17.74</td>
                <td rowspan="1" colspan="1">49.59</td>
                <td rowspan="1" colspan="1">58.17</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CSI:FingerID unit</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">24.82</td>
                <td rowspan="1" colspan="1">60.47</td>
                <td rowspan="1" colspan="1">68.20</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">CSI:FingerID mod</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">28.84</td>
                <td rowspan="1" colspan="1">66.07</td>
                <td rowspan="1" colspan="1">73.07</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Platt</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">IOKR linear</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.58/2.23</td>
                <td rowspan="1" colspan="1">65.99/2.46</td>
                <td rowspan="1" colspan="1">73.53/2.47</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">28.54/2.54</td>
                <td rowspan="1" colspan="1">65.77/2.39</td>
                <td rowspan="1" colspan="1">73.19/3.11</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ADAPTIVE</td>
                <td rowspan="1" colspan="1">100</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.42/2.83</td>
                <td rowspan="1" colspan="1">70.01/2.79</td>
                <td rowspan="1" colspan="1">77.48/2.98</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">linear</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.19/3.21</td>
                <td rowspan="1" colspan="1">69.52/2.89</td>
                <td rowspan="1" colspan="1">77.64/3.23</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">200</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.57/3.96</td>
                <td rowspan="1" colspan="1">69.38/3.05</td>
                <td rowspan="1" colspan="1">76.95/2.98</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.11/3.45</td>
                <td rowspan="1" colspan="1">69.53/2.52</td>
                <td rowspan="1" colspan="1">77.56/2.43</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">300</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.22/3.47</td>
                <td rowspan="1" colspan="1">70.48/2.72</td>
                <td rowspan="1" colspan="1">78.18/2.67</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">
                  <bold>30.61/3.23</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>70.51/2.52</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>78.23/2.75</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">IOKR Gaussian</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.66/2.34</td>
                <td rowspan="1" colspan="1">66.51/2.87</td>
                <td rowspan="1" colspan="1">73.94/2.54</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.59/2.58</td>
                <td rowspan="1" colspan="1">66.13/2.09</td>
                <td rowspan="1" colspan="1">73.62/1.85</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ADAPTIVE</td>
                <td rowspan="1" colspan="1">100</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.47/3.21</td>
                <td rowspan="1" colspan="1">70.01/2.83</td>
                <td rowspan="1" colspan="1">77.51/2.11</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Gaussian</td>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">29.37/3.21</td>
                <td rowspan="1" colspan="1">69.91/2.64</td>
                <td rowspan="1" colspan="1">77.48/2.33</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">200</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">29.44/3.86</td>
                <td rowspan="1" colspan="1">69.84/2.78</td>
                <td rowspan="1" colspan="1">77.08/2.95</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1">28.98/3.32</td>
                <td rowspan="1" colspan="1">69.65/2.71</td>
                <td rowspan="1" colspan="1">77.15/2.74</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">300</td>
                <td rowspan="1" colspan="1">UNIMKL</td>
                <td rowspan="1" colspan="1">30.31/3.48</td>
                <td rowspan="1" colspan="1"><bold>71.10</bold>/2.73</td>
                <td rowspan="1" colspan="1">78.51/2.65</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">ALIGNF</td>
                <td rowspan="1" colspan="1"><bold>31.03</bold>/3.40</td>
                <td rowspan="1" colspan="1">70.89/2.74</td>
                <td rowspan="1" colspan="1"><bold>78.52</bold>/2.52</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic>Note</italic>: The highest value (indicating the most accurate prediction) are in boldface for each <italic>k</italic>.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>We used one-sided paired <italic>t</italic>-test to verify if the differences between ADAPTIVE and IOKR are statistically significant. For example, considering the top 10 accuracy with Gaussian kernel and ALIGNF, the calculated <italic>P</italic>-value was <italic>P </italic>=<italic> </italic>0.0012. Since it is less than the significance level of <italic>α</italic>  =  0.01, we can claim the statistical significance of the advantage of ADAPTIVE in terms of the top 10 accuracy over IOKR under Gaussian kernel and ALIGNF. We conclude that the performance advantage of ADAPTIVE was confirmed by checking a larger number of top candidates. Another finding is the performance difference between linear and Gaussian kernels was very slight (almost nothing) for ADAPTIVE under the same other conditions. This is also true with the settings of UNIMKL and ALIGNF, the performance for them was rather the same. However, the size of fingerprints strongly affected the performance in the sense that a larger size of fingerprints achieved a higher performance. In summary, ADAPTIVE clearly outperformed competing methods with, for example, for <italic>k </italic>=<italic> </italic>20, the difference of 3–5%, which is very sizable.</p>
      </sec>
      <sec>
        <title>4.2.2 Computation time for prediction</title>
        <p>IOKR was already shown to be faster than previous kernel-based methods in prediction (<xref rid="btz319-B1" ref-type="bibr">Brouard <italic>et al.</italic>, 2016</xref>). Thus, we consider only IOKR as a competing method for examining computational efficiency. <xref rid="btz319-T3" ref-type="table">Table 3</xref> shows the computation time of ADAPTIVE and IOKR with linear and Gaussian kernels for prediction. The computation time was averaged over the 10-fold CV. This table shows that ADAPTIVE was significantly faster than IOKR. Specifically, under both linear and Gaussian kernels, ADAPTIVE with the fingerprint size of 100 was four to seven times faster than IOKR. This is because molecular vectors by ADAPTIVE are much more precise and adaptive to given data than those used in IOKR.</p>
        <table-wrap id="btz319-T3" orientation="portrait" position="float">
          <label>Table 3.</label>
          <caption>
            <p>Computation time for prediction by ADAPTIVE and IOKR</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Method</th>
                <th rowspan="1" colspan="1">Mol. vec. size</th>
                <th colspan="2" rowspan="1">prediction time (ms/example)<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1"/>
                <th rowspan="1" colspan="1">Linear</th>
                <th rowspan="1" colspan="1">Gaussian</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">IOKR</td>
                <td rowspan="1" colspan="1">2765</td>
                <td rowspan="1" colspan="1">140.22</td>
                <td rowspan="1" colspan="1">3352.4</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">ADAPTIVE</td>
                <td rowspan="1" colspan="1">100</td>
                <td rowspan="1" colspan="1">
                  <bold>20.32</bold>
                </td>
                <td rowspan="1" colspan="1">
                  <bold>802.6</bold>
                </td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">200</td>
                <td rowspan="1" colspan="1">39.88</td>
                <td rowspan="1" colspan="1">844.33</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1"/>
                <td rowspan="1" colspan="1">300</td>
                <td rowspan="1" colspan="1">54.14</td>
                <td rowspan="1" colspan="1">1071.8</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn2">
              <p><italic>Note</italic>: The smallest values (indicating the fastest) were in boldface for linear and Gaussian kernels.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
    </sec>
    <sec>
      <title>4.3 Case study</title>
      <p>To understand the results obtained by ADAPTIVE more, in the obtained molecular vectors, we examined substructures rooted at atoms, which activated several example features most. As shown in Section 3.2.1, each substructure rooted at an atom is represented by a state vector and contributes to computing the molecular vector of the whole molecule. Then, given a feature, we can estimate the contribution of each substructure by simply computing the softmax value from the corresponding state vector. We use these values of substructures as scores to rank substructures to activate the given feature.</p>
      <p><xref ref-type="fig" rid="btz319-F5">Figure 5</xref> shows three example features (#2, #39 and #83). For each feature, we show three substructures with the highest scores (each score is shown above the substructure). The first row shows three substructures which activated feature #2 most. Interestingly, we can see that these substructures share a further smaller, similar group of atoms: O, P and S (highlighted in blue). Similarly, the second row shows three substructures sharing a group of atoms: O and N, where these substructures activated feature #39 most. Also the third row shows substructures which activated feature #83, all having atom: Cl. Thus, <xref ref-type="fig" rid="btz319-F5">Figure 5</xref> shows that each feature of ADAPTIVE is activated by multiple different substructures sharing some similar properties, which must be important in data and probably for prediction. In contrast, each feature in regular molecular fingerprints is activated by only one predefined substructure. In summary, from this case study, learned features of ADAPTIVE are more concise and specific to the task of metabolite identification than regular molecular fingerprints, leading to the advantage of predictive performance and computation time.
</p>
      <fig id="btz319-F5" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Example features (#2, #39 and #83) and their three substructures (and their scores) which activated the corresponding feature most. Note that three substructures of each feature share a similar group (set) of atoms which are shown in blue</p>
        </caption>
        <graphic xlink:href="btz319f5"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>5 Discussion and conclusion</title>
    <p>Supervised learning for metabolite identification uses fingerprints as intermediate representation vectors between spectra and metabolites, while such fixed vectors are too redundant to cover all possible substructures and chemical properties in metabolites, causing limitations in predictive performance and high computational costs. To overcome this problem, we have proposed ADAPTIVE, which generates representations of metabolites specific to given spectrum-structure pairs. ADAPTIVE learns a model to generate molecular vectors for metabolites, which is parameterized by a MPNN over given molecular structures and trained through optimizing the objective function to maximize the correlation between molecular vectors and corresponding spectra. Our empirical validation of ADAPTIVE with the benchmark dataset showed the advantage of ADAPTIVE over existing methods including IOKR, the current cutting-edge method, both in predictive performance and computation time for prediction.</p>
    <p>A drawback of ADAPTIVE would be interpretability, because structural information is implicitly encoded in compact vectors in ADAPTIVE and cannot be made explicit easily. In metabolite identification, it would be desirable to connect the set of peaks to the corresponding substructures/chemical properties of metabolites (<xref rid="btz319-B14" ref-type="bibr">Nguyen <italic>et al.</italic>, 2018b</xref>). Developing a model with such interpretability would be interesting future work.</p>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Funding</title>
    <p>D.H.N. has been supported in part by Otsuka Toshimi scholarship and JSPS KAKENHI [grant number 19J14714]. C.H.N. has been supported in part by MEXT Kakenhi 18K11434. H.M. has been supported in part by JST ACCEL [grant number JPMJAC1503], MEXT Kakenhi [grant numbers 16H02868 and 19H04169], FiDiPro by Tekes (currently Business Finland) and AIPSE program by Academy of Finland.</p>
    <p><italic>Conflict of Interest</italic>: none declared.</p>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz319-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brouard</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Fast metabolite identification with input output kernel regression</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>i28</fpage>–<lpage>i36</lpage>.<pub-id pub-id-type="pmid">27307628</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B2">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>de Hoffmann</surname><given-names>E.</given-names></name>, <name name-style="western"><surname>Stroobant</surname><given-names>V.</given-names></name></person-group> (<year>2007</year>). <source>Mass Spectrometry, Principles and Applications</source>. <edition>3</edition>rd edn. 
<publisher-name>John Wiley &amp; Sons</publisher-name>, Hoboken, New York.</mixed-citation>
    </ref>
    <ref id="btz319-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dührkop</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Searching molecular structure databases with tandem mass spectra using CSI:FingerID</article-title>. <source>Proc. Natl. Acad. Sci</source>., <volume>112</volume>, <fpage>12580</fpage>–<lpage>12585</lpage>.<pub-id pub-id-type="pmid">26392543</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Duvenaud</surname><given-names>D.K.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>). <chapter-title>Convolutional networks on graphs for learning molecular fingerprints</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Cortes</surname><given-names>C.</given-names></name>, <name name-style="western"><surname>Lawrence</surname><given-names>N.D.</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>D.D.</given-names></name>, <name name-style="western"><surname>Sugiyama</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Garnett</surname><given-names>R.</given-names></name></person-group> (eds.) <source>Proceedings of the 28th International Conference on Neural Information Processing Systems</source>, Vol. 2. 
<publisher-name>Curran Associates, Inc</publisher-name>, Montreal, Canada, pp. <fpage>2224</fpage>–<lpage>2232</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gilmer</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>). <chapter-title>Neural message passing for quantum chemistry</chapter-title> In: <person-group person-group-type="editor"><name name-style="western"><surname>Precup</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Teh</surname><given-names>Y.W.</given-names></name></person-group> (eds.) <source>Proceedings of the 34th International Conference on Machine Learning, Volume 70 of Proceedings of Machine Learning Research</source>. 
<publisher-name>International Convention Centre, PMLR</publisher-name>, 
<publisher-loc>Sydney, Australia</publisher-loc>, pp. <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gönen</surname><given-names>M.</given-names></name>, <name name-style="western"><surname>Alpaydin</surname><given-names>E.</given-names></name></person-group> (<year>2011</year>) 
<article-title>Multiple kernel learning algorithms</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2211</fpage>–<lpage>2268</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gretton</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>). <chapter-title>Measuring statistical dependence with Hilbert-Schmidt norms</chapter-title> In: <source>Proceedings of the 16th International Conference on Algorithmic Learning Theory, ALT’05</source>. 
<publisher-name>Springer-Verlag</publisher-name>, 
<publisher-loc>Berlin, Heidelberg</publisher-loc>, pp. <fpage>63</fpage>–<lpage>77</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heinonen</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2012</year>) 
<article-title>Metabolite identification and molecular fingerprint prediction through machine learning</article-title>. <source>Bioinformatics</source>, <volume>28</volume>, <fpage>2333</fpage>–<lpage>2341</lpage>.<pub-id pub-id-type="pmid">22815355</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jebara</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2004</year>) 
<article-title>Probability product kernels</article-title>. <source>J. Mach. Learn. Res</source>., <volume>5</volume>, <fpage>819</fpage>–<lpage>844</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kipf</surname><given-names>T.N.</given-names></name>, <name name-style="western"><surname>Welling</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). Semi-supervised classification with graph convolutional networks. <italic>arXiv preprint arXiv: 1609.02907</italic>.</mixed-citation>
    </ref>
    <ref id="btz319-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Gated graph sequence neural networks</article-title>. <source>CoRR</source>, abs/1511.05493.</mixed-citation>
    </ref>
    <ref id="btz319-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Micchelli</surname><given-names>C.A.</given-names></name>, <name name-style="western"><surname>Pontil</surname><given-names>M.A.</given-names></name></person-group> (<year>2005</year>) 
<article-title>On learning vector-valued functions</article-title>. <source>Neural Comput</source>., <volume>17</volume>, <fpage>177</fpage>–<lpage>204</lpage>.<pub-id pub-id-type="pmid">15563752</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B13">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>D.H.</given-names></name></person-group><etal>et al</etal> (<year>2018a</year>). Recent advances and prospects of computational methods for metabolite identification: a review with emphasis on machine learning approaches. <italic>Brief. Bioinf.</italic> doi: 10.1093/bib/bby066. [Epub ahead of print].</mixed-citation>
    </ref>
    <ref id="btz319-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>D.H.</given-names></name></person-group><etal>et al</etal> (<year>2018b</year>) 
<article-title>Simple: sparse interaction model over peaks of molecules for fast, interpretable metabolite identification from tandem mass spectra</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i323</fpage>–<lpage>i332</lpage>.<pub-id pub-id-type="pmid">29950009</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B15">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Nguyen</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>). Semi-supervised learning of hierarchical representations of molecules using neural message passing. <italic>CoRR</italic>, abs/1711.10168.</mixed-citation>
    </ref>
    <ref id="btz319-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rasche</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2011</year>) 
<article-title>Computing fragmentation trees from tandem mass spectrometry data</article-title>. <source>Anal. Chem</source>., <volume>83</volume>, <fpage>1243</fpage>–<lpage>1251</lpage>. PMID: 21182243.<pub-id pub-id-type="pmid">21182243</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B114">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tokui</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Chainer: a next-generation open source framework for deep learning</article-title>. In: <source>Proceedings of Workshop on Machine Learning Systems (LearningSys) in the Twenty-Ninth Annual Conference on Neural Information Processing Systems (NIPS)</source>, Vol. 5, pp. 1–6.</mixed-citation>
    </ref>
    <ref id="btz319-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vaniya</surname><given-names>A.</given-names></name>, <name name-style="western"><surname>Fiehn</surname><given-names>O.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Using fragmentation trees and mass spectral trees for identifying unknown compounds in metabolomics</article-title>. <source>Trends Analyt. Chem</source>., <volume>69</volume>, <fpage>52</fpage>–<lpage>61</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wishart</surname><given-names>D.S.</given-names></name></person-group> (<year>2007</year>) 
<article-title>Current progress in computational metabolomics</article-title>. <source>Brief. Bioinf</source>., <volume>8</volume>, <fpage>279</fpage>–<lpage>293</lpage>.</mixed-citation>
    </ref>
    <ref id="btz319-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wishart</surname><given-names>D.S.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>HMDB 3.0—the human metabolome database in 2013</article-title>. <source>Nucleic Acids Res</source>., <volume>41</volume>, <fpage>D801</fpage>–<lpage>D807</lpage>.<pub-id pub-id-type="pmid">23161693</pub-id></mixed-citation>
    </ref>
    <ref id="btz319-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Yamada</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>). Post selection inference with kernels. In: Storkey,A. and Perez-Cruz,F. (eds.) <italic>Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, volume 84 of Proceedings of Machine Learning Research</italic>. Playa Blanca, PMLR, Lanzarote, Canary Islands, pp. 152–160.</mixed-citation>
    </ref>
    <ref id="btz319-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Q.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Large-scale kernel methods for independence testing</article-title>. <source>Stat. Comput</source>., <volume>28</volume>, <fpage>113</fpage>–<lpage>130</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
