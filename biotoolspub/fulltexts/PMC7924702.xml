<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PeerJ Comput Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">PeerJ Comput Sci</journal-id>
    <journal-id journal-id-type="pmc">peerj-cs</journal-id>
    <journal-id journal-id-type="publisher-id">peerj-cs</journal-id>
    <journal-title-group>
      <journal-title>PeerJ Computer Science</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2376-5992</issn>
    <publisher>
      <publisher-name>PeerJ Inc.</publisher-name>
      <publisher-loc>San Diego, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7924702</article-id>
    <article-id pub-id-type="publisher-id">cs-176</article-id>
    <article-id pub-id-type="doi">10.7717/peerj-cs.176</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Computer Vision</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Data Science</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Robotics</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Software Engineering</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EPypes: a framework for building event-driven data processing pipelines</article-title>
    </title-group>
    <contrib-group>
      <contrib id="author-1" contrib-type="author" corresp="yes">
        <name>
          <surname>Semeniuta</surname>
          <given-names>Oleksandr</given-names>
        </name>
        <xref ref-type="aff" rid="aff-1">1</xref>
        <email>oleksandr.semeniuta@ntnu.no</email>
      </contrib>
      <contrib id="author-2" contrib-type="author">
        <name>
          <surname>Falkman</surname>
          <given-names>Petter</given-names>
        </name>
        <xref ref-type="aff" rid="aff-2">2</xref>
      </contrib>
      <aff id="aff-1"><label>1</label><institution>Department of Manufacturing and Civil Engineering, NTNU Norwegian University of Science and Technology</institution>, <addr-line>Gjøvik</addr-line>, <country>Norway</country></aff>
      <aff id="aff-2"><label>2</label><institution>Department of Electrical Engineering, Chalmers University of Technology</institution>, <addr-line>Gothenburg</addr-line>, <country>Sweden</country></aff>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Bernardi</surname>
          <given-names>Mario Luca</given-names>
        </name>
      </contrib>
    </contrib-group>
    <pub-date pub-type="epub" date-type="pub" iso-8601-date="2019-02-11">
      <day>11</day>
      <month>2</month>
      <year iso-8601-date="2019">2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>5</volume>
    <elocation-id>e176</elocation-id>
    <history>
      <date date-type="received" iso-8601-date="2018-08-31">
        <day>31</day>
        <month>8</month>
        <year iso-8601-date="2018">2018</year>
      </date>
      <date date-type="accepted" iso-8601-date="2019-01-22">
        <day>22</day>
        <month>1</month>
        <year iso-8601-date="2019">2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 Semeniuta and Falkman</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Semeniuta and Falkman</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, reproduction and adaptation in any medium and for any purpose provided that it is properly attributed. For attribution, the original author(s), title, publication source (PeerJ Computer Science) and either DOI or URL of the article must be cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="https://peerj.com/articles/cs-176"/>
    <abstract>
      <p>Many data processing systems are naturally modeled as pipelines, where data flows though a network of computational procedures. This representation is particularly suitable for computer vision algorithms, which in most cases possess complex logic and a big number of parameters to tune. In addition, online vision systems, such as those in the industrial automation context, have to communicate with other distributed nodes. When developing a vision system, one normally proceeds from ad hoc experimentation and prototyping to highly structured system integration. The early stages of this continuum are characterized with the challenges of developing a feasible algorithm, while the latter deal with composing the vision function with other components in a networked environment. In between, one strives to manage the complexity of the developed system, as well as to preserve existing knowledge. To tackle these challenges, this paper presents EPypes, an architecture and Python-based software framework for developing vision algorithms in a form of computational graphs and their integration with distributed systems based on publish-subscribe communication. EPypes facilitates flexibility of algorithm prototyping, as well as provides a structured approach to managing algorithm logic and exposing the developed pipelines as a part of online systems.</p>
    </abstract>
    <kwd-group kwd-group-type="author">
      <kwd>Computer vision</kwd>
      <kwd>Computational graph</kwd>
      <kwd>Publish-subscribe</kwd>
      <kwd>Robotics</kwd>
      <kwd>Python</kwd>
      <kwd>Pipeline</kwd>
      <kwd>Distributed systems</kwd>
      <kwd>Algorithm development</kwd>
      <kwd>Event-driven systems</kwd>
      <kwd>Concurrency</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="fund-1">
        <funding-source>MultiMat project and SFI Manufacturing, funded by the Norwegian Research Council</funding-source>
      </award-group>
      <funding-statement>This paper was written in association with the MultiMat project and SFI Manufacturing, funded by the Norwegian Research Council. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro">
    <title>Introduction</title>
    <p>In recent years, the increased availability of computational resources, coupled with the advances in machine learning methods and ability to gather large amounts of data, opened new possibilities of developing more advanced data-driven systems. Visual data, acquired by various types of imaging equipment, constitutes one of the main inputs to advanced data analysis algorithms.</p>
    <p>In manufacturing automation, vision systems has a long history of use in combination with dedicated automated equipment and industrial robots, serving a role of contact-less sensing for, amongst others, quality inspection and robot guidance. What differentiates industrial vision solutions from general-purpose computer vision systems, is their coupling with the associated mechatronic components possessing an actuation function. This entails that most industrial vision systems operate in online mode, with their operation being synchronized with external systems by various forms of remote communication.</p>
    <p>When a new robotic system with vision sensing is developed, the early-stage system prototyping favors flexible tools and techniques that allow for iterating toward a functional solution quickly. When it comes to computer vision prototyping, the tools of the trade include OpenCV, as well as libraries from the Python data science ecosystem, most notably NumPy, SciPy, Pandas, Scikit-learn, Scikit-image, and others. Vision algorithm development is a challenging task in itself, as it requires a great deal of experimentation and tuning of numerous parameters and thresholds. Another challenge with early-stage prototyping of vision algorithms to be used with robotics and automation solutions is their coupling to other networked components. Establishing communication interfaces can be time consuming, and is often done as a patchwork, which is difficult to maintain.</p>
    <p>Many data processing systems can be logically modeled as direct graphs in which data is being gradually processed by the computational nodes. This is particularly characteristic of vision systems: after image capture and acquisition, an input image is obtained in memory and fed to a series of transformations leading to the application-specific output. Such pipeline can be comprised of the steps of image enhancement, image segmentation, and feature detection (<xref ref-type="fig" rid="fig-1">Fig. 1</xref>).</p>
    <fig id="fig-1" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-1</object-id>
      <label>Figure 1</label>
      <caption>
        <title>Common steps of a vision pipeline.</title>
      </caption>
      <graphic xlink:href="peerj-cs-05-176-g001"/>
    </fig>
    <p>This idea has been formalized with the abstract concept of data flow, and has found its application in many areas, including distributed data processing, machine learning, embedded software development, and digital signal processing. MATLAB Simulink and LabVIEW are the traditional engineering tools whose programming model is based on data flow. In data engineering and data science areas, tools like Apache Storm, Apache Airflow, Luigi, and Dask employ explicit data flow construction and execution. Needless to mention that the deep learning libraries, such as TensorFlow, Caffe, and Theano, construct and train models as directed acyclic graphs (DAGs).</p>
    <p>This paper tackles the problems of both (1) vision algorithms development and (2) their integration into distributed environments. This is done by introducing EPypes, a Python library<xref ref-type="fn" rid="fn-1"><sup>1</sup></xref><fn id="fn-1"><label>1</label><p>The EPypes implementation is available under the 3-clause BSD license at <uri xlink:href="https://github.com/semeniuta/EPypes">https://github.com/semeniuta/EPypes</uri>.</p></fn> for construction and execution of computational graphs, with the built-in capability of exposing the graphs as reactive pipelines. The latter are intended to be a part of publish-subscribe systems. In addition to the software tools, this paper presents a system development method that facilitates transition from ad hoc prototyping phase to well-structured system integration phase without compromising the developers’ flexibility.</p>
    <p>The practical applicability of the proposed framework is validated in a distributed experimental setup comprised of a robot, an image acquisition service, and an image processing component, communicating in a publish-subscribe manner using ZeroMQ middleware. It is shown that the EPypes architecture facilitates seamless transition between various deployment configurations in a distributed computing environment.</p>
    <p>This paper is structured as follows. First, the background areas are introduced, including overview of computational systems based on DAGs, the Python data science/computer vision ecosystem, and event-based middleware. Further, the EPypes abstractions are presented with code examples and architectural relationships. Finally, a distributed system experiment based on EPypes provides a more detailed view into the runtime properties of the framework.</p>
  </sec>
  <sec sec-type="background">
    <title>Background</title>
    <sec>
      <title>Graph-based representation of computational systems</title>
      <p>A wide range of computational systems, particularly those with streaming behavior, can be represented as directed graphs, in which data is routed through processing nodes. Not only is this representation accessible to human understanding (particularly for engineers), but it also has been used in various settings to realize improvement of the function of the systems.</p>
      <p>Control engineering and signal processings has a long tradition of graphically modeling systems in a form of block diagrams. MATLAB Simulink and LabVIEW are widely used in this context as engineering tools with formally defined abstractions. The field of cyber-physical systems (CPS) makes great use of graph-based system models together with the associated models of computations (<xref rid="ref-9" ref-type="bibr">Lee &amp; Seshia, 2011</xref>). A notable CPS modeling environment is Ptolemy II.</p>
      <p>In computer science, graph-based representation of systems has been used for a range of different purposes: data flow models, task graphs (for parallel processing scheduling), symbolic representation of computational expressions (for machine learning and automatic computation of gradients), representation of concurrent process networks (e.g., Communicating Sequential Processes), workflow languages, etc. In the software engineering community, the <italic>pipes and filters</italic> architecture applies the same ideas to data processing systems design and development. The well-known pipes mechanism of Unix-like operating systems has proved to be particularly powerful when it comes to composition of multiple tools to solve a complex task.</p>
      <p>Data science has seen a surge of tools based on explicit handling of data processing systems in a form of DAG. Many of them are intended to be run on a computing cluster, and the DAG architecture in this case facilitates scheduling of parallel execution of data processing tasks. Apache Storm is a cluster-based stream processing engine. Apache Airflow is workflow management platform for batch processing on a cluster. Dask is a Python parallelization library that utilizes DAG modeling for scaling algorithms written with NumPy and Pandas primitives to be used with massive datasets.</p>
    </sec>
    <sec>
      <title>Python data science/computer vision ecosystem</title>
      <p>The open source movement has gained a big popularity within the fields of data science, computer vision, and robotics in recent years. Even though the established proprietary engineering tools are pervasive in the industrial context, they often lack flexibility and hinder a deeper understanding of how a system functions. Conversely, open source tools provide community-contributed implementation of common functionality, which is flexible to use and allows for building more scalable and reproducible solutions.</p>
      <p>In computer vision, the OpenCV library has become a de-facto standard providing a pool of community-contributed image processing and computer vision algorithms. Similarly, the point cloud library (PCL) provides open-source routines for point clouds processing. A multitude of tools from the Python ecosystem are widely used for data science and scientific computing. They are built upon the NumPy array library, and include Pandas, Scikit-learn, Scikit-image, and many others. The abovementioned OpenCV and PCL, as well as many other low-level tools, expose Python bindings, which makes it possible to perform rapid system developed with preserved high performance of the applied algorithms.</p>
    </sec>
    <sec>
      <title>Events and publish-subscribe middleware</title>
      <p>An event-driven system is characterized by a discrete state space, where state transition happen on occurrence of events at sporadic time instants (<xref rid="ref-2" ref-type="bibr">Cassandras &amp; Lafortune, 2008</xref>). In distributed systems, events are often embodied as messages sent over a network in a <italic>publish-subscribe</italic> communication system. Such messages can signalize a change of a system state (change event) or a notification from an observation (status event), expressed as a tuple with a timestamp and an application-specific descriptive parameters (<xref rid="ref-7" ref-type="bibr">Hinze, Sachs &amp; Buchmann, 2009</xref>). Message-based middleware provides a unified set of communication and input/output capabilities in such <italic>sense-respond</italic> systems.</p>
      <p>Middleware allows to decouple the communicating components by introducing message queuing, built-in address resolution (e.g., via handling logical addresses such as topic names), and usage of a common data serialization format (<xref rid="ref-10" ref-type="bibr">Magnoni, 2015</xref>).</p>
      <p>The defining components of a particular middleware solution are the communication protocol (transport-level TCP and UDP, wire-level AMQP, ZeroMQ/ZMTP, MQTT), the communication styles (request/reply, publish/subscribe), and the data serialization method (typically realized with an interface definition language like Protobuf or Apache Thrift). Many middleware solutions are based on a central broker, for example, ActiveMQ and RabbitMQ. The additional hop through the broker adds a constant value to the communication latency (<xref rid="ref-4" ref-type="bibr">Dworak et al., 2012</xref>). ZeroMQ is an example of broker-less middleware, in which the message queuing logic runs locally within each communicating component (<xref rid="ref-16" ref-type="bibr">ZeroMQ, 2008</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="epypes">
    <title>Epypes</title>
    <p>EPypes is a Python-based software framework that combines <italic>pipes and filters</italic> and <italic>publish-subscribe</italic> architectures. It allows to develop data processing pipelines, the behavior of which is defined by their response to events. EPypes defines a <italic>computational graph</italic>, which is a static data structure modeling a data processing algorithm, abstractions used for execution of computational graphs, and a hierarchy of <italic>pipelines</italic>, which extend the algorithm logic defined with computational graphs to be a part of a publish-subscribe system.</p>
    <sec>
      <title>Computational graph</title>
      <p>At the core of EPypes lies <monospace>CompGraph</monospace>, a data structure that models a data processing algorithm as a computational graph, that is, as a network of functions and data tokens. Formally, a <monospace>CompGraph</monospace> can be described as a bipartite DAG <italic>G</italic>:
<disp-formula id="eqn-1"><alternatives><graphic xlink:href="peerj-cs-05-176-e001.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$G = (F,T,E)$$\end{document}</tex-math><mml:math id="mml-eqn-1"><mml:mrow><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
where <italic>F</italic> is a set of functions, <italic>T</italic> is a set of data tokens, and <italic>E</italic> is a set of directed edges between functions and tokens and vice-versa. The latter implies that edges of only the following two kinds are permitted: (<italic>f</italic>, <italic>t<sub>i</sub></italic>), where <italic>f</italic> ∈ <italic>F</italic>, <italic>t<sub>i</sub></italic> ∈ <italic>T</italic>, and (<italic>t<sub>j</sub></italic>, <italic>g</italic>), where <italic>g</italic> ∈ <italic>F</italic>, <italic>t<sub>j</sub></italic> ∈ <italic>T</italic>.</p>
      <p>A function <italic>f</italic> ∈ <italic>F</italic> is associated with a Python callable. A token <italic>t</italic> ∈ <italic>T</italic> represents a data object of an arbitrary type. If function <italic>f</italic> correspond to a callable with <italic>m</italic> input parameters and <italic>n</italic> outputs, it has to be connected to <italic>n</italic> input and <italic>m</italic> output tokens. Let <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-05-176-i001.jpg"/><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\rm{I}}{{\rm{n}}_f} \subset T$\end{document}</tex-math><mml:math id="mml-ieqn1"><mml:mrow><mml:msub><mml:mrow><mml:mtext>In</mml:mtext></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo>⊂</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denote the set of input tokens to <italic>f</italic>, and <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-05-176-i002.jpg"/><tex-math id="M3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\rm{Ou}}{{\rm{t}}_f} \subset T$\end{document}</tex-math><mml:math id="mml-ieqn-2"><mml:mrow><mml:msub><mml:mrow><mml:mtext>Out</mml:mtext></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mo>⊂</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></alternatives></inline-formula> denote the set of output tokens from <italic>f</italic>.</p>
      <p>Functions in <italic>G</italic> are uniquely identified by their string-based names. This allows to use the same Python callable several times in the computational graph.</p>
      <p>Once a computational graph <italic>G</italic> is constructed, and it conforms to the requirements of acyclicity, its execution can be scheduled. Topological sort of <italic>G</italic> results in an order of vertices (functions and tokens) so that all the directed edges point from a vertex earlier in the order to a vertex later in the order. With invoking functions in this topological order, all the precedence constraints will be satisfied.</p>
      <p>For many computational procedures, one can typically distinguish between parameters carrying the primary data entities and parameters that tune the procedure. In this paper, the former are referred to as <italic>payload parameters</italic>, and the latter as <italic>hyperparameters</italic><xref ref-type="fn" rid="fn-2"><sup>2</sup></xref><fn id="fn-2"><label>2</label><p>The term <italic>hyperparameters</italic> is borrowed from machine learning, where it refers to parameters that characterize a particular algorithm, as opposed to model parameters. Semantics of hyperparameter tokens in this paper is similar, although the considered computational graphs can be used to model a wide variety of algorithms.</p></fn>. Thus, tokens belonging to these two parameter sets of function <italic>f</italic> form the input parameter set of <italic>f</italic>: In<sub><italic>f</italic></sub> = <italic>P<sub>f</sub></italic> ∪ <italic>H<sub>f</sub></italic>. It is further presumed that all hyperparameter tokens are <italic>frozen</italic>, that is, given fixed values, during the construction of graph <italic>G</italic>. The set of non-frozen source tokens is referred to as <italic>free source tokens</italic>, and is used to provide input to the computational graph.</p>
      <p>In the computational graph example shown in <xref ref-type="fig" rid="fig-2">Fig. 2</xref>, rectangle vertices represent functions in the function set <italic>F</italic> = {<italic>f</italic><sub>1</sub>, <italic>f</italic><sub>2</sub>, <italic>f</italic><sub>3</sub>}, white circular vertices represent payload tokens, and gray circular vertices—hyperparameter tokens. In accordance with the previously defined notation, each function in <italic>F</italic> has the following associated token sets:
<disp-formula id="eqn-2"><alternatives><graphic xlink:href="peerj-cs-05-176-e002.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\matrix{ {{f_1}} &amp; {{H_{{f_1}}} = \{ {t_2},{t_3}\} } &amp; {{P_{{f_1}}} = \{ {t_1}\} } &amp; {{\rm{Ou}}{{\rm{t}}_{{f_1}}} = \{ {t_4},{t_5}\} } \cr {{f_2}} &amp; {{H_{{f_2}}} = \emptyset } &amp; {{P_{{f_2}}} = \{ {t_4}\} } &amp; {{\rm{Ou}}{{\rm{t}}_{{f_2}}} = \{ {t_6}\} } \cr {{f_3}} &amp; {{H_{{f_3}}} = \{ {t_7}\} } &amp; {{P_{{f_3}}} = \{ {t_5},{t_6}\} } &amp; {{\rm{Ou}}{{\rm{t}}_{{f_3}}} = \{ {t_8}\} } \cr } $$\end{document}</tex-math><mml:math id="mml-eqn-2"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mtext>Out</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∅</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mtext>Out</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mtext>Out</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>8</mml:mn></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives></disp-formula>
Token <italic>t</italic><sub>1</sub> is the only free source token, and its value is required to perform a computation.</p>
      <fig id="fig-2" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-2</object-id>
        <label>Figure 2</label>
        <caption>
          <title>An example abstract computational graph.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g002"/>
      </fig>
    </sec>
    <sec>
      <title>A concrete example</title>
      <p>Consider a simple computational graph that defines a processing chain in which a color image is first converted to grayscale, then blurred with a Gaussian kernel, with the blurred image further used to perform edge detection with the Canny algorithm.</p>
      <p>The following listing shows the steps of the computational graph construction.</p>
      <code language="java" position="float" orientation="portrait" xml:space="preserve">
<bold>import</bold> cv2

<bold>def</bold> grayscale (im):
   <bold>return</bold> cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)

<bold>def</bold> gaussian_blur(img,kernel_size):
   <bold>return</bold> cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)

func_dict = {
   ‘grayscale’: grayscale,
   ‘canny’: cv2.Canny,
   ‘blur’: gaussian_blur
}

func_io = {
   ‘grayscale’: (‘image,’ ‘image_gray’),
   ‘blur’: ((‘image_gray,’ ‘blur_kernel’),’image_blurred’),
   ‘canny’:((‘image_blurred,’’canny_lo,’ ‘canny_hi’), ‘edges’),
}

cg = CompGraph(func_dict, func_io)</code>
      <p>After importing the OpenCV Python module (<monospace>cv2</monospace>), two helper functions are defined for grayscaling and blurring (the function for edge detection is used as-is). The structure of the computational graph is specified as two dictionaries. The <monospace>func_dict</monospace> dictionary defines mapping from unique function identifiers (in this case, strings <monospace>“grayscale”</monospace>, <monospace>“blur”</monospace>, <monospace>“canny”</monospace>) to the respective callable objects. The <monospace>func_io</monospace> dictionary defines input/output relationships between the functions in a form of tokens. Each function identifier is mapped to a tuple describing input and output tokens that can be one of the following forms, depending on the respective functions’ signatures:
<list list-type="bullet"><list-item><p>(<italic>x</italic>, <italic>y</italic>) for single input and single output;</p></list-item><list-item><p>((<italic>x</italic><sub>1</sub>, …, <italic>x<sub>m</sub></italic>), <italic>y</italic>) for multiple inputs and single output;</p></list-item><list-item><p>(<italic>x</italic>, (<italic>y</italic><sub>1</sub>, …, <italic>y<sub>n</sub></italic>)) for single input and multiple outputs;</p></list-item><list-item><p>((<italic>x</italic><sub>1</sub>, …, <italic>x<sub>m</sub></italic>), (<italic>y</italic><sub>1</sub>, …, <italic>y<sub>n</sub></italic>)) for multiple inputs and multiple outputs.</p></list-item></list></p>
      <p>An instance of <monospace>CompGraph</monospace> is then constructed based on <monospace>func_dict</monospace> and <monospace>func_io</monospace>.</p>
      <p>To be executable, a computational graph has to be supplied to the constructor of <monospace>CompGraphRunner</monospace>. The latter is used to store the hyperparameter tokens and schedule execution of the graph with the topological sort. Internally <monospace>CompGraphRunner</monospace> delegates storage and retrieval of token data to an instance of <monospace>TokenManager</monospace> (<xref ref-type="fig" rid="fig-3">Fig. 3</xref>).</p>
      <fig id="fig-3" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-3</object-id>
        <label>Figure 3</label>
        <caption>
          <title>Class digram of EPypes abstractions dealing with computational graphs.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g003"/>
      </fig>
      <p>In the following example, we specify the Gaussian blur kernel, and low/high threshold of the Canny algorithm in dictionary <monospace>params</monospace>. The latter, together with the original computational graph <monospace>cg</monospace> is used to construct a <monospace>CompGraphRunner</monospace>:
<code language="java" position="float" orientation="portrait" xml:space="preserve">
hparams = {
    ‘blur_kernel’: 11,
    ‘canny_lo’: 70,
    ‘canny_hi’: 200
}

runner = CompGraphRunner(cg, hparams)
</code>
</p>
      <p>Visualization of this parametrized computational graph is shown in <xref ref-type="fig" rid="fig-4">Fig. 4</xref>. The hyperparameter tokens are highlighted in gray.</p>
      <fig id="fig-4" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-4</object-id>
        <label>Figure 4</label>
        <caption>
          <title>Computational graph for edge detection.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g004"/>
      </fig>
      <p>To run a <monospace>CompGraphRunner</monospace>, its <monospace>run</monospace> method is invoked with keyword arguments corresponding to names and values of free source tokens. In the provided example the only free source token is <monospace>image</monospace>. Therefore, the running syntax is the following:
<code language="java" position="float" orientation="portrait" xml:space="preserve">
im = cv2.imread(‘image.jpg,’cv2.IMREAD_COLOR)
runner.run(image=im)
</code>
</p>
      <p>A <monospace>CompGraphRunner</monospace> can be used as a namespace for accessing any token value by the token key. The interface for this operation is the same as for a Python dictionary. For example, to visualize the blurred image from the computational graph in <xref ref-type="fig" rid="fig-4">Fig. 4</xref> using Matplotlib, the following syntax is applied:
<code language="java" position="float" orientation="portrait" xml:space="preserve">
plt.imshow(runner[‘image_blurred’])
</code>
</p>
    </sec>
    <sec>
      <title>Pipelines</title>
      <p>To introduce additional functionality to algorithms expressed as computational graphs and transform them into runtime reactive components, a hierarchy of <italic>pipeline</italic> classes is defined.</p>
      <p>As shown in <xref ref-type="fig" rid="fig-5">Fig. 5</xref>, the basic building block of EPypes pipelines is a <monospace>Node</monospace>, which is a runtime counterpart to a function. An instance of <monospace>Node</monospace> based on function <italic>f</italic> can be invoked as a callable object, with parameter values corresponding to the positional input arguments of <italic>f</italic>. A network of node instances corresponding to the graph <italic>G</italic> form a <monospace>NodeBasedCompGraph</monospace>. The latter constitutes the main component of a <monospace>Pipeline</monospace>, as well as its subclasses (<monospace>SourcePipeline</monospace>, <monospace>SinkPipeline</monospace>, and <monospace>FullPipeline</monospace>).</p>
      <fig id="fig-5" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-5</object-id>
        <label>Figure 5</label>
        <caption>
          <title>Class digram of EPypes <monospace>Pipelines</monospace>.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g005"/>
      </fig>
      <p>An instance of the <monospace>Pipeline</monospace> class is constructed similarly to the earlier example of <monospace>CompGraphRunner</monospace>, but with the additional name argument:
<code language="java" position="float" orientation="portrait" xml:space="preserve">
pipe = Pipeline(‘MyPipeline’, cg, hparams)
</code>
</p>
      <p>Because <monospace>Pipeline</monospace> is defined as a subclass of <monospace>Node</monospace>, its instances constitute callable objects, and are functionally equivalent to instances of <monospace>Node</monospace>. The whole pipeline is orchestrated by an instance of <monospace>CompGraphRunner</monospace> (<xref ref-type="fig" rid="fig-5">Fig. 5</xref>). The internal structure of a <monospace>Pipeline</monospace> is visualized in <xref ref-type="fig" rid="fig-6">Fig. 6</xref>.</p>
      <fig id="fig-6" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-6</object-id>
        <label>Figure 6</label>
        <caption>
          <title>Structure of an instance of <monospace>Pipeline</monospace>.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g006"/>
      </fig>
      <p>Additional capabilities of a <monospace>Pipeline</monospace>, as compared with a raw <monospace>CompGraphRunner</monospace>, include time measurement of nodes’ durations, computation of computational graph overhead, storage of additional attributes, and other functionality added by subclassing <monospace>Pipeline</monospace>.</p>
      <p>To allow for reactive behavior of pipelines, they are combined with event queues, which can be used for subscription to triggering events and publishing the results of data processing. To realize this, aside from <monospace>Pipeline</monospace>, which is not reactive, three other types of pipelines, coupled with event queues, are defined. Reactive pipelines operate in context of <italic>thread-based concurrency</italic> with blocking queues as the synchronization mechanism. In the Python standard library, a <monospace>queue.Queue</monospace> object can be used to communicate between two threads: the <italic>producer</italic> thread puts an object on the queue, and the <italic>consumer</italic> thread request the object and blocks until the latter becomes available. The principle of such interaction is shown in a sequence diagram in <xref ref-type="fig" rid="fig-7">Fig. 7</xref>.</p>
      <fig id="fig-7" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-7</object-id>
        <label>Figure 7</label>
        <caption>
          <title>Sequence diagram of thread-based producer and consumer interacting through a queue.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g007"/>
      </fig>
      <p>A <monospace>SourcePipeline</monospace>, see <xref ref-type="fig" rid="fig-8">Fig. 8</xref>, is a subclass of <monospace>Pipeline</monospace> whose final output is put to the output queue <italic>q</italic><sub>out</sub>. A <monospace>SourcePipeline</monospace> is in addition parametrized by <italic>f</italic><sub>out</sub>, an output preparation function, responsible for packaging the chosen data from the pipeline tokens into a single message that gets published on <italic>q</italic><sub>out</sub>.</p>
      <fig id="fig-8" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-8</object-id>
        <label>Figure 8</label>
        <caption>
          <title>Structure of an instance of <monospace>SourcePipeline</monospace>.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g008"/>
      </fig>
      <p>An instance of <monospace>SourcePipeline</monospace> is constructed as follows:
<code language="java" position="float" orientation="portrait" xml:space="preserve">
src_pipe = SourcePipeline(‘MySourcePipeline’, cg, q_out, f_out, hparams)
</code></p>
      <p>As an example of the output preparation function, consider a pipeline, whose computational graph contains a token with the key <monospace>pose</monospace>, corresponding to a 3D pose estimated from images. To take the data corresponding to this token and package it as a Python pickle, the following function can be defined:
<code language="java" position="float" orientation="portrait" xml:space="preserve">
<bold>def</bold> prepare_output(pipe):
   pose = pipe[‘pose’]
   wire_data = pickle.dumps(pose)
   <bold>return</bold> wire_data
</code></p>
      <p>Another subclass of <monospace>Pipeline</monospace> is <monospace>SinkPipeline</monospace>, shown in <xref ref-type="fig" rid="fig-9">Fig. 9</xref>. It is meant not to be called manually, but to be triggered as an event <italic>e</italic> is announced in <italic>q</italic><sub>in</sub>. Because <italic>e</italic> can be an arbitrary object, it is necessary to map its contents to a dictionary <italic>d<sub>e</sub></italic> that describes what data should correspond to the pipeline’s free source tokens. Such mapping is defined by event dispatcher function <italic>f</italic><sub>in</sub>.</p>
      <fig id="fig-9" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-9</object-id>
        <label>Figure 9</label>
        <caption>
          <title>Structure of an instance of <monospace>SinkPipeline</monospace>.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g009"/>
      </fig>
      <p>An instance of <monospace>SinkPipeline</monospace> is constructed in a familiar way:
<code language="java" position="float" orientation="portrait" xml:space="preserve">
snk_pipe = SinkPipeline(‘MySinkPipeline’, cg, q_in, f_in, hparams)
</code>
</p>
      <p>The idea of event dispatcher can be illustrated by referring to the computational graph in the earlier example (<xref ref-type="fig" rid="fig-4">Fig. 4</xref>). Consider that <italic>e</italic> constitutes an image as a <monospace>numpy.ndarray</monospace>. Because a <monospace>CompGraphRunner</monospace> is invoked with keyword arguments, <italic>f</italic><sub>in</sub> is defined to map to the required <monospace>kwargs</monospace> dictionary:
<code language="java" position="float" orientation="portrait" xml:space="preserve"><bold>def</bold> dispatch_image(im):
   <bold>return</bold> {‘image’: im}
</code></p>
      <p>The behavior of waiting for an event is realized with an event loop, an instance of <monospace>EventLoop</monospace> class, which is continuously run in a separate thread of execution. It monitors <italic>q</italic><sub>in</sub>, and, as a new event <italic>e</italic> becomes available, invokes the associated instance of <monospace>SinkPipeline</monospace> (<xref ref-type="fig" rid="fig-9">Fig. 9</xref>) having the <monospace>kwargs</monospace> from the event dispatcher:
<code language="java" position="float" orientation="portrait" xml:space="preserve">
input_kwargs = self._event_dispatcher(event)
self._callback_pipeline.run(**input_kwargs)
</code></p>
      <p>Finally, the most comprehensive EPypes entity is <monospace>FullPipeline</monospace>, shown in <xref ref-type="fig" rid="fig-10">Fig. 10</xref>. It subclasses <monospace>Pipeline</monospace>, and provides functionality of both reacting to a stream of incoming events in <italic>q</italic><sub>in</sub> and publishing a subset of its processing results to <italic>q</italic><sub>out</sub> as outgoing events. It is instantiated in the following way:
<code language="java" position="float" orientation="portrait" xml:space="preserve">
snk_pipe = FullPipeline(‘MyFullPipeline’, cg, q_in, q_out, f_in, f_out, hparams)
</code>
</p>
      <fig id="fig-10" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-10</object-id>
        <label>Figure 10</label>
        <caption>
          <title>Structure of an instance of <monospace>FullPipeline</monospace>.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g010"/>
      </fig>
    </sec>
    <sec>
      <title>EPypes-based system development</title>
      <p>A distinction between a static computational graph and its runtime counterparts is realized in order to facilitate smooth system evolution from an early ad hoc development phase to a more integral whole with well-functioning reactive behavior. As shown in <xref ref-type="fig" rid="fig-11">Fig. 11</xref>, the development starts with components having less structure, and proceeds by extension of these components with functionality and behavior that are facilitated by the proposed tools.</p>
      <p>In the early development phase, vision algorithms, as well as other data processing routines, are prototyped using the available tool set: different alternatives can be implemented and evaluated in an interactive manner using tools like Jupyter and supported by OpenCV and a pool of scientific Python libraries (NumPy, Pandas, Scikit-image, Scikit-learn). As the result of prototyping, a collection of well-tested functions is developed. At this stage, the developer can specify computational graphs from the pool of these functions.</p>
      <fig id="fig-11" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-11</object-id>
        <label>Figure 11</label>
        <caption>
          <title>Layered system development framework.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g011"/>
      </fig>
      <p>The process of computational graph engineering involves a great deal of prototyping itself. Despite the fact that <monospace>CompGraph</monospace> constitutes a highly-structured entity, the flexibility of its definition brings a number of advantages over coding up the algorithm as a single function. Most importantly, the flat structure of the computational graph, along with Graphviz-based visualization capabilities, gives a transparent view on the data flow in the developed algorithm. It also allows for incorporating several alternative branches as a part of the same graph. The uniquely-named tokens provide an isolated namespace, which is specifically useful when prototyping in a Jupyter notebook. The mechanism of hyperparameter tokens allows for systematic management of the set of thresholds and other configuration values while being on a single hierarchical level (without a cascade of function calls). The well-defined structure of a computational graph facilitates automated manipulation of it, for example, extending the original graph with additional functions, union of two or more graphs, and union with renaming of functions and tokens.</p>
      <p>When a computational graph is developed, it can be used to construct pipelines. The non-reactive <monospace>Pipeline</monospace> provides additional capabilities to the computational graph: it is runnable, includes time measurement functionality, and can be flexibly subclassed, as done in reactive pipelines (<monospace>SinkPipeline</monospace>, <monospace>SourcePipeline</monospace>, and <monospace>FullPipeline</monospace>). The latter are used to expose the developed algorithm in online mode.</p>
    </sec>
    <sec>
      <title>EPypes use case</title>
      <p>In order to illustrate practical application of the EPypes framework and show its suitability for building data processing components in distributed environments, this section presents a run time use case scenario with the associated experiment. The presented scenario demonstrates how EPypes can be deployed as a part of a real distributed system (with the communication based on ZeroMQ and Protobuf) and what timing properties may be expected in this case. In particular, a major concern is how much overhead is introduced by the additional abstractions in the EPypes architecture. Furthermore, it is of interest how repeatable this overhead is, as well as what role it plays comparing to communication latency and the application-specific processing time.</p>
    </sec>
    <sec>
      <title>System description</title>
      <p>As shown in <xref ref-type="fig" rid="fig-12">Fig. 12</xref>, the case system is comprised of three nodes: (1) the robot control node, (2) the image acquisition service, and (3) the EPypes-based image processing node. The robot control node coordinates the robot’s work cycle and realizes communication with external systems. The system performing stereo acquisition from two cameras is designed as a streaming service, built using the FxIS framework (<xref rid="ref-15" ref-type="bibr">Semeniuta &amp; Falkman, 2018</xref>). For each associated camera, a stream of images is captured in its own thread of execution, and a number of recent frames are retained at each moment. External systems can request images from the service that closely correspond to the request timestamp.</p>
      <fig id="fig-12" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-12</object-id>
        <label>Figure 12</label>
        <caption>
          <title>System configuration.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g012"/>
      </fig>
      <p>The nodes run in the distributed environment and communicate through ZeroMQ publish/subscribe sockets and in-process blocking queues. For publishing and subscribing, EPypes provides two thread-based abstractions, namely <monospace>ZMQPublisher</monospace> and <monospace>ZMQSubscriber</monospace>. The former encapsulates a ZeroMQ <monospace>PUB</monospace> socket and acts as a consumer of an in-process queue: as a new data is available on the queue, it gets published. An example in <xref ref-type="fig" rid="fig-12">Fig. 12</xref> is the <monospace>PUB2</monospace>/<italic>q</italic><sub>out</sub> pair. <monospace>ZMQSubscriber</monospace> encapsulates a ZeroMQ <monospace>SUB</monospace> socket, which is polled with the <monospace>Poller</monospace> object. On arrival of a new message, the latter is put on the connected in-process queue. An example in <xref ref-type="fig" rid="fig-12">Fig. 12</xref> is the <monospace>SUB1</monospace>/<italic>q</italic><sub>in</sub> pair</p>
      <p>The robot control node runs on an ARM-based Raspberry Pi 3 single-board computer with the Raspbian operating system, while the vision-related components are deployed to an Ubuntu-based x86-64 machine. The latter has an Ethernet connection to a stereo camera pair (GigE Vision-based Prosilica GC1350), which are used by the image acquisition node.</p>
      <p>The following communication loop is considered:
<list list-type="order"><list-item><p>Robot announces request for timely image processing results; the image request is announced asynchronously as an event at <monospace>PUB1</monospace>.</p></list-item><list-item><p>Images most closely associated with the request are acquired and, as a tuple of <monospace>numpy.ndarray</monospace>, communicated to the processing component via the common in-process queue <italic>q</italic><sub>images</sub>.</p></list-item><list-item><p>Image processing node extracts the desired features from the images, which are communicated back to the robot via the <monospace>PUB2/SUB2</monospace> asynchronous socket pair.</p></list-item></list></p>
      <p>The target vision algorithm performs ORB feature detection, description, and matching (<xref rid="ref-12" ref-type="bibr">Rublee &amp; Bradski, 2011</xref>). <xref ref-type="fig" rid="fig-13">Figure 13</xref> shows the corresponding computational graph. After image features are identified in each image, collections of feature descriptors are matched against each other using OpenCV’s <monospace>BFMatcher</monospace> object, with the matches returned in sorted order by match distance. The final <monospace>gather_keypoints</monospace> function produces an array of the matched keypoints’ coordinates.</p>
      <fig id="fig-13" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-13</object-id>
        <label>Figure 13</label>
        <caption>
          <title>ORB computational graph.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g013"/>
      </fig>
      <p>The communicated messages that are send over wire are serialized in the Google’s Protocol Buffers (Protobuf) interchange format. Three message types are used:
<list list-type="bullet"><list-item><p><monospace>AttributeList</monospace> represents a collection of key/value attributes, where an attribute can be either a <monospace>string</monospace>, a <monospace>double</monospace>, or an <monospace>int32</monospace>.</p></list-item><list-item><p><monospace>Event</monospace>, sent over <monospace>PUB1/SUB1</monospace>, is comprised of an <monospace>id (string)</monospace>, a <monospace>type (string)</monospace>, and <monospace>attributes (AttributeList)</monospace>;</p></list-item><list-item><p><monospace>JustBytes</monospace>, sent over <monospace>PUB2/SUB2</monospace>, is comprised of an id <monospace>(string)</monospace>, <monospace>content (bytes)</monospace>, and <monospace>attributes (AttributeList)</monospace>;</p></list-item></list></p>
      <p>The computational graph shown in <xref ref-type="fig" rid="fig-13">Fig. 13</xref> forms a basis for an instance of a <monospace>FullPipeline</monospace>. Its event dispatcher <italic>f</italic><sub>in</sub> handles tuples with pairs of images put onto <italic>q</italic><sub>images</sub>. The output preparation function <italic>f</italic><sub>out</sub> is responsible for packaging the output data as a <monospace>JustBytes</monospace> Protobuf message, with its <monospace>content</monospace> being the Pickle-serialized value of the first 20 rows of the <monospace>keypoints_paired</monospace> token <monospace>(numpy.ndarray)</monospace>, and the <monospace>attributes</monospace> filled by timestamps and durations captured with the image acquisition service and the EPypes pipeline.</p>
    </sec>
    <sec>
      <title>Time measurement experiment</title>
      <p>The robot control node announces a series of vision requests and extracts attributes from the response Protobuf messages. In addition, it records the timestamps of when the vision request get announced (<italic>t</italic><sub>vreq</sub>) and when the corresponding response is obtained (<italic>t</italic><sub>vresp</sub>). The difference between these timestamps accounts for the trip duration of the current request:
<disp-formula id="eqn-3"><alternatives><graphic xlink:href="peerj-cs-05-176-e003.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm\tau _{{\rm{trip}}}} = {t_{{\rm{vresp}}}}-{t_{{\rm{vreq}}}}$$\end{document}</tex-math><mml:math id="mml-eqn-3"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>trip</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>vresp</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>vreq</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></disp-formula>
</p>
      <p>For execution of both the image acquisition service and the vision pipeline, two timestamps are added to the properties set: <italic>t</italic><sub>react</sub>, when the component reacted to the incoming event, and <italic>t</italic><sub>pub</sub>, right before publishing the outgoing event. Their difference <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-05-176-i003.jpg"/><tex-math id="M6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${\rm\tau _{r \to p}}$\end{document}</tex-math><mml:math id="mml-ieqn-3"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>→</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula> provides the measurement of the component’s processing time, including processing of incoming and outgoing events:
<disp-formula id="eqn-4"><alternatives><graphic xlink:href="peerj-cs-05-176-e004.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${\rm\tau _{r \to p}} = {t_{{\rm{pub}}}}-{t_{{\rm{react}}}}$$\end{document}</tex-math><mml:math id="mml-eqn-4"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>→</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>pub</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>react</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></disp-formula>
</p>
      <p>Properties related to the vision pipeline that get added to the outgoing Protobuf message comprise vision processing time <italic>τ<sub>p</sub></italic>, overhead from orchestrating the computational graph <italic>o</italic><sub>cg</sub>, and timestamps of start and finish of the event dispatcher <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-05-176-i004.jpg"/><tex-math id="M8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${f_{{\rm{in}}}}\left({{t_{{f_{{\rm{in}}}} \uparrow }},{t_{{f_{{\rm{in}}}} \downarrow }}} \right)$\end{document}</tex-math><mml:math id="mml-ieqn-4"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub><mml:mo>↑</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub><mml:mo>↓</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula> and the output preparation function <inline-formula><alternatives><inline-graphic xlink:href="peerj-cs-05-176-i005.jpg"/><tex-math id="M9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${f_{{\rm{out}}}}\left({{t_{{f_{{\rm{out}}}} \uparrow }},{t_{{f_{{\rm{out}}}} \downarrow }}} \right)$\end{document}</tex-math><mml:math id="mml-ieqn-5"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub><mml:mo>↑</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub><mml:mo>↓</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>, which define the corresponding function durations:
<disp-formula id="eqn-5"><alternatives><graphic xlink:href="peerj-cs-05-176-e005.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\matrix{ {{\tau _{{f_{{\rm{in}}}}}} = {t_{{f_{{\rm{in}}}} \downarrow }}-{t_{{f_{{\rm{in}}}} \uparrow }}} \hfill \cr {{\tau _{{f_{{\rm{out}}}}}} = {t_{{f_{{\rm{out}}}} \downarrow }}-{t_{{f_{{\rm{out}}}} \uparrow }}} \hfill \cr } $$\end{document}</tex-math><mml:math id="mml-eqn-5"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub><mml:mo>↓</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub><mml:mo>↑</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub><mml:mo>↓</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub><mml:mo>↑</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>Computational graph overhead <italic>o</italic><sub>cg</sub> is measured internally by the pipeline <monospace>(p.compute_overhead())</monospace>, and constitutes the difference between total processing time of the pipeline and the sum of processing times of all the enclosed nodes:
<disp-formula id="eqn-6"><alternatives><graphic xlink:href="peerj-cs-05-176-e006.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${o_{cg}} = {\tau _p}-\sum \{ {\tau _n}\ for\,each\,node\,\,n \in p\} $$\end{document}</tex-math><mml:math id="mml-eqn-6"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mo>{</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mtext> </mml:mtext><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mtext>  </mml:mtext><mml:mi>n</mml:mi><mml:mo>∈</mml:mo><mml:mi>p</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>After each request is finished, the robot control node records all the obtained properties. The latter are further aggregated in a Pandas data frame, with a row of properties’ values per each request. From the available data, the following overhead metrics can be computed:
<list list-type="order"><list-item><p><italic>Network overhead</italic> measures how much the trip duration is greater than the time spent in all the components:
<disp-formula id="eqn-7"><alternatives><graphic xlink:href="peerj-cs-05-176-e007.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${o_{{\rm{network}}}} = {\tau _{{\rm{trip}}}}-({\rm\tau} _{r \to p}^{(image\,acquistition)} + {\rm\tau} _{r \to p}^{(vision\,pipeline)})$$\end{document}</tex-math><mml:math id="mml-eqn-7"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mtext>network</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>trip</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>→</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>→</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p></list-item><list-item><p><italic>EPypes overhead</italic> is computed as an excess time in the vision pipeline in addition to the processing in the computational graph and in the functions <italic>f</italic><sub>in</sub> and <italic>f</italic><sub>out</sub>:
<disp-formula id="eqn-8"><alternatives><graphic xlink:href="peerj-cs-05-176-e008.jpg" mimetype="image" mime-subtype="png" position="float" orientation="portrait"/><tex-math id="M13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$${o_{{\rm{epypes}}}} = \tau _{r \to p}^{(\rm vision\,pipeline)}-({\tau _p} + {\tau _{{f_{{\rm{in}}}}}} + {\tau _{{f_{out}}}})$$\end{document}</tex-math><mml:math id="mml-eqn-8"><mml:mrow><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mtext>epypes</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>→</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
</p></list-item></list></p>
      <p><xref ref-type="fig" rid="fig-14">Figure 14</xref> demonstrates the timeline of 100 vision requests and the associated durations of the image acquisition service, the vision pipeline, and the overhead from network communication.</p>
      <fig id="fig-14" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-14</object-id>
        <label>Figure 14</label>
        <caption>
          <title>Components’ durations and network overhead for a series of vision requests.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g014"/>
      </fig>
      <p>Data has been collected from five experiments, each with 500 vision requests. For each experiment, a maximum likelihood estimation of log-normal probability density function is performed for distributions of <italic>o</italic><sub>cg</sub> and <italic>o</italic><sub>epypes</sub>. The same estimation is performed for all data combined. <xref ref-type="fig" rid="fig-15">Figures 15</xref> and <xref ref-type="fig" rid="fig-16">16</xref> show visualization of these PDFs. A PDF for each individual experiment is visualized as a shaded area under the curve. The PDF for all data is shown as a thick curve. The thin vertical line specify the modal value of the PDF for the combined dataset, and the enclosing thick vertical lines delimit the overall range of measurements for the combined dataset.</p>
      <fig id="fig-15" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-15</object-id>
        <label>Figure 15</label>
        <caption>
          <title>Computational graph overhead.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g015"/>
      </fig>
      <fig id="fig-16" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.7717/peerj-cs.176/fig-16</object-id>
        <label>Figure 16</label>
        <caption>
          <title>EPypes overhead.</title>
        </caption>
        <graphic xlink:href="peerj-cs-05-176-g016"/>
      </fig>
      <p>It can be seen from <xref ref-type="fig" rid="fig-15">Fig. 15</xref> that overhead from performing data processing based on a computational graph <italic>o</italic><sub>cg</sub> is characterized by matching log-normal distributions for every experiment, with most of the probability density located around 0.3 ms. The EPypes overhead <italic>o</italic><sub>epypes</sub>, as shown in <xref ref-type="fig" rid="fig-16">Fig. 16</xref>, has much tighter range of possible values, distributed log-normally with matching distributions for every experiment, and with most of the probability density around 0.02 ms. Overall, for a vision algorithm that naturally requires tens of milliseconds to perform the processing, the overheads introduces by EPypes can be considered negligible.</p>
    </sec>
    <sec>
      <title>Related work</title>
      <p>The idea of explicit utilization graph-based representation of data processing algorithms has been on the surface for many years. The availability of engineering tools, data science frameworks, and modeling formalisms, described in the Background section, shows the efficacy of the pipeline thinking when designing systems with streaming logic. The distinctive approach of EPypes lies in its tight integration with the Python ecosystem, support for algorithm prototyping, and abstractions for integration of the developed computational graphs into distributed systems.</p>
      <p>The EPypes architecture is a logical continuation of the concept of discrete event data flow, earlier presented by <xref rid="ref-14" ref-type="bibr">Semeniuta &amp; Falkman (2015)</xref>. This earlier work attempted to define a data flow formalism with distinct notion of event as the one used in publish/subscribe systems. However, the presented formalism didn’t include a reference implementation at the time. EPypes has, in turn, refined the notion of reactive pipelines and made it usable in real scenarios.</p>
      <p>Other highly related work within the formal methods domain is Stream Algebra (<xref rid="ref-5" ref-type="bibr">Helala, Pu &amp; Qureshi, 2014</xref>), with its Go-based implementation (<xref rid="ref-6" ref-type="bibr">Helala, Pu &amp; Qureshi, 2016</xref>). This approach models an image processing algorithm as a set of data streams that get altered by a set of operators. In the algebra implementation, a stream corresponds to a Go channel, and the set of defined operators allow to define usable workflow patterns such as pipeline graphs, fork-join graphs, and pipeline graphs with feedback. The latter option is naturally supported due to the concurrency features of Go. This approach, similarly to EPypes, allows to construct high level algorithm from finer functions, including those from the OpenCV library. The distinctive feature is the support for feedback, which is disallowed in EPypes due to the acyclicity requirement. The feedback with EPypes, however, can be realized on a higher systemic level, by incorporating additional distributed components.</p>
      <p>In the contemporary robotics research, the robot operating system (ROS) is widely used as the underlying platform for the distributed robotic applications relying on data from sensors and cameras. The general architecture in this case is based on a collection of <italic>nodes</italic> that react to arrival of data through publish/subscribe topics, which makes the overall logic graph-based. The related concept of <italic>nodelet</italic> (and <italic>component</italic> in ROS2) allows to realize a processing graph structure as a part of a single operating system process. Examples of this approach is often demonstrated on the applications of point cloud processing (<xref rid="ref-13" ref-type="bibr">Rusu &amp; Cousins, 2011</xref>; <xref rid="ref-11" ref-type="bibr">Munaro et al., 2013</xref>; <xref rid="ref-1" ref-type="bibr">Carraro, Munaro &amp; Menegatti, 2017</xref>), as to minimize latency due to inter-process or remote communication. ROS-based processing graphs, especially in the single-process case, are somewhat similar to EPypes pipelines. They, however, target applications with already developed algorithms, as opposed to EPypes, which supports early-stage prototyping using the graph-based abstractions.</p>
      <p>Other academic examples of similar robot/vision architectures include the one based on the supervisory control theory of discrete-event systems (<xref rid="ref-8" ref-type="bibr">Košecka, Christensen &amp; Bajcsy, 1995</xref>) and service-oriented dataflow-like components, auto-tuned by higher-level supervisors (<xref rid="ref-3" ref-type="bibr">Crowley, Hall &amp; Emonet, 2007</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="conclusions|further">
    <title>Conclusions and Further Work</title>
    <p>This paper has presented EPypes, an architecture and Python-based software framework for building event-driven data processing pipelines. Because most of vision algorithms and many data processing routines are naturally modeled as pipelines, EPypes offers a capability of implementing data processing systems as DAGs. Apart from the functional components comprising the prototype implementation of EPypes, this paper has presented a system development framework that supports evolution of computational graphs from an early prototyping phase to their deployment as reactive pipelines.</p>
    <p>The principle of the EPypes abstraction is demonstrated on the example of constructing a computational graph for edge detection and discussing the inner structure of the hierarchy of pipelines. Further, a real scenario of deployment of an EPypes pipeline for features detection and matching to a distributed system is experimentally studied. It was shown that the ability to adapt reactive behavior to various publish/subscribe middleware solutions allows to combine EPypes pipelines with already available systems. The measured timing properties of the image processing component based on EPypes show that the latter introduces negligible overhead comparing to the application-inherent processing time.</p>
    <p>An important part of further work should be connected with development of software abstractions on the highest level of the system development continuum shown in <xref ref-type="fig" rid="fig-11">Fig. 11</xref>. This will enable fine-tuning and enhancing of reactive pipelines, for example, with adapters to different messaging systems (e.g., MQTT, RabbitMQ, DDS), parallelizable nodes, and specialized pipeline management logic. An important task in this case is implementation of systematic error handling. A failure inside the pipeline (e.g., in the case of a vision system, due to changed lighting conditions) can be handled by issuing the corresponding event that will be processed by a remote component. In addition to queues providing asynchronous messaging, other communication modalities can be used. An RPC API (such as REST or gRPC) can be established to allow external systems getting meta-information about the running pipeline and changing values of hyperparameters. Last, but not least, functionality for interaction with databases should be integrated.</p>
    <p>As the presented software framework is implemented in Python, it naturally gears toward system prototyping use cases. The static abstractions are useful for algorithm prototyping, while the transition to the reactive components allow for rapid deployment of the computational graphs to distributed environments. This allows for harnessing the available Python data science tools and integrating them into industrial automation workflow.</p>
    <p>The limitation of the proposed implementation lies in its non-deterministic overhead due to the use of the interpreted garbage-collected programming language. Hence, applications requiring high rate of operation and more deterministic running time are more likely to be developed in C++ with custom UDP-based communication protocols or real-time middleware such as DDS. It is of interest therefore to validate the principles of EPypes using C++ codebase, as well as to devise a strategy of transforming EPypes-based computational graphs to high-performance computing components, for example, via code generation.</p>
  </sec>
</body>
<back>
  <sec sec-type="additional-information">
    <title>Additional Information and Declarations</title>
    <fn-group content-type="competing-interests">
      <title>Competing Interests</title>
      <fn fn-type="COI-statement" id="conflict-1">
        <p>The authors declare that they have no competing interests.</p>
      </fn>
    </fn-group>
    <fn-group content-type="author-contributions">
      <title>Author Contributions</title>
      <fn fn-type="con" id="contribution-1">
        <p><xref ref-type="contrib" rid="author-1">Oleksandr Semeniuta</xref> conceived and designed the experiments, performed the experiments, analyzed the data, contributed reagents/materials/analysis tools, prepared figures and/or tables, performed the computation work, authored or reviewed drafts of the paper, approved the final draft.</p>
      </fn>
      <fn fn-type="con" id="contribution-2">
        <p><xref ref-type="contrib" rid="author-2">Petter Falkman</xref> authored or reviewed drafts of the paper, approved the final draft.</p>
      </fn>
    </fn-group>
    <fn-group content-type="other">
      <title>Data Availability</title>
      <fn id="addinfo-1">
        <p>The following information was supplied regarding data availability:</p>
        <p>The source code of the EPypes library is available at GitHub: <uri xlink:href="https://github.com/semeniuta/EPypes">https://github.com/semeniuta/EPypes</uri>.</p>
      </fn>
    </fn-group>
  </sec>
  <ref-list content-type="authoryear">
    <title>References</title>
    <ref id="ref-1">
      <label>Carraro, Munaro &amp; Menegatti (2017)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Carraro</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Munaro</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Menegatti</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Chen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Hosoda</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Menegatti</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Shimizu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>A Powerful and Cost-Efficient Human Perception System for Camera Networks and Mobile Robotics</article-title>
        <source>Intelligent Autonomous Systems 14. IAS 2016. Advances in Intelligent Systems and Computing</source>
        <year>2017</year>
        <volume>531</volume>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>485</fpage>
        <lpage>497</lpage>
      </element-citation>
    </ref>
    <ref id="ref-2">
      <label>Cassandras &amp; Lafortune (2008)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Cassandras</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lafortune</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <source>Introduction to discrete event systems</source>
        <year>2008</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-3">
      <label>Crowley, Hall &amp; Emonet (2007)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Crowley</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Emonet</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Autonomic computer vision systems</article-title>
        <year>2007</year>
        <conf-name>2007 International Conference on Computer Vision Systems, ICVS</conf-name>
        <volume>7</volume>
        <publisher-loc>Bielefeld</publisher-loc>
        <publisher-name>Applied Computer Science Group</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-4">
      <label>Dworak et al. (2012)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dworak</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ehm</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Charrue</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sliwinski</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>The new cern controls middleware</article-title>
        <source>Journal of Physics: Conference Series</source>
        <year>2012</year>
        <volume>396</volume>
        <issue>1</issue>
        <fpage>012017</fpage>
        <pub-id pub-id-type="doi">10.1088/1742-6596/396/1/012017</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-5">
      <label>Helala, Pu &amp; Qureshi (2014)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Helala</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Pu</surname>
            <given-names>KQ</given-names>
          </name>
          <name>
            <surname>Qureshi</surname>
            <given-names>FZ</given-names>
          </name>
        </person-group>
        <article-title>A stream algebra for computer vision pipelines</article-title>
        <year>2014</year>
        <conf-name>2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops</conf-name>
        <conf-loc>Columbus, OH</conf-loc>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>800</fpage>
        <lpage>807</lpage>
        <pub-id pub-id-type="doi">10.1109/CVPRW.2014.122</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-6">
      <label>Helala, Pu &amp; Qureshi (2016)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Helala</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pu</surname>
            <given-names>KQ</given-names>
          </name>
          <name>
            <surname>Qureshi</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A formal algebra implementation for distributed image and video stream processing</article-title>
        <year>2016</year>
        <conf-name>Proceedings 10th International Conference on Distributed Smart Cameras (ICDSC 16)</conf-name>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-7">
      <label>Hinze, Sachs &amp; Buchmann (2009)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Hinze</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sachs</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Buchmann</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Event-based applications and enabling technologies</article-title>
        <year>2009</year>
        <conf-name>Proceedings of the Third ACM International Conference on Distributed Event-Based Systems—DEBS ’09</conf-name>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM Press</publisher-name>
        <fpage>1</fpage>
      </element-citation>
    </ref>
    <ref id="ref-8">
      <label>Košecka, Christensen &amp; Bajcsy (1995)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Košecka</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Christensen</surname>
            <given-names>HI</given-names>
          </name>
          <name>
            <surname>Bajcsy</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Discrete event modeling of visually guided behaviors</article-title>
        <source>International Journal of Computer Vision</source>
        <year>1995</year>
        <volume>14</volume>
        <issue>2</issue>
        <fpage>179</fpage>
        <lpage>191</lpage>
        <pub-id pub-id-type="doi">10.1007/bf01418982</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-9">
      <label>Lee &amp; Seshia (2011)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>EA</given-names>
          </name>
          <name>
            <surname>Seshia</surname>
            <given-names>SA</given-names>
          </name>
        </person-group>
        <source>Introduction to embedded systems, a cyber-physical systems approach</source>
        <year>2011</year>
        <edition designator="2">Second Edition</edition>
        <publisher-loc>Cambridge</publisher-loc>
        <publisher-name>MIT Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="ref-10">
      <label>Magnoni (2015)</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Magnoni</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Modern messaging for distributed sytems</article-title>
        <source>Journal of Physics: Conference Series</source>
        <year>2015</year>
        <volume>608</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="ref-11">
      <label>Munaro et al. (2013)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Munaro</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Basso</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Michieletto</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Pagello</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Menegatti</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>A software architecture for rgb-d people tracking based on ros framework for a mobile robot</article-title>
        <source>Frontiers of Intelligent Autonomous Systems</source>
        <year>2013</year>
        <volume>466</volume>
        <publisher-loc>Berlin, Heidelberg</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>53</fpage>
        <lpage>68</lpage>
      </element-citation>
    </ref>
    <ref id="ref-12">
      <label>Rublee &amp; Bradski (2011)</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Rublee</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Bradski</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <source>Orb—an efficient alternative to sift or surf</source>
        <year>2011</year>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>2564</fpage>
        <lpage>2571</lpage>
      </element-citation>
    </ref>
    <ref id="ref-13">
      <label>Rusu &amp; Cousins (2011)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Rusu</surname>
            <given-names>RB</given-names>
          </name>
          <name>
            <surname>Cousins</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>3d is here: point cloud library (PCL)</article-title>
        <year>2011</year>
        <conf-name>Proceedings—IEEE International Conference on Robotics and Automation</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>4</lpage>
      </element-citation>
    </ref>
    <ref id="ref-14">
      <label>Semeniuta &amp; Falkman (2015)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Semeniuta</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Falkman</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Discrete event dataflow as a formal approach to specification of industrial vision systems</article-title>
        <year>2015</year>
        <conf-name>2015 IEEE International Conference on Automation Science and Engineering (CASE)</conf-name>
        <conf-loc>2015-October</conf-loc>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>849</fpage>
        <lpage>854</lpage>
      </element-citation>
    </ref>
    <ref id="ref-15">
      <label>Semeniuta &amp; Falkman (2018)</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Semeniuta</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Falkman</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Flexible image acquisition service for distributed robotic systems</article-title>
        <year>2018</year>
        <conf-name>2018 Second IEEE International Conference on Robotic Computing (IRC)</conf-name>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
        <fpage>106</fpage>
        <lpage>112</lpage>
      </element-citation>
    </ref>
    <ref id="ref-16">
      <label>ZeroMQ (2008)</label>
      <element-citation publication-type="software">
        <person-group person-group-type="author">
          <collab>
            <institution>ZeroMQ</institution>
          </collab>
        </person-group>
        <data-title>Broker vs. brokerless</data-title>
        <year>2008</year>
        <uri xlink:href="http://zeromq.org/whitepapers:brokerless">http://zeromq.org/whitepapers:brokerless</uri>
        <date-in-citation content-type="access-date" iso-8601-date="2018-02-28">28 February 2018</date-in-citation>
      </element-citation>
    </ref>
  </ref-list>
</back>
