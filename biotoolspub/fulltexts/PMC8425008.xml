<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8425008</article-id>
    <article-id pub-id-type="publisher-id">4329</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-021-04329-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DI2: prior-free and multi-item discretization of biological data and its applications</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7061-4097</contrib-id>
        <name>
          <surname>Alexandre</surname>
          <given-names>Leonardo</given-names>
        </name>
        <address>
          <email>leonardoalexandre@tecnico.ulisboa.pt</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Costa</surname>
          <given-names>Rafael S.</given-names>
        </name>
        <address>
          <email>rs.costa@fct.unl.pt</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Henriques</surname>
          <given-names>Rui</given-names>
        </name>
        <address>
          <email>rmch@tecnico.ulisboa.pt</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.9983.b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2181 4263</institution-id><institution>IDMEC, Instituto Superior Técnico, </institution><institution>Universidade de Lisboa, </institution></institution-wrap>Av. Rovisco Pais, 1049-001 Lisbon, Portugal </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.14647.30</institution-id><institution-id institution-id-type="ISNI">0000 0001 0279 8114</institution-id><institution>INESC-ID, </institution></institution-wrap>Lisbon, Portugal </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.9983.b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2181 4263</institution-id><institution>Instituto Superior Técnico, </institution><institution>Universidade de Lisboa, </institution></institution-wrap>Lisbon, Portugal </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.10772.33</institution-id><institution-id institution-id-type="ISNI">0000000121511713</institution-id><institution>LAQV-REQUIMTE, DQ, NOVA School of Science and Technology, </institution><institution>Universidade NOVA de Lisboa, </institution></institution-wrap>2829-516 Caparica, Portugal </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>8</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>8</day>
      <month>9</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>22</volume>
    <elocation-id>426</elocation-id>
    <history>
      <date date-type="received">
        <day>7</day>
        <month>3</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>8</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">A considerable number of data mining approaches for biomedical data analysis, including state-of-the-art associative models, require a form of data discretization. Although diverse discretization approaches have been proposed, they generally work under a strict set of statistical assumptions which are arguably insufficient to handle the diversity and heterogeneity of clinical and molecular variables within a given dataset. In addition, although an increasing number of symbolic approaches in bioinformatics are able to assign multiple items to values occurring near discretization boundaries for superior robustness, there are no reference principles on how to perform multi-item discretizations.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this study, an unsupervised discretization method, DI2, for variables with arbitrarily skewed distributions is proposed. Statistical tests applied to assess differences in performance confirm that DI2 generally outperforms well-established discretizations methods with statistical significance. Within classification tasks, DI2 displays either competitive or superior levels of predictive accuracy, particularly delineate for classifiers able to accommodate border values.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">This work proposes a new unsupervised method for data discretization, DI2, that takes into account the underlying data regularities, the presence of outlier values disrupting expected regularities, as well as the relevance of border values. DI2 is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/JupitersMight/DI2">https://github.com/JupitersMight/DI2</ext-link></p>
      </sec>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1186/s12859-021-04329-8.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Multi-item discretization</kwd>
      <kwd>Prior-free discretization</kwd>
      <kwd>Heterogeneous biological data</kwd>
      <kwd>Data mining</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001871</institution-id>
            <institution>Fundação para a Ciência e a Tecnologia</institution>
          </institution-wrap>
        </funding-source>
        <award-id>UIDB/50021/2020</award-id>
        <award-id>DSAIPA/DS/0042/2018</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001871</institution-id>
            <institution>Fundação para a Ciência e a Tecnologia</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DSAIPA/DS/0111/2018</award-id>
        <award-id>UIDB/50006/2020</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001871</institution-id>
            <institution>Fundação para a Ciência e a Tecnologia</institution>
          </institution-wrap>
        </funding-source>
        <award-id>UIDP/50006/2020</award-id>
        <award-id>UIDB/50021/2020</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001871</institution-id>
            <institution>Fundação para a Ciência e a Tecnologia</institution>
          </institution-wrap>
        </funding-source>
        <award-id>CEECIND/01399/2017</award-id>
        <principal-award-recipient>
          <name>
            <surname>Costa</surname>
            <given-names>Rafael S.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par10">Approaches to discretization of continuous variables have long been discussed alongside their pros and cons. Altman et al. [<xref ref-type="bibr" rid="CR1">1</xref>] and Bennette et al. [<xref ref-type="bibr" rid="CR2">2</xref>] both discuss the relevance and impact of categorizing continuous variables and reducing the cardinality of categorical variables. Liao et al. [<xref ref-type="bibr" rid="CR3">3</xref>] compares various categorization techniques in the context of classification tasks in medical domains, without using domain knowledge of field experts. Considerable advances in data mining are being driven by symbolic approaches, particularly those rooted in bioinformatic, compression and pattern mining research, including contributions pertaining to the analysis of symbolic sequences, text or basket transactions. The relevance of discretization meets both descriptive and predictive ends, encompassing state-of-the-art approaches such as pattern-based biclustering [<xref ref-type="bibr" rid="CR4">4</xref>] and associative models such as XGBoost [<xref ref-type="bibr" rid="CR5">5</xref>].</p>
    <p id="Par11">In this work we present DI2, a Python library that extends non-parametric tests to find the best fitting distribution for a given variable and discretize it accordingly. DI2 offers three major contributions: (i) corrections to the empirical distribution before statistical fitting to guarantee a more robust approximation of candidate distributions; (ii) efficient statistical fitting of 100 theoretical probability distributions; and, finally, (iii) assignment of multiple items according to the proximity of values to the boundaries of discretization, a possibility supported by numerous symbolic approaches [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>]. The assignment of multiple items [<xref ref-type="bibr" rid="CR8">8</xref>], generally referred as multi-item discretization, conferes the possibility to avail the wealth of data structures and algorithms from the text processing and bioinformatics communities without the risks of the well-studied item-boundaries problem.</p>
    <p id="Par12">Discretization methods have wide taxonomy [<xref ref-type="bibr" rid="CR9">9</xref>] with a determinant division in: (1) supervised, where the method uses the class variable to bin the data, and, (2) unsupervised, where the method is independent of the class variable. DI2 places itself on the latter, it works independently of the class variable. Other characteristics of DI2 are: (1) static, where discretization of the variables takes place prior to an algorithm; (2) global, uses information about the variable as a whole to make the partitions and can still be applied with a scarce number of observations; (3) direct and splitting, splits the whole range of values into <italic>k</italic> intervals simultaneously; and (4) multivariate and univariate, DI2 can use either the whole dataset to create the intervals and discretize each variable or use each variable individually to create the respective intervals.</p>
    <p id="Par13">Some examples of unsupervised discretization methods are Proportional Discretization (PD), Fixed Frequency Discretization (FFD) [<xref ref-type="bibr" rid="CR10">10</xref>], equal-width/frequency (also known as uniform and quantile) and k-means [<xref ref-type="bibr" rid="CR11">11</xref>]. In this work, DI2 is compared with such classic discretization methods. These are illustrated in Figs. <xref rid="Fig1" ref-type="fig">1</xref>, <xref rid="Fig2" ref-type="fig">2</xref>, and <xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig1"><label>Fig. 1</label><caption><p>Illustration of equal-frequency method with 9 points along an axis and 3 categories. This method is based on the frequency of the items, where each category has the same number of items, in order to set the intervals</p></caption><graphic xlink:href="12859_2021_4329_Fig1_HTML" id="MO1"/></fig><fig id="Fig2"><label>Fig. 2</label><caption><p>Illustration of equal-width method with 9 points along an axis and 3 categories. This method is based on the range taken by the items, where each category has the same width interval</p></caption><graphic xlink:href="12859_2021_4329_Fig2_HTML" id="MO2"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><p>Illustration of K-means method with 9 points along an axis and 3 categories. This method is based in the k-means clustering, where each category is defined by a centroid</p></caption><graphic xlink:href="12859_2021_4329_Fig3_HTML" id="MO3"/></fig></p>
    <sec id="Sec2">
      <title>Normalization and feature scaling</title>
      <p id="Par14">While not mandatory, DI2 supports: <italic>min-max scaling</italic>,<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} X'= \frac{X - X_{min}}{X_{max} - X_{min}}, \end{aligned}$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>X</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4329_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>X</italic> is an ordered set of observed values, and <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{max}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{min}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq2.gif"/></alternatives></inline-formula> are the maximum and minimum value within <italic>X</italic>; <italic>z</italic>-<italic>score standardization</italic> for normally distributed observations [<xref ref-type="bibr" rid="CR12">12</xref>],<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} X'= \frac{X - {\overline{x}}}{S_n}, \end{aligned}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>X</mml:mi><mml:mo>-</mml:mo><mml:mover><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4329_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>X</italic> is an ordered set of observed values, <inline-formula id="IEq3"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\overline{x}}$$\end{document}</tex-math><mml:math id="M10"><mml:mover><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq3.gif"/></alternatives></inline-formula> is the sample mean, and <inline-formula id="IEq4"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_n$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq4.gif"/></alternatives></inline-formula> is the sample variance; and mean normalization,<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} X'= \frac{X - {\overline{x}}}{X_{max} - X_{min}}. \end{aligned}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>X</mml:mi><mml:mo>-</mml:mo><mml:mover><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4329_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <italic>X</italic> is an ordered set of observed values, <inline-formula id="IEq5"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\overline{x}}$$\end{document}</tex-math><mml:math id="M16"><mml:mover><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq5.gif"/></alternatives></inline-formula> is the sample mean, and <inline-formula id="IEq6"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{max}$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">max</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{min}$$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">min</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq7.gif"/></alternatives></inline-formula> are the maximum and minimum value within <italic>X</italic>.</p>
    </sec>
    <sec id="Sec3">
      <title>Statistical hypotheses</title>
      <p id="Par15">In order to discretize the data into intervals, DI2 provides two statistical hypothesis tests: (1) <inline-formula id="IEq8"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M22"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq8.gif"/></alternatives></inline-formula> test [<xref ref-type="bibr" rid="CR13">13</xref>], and (2) Kolmogorov–Smirnov goodness-of-fit test [<xref ref-type="bibr" rid="CR14">14</xref>].</p>
      <p id="Par16">In the aforementioned tests, the empirical distribution is matched with a theoretical continuous distribution<xref ref-type="fn" rid="Fn1">1</xref>, provided by the SciPy open-source library [<xref ref-type="bibr" rid="CR15">15</xref>], where the parameters are estimated through maximum likelihood estimation function. We consider the null hypothesis to be “the empirical probability distribution matches the theoretical probability distribution”. Considering a significance level of 0.05 and the number of degrees of freedom to be the number of categories inputted by the user minus one minus the number of estimated parameters [<xref ref-type="bibr" rid="CR16">16</xref>] (excluding scale and location parameters). If the <inline-formula id="IEq9"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M24"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq9.gif"/></alternatives></inline-formula> statistic is higher than the critical value at 0.05 we reject the hypothesis. The same logic is applied to the Kolmogorov–Smirnov statistic. The expected distribution of each category used in the <inline-formula id="IEq10"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M26"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq10.gif"/></alternatives></inline-formula> test corresponds to the number of inputted categories by the user. The user can either choose the <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M28"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq11.gif"/></alternatives></inline-formula> or the Kolmogorov–Smirnov goodness-of-fit as the <italic>primary</italic> fitting test. Both statistical tests yield properties of interest. While Kolmogorov–Smirnov does not provide an exhaustive characterization of the differences between the reference and empirical probability distributions as its statistic is derived from the highest distant point between the cumulative distributions, <inline-formula id="IEq12"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M30"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq12.gif"/></alternatives></inline-formula> is dependent on the selected number of categories to assess the goodness of fitting. Having these concerns in mind, <inline-formula id="IEq13"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M32"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq13.gif"/></alternatives></inline-formula> test is suggested as the default option unless a high number of data instances are available. In this latter case, the Kolmogorov–Smirnov test provides a finer-grained view as it more accurately models the empirical cumulative distribution.</p>
      <p id="Par17">DI2 informs the user of the selected distribution per column, the statistic of the applied test, and whether the computed statistic passes the goodness-of-fit test. One of the following scenarios can occur: (1) at least one theoretical distribution passes the statistical test, or (2) no theoretical distribution passes the statistical test. In both cases, the distribution with the lowest test statistic is chosen. The second scenario might be intentional. Consider the following, if the user knows that the empirical distribution is a sample from a population that follows a normal distribution, he can input the theoretical continuous distributions accordingly (normal distribution and its variants).</p>
    </sec>
    <sec id="Sec4">
      <title>Outlier correction</title>
      <p id="Par18">The Kolmogorov–Smirnov goodness-of-fit test can optionally be used to remove up to 5% outlier points, from the empirical distribution, according to the theoretical continuous distribution under assessment. Kolmogorov–Smirnov goodness-of-fit test returns a statistic (D statistic) measuring the maximum distance between the empirical and theoretical distributions,<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} D = \max \Big \{\max _{1 \le j \le n} \big \{\frac{j}{n} - F(X_j)\big \}, \max _{1 \le j \le n} \big \{F(X_j) - \frac{(j-1)}{n}\big \}\Big \}, \end{aligned}$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em" stretchy="true">{</mml:mo></mml:mrow><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">{</mml:mo></mml:mrow><mml:mfrac><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mfrac><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">{</mml:mo></mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em" stretchy="true">}</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em" stretchy="true">}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4329_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <italic>n</italic> is the number of observations, <italic>j</italic> is the index of a given observation, and <italic>F</italic> is the frequency of observation <inline-formula id="IEq14"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_j$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq14.gif"/></alternatives></inline-formula>. The first inner max function is referred as <italic>D</italic>-plus statistic, while the second inner max function is termed <italic>D</italic>-minus statistic. Using the <italic>D</italic> statistic we can pinpoint where the farthest point between the distributions is and remove it. After up to 5% of the observations have been removed, the iteration with the best Kolmogorov–Smirnov statistic is picked (from 0 outliers removed to up to 5%). The data produced by outlier removal is then used to run the main statistical hypothesis test picked (<inline-formula id="IEq15"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M38"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq15.gif"/></alternatives></inline-formula> or Kolmogorov–Smirnov). This correction guarantees the absence of penalizations caused by abrupt yet spurious deviations driven by the selected histogram granularity and help consolidate the choice of the theoretical continuous distribution. The outlier observations are only temporarily removed to fine tune the statistical hypothesis tests previously mentioned. Once the best fitting distribution is selected and category borders imputed, the library returns the original data (with all the outliers and missing values), not yielding impact on the remaining variables or subsequent data mining tasks.</p>
    </sec>
    <sec id="Sec5">
      <title>Multi-item discretization</title>
      <p id="Par19">After selecting the theoretical probability distribution that best fits the continuous variable, DI2 proceeds with the discretization. Given a desirable number of categories (bins), multiple cut-off points are generated using the inverse cumulative distribution function of the theoretical distribution. The cut-off points guarantee an approximately uniform frequency of observations per category, although empirical-theoretical distribution differences can underlie imbalances. The possibility to parameterize the number of bins is offered since in some application domains the desirable number is known a priori (e.g. well-defined number of gene activation levels for expression data analysis).</p>
      <p id="Par20">The optimal number of bins can be alternatively hyperparameterized. In supervised settings, cross-validation on training data can be pursued to this end. Similarly, in unsupervised settings, different cardinalities can be assessed against a well-defined quality criteria (e.g. silhouette in clustering solutions or number of statistically significant patterns in biclustering solutions) to estimate the number of bins. Alternatives for parameterizing the number of bins, including heuristic searches have been suggested [<xref ref-type="bibr" rid="CR17">17</xref>]. In clinical domains, Maslove et al. [<xref ref-type="bibr" rid="CR18">18</xref>] used an heuristic for determining the number of bins when discretizing data with unsupervised methods.</p>
      <p id="Par21">Unlike other well-known unsupervised discretization methods,(e.g. the aforementioned methods) DI2 supports multi-item assignments by identifying border values for each category, this is exemplified in Figure <xref rid="Fig4" ref-type="fig">4</xref>. Note also that in the presence of algorithms able to handle multi-items derived from category borders, the items-boundary problem associated with different bin choices is ameliorated. To this end, the user can optionally also define a boundary proximity percentage (between 0 and 50%, 20% being the default) to affect the distance from category borders. Let us introduce an example: the discretization of a variable following a Normal distribution, N(0, 1), with three categories. The cut-off points are − 0.43 and 0.43. To allow the presence of border values, observations with values near the frontiers of discretization are assigned with two categories. By default, a proximity of 20% to a discretization boundary is assumed for the assignment of multiple items. Proximity percentage is estimated by dividing the area under the probability distribution curve between the observation and the closest discretization boundary by the area between the discretization boundaries of the observation’s category. In the given example, observations falling between − 0.63 and − 0.43, as well as between − 0.43 and − 0.26, are assigned with two items. It can also be observed that the proximity percentages translate into border boundaries (smaller brackets) being placed to the left and right of the discretization boundary (medium-sized brackets).<fig id="Fig4"><label>Fig. 4</label><caption><p>Illustration example of discretization with 9 points along an axis and 3 categories considering border values (values which belong to 2 categories)</p></caption><graphic xlink:href="12859_2021_4329_Fig4_HTML" id="MO8"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec6">
    <title>Implementation</title>
    <p id="Par22">DI2 tool is fully implemented in Python 3.7<xref ref-type="fn" rid="Fn2">2</xref> (Additional file <xref rid="MOESM1" ref-type="media">1</xref>). DI2 is provided as an open-source method at GitHub with well-annotated APIs and notebook tutorials for a practical illustration of its major functionalities. The algorithm workflow is shown in Algorithm 1 and the Kolmogorov–Smirnov correction is shown in Algorithm 2. DI2 workflow is further shown in Figure <xref rid="Fig5" ref-type="fig">5</xref>. All the code was executed on a computer with Intel(R) Core(TM) i5-8265U CPU @ 1.60 GHz 1.80 GHz, and 24 GB of RAM.<fig id="Fig5"><label>Fig. 5</label><caption><p>The flowchart of DI2. From data input, passing through data normalization, fitting of categories, and finally discretization</p></caption><graphic xlink:href="12859_2021_4329_Fig5_HTML" id="MO9"/></fig></p>
    <graphic position="anchor" xlink:href="12859_2021_4329_Figa_HTML" id="MO10"/>
    <graphic position="anchor" xlink:href="12859_2021_4329_Figb_HTML" id="MO11"/>
  </sec>
  <sec id="Sec7">
    <title>Results and discussion</title>
    <p id="Par23">In order to illustrate some of the DI2 properties, we considered two published datasets: (1) the <italic>breast-tissue</italic>
<italic>dataset</italic> [<xref ref-type="bibr" rid="CR19">19</xref>], containing electrical impedance measurements in samples of freshly excised tissue from the breast, and (2) the <italic>yeast</italic>
<italic>dataset</italic> [<xref ref-type="bibr" rid="CR20">20</xref>], containing molecular statistics variables. Both of these are available at the UCI Machine Learning repository [<xref ref-type="bibr" rid="CR21">21</xref>] and a more detailed variable explanation is presented in Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Variables of the <italic>breast-tissue</italic> dataset and their respective description</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Variables</th><th align="left">Type</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">I0</td><td align="left">Continuous</td><td align="left">Impedivity (ohm) at zero frequency</td></tr><tr><td align="left">PA500</td><td align="left">Continuous</td><td align="left">Phase angle at 500 KHz</td></tr><tr><td align="left">HFS</td><td align="left">Continuous</td><td align="left">High-frequency slope of phase angle</td></tr><tr><td align="left">DA</td><td align="left">Continuous</td><td align="left">Impedance distance between spectral ends</td></tr><tr><td align="left">Area</td><td align="left">Continuous</td><td align="left">Area under spectrum</td></tr><tr><td align="left">A/DA</td><td align="left">Continuous</td><td align="left">Area normalized by DA</td></tr><tr><td align="left">Max IP</td><td align="left">Continuous</td><td align="left">IP maximum of the spectrum</td></tr><tr><td align="left">DR</td><td align="left">Continuous</td><td align="left">Distance between I0 and real part of the maximum frequency point</td></tr><tr><td align="left">P</td><td align="left">Continuous</td><td align="left">Length of the spectral curve</td></tr><tr><td align="left">Class</td><td align="left">Categorical</td><td align="left">Carcinoma, fibro-adenoma, mastopathy, glandular, connective, adipose</td></tr></tbody></table></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Variables of the <italic>yeast</italic> dataset and their respective description</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Variables</th><th align="left">Type</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">Sequence</td><td align="left">Text</td><td align="left">Accession number</td></tr><tr><td align="left">mcg</td><td align="left">Continuous</td><td align="left">McGeoch’s method for signal sequence recognition</td></tr><tr><td align="left">gvh</td><td align="left">Continuous</td><td align="left">von Heijne’s method for signal sequence recognition</td></tr><tr><td align="left">alm</td><td align="left">Continuous</td><td align="left">Score of the ALOM membrane spanning region prediction program</td></tr><tr><td align="left">mit</td><td align="left">Continuous</td><td align="left">Discriminant score of amino acid content of N-terminal regions</td></tr><tr><td align="left">erl</td><td align="left">Binary</td><td align="left">Presence of retention signals in the endoplasmic reticulum lumen</td></tr><tr><td align="left">pox</td><td align="left">Continuous</td><td align="left">Peroxisomal targeting signal in the C-terminus</td></tr><tr><td align="left">vac</td><td align="left">Continuous</td><td align="left">Discriminant score of aminoacid content of vacuolar/extracellular proteins</td></tr><tr><td align="left">nuc</td><td align="left">Continuous</td><td align="left">Discriminant score of nuclear localization signals</td></tr><tr><td align="left">Class</td><td align="left">Categorical</td><td align="left">Localization site of protein.</td></tr></tbody></table></table-wrap></p>
    <p id="Par24">DI2 is executed with <inline-formula id="IEq16"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M40"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq16.gif"/></alternatives></inline-formula> as the main statistical test, with and without Kolmogorov outlier removal, with single and whole column discretization, and 3, 5 and 7 categories per variable outputted. Predictive performance is further assessed against raw continuous data. The acronyms for the probability distributions referred throughout this section are described in Table <xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Theoretical probability distribution acronyms (for full list visit <ext-link ext-link-type="uri" xlink:href="https://docs.scipy.org/doc/scipy/reference/stats.html">https://docs.scipy.org/doc/scipy/reference/stats.html</ext-link>—SciPy statistical functions)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Distribution acronym</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">Alpha</td><td align="left">Alpha continuous random variable</td></tr><tr><td align="left">Exponnorm</td><td align="left">Exponentially modified Normal continuous random variable</td></tr><tr><td align="left">Foldcauchy</td><td align="left">Folded Cauchy continuous random variable</td></tr><tr><td align="left">Recipinvgauss</td><td align="left">Reciprocal inverse Gaussian continuous random variable</td></tr><tr><td align="left">Frechet_r</td><td align="left">Frechet right (or Weibull minimum) continuous random variable</td></tr><tr><td align="left">Mielke</td><td align="left">Mielke Beta-Kappa / Dagum continuous random variable</td></tr><tr><td align="left">Johnsonsu</td><td align="left">Johnson SU continuous random variable</td></tr><tr><td align="left">Johnsonsb</td><td align="left">Johnson SB continuous random variable</td></tr><tr><td align="left">Genextreme</td><td align="left">Generalized extreme value continuous random variable</td></tr><tr><td align="left">chi2</td><td align="left">Chi-squared continuous random variable</td></tr><tr><td align="left">genlogistic</td><td align="left">Generalized logistic continuous random variable</td></tr><tr><td align="left">Laplace</td><td align="left">Laplace continuous random variable</td></tr><tr><td align="left">Genhalflogistic</td><td align="left">Generalized half-logistic continuous random variable</td></tr><tr><td align="left">Gengamma</td><td align="left">Generalized gamma continuous random variable</td></tr><tr><td align="left">Pearson3</td><td align="left">Pearson type III continuous random variable</td></tr></tbody></table></table-wrap></p>
    <sec id="Sec8">
      <title>Case study: <italic>breast-tissue dataset</italic></title>
      <p id="Par25">
        <fig id="Fig6">
          <label>Fig. 6</label>
          <caption>
            <p>Distribution matching of DA variable from breast-tissue againt two statistical distributions (<italic>recipinvgauss</italic> in <bold>a</bold> and <italic>chi2</italic> in <bold>b</bold>, as well as the corresponding discretization boundaries and border values in V.c</p>
          </caption>
          <graphic xlink:href="12859_2021_4329_Fig6_HTML" id="MO12"/>
        </fig>
      </p>
      <p id="Par26">The <italic>breast-tissue</italic> dataset contains 106 data instances and 10 variables (9 continuous and 1 categorical), presented in Table <xref rid="Tab1" ref-type="table">1</xref>. The gathered results show the decisions placed by DI2 in the absence and presence of Kolmogorov–Smirnov optimization.</p>
      <p id="Par27">Table <xref rid="Tab4" ref-type="table">4</xref> shows the distributions yielding best fit for each continuous variable of the dataset. Variables “I0”, “PA500”, “A/DA”, “DR”, and “P” remained unchanged with a removal of up to 5% of outlier points. Variables “HFS” and “Area” produced better results in the <inline-formula id="IEq17"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M42"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq17.gif"/></alternatives></inline-formula> test with the removal of outliers solidifying the distribution choice. Finally, the fitting choice changed for variables “DA” and “Max IP” under the <inline-formula id="IEq18"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M44"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq18.gif"/></alternatives></inline-formula> test, revealing a more solid choice from the analysis of the residuals.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Best fitting distributions for each continuous variable, without and with Kolmogorov–Smirnov correction</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Variables</th><th align="left">Without opt.</th><th align="left"><inline-formula id="IEq19"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M46"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq19.gif"/></alternatives></inline-formula> statistic</th><th align="left"><italic>p</italic>-value &gt;0.05 (<inline-formula id="IEq20"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M48"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq20.gif"/></alternatives></inline-formula>)</th><th align="left">D statistic</th><th align="left">With opt.</th><th align="left"><inline-formula id="IEq21"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M50"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq21.gif"/></alternatives></inline-formula> statistic</th><th align="left"><italic>p</italic>-value &gt;0.05 (<inline-formula id="IEq22"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M52"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq22.gif"/></alternatives></inline-formula>)</th><th align="left">D statistic</th></tr></thead><tbody><tr><td align="left">I0</td><td align="left">alpha</td><td align="left">8.8</td><td align="left">False</td><td align="left">0.12</td><td align="left">alpha</td><td align="left">8.8</td><td align="left">False</td><td align="left">0.11</td></tr><tr><td align="left">PA500</td><td align="left">exponnorm</td><td align="left">2.98</td><td align="left">True</td><td align="left">0.07</td><td align="left">exponnorm</td><td align="left">2.98</td><td align="left">True</td><td align="left">0.07</td></tr><tr><td align="left">HFS</td><td align="left">foldcauchy</td><td align="left">2.25</td><td align="left">True</td><td align="left">0.07</td><td align="left">foldcauchy</td><td align="left">1.57</td><td align="left">True</td><td align="left">0.07</td></tr><tr><td align="left">DA</td><td align="left">recipinvgauss</td><td align="left">1.6</td><td align="left">True</td><td align="left">0.06</td><td align="left">chi2</td><td align="left">1.01</td><td align="left">True</td><td align="left">0.06</td></tr><tr><td align="left">Area</td><td align="left">frechet_r</td><td align="left">0.5</td><td align="left">True</td><td align="left">0.07</td><td align="left">frechet_r</td><td align="left">0.25</td><td align="left">True</td><td align="left">0.05</td></tr><tr><td align="left">A/DA</td><td align="left">mielke</td><td align="left">1.17</td><td align="left">True</td><td align="left">0.06</td><td align="left">mielke</td><td align="left">1.17</td><td align="left">True</td><td align="left">0.05</td></tr><tr><td align="left">Max IP</td><td align="left">johnsonsu</td><td align="left">4.72</td><td align="left">True</td><td align="left">0.05</td><td align="left">alpha</td><td align="left">1.09</td><td align="left">True</td><td align="left">0.07</td></tr><tr><td align="left">DR</td><td align="left">johnsonsb</td><td align="left">1.2</td><td align="left">True</td><td align="left">0.05</td><td align="left">johnsonsb</td><td align="left">1.2</td><td align="left">True</td><td align="left">0.05</td></tr><tr><td align="left">P</td><td align="left">genextreme</td><td align="left">5.13</td><td align="left">True</td><td align="left">0.09</td><td align="left">genextreme</td><td align="left">5.13</td><td align="left">True</td><td align="left">0.09</td></tr></tbody></table><table-wrap-foot><p>Both <inline-formula id="IEq23"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M54"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq23.gif"/></alternatives></inline-formula> (primary) and KS statistics are shown</p></table-wrap-foot></table-wrap></p>
      <p id="Par28">Considering “DA” variable, Fig. <xref rid="Fig6" ref-type="fig">6</xref>a, b show its Q-Q (quantile-quantile) plot, offering a view on the adequacy of the statistical fitting. In this context, we depict histograms for the empirical data with 100 bins (blue dots), to better visualize the impact of outlier removal, and the best theoretical distribution picked without and with Kolmogorov–Smirnov correction (red line). A moderate improvement from Fig. <xref rid="Fig6" ref-type="fig">6</xref>a, b can be detected, with the empirical quantiles (blue dots) being closer to the theoretical continuous quantiles (red line).</p>
      <p id="Par29">After the fitting stage, cut-off points are calculated to produce the final categories. Figure <xref rid="Fig5" ref-type="fig">5</xref>c compares different discretization options: quantile, uniform, and the two best fitting theoretical continuous distributions (without and with Kolmogorov–Smirnov optimization). Category cut-off points are marked as red lines, and the border values cut-off points in yellow. This analysis shows how critical discretization can be for determining the inclusion or exclusion of high density bins. The ability of DI2 to assign multiple items using borders can thus be explored by symbolic approaches to mitigate vulnerabilities inherent to the discretization process [<xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR23">23</xref>].</p>
    </sec>
    <sec id="Sec9">
      <title>Case study: <italic>yeast dataset</italic></title>
      <p id="Par30">The <italic>yeast</italic> dataset contains 1484 data instances and 10 variables, including the sample identification, class, and 8 molecular statistics variables (Table <xref rid="Tab2" ref-type="table">2</xref>). In the previous analysis, <italic>breast-tissue dataset</italic> was considered to compared DI2 category cut-off points against alternative unsupervised discretization procedures – quantile (equal-frequency) and uniform (equal-width). The <italic>yeast</italic> data is used to comprehensively assess the predictive capabilities of discretization approaches, including the k-means method.</p>
      <p id="Par31">Table <xref rid="Tab5" ref-type="table">5</xref> displays the results of the statistical tests produced by DI2 when applied to each variable independently and the whole dataset together, considering 5 categories per variable. As presented in Table <xref rid="Tab5" ref-type="table">5</xref>, the empirical distribution of a variable does not always match a known theoretical distribution with statistical significance (e.g. variable “alm”). Nonetheless, the theoretical distribution with the lowest test statistic is still selected in an effort to ameliorate bad discretization decisions by preventing critically misadjusted probability distributions.</p>
      <p id="Par32">Figure <xref rid="Fig7" ref-type="fig">7</xref>a displays the distribution of values in the variable “mit” before outlier removal (brown and blue area of histogram) and after outlier removal (brown area of histogram). Figure <xref rid="Fig7" ref-type="fig">7</xref>b compares the distribution of the categories of all the discretization techniques (DI2, quantile, uniform, and k-means), and further assesses the impact of outlier removal had in categorizing the data in different executions of DI2. Figure <xref rid="Fig8" ref-type="fig">8</xref> presents the frequency distribution of observation per category, as well as intermediate categories produced by DI2’s border values.<fig id="Fig7"><label>Fig. 7</label><caption><p>Variable “mit” distribution (<bold>a</bold>). Categories distribution after k-means, quantile, uniform, and DI2 discretization (<bold>b</bold>)</p></caption><graphic xlink:href="12859_2021_4329_Fig7_HTML" id="MO13"/></fig><fig id="Fig8"><label>Fig. 8</label><caption><p>Variable “mit” categories distribution after DI2 discretization with different settings with border values. Single column discretization with Kolmogorov–Smirnov outlier removal (light blue columns), single column discretization without Kolmogorov–Smirnov outlier removal (dark blue columns), whole dataset discretization with Kolmogorov–Smirnov outlier removal (light purple columns), whole discretization without Kolmogorov–Smirnov outlier removal (dark purple columns)</p></caption><graphic xlink:href="12859_2021_4329_Fig8_HTML" id="MO14"/></fig><table-wrap id="Tab5"><label>Table 5</label><caption><p>Best fitting distributions for each continuous variable, without and with Kolmogorov–Smirnov outlier removal, considering 5 categories per variable</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Variables</th><th align="left">Without opt.</th><th align="left"><inline-formula id="IEq24"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M56"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq24.gif"/></alternatives></inline-formula> statistic</th><th align="left"><italic>p</italic>-value &gt;0.05 (<inline-formula id="IEq25"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M58"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq25.gif"/></alternatives></inline-formula>)</th><th align="left">D statistic</th><th align="left">With opt.</th><th align="left"><inline-formula id="IEq26"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M60"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq26.gif"/></alternatives></inline-formula> statistic</th><th align="left"><italic>p</italic>-value &gt;0.05 (<inline-formula id="IEq27"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{\chi }}^2$$\end{document}</tex-math><mml:math id="M62"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>χ</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq27.gif"/></alternatives></inline-formula>)</th><th align="left">D statistic</th></tr></thead><tbody><tr><td align="left">mcg</td><td align="left">foldcauchy</td><td align="left">3.72</td><td align="left">True</td><td align="left">0.08</td><td align="left">exponnorm</td><td align="left">3.18</td><td align="left">True</td><td align="left">0.02</td></tr><tr><td align="left">gvh</td><td align="left">genlogistic</td><td align="left">3.57</td><td align="left">True</td><td align="left">0.03</td><td align="left">genlogistic</td><td align="left">2.02</td><td align="left">True</td><td align="left">0.02</td></tr><tr><td align="left">alm</td><td align="left">genlogistic</td><td align="left">17.00</td><td align="left">False</td><td align="left">0.05</td><td align="left">genlogistic</td><td align="left">12.08</td><td align="left">False</td><td align="left">0.03</td></tr><tr><td align="left">mit</td><td align="left">exponnorm</td><td align="left">19.23</td><td align="left">False</td><td align="left">0.05</td><td align="left">exponnorm</td><td align="left">6.11</td><td align="left">True</td><td align="left">0.03</td></tr><tr><td align="left">pox</td><td align="left">chi2</td><td align="left"><inline-formula id="IEq28"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4.4 \times 10^{-14}$$\end{document}</tex-math><mml:math id="M64"><mml:mrow><mml:mn>4.4</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>14</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq28.gif"/></alternatives></inline-formula></td><td align="left">True</td><td align="left">0.99</td><td align="left">gengamma</td><td align="left"><inline-formula id="IEq29"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$4.2 \times 10^{-14}$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mn>4.2</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>14</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4329_Article_IEq29.gif"/></alternatives></inline-formula></td><td align="left">True</td><td align="left">0.99</td></tr><tr><td align="left">vac</td><td align="left">laplace</td><td align="left">20.99</td><td align="left">False</td><td align="left">0.08</td><td align="left">pearson3</td><td align="left">14.18</td><td align="left">False</td><td align="left">1.00</td></tr><tr><td align="left">nuc</td><td align="left">exponnorm</td><td align="left">1116.63</td><td align="left">False</td><td align="left">0.26</td><td align="left">mielke</td><td align="left">795.28</td><td align="left">False</td><td align="left">0.26</td></tr><tr><td align="left">all variables</td><td align="left">genhalflogistic</td><td align="left">45.69</td><td align="left">False</td><td align="left">0.25</td><td align="left">genhalflogistic</td><td align="left">10.25</td><td align="left">False</td><td align="left">0.21</td></tr></tbody></table></table-wrap></p>
      <p id="Par33">The performed analysis for the <italic>yeast dataset</italic> shows how critical the category border, previously discussed in more detail with the <italic>breast-tissue</italic> dataset, can be. The ability of DI2 to assign multiple items using borders can be explored by symbolic approaches to mitigate vulnerabilities inherent to the discretization process as discussed in the following subsection.</p>
    </sec>
    <sec id="Sec10">
      <title>Predictive performance</title>
      <p id="Par34">To assess the predictive impact of DI2, we reuse the <italic>yeast</italic> dataset, applying a cross-validation scheme with 10 folds, and six supervised classification methods: Naive Bayes [<xref ref-type="bibr" rid="CR24">24</xref>], Random Forest [<xref ref-type="bibr" rid="CR25">25</xref>], support vector machines using Sequential Minimal Optimization (SMO) [<xref ref-type="bibr" rid="CR26">26</xref>], C4.5 [<xref ref-type="bibr" rid="CR27">27</xref>], Multinomial Logistic Regression Model (MLRM) [<xref ref-type="bibr" rid="CR28">28</xref>] and FleBiC [<xref ref-type="bibr" rid="CR29">29</xref>]. Discretization procedures are applied with 3, 5 and 7 categories per variable. To preserve the soundness of assessments, the discretization thresholds are learned only on the training data per fold. The testing data instances are then discretized using the learned discretization thresholds from training data.</p>
      <p id="Par35">Figure <xref rid="Fig9" ref-type="fig">9</xref> presents the results of the aforementioned models with the original numerical data and a discretization of 5 categories per variable. In each model, DI2, with configurations of single column discretization and outlier removal, is among the top performing procedure. In particular, the C4.5 model, DI2, with configurations of combined column discretization, achieved the highest accuracy compared with other discretization methods. Considering Naïve Bayes and SMO models, DI2 achieves competitive performance against the original numerical data, with a generally higher average accuracy for single column discretizations, yet not yielding statistically significant improvements.<fig id="Fig9"><label>Fig. 9</label><caption><p>Average accuracy per classifier and discretization method available without border values and considering 5 categories per variable﻿(for more information consult Additional file <xref rid="MOESM2" ref-type="media">2</xref>). From left to right in each group of bars: K-means, Quartile, Uniform, DI2 (single, kol. correction), DI2 (single), DI2 (whole, kol. correction), DI2 (whole) and original data</p></caption><graphic xlink:href="12859_2021_4329_Fig9_HTML" id="MO15"/></fig></p>
      <p id="Par36">Figure <xref rid="Fig10" ref-type="fig">10</xref> displays the average accuracy achieved by each model with a discretization of 3 and 7 categories per variable. Results considering 3 and 7 categories were not as optimal as with 5 categories, in terms of accuracy. Nonetheless, these results further encourage hyperparameterization to find an optimal number of bins.<fig id="Fig10"><label>Fig. 10</label><caption><p>Accuracy when executing different models with multiple discretization methods. From left to right the bars are: K-means, Quartile, Uniform, DI2 (single, kol. correction) without border values and original data</p></caption><graphic xlink:href="12859_2021_4329_Fig10_HTML" id="MO16"/></fig></p>
      <p id="Par37">In order to fully test out the potential of DI2, we now considered border values. FleBiC [<xref ref-type="bibr" rid="CR29">29</xref>] is a classifier able to place decisions based on multi-item assignments. Other approaches, such as BicPAMS [<xref ref-type="bibr" rid="CR4">4</xref>] (a patterned-based biclustering algorithm), can be alternatively consider to accommodate border values and thus minimize potential discretization drawbacks. FleBiC is here executed as a stand-alone classifier and as an adjunct classifier to guide decisions of Random Forests, where decisions are derived from both the probabilistic outputs of FleBiC (50%) and Random Forests (50%), which will be denoted by FleBiC Hybrid. Figure <xref rid="Fig11" ref-type="fig">11</xref> shows the results of FleBiC and FleBiC Hybrid. In terms of average accuracy (Figure <xref rid="Fig11" ref-type="fig">11</xref>.a), both FleBiC and FleBiC Hybrid yield higher predictive accuracy with DI2 method than with other discretization methods. Within the different settings of DI2, the best predictive accuracy is achieved for FleBiC Hybrid when the predictive model considers border values. Figure <xref rid="Fig12" ref-type="fig">12</xref> presents the results when considering 3 and 7 categories. Finally, when considering the sensitivity of the NUC outcome (Figure <xref rid="Fig11" ref-type="fig">11</xref>.b), we can see that the incorporation of border values plays a decisive role, making it possible to break through a ceiling on the NUC predictability against discretization methods unable to consider border values. More details on the relevance of border values to improve the sensitivity of other classes are provided in supplementary material. This analysis shows that the use of border values can yield significant improvements.<fig id="Fig11"><label>Fig. 11</label><caption><p>Accuracy when executing different FleBiC versions, and Sensitivity of when predicting class NUC, with multiple discretization methods considering 5 categories per variable (for more information consult Additional file <xref rid="MOESM2" ref-type="media">2</xref>). From left to right the bars are: K-means, Quartile, Uniform, DI2 (single, kol. correction), DI2 (single), DI2 (border values, single, kol. correction) and DI2 (border values, single)</p></caption><graphic xlink:href="12859_2021_4329_Fig11_HTML" id="MO17"/></fig><fig id="Fig12"><label>Fig. 12</label><caption><p>Accuracy when executing different FleBiC versions with multiple discretization methods considering 7 categories per variable. From left to right the bars are: original data, K-means, Quantile, Uniform, DI2 (single, kol. correction), DI2 (single), DI2 (border values, single, kol. correction), DI2 (border values, single)</p></caption><graphic xlink:href="12859_2021_4329_Fig12_HTML" id="MO18"/></fig></p>
      <p id="Par38">To assess if the previous differences in predictive accuracy are statistically significant, a one-tailed paired <italic>t</italic>-test is applied. We consider the alternative hypothesis (<italic>p</italic>-value &lt; 0.05) to be “DI2 is superior to the identified discretization procedure using the same classifier”. Results obtained considering the discretization of 5 categories per variable are presented in Table <xref rid="Tab6" ref-type="table">6</xref>. DI2 shows statistically significant improvements against uniform discretization in all classification models. DI2, with single column and optimized single column configurations, despite displaying competitive predictive accuracy in most of the classifiers against k-means and quantile discretizations, it does not show statistically significant improvement. However, when considering FleBiC, DI2 outperformed all remaining discretization methods, with or without border values (<italic>p</italic>-value&lt;0.05). In FleBiC Hybrid, DI2 also outperformed all other discretization methods with the exception of quantile discretization when no border values are considered.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Gathered <italic>p</italic>-values from statistically testing the superiority of DI2 with respect to predictive accuracy against alternative discretization procedures, and original data, using one-tailed paired <italic>t</italic>-test and considering 5 categories per variable (complementary information in Additional file <xref rid="MOESM3" ref-type="media">﻿3﻿</xref>)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="4">DI2 (single)</th><th align="left" colspan="4">DI2 (single, optimized)</th></tr><tr><th align="left">K-means</th><th align="left">Quantile</th><th align="left">Uniform</th><th align="left">Original</th><th align="left">K-means</th><th align="left">Quantile</th><th align="left">Uniform</th><th align="left">Original</th></tr></thead><tbody><tr><td align="left">Naïve Bayes</td><td align="left">0.686</td><td align="left">0.897</td><td align="left"><bold>0</bold>.<bold>005</bold></td><td align="left">0.719</td><td align="left">0.287</td><td align="left">0.431</td><td align="left"><bold>0</bold>.<bold>002</bold></td><td align="left">0.325</td></tr><tr><td align="left">Random Forest</td><td align="left">0.404</td><td align="left">0.921</td><td align="left">0.101</td><td align="left">0.998</td><td align="left">0.126</td><td align="left">0.653</td><td align="left"><bold>0</bold>.<bold>016</bold></td><td align="left">0.998</td></tr><tr><td align="left">SMO</td><td align="left">0.980</td><td align="left">0.968</td><td align="left"><bold>0</bold>.<bold>014</bold></td><td align="left">0.456</td><td align="left">0.790</td><td align="left">0.773</td><td align="left"><bold>0</bold>.<bold>017</bold></td><td align="left">0.441</td></tr><tr><td align="left">C4.5</td><td align="left">0.500</td><td align="left">0.345</td><td align="left"><bold>0</bold>.<bold>044</bold></td><td align="left">0.965</td><td align="left">0.230</td><td align="left">0.194</td><td align="left"><bold>0</bold>.<bold>013</bold></td><td align="left">0.891</td></tr><tr><td align="left">MLRM</td><td align="left">0.500</td><td align="left">0.907</td><td align="left"><bold>0</bold>.<bold>009</bold></td><td align="left">0.803</td><td align="left">0.316</td><td align="left">0.821</td><td align="left"><bold>0</bold>.<bold>013</bold></td><td align="left">0.588</td></tr><tr><td align="left">FleBiC</td><td align="left"><bold>0</bold>.<bold>001</bold></td><td align="left"><bold>0</bold>.<bold>007</bold></td><td align="left"><bold>1.9E−08</bold></td><td align="left">–</td><td align="left"><bold>2.1E−05</bold></td><td align="left"><bold>1.0E−04</bold></td><td align="left"><bold>6.7E−09</bold></td><td align="left">–</td></tr><tr><td align="left">FleBiC Hybrid</td><td align="left"><bold>5.4E−04</bold></td><td align="left">0.693</td><td align="left"><bold>5.2E−05</bold></td><td align="left">–</td><td align="left"><bold>0</bold>.<bold>030</bold></td><td align="left">0.873</td><td align="left"><bold>2.0E−04</bold></td><td align="left">–</td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="4">DI2 (whole)</th><th align="left" colspan="4">DI2 (whole, optimized)</th></tr><tr><th align="left">K-means</th><th align="left">Quantile</th><th align="left">Uniform</th><th align="left">Original</th><th align="left">K-means</th><th align="left">Quantile</th><th align="left">Uniform</th><th align="left">Original</th></tr></thead><tbody><tr><td align="left">Naïve Bayes</td><td align="left">0.948</td><td align="left">0.991</td><td align="left"><bold>0</bold>.<bold>020</bold></td><td align="left">0.965</td><td align="left">0.662</td><td align="left">0.822</td><td align="left"><bold>0</bold>.<bold>004</bold></td><td align="left">0.712</td></tr><tr><td align="left">Random Forest</td><td align="left">0.066</td><td align="left">0.426</td><td align="left"><bold>0</bold>.<bold>012</bold></td><td align="left">0.992</td><td align="left">0.074</td><td align="left">0.666</td><td align="left">0.195</td><td align="left">0.999</td></tr><tr><td align="left">SMO</td><td align="left">0.906</td><td align="left">0.914</td><td align="left"><bold>0</bold>.<bold>042</bold></td><td align="left">0.641</td><td align="left">0.805</td><td align="left">0.813</td><td align="left"><bold>0</bold>.<bold>026</bold></td><td align="left">0.406</td></tr><tr><td align="left">C4.5</td><td align="left">0.085</td><td align="left">0.072</td><td align="left"><bold>0</bold>.<bold>004</bold></td><td align="left">0.702</td><td align="left">0.687</td><td align="left">0.500</td><td align="left"><bold>0</bold>.<bold>028</bold></td><td align="left">0.958</td></tr><tr><td align="left">MLRM</td><td align="left">0.952</td><td align="left">0.986</td><td align="left">0.148</td><td align="left">0.993</td><td align="left">0.721</td><td align="left">0.896</td><td align="left"><bold>0</bold>.<bold>047</bold></td><td align="left">0.942</td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"/><th align="left" colspan="4">DI2 (borders, single)</th><th align="left" colspan="4">DI2 (borders, single, optimized)</th></tr><tr><th align="left">K-means</th><th align="left">Quantile</th><th align="left">Uniform</th><th align="left">Original</th><th align="left">K-means</th><th align="left">Quantile</th><th align="left">Uniform</th><th align="left">Original</th></tr></thead><tbody><tr><td align="left">FleBiC</td><td align="left"><bold>8.0E−05</bold></td><td align="left"><bold>7.3E−05</bold></td><td align="left"><bold>1.5E−08</bold></td><td align="left">–</td><td align="left"><bold>0</bold>.<bold>002</bold></td><td align="left"><bold>0</bold>.<bold>016</bold></td><td align="left"><bold>9.1E−08</bold></td><td align="left">–</td></tr><tr><td align="left">FleBiC Hybrid</td><td align="left"><bold>1.4E−05</bold></td><td align="left"><bold>0</bold>.<bold>001</bold></td><td align="left"><bold>4.3E−06</bold></td><td align="left">–</td><td align="left"><bold>6.1E−04</bold></td><td align="left">0.084</td><td align="left"><bold>1.0E−04</bold></td><td align="left">–</td></tr></tbody></table><table-wrap-foot><p>DI2 is assessed without and with border values, single column and whole dataset, and in the absence and presence of outlier removal</p><p>Bold values indicate that the accuracy achieved using DI2 discretization is statistically superior against the corresponding discretization</p></table-wrap-foot></table-wrap></p>
      <p id="Par39">The benefits of discretization go beyond the previously assessed predictive settings. In the context of deep learning approaches, Rabanser et al. [<xref ref-type="bibr" rid="CR30">30</xref>] surveyed the effect of data input and output transformations on the predictive performance of several neural forecasting architectures, concluding that the WaveNet model, when input data is discretized, yields best results.</p>
    </sec>
    <sec id="Sec11">
      <title>Scalability</title>
      <p id="Par40">The execution time of DI2 is presented in Fig. <xref rid="Fig13" ref-type="fig">13</xref>. Figure <xref rid="Fig13" ref-type="fig">13</xref>a displays the efficiency according to the number of tested theoretical distributions (from fastest to slowest in terms of parameter estimation) using the <italic>yeast</italic> dataset (1484 observations). Figure <xref rid="Fig13" ref-type="fig">13</xref>.b depicts how the computational time varies in accordance with the number of observations for the DI2 default setting, considering the <italic>yeast</italic> data with all variables.<fig id="Fig13"><label>Fig. 13</label><caption><p>Computational time efficiency of DI2 (without outlier removal) according to the number of underlying probability distributions (<bold>a</bold>) and number of observations. Candidate distributions (from 0 to 95) are added with respect to ascending computational time, i.e. from fastest to slowest estimation of the theoretical distribution’s parameters</p></caption><graphic xlink:href="12859_2021_4329_Fig13_HTML" id="MO19"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Conclusion</title>
    <p id="Par41">This work proposed a new unsupervised method for data discretization, DI2, that takes into account the underlying data regularities, the presence of outlier values disrupting expected regularities, as well as the relevance of border values. A tool for the autonomous, prior-free discretization of biological data with arbitrarily skewed variable distributions is provided to this end.</p>
    <p id="Par42">Our study showed that DI2 is a viable and robust discretization procedure when compared against well-established unsupervised discretization methods. Statistical tests applied to assess differences in performance confirm that DI2 generally outperforms alternative discretization methods with statistical significance. The combined use of DI2 within classification tasks results in either competitive or superior levels of predictive accuracy. DI2 as the unique feature of allowing the incorporation of border values. FleBiC, a classifier able to accommodate border values, achieved statistically significant performance improvements in the presence of multi-item assignments.</p>
  </sec>
  <sec id="Sec13">
    <title>Availability and requirements</title>
    <p id="Par43">Project name: DI2: prior-free and multi-item discretization.</p>
    <p id="Par44">Software homepage: <ext-link ext-link-type="uri" xlink:href="https://github.com/JupitersMight/DI2">https://github.com/JupitersMight/DI2</ext-link>.</p>
    <p id="Par45">Programming language: Python.</p>
    <p id="Par46">Other requirements: python 3.7, pandas 1.2.4, scipy 1.5.1 and numpy 1.20.2.</p>
    <p id="Par47">License: MIT License.</p>
    <p id="Par48">Any restrictions to use by non-academics: None.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec14">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2021_4329_MOESM1_ESM.zip">
            <caption>
              <p><bold>Additional file 1.</bold> Folder containing DI2 and an example in Jupyter Notebook using Breast Tissue dataset example.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="12859_2021_4329_MOESM2_ESM.xlsx">
            <caption>
              <p><bold>Additional file 2.</bold> File with the average accuracy achieved by models with discretization method considering 5 categories.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="12859_2021_4329_MOESM3_ESM.xlsx">
            <caption>
              <p><bold>Additional file 3.</bold> File with the accuracy achieved in cross validation by each discretization method in each model considering 5 categories.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>DI2</term>
        <def>
          <p id="Par4">Distribution Discretizer</p>
        </def>
      </def-item>
      <def-item>
        <term>Quantile</term>
        <def>
          <p id="Par5">Equal-frequency</p>
        </def>
      </def-item>
      <def-item>
        <term>Uniform</term>
        <def>
          <p id="Par6">Equal-width</p>
        </def>
      </def-item>
      <def-item>
        <term>Q-Q plot</term>
        <def>
          <p id="Par7">Quantile–Quantile plot</p>
        </def>
      </def-item>
      <def-item>
        <term>FleBiC</term>
        <def>
          <p id="Par8">Flexible Biclustering-based Classifier</p>
        </def>
      </def-item>
      <def-item>
        <term>BicPAMS</term>
        <def>
          <p id="Par9">Biclustering based on PAttern Mining Software</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par53"><ext-link ext-link-type="uri" xlink:href="https://docs.scipy.org/doc/scipy/reference/stats.html">https://docs.scipy.org/doc/scipy/reference/stats.html</ext-link>.</p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par54">DI2 currently uses the following libraries: pandas 1.2.4, scipy 1.5.1, and numpy 1.20.2</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes notes-type="author-contribution">
    <title>Authors' contributions</title>
    <p>All authors contributed to the design of the methodology. LA implemented the software and produced the first draft of the manuscript. RH provided the results for the predictive performance. RSC validated the datasets and results guaranteeing their usability. Both RSC and RH revised the manuscript extensively. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported by Fundação para a Ciência e a Tecnologia (FCT), through IDMEC, under LAETA project (UIDB/50022/2020), IPOscore with reference (DSAIPA/DS/0042/2018), and ILU (DSAIPA/DS/0111/2018). This work was further supported by the Associate Laboratory for Green Chemistry (LAQV), financed by national funds from FCT/MCTES (UIDB/50006/2020 and UIDP/50006/2020), INESC-ID plurianual (UIDB/50021/2020) and the contract CEECIND/01399/2017 to RSC. The funding entities did not partake in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The software is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/JupitersMight/DI2">https://github.com/JupitersMight/DI2</ext-link>. The data is publicly available at the UCI Machine Learning repository [<xref ref-type="bibr" rid="CR31">31</xref>]. The <italic>breast-tissue</italic> dataset is available at: <ext-link ext-link-type="uri" xlink:href="https://archive.ics.uci.edu/ml/datasets/Breast+Tissue">https://archive.ics.uci.edu/ml/datasets/Breast+Tissue</ext-link> and the <italic>yeast</italic> dataset is available at: <ext-link ext-link-type="uri" xlink:href="https://archive.ics.uci.edu/ml/datasets/yeast">https://archive.ics.uci.edu/ml/datasets/yeast</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar2">
      <title>Ethics approval and consent to participate</title>
      <p id="Par49">Not applicable.</p>
    </notes>
    <notes id="FPar3">
      <title>Consent for publication</title>
      <p id="Par50">Not Applicable.</p>
    </notes>
    <notes id="FPar4" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par51">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Altman DG. Categorizing continuous variables. Wiley StatsRef: Statistics Reference. Online; 2014.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bennette</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vickers</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Against quantiles: categorization of continuous variables in epidemiologic research, and its discontents</article-title>
        <source>BMC Med Res Methodol</source>
        <year>2012</year>
        <volume>12</volume>
        <issue>1</issue>
        <fpage>21</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2288-12-21</pub-id>
        <pub-id pub-id-type="pmid">22375553</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liao</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>IN</given-names>
          </name>
        </person-group>
        <article-title>Appropriate medical data categorization for data mining classification techniques</article-title>
        <source>Med Inform Internet Med</source>
        <year>2002</year>
        <volume>27</volume>
        <issue>1</issue>
        <fpage>59</fpage>
        <lpage>67</lpage>
        <pub-id pub-id-type="doi">10.1080/14639230210153749</pub-id>
        <pub-id pub-id-type="pmid">12509124</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Henriques</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Madeira</surname>
            <given-names>SC</given-names>
          </name>
        </person-group>
        <article-title>BicPAM: pattern-based biclustering for biomedical data analysis</article-title>
        <source>Algorithms Mol Biol</source>
        <year>2014</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>27</fpage>
        <pub-id pub-id-type="doi">10.1186/s13015-014-0027-z</pub-id>
        <pub-id pub-id-type="pmid">25649207</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Chen T, Guestrin C. Xgboost: a scalable tree boosting system. In: Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining; 2016. p. 785–794.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Okada</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Okubo</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Horton</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Fujibuchi</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Exhaustive search method of gene expression modules and its application to human tissue data</article-title>
        <source>IAENG Int J Comput Sci</source>
        <year>2007</year>
        <volume>34</volume>
        <issue>1</issue>
        <fpage>119126</fpage>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Kakadiaris</surname>
            <given-names>IA</given-names>
          </name>
        </person-group>
        <article-title>Hierarchical multi-label classification using fully associative ensemble learning</article-title>
        <source>Pattern Recognit</source>
        <year>2017</year>
        <volume>70</volume>
        <fpage>89</fpage>
        <lpage>103</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2017.05.007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Wang T. Multi-value rule sets for interpretable classification with feature-efficient representations. In: Proceedings of the 32nd International Conference on Neural Information Processing Systems; 2018. p. 10858–68.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Garcia</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Luengo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sáez</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Lopez</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Herrera</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A survey of discretization techniques: taxonomy and empirical analysis in supervised learning</article-title>
        <source>IEEE Trans Knowl Data Eng</source>
        <year>2012</year>
        <volume>25</volume>
        <issue>4</issue>
        <fpage>734</fpage>
        <lpage>750</lpage>
        <pub-id pub-id-type="doi">10.1109/TKDE.2012.35</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Webb</surname>
            <given-names>GI</given-names>
          </name>
        </person-group>
        <article-title>Discretization for Naive–Bayes learning: managing discretization bias and variance</article-title>
        <source>Mach Learn</source>
        <year>2009</year>
        <volume>74</volume>
        <issue>1</issue>
        <fpage>39</fpage>
        <lpage>74</lpage>
        <pub-id pub-id-type="doi">10.1007/s10994-008-5083-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Tou JT, Gonzalez RC. Pattern recognition principles; 1974.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Dodge</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Commenges</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <source>The Oxford dictionary of statistical terms</source>
        <year>2006</year>
        <publisher-loc>Oxford</publisher-loc>
        <publisher-name>Oxford University Press on Demand</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Lowry R. Concepts and applications of inferential statistics; 2014.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gonzalez</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Sahni</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Franta</surname>
            <given-names>WR</given-names>
          </name>
        </person-group>
        <article-title>An efficient algorithm for the Kolmogorov–Smirnov and Lilliefors tests</article-title>
        <source>ACM Trans Math Softw</source>
        <year>1977</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>60</fpage>
        <lpage>64</lpage>
        <pub-id pub-id-type="doi">10.1145/355719.355724</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Virtanen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Gommers</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Oliphant</surname>
            <given-names>TE</given-names>
          </name>
          <name>
            <surname>Haberland</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Reddy</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Cournapeau</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title>
        <source>Nat Methods</source>
        <year>2020</year>
        <volume>17</volume>
        <issue>3</issue>
        <fpage>261</fpage>
        <lpage>272</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
        <pub-id pub-id-type="pmid">32015543</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Watson</surname>
            <given-names>GS</given-names>
          </name>
        </person-group>
        <article-title>Some recent results in chi-square goodness-of-fit tests</article-title>
        <source>Biometrics</source>
        <year>1959</year>
        <volume>15</volume>
        <fpage>440</fpage>
        <lpage>68</lpage>
        <pub-id pub-id-type="doi">10.2307/2527749</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Martignon</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Katsikopoulos</surname>
            <given-names>KV</given-names>
          </name>
          <name>
            <surname>Woike</surname>
            <given-names>JK</given-names>
          </name>
        </person-group>
        <article-title>Categorization with limited resources: a family of simple heuristics</article-title>
        <source>J Math Psychol</source>
        <year>2008</year>
        <volume>52</volume>
        <issue>6</issue>
        <fpage>352</fpage>
        <lpage>361</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmp.2008.04.003</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maslove</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Podchiyska</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Lowe</surname>
            <given-names>HJ</given-names>
          </name>
        </person-group>
        <article-title>Discretization of continuous features in clinical datasets</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2013</year>
        <volume>20</volume>
        <issue>3</issue>
        <fpage>544</fpage>
        <lpage>553</lpage>
        <pub-id pub-id-type="doi">10.1136/amiajnl-2012-000929</pub-id>
        <pub-id pub-id-type="pmid">23059731</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jossinet</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Variability of impedivity in normal and pathological breast tissue</article-title>
        <source>Med Biol Eng Comput</source>
        <year>1996</year>
        <volume>34</volume>
        <issue>5</issue>
        <fpage>346</fpage>
        <lpage>350</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02520002</pub-id>
        <pub-id pub-id-type="pmid">8945857</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Horton</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Nakai</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>A probabilistic classification system for predicting the cellular localization sites of proteins</article-title>
        <source>Proc Int Conf Intell Syst Mol Biol</source>
        <year>1996</year>
        <volume>4</volume>
        <fpage>109</fpage>
        <lpage>115</lpage>
        <?supplied-pmid 8877510?>
        <pub-id pub-id-type="pmid">8877510</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Dua D, Graff C. UCI machine learning repository; 2017. <ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Ushakov N, Ushakov V. Recovering information lost due to discretization. In: XXXIV. International seminar on stability problems for stochastic models. p. 102.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Chmielewski MR, Grzymala-Busse JW. Global discretization of continuous attributes as preprocessing for machine learning. In: Third international workshop on rough sets and soft computing; 1994. p. 294–301.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">John GH, Langley P. Estimating continuous distributions in Bayesian classifiers. In: Eleventh conference on uncertainty in artificial intelligence. San Mateo: Morgan Kaufmann; 1995. p. 338–45.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Learn</source>
        <year>2001</year>
        <volume>45</volume>
        <issue>1</issue>
        <fpage>5</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Platt J. Sequential minimal optimization: a fast algorithm for training support vector machines. 1998.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Quinlan</surname>
            <given-names>JR</given-names>
          </name>
        </person-group>
        <source>C4. 5: programs for machine learning</source>
        <year>2014</year>
        <publisher-loc>Amsterdam</publisher-loc>
        <publisher-name>Elsevier</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>le Cessie</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>van Houwelingen</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>Ridge estimators in logistic regression</article-title>
        <source>Appl Stat</source>
        <year>1992</year>
        <volume>41</volume>
        <issue>1</issue>
        <fpage>191</fpage>
        <lpage>201</lpage>
        <pub-id pub-id-type="doi">10.2307/2347628</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Henriques</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Madeira</surname>
            <given-names>SC</given-names>
          </name>
        </person-group>
        <article-title>FleBiC: learning classifiers from high-dimensional biomedical data using discriminative biclusters with non-constant patterns</article-title>
        <source>Pattern Recognit</source>
        <year>2021</year>
        <volume>115</volume>
        <fpage>107900</fpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2021.107900</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Rabanser S, Januschowski T, Flunkert V, Salinas D, Gasthaus J. The effectiveness of discretization in forecasting: an empirical study on neural time series models. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/200510111">arXiv:200510111</ext-link>. 2020.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Asuncion A, Newman D. UCI machine learning repository; 2007.</mixed-citation>
    </ref>
  </ref-list>
</back>
