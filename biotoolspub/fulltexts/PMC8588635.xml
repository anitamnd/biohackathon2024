<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName nihms2pmcx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr IEEE Trans Med Imaging?>
<?submitter-system nihms?>
<?submitter-canonical-name IEEE Publishing Technology?>
<?submitter-canonical-id IEEE?>
<?submitter-userid 8201177?>
<?submitter-authority myNCBI?>
<?submitter-login ieee?>
<?submitter-name IEEE Publishing Technology?>
<?domain nihpa?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">8310780</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20511</journal-id>
    <journal-id journal-id-type="nlm-ta">IEEE Trans Med Imaging</journal-id>
    <journal-id journal-id-type="iso-abbrev">IEEE Trans Med Imaging</journal-id>
    <journal-title-group>
      <journal-title>IEEE transactions on medical imaging</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0278-0062</issn>
    <issn pub-type="epub">1558-254X</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8588635</article-id>
    <article-id pub-id-type="pmid">33909561</article-id>
    <article-id pub-id-type="doi">10.1109/TMI.2021.3076191</article-id>
    <article-id pub-id-type="manuscript">nihpa1751922</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>MDPET: A Unified Motion Correction and Denoising Adversarial Network for Low-Dose Gated PET</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Bo</given-names>
        </name>
        <role>Student Member, IEEE</role>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2906-0897</contrib-id>
        <aff id="A1">Department of Biomedical Engineering, Yale University, New Haven, CT 06511 USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tsai</surname>
          <given-names>Yu-Jung</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1749-7917</contrib-id>
        <aff id="A2">Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT 06511 USA.</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Xiongchao</given-names>
        </name>
        <role>Student Member, IEEE</role>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4112-8492</contrib-id>
        <aff id="A3">Department of Biomedical Engineering, Yale University, New Haven, CT 06511 USA</aff>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Duncan</surname>
          <given-names>James S.</given-names>
        </name>
        <role>Life Fellow, IEEE</role>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5167-9856</contrib-id>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Chi</given-names>
        </name>
        <role>Senior Member, IEEE</role>
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7007-1037</contrib-id>
      </contrib>
      <aff id="A4">Department of Biomedical Engineering and the Department of Radiology and Biomedical Imaging, Yale University, New Haven, CT 06511 USA</aff>
    </contrib-group>
    <author-notes>
      <corresp id="CR1"><italic>Corresponding authors: Bo Zhou; Chi Liu</italic>. <email>bo.zhou@yale.edu</email>, <email>chi.liu@yale.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>30</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>11</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>11</month>
      <year>2021</year>
    </pub-date>
    <volume>40</volume>
    <issue>11</issue>
    <fpage>3154</fpage>
    <lpage>3164</lpage>
    <!--elocation-id from pubmed: 10.1109/TMI.2021.3076191-->
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link></license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">In positron emission tomography (PET), gating is commonly utilized to reduce respiratory motion blurring and to facilitate motion correction methods. In application where low-dose gated PET is useful, reducing injection dose causes increased noise levels in gated images that could corrupt motion estimation and subsequent corrections, leading to inferior image quality. To address these issues, we propose MDPET, a unified motion correction and denoising adversarial network for generating motion-compensated low-noise images from low-dose gated PET data. Specifically, we proposed a Temporal Siamese Pyramid Network (TSP-Net) with basic units made up of 1.) Siamese Pyramid Network (SP-Net), and 2.) a recurrent layer for motion estimation among the gates. The denoising network is unified with our motion estimation network to simultaneously correct the motion and predict a motion-compensated denoised PET reconstruction. The experimental results on human data demonstrated that our MDPET can generate accurate motion estimation directly from low-dose gated images and produce high-quality motion-compensated low-noise reconstructions. Comparative studies with previous methods also show that our MDPET is able to generate superior motion estimation and denoising performance. Our code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/bbbbbbzhou/MDPET">https://github.com/bbbbbbzhou/MDPET</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>Low-dose gated PET</kwd>
      <kwd>denoising</kwd>
      <kwd>motion estimation</kwd>
      <kwd>motion correction</kwd>
      <kwd>unified network</kwd>
      <kwd>deep learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>I.</label>
    <title>Introduction</title>
    <p id="P2">Positron emission tomography (PET) is a commonly used functional imaging modality with wide applications in oncology, cardiology, neurology, and biomedical research. PET scans require injection of a small amount of radioactive tracer to patients, introducing radiation exposure to both patients and healthcare providers. By reducing the administered injection dose, low-dose PET is of-great-interests according to the As Low As Reasonably Achievable concept (ALARA) [<xref rid="R1" ref-type="bibr">1</xref>], in particular for applications of serial PET scans to measure response to therapy. Since the data acquisition typically takes 10 to 20 minutes, the patient’s respiratory motion in the thorax and upper abdomen areas inevitably introduces blurring in the reconstructed images, affecting subsequent diagnosis and treatments [<xref rid="R2" ref-type="bibr">2</xref>]. Respiratory gating facilitated by external motion monitoring devices, such as Anzai [<xref rid="R3" ref-type="bibr">3</xref>], is typically used to provide gated images with reduced respiratory motion effect. The gated image that shows minimum motion effects is then used for clinical interpretation. However, the interpretation can still be hampered by the increased image noise level as each gated image is generated by only a fraction of all detected events. To tackle the issue, previous works proposed approaches involving an initial image reconstruction for each gate followed by an image registration for motion estimation among different gates. The motion vectors derived from the image registration were then utilized to average transformed images or incorporated into a final reconstruction to generate a motion compensated image with all events. In addition to using the conventional non-rigid image registration algorithms [<xref rid="R4" ref-type="bibr">4</xref>]–[<xref rid="R7" ref-type="bibr">7</xref>], deep learning based methods were explored recently as well [<xref rid="R8" ref-type="bibr">8</xref>], [<xref rid="R9" ref-type="bibr">9</xref>]. However, the noisy gated images could lead to inaccurate motion estimation and alignment errors. In applications of low-dose gated PET, this makes extending the previously mentioned approaches for motion estimation/correction challenging because the noise level is further increased in each gated images. The highly noisy gated image could lead to non-ideal motion estimation results by previous methods, and could subsequently degrade the final motion-compensated reconstructions. Moreover, in low-dose gated PET, denoising methods should also be applied to the final motion-compensated image reconstructed with all events because there are limited events from low-dose data.</p>
    <p id="P3">Previous works on denoising low-dose PET can be summarized into two categories: conventional image post-processing [<xref rid="R10" ref-type="bibr">10</xref>]–[<xref rid="R12" ref-type="bibr">12</xref>] and deep learning based methods [<xref rid="R13" ref-type="bibr">13</xref>]–[<xref rid="R22" ref-type="bibr">22</xref>]. Conventional image post-processing techniques, such as Gaussian filtering, are standard techniques in practice, but have challenges to preserve local structures. Non-local mean filter [<xref rid="R10" ref-type="bibr">10</xref>] and block-matching 4D filter [<xref rid="R11" ref-type="bibr">11</xref>] were proposed to denoise low-dose PET while better preserving the structural information. Although these conventional image post-processing methods may substantially improve the image quality, over-smoothing is often observed in ultra-low-dose data. Recently, deep learning techniques have achieved promising performance in medical imaging applications, such as reconstruction [<xref rid="R23" ref-type="bibr">23</xref>]–[<xref rid="R27" ref-type="bibr">27</xref>], segmentation [<xref rid="R28" ref-type="bibr">28</xref>]–[<xref rid="R30" ref-type="bibr">30</xref>], registration [<xref rid="R31" ref-type="bibr">31</xref>] and denoising [<xref rid="R32" ref-type="bibr">32</xref>]. As the statistical characteristics of noise in medical imaging is complex and hard to model, deep learning models can learn the highly non-linear relationship from data and recover the original signal from noise. For deep learning based low-dose PET denoising, previous works can be further divided into two categories. The first category only uses the low-dose PET data as input. Kaplan and Zhu [<xref rid="R16" ref-type="bibr">16</xref>] proposed using a GAN [<xref rid="R33" ref-type="bibr">33</xref>] with UNet [<xref rid="R28" ref-type="bibr">28</xref>] as generator to predict standard-dose PET images from low-dose PET images. Similarly, Wang et el. [<xref rid="R14" ref-type="bibr">14</xref>] proposed using a 3D-conditional-GAN [<xref rid="R34" ref-type="bibr">34</xref>] also with UNet as generator to translate low-dose PET images to standard-dose PET images. In addition to GAN, Ouyang <italic>et el</italic>. [<xref rid="R20" ref-type="bibr">20</xref>] further improves the denoising performance by incorporating patient specific diagnosis information. Zhou <italic>et el</italic>. [<xref rid="R19" ref-type="bibr">19</xref>] and Gong <italic>et el</italic>. [<xref rid="R18" ref-type="bibr">18</xref>] found incorporating Wasserstein GAN [<xref rid="R35" ref-type="bibr">35</xref>] can also achieve promising low-dose PET denoising performance. Furthermore, Hu <italic>et el</italic>. [<xref rid="R17" ref-type="bibr">17</xref>] proposed a DPIR network that directly predicts the standard-dose PET image from low-dose PET sinogram data. The second category uses the low-dose PET images and MR/CT images as input. Xiang <italic>et el</italic>. [<xref rid="R13" ref-type="bibr">13</xref>] proposed a deep auto-context CNN that takes low-dose PET image and T1 MR image as input for prediction of standard-dose PET image. Similarly, Chen <italic>et el</italic>. [<xref rid="R21" ref-type="bibr">21</xref>] proposed to input low-dose PET images along with multi-contrast MR images into a UNet [<xref rid="R28" ref-type="bibr">28</xref>] for ultra-low-dose PET denoising. Cui <italic>et el</italic>. [<xref rid="R36" ref-type="bibr">36</xref>] suggested to use a UNet to iteratively predict the denoised PET from the CT image. Comparing to conventional PET denoising methods, all these deep learning based methods achieved superior denoising performance on static low-dose PET.</p>
    <p id="P4">However, none of the above mentioned studies addressed motion estimation and denoising in low-dose respiratory gated PET. Recently, our group proposed a Siamese Adversarial Network (SAN) to estimate the motion between pairs of low-dose gated images by first denoising the low-dose gated images and estimating the motion based on them [<xref rid="R37" ref-type="bibr">37</xref>]. One limitation of this approach is that the motion estimation network only considers pairs of gated images for registration and relies on high-quality denoised images of each gates, while disregarding the temporal information over the gated images. The temporal information containing respiratory motion patterns may be potentially helpful for motion estimation tasks. Therefore, it is desirable to develop a motion estimation algorithm that does not rely on denoised low-dose gated images and can directly estimate the motion from original low-dose gated images, while incorporating the temporal information among gates. With accurate motion estimation from low-dose gated images, we can register the low-dose gated images to a reference low-dose gated images and average all the aligned low-dose gated images to generate a motion-compensated PET image with preliminary denoising. This image can be fed into another deep network for further denoising. The general pipeline of the idea is illustrated in <xref rid="F1" ref-type="fig">Figure 1</xref>. In this work, we design a unified motion correction and denoising adversarial network for low-dose gated PET, called MDPET. As illustrated in <xref rid="F2" ref-type="fig">Figure 2</xref>, our MDPET is a unified network consisting of a Temporal Siamese Pyramid motion estimation network (TSP-Net), a denoising network, and a discriminator. Specifically, our TSP-Net consists of multiple shared-weights Siamese Pyramid Networks (SP-Net) and a bi-directional LSTM (<xref rid="F3" ref-type="fig">Figure 3</xref>). Each SP-Net predicts the transformation field between the source gated image and the reference gated image by utilizing the coarse-to-fine pyramid features from pairs of low-dose gated images. After registering all the source low-dose gated images with the reference low-dose gated image via Spatial Transformation Layers (STL) [<xref rid="R38" ref-type="bibr">38</xref>], the average image is fed into the denoising network for generation of our final motion-compensated denoised PET image. The network structure and training details are described in the following sections. The experimental results on human data demonstrate that our MDPET can accurately estimate the motion from low-dose gated images and generate high-quality motion-compensated PET images.</p>
  </sec>
  <sec id="S2">
    <label>II.</label>
    <title>Problem Formulation</title>
    <p id="P5">As illustrated in <xref rid="F1" ref-type="fig">Figure 1</xref>, assuming a phase gated PET scan generates 6 gates, we denote high-dose gated images and low-dose gated images as <italic>H</italic><sub><italic>n</italic></sub>, <inline-formula><mml:math display="inline" id="M5"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> with gate index of <italic>n</italic> ∈ {1, 2, 3, 4, 5, 6} and image size of <italic>h</italic> × <italic>w</italic> × <italic>d</italic>. Here, typical end-expiration gate 4 with the least intra-gate motion is used as our reference gate, and we denote <italic>H</italic><sub><italic>ref</italic></sub> = <italic>H</italic><sub>4</sub> and <italic>L</italic><sub><italic>ref</italic></sub> = <italic>L</italic><sub>4</sub>, respectively.</p>
    <p id="P6">First, our goal is to accurately estimate a set of transformation fields <italic>T</italic><sub><italic>n</italic></sub> between <italic>L</italic><sub><italic>ref</italic></sub> and <italic>L</italic><sub><italic>n</italic></sub> with <italic>n</italic> ∈ {1, 2, 3, 5, 6}. Denoting our motion estimation model as <inline-formula><mml:math display="inline" id="M6"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mtext> </mml:mtext><mml:mi>S</mml:mi><mml:mtext> </mml:mtext><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> parameterized by <italic>θ</italic><sub><italic>TSP</italic></sub>, the transformation fields can be described as:
<disp-formula id="FD1"><label>(1)</label><mml:math display="block" id="M7"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mtext> </mml:mtext><mml:mi>S</mml:mi><mml:mtext> </mml:mtext><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mtext> </mml:mtext><mml:mi>S</mml:mi><mml:mtext> </mml:mtext><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
    <p id="P7">Each transformation field <italic>T</italic><sub><italic>n</italic></sub> is used to deform the low-dose gated image <italic>L</italic><sub><italic>n</italic></sub> to generate an average image <italic>L</italic><sub><italic>avg</italic></sub>:
<disp-formula id="FD2"><label>(2)</label><mml:math display="block" id="M8"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>≠</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <italic>N</italic> = 6 for 6 gates in our experiments. Then, our goal is to denoise the motion-compensated low-dose averaged image and generate a high-quality final PET image. Denoting our denoising model as <inline-formula><mml:math display="inline" id="M9"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> parameterized by <italic>θ</italic><sub><italic>DN</italic></sub>, the denoised motion compensated average low-dose image is given by:
<disp-formula id="FD3"><label>(3)</label><mml:math display="block" id="M10"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
    <p id="P8">Our customized motion estimation model <inline-formula><mml:math display="inline" id="M11"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mi>T</mml:mi><mml:mtext> </mml:mtext><mml:mi>S</mml:mi><mml:mtext> </mml:mtext><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, denoising model <inline-formula><mml:math display="inline" id="M12"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and the unified training strategy are discussed in details in the following section.</p>
  </sec>
  <sec id="S3">
    <label>III.</label>
    <title>Methods</title>
    <sec id="S4">
      <label>A.</label>
      <title>Unified Motion Estimation and Denoising Adversarial Network</title>
      <p id="P9">The general pipeline of our unified motion estimation and denoising network (MDPET) is illustrated in <xref rid="F2" ref-type="fig">Figure 2</xref>. Our MDPET consists of a motion estimation module and a denoising module. The two modules are unified and trained in an end-to-end fashion.</p>
      <sec id="S5">
        <label>1)</label>
        <title>Motion Estimation Network:</title>
        <p id="P10">We build a Temporal Siamese Pyramid Network (TSP-Net) consisting of basic units of Siamese Pyramid Network (SP-Net) and a Bidirectional Convolutional Long Short Term Memory (BiConvLSTM) [<xref rid="R39" ref-type="bibr">39</xref>]. Each SP-Net is responsible for generating features for predicting the transformation between each source low-dose gated image <italic>L</italic><sub><italic>n</italic></sub> and the reference low-dose gated image <italic>L</italic><sub><italic>ref</italic></sub> with all SP-Nets share the same network parameters. Details of our SP-Net are provided in <xref rid="F3" ref-type="fig">Figure 3</xref>. In general, our SP-Net has two input branches for generating coarse-to-fine pyramid features of the reference low-dose gated image <italic>L</italic><sub><italic>ref</italic></sub> and the source low-dose gated images <italic>L</italic><sub><italic>n</italic></sub> separately. Then, the coarse-to-fine pyramid features are fed into our decoder for estimating transformation, similar to the image pyramid used in traditional image registration methods [<xref rid="R40" ref-type="bibr">40</xref>]. More specifically, we use two 3D UNet in each SP-Net for generating 5 levels of pyramid features with goals of learning coarse-to-fine features and denoising the input images for robust feature representations. To achieve these goals, the finest decoded feature maps from the source low-dose image <italic>L</italic><sub><italic>n</italic></sub> and the reference low-dose image <italic>L</italic><sub><italic>re</italic></sub><italic>f</italic> are passed through two 1-channel 3D convolutional layers, and the outputs <inline-formula><mml:math display="inline" id="M13"><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> are supervised by the high-dose gated images <italic>H</italic> with mean square error loss (MSE):
<disp-formula id="FD4"><label>(4)</label><mml:math display="block" id="M14"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD5"><label>(5)</label><mml:math display="block" id="M15"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>p</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD6"><label>(6)</label><mml:math display="block" id="M16"><mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>p</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
where <italic>p</italic> denotes the voxel location in the images. |<italic>H</italic>| is the number of voxel in each image. <italic>n</italic> is the index of the gates. <inline-formula><mml:math display="inline" id="M17"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math display="inline" id="M18"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>c</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are the losses for reference gated image branch and source gated image branch, respectively. As illustrated in <xref rid="F3" ref-type="fig">Figure 3</xref>, the pyramid feature maps from the UNet’s decoder successively recover the original high-dose signal from the low-dose signal, thus providing noise-reduced feature representations at different levels. Then, the coarse-to-fine pyramid features from the reference image and source image are successively fused together and decoded to generate features for predicting the transformation.</p>
        <p id="P11">While each SP-Net generates features for predicting the transformation between the reference low-dose image and one of the source low-dose gated images, the adjacent and non-adjacent SP-Net’s features can provide additional non-local information, such as motion pattern in a full respiratory cycle, which can be potentially helpful for accurate motion estimation over low-dose gated images. Recurrent convolutional neural network, such as BiConvLSTM, is able to learn the feature pattern among correlated data samples over time. The cell state of BiConvLSTM allows temporal feature from adjacent or non-adjacent frames to be transferred along forward and backward temporal directions. Therefore, we concatenate a 3D BiConvLSTM to the output features of the SP-Nets to allow the temporal feature exchange from different gate’s motion estimation features (TSP-Net). The output features with 32 channels, as shown in <xref rid="F2" ref-type="fig">Figure 2</xref>, are then fed into convolutional layers with 3 channels of output for predicting the transformation fields <italic>T</italic><sub><italic>n</italic></sub> over the gates.</p>
        <p id="P12">For each gate, the spatial transformation layer [<xref rid="R38" ref-type="bibr">38</xref>] transforms both the high-dose gated image <italic>H</italic><sub><italic>n</italic></sub> and the low-dose gated image <italic>L</italic><sub><italic>n</italic></sub> with the predicted transformation field <italic>T</italic><sub><italic>n</italic></sub> from the TSP-Net. The loss function for supervising the motion estimation here can be written as:
<disp-formula id="FD7"><label>(7)</label><mml:math display="block" id="M19"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>n</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>n</mml:mi></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="italic">sim</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="italic">smooth</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
with
<disp-formula id="FD8"><label>(8)</label><mml:math display="block" id="M20"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="italic">sim</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>p</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD9"><label>(9)</label><mml:math display="block" id="M21"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>p</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD10"><label>(10)</label><mml:math display="block" id="M22"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="italic">smooth</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>p</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mo>∇</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
where <italic>n</italic> is the index of the gates. <inline-formula><mml:math display="inline" id="M23"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the transformed <italic>H</italic><sub><italic>n</italic></sub> with transformation field <italic>T</italic><sub><italic>n</italic></sub>. <inline-formula><mml:math display="inline" id="M24"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="italic">sim</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the mean square error in image appearance, and <inline-formula><mml:math display="inline" id="M25"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:msub><mml:mrow><mml:mtext mathvariant="italic">smooth</mml:mtext></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a deformation regularization that adopts a L2-norm of the gradient of the transformation field <italic>T</italic><sub><italic>n</italic></sub> with a weighing term of <italic>λ</italic>. As suggested in [<xref rid="R9" ref-type="bibr">9</xref>], we empirically set <italic>λ</italic> = 0.01 in our experiments.</p>
      </sec>
      <sec id="S6">
        <label>2)</label>
        <title>Unified With Denoising Network:</title>
        <p id="P13">As mentioned above, the spatial transformation layer simultaneously transforms the low-dose gated image <italic>L</italic><sub><italic>n</italic></sub> with the predicted transformation field <italic>T</italic><sub><italic>n</italic></sub> from TSP-Net. Then, a motion-compensated low-dose gated image can be generated with:
<disp-formula id="FD11"><label>(11)</label><mml:math display="block" id="M26"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>≠</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>T</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∘</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>N</italic> = 6 for 6 gates setup in our experiments. While <inline-formula><mml:math display="inline" id="M27"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> with 6 fold counts can significantly reduce the low-dose image’s noise, we further reduce the image noise by feeding <inline-formula><mml:math display="inline" id="M28"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> to a denoising network. As UNet [<xref rid="R28" ref-type="bibr">28</xref>] has demonstrated outstanding performance in low-dose PET denoising [<xref rid="R15" ref-type="bibr">15</xref>], we adapt UNet as our denoising network in this work. However, our denoising network is not limited to UNet and can be substituted by other networks as well. The denoising loss can be formulated as:
<disp-formula id="FD12"><label>(12)</label><mml:math display="block" id="M29"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>p</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD13"><label>(13)</label><mml:math display="block" id="M30"><mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>p</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
where <italic>G</italic> is our denoising network and <inline-formula><mml:math display="inline" id="M31"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the denoised image generated from <inline-formula><mml:math display="inline" id="M32"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Moreover, we incorporate a patch discriminator <italic>D</italic> for adversarial learning on the denoising output [<xref rid="R34" ref-type="bibr">34</xref>]. To achieve stable adversarial training, we used the LSGAN adversarial loss [<xref rid="R41" ref-type="bibr">41</xref>] that can be formulated as:
<disp-formula id="FD14"><label>(14)</label><mml:math display="block" id="M33"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p>
        <p id="P14">Unifying the denoising network and the motion estimation network allows the denoising supervised gradient to back-propagate to the motion estimation network. As the denoising result relies on an accurate motion estimation over low-dose gated images and the alignment, the unified motion estimation and denoising adversarial network can be mutually beneficial. Therefore, the total loss for training our MDPET can be written as:
<disp-formula id="FD15"><label>(15)</label><mml:math display="block" id="M34"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext mathvariant="italic">tot</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mtext mathvariant="italic">reg</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext mathvariant="italic">reg</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>n</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>
where the weighting parameters are empirically set to <italic>λ</italic><sub><italic>DN</italic></sub> = 10, <italic>λ</italic><sub><italic>adv</italic></sub> = 1, <italic>λ</italic><sub><italic>reg</italic></sub> = 5, and <italic>λ</italic><sub><italic>SP</italic></sub> = 0.2 for a balance adversarial training.</p>
      </sec>
    </sec>
    <sec id="S7">
      <label>B.</label>
      <title>Evaluation on Human Data</title>
      <p id="P15">We included 28 pancreas <sup>18</sup>F-FPDTBZ [<xref rid="R42" ref-type="bibr">42</xref>] PET/CT studies. All PET data were obtained in list mode using the 4-ring Siemens Biograph mCT scanners located at the Yale PET Center. External respiratory motion was tracked using the AZ-733V respiratory gating system (Anzai Medical, Tokyo, Japan). The Anzai respiratory trace was recorded at 40 Hz for all subjects. The averaged dose administered to the patients is 9.13±1.37 mCi. Our patient dataset consists of 15 healthy patients and 13 Type-2 diabetic patients. All studies were approved by the Institutional Review Board and Radiation Safety Committee at Yale University. The total acquisition time was 120 mins for each study. We used phase gating to generate 6 gates for each study. To eliminate the mismatch between the attenuation correction (AC) map and the gated PET images, instead of using CT images to derive the AC-map, we utilized the maximum likelihood estimation of activity and attenuation (MLAA) [<xref rid="R43" ref-type="bibr">43</xref>] to generate AC-map for each gated volume to ensure phase-matched attenuation correction. The CT-derived AC-map was used as initial estimation for MLAA iterations. The high-dose images were reconstructed with 100% of the listmode data mimicking high radiation dose data with a large amount of tracer injection. Thus, each high-dose gated image was reconstructed with about 16.67% of the listmode data. The low-dose images were reconstructed with 1.5% of the listmode data with uniform sampling. Thus, each low-dose gated image was reconstructed with about 0.25% of the listmode data. Each dataset was reconstructed into a 400 × 400 × 109 volume with voxel size of 2.032 × 2.032 × 2.027 <italic>mm</italic><sup>3</sup> using ordered subset expectation maximization (OSEM) with 21 subsets and 1 iteration. The central 200 × 200 × 109 voxels were cropped to remove most voxels outside the human body contour. The resulted image was then resized to 96 × 96 × 96 voxels. The end expiration gate (typically Gate 4) was used as the reference gate since it shows minimum intra-gate motion.</p>
      <p id="P16">We performed four-fold cross validation with each fold consisting of 7 studies. During each validation, 21 studies were used for training and 7 studies were used for testing. The evaluation was performed on all 28 studies with 6 gated images in each study. For motion estimation evaluation, the transformation fields estimated from low-dose gated images were used to transform the corresponding high-dose gated images, and then the Normalized Mean Absolute Error (NMAE) were computed between the reference high-dose gated image and the transformed high-dose gated images. For comparative study, we compared our motion estimation results against VoxelMorph (VM) [<xref rid="R9" ref-type="bibr">9</xref>], the previously proposed Siamese Adversarial Network (SAN) [<xref rid="R37" ref-type="bibr">37</xref>], and a non-deep learning based Non-Rigid B-spline Registration (NRB) implemented in BioImage Suite [<xref rid="R40" ref-type="bibr">40</xref>]. VM is a deep learning based registration framework that exhibits top-performance in a wide range of medical imaging applications. With NRB, we used normalized mutual information as the similarity metric and we set the parameter of control point spacing to be 15mm, same as the optimized parameters demonstrated in [<xref rid="R7" ref-type="bibr">7</xref>]. For denoising evaluation, we computed the Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and NMAE between our final synthetic high-dose image and the reference high-dose gated image.</p>
    </sec>
    <sec id="S8">
      <label>C.</label>
      <title>Implementation Details</title>
      <p id="P17">We implemented our method using Pytorch [<xref rid="R44" ref-type="bibr">44</xref>]. We used the ADAM optimizer [<xref rid="R45" ref-type="bibr">45</xref>] with a learning rate of 10<sup>−4</sup>. We set the batch size to 1 with each training batch consisting of gated images from one patient. We first pre-trained the TSP-Net by setting <italic>λ</italic><sub><italic>DN</italic></sub> = <italic>λ</italic><sub><italic>adv</italic></sub> = 0. Then, we pre-trained the denoising network using the predicted averaged images from our pre-trained TSP-Net and its denoising ground-truth. Finally, the pre-trained TSP-Net and denoising network were loaded into MDPET to train in an end-to-end fashion. Our model was trained on an NVIDIA Quadro RTX 8000 GPU for 200 epochs. To prevent overfitting, we also implemented ‘on-the-fly’ data augmentation for all the training steps. During training, we first resized the image to 106 × 106 × 106 and performed 96 × 96 × 96 random cropping, and then randomly rotated the images along the z-axis with angle between −30 to 30 degrees.</p>
    </sec>
  </sec>
  <sec id="S9">
    <label>IV.</label>
    <title>Results</title>
    <sec id="S10">
      <label>A.</label>
      <title>Motion Estimation</title>
      <p id="P18">A sample set of low-dose gated PET images with and without applying the deformation fields predicted by our MDPET network is shown in <xref rid="F4" ref-type="fig">Figure 4</xref>. The corresponding averaged images are provided as well. To assist the evaluation, difference images between the reference gate and each source gate with and without applying the transformation fields were calculated using the corresponding high-dose gated images. As we can see from the first row of <xref rid="F4" ref-type="fig">Figure 4</xref>, the low-dose gated images with only 0.25% count level are noisy. Although directly averaging the low-dose gated images reduced the noise, important anatomical structure or pathological findings were blurred. As shown in the second row of <xref rid="F4" ref-type="fig">Figure 4</xref>, our MDPET can accurately predict and deform each low-dose gated image to the reference low-dose gated image (L4), leading to sharper anatomic boundaries in the averaged image. Moreover, without applying the predicted deformation fields, significant amounts of misalignment can be observed between the reference gate and Gate 1 / Gate 6 / Gate 2 due to the position difference between expiration and inspiration motion (<xref rid="F4" ref-type="fig">Figure 4</xref>, third row). The bright and dark intensity difference at the top and bottom of the kidney and liver indicated the error caused by the inter-gate motion. On the other hand, the position difference between the reference gate and Gate 3 / Gate 5 was small because the expiration phase is relatively long and steady. After applying the MDPET-predicted transformation fields, as illustrated in the fourth row, the differences in <italic>H</italic> were significantly reduced for the gates with large position difference. Specifically, the bright and dark errors at the top and bottom of the kidney and liver were reduced. The remaining differences were largely due to the different amount of intra-gate motion, which is larger for inspiration gates, i.e. Gated 1 / Gate 6 / Gate 2 in our experiments.</p>
      <p id="P19">The results of the proposed MDPET were compared with those of VM [<xref rid="R9" ref-type="bibr">9</xref>], NRB [<xref rid="R40" ref-type="bibr">40</xref>], and SAN [<xref rid="R37" ref-type="bibr">37</xref>]. Similar to the third and fourth rows of <xref rid="F4" ref-type="fig">Figure 4</xref>, we used the difference image between <italic>H</italic> with and without applying the deformation to visualize the motion estimation errors (<xref rid="F5" ref-type="fig">Figure 5</xref>). Two coronal slices containing different organs of interest are provided to assist the visual comparison. As we can see from the <xref rid="S9" ref-type="sec">results</xref> for Gate 1 and Gate 6 in which large motion displacement was observed, even though VM and NRB were able to reduce the position difference in the kidney, liver and pancreas, they introduced additional misalignments in the spine regions that should remain unmoved over the scan. From the results of Gate 3 with minimal motion displacement, VM and NRB introduced additional misalignments. On the other hand, our previously proposed method, SAN, was able to better align the kidney, liver, and pancreas with less misalignments in the spine region for Gates 1, 3 and 6. The MDPET network further reduced the small residual misalignment errors in SAN for all the gates, providing superior motion estimation results as compared to other methods (<xref rid="F5" ref-type="fig">Figure 5</xref>, bottom row).</p>
      <p id="P20">The quantitative results are summarized in <xref rid="T1" ref-type="table">Table I</xref>. Similar to the assessment in <xref rid="F5" ref-type="fig">Figure 5</xref>, we used the transformation field <italic>T</italic><sub><italic>n</italic></sub> estimated from low-dose gated images <italic>L</italic><sub><italic>n</italic></sub> to transform the corresponding high-dose gated images <italic>H</italic><sub><italic>n</italic></sub> to minimize the impact of noise on motion vector evaluation, and calculated the NMAE between the reference high-dose gated image <italic>H</italic><sub><italic>ref</italic></sub> and the transformed high-dose gated images <inline-formula><mml:math display="inline" id="M35"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>H</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. For Gate 1 and Gate 6 with large intra-gate motion, our MDPET was able to significantly reduce the NMAE from 0.185 to 0.110 for Gate 1 and from 0.136 to 0.091 for Gate 6, demonstrating superior motion estimation performance than SAN, VM and NRB. For gates with small or no intra-gate motion, such as Gate 3, our MDPET could maintain the overall alignment and finely adjust the small misalignment in local regions. Thus, we observed small NMAE reduction for Gate 2, Gate 3 and Gate 5 when using our MDPET. In contrast, NRB and VM both led to degradation of NMAE for Gate 2, Gate 3, and Gate 5. The results were even worse than those without applying motion estimation. For example, NRB increased NMAE from 0.065 to 0.121 at gate 3. Previous methods of VM and NRB are limited for accurate registration in the low-dose gated images, and our MDPET can generate reasonable registration across all the gates. The run time analysis is summarized in the last two columns of <xref rid="T1" ref-type="table">Table I</xref>. NRB with iterative optimization required the longest run time, about 1489 seconds on average using CPU. On the other hand, deep learning based VM and SAN could directly infer the transformation once the models are trained, thus requiring much shorter run time on CPU or GPU. Unlike VM and SAN that required 5 times of inference over different gates, our MDPET used all the gated images at once, thus further reducing the GPU run time to 0.54 seconds on average.</p>
      <p id="P21">We also performed ablation study on motion estimation for our MDPET. The results are summarized in <xref rid="T2" ref-type="table">Table II</xref>. As we can see, the BiConvLSTM in our TSP-Net could improve the motion estimation performance. The performance was slightly further boosted by the additional adversarial learning. However, adding BiConvLSTM slightly increased the GPU run time from 0.38 seconds to 0.54 seconds.</p>
    </sec>
    <sec id="S11">
      <label>B.</label>
      <title>Denoising Different Motion-Compensated Images</title>
      <p id="P22">After motion prediction, the averaged image of the transformed low-dose gated images was inputted into the denoising network to further reduce the noise. In <xref rid="F6" ref-type="fig">Figure 6</xref>, we compared our MDPET results with other two-stage processing methods, including UNet denoising on the averaged image based on the NRB-derived transformation fields (NRB+UNet), UNet denoising on the averaged image based on the VM-predicted transformation fields (VM+UNet), and UNet denoising on the averaged image based on the SAN-predicted transformation fields (SAN+UNet). In NRB+UNet, the UNet was independently trained with paired motion-compensated averaged images from NRB and the ground truth high-dose image. The same UNet training protocol was used in VM+UNet and SAN+UNet. As observed in the figure, NRB+UNet and VM+UNet could reduce the global noise level. Subtle anatomic details, such as liver veins, were hard to observe for these two methods given the signal could have already been blurred out by motion in the input averaged image. On the other hand, in addition to reducing the global noise level, both SAN+UNet and our MDPET can better preserve anatomical details in the final image by efficiently reducing the motion blurring in the input averaged image. Our MDPET can generate anatomic details that best match with the ground-truth in terms of shape and intensity.</p>
      <p id="P23">The quantitative results are summarized in <xref rid="T3" ref-type="table">Table III</xref>. In addition to UNet, we also explored the application of GAN with the same UNet generator in the two-stage methods, since adversarial learning is also implemented in our MDPET. Therefore, the quantitative results of our MDPET were compared not only with those of NRB+UNet / VM+UNet / SAN+UNet, but also with those of NRB+GAN / VM+GAN / SAN+GAN. As we can see, the evaluated image quality metrics were slightly improved while applying any of the two-stage processing methods, regardless of the incorporated image denoising network. The two-stage processing methods can reduce the NMAE from 0.17 to about 0.08. However, in the two-stage processing methods, changing the denoising network from UNet to GAN does not lead to significant improvements. On the other hand, our MDPET unifying motion estimation and denoising demonstrated the superior performance with mean NMAE=0.088, SSIM=0.966, and PSNR=32.28. Note that the image quality metrics for our MEPET’s averaged image (✓Ours+✗DN) were worse than those for NRB’s averaged image (✓NRB+✗DN) and VM’s averaged image (✓VM+✗DN). However, the denoising results based on our MDPET’s averaged image demonstrated the best performance. This is caused by the fact that NRB and VM register the image merely based on the image appearance, including anatomical structure and noise. Registering the noise will result in smoother averaged image, thus generating better image quality metrics for NRB and VM. Our MDPET registration can mitigate the impact from noise, thus providing averaged image with better anatomic details for denoising. The boxplot of our comparison results along with statistical analysis are summarized in <xref rid="F7" ref-type="fig">Figure 7</xref>.</p>
      <p id="P24">We also performed ablation study on denoising for our MDPET. The results are summarized in <xref rid="T4" ref-type="table">Table IV</xref>. According to <xref rid="T2" ref-type="table">Table II</xref> in the previous section, incorporating BiConvLSTM could improve the motion estimation performance thus generating sharper averaged image for denoising. Therefore, as we can observe from <xref rid="T4" ref-type="table">Table IV</xref>, adding BiConvLSTM could produce better image quality over the baseline MDPET. Moreover, adding adversarial learning could further improve the denoising performance. Three human subjects are illustrated in <xref rid="F8" ref-type="fig">Figure 8</xref>. Overall, our MDPET with both BiConvLSTM and adversarial learning achieved the best motion estimation and denoising performance.</p>
    </sec>
  </sec>
  <sec id="S12">
    <label>V.</label>
    <title>Discussion and Conclusion</title>
    <p id="P25">In this work, we proposed a unified motion estimation and denoising adversarial network, called MDPET, for generating motion-compensated low-noise PET image from low-dose respiratory gated PET. First, we developed a motion estimation module, TSP-Net, that can reliably estimate the motion from the low-dose gated images, which also incorporates the temporal motion features to improve the motion estimation. The basic unit of SP-Net in TSP-Net utilizes the denoised coarse-to-fine pyramid features to generate the motion features for each gate. Our TSP-Net then takes the motion features from each SP-Net into a recurrent layer to learn the temporal motion relationship over the gates, thus generating accurate motion estimation for all gates at once. Second, we unify the motion estimation network with a denoising network to directly generate motion-compensated low-noise PET images. Specifically, the gated images are deformed using the transformation fields predicted by TSP-Net and averaged such that all the counts in low-dose scan can be utilized to reduce the noise. Then, the averaged image is fed into a denoising network to further reduce the noise. A discriminator is added to the denoising output to enable adversarial learning for both motion estimation and denoising in our MDPET.</p>
    <p id="P26">We demonstrated successful application on low-dose respiratory gated PET with evaluations on both motion estimation and denoising. For motion estimation, we compared with other previous motion estimation methods, including NRB, VM, and SAN. NRB and VM are not robust to noise in the low-dose gated images, thus leading to significant increases in registration errors in Gate 2 / Gate 3 / Gate 5, as illustrated in <xref rid="T1" ref-type="table">Table I</xref>. SAN with denoising first then motion estimation leads to better motion estimation as the noise in the low-dose gated images was first suppressed. However, SAN requires two-steps processing and requires 5 times inference for each study. On the other hand, our MDPET was able to generate superior motion estimation over all respiratory gates with the shortest inference time of 0.5 seconds. Ablation studies also demonstrated that adding the recurrent layer for temporal motion feature learning allows our MDPET to generate better motion estimation. For denoising, we compared our end-to-end denoising output with conventional two-stage processing methods, i.e. motion estimation then denoising. Because the motion estimation of NRB and VM are prone to error due to high noise level in the low-dose gated images, their averaged image may have already suffered from residual motion blurring and the denoising network cannot recover the motion blurred signals. The denoising results from SAN’s averaged images are more reasonable as SAN can better align the low-dose gated images. However, the motion estimation and final denoising are in two separate stages. The denoising network trained separately may not be able to correct the residual motion blurring in the averaged image. In this case, our MDPET is an end-to-end framework and the denoising output based on our motion-compensated averaged image provides the best reconstructed image quality with PSNR = 32.28.</p>
    <p id="P27">The presented work also has potential limitations. First of all, the denoising result is still not as distinct as the ground truth from high-dose gated image. In our current MDPET implementation, we use UNet as our denoising network because its efficiency has been extensively studied and demonstrated in literature [<xref rid="R15" ref-type="bibr">15</xref>]. However, the denoising network in our MDPET is interchangeable with other advanced denoising networks [<xref rid="R13" ref-type="bibr">13</xref>], [<xref rid="R17" ref-type="bibr">17</xref>], [<xref rid="R46" ref-type="bibr">46</xref>], [<xref rid="R47" ref-type="bibr">47</xref>] to potentially further improve the image quality. Moreover, perceptual loss [<xref rid="R17" ref-type="bibr">17</xref>] could also be incorporated into the MDPET to help further recover the image details. However, perceptual loss is currently only available for 2D image but not 3D imaging data as in our work. In addition, more patient data could be collected for training our MDPET in the future for further improving the performance. Secondly, our work only addressed the inter-gate motion (motion between gates) but not the intra-gate motion (motion within each gate) for low-dose gated PET. The gated images may already suffer from intra-gate motion blurring, potentially affecting our inter-gate motion estimation and the subsequent denoising. Although we have chosen the end-expiration gate image with the least intra-gate motion as the ground truth for supervising the MDPET’s output to mitigate the impact, future work could also consider event-by-event listmode based correction to further limit the amount of intra-gate motion in each gate. Finally, current image reconstructions were based OSEM with 1 iteration. Additional iteration numbers and filtering settings need to be investigated in our future work.</p>
    <p id="P28">Our MDPET also suggests several potential clinical applications for our future studies. First of all, since MDPET could generate high-quality motion compensated PET image under low-dose injection protocol, our generated image is potentially useful for diagnosis purposes, especially for abdominal regions where respiratory motion is inevitable. Second, our MDPET is also potentially useful for registering continuous bed motion (CBM) multi-pass for whole body dynamic PET. To elaborate, each CBM pass is scanned with a short time period (2–5 min) that contains a high noise level, similar to low-dose gated PET. The respiratory motion is inevitable in a CBM acquisition. Thus, our method can potentially apply to CBM inter-pass and intra-pass motion correction. Lastly, our method could potentially be adapted to deviceless low-dose gating reconstruction as well.</p>
    <p id="P29">In conclusion, we proposed a unified motion estimation and denoising adversarial network for low-dose gated PET. The experimental results using human data show that our MDPET can accurately estimate the motion over the noisy low-dose gated images and simultaneously produce high-quality motion-compensated denoised PET image. Future work would also investigate the potential of further improving the performance of MDPET by substituting our current MDPET framework with different state-of-the-art motion estimation and denoising sub-networks on different applications.</p>
  </sec>
</body>
<back>
  <ack id="S13">
    <p id="P30">This work was supported by the National Institutes of Health (NIH) under Grant R01CA224140 and Grant R01EB025468.</p>
    <p id="P31">This work involved human subjects or animals in its research. Approval of all ethical and experimental procedures and protocols was granted by Yale Institutional Review board (HIC # 1309012692).</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <label>[1]</label>
      <mixed-citation publication-type="journal"><name><surname>Strauss</surname><given-names>KJ</given-names></name> and <name><surname>Kaste</surname><given-names>SC</given-names></name>, “<article-title>The ALARA (as low as reasonably achievable) concept in pediatric interventional and fluoroscopic imaging: Striving to keep radiation doses as low as possible during fluoroscopy of pediatric patients—A white paper executive summary</article-title>,” <source>Radiology</source>, vol. <volume>240</volume>, no. <issue>3</issue>, pp. <fpage>621</fpage>–<lpage>622</lpage>, <year>2006</year>.<pub-id pub-id-type="pmid">16926319</pub-id></mixed-citation>
    </ref>
    <ref id="R2">
      <label>[2]</label>
      <mixed-citation publication-type="journal"><name><surname>Callahan</surname><given-names>J</given-names></name><etal/>, “<article-title>Geographic miss of lung tumours due to respiratory motion: A comparison of 3D vs 4D PET/CT defined target volumes</article-title>,” <source>Radiat. Oncol</source>, vol. <volume>9</volume>, no. <issue>1</issue>, p. <fpage>291</fpage>, <month>12</month>. <year>2014</year>.<pub-id pub-id-type="pmid">25511904</pub-id></mixed-citation>
    </ref>
    <ref id="R3">
      <label>[3]</label>
      <mixed-citation publication-type="journal"><name><surname>Van Der Gucht</surname><given-names>A</given-names></name>, <name><surname>Serrano</surname><given-names>B</given-names></name>, <name><surname>Hugonnet</surname><given-names>F</given-names></name>, <name><surname>Paulmier</surname><given-names>B</given-names></name>, <name><surname>Garnier</surname><given-names>N</given-names></name>, and <name><surname>Faraggi</surname><given-names>M</given-names></name>, “<article-title>Impact of a new respiratory amplitude-based gating technique in evaluation of upper abdominal PET lesions</article-title>,” <source>Eur. J. Radiol</source>, vol. <volume>83</volume>, no. <issue>3</issue>, pp. <fpage>509</fpage>–<lpage>515</lpage>, <month>3</month>. <year>2014</year>.<pub-id pub-id-type="pmid">24332351</pub-id></mixed-citation>
    </ref>
    <ref id="R4">
      <label>[4]</label>
      <mixed-citation publication-type="journal"><name><surname>Dawood</surname><given-names>M</given-names></name>, <name><surname>Buther</surname><given-names>F</given-names></name>, <name><surname>Jiang</surname><given-names>X</given-names></name>, and <name><surname>Schafers</surname><given-names>KP</given-names></name>, “<article-title>Respiratory motion correction in 3-D PET data with advanced optical flow algorithms</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>27</volume>, no. <issue>8</issue>, pp. <fpage>1164</fpage>–<lpage>1175</lpage>, <month>8</month>. <year>2008</year>.</mixed-citation>
    </ref>
    <ref id="R5">
      <label>[5]</label>
      <mixed-citation publication-type="journal"><name><surname>Dawood</surname><given-names>M</given-names></name>, <name><surname>Lang</surname><given-names>N</given-names></name>, <name><surname>Jiang</surname><given-names>X</given-names></name>, and <name><surname>Schafers</surname><given-names>KP</given-names></name>, “<article-title>Lung motion correction on respiratory gated 3-D PET/CT images</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>25</volume>, no. <issue>4</issue>, pp. <fpage>476</fpage>–<lpage>485</lpage>, <month>4</month>. <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="R6">
      <label>[6]</label>
      <mixed-citation publication-type="journal"><name><surname>Chan</surname><given-names>C</given-names></name><etal/>, “<article-title>Non-rigid event-by-event continuous respiratory motion compensated list-mode reconstruction for PET</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>37</volume>, no. <issue>2</issue>, pp. <fpage>504</fpage>–<lpage>515</lpage>, <month>2</month>. <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R7">
      <label>[7]</label>
      <mixed-citation publication-type="journal"><name><surname>Lu</surname><given-names>Y</given-names></name><etal/>, “<article-title>Respiratory motion compensation for PET/CT with motion information derived from matched attenuation-corrected gated PET data</article-title>,” <source>J. Nucl. Med</source>, vol. <volume>59</volume>, no. <issue>9</issue>, pp. <fpage>1480</fpage>–<lpage>1486</lpage>, <month>9</month>. <year>2018</year>.<pub-id pub-id-type="pmid">29439015</pub-id></mixed-citation>
    </ref>
    <ref id="R8">
      <label>[8]</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>T</given-names></name>, <name><surname>Zhang</surname><given-names>M</given-names></name>, <name><surname>Qi</surname><given-names>W</given-names></name>, <name><surname>Asma</surname><given-names>E</given-names></name>, and <name><surname>Qi</surname><given-names>J</given-names></name>, “<article-title>Motion correction of respiratory-gated PET images using deep learning based image registration framework</article-title>,” <source>Phys. Med. Biol</source>, vol. <volume>65</volume>, no. <issue>15</issue>, <month>7</month>. <year>2020</year>, Art. no. <fpage>155003</fpage>.<pub-id pub-id-type="pmid">32244230</pub-id></mixed-citation>
    </ref>
    <ref id="R9">
      <label>[9]</label>
      <mixed-citation publication-type="journal"><name><surname>Balakrishnan</surname><given-names>G</given-names></name>, <name><surname>Zhao</surname><given-names>A</given-names></name>, <name><surname>Sabuncu</surname><given-names>MR</given-names></name>, <name><surname>Guttag</surname><given-names>J</given-names></name>, and <name><surname>Dalca</surname><given-names>AV</given-names></name>, “<article-title>VoxelMorph: A learning framework for deformable medical image registration</article-title>,” <source>IEEE Trans. Med. Imag</source>, vol. <volume>38</volume>, no. <issue>8</issue>, pp. <fpage>1788</fpage>–<lpage>1800</lpage>, <month>8</month>. <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R10">
      <label>[10]</label>
      <mixed-citation publication-type="journal"><name><surname>Dutta</surname><given-names>J</given-names></name>, <name><surname>Leahy</surname><given-names>RM</given-names></name>, and <name><surname>Li</surname><given-names>Q</given-names></name>, “<article-title>Non-local means denoising of dynamic PET images</article-title>,” <source>PLoS ONE</source>, vol. <volume>8</volume>, no. <issue>12</issue>, <month>12</month>. <year>2013</year>, Art. no. <fpage>e81390</fpage>.<pub-id pub-id-type="pmid">24339921</pub-id></mixed-citation>
    </ref>
    <ref id="R11">
      <label>[11]</label>
      <mixed-citation publication-type="journal"><name><surname>Maggioni</surname><given-names>M</given-names></name>, <name><surname>Katkovnik</surname><given-names>V</given-names></name>, <name><surname>Egiazarian</surname><given-names>K</given-names></name>, and <name><surname>Foi</surname><given-names>A</given-names></name>, “<article-title>Nonlocal transform-domain filter for volumetric data denoising and reconstruction</article-title>,” <source>IEEE Trans. Image Process</source>, vol. <volume>22</volume>, no. <issue>1</issue>, pp. <fpage>119</fpage>–<lpage>133</lpage>, <month>1</month>. <year>2013</year>.<pub-id pub-id-type="pmid">22868570</pub-id></mixed-citation>
    </ref>
    <ref id="R12">
      <label>[12]</label>
      <mixed-citation publication-type="journal"><name><surname>Mejia</surname><given-names>J</given-names></name>, <name><surname>Mederos</surname><given-names>B</given-names></name>, <name><surname>Mollineda</surname><given-names>RA</given-names></name>, and <name><surname>Maynez</surname><given-names>LO</given-names></name>, “<article-title>Noise reduction in small animal PET images using a variational non-convex functional</article-title>,” <source>IEEE Trans. Nucl. Sci</source>, vol. <volume>63</volume>, no. <issue>5</issue>, pp. <fpage>2577</fpage>–<lpage>2585</lpage>, <month>10</month>. <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="R13">
      <label>[13]</label>
      <mixed-citation publication-type="journal"><name><surname>Xiang</surname><given-names>L</given-names></name><etal/>, “<article-title>Deep auto-context convolutional neural networks for standard-dose PET image estimation from low-dose PET/MRI</article-title>,” <source>Neurocomputing</source>, vol. <volume>267</volume>, pp. <fpage>406</fpage>–<lpage>416</lpage>, <month>12</month>. <year>2017</year>.<pub-id pub-id-type="pmid">29217875</pub-id></mixed-citation>
    </ref>
    <ref id="R14">
      <label>[14]</label>
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>Y</given-names></name><etal/>, “<article-title>3D conditional generative adversarial networks for high-quality PET image estimation at low dose</article-title>,” <source>NeuroImage</source>, vol. <volume>174</volume>, pp. <fpage>550</fpage>–<lpage>562</lpage>, <month>7</month>. <year>2018</year>.<pub-id pub-id-type="pmid">29571715</pub-id></mixed-citation>
    </ref>
    <ref id="R15">
      <label>[15]</label>
      <mixed-citation publication-type="journal"><name><surname>Lu</surname><given-names>W</given-names></name><etal/>, “<article-title>An investigation of quantitative accuracy for deep learning based denoising in oncological PET</article-title>,” <source>Phys. Med. Biol</source>, vol. <volume>64</volume>, no. <issue>16</issue>, <month>8</month>. <year>2019</year>, Art. no. <fpage>165019</fpage>.<pub-id pub-id-type="pmid">31307019</pub-id></mixed-citation>
    </ref>
    <ref id="R16">
      <label>[16]</label>
      <mixed-citation publication-type="journal"><name><surname>Kaplan</surname><given-names>S</given-names></name> and <name><surname>Zhu</surname><given-names>Y-M</given-names></name>, “<article-title>Full-dose PET image estimation from low-dose PET image using deep learning: A pilot study</article-title>,” <source>J. Digit. Imag</source>, vol. <volume>32</volume>, no. <issue>5</issue>, pp. <fpage>773</fpage>–<lpage>778</lpage>, <month>10</month>. <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R17">
      <label>[17]</label>
      <mixed-citation publication-type="journal"><name><surname>Hu</surname><given-names>Z</given-names></name><etal/>, “<article-title>DPIR-Net: Direct PET image reconstruction based on the Wasserstein generative adversarial network</article-title>,” <source>IEEE Trans. Radiat. Plasma Med. Sci</source>, vol. <volume>5</volume>, no. <issue>1</issue>, pp. <fpage>35</fpage>–<lpage>43</lpage>, <month>1</month>. <year>2021</year>.</mixed-citation>
    </ref>
    <ref id="R18">
      <label>[18]</label>
      <mixed-citation publication-type="journal"><name><surname>Gong</surname><given-names>Y</given-names></name><etal/>, “<article-title>Parameter-transferred Wasserstein generative adversarial network (PT-WGAN) for low-dose PET image denoising</article-title>,” <source>IEEE Trans. Radiat. Plasma Med. Sci</source>, vol. <volume>5</volume>, no. <issue>2</issue>, pp. <fpage>213</fpage>–<lpage>223</lpage>, <month>3</month>. <year>2021</year>.</mixed-citation>
    </ref>
    <ref id="R19">
      <label>[19]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>L</given-names></name>, <name><surname>Schaefferkoetter</surname><given-names>JD</given-names></name>, <name><surname>Tham</surname><given-names>IWK</given-names></name>, <name><surname>Huang</surname><given-names>G</given-names></name>, and <name><surname>Yan</surname><given-names>J</given-names></name>, “<article-title>Supervised learning with cyclegan for low-dose FDG PET image denoising</article-title>,” <source>Med. Image Anal</source>, vol. <volume>65</volume>, <month>10</month>. <year>2020</year>, Art. no. <fpage>101770</fpage>.<pub-id pub-id-type="pmid">32674043</pub-id></mixed-citation>
    </ref>
    <ref id="R20">
      <label>[20]</label>
      <mixed-citation publication-type="journal"><name><surname>Ouyang</surname><given-names>J</given-names></name>, <name><surname>Chen</surname><given-names>KT</given-names></name>, <name><surname>Gong</surname><given-names>E</given-names></name>, <name><surname>Pauly</surname><given-names>J</given-names></name>, and <name><surname>Zaharchuk</surname><given-names>G</given-names></name>, “<article-title>Ultra-low-dose PET reconstruction using generative adversarial network with feature matching and task-specific perceptual loss</article-title>,” <source>Med. Phys</source>, vol. <volume>46</volume>, no. <issue>8</issue>, pp. <fpage>3555</fpage>–<lpage>3564</lpage>, <month>8</month>. <year>2019</year>.<pub-id pub-id-type="pmid">31131901</pub-id></mixed-citation>
    </ref>
    <ref id="R21">
      <label>[21]</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>KT</given-names></name><etal/>, “<article-title>Ultra–low-dose <sup>18</sup>F-florbetaben amyloid pet imaging using deep learning with multi-contrast MRI inputs</article-title>,” <source>Radiology</source>, vol. <volume>290</volume>, no. <issue>3</issue>, pp. <fpage>649</fpage>–<lpage>656</lpage>, <year>2019</year>.<pub-id pub-id-type="pmid">30526350</pub-id></mixed-citation>
    </ref>
    <ref id="R22">
      <label>[22]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>H</given-names></name>, <name><surname>Wu</surname><given-names>J</given-names></name>, <name><surname>Lu</surname><given-names>W</given-names></name>, <name><surname>Onofrey</surname><given-names>JA</given-names></name>, <name><surname>Liu</surname><given-names>Y-H</given-names></name>, and <name><surname>Liu</surname><given-names>C</given-names></name>, “<article-title>Noise reduction with cross-tracer and cross-protocol deep transfer learning for low-dose PET</article-title>,” <source>Phys. Med. Biol</source>, vol. <volume>65</volume>, no. <issue>18</issue>, <month>9</month>. <year>2020</year>, Art. no. <fpage>185006</fpage>.<pub-id pub-id-type="pmid">32924973</pub-id></mixed-citation>
    </ref>
    <ref id="R23">
      <label>[23]</label>
      <mixed-citation publication-type="journal"><name><surname>Reader</surname><given-names>AJ</given-names></name>, <name><surname>Corda</surname><given-names>G</given-names></name>, <name><surname>Mehranian</surname><given-names>A</given-names></name>, <name><surname>da Costa-Luis</surname><given-names>C</given-names></name>, <name><surname>Ellis</surname><given-names>S</given-names></name>, and <name><surname>Schnabel</surname><given-names>JA</given-names></name>, “<article-title>Deep learning for PET image reconstruction</article-title>,” <source>IEEE Trans. Radiat. Plasma Med. Sci</source>, vol. <volume>5</volume>, no. <issue>1</issue>, pp. <fpage>1</fpage>–<lpage>25</lpage>, <month>1</month>. <year>2021</year>.</mixed-citation>
    </ref>
    <ref id="R24">
      <label>[24]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>F</given-names></name>, <name><surname>Jang</surname><given-names>H</given-names></name>, <name><surname>Kijowski</surname><given-names>R</given-names></name>, <name><surname>Bradshaw</surname><given-names>T</given-names></name>, and <name><surname>Mcmillan</surname><given-names>AB</given-names></name>, “<article-title>Deep learning MR imaging–based attenuation correction for PET/MR imaging</article-title>,” <source>Radiology</source>, vol. <volume>286</volume>, no. <issue>2</issue>, pp. <fpage>676</fpage>–<lpage>684</lpage>, <month>2</month>. <year>2018</year>.<pub-id pub-id-type="pmid">28925823</pub-id></mixed-citation>
    </ref>
    <ref id="R25">
      <label>[25]</label>
      <mixed-citation publication-type="book"><name><surname>Zhou</surname><given-names>B</given-names></name>, <name><surname>Lin</surname><given-names>X</given-names></name>, <name><surname>Eck</surname><given-names>B</given-names></name>, <name><surname>Hou</surname><given-names>J</given-names></name>, and <name><surname>Wilson</surname><given-names>D</given-names></name>, “<part-title>Generation of virtual dual energy images from standard single-shot radiographs using multi-scale and conditional adversarial network</part-title>,” in <source>Proc. Asian Conf. Comput. Vis</source>
<publisher-loc>Perth, WA, Australia</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2018</year>, pp. <fpage>298</fpage>–<lpage>313</lpage>.</mixed-citation>
    </ref>
    <ref id="R26">
      <label>[26]</label>
      <mixed-citation publication-type="book"><name><surname>Zhou</surname><given-names>B</given-names></name>, <name><surname>Lin</surname><given-names>X</given-names></name>, and <name><surname>Eck</surname><given-names>B</given-names></name>, “<part-title>Limited angle tomography reconstruction: Synthetic reconstruction via unsupervised sinogram adaptation</part-title>,” in <source>Proc. Int. Conf. Inf. Process. Med. Imag</source>
<publisher-name>Springer</publisher-name>, <year>2019</year>, pp. <fpage>141</fpage>–<lpage>152</lpage>.</mixed-citation>
    </ref>
    <ref id="R27">
      <label>[27]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>B</given-names></name>, <name><surname>Kevin Zhou</surname><given-names>S</given-names></name>, <name><surname>Duncan</surname><given-names>JS</given-names></name>, and <name><surname>Liu</surname><given-names>C</given-names></name>, “<article-title>Limited view tomographic reconstruction using a cascaded residual dense spatial-channel attention network with projection data fidelity layer</article-title>,” <source>IEEE Trans. Med. Imag</source>, <comment>early access</comment>, <month>3</month>. <day>17</day>, <year>2021</year>, doi: <pub-id pub-id-type="doi">10.1109/TMI.2021.3066318</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R28">
      <label>[28]</label>
      <mixed-citation publication-type="book"><name><surname>Ronneberger</surname><given-names>O</given-names></name>, <name><surname>Fischer</surname><given-names>P</given-names></name>, and <name><surname>Brox</surname><given-names>T</given-names></name>, “<part-title>U-Net: Convolutional networks for biomedical image segmentation</part-title>,” in <source>Proc. Int. Conf. Med. Image Comput. Comput.-Assist. Intervent</source>
<publisher-loc>Munich, Germany</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2015</year>, pp. <fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="R29">
      <label>[29]</label>
      <mixed-citation publication-type="confproc"><name><surname>Zhou</surname><given-names>B</given-names></name>, <name><surname>Crawford</surname><given-names>R</given-names></name>, <name><surname>Dogdas</surname><given-names>B</given-names></name>, <name><surname>Goldmacher</surname><given-names>G</given-names></name>, and <name><surname>Chen</surname><given-names>A</given-names></name>, “<article-title>A progressively-trained scale-invariant and boundary-aware deep neural network for the automatic 3D segmentation of lung lesions</article-title>,” in <conf-name>Proc. IEEE Winter Conf. Appl. Comput. Vis. (WACV)</conf-name>, <month>1</month>. <year>2019</year>, pp. <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="R30">
      <label>[30]</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>A</given-names></name><etal/>, “<article-title>A deep learning-facilitated radiomics solution for the prediction of lung lesion shrinkage in non-small cell lung cancer trials</article-title>,” in <source>Proc. IEEE 17th Int. Symp. Biomed. Imag. (ISBI)</source>, <month>4</month>. <year>2020</year>, pp. <fpage>678</fpage>–<lpage>682</lpage>.</mixed-citation>
    </ref>
    <ref id="R31">
      <label>[31]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>B</given-names></name>, <name><surname>Augenfeld</surname><given-names>Z</given-names></name>, <name><surname>Chapiro</surname><given-names>J</given-names></name>, <name><surname>Zhou</surname><given-names>SK</given-names></name>, <name><surname>Liu</surname><given-names>C</given-names></name>, and <name><surname>Duncan</surname><given-names>JS</given-names></name>, “<article-title>Anatomy-guided multimodal registration by learning segmentation without ground truth: Application to intraprocedural CBCT/MR liver segmentation and registration</article-title>,” <source>Med. Image Anal</source>, vol. <volume>71</volume>, <month>7</month>. <year>2021</year>, Art. no. <fpage>102041</fpage>.<pub-id pub-id-type="pmid">33823397</pub-id></mixed-citation>
    </ref>
    <ref id="R32">
      <label>[32]</label>
      <mixed-citation publication-type="journal"><name><surname>Shan</surname><given-names>H</given-names></name><etal/>, “<article-title>Competitive performance of a modularized deep neural network compared to commercial algorithms for low-dose CT image reconstruction</article-title>,” <source>Nature Mach. Intell</source>, vol. <volume>1</volume>, no. <issue>6</issue>, pp. <fpage>269</fpage>–<lpage>276</lpage>, <month>6</month>. <year>2019</year>.<pub-id pub-id-type="pmid">33244514</pub-id></mixed-citation>
    </ref>
    <ref id="R33">
      <label>[33]</label>
      <mixed-citation publication-type="journal"><name><surname>Goodfellow</surname><given-names>I</given-names></name><etal/>, “<article-title>Generative adversarial nets</article-title>,” in <source>Proc. Adv. Neural Inf. Process. Syst</source>, <year>2014</year>, pp. <fpage>2672</fpage>–<lpage>2680</lpage>.</mixed-citation>
    </ref>
    <ref id="R34">
      <label>[34]</label>
      <mixed-citation publication-type="confproc"><name><surname>Isola</surname><given-names>P</given-names></name>, <name><surname>Zhu</surname><given-names>J-Y</given-names></name>, <name><surname>Zhou</surname><given-names>T</given-names></name>, and <name><surname>Efros</surname><given-names>AA</given-names></name>, “<article-title>Image-to-image translation with conditional adversarial networks</article-title>,” in <conf-name>Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</conf-name>, <month>7</month>. <year>2017</year>, pp. <fpage>1125</fpage>–<lpage>1134</lpage>.</mixed-citation>
    </ref>
    <ref id="R35">
      <label>[35]</label>
      <mixed-citation publication-type="journal"><name><surname>Arjovsky</surname><given-names>M</given-names></name>, <name><surname>Chintala</surname><given-names>S</given-names></name>, and <name><surname>Bottou</surname><given-names>L</given-names></name>, “<article-title>Wasserstein GAN</article-title>,” <year>2017</year>, <source>arXiv:1701.07875</source>. <comment>[Online]. Available: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1701.07875">http://arxiv.org/abs/1701.07875</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R36">
      <label>[36]</label>
      <mixed-citation publication-type="journal"><name><surname>Cui</surname><given-names>J</given-names></name><etal/>, “<article-title>PET image denoising using unsupervised deep learning</article-title>,” <source>Eur. J. Nucl. Med. Mol. Imag</source>, vol. <volume>46</volume>, no. <issue>13</issue>, pp. <fpage>2780</fpage>–<lpage>2789</lpage>, <month>12</month>. <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="R37">
      <label>[37]</label>
      <mixed-citation publication-type="book"><name><surname>Zhou</surname><given-names>B</given-names></name>, <name><surname>Tsai</surname><given-names>Y-J</given-names></name>, and <name><surname>Liu</surname><given-names>C</given-names></name>, “<part-title>Simultaneous denoising and motion estimation for low-dose gated pet using a Siamese adversarial network with gate-to-gate consistency learning</part-title>,” in <source>Int. Conf. Med. Image Comput. Comput.-Assist. Intervent</source>
<publisher-loc>Lima, Peru</publisher-loc>: <publisher-name>Springer</publisher-name>, <year>2020</year>, pp. <fpage>743</fpage>–<lpage>752</lpage>.</mixed-citation>
    </ref>
    <ref id="R38">
      <label>[38]</label>
      <mixed-citation publication-type="journal"><name><surname>Jaderberg</surname><given-names>M</given-names></name><etal/>, “<article-title>Spatial transformer networks</article-title>,” in <source>Proc. Adv. Neural Inf. Process. Syst</source>, <year>2015</year>, pp. <fpage>2017</fpage>–<lpage>2025</lpage>.</mixed-citation>
    </ref>
    <ref id="R39">
      <label>[39]</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>Q</given-names></name>, <name><surname>Zhou</surname><given-names>F</given-names></name>, <name><surname>Hang</surname><given-names>R</given-names></name>, and <name><surname>Yuan</surname><given-names>X</given-names></name>, “<article-title>Bidirectional-convolutional LSTM based spectral-spatial feature learning for hyperspectral image classification</article-title>,” <source>Remote Sens</source>, vol. <volume>9</volume>, no. <issue>12</issue>, p. <fpage>1330</fpage>, <month>12</month>. <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="R40">
      <label>[40]</label>
      <mixed-citation publication-type="journal"><name><surname>Papademetris</surname><given-names>X</given-names></name><etal/>, “<article-title>Bioimage suite: An integrated medical image analysis suite: An update</article-title>,” <source>Insight J</source>, vol. <volume>2006</volume>, p. <fpage>209</fpage>, <month>6</month>. <year>2006</year>.<pub-id pub-id-type="pmid">25364771</pub-id></mixed-citation>
    </ref>
    <ref id="R41">
      <label>[41]</label>
      <mixed-citation publication-type="confproc"><name><surname>Mao</surname><given-names>X</given-names></name>, <name><surname>Li</surname><given-names>Q</given-names></name>, <name><surname>Xie</surname><given-names>H</given-names></name>, <name><surname>Lau</surname><given-names>RYK</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, and <name><surname>Smolley</surname><given-names>SP</given-names></name>, “<article-title>Least squares generative adversarial networks</article-title>,” in <conf-name>Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</conf-name>, <month>10</month>. <year>2017</year>, pp. <fpage>2794</fpage>–<lpage>2802</lpage>.</mixed-citation>
    </ref>
    <ref id="R42">
      <label>[42]</label>
      <mixed-citation publication-type="journal"><name><surname>Normandin</surname><given-names>MD</given-names></name><etal/>, “<article-title><italic>In vivo</italic> imaging of endogenous pancreatic <italic>β</italic>-cell mass in healthy and type 1 diabetic subjects using <sup>18</sup>F-fluoropropyl-dihydrotetrabenazine and PET</article-title>,” <source>J. Nucl. Med</source>, vol. <volume>53</volume>, no. <issue>6</issue>, pp. <fpage>908</fpage>–<lpage>916</lpage>, <month>6</month>. <year>2012</year>.<pub-id pub-id-type="pmid">22573821</pub-id></mixed-citation>
    </ref>
    <ref id="R43">
      <label>[43]</label>
      <mixed-citation publication-type="journal"><name><surname>Rezaei</surname><given-names>A</given-names></name>, <name><surname>Michel</surname><given-names>C</given-names></name>, <name><surname>Casey</surname><given-names>ME</given-names></name>, and <name><surname>Nuyts</surname><given-names>J</given-names></name>, “<article-title>Simultaneous reconstruction of the activity image and registration of the ct image in TOF-PET</article-title>,” <source>Phys. Med. Biol</source>, vol. <volume>61</volume>, no. <issue>4</issue>, p. <fpage>1852</fpage>, <year>2016</year>.<pub-id pub-id-type="pmid">26854817</pub-id></mixed-citation>
    </ref>
    <ref id="R44">
      <label>[44]</label>
      <mixed-citation publication-type="journal"><name><surname>Paszke</surname><given-names>A</given-names></name><etal/>, “<article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>,” in <source>Proc. Adv. Neural Inf. Process. Syst</source>, <year>2019</year>, pp. <fpage>8026</fpage>–<lpage>8037</lpage>.</mixed-citation>
    </ref>
    <ref id="R45">
      <label>[45]</label>
      <mixed-citation publication-type="journal"><name><surname>Kingma</surname><given-names>DP</given-names></name> and <name><surname>Ba</surname><given-names>J</given-names></name>, “<article-title>Adam: A method for stochastic optimization</article-title>,” <year>2014</year>, <source>arXiv:1412.6980</source>. <comment>[Online]. Available: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link></comment></mixed-citation>
    </ref>
    <ref id="R46">
      <label>[46]</label>
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>Y</given-names></name>, <name><surname>Tian</surname><given-names>Y</given-names></name>, <name><surname>Kong</surname><given-names>Y</given-names></name>, <name><surname>Zhong</surname><given-names>B</given-names></name>, and <name><surname>Fu</surname><given-names>Y</given-names></name>, “<article-title>Residual dense network for image restoration</article-title>,” <source>IEEE Trans. Pattern Anal. Mach. Intell</source>, <comment>early access</comment>, <month>1</month>. <volume>21</volume>, <fpage>2020</fpage>, doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2020.2968521</pub-id><comment>.</comment></mixed-citation>
    </ref>
    <ref id="R47">
      <label>[47]</label>
      <mixed-citation publication-type="confproc"><name><surname>Yu</surname><given-names>S</given-names></name>, <name><surname>Park</surname><given-names>B</given-names></name>, and <name><surname>Jeong</surname><given-names>J</given-names></name>, “<article-title>Deep iterative down-up CNN for image denoising</article-title>,” in <conf-name>Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW)</conf-name>, <month>6</month>. <year>2019</year>, pp. <fpage>2095</fpage>–<lpage>2103</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Fig. 1.</label>
    <caption>
      <p id="P32">Illustration of phase gated PET and the proposed method. The Anzai signal (red curve) can guide the assignment of the detected events to different respiratory phases and generate 6 gated images. End-expiration gate with the least intra-gate motion (G4) is used as our reference gate. Our goal is to register all the low-dose gated images to the reference gate, averaging them, and denoise the averaged image to generate a high-dose gated image at the reference gate with the least intra-gate motion.</p>
    </caption>
    <graphic xlink:href="nihms-1751922-f0001"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Fig. 2.</label>
    <caption>
      <p id="P33">The overall structure of our unified motion correction and denoising network (MDPET). The reference gate low-dose image <italic>L</italic><sub>ref</sub> and N-th gate low-dose images <italic>L<sub>n</sub></italic> are fed into each Siamese Pyramid Network (SP-Net) within our Temporal Siamese Pyramid Network (TSP-Net). The predicted transformation fields <italic>T<sub>n</sub></italic> simultaneously transform the paired <italic>L<sub>n</sub></italic> and <italic>H<sub>n</sub></italic>. The transformed low-dose gated image <inline-formula><mml:math display="inline" id="M1"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are averaged and subsequently fed into the denoising network for denoising. Our MDPET is trained in a unified fashion with registration loss <inline-formula><mml:math display="inline" id="M2"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext mathvariant="italic">reg</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, denoising loss <inline-formula><mml:math display="inline" id="M3"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and adversarial loss <inline-formula><mml:math display="inline" id="M4"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext mathvariant="italic">adv</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> combined.</p>
    </caption>
    <graphic xlink:href="nihms-1751922-f0002"/>
  </fig>
  <fig id="F3" orientation="portrait" position="float">
    <label>Fig. 3.</label>
    <caption>
      <p id="P34">Design of our Siamese Pyramid Network (SP-Net). 5 levels of pyramid features are generated from the reference low-dose gated image <italic>L<sub>ref</sub></italic> and the source low-dose image <italic>L<sub>n</sub></italic>. Generation of pyramid features are supervised by the reference high-dose image <italic>H<sub>ref</sub></italic> and the source high-dose image <italic>H<sub>n</sub></italic>. The pyramid features are fused and decoded to generate the transformation features. The number of feature channel is denoted inside the feature map. The spatial resolution of each feature map with respect to the input image is printed next to the feature map.</p>
    </caption>
    <graphic xlink:href="nihms-1751922-f0003"/>
  </fig>
  <fig id="F4" orientation="portrait" position="float">
    <label>Fig. 4.</label>
    <caption>
      <p id="P35">Low-dose gated image before and after deformation by our MDPET. The unregistered low-dose gated images <italic>L<sub>n</sub></italic> and the corresponding averaged image <italic>L<sub>avg</sub></italic> are shown in the 1st row. The deformed low-dose gated image <italic>L<sub>n</sub></italic> and the corresponding averaged image <italic>L<sub>avg</sub></italic> are shown in the 2nd row. The predicted transformations are applied to the corresponding high-dose gated images <italic>H<sub>n</sub></italic>, where the difference of H between reference gate and source gate are visualized. The difference of <italic>H</italic> before and after registration over all gates are shown in the 3rd and 4th row, respectively. The motion blurred regions are indicated by gray arrows.</p>
    </caption>
    <graphic xlink:href="nihms-1751922-f0004"/>
  </fig>
  <fig id="F5" orientation="portrait" position="float">
    <label>Fig. 5.</label>
    <caption>
      <p id="P36">Comparison of registration errors between previous registration methods and our MDPET over Gate 1, Gate 3, and Gate 6 at kidney, liver, and pancreas regions. From top to bottom: without registration, VM [<xref rid="R9" ref-type="bibr">9</xref>], NRB [<xref rid="R40" ref-type="bibr">40</xref>], SAN [<xref rid="R37" ref-type="bibr">37</xref>], and our MDPET. Using NRB and VM, misalignment errors can be found in spine region at gate 6 (red arrows), and additional misalignment errors are introduced in kidney, liver and pancreas regions at Gate 3 (blue arrows).</p>
    </caption>
    <graphic xlink:href="nihms-1751922-f0005"/>
  </fig>
  <fig id="F6" orientation="portrait" position="float">
    <label>Fig. 6.</label>
    <caption>
      <p id="P37">Comparison of denoising results. The averaged low-dose gated image generated from different motion estimation methods are shown in the 1st row. The corresponding denoised images are shown in the 2rd row. From left to right: ground truth, UNet denoising from the averaged image without any deformation, UNet denoising on the averaged image based on NRB-derived deformation fields, UNet denoising on the averaged image based on VM-derived deformation fields, UNet denoising on the averaged image based on SAN-derived deformation fields, and the end-to-end output from our MDPET. Our MDPET can reduce the motion blurring between the liver and kidney (gray box), as well as improving the visualization of small anatomic structures, such as portal veins (blue arrows).</p>
    </caption>
    <graphic xlink:href="nihms-1751922-f0006"/>
  </fig>
  <fig id="F7" orientation="portrait" position="float">
    <label>Fig. 7.</label>
    <caption>
      <p id="P38">The boxplot results of all denoising testing images, where “*” means the difference are significant at <italic>p</italic> &lt; 0.05, while “N.S” means not significant.</p>
    </caption>
    <graphic xlink:href="nihms-1751922-f0007"/>
  </fig>
  <fig id="F8" orientation="portrait" position="float">
    <label>Fig. 8.</label>
    <caption>
      <p id="P39">Three subjects with low-dose gated PET. The averaged images <italic>L</italic> and the corresponding denoised image from different MDPET configurations are shown in the 1st row and 2rd row in each patient’s image group. Motion blurred anatomic structure are recovered using our MDPET (blue arrows).</p>
    </caption>
    <graphic xlink:href="nihms-1751922-f0008"/>
  </fig>
  <table-wrap id="T1" position="float" orientation="landscape">
    <label>TABLE I</label>
    <caption>
      <p id="P40">Comparison of Different Motion Estimation Methods for Different Gates. NMAE Is Calculated Based on the <italic>H</italic> Transformed by the Predicted <italic>T</italic>. The Run Time of Each Algorithm on CPU and GPU Is Shown on the Last Two Columns.</p>
    </caption>
    <table frame="hsides" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">NMAE</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 1</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 2</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 3</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 4</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 5</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 6</th>
          <th align="center" valign="top" style="border-right-style: hidden" rowspan="1" colspan="1">GPU sec</th>
          <th align="center" valign="top" rowspan="1" colspan="1">CPU sec</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">w/o registration</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1846</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1066</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0653</td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0659</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1360</td>
          <td align="center" valign="top" style="border-right-style: hidden" rowspan="1" colspan="1">0</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">NRB [<xref rid="R40" ref-type="bibr">40</xref>]</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1564<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1347<styled-content style="color:#ff0000">↑</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1212<styled-content style="color:#ff0000">↑</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1288<styled-content style="color:#ff0000">↑</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1339<styled-content style="color:#0000ff">↓</styled-content><sup><xref rid="TFN2" ref-type="table-fn">†</xref></sup></td>
          <td align="center" valign="top" style="border-right-style: hidden" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">1489</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">VM [<xref rid="R9" ref-type="bibr">9</xref>]</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1362<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1202<styled-content style="color:#ff0000">↑</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1126<styled-content style="color:#ff0000">↑</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1144<styled-content style="color:#ff0000">↑</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1232<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" style="border-right-style: hidden" rowspan="1" colspan="1">2.1</td>
          <td align="center" valign="top" rowspan="1" colspan="1">220</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">SAN [<xref rid="R37" ref-type="bibr">37</xref>]</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1298<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0882<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0682<styled-content style="color:#ff0000">↑</styled-content><sup><xref rid="TFN2" ref-type="table-fn">†</xref></sup></td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0751<styled-content style="color:#ff0000">↑</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1103<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" style="border-right-style: hidden" rowspan="1" colspan="1">4.3</td>
          <td align="center" valign="top" rowspan="1" colspan="1">423</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Ours</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1098<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0749<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0582<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0619<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0908<styled-content style="color:#0000ff">↓</styled-content><xref rid="TFN2" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" style="border-right-style: hidden" rowspan="1" colspan="1">0.54</td>
          <td align="center" valign="top" rowspan="1" colspan="1">59</td>
        </tr>
      </tbody>
    </table>
    <table-wrap-foot>
      <fn id="TFN1">
        <p id="P41"><styled-content style="color:#0000ff">↓</styled-content> and <styled-content style="color:#ff0000">↑</styled-content> Mean the NMAE Decrease and Increase as Compared to Baseline NMAE Without Registration, Respectively.</p>
      </fn>
      <fn id="TFN2">
        <label>“*”</label>
        <p id="P42">Means the Difference to the Baseline NMAE Without Registration Are Significant at <italic>p</italic> &lt; 0.05, While “†” Means Not Significant</p>
      </fn>
    </table-wrap-foot>
  </table-wrap>
  <table-wrap id="T2" position="float" orientation="landscape">
    <label>TABLE II</label>
    <caption>
      <p id="P43">Ablation Study on Our MDPET in Terms of Motion Estimation. ± LSTM Means MDPET With or Without BiConvLSTM and ± GAN Means MDPET With or Without Adversarial Learning.</p>
    </caption>
    <table frame="hsides" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">NMAE</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 1</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 2</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 3</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 4</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 5</th>
          <th align="center" valign="top" rowspan="1" colspan="1">Gate 6</th>
          <th align="center" valign="top" style="border-right-style: hidden" rowspan="1" colspan="1">GPU sec</th>
          <th align="center" valign="top" rowspan="1" colspan="1">CPU sec</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Ours-LSTM-GAN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1283</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0849</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0660</td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0701</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1010</td>
          <td align="center" valign="top" style="border-right-style: hidden" rowspan="1" colspan="1">0.38</td>
          <td align="center" valign="top" rowspan="1" colspan="1">36</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Ours+LSTM-GAN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1116<xref rid="TFN3" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0762<xref rid="TFN3" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0588<xref rid="TFN3" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0623<xref rid="TFN3" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0920<xref rid="TFN3" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" style="border-right-style: hidden" rowspan="1" colspan="1">0.54</td>
          <td align="center" valign="top" rowspan="1" colspan="1">59</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Ours+LSTM+GAN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.1098<xref rid="TFN3" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0749<xref rid="TFN3" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0582<xref rid="TFN3" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">-</td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0619<xref rid="TFN3" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">0.0908<xref rid="TFN3" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" style="border-right-style: hidden" rowspan="1" colspan="1">0.54</td>
          <td align="center" valign="top" rowspan="1" colspan="1">59</td>
        </tr>
      </tbody>
    </table>
    <table-wrap-foot>
      <fn id="TFN3">
        <label>“*”</label>
        <p id="P44">Means the Difference to the Baseline (1st Row) Are Significant at <italic>p &lt;</italic> 0.05</p>
      </fn>
    </table-wrap-foot>
  </table-wrap>
  <table-wrap id="T3" position="float" orientation="portrait">
    <label>TABLE III</label>
    <caption>
      <p id="P45">Comparison of Denoising Performance on Different Motion-Compensated Images. Our MDPET Is Compared With 1) SAN and 2) Two-Stage Processing Methods That Consist of Motion Estimation and Denoising (DN).</p>
    </caption>
    <table frame="hsides" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Method-mean(std)</th>
          <th align="center" valign="top" rowspan="1" colspan="1">NMAE</th>
          <th align="center" valign="top" rowspan="1" colspan="1">SSIM</th>
          <th align="center" valign="top" rowspan="1" colspan="1">PSNR</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">✗REG+✗DN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1712(.0225)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9018(.0175)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">25.87(1.87)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">✓NRB+✗DN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1174(.0198)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9424(.0096)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">28.97(1.79)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">NRB+UNet</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1166(.0177)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9479(.0068)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">29.49(1.85)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">NRB+GAN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1147(.0179)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9489(.0071)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">29.66(1.91)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">✓VM+✗DN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1165(.0130)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9431(.0080)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">28.98(1.90)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">VM+UNet</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1125(.0124)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9480(.0052)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">29.43(1.99)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">VM+GAN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1128(.0130)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9490(.0061)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">29.48(1.98)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">✓SAN+✗DN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1401(.0187)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9191(.0154)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">27.99(1.49)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">SAN+UNet</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1062(.0122)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9498(.0061)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">30.31(1.87)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">SAN+GAN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1036(.0117)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9503(.0061)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">30.87(1.79)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">✓Ours+✗DN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1383(.0185)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9193(.0153)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">28.14(1.46)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Ours</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.0883(.0133)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9669(.0054)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">32.28(1.89)</td>
        </tr>
      </tbody>
    </table>
    <table-wrap-foot>
      <fn id="TFN4">
        <p id="P46">✓ and ✗ Denote Use or Not Use of a Specific Processing Stage. For Example, ✓NRB+✗DN Means NRB Is Used For Estimating the Motion and Generating The Averaged Image, but No Denoising Step Is Applied. The Corresponding Boxplot Comparison Results With Statistical Analysis Are Shown in <xref rid="F7" ref-type="fig">Figure 7</xref></p>
      </fn>
    </table-wrap-foot>
  </table-wrap>
  <table-wrap id="T4" position="float" orientation="portrait">
    <label>TABLE IV</label>
    <caption>
      <p id="P47">Ablation Study on Our MDPET in Terms of Denoising. ± LSTM Means MDPET With or Without BiConvLSTM and ± GAN Means MDPET With or Without Adversarial Learning.</p>
    </caption>
    <table frame="hsides" rules="all">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Method-mean(std)</th>
          <th align="center" valign="top" rowspan="1" colspan="1">NMAE</th>
          <th align="center" valign="top" rowspan="1" colspan="1">SSIM</th>
          <th align="center" valign="top" rowspan="1" colspan="1">PSNR</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Ours-LSTM-GAN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.1058(.0140)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9587(.0063)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">30.95(1.89)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Ours+LSTM-GAN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.0921(.0137)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9613(.0062)</td>
          <td align="center" valign="top" rowspan="1" colspan="1">31.64(1.88)</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Ours+LSTM+GAN</td>
          <td align="center" valign="top" rowspan="1" colspan="1">.0883(.0133)<xref rid="TFN5" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">.9669(.0054)<xref rid="TFN5" ref-type="table-fn">*</xref></td>
          <td align="center" valign="top" rowspan="1" colspan="1">32.28(1.89)<xref rid="TFN5" ref-type="table-fn">*</xref></td>
        </tr>
      </tbody>
    </table>
    <table-wrap-foot>
      <fn id="TFN5">
        <label>“*”</label>
        <p id="P48">Means the Difference to the Baseline (1st Row) Are Significant at <italic>p &lt;</italic> 0.05</p>
      </fn>
    </table-wrap-foot>
  </table-wrap>
</floats-group>
