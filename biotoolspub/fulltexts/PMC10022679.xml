<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Cell Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Cell Sci</journal-id>
    <journal-id journal-id-type="publisher-id">JCS</journal-id>
    <journal-title-group>
      <journal-title>Journal of Cell Science</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0021-9533</issn>
    <issn pub-type="epub">1477-9137</issn>
    <publisher>
      <publisher-name>The Company of Biologists Ltd</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10022679</article-id>
    <article-id pub-id-type="pmid">36727532</article-id>
    <article-id pub-id-type="doi">10.1242/jcs.260728</article-id>
    <article-id pub-id-type="publisher-id">JCS260728</article-id>
    <article-version vocab="JAV" vocab-identifier="http://www.niso.org/publications/rp/RP-8-2008.pdf" article-version-type="VoR">Version of Record</article-version>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Tools and Resources</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Fast4DReg – fast registration of 4D microscopy datasets</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Pylvänäinen</surname>
          <given-names>Joanna W.</given-names>
        </name>
        <xref rid="af1" ref-type="aff">1</xref>
        <xref rid="af2" ref-type="aff">2</xref>
        <xref rid="af3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Laine</surname>
          <given-names>Romain F.</given-names>
        </name>
        <xref rid="af4" ref-type="aff">4</xref>
        <xref rid="af5" ref-type="aff">5</xref>
        <xref rid="AN1" ref-type="author-notes">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Saraiva</surname>
          <given-names>Bruno M. S.</given-names>
        </name>
        <xref rid="af6" ref-type="aff">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ghimire</surname>
          <given-names>Sujan</given-names>
        </name>
        <xref rid="af1" ref-type="aff">1</xref>
        <xref rid="af3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Follain</surname>
          <given-names>Gautier</given-names>
        </name>
        <xref rid="af1" ref-type="aff">1</xref>
        <xref rid="af3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Henriques</surname>
          <given-names>Ricardo</given-names>
        </name>
        <xref rid="af6" ref-type="aff">6</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-9286-920X</contrib-id>
        <name>
          <surname>Jacquemet</surname>
          <given-names>Guillaume</given-names>
        </name>
        <email>guillaume.jacquemet@abo.fi</email>
        <xref rid="af1" ref-type="aff">1</xref>
        <xref rid="af2" ref-type="aff">2</xref>
        <xref rid="af3" ref-type="aff">3</xref>
        <xref rid="af7" ref-type="aff">7</xref>
        <xref rid="cor1" ref-type="corresp">‡</xref>
      </contrib>
    </contrib-group>
    <aff id="af1"><label><sup>1</sup></label><institution>Åbo Akademi University, Faculty of Science and Engineering, Biosciences</institution>, <addr-line>Turku 20520</addr-line>, <country>Finland</country></aff>
    <aff id="af2"><label><sup>2</sup></label><institution>Turku Bioimaging, University of Turku and Åbo Akademi University</institution>, <addr-line>Turku 20520</addr-line>, <country>Finland</country></aff>
    <aff id="af3"><label><sup>3</sup></label><institution>Turku Bioscience Centre, University of Turku and Åbo Akademi University</institution>, <addr-line>Turku 20520</addr-line>, <country>Finland</country></aff>
    <aff id="af4"><label><sup>4</sup></label><institution>MRC Laboratory for Molecular Cell Biology, University College London</institution>, <addr-line>London WC1E 6BT</addr-line>, <country>UK</country></aff>
    <aff id="af5"><label><sup>5</sup></label><institution>The Francis Crick Institute</institution>, <addr-line>London NW1 1AT</addr-line>, <country>UK</country></aff>
    <aff id="af6"><label><sup>6</sup></label><institution>Instituto Gulbenkian de Ciência</institution>, <addr-line>Oeiras 2780-156</addr-line>, <country>Portugal</country></aff>
    <aff id="af7"><label><sup>7</sup></label><institution>InFLAMES Research Flagship Center, Åbo Akademi University</institution>, <addr-line>Turku 20520</addr-line>, <country>Finland</country></aff>
    <author-notes>
      <fn id="AN1" fn-type="present-address">
        <label>*</label>
        <p>Present address: Micrographia Bio, Translation and Innovation Hub, 84 Wood Lane, London, UK.</p>
      </fn>
      <corresp id="cor1"><label>‡</label>Author for correspondence (<email>guillaume.jacquemet@abo.fi</email>)
</corresp>
      <fn id="edited1" fn-type="edited-by">
        <p>Handling Editor: Michael Way</p>
      </fn>
      <fn fn-type="COI-statement">
        <p>
          <bold>Competing interests</bold>
        </p>
        <p>The authors declare no competing or financial interests.</p>
      </fn>
    </author-notes>
    <pub-date iso-8601-date="2023-02-15" pub-type="collection">
      <day>15</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date iso-8601-date="2023-02-27" pub-type="epub">
      <day>27</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <!--PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>.-->
    <volume>136</volume>
    <issue>4</issue>
    <elocation-id>jcs260728</elocation-id>
    <history>
      <date iso-8601-date="2022-10-16" date-type="received">
        <day>16</day>
        <month>10</month>
        <year>2022</year>
      </date>
      <date iso-8601-date="2023-01-25" date-type="accepted">
        <day>25</day>
        <month>1</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023. Published by The Company of Biologists Ltd</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<uri xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0</uri>), which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="joces-136-260728.pdf"/>
    <related-article related-article-type="article-reference" ext-link-type="doi" id="RA1" xlink:href="10.1242/jcs.261053"/>
    <abstract>
      <title>ABSTRACT</title>
      <p>Unwanted sample drift is a common issue that plagues microscopy experiments, preventing accurate temporal visualization and quantification of biological processes. Although multiple methods and tools exist to correct images post acquisition, performing drift correction of three-dimensional (3D) videos using open-source solutions remains challenging and time consuming. Here, we present a new tool developed for ImageJ or Fiji called Fast4DReg that can quickly correct axial and lateral drift in 3D video-microscopy datasets. Fast4DReg works by creating intensity projections along multiple axes and estimating the drift between frames using two-dimensional cross-correlations. Using synthetic and acquired datasets, we demonstrate that Fast4DReg can perform better than other state-of-the-art open-source drift-correction tools and significantly outperforms them in speed. We also demonstrate that Fast4DReg can be used to register misaligned channels in 3D using either calibration slides or misaligned images directly. Altogether, Fast4DReg provides a quick and easy-to-use method to correct 3D imaging data before further visualization and analysis.</p>
    </abstract>
    <abstract abstract-type="teaser">
      <p><bold>Summary:</bold> Fast4DReg is an open-source Fiji plugin that can quickly correct axial and lateral drift in multidimensional microscopy datasets.</p>
    </abstract>
    <kwd-group>
      <kwd>3D drift correction</kwd>
      <kwd>Live imaging</kwd>
      <kwd>Image analysis</kwd>
      <kwd>ImageJ</kwd>
      <kwd>Fiji</kwd>
    </kwd-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Academy of Finland</institution>
            <institution-id>http://dx.doi.org/10.13039/501100002341</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>338537</award-id>
        <award-id>337531</award-id>
        <award-id>332402</award-id>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Syöpäjärjestöt</institution>
            <institution-id>http://dx.doi.org/10.13039/501100006383</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Åbo Akademi</institution>
            <institution-id>http://dx.doi.org/10.13039/501100008019</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Medical Research Council</institution>
            <institution-id>http://dx.doi.org/10.13039/501100007155</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>MR/T027924/1</award-id>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Fundação Calouste Gulbenkian</institution>
            <institution-id>http://dx.doi.org/10.13039/501100005635</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>European Research Council</institution>
            <institution-id>http://dx.doi.org/10.13039/100010663</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Horizon 2020</institution>
            <institution-id>http://dx.doi.org/10.13039/100010661</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>101001332</award-id>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>European Molecular Biology Organization</institution>
            <institution-id>http://dx.doi.org/10.13039/100004410</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>EMBO-2020-IG4734</award-id>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>Chan Zuckerberg Initiative</institution>
            <institution-id>http://dx.doi.org/10.13039/100014989</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>vpi-0000000044</award-id>
      </award-group>
    </funding-group>
    <funding-group specific-use="FundRef">
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution>University of Turku</institution>
            <institution-id>http://dx.doi.org/10.13039/501100005609</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>Introduction</title>
    <p>Live imaging is essential in biomedical research, enabling scientists to follow biological processes over time. Despite being heavily used, performing live-imaging experiments using fluorescence microscopy remains technically challenging. The user must carefully balance illumination power and acquisition speed while maintaining specimen health. In addition, imaging is often prone to drift. Drift can be caused, for example, by temperature changes leading to thermal expansion of the microscope mechanical components or by the movement of the sample itself.</p>
    <p>Multiple software and hardware solutions have been developed to minimize drifting during acquisition. For instance, axial drifting can be limited using an infrared light that is reflected on the glass-sample interface and captured by a detector (e.g. Leica's Adaptive Focus Control or Nikon's Perfect Focus System). Lateral drift due to sample movement can also be compensated by tracking algorithms that follow the sample over time and move the microscope stage accordingly (<xref rid="JCS260728C2" ref-type="bibr">Fox et al., 2022</xref>; <xref rid="JCS260728C18" ref-type="bibr">von Wangenheim et al., 2017</xref>). Yet, drifting is rarely entirely eliminated at the acquisition stage, especially when acquiring multiple positions for an extended period. Therefore, it is often necessary to perform drift correction (via image registration) as a post-processing step before image visualization and quantification. Beyond live imaging, drift correction and/or channel registration is a crucial processing step for multiple image-analysis pipelines, including colocalization analysis (using calibration slides) or the reconstruction of super-resolution microscopy images.</p>
    <p>Most drift-correction/registration algorithms work sequentially by comparing a reference image to a moving image and estimating the movement between these two images to correct the drift. Multiple open-source tools capable of correcting four-dimensional (4D) datasets already exist. Popular tools include, for instance, Insight ToolKit (<xref rid="JCS260728C8" ref-type="bibr">McCormick et al., 2014</xref>), elastix (<xref rid="JCS260728C4" ref-type="bibr">Klein et al., 2010</xref>), Multiview Reconstruction (<xref rid="JCS260728C12" ref-type="bibr">Preibisch et al., 2010</xref>; <xref rid="JCS260728C13" ref-type="bibr">2014</xref>), Fijiyama (<xref rid="JCS260728C1" ref-type="bibr">Fernandez and Moisy, 2021</xref>) or Correct 3D drift (Correct3DD) (<xref rid="JCS260728C10" ref-type="bibr">Parslow et al., 2014</xref>). However, except for Multiview Reconstruction and Correct3DD, these tools are geared toward correcting medical imaging datasets and can be impractical to use for the correction of long three-dimensional (3D) videos. Multiview Reconstruction, which was designed to register large light-sheet fluorescence microscopy datasets, uses interest points (e.g. fluorescent beads, nuclei or membrane markers) in the imaging volume to perform the 3D registration, which are not always available (<xref rid="JCS260728C12" ref-type="bibr">Preibisch et al., 2010</xref>; <xref rid="JCS260728C13" ref-type="bibr">2014</xref>). Although we routinely use Correct3DD, we felt limited by its speed and available features.</p>
    <p>Here, prompted by a need to correct our 3D videos more easily and more efficiently, we developed Fast4DReg, a fast two-dimensional (2D) and 3D video drift-correction tool. Using multiple datasets, we show that Fast4DReg can outperform two state-of-the-art 3D video drift-correction tools available in Fiji, namely, Correct3DD (<xref rid="JCS260728C10" ref-type="bibr">Parslow et al., 2014</xref>) and Fijiyama (<xref rid="JCS260728C1" ref-type="bibr">Fernandez and Moisy, 2021</xref>). In addition, we show that Fast4DReg can register misaligned channels in 3D using either calibration slides or misaligned images directly. Fast4DReg is fast and has an easy-to-use graphical interface. These features make Fast4DReg a versatile and easy-to-use open-source 2D/3D drift-correction tool.</p>
  </sec>
  <sec sec-type="results" id="s2">
    <title>RESULTS</title>
    <sec id="s2a">
      <title>The Fast4DReg pipeline</title>
      <p>Fast4DReg breaks the drift-correction task into two steps: image registration (estimation of a transformation that corrects the drift optimally) followed by image transformation (applying the determined parameters to produce a corrected image). To estimate the drift of a 3D video in the <italic toggle="yes">x-</italic>, <italic toggle="yes">y-</italic> and <italic toggle="yes">z-</italic>coordinates, Fast4DReg sequentially estimates the lateral drift, corrects the lateral drift, then estimates and corrects the axial drift (<xref rid="JCS260728F1" ref-type="fig">Fig. 1</xref>). Lateral and axial drift corrections can also be performed independently, which can be particularly useful when only the axial drift needs to be corrected. As an output of the drift estimation step, Fast4DReg creates a new folder containing the corrected images, drift plots (graphs indicating the amount of drift detected), a drift table (drift detected in numerical values) and a settings file containing the selected parameters. Notably, the drift table can then be applied to correct other images using the same parameters (i.e. to correct another channel). Indeed, when correcting multichannel 3D videos, the user needs to choose one channel to use to estimate the drift. The other channel(s) can then be corrected using the same drift table (<xref rid="JCS260728F1" ref-type="fig">Fig. 1</xref>). To estimate the lateral or axial drift of a 3D video, Fast4DReg creates <italic toggle="yes">z-</italic> or <italic toggle="yes">y</italic>-intensity projections for each time point to create a 2D video. Fast4DReg then estimates the linear drift between the reference and moving frames by calculating their cross-correlation matrix (CCM) (see Materials and Methods for more details). The location of the peak intensity in the CCM defines the linear shift between the two images. Sub-pixel accuracy is accomplished by upscaling the CCM via bicubic spline interpolation (as demonstrated by <xref rid="JCS260728C5" ref-type="bibr">Laine et al., 2019</xref>). Depending on their data, users can choose the first frame (best for fixed data) or consecutive frames from the movie (best for live-imaging data) as the reference frame.</p>
      <fig position="float" id="JCS260728F1">
        <label>Fig. 1.</label>
        <caption>
          <p><bold>Drift correction of 3D videos using Fast4DReg.</bold> Scheme highlighting the Fast4DReg pipeline. (A) Fast4DReg sequentially estimates the lateral drift, corrects the lateral drift, and then estimates and corrects the axial drift. Fast4DReg creates intensity projections along multiple axes and estimates the drift between the reference and moving frames by calculating their cross-correlation matrix (CCM). The location of the peak intensity in the CCM (pink asterisk) defines the linear shift between the two images (as highlighted by the pink double-headed arrow). Fast4DReg outputs the corrected images, the drift plots, a drift table, and a settings file containing all selected parameters and paths to the drift table. (B) The settings file inducing the used parameters and path to the drift table can then be applied to correct other datasets (i.e. another channel) directly.</p>
        </caption>
        <graphic xlink:href="joces-136-260728-g1" position="float"/>
      </fig>
    </sec>
    <sec id="s2b">
      <title>Fast4DReg outperforms Correct3DD or Fijiyama on our synthetic dataset</title>
      <p>To assess the capabilities of Fast4DReg to correct 3D videos, we compared Fast4DReg results to two other state-of-the-art drift-correction methods available in Fiji (<xref rid="JCS260728C15" ref-type="bibr">Schindelin et al., 2012</xref>): Correct3DD (<xref rid="JCS260728C10" ref-type="bibr">Parslow et al., 2014</xref>) and Fijiyama (<xref rid="JCS260728C1" ref-type="bibr">Fernandez and Moisy, 2021</xref>). For this purpose, two synthetic videos with known amounts of drift were created: one with no drift and another with a large amount of drift (<xref rid="JCS260728F2" ref-type="fig">Fig. 2</xref>A; <xref rid="sup1" ref-type="supplementary-material">Fig. S1A</xref>). As these videos were generated by duplicating an acquired single 3D stack and adding artificial drift, a perfect drift correction should generate near identical time frames as only the background noise will differ.</p>
      <fig position="float" id="JCS260728F2">
        <label>Fig. 2.</label>
        <caption>
          <p><bold>Fast4DReg outperforms Correct3DD and Fijiyama on a synthetic dataset.</bold> (A) Two synthetic 3D video datasets were created, one with no drift and another with a large amount of drift (see also <xref rid="sup1" ref-type="supplementary-material">Fig. S1</xref>). The large drift dataset was then corrected using Fast4DReg, Correct3DD and Fijiyama. (B) The drift-correction performance of the three algorithms was assessed using temporal color projections of a selected <italic toggle="yes">z</italic>-slice (middle of the cell) and kymographs (along the green dashed lines; dimensions, 25 μm × 25 frames). (C) Standard deviation time projection of the middle slice of the cell. This projection takes the standard deviation of the pixel intensities through time. Positions with large differences in the pixel intensities through the stack appear brighter in this projection. Therefore, a black image highlights no variation between the frames over time (perfect drift correction), whereas signals highlight slight errors in the drift correction. For each <italic toggle="yes">z</italic>-slice, the standard deviation projection over time was generated and quantified using Fiji, and the results are shown as boxplots created by PlotsOfData (<xref rid="JCS260728C11" ref-type="bibr">Postma and Goedhart, 2019</xref>), in which the boxes show the 25th and 75th percentiles, the whiskers represent the minimum and maximum values, and the median is marked with a line. No drift shows a high baseline value as specified noise was added during background homogenization. (D,E) The Pearson's correlation coefficient (PCC) (D) and peak signal-to-noise ratio (PSNR) (E) between the first and each subsequent frame were calculated. For PSNR, a higher value indicates a better drift correction. For Pearson's correlation coefficient, a value of 1 indicates perfect drift correction. For all panels, scale bars: 10 μm.</p>
        </caption>
        <graphic xlink:href="joces-136-260728-g2" position="float"/>
      </fig>
      <p>Visually, all three tools corrected the artificially drifting 3D videos regardless of the amount of drift (<xref rid="JCS260728F2" ref-type="fig">Fig. 2</xref>B; <uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-1">Movie 1</uri>). To carefully quantify the performance of these three software, we selected a <italic toggle="yes">z</italic>-slice and plotted the standard deviation projection of the corrected stack (<xref rid="JCS260728F2" ref-type="fig">Fig. 2</xref>C). Next, we calculated multiple image-similarity metrics between the first and each subsequent frame (<xref rid="JCS260728F2" ref-type="fig">Fig. 2</xref>D,E; <xref rid="sup1" ref-type="supplementary-material">Fig. S1B,C</xref>). Both assessment methods indicated that Fast4DReg performed better than Correct3DD or Fijiyama on our synthetic dataset (<uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-2">Movie 2</uri>; <xref rid="JCS260728F2" ref-type="fig">Fig. 2</xref>B–E; <xref rid="sup1" ref-type="supplementary-material">Fig. S1B,C</xref>). Importantly, these results demonstrated that using 2D intensity projections followed by 2D cross-correlation is a suitable method to correct drifting 3D videos.</p>
    </sec>
    <sec id="s2c">
      <title>Fast4DReg is relatively resistant to noise</title>
      <p>Live fluorescence imaging often requires using low illumination levels to avoid harming the sample, which can result in the acquisition of noisy images. In order to evaluate the sensitivity of Fast4DReg to noise, synthetic datasets with varying levels of noise were generated and processed using Fast4DReg. To assess and compare the results using image-similarity metrics, we applied the drift tables calculated by Fast4DReg to the original dataset, then calculated the image-similarity metrics on these corrected datasets (<xref rid="JCS260728F3" ref-type="fig">Fig. 3</xref>A,B). These analyses indicated that Fast4DReg was not affected by noise when the signal-to-noise ratio (SNR) was above 2. When the SNR was below 2, a decrease in performance could be observed. Interestingly, when the images started to be very noisy (SNR below 1.6), Fast4DReg performed much better when using average-intensity projections instead of maximum-intensity projections (<xref rid="JCS260728F3" ref-type="fig">Fig. 3</xref>C–F). Taken together, these results indicate that Fast4DReg is relatively resistant to noise and that, when correcting noisy data, it is more effective to use average-intensity projections.</p>
      <fig position="float" id="JCS260728F3">
        <label>Fig. 3.</label>
        <caption>
          <p><bold>Fast4DReg is relatively resistant to noise.</bold> Twelve synthetic 3D video datasets with varying amounts of noise were created and corrected using Fast4DReg, either using maximum- or average-intensity projections. The drift tables were then applied to the original data to assess drift-correction accuracy. (A,B) Schematic illustrating the pipeline used to assess Fast4DReg sensitivity to noise. (C) Example of three noisy datasets used to assess Fast4DReg sensitivity to noise. (D,E) Fast4DReg drift-correction performance for three noisy datasets (C) was assessed using temporal color projections of a selected <italic toggle="yes">z</italic>-slice (middle of the cell) and kymographs (along the green dashed lines; dimensions, 25 μm × 25 frames). Note that Fast4DReg fails to register the images with an SNR of 1.2 when using maximum-intensity projections (E). (F) Fast4DReg drift-correction performance for the twelve noisy datasets was assessed using image-similarity metrics. The PSNR and PCC between the first and subsequent frames were calculated for each noise amount. For all panels, scale bars: 10 μm.</p>
        </caption>
        <graphic xlink:href="joces-136-260728-g3" position="float"/>
      </fig>
    </sec>
    <sec id="s2d">
      <title>Fast4DReg is fast and successfully corrects drift from acquired 3D videos</title>
      <p>Next, we assessed the suitability of Fast4DReg to correct drifts in acquired 3D biological images. We used a long 3D video of a human umbilical vein endothelial cell (HUVEC) monolayer labeled with silicon rhodamine (SiR)-actin and imaged using an Airyscan confocal microscope. We also registered this dataset with Correct3DD and Fijiyama. Although both Fast4DReg and Correct3DD produced good results when assessed visually (<uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-3">Movie 3</uri>), we failed to generate meaningful results with Fijiyama as the processing made the video drift even more than the raw data (data not shown).</p>
      <p>To estimate the correction efficiency of Fast4DReg and Correct3DD on this dataset, we first searched for a structure that should remain immobile across multiple time points in the movie and chose a large stress fiber. We then color-coded three consecutive frames (one color per frame) and observed the overlaps of this stable structure between frames using line profiles (<xref rid="JCS260728F4" ref-type="fig">Fig. 4</xref>A). In the uncorrected movie, the stress fiber did not overlap in these three frames, clearly indicating drift. In the movies corrected by Fast4DReg and Correct3DD, the stress fiber overlap between frames improved, showing that the drift correction worked in both cases. Interestingly, the drift correction provided by Fast4DReg was superior here as the stress fiber overlap between the three frames was greater (<xref rid="JCS260728F4" ref-type="fig">Fig. 4</xref>A).</p>
      <fig position="float" id="JCS260728F4">
        <label>Fig. 4.</label>
        <caption>
          <p><bold>Fast4DReg can rapidly correct axial and lateral drift in 3D videos.</bold> A 3D video of HUVECs cells labeled with SiR-actin displaying a large <italic toggle="yes">xyz</italic>-drift was corrected with Fast4DReg and Correct3DD. (A) A region of interest containing a stress fiber that should remain immobile across multiple time points was chosen. Three consecutive frames were pseudo-colored blue, green and red, and merged. White indicates structural overlaps between the three frames. Line profiles along the dashed white lines to further study the overlap between frames were drawn as shown. (B) A kymograph (dimensions, 15 μm × 7146 s) of a selected line (monolayer ventral plane) in the <italic toggle="yes">y</italic>-projection was created to visualize the <italic toggle="yes">z</italic>-drift over time. (C) Image similarity metrics were calculated between each consecutive frame pair. Values show the mean±s.d. PCC, Pearson's correlation coefficient; mSSIM, mean structural similarity index; NRMSE, normalized root mean squared error; PSNR, peak signal-to-noise ratio. N.D., not determined. (D) Two computers, computer 1 (a high-performance desktop) and computer 2 (a laptop), were used to measure the speed of the correction methods. Shown values are the average times of three measurements.</p>
        </caption>
        <graphic xlink:href="joces-136-260728-g4" position="float"/>
      </fig>
      <p>To visualize the axial drift-correction efficiency of Fast4DReg and Correct3DD on this dataset, we generated kymographs from the <italic toggle="yes">y</italic>-projections (<xref rid="JCS260728F4" ref-type="fig">Fig. 4</xref>B). In the original data, the kymograph showed a clear band pattern due to the microscope stage jumping cyclically. This banding pattern was improved in the movies corrected by Fast4DReg and Correct3DD. Still, it did not entirely disappear, indicating that although both registration methods worked well on this dataset, the correction was not perfect (<xref rid="JCS260728F4" ref-type="fig">Fig. 4</xref>B; <uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-3">Movie 3</uri>). This was perhaps because part of the data went out of the imaging volume several times. Of note, Correct3DD processing led to the monolayer slowly sinking over time, which is less desirable.</p>
      <p>To obtain a quantitative estimate of the performance of Fast4DReg and Correct3DD on the HUVEC dataset, we measured the image-similarity metric between each adjacent frame pair for a selected <italic toggle="yes">z</italic>-plane in the corrected videos. Indeed, efficient drift correction should make successive frames more similar despite the inevitable biological changes. These biological changes would, however, lead to lower image-similarity metrics than those measured with our synthetic dataset (<xref rid="JCS260728F2" ref-type="fig">Figs 2</xref> and <xref rid="JCS260728F3" ref-type="fig">3</xref>), even if the data was perfectly registered. Using these metrics, we found that Fast4DReg and Correct3DD improved adjacent frame similarity on this dataset and performed similarly (<xref rid="JCS260728F4" ref-type="fig">Fig. 4</xref>C).</p>
      <p>Finally, we assessed the computing time required by Fast4DReg, Correct3DD and Fijiyama to process the HUVEC dataset using two different computers. We found that Fast4DReg was four to nine times faster than Correct3DD and 20 to 90 times faster than Fijiyama when correcting the HUVEC dataset (<xref rid="JCS260728F4" ref-type="fig">Fig. 4</xref>D). These differences are relevant as the registration of datasets often requires tweaking hyperparameters to obtain the best possible results. Overall, Fast4DReg outperformed Correct3DD when correcting the HUVEC dataset, as the corrected data did not sink over time and the processing time was faster.</p>
    </sec>
    <sec id="s2e">
      <title>Fast4DReg can register multichannel 3D videos</title>
      <p>Next, we assessed the ability of Fast4DReg to correct acquired multichannel 3D videos. We used a movie of cancer cells migrating inside the lung vasculature, which was imaged <italic toggle="yes">ex vivo</italic> using an Airyscan confocal microscope (<xref rid="JCS260728F5" ref-type="fig">Fig. 5</xref>A; <uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-4">Movies 4</uri> and <uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-5">5</uri>). In this dataset, both the cancer cells and the vasculature were labeled, and a noticeable <italic toggle="yes">xyz</italic>-drift perturbated the visualization (<xref rid="JCS260728F5" ref-type="fig">Fig. 5</xref>; <uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-4">Movies 4</uri> and <uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-5">5</uri>). To correct these data with Fast4DReg, we first estimated the drift using the vasculature images. Once the drift was estimated, the same drift tables were then applied to the cancer cell images (<xref rid="JCS260728F5" ref-type="fig">Fig. 5</xref>B). Visually, Fast4DReg corrected this dataset very well (<uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-4">Movies 4</uri> and <uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-5">5</uri>). Using time projection of a selected <italic toggle="yes">z</italic>-plane and line-intensity profiles, we found that Fast4DReg could indeed successfully register the images of the vasculature (<xref rid="JCS260728F5" ref-type="fig">Fig. 5</xref>C). Importantly, applying the same drift tables also efficiently corrected the drift in the cancer cell images (<xref rid="JCS260728F5" ref-type="fig">Fig. 5</xref>D). We believe that the ability of Fast4DReg to apply drift tables to other datasets will significantly simplify the registration of multichannel data.</p>
      <fig position="float" id="JCS260728F5">
        <label>Fig. 5.</label>
        <caption>
          <p><bold>Registration of a 3D multichannel video using Fast4DReg.</bold> A 3D multichannel video of cancer cells migrating inside the lung vasculature was corrected using Fast4DReg. (A) 3D surface rendering of a selected time point (see also <uri xlink:href="http://movie.biologists.com/video/10.1242/jcs.260728/video-4">Movie 4</uri>) created using Arivis Vision4D. The lung vasculature is shown in grey, and the cancer cell (AsPC1) is in green. The red rectangle indicates the clipping plane used to render the image. (B) Schematic illustrating the pipeline used to correct a multichannel 3D video using Fast4DReg. For this dataset, the drift was first estimated using the vasculature images (channel 1), and the resulting drift table was then applied to the cancer cell images (channel 2). (C–E) Three consecutive frames of the vasculature (C) and cancer cell images (D) were pseudo-colored blue, green and red, and merged. White indicates structural overlaps between the three frames. Line profiles along the dotted white lines to further study the overlap between frames were drawn as shown. (C,E) <italic toggle="yes">Z</italic>-projections are displayed to visualize the lateral misalignments corrected by Fast4DReg. Scale bars: 10 μm. (D,F) <italic toggle="yes">Y</italic>-projections are displayed to visualize the axial misalignment corrected by Fast4DReg. Scale bars: 5 μm.</p>
        </caption>
        <graphic xlink:href="joces-136-260728-g5" position="float"/>
      </fig>
    </sec>
    <sec id="s2f">
      <title>Fast4DReg can also register misaligned 3D channel stacks</title>
      <p>Finally, we tested whether Fast4DReg could be used to align 3D multichannel images instead of 3D videos. In this case, Fast4DReg uses the same pipeline as described for time series but first converts the channels into time frames.</p>
      <p>To test this approach, we registered, using Fast4DReg, a three-channel 3D image of a calibration slide. In this dataset, the raw images displayed significant <italic toggle="yes">xyz</italic>-misalignment owing to chromatic aberrations and the fact that the channels were acquired using different cameras (<xref rid="JCS260728F6" ref-type="fig">Fig. 6</xref>A,B). Using line-intensity profiles, we found that Fast4DReg could successfully register this dataset laterally and axially (<xref rid="JCS260728F6" ref-type="fig">Fig. 6</xref>A,B). Importantly, the drift tables measured using the calibration slide could then be used to correct any microscopy images acquired using the same conditions (<xref rid="JCS260728F6" ref-type="fig">Fig. 6</xref>C,D). Combined with the Fast4DReg batch-processing mode, we envision that the indirect channel alignment approach described here will be advantageous when performing colocalization analyses.</p>
      <fig position="float" id="JCS260728F6">
        <label>Fig. 6.</label>
        <caption>
          <p><bold>Fast4DReg can align 3D multichannel images.</bold> (A,B) A three-channel 3D calibration slide image was aligned using Fast4DReg. Merged images and line-intensity profiles (along the dashed white lines) are displayed to highlight the level of overlap between the three channels. (A) A <italic toggle="yes">z</italic>-projection is displayed to visualize the lateral misalignment corrected by Fast4DReg. (B) A <italic toggle="yes">y</italic>-projection of one of the calibration slide spots is displayed to illustrate the axial misalignment corrected by Fast4Dreg. (C,D) The drift table generated in A,B was then used to correct a 3D SIM image of a U2-OS cell expressing GFP-tagged lamellipodin (RAPH1, red) and MYO10–mScarlet (green), and labeled to visualize its actin cytoskeleton (blue). (C) A <italic toggle="yes">z</italic>-projection is displayed to visualize the lateral misalignment, evident in filopodia, corrected by Fast4Dreg. (D) A <italic toggle="yes">y</italic>-projection of one filopodium visualizes the axial misalignment corrected by Fast4Dreg. (E,F) A 3D SIM image of MCF10DCIS.com cells expressing RFP–Lifeact (magenta) and stained to visualize F-actin (green) was aligned using Fast4DReg directly. (E) A <italic toggle="yes">z</italic>-projection is displayed to visualize the lateral misalignment corrected by Fast4DReg. (F) A <italic toggle="yes">y</italic>-projection visualizes the slight axial misalignment corrected by Fast4DReg.</p>
        </caption>
        <graphic xlink:href="joces-136-260728-g6" position="float"/>
      </fig>
      <p>However, not all datasets have a corresponding calibration slide or bead image, so we also tested the ability of Fast4DReg to correct misaligned channels directly. In this case, we used a two-channel 3D image that was acquired to train a supervised image-restoration deep-learning algorithm (<xref rid="JCS260728C17" ref-type="bibr">von Chamier et al., 2021</xref>; <xref rid="JCS260728C19" ref-type="bibr">Weigert et al., 2018</xref>). As with the previous example, Fast4DReg performed well in registering this dataset (<xref rid="JCS260728F6" ref-type="fig">Fig. 6</xref>E,F). It is worth noting that a direct channel alignment approach might only work for some datasets, as Fast4DReg requires structural overlap between the channels to perform the registration. Additionally, we do not recommend directly aligning images aimed for colocalization analysis as it might lead to artefactual results.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s3">
    <title>DISCUSSION</title>
    <p>In live microscopy, sample drifting can be a significant challenge, and implementing post-processing drift-correction pipelines is not always fast or straightforward. Here, we developed Fast4DReg, an ImageJ or Fiji-based tool that can quickly correct axial and lateral drift in 2D and 3D videos. We show that Fast4DReg can outperform two open-source 3D drift-correction tools on our test datasets. A significant advantage of Fast4DReg is that it can correct 3D videos in a fraction of the time compared to other tested tools and comes with an easy-to-use graphical user interface (<xref rid="sup1" ref-type="supplementary-material">Fig. S2</xref>). Fast4DReg speed is likely due to two factors: (1) using 2D projections greatly simplifies the computations required and (2) using CPU multithreading further accelerates the 2D cross-correlation process. In the future, Fast4DReg speed could be further improved by enabling graphic card acceleration.</p>
    <p>Despite its performance, Fast4DReg has several limitations. Firstly, Fast4DReg can only perform translations when correcting a dataset. Rotation, scaling or shearing transformations are not supported, although these should not be required to correct most time-course video or multichannel microscopy datasets. Secondly, the channel alignment is limited to images with structural conservation between channels or requires calibration slides or beads images to compute the shift maps.</p>
    <p>With Fast4DReg, we demonstrate that using intensity projections followed by 2D cross-correlation is a quick and efficient way to register various multidimensional data types, including 3D videos and 3D multichannel datasets. In the future, it would be interesting to assess the suitability of using 3D cross-correlation directly to register similar images. But using 3D cross-correlation will likely impede processing times.</p>
    <p>To promote adoption by the community, Fast4DReg is available through a Fiji update site, GitHub and Zenodo. We also provide test datasets and detailed step-by-step instructions. With Fast4DReg, we hope to make the process of multidimensional data registration more straightforward and faster, and, therefore, more accessible to the community.</p>
  </sec>
  <sec sec-type="methods" id="s4">
    <title>MATERIALS AND METHODS</title>
    <sec id="s4a">
      <title>Algorithms</title>
      <p>To estimate the lateral or axial drift of a 3D video, Fast4DReg creates <italic toggle="yes">z-</italic> or <italic toggle="yes">y</italic>-intensity projections for each time point to create a 2D video. Fast4DReg then estimates the linear drift between the reference and moving frames by calculating their CCM. In Fast4DReg, in a similar fashion to the work of <xref rid="JCS260728C16" ref-type="bibr">Sun (2002)</xref>, the cross-correlation between two images is calculated by performing a discrete Hartley transform on both images, followed by a multiplication of one of the transformed images by the complex conjugate of the other. The result of this multiplication is then inversely transformed back to real space, generating the CCM. A bicubic spline interpolation is then used to upscale the CCM and achieve subpixel precision. The upscaled CCM is normalized by calculating the Pearson's correlation coefficient between the two images shifted according to the minimum and maximum values of the upscaled CCM. Finally, the linear shift between the two images is then calculated by taking the global maximum peak of the normalized up-scaled CCM (as demonstrated by <xref rid="JCS260728C5" ref-type="bibr">Laine et al., 2019</xref>).</p>
      <p>Fast4DReg can also be used to register channels from misaligned 3D stacks. In this case, Fast4DReg simply converts the channels into time frames before applying the Fast4DReg drift-correction pipeline described above. As a note of caution, cross-correlation only works well to register channels in which similar structures or cells are labeled. Importantly Fast4DReg can also register 2D video and 2D multichannel images either one at a time or in batches.</p>
      <p>Fast4DReg can run on any computer on which Fiji (<xref rid="JCS260728C15" ref-type="bibr">Schindelin et al., 2012</xref>) and the Bio-Formats (<xref rid="JCS260728C7" ref-type="bibr">Linkert et al., 2010</xref>) plugin are installed. Fast4DReg also has a memory-saving mode that allows the registration of larger datasets using a computer with limited resources (processing time available in <xref rid="sup1" ref-type="supplementary-material">Fig. S1D</xref>).</p>
      <p>Fast4DReg expects as input one or multiple single-channel 2D or 3D videos. Fast4DReg outputs corrected files, drift tables, drift plots and a settings file. Owing to Bio-Formats (<xref rid="JCS260728C7" ref-type="bibr">Linkert et al., 2010</xref>), Fast4DReg can handle various image formats as input. Fast4DReg can be tested using our test datasets available on Zenodo (<uri xlink:href="https://zenodo.org/record/7514913">https://zenodo.org/record/7514913</uri>). Fast4DReg is written using a combination of an ImageJ macro and Java, and is distributed via an ImageJ update site. The installation procedure and up-to-date, step-by-step instructions are available on the Fast4DReg GitHub page (<uri xlink:href="https://github.com/guijacquemet/Fast4DReg">https://github.com/guijacquemet/Fast4DReg</uri>).</p>
    </sec>
    <sec id="s4b">
      <title>Cells</title>
      <p>AsPC1 cells (a pancreatic ductal adenocarcinoma cell line) were purchased from American Type Culture Collection (CRL-1682) and grown in Roswell Park Memorial Institute medium (Thermo Fisher Scientific, 11875093) supplemented with 10% fetal bovine serum (FBS) (Biowest, S1860). HUVECs were purchased from PromoCell (C-12203) and grown in endothelial cell growth medium (PromoCell, C-22010). U2-OS osteosarcoma cells were purchased from the Leibniz Institute DSMZ, German Collection of Microorganisms and Cell Cultures (Braunschweig, Germany; ACC 785) and grown in Dulbecco's modified Eagle medium (DMEM; Merck, D5671) supplemented with 10% FBS. MCF10DCIS.com Lifeact–RFP cells were generated previously (<xref rid="JCS260728C3" ref-type="bibr">Jacquemet et al., 2017</xref>) and cultured in a 1:1 mix of DMEM and F12 (Merck, 51651C) supplemented with 5% horse serum (Gibco, 16050122), 20 ng/ml human epidermal growth factor (Merck, E9644), 0.5 mg/ml hydrocortisone (Merck, H0888-1G), 100 ng/ml cholera toxin (Merck, C8052-1MG), 10 mg/ml insulin (Merck, I9278-5ML) and 1% (vol/vol) penicillin/streptomycin (Merck, P0781-100ML). All cell lines tested negative for mycoplasma. AsPC1 cells were authenticated by DSMZ. All other cell lines were not authenticated.</p>
    </sec>
    <sec id="s4c">
      <title>Datasets with synthetic drift</title>
      <p>The synthetic drift datasets were created by duplicating a 3D stack image 25 times and artificially adding a known amount of <italic toggle="yes">x-</italic>, <italic toggle="yes">y</italic>- and <italic toggle="yes">z</italic>-drift between each frame. The original image was that of an AsPC1 cell expressing Lifeact–mScarletI migrating inside the vasculature of a zebrafish embryo. This image was acquired using a 3i CSU-W1 spinning-disk confocal microscope equipped with a 40× water immersion objective (NA 1.15) and a Hamamatsu sCMOS Orca Flash camera. The microscope was controlled using the Slidebook 6 software (Intelligent Imaging Innovations, Inc.; <uri xlink:href="https://www.intelligent-imaging.com/slidebook">https://www.intelligent-imaging.com/slidebook</uri>).</p>
      <p>The amount of drift added corresponds to the drift typically observed in our live-cell imaging experiments. Using this method, two videos were created: one with no drift (ground-truth video) and one with a large drift (across the field of view) (see also <xref rid="sup1" ref-type="supplementary-material">Fig. S1A</xref>). After the drift was simulated, the image background was made homogeneous via pixel intensity subtraction and by adding specified noise using Fiji (‘add specified noise’ function). The <italic toggle="yes">xy-</italic> and <italic toggle="yes">z</italic>-drift in these synthetic datasets was corrected using Fast4DReg, Correct3DD and Fijiyama. For each software, the parameters providing the best possible drift correction were chosen (the settings used are described in <xref rid="sup1" ref-type="supplementary-material">Table S1</xref>).</p>
      <p>After correcting the drift in the synthetic datasets, the images were first cropped to be the same size (352×275 pixels, 69 <italic toggle="yes">z</italic>-slices, 25 frames) using Fiji. The drift-correction performance was then quantified by measuring image-similarity metrics between frames (the reference frame was the first frame) of a selected z-slice (<italic toggle="yes">z</italic>-slice 51) using a custom-made Jupyter notebook (available in Zenodo). This <italic toggle="yes">z</italic>-slice was selected as it was in the middle of the cell.</p>
    </sec>
    <sec id="s4d">
      <title>The noisy synthetic drift dataset</title>
      <p>To generate the 12 noisy synthetic drift datasets, a specified amount of Gaussian noise was added to the original synthetically drifting dataset using Fiji (‘add specified noise’ function). The added selected Gaussian noise had standard deviations of 0, 5000, 10,000, 15,000, 20,000, 25,000, 30,000, 35,000, 40,000, 45,000, 50,000 and 60,000, yielding images with SNR of 30.053, 5.586, 2.964, 2.111, 1.686, 1.478, 1.327, 1.227, 1.196, 1.127, 1.119 and 1.070, respectively. The SNR was calculated by dividing the mean cell signal by the mean background signal. All 12 generated noisy synthetic drifts were corrected using Fast4DReg using maximum- or average-intensity projections (the settings used are described in <xref rid="sup1" ref-type="supplementary-material">Table S1</xref>). The generated drift tables were then used to correct the original large drift dataset (<xref rid="JCS260728F3" ref-type="fig">Fig. 3</xref>A,B). Corrected images were then cropped to be the same size (192×192 pixels, 69 <italic toggle="yes">z</italic>-slices, 25 frames) using Fiji. The drift-correction performance was then quantified by measuring image-similarity metrics between frames (the reference frame was the first frame) of a selected <italic toggle="yes">z</italic>-slice (z-slice 51) using a custom-made Jupyter notebook (available in Zenodo). This <italic toggle="yes">z</italic>-slice was selected as it was in the middle of the cell.</p>
    </sec>
    <sec id="s4e">
      <title>The HUVEC monolayer dataset</title>
      <p>The HUVEC monolayer dataset consists of a 3D video of HUVECs labeled with SiR-actin (Spirochrome). The video was acquired using a laser scanning confocal LSM880 microscope (Zeiss) equipped with an Airyscan detector (Carl Zeiss) and a 63× oil (NA 1.4) objective. The microscope was controlled using Zen Black (2.3) (Zeiss), and the Airyscan detector was used in standard super-resolution mode. This dataset has 200 frames (488×488 pixels) and 24 <italic toggle="yes">z</italic>-slices. This dataset was corrected using Fast4DReg, Correct3DD and Fijiyama using the parameters providing the best possible drift correction (the settings used are described in <xref rid="sup1" ref-type="supplementary-material">Table S1</xref>). The correction performance was quantified by measuring image-similarity metrics between adjacent frames (the reference frame was the previous frame) of a selected <italic toggle="yes">z</italic>-slice (<italic toggle="yes">z</italic>-slice 8) using a custom-made Jupyter notebook.</p>
      <p>Two computers were used to compare the execution times of all compared methods: computer 1 (operating system, Windows; processor, AMD Ryzen 7 5800X 8-Core; graphics card, GeForce GTX 3080; RAM, 32 GB; Fiji version 1.53q) and computer 2 [operating system, macOS; processor, M1 chip (8-core CPU, 8-core GPU); RAM: 16 GB; Fiji version 1.53q].</p>
    </sec>
    <sec id="s4f">
      <title>The mouse lung dataset</title>
      <p>The mouse lung dataset (624×626 pixels, 55 <italic toggle="yes">z</italic>-slices, eight frames, two channels) consists of a 3D video of an AsPC1 cell expressing Lifeact–mNeonGreen migrating inside the lung vasculature. Briefly, labeled AsPC1 cells were injected into the tail vein of a 10-week-old immunocompromised female mouse (Hsd: Athymic Nude-<italic toggle="yes">Foxn1<sup>nu</sup></italic> strain). The mouse was euthanized shortly after injection, and precision-cut lung slices were prepared (<xref rid="JCS260728C14" ref-type="bibr">Puttur et al., 2019</xref>). The National Animal Experiment Board authorized all animal studies, and per The Finnish Act on Animal Experimentation (animal license number 12558/2021). The lung endothelial cells were labeled using an Alexa Fluor 488-conjugated anti-PECAM antibody (1:100, BioLegend, 102413). The precision-cut lung slices were then imaged using an Airyscan confocal LSM880 microscope (Carl Zeiss) equipped with a 63× water (NA 1.15) objective. The <italic toggle="yes">xy</italic>- and <italic toggle="yes">z</italic>-drift in this dataset was corrected with Fast4DReg using the PECAM staining (settings used described in <xref rid="sup1" ref-type="supplementary-material">Table S1</xref>). The drift table generated by Fast4DReg was then applied to the second channel (cancer cells), after which these channels were merged using Fiji. Line-intensity profiles of three consecutive frames of selected structures were measured using Fiji to quantify the correction. Arivis Vision4D (v 3.5.0, Zeiss, <uri xlink:href="https://www.zeiss.com/microscopy/en/products/software/arivis-vision4d.html">https://www.zeiss.com/microscopy/en/products/software/arivis-vision4d.html</uri>) was used for the 3D reconstruction of the time-lapse movies. Surface rendering was performed using the 'Extract Isosurface' function.</p>
    </sec>
    <sec id="s4g">
      <title>The calibration slide dataset</title>
      <p>The calibration slide dataset (1024×1024 pixels, 25 <italic toggle="yes">z</italic>-slices, three channels) was created by imaging a channel calibration slide (Argolight HM) using a DeltaVision OMX v4 (GE Healthcare Life Sciences) microscope fitted with a 60× Plan-Apochromat objective lens, 1.42 NA (immersion oil refractive index of 1.516), used in wide-field illumination mode. The emitted light was collected on a front-illuminated pco.edge sCMOS camera (pixel size 6.5 mm, readout speed 95 MHz; PCO) controlled by SoftWorx (AppliedPrecision). The <italic toggle="yes">xy</italic>- and <italic toggle="yes">z</italic>-drift in this dataset was corrected using Fast4DReg (the settings used are described in <xref rid="sup1" ref-type="supplementary-material">Table S1</xref>).</p>
    </sec>
    <sec id="s4h">
      <title>The filopodia dataset</title>
      <p>The filopodia dataset (1024×1024 pixels, 17 <italic toggle="yes">z</italic>-slices, three channels) consists of a 3D structured illumination microscopy (SIM) image of a U2-OS cell expressing GFP-tagged lamellipodin and MYO10–mScarlet, and labelled with SiR-actin (<xref rid="JCS260728C9" ref-type="bibr">Popovic et al., 2023</xref>). This dataset was acquired using a DeltaVision OMX v4 (GE Healthcare Life Sciences) microscope fitted with a 60× Plan-Apochromat objective lens, 1.42 NA (immersion oil refractive index of 1.516) used in SIM illumination mode (five phases and three rotations). The emitted light was collected on a front-illuminated pco.edge sCMOS camera (pixel size 6.5 mm, readout speed 95 MHz; PCO) controlled by SoftWorx. The <italic toggle="yes">xy</italic>- and <italic toggle="yes">z</italic>-drift in this dataset was corrected using Fast4DReg using the drift table computed using the calibration slide dataset (the settings used are described in <xref rid="sup1" ref-type="supplementary-material">Table S1</xref>).</p>
    </sec>
    <sec id="s4i">
      <title>The DCIS.com filopodia dataset</title>
      <p>The DCIS.com Filopodia dataset consists of a SIM image of MCF10DCIS.COM Lifeact–RFP cells labeled with phalloidin. Briefly, MCF10DCIS.COM Lifeact–RFP cells (<xref rid="JCS260728C3" ref-type="bibr">Jacquemet et al., 2017</xref>) were grown on high-tolerance glass-bottomed dishes (MatTek Corporation, coverslip 1.5). Cells were fixed and permeabilized simultaneously using 4% (wt/vol) paraformaldehyde and 0.25% (vol/vol) Triton X-100 for 10 min. Cells were then washed with PBS, quenched using a solution of 1 M glycine for 30 min, and incubated with Alexa Fluor 488 phalloidin (1:200 in PBS; A12379, Thermo Fisher Scientific) at 4°C overnight until imaging. Samples were washed three times in PBS, mounted in Vectashield (Vectorlabs), and imaged using a DeltaVision OMX v4 (GE Healthcare Life Sciences) used in SIM illumination mode (five phases and three rotations). The microscope was fitted with a ×60 Plan-Apochromat objective lens (1.42 NA, immersion oil refractive index of 1.516). The fluorescent light was collected using front-illuminated pco.edge sCMOS camera (pixel size 6.5 μm, readout speed 95 MHz; PCO). The high SNR ratio images were acquired from the phalloidin-488 staining using acquisition parameters optimal to obtain high-quality SIM images (50 ms of exposure time, 10% laser power). The low SNR ratio images were acquired from the Lifeact-RFP channel using acquisition parameters more suitable for live-cell imaging (100 ms of exposure time, 1% laser power). The <italic toggle="yes">xy</italic>- and <italic toggle="yes">z</italic>-drift in this dataset was corrected using Fast4DReg (the settings used are described in <xref rid="sup1" ref-type="supplementary-material">Table S1</xref>).</p>
    </sec>
    <sec id="s4j">
      <title>Metrics</title>
      <p>To quantitatively assess the drift-correction performance of Fast4DReg and the other tools tested, four image-similarity metrics were used. These metrics were calculated using a custom-made Jupyter notebook (modified from <xref rid="JCS260728C6" ref-type="bibr">Laine et al., 2021</xref>). This notebook is available on Zenodo (<uri xlink:href="https://zenodo.org/record/7514913">https://zenodo.org/record/7514913</uri>).</p>
      <p>Pearson's correlation coefficient (PCC) measures the linear correlation between two images. A PCC value of 1 indicates a perfect linear relationship, or perfect similarity, between the two images. The mean structural similarity index (mSSIM) evaluates the similarity of two images based on their contrast, luminance and structural content. An mSSIM value of 1 indicates that the two images are perfectly similar. The peak SNR ratio (PSNR) is a metric that compares the peak signal amplitudes of two images and is typically expressed in decibels. A higher PSNR value indicates greater similarity between the two images. The normalized root mean squared error (NRMSE) measures the average difference between the pixel intensity in two images. A lower NRMSE value indicates greater similarity between the images.</p>
    </sec>
    <sec id="s4k">
      <title>Fast4DReg downloads and source code</title>
      <p>Fast4DReg, the generator of synthetic drift, and the notebook used to make the image-similarity measurements (all under MIT licenses) are available on GitHub (<uri xlink:href="https://github.com/guijacquemet/Fast4DReg">https://github.com/guijacquemet/Fast4DReg</uri>) and their source code is archived on Zenodo (<uri xlink:href="https://zenodo.org/record/7514913">https://zenodo.org/record/7514913</uri>). Fast4DReg is also available through a Fiji update site (<uri xlink:href="https://imagej.net/plugins/fast4dreg">https://imagej.net/plugins/fast4dreg</uri>).</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material position="float" content-type="local-data">
      <media xlink:href="joces-136-260728_review_history.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <object-id pub-id-type="doi">10.1242/joces.260728_sup1</object-id>
      <label>Supplementary information</label>
      <media xlink:href="joces-136-260728-s1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgements</title>
    <p>We acknowledge the Cell Imaging and Cytometry Core facility (Turku Bioscience, University of Turku, Åbo Akademi University and Biocenter Finland) and Turku Bioimaging for their services, instrumentation and expertise. We thank Junel Solis and Anting Li for testing Fast4DReg and providing feedback.</p>
  </ack>
  <fn-group>
    <title>Footnotes</title>
    <fn>
      <p>
        <bold>Author contributions</bold>
      </p>
      <p>Conceptualization: R.F.L., G.J.; Methodology: J.W.P., R.F.L., G.J.; Software: J.W.P., R.F.L., B.M.S.S., R.H., G.J.; Validation: G.J.; Formal analysis: J.W.P., G.J.; Investigation: J.W.P., S.G., G.F., G.J.; Resources: G.J.; Data curation: J.W.P., G.J.; Writing - original draft: J.W.P., G.J.; Writing - review &amp; editing: J.W.P., R.F.L., B.M.S.S., G.F., R.H., G.J.; Visualization: J.W.P., S.G., G.F., G.J.; Supervision: R.H., G.J.; Project administration: G.J.; Funding acquisition: R.H., G.J.</p>
    </fn>
    <fn fn-type="financial-disclosure">
      <p>
        <bold>Funding</bold>
      </p>
      <p>This study was supported by the Academy of Finland (338537 to G.J.), the Cancer Society of Finland (Syöpäjärjestöt; to G.J.) and the Solutions for Health strategic funding to Åbo Akademi University (to G.J.). J.W.P. was supported by Health Campus Turku 2.0 funded by the Academy of Finland. R.F.L. was supported by a Medical Research Council Skills development fellowship (MR/T027924/1). G.F. was supported by an Academy of Finland postdoctoral fellowship (332402). R.H. is supported by the Gulbenkian Foundation (Fundação Calouste Gulbenkian) and received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement number 101001332), the European Molecular Biology Organization (EMBO) Installation Grant (EMBO-2020-IG4734) and the Chan Zuckerberg Initiative Visual Proteomics Grant (vpi-0000000044). This research was supported by InFLAMES Flagship Programme of the Academy of Finland (decision number: 337531). Open access funding provided by the University of Turku. Deposited in PMC for immediate release.</p>
    </fn>
    <fn>
      <p>
        <bold>Data availability</bold>
      </p>
      <p>All datasets used in this study and the code used to generate them are available on Zenodo (<uri xlink:href="https://zenodo.org/record/7514913">https://zenodo.org/record/7514913</uri>).</p>
    </fn>
    <fn>
      <p>
        <bold>First Person</bold>
      </p>
      <p>
        <uri xlink:href="https://doi.org/10.1242/jcs.261053">This article has an associated First Person interview with the first author of the paper.</uri>
      </p>
    </fn>
  </fn-group>
  <sec sec-type="aggregated-review-documents" id="s5">
    <title>Peer review history</title>
    <p>The peer review history is available online at <uri xlink:href="https://journals.biologists.com/jcs/article-lookup/doi/10.1242/jcs.260728.reviewer-comments.pdf">https://journals.biologists.com/jcs/article-lookup/doi/10.1242/jcs.260728.reviewer-comments.pdf</uri></p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="JCS260728C1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fernandez</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Moisy</surname>, <given-names>C.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Fijiyama: a registration tool for 3D multimodal time-lapse imaging</article-title>. <source><italic toggle="yes">Bioinformatics</italic></source>
<volume>37</volume>, <fpage>1482</fpage>-<lpage>1484</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa846</pub-id><pub-id pub-id-type="pmid">32997734</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fox</surname>, <given-names>Z. R.</given-names></string-name>, <string-name><surname>Fletcher</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Fraisse</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Aditya</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Sosa-Carrillo</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Petit</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Gilles</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Bertaux</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Ruess</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Batt</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2022</year>). <article-title>Enabling reactive microscopy with MicroMator</article-title>. <source><italic toggle="yes">Nat. Commun.</italic></source>
<volume>13</volume>, <fpage>1</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-022-29888-z</pub-id><pub-id pub-id-type="pmid">34983933</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jacquemet</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Paatero</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Carisey</surname>, <given-names>A. F.</given-names></string-name>, <string-name><surname>Padzik</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Orange</surname>, <given-names>J. S.</given-names></string-name>, <string-name><surname>Hamidi</surname>, <given-names>H.</given-names></string-name> and <string-name><surname>Ivaska</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2017</year>). <article-title>FiloQuant reveals increased filopodia density during breast cancer progression</article-title>. <source><italic toggle="yes">J. Cell Biol.</italic></source>
<volume>216</volume>, <fpage>3387</fpage>-<lpage>3403</lpage>. <pub-id pub-id-type="doi">10.1083/jcb.201704045</pub-id><pub-id pub-id-type="pmid">28765364</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Klein</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Staring</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Murphy</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Viergever</surname>, <given-names>M. A.</given-names></string-name> and <string-name><surname>Pluim</surname>, <given-names>J. P. W.</given-names></string-name></person-group> (<year>2010</year>). <article-title>elastix: a toolbox for intensity-based medical image registration</article-title>. <source><italic toggle="yes">IEEE Trans. Med. Imaging</italic></source>
<volume>29</volume>, <fpage>196</fpage>-<lpage>205</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2009.2035616</pub-id><pub-id pub-id-type="pmid">19923044</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laine</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Tosheva</surname>, <given-names>K. L.</given-names></string-name>, <string-name><surname>Gustafsson</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Gray</surname>, <given-names>R. D. M.</given-names></string-name>, <string-name><surname>Almada</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Albrecht</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Risa</surname>, <given-names>G. T.</given-names></string-name>, <string-name><surname>Hurtig</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Lindås</surname>, <given-names>A.-C.</given-names></string-name>, <string-name><surname>Baum</surname>, <given-names>B.</given-names></string-name><etal>et al.</etal></person-group> (<year>2019</year>). <article-title>NanoJ: a high-performance open-source super-resolution microscopy toolbox</article-title>. <source><italic toggle="yes">J. Phys. D Appl. Phys.</italic></source>
<volume>52</volume>, <fpage>163001</fpage>. <pub-id pub-id-type="doi">10.1088/1361-6463/ab0261</pub-id><pub-id pub-id-type="pmid">33191949</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laine</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Arganda-Carreras</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Henriques</surname>, <given-names>R.</given-names></string-name> and <string-name><surname>Jacquemet</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2021</year>). <article-title>Avoiding a replication crisis in deep-learning-based bioimage analysis</article-title>. <source><italic toggle="yes">Nat. Methods</italic></source>
<volume>18</volume>, <fpage>1136</fpage>-<lpage>1144</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01284-3</pub-id><pub-id pub-id-type="pmid">34608322</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Linkert</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Rueden</surname>, <given-names>C. T.</given-names></string-name>, <string-name><surname>Allan</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Burel</surname>, <given-names>J.-M.</given-names></string-name>, <string-name><surname>Moore</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Patterson</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Loranger</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Moore</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Neves</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Macdonald</surname>, <given-names>D.</given-names></string-name><etal>et al.</etal></person-group> (<year>2010</year>). <article-title>Metadata matters: access to image data in the real world</article-title>. <source><italic toggle="yes">J. Cell Biol.</italic></source>
<volume>189</volume>, <fpage>777</fpage>-<lpage>782</lpage>. <pub-id pub-id-type="doi">10.1083/jcb.201004104</pub-id><pub-id pub-id-type="pmid">20513764</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mccormick</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Liu</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ibanez</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Jomier</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Marion</surname>, <given-names>C</given-names></string-name></person-group>. (<year>2014</year>). <article-title>ITK: enabling reproducible research and open science</article-title>. <source><italic toggle="yes">Front. Neuroinform.</italic></source><volume>8</volume>, <fpage>13</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2014.00013</pub-id><comment><uri xlink:href="https://www.frontiersin.org/articles/10.3389/fninf.2014.00013">https://www.frontiersin.org/articles/10.3389/fninf.2014.00013</uri></comment>.<pub-id pub-id-type="pmid">24600387</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Parslow</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Cardona</surname>, <given-names>A.</given-names></string-name> and <string-name><surname>Bryson-Richardson</surname>, <given-names>R. J.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Sample drift correction following 4D confocal time-lapse imaging</article-title>. <source><italic toggle="yes">J. Vis. Exp.</italic></source>
<volume>86</volume>, <fpage>51086</fpage>. <pub-id pub-id-type="doi">10.3791/51086</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Popovic</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Miihkinen</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ghimire</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Saup</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Grönloh</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Ball</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Goult</surname>, <given-names>B.T.</given-names></string-name>, <string-name><surname>Ivaska</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Jacquemet</surname>, <given-names>G.</given-names></string-name></person-group> (<year>2023</year>). <article-title>Myosin-X recruits lamellipodin to filopodia tips</article-title>. <source><italic toggle="yes">J. Cell Sci.</italic></source>
<volume>136</volume>, <fpage>jcs260574</fpage>. <pub-id pub-id-type="doi">10.1242/jcs.260574</pub-id><pub-id pub-id-type="pmid">36861887</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Postma</surname>, <given-names>M.</given-names></string-name> and <string-name><surname>Goedhart</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2019</year>). <article-title>PlotsOfData—A web app for visualizing data together with their summaries</article-title>. <source><italic toggle="yes">PLoS Biol.</italic></source>
<volume>17</volume>, <fpage>e3000202</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pbio.3000202</pub-id><pub-id pub-id-type="pmid">30917112</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Preibisch</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Saalfeld</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schindelin</surname>, <given-names>J.</given-names></string-name> and <string-name><surname>Tomancak</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2010</year>). <article-title>Software for bead-based registration of selective plane illumination microscopy data</article-title>. <source><italic toggle="yes">Nat. Methods</italic></source>
<volume>7</volume>, <fpage>6</fpage>. <pub-id pub-id-type="doi">10.1038/nmeth0610-418</pub-id><pub-id pub-id-type="pmid">20038950</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Preibisch</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Amat</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Stamataki</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Sarov</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Singer</surname>, <given-names>R. H.</given-names></string-name>, <string-name><surname>Myers</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Tomancak</surname>, <given-names>P.</given-names></string-name></person-group> (<year>2014</year>). <article-title>Efficient Bayesian-based multiview deconvolution</article-title>. <source><italic toggle="yes">Nat. Methods</italic></source>
<volume>11</volume>, <fpage>645</fpage>-<lpage>648</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2929</pub-id><pub-id pub-id-type="pmid">24747812</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Puttur</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Denney</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Gregory</surname>, <given-names>L. G.</given-names></string-name>, <string-name><surname>Vuononvirta</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Oliver</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Entwistle</surname>, <given-names>L. J.</given-names></string-name>, <string-name><surname>Walker</surname>, <given-names>S. A.</given-names></string-name>, <string-name><surname>Headley</surname>, <given-names>M. B.</given-names></string-name>, <string-name><surname>Mcghee</surname>, <given-names>E. J.</given-names></string-name>, <string-name><surname>Pease</surname>, <given-names>J. E.</given-names></string-name><etal>et al.</etal></person-group> (<year>2019</year>). <article-title>Pulmonary environmental cues drive group 2 innate lymphoid cell dynamics in mice and humans</article-title>. <source><italic toggle="yes">Sci. Immunol.</italic></source>
<volume>4</volume>, <fpage>eaav7638</fpage>. <pub-id pub-id-type="doi">10.1126/sciimmunol.aav7638</pub-id><pub-id pub-id-type="pmid">31175176</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schindelin</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Arganda-Carreras</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Frise</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Kaynig</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Longair</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Pietzsch</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Preibisch</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Rueden</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Saalfeld</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schmid</surname>, <given-names>B.</given-names></string-name><etal>et al.</etal></person-group> (<year>2012</year>). <article-title>Fiji: an open-source platform for biological-image analysis</article-title>. <source><italic toggle="yes">Nat. Methods</italic></source>
<volume>9</volume>, <fpage>676</fpage>-<lpage>682</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id><pub-id pub-id-type="pmid">22743772</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sun</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2002</year>). <article-title>The Hartley transform applied to particle image velocimetry</article-title>. <source><italic toggle="yes">Meas. Sci. Technol.</italic></source>
<volume>13</volume>, <fpage>1996</fpage>. <pub-id pub-id-type="doi">10.1088/0957-0233/13/12/326</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Von Chamier</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Laine</surname>, <given-names>R. F.</given-names></string-name>, <string-name><surname>Jukkala</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Spahn</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Krentzel</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Nehme</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Lerche</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Hernández-Pérez</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Mattila</surname>, <given-names>P. K.</given-names></string-name>, <string-name><surname>Karinou</surname>, <given-names>E.</given-names></string-name><etal>et al.</etal></person-group> (<year>2021</year>). <article-title>Democratising deep learning for microscopy with ZeroCostDL4Mic</article-title>. <source><italic toggle="yes">Nat. Commun.</italic></source>
<volume>12</volume>, <fpage>2276</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-22518-0</pub-id><pub-id pub-id-type="pmid">33859193</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Von Wangenheim</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Hauschild</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Fendrych</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Barone</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Benková</surname>, <given-names>E.</given-names></string-name> and <string-name><surname>Friml</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Live tracking of moving samples in confocal microscopy for vertically grown roots</article-title>. <source><italic toggle="yes">ELife</italic></source>
<volume>6</volume>, <fpage>e26792</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.26792</pub-id><pub-id pub-id-type="pmid">28628006</pub-id></mixed-citation>
    </ref>
    <ref id="JCS260728C19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weigert</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>U.</given-names></string-name>, <string-name><surname>Boothe</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>Müller</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Dibrov</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Jain</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Wilhelm</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Schmidt</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Broaddus</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Culley</surname>, <given-names>S.</given-names></string-name><etal>et al.</etal></person-group> (<year>2018</year>). <article-title>Content-aware image restoration: pushing the limits of fluorescence microscopy</article-title>. <source><italic toggle="yes">Nat. Methods</italic></source>
<volume>15</volume>, <fpage>1090</fpage>-<lpage>1097</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0216-7</pub-id><pub-id pub-id-type="pmid">30478326</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
