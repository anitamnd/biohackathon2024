<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?covid-19-tdm?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7808115</article-id>
    <article-id pub-id-type="pmid">33443729</article-id>
    <article-id pub-id-type="publisher-id">1515</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-020-01515-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SimplePhy: An open-source tool for quick online perception experiments</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Lago</surname>
          <given-names>Miguel A.</given-names>
        </name>
        <address>
          <email>lago@psych.ucsb.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.133342.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9676</institution-id><institution>University of California, </institution></institution-wrap>Santa Barbara, Santa Barbara, CA 93105 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2021</year>
    </pub-date>
    <volume>53</volume>
    <issue>4</issue>
    <fpage>1669</fpage>
    <lpage>1676</lpage>
    <history>
      <date date-type="accepted">
        <day>19</day>
        <month>11</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Psychonomic Society, Inc. 2021</copyright-statement>
      <license>
        <license-p>This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Because of the COVID-19 pandemic, researchers are facing unprecedented challenges that affect our ability to run in-person experiments. With mandated social distancing in a controlled laboratory environment, many researchers are searching for alternative options to conduct research, such as online experimentation. However, online experimentation comes at a cost; learning online tools for building and publishing psychophysics experiments can be complicated and time-consuming. This learning cost is unfortunate because researchers typically only need to use a small percentage of these tools’ capabilities, but they still have to deal with these systems’ complexities (e.g., complex graphical user interfaces or difficult programming languages). Furthermore, after the experiment is built, researchers often have to find an online platform compatible with the tool they used to program the experiment. To simplify and streamline the online process of programming and hosting an experiment, I have created SimplePhy. SimplePhy can save researchers’ time and energy by allowing them to create a study in just a few clicks. All researchers have to do is select among a few experiment settings and upload the stimuli. SimplePhy is able to run most psychophysical perception experiments that require mouse clicks and button presses. In addition to collecting online behavioral data, SimplePhy can also collect information regarding the estimated viewing distance between the participant and the monitor, the screen size, and the experimental trial’s timing—features not always offered in other online platforms. Overall, SimplePhy is a simple, free, open-source tool (code can be found here: <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/malago/simplephy">https://gitlab.com/malago/simplephy</ext-link>) aimed to help labs conduct their experiments online.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Online</kwd>
      <kwd>Perception</kwd>
      <kwd>Psychophysics</kwd>
      <kwd>Experiment</kwd>
      <kwd>Open-source</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Psychonomic Society, Inc. 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">In the age of online services, many researchers use the Internet to conduct their experiments. Compared to face-to-face experiments, running an experiment online has obvious advantages. For example, researchers are able to collect data from a diverse population (Reinecke &amp; Gajos, <xref ref-type="bibr" rid="CR10">2015</xref>), avoid scheduling constraints, and have the potential for a significant increase in the number of participants. However, when it comes to online platforms, there are many challenges that psychophysical perception experiments face. Many perceptual decisions are sensitive to variables such as to monitor specifications, stimulus size, viewing distance, room illumination, and monitor brightness calibration (Birnbaum &amp; Birnbaum, <xref ref-type="bibr" rid="CR1">2000</xref>; de Leeuw &amp; Motz, <xref ref-type="bibr" rid="CR3">2016</xref>). While some of these inconsistencies might balance out when running more participants (e.g., the differences in monitor sizes), many of these other factors require extra steps to alleviate it that are unreasonable to expect participants to perform (e.g., downloading software to get the monitor brightness or color calibration). To transition from in-person to online experimentation, researchers need to be aware of the advantages and disadvantages of running online experiments, carefully select the experiment builder that matches their requirements, and find a way to make their experiments publicly available on the Internet (see (Grootswagers, <xref ref-type="bibr" rid="CR4">2020</xref>) and (Sauter, Draschkow, &amp; Mack, <xref ref-type="bibr" rid="CR11">2020</xref>) for a comprehensive review of the challenges in online experiment design and deployment).</p>
    <p id="Par3">Currently, there are many options to build psychophysical perception experiments online. The most popular options are PsychoJS (Peirce, <xref ref-type="bibr" rid="CR9">2007</xref>), OSWeb (Mathôt, Schreij, &amp; Theeuwes, <xref ref-type="bibr" rid="CR8">2012</xref>), jsPsych (de Leeuw, <xref ref-type="bibr" rid="CR2">2015</xref>), PsyToolkit (Stoet, <xref ref-type="bibr" rid="CR12">2010</xref>), and lab.js (Henninger, Shevchenko, Mertens, Kieslich, &amp; Hilbig, <xref ref-type="bibr" rid="CR5">2020</xref>). All of these tools require researchers to learn how to design and deploy their experiment with either a graphical user interface (GUI), a code (whether it is an ad hoc programming language or JavaScript), or both. Additionally, many of these tools allow the experiment design to be tweaked by changing the underlying HTML, CSS, and JavaScript code, which are the standard markup, styling, and scripting languages used to render a website. However, the major disadvantage is that they typically have a steep learning curve that keeps researchers from deploying their experiments in a timely manner.</p>
    <p id="Par4">GUI-based tools often require knowledge of the underlying programming language for more complex experiment designs (which may ultimately defeat the original intent of using a GUI-based system) and can be cumbersome to create (e.g., finding the location of the desired functionality may require a large number of repetitive actions: clicks, text input or drag-and-drop actions). Alternatively, programming-based tools require the researcher to learn the programming language or libraries and type the list of orders that the computer has to execute. For researchers, this learning is often done at the beginning of their graduate studies and may take months or even years for less experienced individuals. Because of the time constraints, deadlines, and prior obligations many researchers have, learning how to code at certain stages in one’s academic career is impractical, if not impossible, for the less computer savvy. Finally, more experienced coders can also choose to program their own experiments using the standard JavaScript libraries.</p>
    <p id="Par5">After programming an experiment, researchers are then tasked with making their experiments available online for participants. Current online tools provide a variety of solutions. For example, jsPsych generates the files necessary to run the experiment and can also integrate with the platform Cognition<xref ref-type="fn" rid="Fn1">1</xref>. Similarly, OSWeb can export the experiment to a self-hosted JATOS platform to publish it. PsychoJS and lab.js also provide a way to connect with hosting platforms like Pavlovia<xref ref-type="fn" rid="Fn2">2</xref> to upload the designed experiments. Alternatively, many of these tools allow for downloading the experiment files, leaving the researcher with the task of providing a web hosting service to publish the experiment. This can be beneficial for some experiments since the data collection is controlled by the researcher that can choose which platform to trust to store the data (or self-host it) but can also be inconvenient if that is not a concern. See Table <xref rid="Tab1" ref-type="table">1</xref> for a summary of experiment builders and their integrated publishing platforms.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Comparison of experiment-builders (PsychoPy, OSWeb, jsPsych, PsyToolkit, lab.js, and SimplePhy) including the presence of a graphical user interface (GUI), the coding language necessary to build experiments, and the integrated platforms to publish the experiment on the Internet</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Tool</th><th>GUI</th><th>Coding</th><th>Integrated publishing platform</th></tr></thead><tbody><tr><td><bold>PsychoJS</bold></td><td>✓</td><td>JavaScrip (for advanced purposes)</td><td>Pavlovia</td></tr><tr><td><bold>OSWeb</bold></td><td>✓</td><td>JavaScript (for advanced purposes)</td><td>JATOS</td></tr><tr><td><bold>jsPsych</bold></td><td>✘</td><td>JavaScript</td><td>JATOS, Pavlovia, Cognition</td></tr><tr><td><bold>PsyToolkit</bold></td><td>✘</td><td>Own language</td><td>JATOS, PsyToolkit</td></tr><tr><td><bold>lab.js</bold></td><td>✓</td><td>JavaScript</td><td>JATOS, Open Lab, Pavlovia, Qualtrics</td></tr><tr><td><bold>SimplePhy</bold></td><td>✓</td><td>✘</td><td>SimplePhy</td></tr></tbody></table></table-wrap></p>
    <p id="Par8">The process of recruiting participants can also be done online. Pavlovia, Mechanical Turk<xref ref-type="fn" rid="Fn3">3</xref>, Open Lab<xref ref-type="fn" rid="Fn4">4</xref>, and Labvanced<xref ref-type="fn" rid="Fn5">5</xref> are viable options that provide a place for researchers to host experiments or recruit participants. Some experiment builders (e.g., PsychoJS, OSWeb, and lab.js) can directly connect with these hosting platforms to automatically export the experiments, making this step straightforward. However, all these platforms come at a financial cost and some of them can only be integrated with specific builders. Alternatively, if the researcher is in a university, recruitment systems like SONA, a widespread recruitment and scheduling system for university subject pools, can also be used.</p>
    <p id="Par12">In this paper, I present SimplePhy, a simple open-source tool that integrates the process of creating and publishing online psychophysics experiments on the same platform. While existing tools allow for much flexibility in terms of what they can do, SimplePhy has been designed with basic perception experiments as its core and incorporates mechanisms to estimate the viewing distance, something crucial for some studies. SimplePhy has a simple GUI interface (Fig. <xref rid="Fig1" ref-type="fig">1</xref>) and requires no prior coding knowledge. The available options allow for a wide array of psychophysics experiment designs that either require keyboard presses or mouse clicks to indicate responses (e.g., visual search and multiple-alternative forced-choice, MAFC). Additionally, it can host the experiments for free after they have been created, something that not all platforms currently offer.<fig id="Fig1"><label>Fig. 1</label><caption><p>SimplePhy’s initial screen with the Participant ID field and Start button for participants. The Create experiment box can be expanded to show the available options to create an experiment. Instructions on how to use the latest version of the tool can be found in the GitLab repository wiki. <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/malago/simplephy/-/wikis/home">https://gitlab.com/malago/simplephy/-/wikis/home</ext-link></p></caption><graphic xlink:href="13428_2020_1515_Fig1_HTML" id="MO1"/></fig></p>
  </sec>
  <sec id="Sec2">
    <title>From zero to data</title>
    <p id="Par13">To design an experiment with SimplePhy, researchers choose from a small set of options to configure the experiment and upload the stimuli images so that SimplePhy can link them (see Fig. <xref rid="Fig1" ref-type="fig">1</xref> for a list of available options). Additionally, SimplePhy offers the option to estimate the participant’s viewing distance by using the Virtual Chinrest method described by Li, Joo, Yeatman, and Reinecke (<xref ref-type="bibr" rid="CR7">2020</xref>) (see Section II.F for more information). Once the experiment is generated, participants can then complete it in the same online platform by accessing the corresponding URL. Data is stored on a secure website and locked behind a password. With the known password, researchers can access and download the data at any time to analyze it.</p>
    <sec id="Sec3">
      <title>Experimental design</title>
      <p id="Par14">The first step in experimental design is to find what kind of experiment they need to conduct and what kind of stimuli needs to be presented to participants. SimplePhy is compatible with stimuli in the form of static images (e.g., pictures or shapes) or videos (e.g., movies or animations). The experiment design allows for a few standard response options: mouse clicks, keyboard responses, confidence rating responses, and multiple alternative forced-choice (MAFC) responses. To facilitate participants with more information about the experiment, SimplePhy will also allow the researcher to include a link to a document with instructions<xref ref-type="fn" rid="Fn6">6</xref>.</p>
    </sec>
    <sec id="Sec4">
      <title>Stimuli</title>
      <p id="Par16">After the options for experimental design are selected, researchers then need to create a specific folder structure containing the stimuli files that will be used by SimplePhy. The folder structure must strictly follow the organization shown in Table <xref rid="Tab2" ref-type="table">2</xref>. Each experiment may have multiple conditions that contain multiple trials. Once the folder structure is finished, researchers must upload it to a Google Drive folder and make it publicly visible. SimplePhy will collect and save the trial files’ public URL of each trial. Because stimuli files are stored in a Google Drive server, SimplePhy can remotely link those files (e.g., videos or images). Since they will be downloaded from the Google Drive server, this allows for reduced bandwidth load on the SimplePhy web server.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Folder structure for stimuli import to be created in a Google Drive public folder</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Root folder: one folder per condition</th><th>Condition level: one folder per trial</th><th>Trial level: stimulus information</th></tr></thead><tbody><tr><td/><td/><td>Image #1 (.jpg|.webm|.mp4)</td></tr><tr><td>Condition #1</td><td>Trial #1</td><td><italic>Image #2 (.jpg|.webm|.mp4)</italic></td></tr><tr><td><italic>Condition #2</italic></td><td>Trial #2</td><td>…</td></tr><tr><td><italic>Condition #3</italic></td><td>Trial #3</td><td><italic>Image #N (.jpg|.webm|.mp4)</italic></td></tr><tr><td>…</td><td>…</td><td><italic>Information file (.json)</italic></td></tr><tr><td><italic>Condition #N</italic></td><td>Trial #N</td><td><italic>Feedback image (feedback.jpg|feedback.webm|feedback.mp4)</italic></td></tr></tbody></table><table-wrap-foot><p>This structure may contain one or more folders for each corresponding condition. Each condition must contain one folder per trial. Each trial must contain one or more images/videos and, optionally, an information file and feedback image. Italics denote optional entries</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec5">
      <title>Experiment options</title>
      <p id="Par17">To create an experiment, researchers need to: 1) select the options for their experiment design, 2) indicate the Google Drive folder in which they uploaded the stimuli, and 3) click on the “Create experiment” button. In order to streamline this process, SimplePhy provides a GUI startup screen that allows the users to select what specific parameters they want to use (Fig. <xref rid="Fig1" ref-type="fig">1</xref>).</p>
      <p id="Par18">To further customize an experiment, an optional JSON<xref ref-type="fn" rid="Fn7">7</xref> file (a widely used format for data interchange in JavaScript applications) may be added to include additional information about the corresponding trial within each trial folder. Within this JSON file, users may include information to customize each trial’s custom response options, instructions, allowed keyboard presses, and correct response. To include extra information for the results file, observers simply need to indicate what variables need to be included. For example, in a visual search task, it might be useful information about the target location within an image or the identity of the target. Any information stored in this information file will then be added to the participant’s results file to facilitate the posterior analyses. Furthermore, this information file allows for advanced customization of the available responses at the end of each trial (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>). Correct responses of each trial can also be included in the information file. If included, SimplePhy will save to the results file whether a response was correct or incorrect.<fig id="Fig2"><label>Fig. 2</label><caption><p>Advanced custom response options. <italic>From top to bottom</italic>: text open response, confidence rating, custom options, rating slider for more than ten ratings</p></caption><graphic xlink:href="13428_2020_1515_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par20">While these options offer more flexibility to SimplePhy, this file does not interfere with GUI’s simplicity. To facilitate the generation of these files, SimplePhy includes a JSON file generator GUI in case the user is unfamiliar with creating JSON files. See the GitLab wiki for more examples<xref ref-type="fn" rid="Fn8">8</xref> of currently available advanced options.</p>
    </sec>
    <sec id="Sec6">
      <title>Running an experiment</title>
      <p id="Par22">Once an experiment is created, SimplePhy will generate two URLs. The first URL is a direct link to the experiment, which the researcher can send to the participants. The second URL is a direct link to access all participants’ data. In order to download participants’ data, a password must be first established when entering the results section for the first time to ensure that no one can access the data without permission.</p>
      <p id="Par23">When participants receive the experiment URL, they will be shown a screen with the experiment title and an instructions link (if provided by the researcher; see Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Participants can type in their participant ID in the text box. However, if participants do not type in a participant ID, one will be generated automatically. If the distance estimation calibration was enabled in the experiment design, the calibration screen would next be shown. After this, the web browser’s full-screen mode will activate, and the participant will be presented with a trial. If the researcher uses the feedback option, a feedback screen will be provided after the participant finishes each trial. See Fig. <xref rid="Fig3" ref-type="fig">3</xref> for an outline example of one trial.<fig id="Fig3"><label>Fig. 3</label><caption><p>Outline example of one experiment: <italic>Main screen:</italic> Where participants type their participant ID. <italic>Loading trial:</italic> A filler screen that participants see as the files necessary for each trial are loaded. <italic>Trial run:</italic> Where the trial stimuli are displayed and the participants either press a key, make a mouse click, or wait for a designated timeout when viewing the stimuli. <italic>Response screen:</italic> Where participants can respond to the questions regarding the previously shown stimulus. <italic>Feedback Screen</italic>: If the advanced feedback option is used, participants will see if their response was correct or incorrect</p></caption><graphic xlink:href="13428_2020_1515_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par24">In addition to all these features, SimplePhy includes safeguards to detect whether participants switched to a different software or browser tab once they have started the experiment. If the participant leaves the experiment, data collection immediately stops, and the current data is stored. If a participant returns to the experiment, a cookie in the web-browser will inform SimplePhy that the experiment needs to be resumed (trial start timestamp within the results will indicate if this happens). Participants will then be asked whether they want to continue where they left off or to restart the experiment.</p>
    </sec>
    <sec id="Sec7">
      <title>Accessing the results</title>
      <p id="Par25">SimplePhy provides a results page where the researcher can view an overall summary page with the current number of participants and how much of the experiment they completed (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). A download button will allow the researcher to download a compressed file with all participants’ data (in either JSON or CSV format). In order to keep the service free for researchers, currently, participants’ data that is older than 1 month may be subjected to be automatically removed from the storage server. Data is stored both in SimplePhy’s web server and, for backup purposes, in a Google Firebase service. Although stored data is locked behind a password, it is not encrypted: researchers should avoid using identifiable information (i.e., use anonymous participant IDs). Researchers can download participants’ data at any time, as long as they are in the server, and are encouraged to do so at least every 30 days to avoid data loss.<fig id="Fig4"><label>Fig. 4</label><caption><p>Example of the results page for an experiment. Researchers can set a password to see and download participants’ data. <italic>From left to right</italic>, tab icons correspond to participants, download data, edit experiment configuration, change password, and delete the experiment and its associated data</p></caption><graphic xlink:href="13428_2020_1515_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec8">
      <title>Viewing distance</title>
      <p id="Par26">In perception experiments, controlling the participant’s viewing distance to the monitor, the monitor screen size, and the screen resolution is critical for researchers. Many perception experiments rely on a specific viewing distance or rely on knowing the stimulus size in degrees of visual angle. In order to account for these problems, SimplePhy is the first experiment builder that replicates the virtual chinrest developed by Li et al. (<xref ref-type="bibr" rid="CR7">2020</xref>). Li and colleagues developed a way to compute viewing distance with a credit card. In particular, the credit card computes the pixel size in the screen and combines it with the human retinal blind spot to compute the pixels per degree of visual angle. In SimplePhy, researchers have the option to have participants place a credit card (which is universally the same size, 8.56 cm width) on the screen and adjust a slider to match the width of the credit card. SimplePhy uses this information to calculate how many pixels correspond to the size of the credit card, which leads to a value of pixels per centimeter for the participant’s monitor (<italic>px</italic><sub><italic>cm</italic></sub>). After the measurement is taken, SimplePhy draws a cross on the right part of the screen and asks participants to fixate the cross while closing their right eye. A circle then moves from the cross leftward until it subjectively disappears, indicating that the circle is in the participant’s blind spot. Participants are then asked to press the spacebar when the circle disappears. This process is repeated five times, and the average distance is computed from these outcomes. As suggested by Li et al., SimplePhy assumes that the retinal blind spot is at 13 degrees of visual angle (<italic>dva</italic>) (Wang et al., <xref ref-type="bibr" rid="CR13">2017</xref>) and use that to estimate the number of pixels per degree (<italic>px</italic><sub><italic>dva</italic></sub>). Finally, using both pixels per centimeter and pixels per degree, SimplePhy infers the viewing distance of the participant via the following calculation:<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\displaystyle \begin{array}{c}p{x}_{cm}=\frac{credit\ \mathit{\operatorname{card}}\  size\ (px)}{8.56\  cm}\\ {}\kern8.75em p{x}_{dva}=\frac{distance\  at\  which\ circle\ disapears\ (px)}{13\  dva}\\ {} viewing\ distance=\frac{\tan \left(1\  dva\right)p{x}_{cm}}{p{x}_{dva}}\kern11em \end{array}} $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi mathvariant="italic">cm</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">credit</mml:mtext><mml:mspace width="0.25em"/><mml:mo mathvariant="italic">card</mml:mo><mml:mspace width="0.25em"/><mml:mtext mathvariant="italic">size</mml:mtext><mml:mspace width="0.25em"/><mml:mfenced close=")" open="("><mml:mi mathvariant="italic">px</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mn>8.56</mml:mn><mml:mspace width="0.25em"/><mml:mi mathvariant="italic">cm</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="8.75em"/><mml:mi>p</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi mathvariant="italic">dva</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">distance</mml:mtext><mml:mspace width="0.25em"/><mml:mi mathvariant="italic">at</mml:mi><mml:mspace width="0.25em"/><mml:mtext mathvariant="italic">which circle disapears</mml:mtext><mml:mspace width="0.25em"/><mml:mfenced close=")" open="("><mml:mi mathvariant="italic">px</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mn>13</mml:mn><mml:mspace width="0.25em"/><mml:mi mathvariant="italic">dva</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext mathvariant="italic">viewing distance</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>tan</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.25em"/><mml:mi mathvariant="italic">dva</mml:mi></mml:mrow></mml:mfenced><mml:mi>p</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi mathvariant="italic">cm</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi mathvariant="italic">dva</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="11em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2020_1515_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par27">This calibration process is optional and, if activated, will be completed at the start of the experiment. Additionally, a recalibration process can be required every certain number of minutes (controlled by the researcher) during the experiment. In each recalibration, a new distance is recorded, along with the time of the recalibration (see Fig. <xref rid="Fig5" ref-type="fig">5</xref> for the two steps of the calibration procedure).<fig id="Fig5"><label>Fig. 5</label><caption><p>Credit-card calibration screen (<italic>left</italic>). Retinal blind spot calibration screen (<italic>right</italic>)</p></caption><graphic xlink:href="13428_2020_1515_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec9">
    <title>Limitations</title>
    <p id="Par28">While there are many options (and customizable options) in SimplePhy, it has its limitations compared to other online experimental platforms. Here, I describe some workarounds to overcome these limitations.</p>
    <sec id="Sec10">
      <title>Practice trials</title>
      <p id="Par29">A “practice experiment” can be generated using SimplePhy to present practice trials to the participant. This “practice experiment” would simply be a shorter experiment with fewer trials. Participants could then be instructed to complete the “practice experiment” before the actual experiment. Alternatively, a researcher can design an “instructions experiment” that shows non-randomized trials with screenshots or text intertwined with actual trials. With this experiment, participants can try the experiment while learning about the task and the tool. In both options, the researcher can set a threshold to assess a participant’s performance. If their performance exceeds it, that indicates that the participant understood the given task, and then the researcher can send the link to the actual experiment. Unfortunately, since SimplePhy is always blind to the task, it is the researcher who has to download the participant’s data and do a quick analysis to check if the participant understood the instructions. These solutions require extra micro-managing by the researcher but are ways to help ensure that participants understand the experiment.</p>
    </sec>
    <sec id="Sec11">
      <title>Between-subject and within-subject design</title>
      <p id="Par30">SimplePhy does not offer a native way to conduct between-subject experiments. However, in this case, researchers can divide the experiment in two and provide the corresponding link to one of them to each participant. This comes with the cost of having extra management from the researcher. For within-subject design, SimplePhy offers different ways to control how conditions are shown (e.g., all randomized, conditions are blocked and randomized, or conditions are blocked and sequential).</p>
    </sec>
    <sec id="Sec12">
      <title>Participation control</title>
      <p id="Par31">In order to check that only authorized participants can perform the experiment, researchers can give them a specific, not easily guessed, participant ID (e.g., S1x45RL). The researcher can provide this ID to the participant and then double-check it to ensure that each personal ID is used and that it is only used once.</p>
    </sec>
    <sec id="Sec13">
      <title>Data analysis</title>
      <p id="Par32">Given the simplicity of the tool, SimplePhy does not assist researchers with data analysis (besides simple response checks, if the advanced options are used). In general, SimplePhy does not have any information about the experiment’s purpose or how the participants’ responses should be processed to determine if they are correct or wrong. For example, if mouse clicks are enabled, SimplePhy will record where the observers click on the screen, but not whether participants click in the “correct” location. The researcher must perform posterior analyses such as these after data is collected. However, simple responses can be analyzed by comparing participants’ responses with the correct response in the trial information JSON file, in which case SimplePhy will show the participant a “correct” or “incorrect” screen if they press the correct key or choose the correct options in the response screen. An optional feedback image can be added to each trial to inform the participants what the correct answer was, regardless of their answer.</p>
    </sec>
    <sec id="Sec14">
      <title>Recruiting participants</title>
      <p id="Par33">SimplePhy leaves this task to the researcher, who will have to provide the experiment link to the participant. Since this cannot be done within the SimplePhy platform, recruiting must be outsourced to platforms like Mechanical Turk or SONA. This recruiting platforms only need a link to the experiment, making SimplePhy’s generated unique link ideal for this task.</p>
    </sec>
    <sec id="Sec15">
      <title>Monitor calibrations</title>
      <p id="Par34">For all online experiments, SimplePhy included, the brightness calibration of the monitor cannot be controlled. For perception experiments that heavily rely on similar viewing conditions for all participants, this may result in noisy or biased data and should be something to consider if choosing an online tool. Unfortunately, with a web-browser-based experiment, this is inherently unachievable in any online tool. The only option would be to ask the participant to input their monitor’s model, luminance, and gamma function. While doable, this likely would create another entry barrier that might reduce the number of potential participants<xref ref-type="fn" rid="Fn9">9</xref>.</p>
    </sec>
    <sec id="Sec16">
      <title>Other customizations</title>
      <p id="Par36">The current version of SimplePhy offers a limited amount of trial customization options (see section II.C for more details). If researchers need to control for other variables like trials that adapt to the responses, custom backgrounds, or more complex experiment logic, SimplePhy does not offer a solution for them. Although the tool might add some functionality in the future, it will maintain the simplicity as its core value: SimplePhy will never know what the purpose of the experiment is or what the meaning of correct or incorrect trial is (this would require some coding and it is something that other tools can do).</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Discussion</title>
    <p id="Par37">Designing a perception experiment can be a tricky task. Researchers are often tasked with: 1) making sure that data is effectively collected, 2) accounting/avoiding possible confounding variables that may arise from specific experimental designs (e.g., training effects, memory or vigilance decrement over time), 3) programming the scripts needed for their experiments, and 4) optimizing/fixing the code. (e.g., fixing bugs in the code, correcting non-optimized loops, recovering missing data, or accounting for timing inaccuracies). The purpose of SimplePhy is to <italic>simplify</italic> this entire process and reduce the time between psycho<italic>physics</italic> experiment design and data collection. Researchers only need to design their experiment, generate the stimuli, upload them, and select which options they want SimplePhy to use. The SimplePhy website handles the rest. More advanced options are available by modifying the trial information JSON file (with a helpful GUI interface that can be used to create it) to provide better feedback, add multiple questions, and choose different response types.</p>
    <p id="Par38">SimplePhy’s purpose is to complement the existing experiment builders. If the researcher’s experiment is simple and does not need many customizations, SimplePhy can be useful and speed up the data collection. For more complex experiments, researchers can always use the other experiment builders mentioned above, with the caveat that they might need to spend more time creating or programming the experiment logic. SimplePhy can also be a useful tool for piloting studies quickly, whether it is in a remote environment or the in-person lab.</p>
  </sec>
  <sec id="Sec18">
    <title>Conclusions</title>
    <p id="Par39">SimplePhy is a simple and free way to create psychophysics perception experiments without the hassle of programming and finding a host for an online experiment. It is designed to provide a seamless experience to generate simple experiment designs suitable for many labs and research groups. While current, complete psychophysics experiment builder tools provide more flexibility than SimplePhy, they come with the caveat of having to be more careful while programming the experiment. These tools may ultimately increase the amount of preparation time required by the researcher and the chance of introducing bugs into the code. If the psychophysics experiment does not require complex workflows or non-standard responses, SimplePhy is a tool that researchers should consider. Finally, SimplePhy is an open-source tool, and its source code is publicly available. It is easily extendable, free to use, and will continue to be developed in the future, adding options for more experiment designs while keeping the simplicity as its core value and appeal.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par6">
        <ext-link ext-link-type="uri" xlink:href="https://www.cognition.run/">https://www.cognition.run/</ext-link>
      </p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par7">
        <ext-link ext-link-type="uri" xlink:href="https://www.pavlovia.org/">https://www.pavlovia.org/</ext-link>
      </p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p id="Par9">
        <ext-link ext-link-type="uri" xlink:href="https://www.mturk.com/">https://www.mturk.com/</ext-link>
      </p>
    </fn>
    <fn id="Fn4">
      <label>4</label>
      <p id="Par10">
        <ext-link ext-link-type="uri" xlink:href="https://open-lab.online/">https://open-lab.online/</ext-link>
      </p>
    </fn>
    <fn id="Fn5">
      <label>5</label>
      <p id="Par11">
        <ext-link ext-link-type="uri" xlink:href="https://www.labvanced.com/">https://www.labvanced.com/</ext-link>
      </p>
    </fn>
    <fn id="Fn6">
      <label>6</label>
      <p id="Par15">Although the researcher is responsible for providing this document, solutions such as a public Google Document, a YouTube video or a ZIP file hosted in a service like Dropbox or OneDrive can be used.</p>
    </fn>
    <fn id="Fn7">
      <label>7</label>
      <p id="Par19">
        <ext-link ext-link-type="uri" xlink:href="https://www.json.org/json-en.html">https://www.json.org/json-en.html</ext-link>
      </p>
    </fn>
    <fn id="Fn8">
      <label>8</label>
      <p id="Par21">
        <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/malago/simplephy/-/wikis/Experiment-options">https://gitlab.com/malago/simplephy/-/wikis/Experiment-options</ext-link>
      </p>
    </fn>
    <fn id="Fn9">
      <label>9</label>
      <p id="Par35">One can assume that monitors are calibrated with a standard gamma correction of 2.2 (Hwung, Wang, &amp; Su, <xref ref-type="bibr" rid="CR6">1995</xref>) with the luminance that the participant finds more comfortable.</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The author thanks Dr. Stephen Adamo, Dr. Miguel Eckstein, and all the VIU lab at UC Santa Barbara for their support during the development of this tool and helpful feedback on the manuscript writing. This research was funded by the National Institute of Health grants R01 EB018958 and R01 EB026427.</p>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <mixed-citation publication-type="other">Birnbaum, M. H., &amp; Birnbaum, M. O. (2000). <italic>Psychological experiments on the Internet</italic>. Elsevier.</mixed-citation>
    </ref>
    <ref id="CR2">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de Leeuw</surname>
            <given-names>JR</given-names>
          </name>
        </person-group>
        <article-title>jsPsych: A JavaScript library for creating behavioral experiments in a Web browser</article-title>
        <source>Behavior Research Methods</source>
        <year>2015</year>
        <volume>47</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-014-0458-y</pub-id>
        <?supplied-pmid 24683129?>
        <pub-id pub-id-type="pmid">24683129</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>de Leeuw</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Motz</surname>
            <given-names>BA</given-names>
          </name>
        </person-group>
        <article-title>Psychophysics in a Web browser? Comparing response times collected with JavaScript and Psychophysics Toolbox in a visual search task</article-title>
        <source>Behavior Research Methods</source>
        <year>2016</year>
        <volume>48</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-015-0567-2</pub-id>
        <?supplied-pmid 25761390?>
        <pub-id pub-id-type="pmid">25761390</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <mixed-citation publication-type="other">Grootswagers, T. (2020). A primer on running human behavioural experiments online. <italic>Behavior Research Methods</italic>. 10.3758/s13428-020-01395-3</mixed-citation>
    </ref>
    <ref id="CR5">
      <mixed-citation publication-type="other">Henninger, F., Shevchenko, Y., Mertens, U., Kieslich, P. J., &amp; Hilbig, B. E. (2020). <italic>lab.js: A free, open, online experiment builder</italic>. Zenodo. 10.5281/zenodo.3767907</mixed-citation>
    </ref>
    <ref id="CR6">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hwung</surname>
            <given-names>D-J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J-C</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>D-S</given-names>
          </name>
        </person-group>
        <source>
          <italic>Digital gamma correction system for low, medium and high intensity video signals, with linear and non-linear correction</italic>
        </source>
        <year>1995</year>
      </element-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Joo</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Yeatman</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Reinecke</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Controlling for Participants’ Viewing Distance in Large-Scale, Psychophysical Online Experiments Using a Virtual Chinrest</article-title>
        <source>Scientific Reports</source>
        <year>2020</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-019-57204-1</pub-id>
        <pub-id pub-id-type="pmid">31913322</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathôt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schreij</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Theeuwes</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>OpenSesame: An open-source, graphical experiment builder for the social sciences</article-title>
        <source>Behavior Research Methods</source>
        <year>2012</year>
        <volume>44</volume>
        <issue>2</issue>
        <fpage>314</fpage>
        <lpage>324</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-011-0168-7</pub-id>
        <?supplied-pmid 22083660?>
        <pub-id pub-id-type="pmid">22083660</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peirce</surname>
            <given-names>JW</given-names>
          </name>
        </person-group>
        <article-title>PsychoPy—Psychophysics software in Python</article-title>
        <source>Journal of Neuroscience Methods</source>
        <year>2007</year>
        <volume>162</volume>
        <issue>1</issue>
        <fpage>8</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id>
        <?supplied-pmid 17254636?>
        <pub-id pub-id-type="pmid">17254636</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <mixed-citation publication-type="other">Reinecke, K., &amp; Gajos, K. Z. (2015). LabintheWild: Conducting Large-Scale Online Experiments With Uncompensated Samples. <italic>Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing</italic>, 1364–1378. 10.1145/2675133.2675246</mixed-citation>
    </ref>
    <ref id="CR11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sauter</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Draschkow</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Mack</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Building, Hosting and Recruiting: A Brief Introduction to Running Behavioral Experiments Online</article-title>
        <source>Brain Sciences</source>
        <year>2020</year>
        <volume>10</volume>
        <issue>4</issue>
        <fpage>251</fpage>
        <pub-id pub-id-type="doi">10.3390/brainsci10040251</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stoet</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>PsyToolkit: A software package for programming psychological experiments using Linux</article-title>
        <source>Behavior Research Methods</source>
        <year>2010</year>
        <volume>42</volume>
        <issue>4</issue>
        <fpage>1096</fpage>
        <lpage>1104</lpage>
        <pub-id pub-id-type="doi">10.3758/BRM.42.4.1096</pub-id>
        <?supplied-pmid 21139177?>
        <pub-id pub-id-type="pmid">21139177</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>LQ</given-names>
          </name>
          <name>
            <surname>Boland</surname>
            <given-names>MV</given-names>
          </name>
          <name>
            <surname>Wellik</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>De Moraes</surname>
            <given-names>CG</given-names>
          </name>
          <name>
            <surname>Myers</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Bex</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Elze</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Impact of natural blind spot location on perimetry</article-title>
        <source>Scientific Reports</source>
        <year>2017</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-016-0028-x</pub-id>
        <pub-id pub-id-type="pmid">28127051</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
