<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="publisher-id">sensors</journal-id>
    <journal-title-group>
      <journal-title>Sensors (Basel, Switzerland)</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1424-8220</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6832278</article-id>
    <article-id pub-id-type="doi">10.3390/s19204494</article-id>
    <article-id pub-id-type="publisher-id">sensors-19-04494</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Accurate and Robust Monocular SLAM with Omnidirectional Cameras</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Shuoyuan</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Guo</surname>
          <given-names>Peng</given-names>
        </name>
        <xref rid="c1-sensors-19-04494" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Feng</surname>
          <given-names>Lihui</given-names>
        </name>
        <xref rid="c1-sensors-19-04494" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yang</surname>
          <given-names>Aiying</given-names>
        </name>
      </contrib>
    </contrib-group>
    <aff id="af1-sensors-19-04494">The Key Laboratory of Photonics Information Technology, Ministry of Industry and Information Technology, School of Optics and Photonics, Beijing Institute of Technology, Beijing 100086, China; <email>lsyzge405@163.com</email> (S.L.); <email>yangaiying@bit.edu.cn</email> (A.Y.)</aff>
    <author-notes>
      <corresp id="c1-sensors-19-04494"><label>*</label>Correspondence: <email>guopeng0304@bit.edu.cn</email> (P.G.); <email>lihui.feng@bit.edu.cn</email> (L.F.)</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <volume>19</volume>
    <issue>20</issue>
    <elocation-id>4494</elocation-id>
    <history>
      <date date-type="received">
        <day>15</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>14</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 by the authors.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="open-access">
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Simultaneous localization and mapping (SLAM) are fundamental elements for many emerging technologies, such as autonomous driving and augmented reality. For this paper, to get more information, we developed an improved monocular visual SLAM system by using omnidirectional cameras. Our method extends the ORB-SLAM framework with the enhanced unified camera model as a projection function, which can be applied to catadioptric systems and wide-angle fisheye cameras with 195 degrees field-of-view. The proposed system can use the full area of the images even with strong distortion. For omnidirectional cameras, a map initialization method is proposed. We analytically derive the Jacobian matrices of the reprojection errors with respect to the camera pose and 3D position of points. The proposed SLAM has been extensively tested in real-world datasets. The results show positioning error is less than 0.1% in a small indoor environment and is less than 1.5% in a large environment. The results demonstrate that our method is real-time, and increases its accuracy and robustness over the normal systems based on the pinhole model.</p>
    </abstract>
    <kwd-group>
      <kwd>simultaneous localization and mapping</kwd>
      <kwd>visual SLAM</kwd>
      <kwd>map initialization</kwd>
      <kwd>fisheye cameras</kwd>
      <kwd>omnidirectional cameras</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-sensors-19-04494">
    <title>1. Introduction</title>
    <p>In order to complete various tasks, the robot needs to know the location of its environment. The most common method to localize a robot is to process the sensor information to calculate incremental motion. To achieve drift-free localization, the robot needs a map where the localization is known. To solve these problems, visual simultaneous localization and mapping (SLAM), where the main sensor is a camera, has been actively researched in recent years [<xref rid="B1-sensors-19-04494" ref-type="bibr">1</xref>,<xref rid="B2-sensors-19-04494" ref-type="bibr">2</xref>,<xref rid="B3-sensors-19-04494" ref-type="bibr">3</xref>,<xref rid="B4-sensors-19-04494" ref-type="bibr">4</xref>,<xref rid="B5-sensors-19-04494" ref-type="bibr">5</xref>]. Among different sensor modalities, a monocular camera is low-cost and easy to maintain. The feature distribution of the environments observed by the camera is a key factor in the performance of Visual SLAM. A major limiting factor for the feature distribution is the field-of-view (FoV) of the camera. Especially in sparse-feature or partially non-feature environments, wide FoV cameras can get higher accuracy and better robustness than normal cameras.</p>
    <p>However, traditional Visual SLAM can only use the part area of the wide FoV images, because these approaches are designed for the pinhole camera model which projects measurements of 3D points onto a 2D image plane. In practice, even when distortion model is added, the pinhole camera model demonstrates suboptimal performance for a FoV greater than 120°. To reduce the FoV of largely distorted image, the input images are usually cropped in traditional Visual SLAM.</p>
    <p>To utilize the full area of the wide FoV images, it is necessary to use a new camera model that is well applied to catadioptric systems and wide-angle fisheye cameras. There are several instances of research [<xref rid="B6-sensors-19-04494" ref-type="bibr">6</xref>,<xref rid="B7-sensors-19-04494" ref-type="bibr">7</xref>,<xref rid="B8-sensors-19-04494" ref-type="bibr">8</xref>,<xref rid="B9-sensors-19-04494" ref-type="bibr">9</xref>,<xref rid="B10-sensors-19-04494" ref-type="bibr">10</xref>] on camera models for large FoV cameras. Considering the accuracy and computation cost of camera models, we chose the enhanced unified camera model (EUCM) [<xref rid="B7-sensors-19-04494" ref-type="bibr">7</xref>] as a projection function.</p>
    <p>Since the projection function is changed, a new map initialization method should be proposed. Monocular SLAM needs a process to create an initial map because depth cannot be recovered from a single image. For planar scenes, by using the method of Faugeras et al. [<xref rid="B11-sensors-19-04494" ref-type="bibr">11</xref>], the relative camera pose is recovered from a homography matrix. On the other hand, for non-planar scenes, the relative camera pose is recovered from a fundamental matrix which can be computed with the eight-point algorithm [<xref rid="B12-sensors-19-04494" ref-type="bibr">12</xref>]. Because these methods are only suitable for the pinhole camera model, we propose a new initialization method based on them for the EUCM.</p>
    <p>We improve ORB-SLAM [<xref rid="B1-sensors-19-04494" ref-type="bibr">1</xref>,<xref rid="B2-sensors-19-04494" ref-type="bibr">2</xref>] to make full use of the fisheye images (see <xref ref-type="fig" rid="sensors-19-04494-f001">Figure 1</xref>), so our method can directly use all the information of the input images.</p>
    <p>In the experiments, we evaluated the accuracy and run-time of our approach on a benchmark [<xref rid="B13-sensors-19-04494" ref-type="bibr">13</xref>] whose image sequences are captured with a wide FoV fisheye lens. We compared our approach to the normal systems based on the pinhole model and demonstrated that our method outperforms the normal methods on benchmark datasets.</p>
    <p>The rest of this paper is structured as follows: In <xref ref-type="sec" rid="sec2-sensors-19-04494">Section 2</xref>, we first review the related systems for the normal and omnidirectional cameras. In <xref ref-type="sec" rid="sec3-sensors-19-04494">Section 3</xref> and <xref ref-type="sec" rid="sec4-sensors-19-04494">Section 4</xref>, we introduce notation and the enhanced unified camera model. In <xref ref-type="sec" rid="sec5-sensors-19-04494">Section 5</xref>, we provide an overview of our system, and we analytically derive the Jacobian matrices. In <xref ref-type="sec" rid="sec6-sensors-19-04494">Section 6</xref>, we propose a map initialization method and modify a normal relocalization algorithm for fisheye cameras. In <xref ref-type="sec" rid="sec7-sensors-19-04494">Section 7</xref>, we quantitatively evaluate the results of our system on public datasets and compare it with the normal systems based on the pinhole model; then we discuss and interpret the results. We open source in <uri xlink:href="https://github.com/lsyads/fisheye-ORB-SLAM">https://github.com/lsyads/fisheye-ORB-SLAM</uri>.</p>
  </sec>
  <sec id="sec2-sensors-19-04494">
    <title>2. Related Works</title>
    <p>Over the last decades, many visual SLAM and visual odometry systems have been proposed, such as ORB-SLAM [<xref rid="B1-sensors-19-04494" ref-type="bibr">1</xref>,<xref rid="B2-sensors-19-04494" ref-type="bibr">2</xref>], LSD-SLAM [<xref rid="B4-sensors-19-04494" ref-type="bibr">4</xref>], and direct sparse odometry (DSO) [<xref rid="B3-sensors-19-04494" ref-type="bibr">3</xref>]. The three recent examples are the state-of-the-art systems for the normal cameras and points features. ORB-SLAM is an indirect and keyframe-based visual SLAM algorithm based on graph optimization. LSD-SLAM is the first direct visual SLAM method with monocular cameras, which tracks the camera motion, produces a semi-dense map and performs pose graph optimization. DSO is a direct sparse visual odometry algorithm, which combines a fully direct probabilistic model with joint optimization of all model parameters. In addition to points features, Visual SLAM for point-line [<xref rid="B14-sensors-19-04494" ref-type="bibr">14</xref>] or point-plane [<xref rid="B15-sensors-19-04494" ref-type="bibr">15</xref>] features has been studied for many years. Next, we will discuss the related work on omnidirectional odometry and SLAM.</p>
    <p>By using scale-invariant feature transform (SIFT) features and the extended Kalman filter (EKF), several works [<xref rid="B16-sensors-19-04494" ref-type="bibr">16</xref>,<xref rid="B17-sensors-19-04494" ref-type="bibr">17</xref>] have been proposed to estimate camera localizations for omnidirectional cameras. However, these approaches lack efficient loop closing and relocalization technique, and they are only capable of mapping small work-spaces due to computational limitations.</p>
    <p>By using a direct method, D. Caruso et al. [<xref rid="B18-sensors-19-04494" ref-type="bibr">18</xref>] proposed a real-time, direct monocular SLAM method based on LSD-SLAM by incorporating the unified camera model [<xref rid="B6-sensors-19-04494" ref-type="bibr">6</xref>]. Similarly, H. Matsuki et al. [<xref rid="B19-sensors-19-04494" ref-type="bibr">19</xref>] extended DSO for omnidirectional cameras by using the unified camera model as a projection function. This system was the first fisheye-based direct visual odometry which runs in real time. L. Heng et al. [<xref rid="B20-sensors-19-04494" ref-type="bibr">20</xref>] presented a semi-direct visual odometry for a fisheye-stereo camera. A direct visual odometry for a fisheye-stereo camera was proposed by P. Liu et al. [<xref rid="B21-sensors-19-04494" ref-type="bibr">21</xref>].</p>
    <p>Indirect methods are the most popular techniques for SLAM. They proceed in two steps. First, the feature points in the images are extracted and matched. Second, the geometry and camera motion are estimated through the coordinates of corresponding points. J. Li et al. [<xref rid="B22-sensors-19-04494" ref-type="bibr">22</xref>] presented a SLAM system based on spherical model for full-view images in indoor environments, but the system was not real-time. S. Wang et al. [<xref rid="B23-sensors-19-04494" ref-type="bibr">23</xref>] proposed to extend ORB-SLAM framework by the unified camera model and the semi-dense depth map, whose map initialization method follows the idea from [<xref rid="B18-sensors-19-04494" ref-type="bibr">18</xref>], but there was no evaluation on localization accuracy and robustness of the system.</p>
    <p>In this paper, we propose an omnidirectional camera extension of ORB-SLAM by using the EUCM as a projection function. This is a real-time robust monocular visual SLAM. Since the projection function is changed, we propose a new map initialization method and modify the normal relocalization algorithm.</p>
  </sec>
  <sec id="sec3-sensors-19-04494">
    <title>3. Notation</title>
    <p>Throughout the paper, bold lower-case letters (x) represent vectors, and light lower-case letters (<italic>t</italic>) represent scalars. Matrices will be represented by bold upper-case letters (H), and functions (including images) will be represented by light upper-case letters (<italic>I</italic>). Pixel coordinates generally are denoted as <inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Point coordinates in 3D are denoted as <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced close="]" open="["><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced></mml:mrow><mml:mo>×</mml:mo></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> stands for:<disp-formula id="FD1-sensors-19-04494"><label>(1)</label><mml:math id="mm4"><mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mfenced close="]" open="["><mml:mi mathvariant="bold">x</mml:mi></mml:mfenced></mml:mrow><mml:mo>×</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>z</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mi>x</mml:mi></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
    <p>The camera orientation and position are represented by <inline-formula><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mi>O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">t</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. They transform a 3D point from the camera coordinate system to the world coordinate system. Traditionally, <inline-formula><mml:math id="mm7"><mml:mrow><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula> stands for a camera projection function and <inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> is the camera unprojection function.</p>
  </sec>
  <sec id="sec4-sensors-19-04494">
    <title>4. Camera Models</title>
    <sec id="sec4dot1-sensors-19-04494">
      <title>4.1. Pinhole Model</title>
      <p>The pinhole model is the most common camera model. Image points u are computed by projecting measurements of 3D points x onto a 2D image plane. The projection function of the pinhole model is given as
<disp-formula id="FD2-sensors-19-04494"><label>(2)</label><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi>x</mml:mi><mml:mo>/</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>y</mml:mi><mml:mo>/</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the focal lengths, and <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are the principal points. The projection function is linear in homogeneous coordinates, so it is the simplest model.</p>
    </sec>
    <sec id="sec4dot2-sensors-19-04494">
      <title>4.2. Extended Unified Camera Model</title>
      <p>We use the enhanced unified camera model (EUCM) based on the so-called unified camera model [<xref rid="B6-sensors-19-04494" ref-type="bibr">6</xref>] for a wide FoV fisheye camera. The projection model does not require additional mapping to model distortions, and it takes just two projection parameters more than a simple pinhole model to represent radial distortions (only one parameter more than the unified model). The unprojection function can be expressed in explicit closed-form.</p>
      <p>As shown in <xref ref-type="fig" rid="sensors-19-04494-f002">Figure 2</xref>, a 3D point x in camera coordinates is first mapped to x<italic><sub>p</sub></italic> by projecting it onto a second-order projection surface <italic>P</italic>, then the point x<italic><sub>p</sub></italic> is mapped to q by projecting it onto the plane <italic>M</italic> of <inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The coordinates of the point q are computed as follows:<disp-formula id="FD3-sensors-19-04494"><label>(3)</label><mml:math id="mm13"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi mathvariant="bold">q</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>α</mml:mi><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>α</mml:mi><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The two parameters allow us to better approximate the properties of lenses despite strong distortions. For the fisheye cameras, the EUCM projects on the ellipsoid, where <inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p>Finally, the image point u is computed by projecting the point q onto an image plane using the pinhole camera model (see <xref ref-type="fig" rid="sensors-19-04494-f003">Figure 3</xref>). The projection function of a 3D point is given by
<disp-formula id="FD4-sensors-19-04494"><label>(4)</label><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mi>u</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>v</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>α</mml:mi><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>α</mml:mi><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>The unprojection function is defined as follows:<disp-formula id="FD5-sensors-19-04494"><label>(5)</label><mml:math id="mm18"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi>π</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>u</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>u</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:msup><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>α</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mi>β</mml:mi><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm19"><mml:mrow><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>m</mml:mi><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>, and if <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>α</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> depends on <inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Actually, <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p>Actually, the unprojection function (5), a ray function, is decided by the coordinates of point x<italic><sub>p</sub></italic>, because points x<italic><sub>p</sub></italic> and x are on the same line (see <xref ref-type="fig" rid="sensors-19-04494-f002">Figure 2</xref>). Plane <italic>P</italic> where point x<italic><sub>p</sub></italic> is located is called <italic>the projection surface</italic>. The function, which solves the coordinates of point x<italic><sub>p</sub></italic> from u, is the same as (5):<disp-formula id="FD6-sensors-19-04494"><label>(6)</label><mml:math id="mm26"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
    </sec>
  </sec>
  <sec id="sec5-sensors-19-04494">
    <title>5. System Overview</title>
    <p>By using the EUCM as a projection function, we develop a real-time robust monocular visual SLAM system for omnidirectional cameras based on ORB-SLAM. The system makes use of ORB features [<xref rid="B24-sensors-19-04494" ref-type="bibr">24</xref>], which are based on the FAST keypoint detector and BRIEF descriptor. Not only are they extremely fast in computation and matching, but they also have good invariance in regards of the viewpoint. In order to increase efficiency and accuracy, there are three threads in parallel: tracking, local mapping and loop closing.</p>
    <sec id="sec5dot1-sensors-19-04494">
      <title>5.1. Bundle Adjustment</title>
      <p>Because Bundle Adjustment (BA) provides a powerful network of matches and good initial guesses, our system uses it to optimize the estimations of camera poses and 3D world points.</p>
      <p>There are three kinds of BA in the system: motion-only BA, local BA, and full BA. By minimizing the reprojection error between matched 3D points x in world coordinates and keypoints u, BA optimizes the camera position t, orientation R, and points x. In motion-only BA, it only optimizes camera poses:<disp-formula id="FD7-sensors-19-04494"><label>(7)</label><mml:math id="mm27"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">t</mml:mi><mml:mo stretchy="false">}</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mstyle mathsize="100%" displaystyle="true"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:munder><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>χ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mo>∑</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">R</mml:mi><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold">t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm28"><mml:mrow><mml:mi>χ</mml:mi></mml:mrow></mml:math></inline-formula> means the set of all matches, <inline-formula><mml:math id="mm29"><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:math></inline-formula> is the robust Huber cost function, and <inline-formula><mml:math id="mm30"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="sans-serif">Σ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi mathvariant="bold">I</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the covariance matrix associated to the scale where the keypoint was detected.</p>
      <p>In addition to optimizing camera poses, local BA also optimizes points and minimizes the reprojection error in a collection of nearby keyframes. In full BA, all keyframes are optimized to get camera poses and points.</p>
      <p>The Levenberg–Marquardt algorithm implementation in g2o [<xref rid="B25-sensors-19-04494" ref-type="bibr">25</xref>] is used to solve this minimization problem. In BA, we modified the projection function to EUCM and Jacobian matrices.</p>
    </sec>
    <sec id="sec5dot2-sensors-19-04494">
      <title>5.2. Jacobian Matrices</title>
      <p>Jacobian matrices are used in the Levenberg–Marquardt algorithm to speed up a bundle adjustment process in the SLAM system, because they are more efficient than numeric or automatic differentiation. A 3D point in world and camera coordinates are represented by <inline-formula><mml:math id="mm31"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm32"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. The reprojection error (7) is written as <italic>e</italic>. The small changes of camera rotation and position are represented by <inline-formula><mml:math id="mm33"><mml:mrow><mml:mrow><mml:mo>δ</mml:mo><mml:mi mathvariant="bold">R</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm34"><mml:mrow><mml:mrow><mml:mo>δ</mml:mo><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively.</p>
      <p>The projection relation is <inline-formula><mml:math id="mm35"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">u</mml:mi><mml:mo>=</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from (4), and (3) is reduced to <inline-formula><mml:math id="mm36"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">m</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
      <p>By using the chain rule, the Jacobian matrices of the reprojection errors with respect to the camera rotation, camera position, and 3D points is computed, respectively:<disp-formula id="FD8-sensors-19-04494"><label>(8)</label><mml:math id="mm37"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">J</mml:mi><mml:mi mathvariant="bold">R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>δ</mml:mo><mml:mi mathvariant="bold">R</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>δ</mml:mo><mml:mi mathvariant="bold">R</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mfenced close="]" open="["><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mfenced></mml:mrow><mml:mo>×</mml:mo></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD9-sensors-19-04494"><label>(9)</label><mml:math id="mm38"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">J</mml:mi><mml:mi mathvariant="bold">t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>δ</mml:mo><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mo>δ</mml:mo><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD10-sensors-19-04494"><label>(10)</label><mml:math id="mm39"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">J</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mi mathvariant="bold">R</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where
<disp-formula id="FD11-sensors-19-04494"><label>(11)</label><mml:math id="mm40"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>η</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi><mml:msup><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>η</mml:mi></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi><mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mi>η</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi>ρ</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
      <p>The Jacobian matrixes for the EUCM are different from the Jacobian matrixes for the pinhole model, so it is necessary to analytically derive the Jacobian matrices.</p>
    </sec>
    <sec id="sec5dot3-sensors-19-04494">
      <title>5.3. Tracking, Local Mapping, and Loop Closing</title>
      <p>The tracking is responsible for positioning the camera per frame and deciding when to insert a new keyframe. To get the initial camera pose and initial map points, we first perform a map initialization for omnidirectional cameras, which is explained in detail in <xref ref-type="sec" rid="sec6dot1-sensors-19-04494">Section 6.1</xref>. If the initialization is successful, we optimize the pose using <italic>motion-only BA</italic>. Then a local map is acquired. It contains the map points of nearby keyframes. Matches with the local map points are searched by reprojection, then the camera pose and the map points are optimized with all matches. Finally, the tracking thread decides if a new keyframe is inserted. When the tracking is lost, the relocalization module for omnidirectional cameras starts working, which is explained in <xref ref-type="sec" rid="sec6dot2-sensors-19-04494">Section 6.2</xref>.</p>
      <p>The local mapping processes the new keyframes and executes <italic>the local BA</italic> to optimize camera poses and correct map point positions. New points are created by using a triangulation method which is the same as the triangulation of initialization for omnidirectional cameras.</p>
      <p>The loop closing is in charge of searching for loops with every new keyframe. For omnidirectional cameras, this thread is almost the same as the original thread except <italic>the full BA</italic> and projection function, which should be the EUCM instead of the pinhole model.</p>
    </sec>
  </sec>
  <sec id="sec6-sensors-19-04494">
    <title>6. Map Initialization and Relocalization Algorithm</title>
    <sec id="sec6dot1-sensors-19-04494">
      <title>6.1. Map Initialization Algorithm</title>
      <p>The goal of the map initialization is to get the relative pose and triangulate a set of initial map points from matching feature points of two frame. Two geometrical models, the homography matrix assuming a planar scene and the essential matrix assuming a non-planar scene, are computed in parallel. Then a suitable model is automatically selected, and the relative pose is recovered with the selected model.</p>
      <p>The method first extracts ORB features in the current frame <inline-formula><mml:math id="mm41"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and reference frame <inline-formula><mml:math id="mm42"><mml:mrow><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. It then searches for matches <inline-formula><mml:math id="mm43"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>↔</mml:mo><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. By (6), the coordinates of points <inline-formula><mml:math id="mm44"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm45"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (see <xref ref-type="fig" rid="sensors-19-04494-f004">Figure 4</xref>) are solved from <inline-formula><mml:math id="mm46"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm47"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">u</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, so there are <italic>new matches</italic>
<inline-formula><mml:math id="mm48"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>↔</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. A point position in world, a current frame and the reference frame homogeneous coordinates are represented by <inline-formula><mml:math id="mm49"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm50"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm51"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively.</p>
      <p>As shown in <xref ref-type="fig" rid="sensors-19-04494-f004">Figure 4</xref>, reference frame center o<italic><sub>r</sub></italic>, points x<italic><sub>pr</sub></italic> and x<italic><sub>r</sub></italic> are on the same line, and current frame center o<italic><sub>c</sub></italic>, points x<italic><sub>pc</sub></italic> and x<italic><sub>c</sub></italic> are on the same line. There are two non-zero scale factors <inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm57"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="FD12-sensors-19-04494"><label>(12)</label><mml:math id="mm58"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>λ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm60"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> are <inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> matrices which represent camera poses.</p>
      <sec id="sec6dot1dot1-sensors-19-04494">
        <title>6.1.1. The Homography Matrix</title>
        <p>The homography matrix is suitable for a planar scene where points x are located. The plane can be set to <inline-formula><mml:math id="mm62"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, so <inline-formula><mml:math id="mm63"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. By using (12), we get
<disp-formula id="FD13-sensors-19-04494"><label>(13)</label><mml:math id="mm64"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>λ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm65"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm66"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are <inline-formula><mml:math id="mm67"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> matrices.</p>
        <p>From (13), we get
<disp-formula id="FD14-sensors-19-04494"><label>(14)</label><mml:math id="mm68"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">H</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm69"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is the homography matrix that can recover the relative pose between two frames.</p>
        <p>The scale factors in (14) is eliminated by the cross product:<disp-formula id="FD15-sensors-19-04494"><label>(15)</label><mml:math id="mm70"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
There is no scale factors <inline-formula><mml:math id="mm71"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm72"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> in (15), so all the <italic>new matches points</italic>
<inline-formula><mml:math id="mm73"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>↔</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the two frames satisfy this equation for the same <inline-formula><mml:math id="mm74"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. This form (15) will derive a simple linear solution to <inline-formula><mml:math id="mm75"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, and it is solved by the normalized direct linear transformation (DLT) as explained in [<xref rid="B12-sensors-19-04494" ref-type="bibr">12</xref>] inside a random sample consensus (RANSAC) scheme.</p>
      </sec>
      <sec id="sec6dot1dot2-sensors-19-04494">
        <title>6.1.2. The Essential Matrix</title>
        <p>The essential matrix is suitable for a non-planar scene where points x are located. As shown in <xref ref-type="fig" rid="sensors-19-04494-f004">Figure 4</xref>, point <inline-formula><mml:math id="mm76"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> can be transformed into <inline-formula><mml:math id="mm77"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> by:<disp-formula id="FD16-sensors-19-04494"><label>(16)</label><mml:math id="mm78"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm79"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> stands for the relative pose between current frame and reference frame.</p>
        <p>From (16), we get the definition equation for the essential matrix <inline-formula><mml:math id="mm80"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> in [<xref rid="B12-sensors-19-04494" ref-type="bibr">12</xref>]:<disp-formula id="FD17-sensors-19-04494"><label>(17)</label><mml:math id="mm81"><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>c</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm82"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mo>×</mml:mo></mml:msub><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        <p>By substituting (12) into (17):<disp-formula id="FD18-sensors-19-04494"><label>(18)</label><mml:math id="mm83"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p>Because <inline-formula><mml:math id="mm84"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm85"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, we get
<disp-formula id="FD19-sensors-19-04494"><label>(19)</label><mml:math id="mm86"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p>There are no scale factors <inline-formula><mml:math id="mm87"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm88"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> in (19), so all <italic>new matches points</italic>
<inline-formula><mml:math id="mm89"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>↔</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> of the two frames satisfy this equation for the same <inline-formula><mml:math id="mm90"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. The essential matrix <inline-formula><mml:math id="mm91"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> can be solved from (19) by 8-point algorithms [<xref rid="B12-sensors-19-04494" ref-type="bibr">12</xref>] inside a RANSAC scheme.</p>
      </sec>
      <sec id="sec6dot1dot3-sensors-19-04494">
        <title>6.1.3. Matrix Selection</title>
        <p>The homography matrix <inline-formula><mml:math id="mm92"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and the essential matrix <inline-formula><mml:math id="mm93"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> are computed in parallel threads. Then a suitable matrix is automatically selected by a robust heuristic method [<xref rid="B2-sensors-19-04494" ref-type="bibr">2</xref>], which chooses a matrix in <inline-formula><mml:math id="mm94"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm95"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> with smaller symmetric transfer errors [<xref rid="B12-sensors-19-04494" ref-type="bibr">12</xref>].</p>
      </sec>
      <sec id="sec6dot1dot4-sensors-19-04494">
        <title>6.1.4. Motion Recovery and Triangulation Method</title>
        <p>We make the motion recovery from the selected matrix, then get the relative pose <inline-formula><mml:math id="mm96"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> between two frames. In the case of the homography matrix, we retrieve 8 motion hypotheses using the method of Faugeras et al. [<xref rid="B11-sensors-19-04494" ref-type="bibr">11</xref>]. In the case of the essential matrix, we retrieve 4 motion hypotheses using the singular value decomposition method explained in [<xref rid="B12-sensors-19-04494" ref-type="bibr">12</xref>].</p>
        <p>We triangulate these hypotheses and select a solution with lower reprojection error. The triangulation method, which is based on the linear triangulation method [<xref rid="B12-sensors-19-04494" ref-type="bibr">12</xref>], will be introduced below.</p>
        <p>From (12), we can get on <italic>new matches points</italic>
<inline-formula><mml:math id="mm97"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>↔</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>
<disp-formula id="FD20-sensors-19-04494"><label>(20)</label><mml:math id="mm98"><mml:mrow><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p>
        <p>A cross product of (20) gives three equations for each image point, of which two are linearly independent. Equation (20) can be written as four independent equations:<disp-formula id="FD21-sensors-19-04494"><label>(21)</label><mml:math id="mm99"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:mi>r</mml:mi><mml:mn>3</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:mi>c</mml:mi><mml:mn>3</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:mi>c</mml:mi><mml:mn>3</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi mathvariant="bold">P</mml:mi><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm100"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mi mathvariant="bold">R</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm101"><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">P</mml:mi><mml:mi>i</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> are the rows of P. Equation (21) is solved by using the singular value decomposition, then the world coordinates of x are obtained.</p>
      </sec>
    </sec>
    <sec id="sec6dot2-sensors-19-04494">
      <title>6.2. Relocalization Algorithm</title>
      <p>The relocalization model starts working when the tracking is lost. For omnidirectional cameras, the normal relocalization model based on ORB-SLAM is modified. The relocalization algorithm performs first an ORB matching with each candidate keyframes. The method then performs RANSAC iterations of the EPnP algorithm [<xref rid="B26-sensors-19-04494" ref-type="bibr">26</xref>] to find a camera pose supported by enough inliers.</p>
      <p>Because the EPnP algorithm is only suitable for the pinhole model, and the image surface must be plane, we modified the relocalization model for the EUCM. In order to meet the conditions of the EPnP algorithm, we map first an image point u to a point x<italic><sub>p</sub></italic>, then we map the point x<italic><sub>p</sub></italic> to a point x<italic><sub>m</sub></italic> which is the intersection of the ray ox<italic><sub>p</sub></italic> and the <inline-formula><mml:math id="mm102"><mml:mrow><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> plane (see <xref ref-type="fig" rid="sensors-19-04494-f002">Figure 2</xref>). The coordinates of x<italic><sub>m</sub></italic> are computed by using (5):<disp-formula id="FD22-sensors-19-04494"><label>(22)</label><mml:math id="mm103"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>The point x<italic><sub>m</sub></italic> meets the conditions of the EPnP algorithm. The value of <inline-formula><mml:math id="mm104"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is not allowed to be too small to improve the accuracy of the EPnP algorithm. In our system, we let <inline-formula><mml:math id="mm105"><mml:mrow><mml:mrow><mml:mi>min</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, and there are very few points of <inline-formula><mml:math id="mm106"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Basically, we use the coordinates of x<italic><sub>m</sub></italic> instead of image points u in the EPnP algorithm.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec7-sensors-19-04494">
    <title>7. Results and Discussion</title>
    <p>We performed an experimental evaluation of our omnidirectional ORB-SLAM in terms of accuracy and robustness on the TUM VI benchmark [<xref rid="B13-sensors-19-04494" ref-type="bibr">13</xref>]. The datasets provide camera images at 20Hz with a 195 degrees FoV fisheye lens. The image resolution is 512×512. We obtained the camera intrinsic parameters by using the Kalibr calibration toolbox [<xref rid="B27-sensors-19-04494" ref-type="bibr">27</xref>] with the original checkerboard sequence. We used 3 types of the datasets: room, corridor, and magistrale. <xref ref-type="fig" rid="sensors-19-04494-f005">Figure 5</xref> shows the images from the datasets. The room datasets are the sequences in the room with Motion Capture system, where ground-truth poses are available for the entire trajectory. The corridor datasets are sequences in the corridor and several offices. The magistrale datasets are sequences in the large hall. The ground-truth poses of the above two datasets are available for the start and end segments in the same room with Motion Capture system.</p>
    <p>We provided a quantitative comparison between our system and other systems, including the normal ORB-SLAM and DSO. Since the normal ORB-SLAM and DSO use the pinhole model as the projection function, we used the Kannala-Brandt model [<xref rid="B8-sensors-19-04494" ref-type="bibr">8</xref>] with 8 parameters to rectify the key points in ORB-SLAM and rectify and crop the image in DSO. We evaluated both algorithms in an Intel Core i5-8300H notebook computer with 16GB RAM.</p>
    <sec id="sec7dot1-sensors-19-04494">
      <title>7.1. Accuracy and Robustness Comparison</title>
      <p>We measured the absolute translational root mean square error (RMSE) between the estimated and the ground-truth position. The estimated position is computed on all keyframes and is aligned with the ground-truth trajectory data. We get the scale factor by least square method where we minimize the difference between the vector length of the estimated position and the truth position. To facilitate a fair comparison: In the room datasets, the alignment is performed for all keyframes; in the corridor and magistrale datasets, the alignment is only performed on the start segments; and we disable loop detection in the corridor and magistrale datasets except the room datasets which are used to demonstrate the loop detection for fisheye cameras. We have run each sequence 5 times in these three systems and listed median results of each time, to account for the randomness of the multithreading system. The results shown in <xref rid="sensors-19-04494-t001" ref-type="table">Table 1</xref> demonstrate our system outperforms the other systems in accuracy and robustness. The accuracy of our system is typically below 5 cm in small indoor rooms, below 15 cm in corridors and of a few meters in the large hall, because the length is longer and longer, and the image overlap between frames is smaller and smaller. The drift computed by percentage is below 0.1% in indoor rooms and corridors, and it is also below 1.5% in the large hall. In <xref rid="sensors-19-04494-t001" ref-type="table">Table 1</xref>, there are many “LOST” in the normal systems, when the camera rotates and moves too fast, and the illumination variation is big.</p>
      <p>Some representative visual results and estimated trajectories are shown in <xref ref-type="fig" rid="sensors-19-04494-f006">Figure 6</xref> and <xref ref-type="fig" rid="sensors-19-04494-f007">Figure 7</xref>. <xref ref-type="fig" rid="sensors-19-04494-f006">Figure 6</xref>a shows a room map whose points are almost active due to wide FoV of fisheye camera; <xref ref-type="fig" rid="sensors-19-04494-f006">Figure 6</xref>b is a map of the same room and a corridor, where datasets start and end in the same room. As shown in <xref ref-type="fig" rid="sensors-19-04494-f007">Figure 7</xref>, the error of all systems is very small in the start segment, but our system suffers less drift than other systems in the end segment because constraints between keyframes are stronger in our system. Due to the wider FOV and the well-maintained scale, our system performs better than the normal ORB-SLAM and DSO.</p>
      <p>A major advantage of fisheye cameras is that more features can be observed in an input image. <xref ref-type="fig" rid="sensors-19-04494-f001">Figure 1</xref> shows our system can observe more points in a frame than the normal ORB-SLAM. The image overlap between frames is also bigger, so the constraints between keyframes is stronger in our system, as shown in the red circle of <xref ref-type="fig" rid="sensors-19-04494-f008">Figure 8</xref>.</p>
    </sec>
    <sec id="sec7dot2-sensors-19-04494">
      <title>7.2. Timing Measurement</title>
      <p><xref rid="sensors-19-04494-t002" ref-type="table">Table 2</xref> shows the measured average time over the datasets for the tracking thread. These results demonstrate our system is real-time and each frame can be tracked at least 30Hz. The time cost of our system is slightly larger than the normal ORB-SLAM, because the normal ORB-SLAM needs time to rectify and crop the key points, and our system needs not rectifying and cropping the input key points but needs more time to compute on the projection function. Obviously, DSO needs more time to rectify and crop the input images and more time to compute the map points because the direct method.</p>
    </sec>
    <sec id="sec7dot3-sensors-19-04494">
      <title>7.3. Relocalization Experiments</title>
      <p>We performed two relocalization experiments on the datasets. In the experiments, we built a map with the first 40 s of the two sequences <italic>room2</italic> and <italic>room3</italic>, and performed global relocalization with every successive frame. We performed the same experiment with the normal ORB-SLAM for comparison. <xref rid="sensors-19-04494-t003" ref-type="table">Table 3</xref> shows the number of the initial map keyframes and the recall. Our system recalls better and needs less keyframes to create the initial map than the normal ORB-SLAM because more features can be observed in an input image by using fisheye cameras.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec8-sensors-19-04494">
    <title>8. Conclusions</title>
    <p>We proposed a real-time monocular SLAM system for fisheye cameras. We first incorporated the enhanced unified camera model within the ORB-SLAM. Our system can use the full area of the images, even with strong distortion, except relocalization algorithm, which can also use more points than the normal algorithms because the points on the projection surface of EUCM are directly used. We analytically derived the Jacobian matrices to speed up the bundle adjustment process. A map initialization method was proposed for fisheye cameras. We proved <italic>the new matches points</italic> satisfy the homography matrix assuming a planar scene and the essential matrix assuming a non-planar scene to motion recovery and triangulation method. And we modified the relocalization algorithm for omnidirectional cameras. Then, we compared the performance of our omnidirectional ORB-SLAM and the normal systems on the public datasets, and demonstrated our method yields better performance in accuracy and robustness than the normal methods. In future work, we will extend our method with point features specially for fisheye cameras, inverse depth, and IMU.</p>
  </sec>
</body>
<back>
  <notes>
    <title>Author Contributions</title>
    <p>Methodology, S.L.; resources, S.L.; software, S.L.; supervision, P.G. and L.F.; validation, P.G., L.F. and A.Y.; writing—original draft preparation, S.L.; writing—review and editing, P.G., L.F. and A.Y.</p>
  </notes>
  <notes>
    <title>Funding</title>
    <p>This research was funded by the National Natural Science Foundation of China, grant number 61675025.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest.</p>
  </notes>
  <ref-list>
    <title>References</title>
    <ref id="B1-sensors-19-04494">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mur-Artal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Montiel</surname>
            <given-names>J.M.M.</given-names>
          </name>
          <name>
            <surname>Tardos</surname>
            <given-names>J.D.</given-names>
          </name>
        </person-group>
        <article-title>ORB-SLAM: A Versatile and Accurate Monocular SLAM System</article-title>
        <source>IEEE Trans. Robot.</source>
        <year>2015</year>
        <volume>31</volume>
        <fpage>1147</fpage>
        <lpage>1163</lpage>
        <pub-id pub-id-type="doi">10.1109/TRO.2015.2463671</pub-id>
      </element-citation>
    </ref>
    <ref id="B2-sensors-19-04494">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mur-Artal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Tardos</surname>
            <given-names>J.D.</given-names>
          </name>
        </person-group>
        <article-title>ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras</article-title>
        <source>IEEE Trans. Robot.</source>
        <year>2017</year>
        <volume>33</volume>
        <fpage>1255</fpage>
        <lpage>1262</lpage>
        <pub-id pub-id-type="doi">10.1109/TRO.2017.2705103</pub-id>
      </element-citation>
    </ref>
    <ref id="B3-sensors-19-04494">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Engel</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Koltun</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Cremers</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Direct sparse odometry</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2018</year>
        <volume>40</volume>
        <fpage>611</fpage>
        <lpage>625</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2658577</pub-id>
        <?supplied-pmid 28422651?>
        <pub-id pub-id-type="pmid">28422651</pub-id>
      </element-citation>
    </ref>
    <ref id="B4-sensors-19-04494">
      <label>4.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Engel</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Schöps</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Cremers</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>LSD-SLAM: Large-Scale Direct Monocular SLAM</article-title>
        <source>Proceedings of the Computational Methods in Systems Biology</source>
        <conf-loc>Manchester, UK</conf-loc>
        <conf-date>17–19 November 2014</conf-date>
      </element-citation>
    </ref>
    <ref id="B5-sensors-19-04494">
      <label>5.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Klein</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Murray</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Parallel Tracking and Mapping for Small AR Workspaces</article-title>
        <source>Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality</source>
        <conf-loc>Nara, Japan</conf-loc>
        <conf-date>13–16 November 2007</conf-date>
        <fpage>225</fpage>
        <lpage>234</lpage>
      </element-citation>
    </ref>
    <ref id="B6-sensors-19-04494">
      <label>6.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Geyer</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Daniilidis</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>A Unifying Theory for Central Panoramic Systems and Practical Implications</article-title>
        <source>Proceedings of the European Conference on Computer Vision (ECCV)</source>
        <conf-loc>Dublin, Ireland</conf-loc>
        <conf-date>26 June–1 July 2000</conf-date>
      </element-citation>
    </ref>
    <ref id="B7-sensors-19-04494">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khomutenko</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Garcia</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Martinet</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>An enhanced unified camera model</article-title>
        <source>IEEE Robot. Autom. Lett.</source>
        <year>2016</year>
        <volume>1</volume>
        <fpage>137</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1109/LRA.2015.2502921</pub-id>
      </element-citation>
    </ref>
    <ref id="B8-sensors-19-04494">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kannala</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Brandt</surname>
            <given-names>S.S.</given-names>
          </name>
        </person-group>
        <article-title>A generic camera model and calibration method for conventional, wide-angle, and fish-eye lenses</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>2006</year>
        <volume>28</volume>
        <fpage>1335</fpage>
        <lpage>1340</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2006.153</pub-id>
        <?supplied-pmid 16886867?>
        <pub-id pub-id-type="pmid">16886867</pub-id>
      </element-citation>
    </ref>
    <ref id="B9-sensors-19-04494">
      <label>9.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Usenko</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Demmel</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Cremers</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>The Double Sphere Camera Model</article-title>
        <source>Proceedings of the International Conference on 3D Vision (3DV)</source>
        <conf-loc>Verona, Italy</conf-loc>
        <conf-date>5–8 September 2018</conf-date>
      </element-citation>
    </ref>
    <ref id="B10-sensors-19-04494">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choi</surname>
            <given-names>K.H.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Analysis of Fish-Eye Lens Camera Self-Calibration</article-title>
        <source>Sensors</source>
        <year>2019</year>
        <volume>19</volume>
        <elocation-id>1218</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s19051218</pub-id>
        <?supplied-pmid 30857373?>
        <pub-id pub-id-type="pmid">30857373</pub-id>
      </element-citation>
    </ref>
    <ref id="B11-sensors-19-04494">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Faugeras</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Lustman</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Motion and structure from motion in a piecewise planar environment</article-title>
        <source>Int. J. Pattern Recognit. Artif. Intell.</source>
        <year>1988</year>
        <volume>2</volume>
        <fpage>485</fpage>
        <lpage>508</lpage>
        <pub-id pub-id-type="doi">10.1142/S0218001488000285</pub-id>
      </element-citation>
    </ref>
    <ref id="B12-sensors-19-04494">
      <label>12.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hartley</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <source>Multiple View Geometry in Computer Vision</source>
        <edition>2nd ed.</edition>
        <publisher-name>Cambridge University Press</publisher-name>
        <publisher-loc>Cambridge, UK</publisher-loc>
        <year>2004</year>
      </element-citation>
    </ref>
    <ref id="B13-sensors-19-04494">
      <label>13.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Schubert</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Goll</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Demmel</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Usenko</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Stuckler</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cremers</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>The TUM VI Benchmark for Evaluating Visual-Inertial Odometry</article-title>
        <source>Proceedings of the 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source>
        <conf-loc>Madrid, Spain</conf-loc>
        <conf-date>1–5 October 2018</conf-date>
        <fpage>1680</fpage>
        <lpage>1687</lpage>
      </element-citation>
    </ref>
    <ref id="B14-sensors-19-04494">
      <label>14.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Pumarola</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Vakhitov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Agudo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sanfeliu</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Moreno-Noguer</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>PL-SLAM: Real-time monocular visual SLAM with points and lines</article-title>
        <source>Proceedings of the 2017 IEEE International Conference on Robotics and Automation (ICRA)</source>
        <conf-loc>Singapore</conf-loc>
        <conf-date>29 May–3 June 2017</conf-date>
        <fpage>4503</fpage>
        <lpage>4508</lpage>
      </element-citation>
    </ref>
    <ref id="B15-sensors-19-04494">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Zhai</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>RGB-D SLAM Using Point-Plane Constraints for Indoor Environments</article-title>
        <source>Sensors</source>
        <year>2019</year>
        <volume>19</volume>
        <elocation-id>2721</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s19122721</pub-id>
        <?supplied-pmid 31213001?>
        <pub-id pub-id-type="pmid">31213001</pub-id>
      </element-citation>
    </ref>
    <ref id="B16-sensors-19-04494">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Valiente</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Gil</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Payá</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Sebastián</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>Reinoso, Óscar Robust Visual Localization with Dynamic Uncertainty Management in Omnidirectional SLAM</article-title>
        <source>Appl. Sci.</source>
        <year>2017</year>
        <volume>7</volume>
        <elocation-id>1294</elocation-id>
        <pub-id pub-id-type="doi">10.3390/app7121294</pub-id>
      </element-citation>
    </ref>
    <ref id="B17-sensors-19-04494">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Valiente</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Gil</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Reinoso</surname>
            <given-names>Ó.</given-names>
          </name>
          <name>
            <surname>Juliá</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Holloway</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Improved omnidirectional odometry for a view-based mapping approach</article-title>
        <source>Sensors</source>
        <year>2017</year>
        <volume>17</volume>
        <elocation-id>325</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s17020325</pub-id>
        <?supplied-pmid 28208766?>
        <pub-id pub-id-type="pmid">28208766</pub-id>
      </element-citation>
    </ref>
    <ref id="B18-sensors-19-04494">
      <label>18.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Caruso</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Engel</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cremers</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Large-scale direct SLAM for omnidirectional cameras</article-title>
        <source>Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source>
        <conf-loc>Hamburg, Germany</conf-loc>
        <conf-date>28 September–2 October 2015</conf-date>
        <fpage>141</fpage>
        <lpage>148</lpage>
      </element-citation>
    </ref>
    <ref id="B19-sensors-19-04494">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Matsuki</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Von Stumberg</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Usenko</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Stuckler</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Cremers</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Stueckler</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Omnidirectional DSO: Direct Sparse Odometry With Fisheye Cameras</article-title>
        <source>IEEE Robot. Autom. Lett.</source>
        <year>2018</year>
        <volume>3</volume>
        <fpage>3693</fpage>
        <lpage>3700</lpage>
        <pub-id pub-id-type="doi">10.1109/LRA.2018.2855443</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-sensors-19-04494">
      <label>20.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Heng</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Semi-direct visual odometry for a fisheye-stereo camera</article-title>
        <source>Proceedings of the 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source>
        <conf-loc>Daejeon, Korea</conf-loc>
        <conf-date>9–14 October 2016</conf-date>
        <fpage>4077</fpage>
        <lpage>4084</lpage>
      </element-citation>
    </ref>
    <ref id="B21-sensors-19-04494">
      <label>21.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Sattler</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Geiger</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pollefeys</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Direct visual odometry for a fisheye-stereo camera</article-title>
        <source>Proceedings of the 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source>
        <conf-loc>Vancouver, BC, Canada</conf-loc>
        <conf-date>24–28 September 2017</conf-date>
        <fpage>1746</fpage>
        <lpage>1752</lpage>
      </element-citation>
    </ref>
    <ref id="B22-sensors-19-04494">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Spherical-Model-Based SLAM on Full-View Images for Indoor Environments</article-title>
        <source>Appl. Sci.</source>
        <year>2018</year>
        <volume>8</volume>
        <elocation-id>2268</elocation-id>
        <pub-id pub-id-type="doi">10.3390/app8112268</pub-id>
      </element-citation>
    </ref>
    <ref id="B23-sensors-19-04494">
      <label>23.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wangl</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Yuel</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Shenl</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Real-time Omnidirectional Visual SLAM with Semi-Dense Mapping</article-title>
        <source>Proceedings of the 2018 IEEE Intelligent Vehicles Symposium (IV)</source>
        <conf-loc>Changshu, China</conf-loc>
        <conf-date>26–30 June 2018</conf-date>
        <fpage>695</fpage>
        <lpage>700</lpage>
      </element-citation>
    </ref>
    <ref id="B24-sensors-19-04494">
      <label>24.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Rublee</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Rabaud</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Konolige</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Bradski</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>ORB: An efficient alternative to SIFT or SURF</article-title>
        <source>Proceedings of the 2011 International Conference on Computer Vision</source>
        <conf-loc>Barcelona, Spain</conf-loc>
        <conf-date>6–13 November 2011</conf-date>
        <fpage>2564</fpage>
        <lpage>2571</lpage>
      </element-citation>
    </ref>
    <ref id="B25-sensors-19-04494">
      <label>25.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Kümmerle</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Grisetti</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Strasdat</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Konolige</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Burgard</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>G2o: A general framework for graph optimization</article-title>
        <source>Proceedings of the 2011 IEEE International Conference on Robotics and Automation</source>
        <conf-loc>Shanghai, China</conf-loc>
        <conf-date>9–13 May 2011</conf-date>
        <fpage>3607</fpage>
        <lpage>3613</lpage>
      </element-citation>
    </ref>
    <ref id="B26-sensors-19-04494">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lepetit</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Moreno-Noguer</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Fua</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>EPnP: An accurate O(n) solution to the PnP problem</article-title>
        <source>Int. J. Comput. Vis.</source>
        <year>2009</year>
        <volume>81</volume>
        <fpage>155</fpage>
        <lpage>166</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-008-0152-6</pub-id>
      </element-citation>
    </ref>
    <ref id="B27-sensors-19-04494">
      <label>27.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Furgale</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Rehder</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Siegwart</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Unified temporal and spatial calibration for multi-sensor systems</article-title>
        <source>Proceedings of the 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems</source>
        <conf-loc>Tokyo, Japan</conf-loc>
        <conf-date>3–7 November 2013</conf-date>
        <fpage>1280</fpage>
        <lpage>1286</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="sensors-19-04494-f001" orientation="portrait" position="float">
    <label>Figure 1</label>
    <caption>
      <p>The selected points in a same scene: (<bold>a</bold>) The improved ORB-simultaneous localization and mapping (SLAM) with omnidirectional camera; (<bold>b</bold>) the normal ORB-SLAM; (<bold>c</bold>) Direct Sparse Odometry.</p>
    </caption>
    <graphic xlink:href="sensors-19-04494-g001"/>
  </fig>
  <fig id="sensors-19-04494-f002" orientation="portrait" position="float">
    <label>Figure 2</label>
    <caption>
      <p>The enhanced unified camera model. z is the optical axis.</p>
    </caption>
    <graphic xlink:href="sensors-19-04494-g002"/>
  </fig>
  <fig id="sensors-19-04494-f003" orientation="portrait" position="float">
    <label>Figure 3</label>
    <caption>
      <p>The pinhole camera model.</p>
    </caption>
    <graphic xlink:href="sensors-19-04494-g003"/>
  </fig>
  <fig id="sensors-19-04494-f004" orientation="portrait" position="float">
    <label>Figure 4</label>
    <caption>
      <p>The points <inline-formula><mml:math id="mm52"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm53"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> back project to rays, and the intersection of the two rays is the object point x. From the new matches <inline-formula><mml:math id="mm54"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>↔</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, we can recover the relative pose <inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mi mathvariant="bold">R</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></inline-formula> between two frames and triangulate the point x.</p>
    </caption>
    <graphic xlink:href="sensors-19-04494-g004"/>
  </fig>
  <fig id="sensors-19-04494-f005" orientation="portrait" position="float">
    <label>Figure 5</label>
    <caption>
      <p>Sample images in the datasets: (<bold>a</bold>) room; (<bold>b</bold>) corridor; (<bold>c</bold>) magistrale.</p>
    </caption>
    <graphic xlink:href="sensors-19-04494-g005"/>
  </fig>
  <fig id="sensors-19-04494-f006" orientation="portrait" position="float">
    <label>Figure 6</label>
    <caption>
      <p>Examples of reconstructed map of the omnidirectional ORB-SLAM. Red points are active points, black points are old points, and blue rectangles are keyframes. Dataset: (<bold>a</bold>) room2; (<bold>b</bold>) corridor4.</p>
    </caption>
    <graphic xlink:href="sensors-19-04494-g006"/>
  </fig>
  <fig id="sensors-19-04494-f007" orientation="portrait" position="float">
    <label>Figure 7</label>
    <caption>
      <p>Estimated trajectories. The ground truth is available for the start and end segments in the same room. Dataset: (<bold>a</bold>) corridor4; (<bold>b</bold>) magistrale2.</p>
    </caption>
    <graphic xlink:href="sensors-19-04494-g007"/>
  </fig>
  <fig id="sensors-19-04494-f008" orientation="portrait" position="float">
    <label>Figure 8</label>
    <caption>
      <p>In the red circle, the green lines represent the constraints between keyframes (blue). Our system has more lines than the normal ORB-SLAM. (<bold>a</bold>) The omnidirectional ORB-SLAM; (<bold>b</bold>) The normal ORB-SLAM. (Dataset: room2).</p>
    </caption>
    <graphic xlink:href="sensors-19-04494-g008"/>
  </fig>
  <table-wrap id="sensors-19-04494-t001" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-19-04494-t001_Table 1</object-id>
    <label>Table 1</label>
    <caption>
      <p>Comparison of translation root mean square error (RMSE). DSO = direct sparse odometry.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Sequences</th>
          <th colspan="3" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1">Keyframe Trajectory RMSE (m)<break/>(“X” means LOST)</th>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Length (m)</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">DSO</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Normal ORB-SLAM</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Omnidirectional ORB-SLAM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">corridor1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1252</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">305</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">corridor2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1349</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">322</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">corridor3</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1360</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">300</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">corridor4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">17.6904</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">11.2678</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1196</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">114</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">corridor5</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1159</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">270</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">magistrale1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">20.3902</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">11.5939</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">918</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">magistrale2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">13.4714</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10.0507</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.7239</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">561</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">magistrale3</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">7.4427</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">566</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">magistrale4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">9.9546</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">688</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">magistrale5</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">X</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.2586</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">458</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">room2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.2868</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0623</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0466</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">142</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">room3</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.1806</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0497</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.0446</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">135</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-19-04494-t002" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-19-04494-t002_Table 2</object-id>
    <label>Table 2</label>
    <caption>
      <p>Mean Timing Results (ms).</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Sequences</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">DSO</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Normal ORB-SLAM</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Omnidirectional ORB-SLAM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">corridor4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">137.96</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">24.6</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">30.3</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">magistrale2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">125.17</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">29.1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">30.8</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">room2</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">267.56</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">28.6</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">29.3</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-19-04494-t003" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-19-04494-t003_Table 3</object-id>
    <label>Table 3</label>
    <caption>
      <p>Results for the Relocalization Experiments. KF = Kalman filter.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">System</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">KFs of Initial Map</th>
          <th align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1">Recall of Relocalization (%)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
            <italic>room2, 2081 frames to relocalize</italic>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Normal ORB-SLAM</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">75</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">84.1</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Omni ORB-SLAM</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">52</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.8</td>
        </tr>
        <tr>
          <td colspan="3" align="center" valign="middle" style="border-bottom:solid thin" rowspan="1">
            <italic>room3, 2020 frames to relocalize</italic>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Normal ORB-SLAM</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">93</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">61.6</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Omni ORB-SLAM</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">61</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">76.1</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
