<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with OASIS Tables v1.0 20120330//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing-oasis-article1.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName jpoasis-nisons2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Med Imaging (Bellingham)</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Med Imaging (Bellingham)</journal-id>
    <journal-id journal-id-type="coden">JMIOBU</journal-id>
    <journal-id journal-id-type="publisher-id">JMI</journal-id>
    <journal-title-group>
      <journal-title>Journal of Medical Imaging</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2329-4302</issn>
    <issn pub-type="epub">2329-4310</issn>
    <publisher>
      <publisher-name>Society of Photo-Optical Instrumentation Engineers</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7070161</article-id>
    <article-id pub-id-type="pmid">32206688</article-id>
    <article-id pub-id-type="doi">10.1117/1.JMI.7.4.042803</article-id>
    <article-id pub-id-type="publisher-manuscript">JMI-19234SSR</article-id>
    <article-id pub-id-type="publisher-id">19234SSR</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Special Section on Virtual Clinical Trials</subject>
      </subj-group>
      <subj-group subj-group-type="SPIE-art-type">
        <subject>Paper</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Quantitative imaging feature pipeline: a web-based tool for utilizing, sharing, and building image-processing pipelines</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Mattonen</surname>
          <given-names>Sarah A.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">a</xref>
        <xref ref-type="aff" rid="aff2">b</xref>
        <xref ref-type="aff" rid="aff3">c</xref>
        <xref ref-type="other" rid="b1"/>
        <email>sarah.mattonen@uwo.ca</email>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gude</surname>
          <given-names>Dev</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">a</xref>
        <xref ref-type="other" rid="b2"/>
        <email>dev.gude@dw-systems.com</email>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Echegaray</surname>
          <given-names>Sebastian</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">a</xref>
        <xref ref-type="other" rid="b3"/>
        <email>sechegaray@stanford.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bakr</surname>
          <given-names>Shaimaa</given-names>
        </name>
        <xref ref-type="aff" rid="aff4">d</xref>
        <xref ref-type="other" rid="b4"/>
        <email>sbakr@stanford.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5057-4369</contrib-id>
        <name>
          <surname>Rubin</surname>
          <given-names>Daniel L.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">a</xref>
        <xref ref-type="aff" rid="aff5">e</xref>
        <xref ref-type="aff" rid="aff6">f</xref>
        <xref ref-type="other" rid="b5"/>
        <email>dlrubin@stanford.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Napel</surname>
          <given-names>Sandy</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">a</xref>
        <xref ref-type="aff" rid="aff4">d</xref>
        <xref ref-type="aff" rid="aff5">e</xref>
        <xref ref-type="corresp" rid="cor1">*</xref>
        <xref ref-type="other" rid="b6"/>
        <email>snapel@stanford.edu</email>
      </contrib>
      <aff id="aff1"><label>a</label><institution>Stanford University</institution>, Department of Radiology, Stanford, California, <country>United States</country></aff>
      <aff id="aff2"><label>b</label><institution>The University of Western Ontario</institution>, Department of Medical Biophysics, London, Ontario, <country>Canada</country></aff>
      <aff id="aff3"><label>c</label><institution>The University of Western Ontario</institution>, Department of Oncology, London, Ontario, <country>Canada</country></aff>
      <aff id="aff4"><label>d</label><institution>Stanford University</institution>, Department of Electrical Engineering, Stanford, California, <country>United States</country></aff>
      <aff id="aff5"><label>e</label><institution>Stanford University</institution>, Department of Medicine, Stanford, California, <country>United States</country></aff>
      <aff id="aff6"><label>f</label><institution>Stanford University</institution>, Department of Biomedical Data Science, Stanford, California, <country>United States</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>Address all correspondence to Sandy Napel, E-mail: <email>snapel@stanford.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>3</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <!-- PMC Release delay is 12 months and
						0 days and was based on the <pub-date
						pub-type="epub"/>. -->
    <volume>7</volume>
    <issue>4</issue>
    <elocation-id>042803</elocation-id>
    <history>
      <date date-type="received">
        <day>9</day>
        <month>9</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>2</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 The Authors</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>The Authors</copyright-holder>
      <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
        <license-p>Published by SPIE under a Creative Commons Attribution 4.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.</license-p>
      </license>
    </permissions>
    <self-uri xlink:title="pdf" xlink:href="JMI_7_4_042803.pdf"/>
    <abstract>
      <title>Abstract.</title>
      <p>Quantitative image features that can be computed from medical images are proving to be valuable biomarkers of underlying cancer biology that can be used for assessing treatment response and predicting clinical outcomes. However, validation and eventual clinical implementation of these tools is challenging due to the absence of shared software algorithms, architectures, and the tools required for computing, comparing, evaluating, and disseminating predictive models. Similarly, researchers need to have programming expertise in order to complete these tasks. The quantitative image feature pipeline (QIFP) is an open-source, web-based, graphical user interface (GUI) of configurable quantitative image-processing pipelines for both planar (two-dimensional) and volumetric (three-dimensional) medical images. This allows researchers and clinicians a GUI-driven approach to process and analyze images, without having to write any software code. The QIFP allows users to upload a repository of linked imaging, segmentation, and clinical data or access publicly available datasets (e.g., The Cancer Imaging Archive) through direct links. Researchers have access to a library of file conversion, segmentation, quantitative image feature extraction, and machine learning algorithms. An interface is also provided to allow users to upload their own algorithms in Docker containers. The QIFP gives researchers the tools and infrastructure for the assessment and development of new imaging biomarkers and the ability to use them for single and multicenter clinical and virtual clinical trials.</p>
    </abstract>
    <kwd-group>
      <title>Keywords:</title>
      <kwd>medical image analysis</kwd>
      <kwd>radiomics</kwd>
      <kwd>machine learning</kwd>
      <kwd>feature extraction</kwd>
      <kwd>processing pipeline</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="sp1">
        <funding-source>National Cancer Institute<named-content content-type="fundref:id">https://doi.org/10.13039/100000054</named-content></funding-source>
        <award-id>U01 CA187947</award-id>
        <award-id>U01 CA190214</award-id>
      </award-group>
      <award-group id="sp2">
        <funding-source>Natural Sciences and Engineering Research Council of Canada<named-content content-type="fundref:id">https://doi.org/10.13039/501100000038</named-content></funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="14"/>
      <table-count count="2"/>
      <ref-count count="38"/>
      <page-count count="17"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>running-head</meta-name>
        <meta-value>Mattonen et al.: Quantitative imaging feature pipeline: a web-based tool for utilizing, sharing…</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p>The field of quantitative imaging is rapidly growing, especially in the area of radiomics and machine learning. Radiomics aims to extract quantitative image features from medical images to identify valuable biomarkers of underlying cancer biology.<xref rid="r1" ref-type="bibr"><sup>1</sup></xref><named-content content-type="online"><xref rid="r2" ref-type="bibr"/><xref rid="r3" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r4" ref-type="bibr"><sup>4</sup></xref> These features in combination with machine learning algorithms can be used for diagnosis and to predict clinical outcomes and/or treatment response.<xref rid="r5" ref-type="bibr"><sup>5</sup></xref><named-content content-type="online"><xref rid="r6" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r7" ref-type="bibr"><sup>7</sup></xref> In addition, association of these imaging features with cancer genomics or other patient information may further describe the fundamental biology.<xref rid="r8" ref-type="bibr"><sup>8</sup></xref><named-content content-type="online"><xref rid="r9" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r10" ref-type="bibr"><sup>10</sup></xref> Currently, quantitative image analysis tools are being developed for all disease sites and several imaging modalities to assess outcomes, diagnoses, and/or responses.<xref rid="r11" ref-type="bibr"><sup>11</sup></xref><named-content content-type="online"><xref rid="r12" ref-type="bibr"/><xref rid="r13" ref-type="bibr"/><xref rid="r14" ref-type="bibr"/></named-content><named-content content-type="print"><sup>–</sup></named-content><xref rid="r15" ref-type="bibr"><sup>15</sup></xref> However, current radiomics tools are lacking sufficient evaluation and validation, and there is a lack of translation of these tools into the clinical workflow. This is in part due to the lack of available shared software algorithms and architectures to fully compare and evaluate these quantitative imaging tools across institutions. Similarly, researchers must also have expertise in writing software code to perform many image analysis tasks, including radiomic feature extraction and machine learning. Currently, many open-source pipelines, including Slicer Radiomics,<xref rid="r16" ref-type="bibr"><sup>16</sup></xref> Orange,<xref rid="r17" ref-type="bibr"><sup>17</sup></xref> and KNIME,<xref rid="r18" ref-type="bibr"><sup>18</sup></xref> only process a single image at a time or only perform one quantitative imaging task, such as feature extraction or machine learning.</p>
    <p>Therefore, what is critically needed is a user-friendly platform for sharing and assessing quantitative imaging algorithms. The quantitative imaging feature pipeline (QIFP) is an open-source, web-based platform that allows users access to a wide range of image processing and analysis tools without requiring writing code. Users are also able to upload their own algorithms in Docker containers, which allows the system to evolve and to support code that has been written in a variety of languages. This pipeline gives researchers the tools and infrastructure needed to assess and compare the value of combinations of quantitative image features. For example, researchers may want to create a pipeline that first performs segmentation of a region of interest, then performs feature extraction, and finally trains a machine learning classifier to predict an outcome of interest. The QIFP system allows users to complete these tasks in a single pipeline. It can also allow for the widespread development, assessment, and dissemination of new imaging biomarkers, including the opportunity for external validation of existing software pipelines. This system can also be used to facilitate incorporating quantitative imaging tools into single and multicenter clinical and virtual clinical trials specifically involving image processing, radiomics, and/or machine learning. The QIFP could be used as a central webserver where multiple institutions could upload de-identified imaging data and perform standardized image-processing pipelines.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Architecture</title>
    <p><xref ref-type="fig" rid="f1">Figure 1</xref> shows the architecture of the QIFP, which uses the Common Workflow Language (CWL) execution model and the CWL standard for defining tools and workflows. Simple CWL (json) formatted definitions of tools or workflows can be imported into or exported from the QIFP system. The QIFP leverages Docker for ease of sharing algorithms that have been written in a variety of languages and on a variety of platforms.<xref rid="r19" ref-type="bibr"><sup>19</sup></xref> The entire QIFP is also available as a Docker version for installation on a local server to run within an institutional firewall. The server needs to have at least 4 cores and 64 GB of memory. A detailed user guide is available on the QIFP website<xref rid="r20" ref-type="bibr"><sup>20</sup></xref> under documentation, which provides details on how to perform this installation, including the required Docker-composed file.</p>
    <fig id="f1" orientation="portrait" position="float">
      <label>Fig. 1</label>
      <caption>
        <p>QIFP architecture. The top half of the figure represents a Docker image capable of given task, such as feature extraction or machine learning. The lower half shows the connections to run configuration options files and to various local databases, such as DICOM images/segmentations, clinical features, or workflow results.</p>
      </caption>
      <graphic xlink:href="JMI-007-042803-g001"/>
    </fig>
    <p>The QIFP system, a web application written in java, runs under a Tomcat webserver. When running the QIFP through the webserver, there are no configuration or minimum bandwidth requirements; however, data upload and download speed will be determined by the user’s local network. The QIFP schedules and monitors tools to be executed by a workflow. Each of the blocks in the top half of <xref ref-type="fig" rid="f1">Fig. 1</xref> represents a Docker image capable of, e.g., feature extraction, image conversion, or machine learning. The system acquires the image, semantic, and clinical data from one of various sources [e.g., user’s computer, local database, the electronic Imaging Device (ePAD) system,<xref rid="r21" ref-type="bibr"><sup>21</sup></xref> and the Cancer Imaging Archive (TCIA)<xref rid="r22" ref-type="bibr"><sup>22</sup></xref>]. The appropriate Docker images are then scheduled to run with the input images and clinical data or with the output of a previously scheduled Docker image as input. After each tool has completed, the system stores the output of the tool in the local database and schedules the next tool to be run as defined by the workflow. Once the workflow has completed, it sends an email to the user with a link to the results. The lower half of <xref ref-type="fig" rid="f1">Fig. 1</xref> shows connections to run configuration options files and various local databases.</p>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Interface</title>
    <p>The QIFP is an open-source, web-based system publicly accessible at Ref. <xref rid="r20" ref-type="bibr">20</xref>. After logging into the system with a distinct username and password, users will see the QIFP interface as shown in <xref ref-type="fig" rid="f2">Fig. 2</xref>. Users can request an account on the main login page for the QIFP. Image cohorts are displayed on the left-hand panel and users can choose any of the top menu functions, as described below.</p>
    <fig id="f2" orientation="portrait" position="float">
      <label>Fig. 2</label>
      <caption>
        <p>(A) QIFP interface with the “Images” menu displayed. (B) This example shows the data sources available, with the “Local” data source selected in red. (C) Three local cohorts are available (“myNSCLCData,” “mySCLCData,” and “TCIA NSCLC_Radiogenomics”) with the latter selected. (D) The list of patients in the TCIA NSCLC_Radiogenomics cohort is displayed in the “Image Data” section. (E) Clicking the arrow next to the cohort name allows user to upload images and/or segmentations. (F) Clicking the pencil will allow you to edit details regarding the cohort, including adding or removing other users to this cohort. (G) Clicking the triangle next to the patient name expands all available data for that patient, including studies, series, and annotations. (H) The annotation for this patient (3-D Slicer Segmentation Result) is a DICOM segmentation object. (I) Data can also be downloaded with the down arrow next to the ID or (J) deleted by clicking on the trash can next to the name. (K) Clicking the eye symbol next to an image series opens it in an ePAD image viewer within the window for viewing and potential annotation.</p>
      </caption>
      <graphic xlink:href="JMI-007-042803-g002"/>
    </fig>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Images Menu</title>
      <p>The QIFP maintains its own image repository and can connect to other image sources. <xref ref-type="fig" rid="f2">Figure 2</xref> shows the QIFP user interface when the “Images” menu (along the top row) and the “Local” source are selected, revealing a list of available cohorts in the left-hand panel and the available patient images/annotations in the right-hand panel. Other available sources include: The Cancer Imaging Archive (TCIA),<xref rid="r22" ref-type="bibr"><sup>22</sup></xref> an instance of the ePAD<xref rid="r21" ref-type="bibr"><sup>21</sup></xref> image annotation and storage system, and either the Google or Amazon S3 cloud services [<xref ref-type="fig" rid="f2">Fig. 2(B)</xref>]. QIFP can also be configured to query data from any DICOM compliant local PACS. When QIFP is running on a local server, local cohorts can be created by transferring them from the other sources to the QIFP, or by manually uploading images, segmentations, and/or annotations by clicking on the upload button next to cohort name [<xref ref-type="fig" rid="f2">Fig. 2(E)</xref>]. This will allow users to select a file type to upload and browse to files on their local computer. For example, users can upload a zip file of DICOM or Neuroimaging Informatics Technology Initiative (NIfTI) images and segmentations. Finally, owners of a local cohort can add other QIFP users to the cohort to allow them to access the data and workflows associated with it [<xref ref-type="fig" rid="f2">Fig. 2(F)</xref>].</p>
      <p>Once a cohort has been selected, the QIFP shows the list of DICOM images available in that cohort in the main right-hand panel and lists information on the patient, study, and series. A link to an ePAD image viewer (a freely available open-source DICOM viewer<xref rid="r21" ref-type="bibr"><sup>21</sup></xref>) is also provided to quickly visualize an image series [<xref ref-type="fig" rid="f2">Fig. 2(K)</xref>] and annotate images. Any new annotations (e.g., segmentation seed points) created in ePAD will then be available in the QIFP. Any annotations associated with a specific series are also displayed. Users can select individual series, studies, or patients to process by selecting the box next to each one. Otherwise, users can select the whole cohort by clicking the box next to the cohort name at the top of the screen, or a subset of patients by clicking on each patient and holding down the shift key to select all patients in between. Selected cohorts can then be processed by one of many processing tools and pipelines, described in Secs. 3.5 and 3.6, respectively.</p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>Annotations Menu</title>
      <p>The “Annotations” menu lists all the available image annotations or segmentations for a given cohort. Annotation files can be stored as annotation and image markup (AIM)<xref rid="r23" ref-type="bibr"><sup>23</sup></xref> files or DICOM segmentation objects (DSO). Users can also upload other segmentation file types, e.g., NIfTI file format as described above [<xref ref-type="fig" rid="f2">Fig. 2(E)</xref>],<xref rid="r24" ref-type="bibr"><sup>24</sup></xref> and save them to the local cohort. Only users who are members of the local cohort have access to these segmentations.</p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Models Menu</title>
      <p>Whenever a new predictive model is created through a machine learning workflow, the user has the option to save it and include it in future workflows. The “Models” menu contains a list of previously constructed predictive models that are available in the QIFP system.</p>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Pipeline Results Menu</title>
      <p>Users can see results for past and progress for currently running workflows under the “Pipeline Results” menu. <xref ref-type="fig" rid="f3">Figure 3</xref> shows an example of a workflow in progress. The cohort name is listed on the top left corner of the page and actively running workflows are displayed under “Active Docker Tools.”</p>
      <fig id="f3" orientation="portrait" position="float">
        <label>Fig. 3</label>
        <caption>
          <p>Example of the “Pipeline Results Menu” with a feature extraction workflow in progress. This menu shows all active and completed tools and workflows for the cohort selected prior to invoking this menu. (A) Currently active Docker tool (red box) and the results in progress, including status, elapsed time, parameters used, and access to a log file, pointed to by red arrow. Workflows shown below “Active Docker Tools” are organized by type; for example, clicking the arrow (B) will show all workflows using the PyRadiomics Tool completed or in progress. Clicking on the workflow name, as shown in the red box, will open a new webpage with additional details on that workflow.</p>
        </caption>
        <graphic xlink:href="JMI-007-042803-g003"/>
      </fig>
      <p>While a workflow is running, the Pipeline Results page displays relevant information, such as the elapsed time, the parameters used, log-file entries, and overall status of the workflow. All workflows have an ID name which is based on the tool name (e.g., QIFE), and the date and time that the workflow was started. Users receive an email once the workflow has completed. A log file is also available to provide information on the completed workflow, if it finds an error, and why it may have failed.</p>
    </sec>
    <sec id="sec3.5">
      <label>3.5</label>
      <title>Docker Tools Menu</title>
      <p>The “Docker Tools” menu provides access to all available Docker tools in the system, as well as a “Tool Help.” Currently, the QIFP system has a range of tools available for quantitative image analysis, including preprocessing, segmentation, feature extraction, and machine learning prediction models (<xref rid="t001" ref-type="table">Table 1</xref>). What follows provides a detailed explanation of the tools currently available on the QIFP.</p>
      <table-wrap id="t001" orientation="portrait" position="float">
        <label>Table 1</label>
        <caption>
          <p>Docker tools currently available on the QIFP.</p>
        </caption>
        <!--OASIS TABLE HERE-->
        <table frame="hsides" rules="groups">
          <colgroup>
            <col/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th valign="top">Tool type</th>
              <th align="left" valign="top">Tool name</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="6">Feature extraction<hr/></td>
              <td align="left">PyRadiomics</td>
            </tr>
            <tr>
              <td align="left">2-D JJVector feature extractor</td>
            </tr>
            <tr>
              <td align="left">3-D feature extractor (Mu Zhou)</td>
            </tr>
            <tr>
              <td align="left">3-D feature extractor (QIFE)</td>
            </tr>
            <tr>
              <td align="left">Moffitt feature extractor</td>
            </tr>
            <tr>
              <td align="left">SIFT feature extractor<hr/></td>
            </tr>
            <tr>
              <td rowspan="3">Machine learning prediction engines<hr/></td>
              <td align="left">LASSO train prediction engine</td>
            </tr>
            <tr>
              <td align="left">LASSO test prediction engine</td>
            </tr>
            <tr>
              <td align="left">LASSO randomization prediction engine<hr/></td>
            </tr>
            <tr>
              <td rowspan="6">Preprocessing tools<hr/></td>
              <td align="left">Analyze segmentation to NIfTI conversion</td>
            </tr>
            <tr>
              <td align="left">DICOM-RT to DSO conversion</td>
            </tr>
            <tr>
              <td align="left">NIfTI to DSO conversion</td>
            </tr>
            <tr>
              <td align="left">DICOMs to NIfTI conversion</td>
            </tr>
            <tr>
              <td align="left">DSO to Nifiti conversion</td>
            </tr>
            <tr>
              <td align="left">DICOM validation tool<hr/></td>
            </tr>
            <tr>
              <td rowspan="6">Segmentation<hr/></td>
              <td align="left">Lung segmentation</td>
            </tr>
            <tr>
              <td align="left">Tumor segmentation</td>
            </tr>
            <tr>
              <td align="left">2-D lesion segmentation</td>
            </tr>
            <tr>
              <td align="left">3-D lesion segmentation</td>
            </tr>
            <tr>
              <td align="left">CIP DICOM 3-D segmentation</td>
            </tr>
            <tr>
              <td align="left">CIP NIfTI 3-D segmentation<hr/></td>
            </tr>
            <tr>
              <td rowspan="2">Other</td>
              <td align="left">CoLiAGe feature map</td>
            </tr>
            <tr>
              <td align="left">Delta features</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <sec id="sec3.5.1">
        <label>3.5.1</label>
        <title>Preprocessing tools</title>
        <p>There are many different formats for images, segmentations, and annotations and not all tools can process all formats. Therefore, QIFP contains tools that can be used for file conversion. For example, there are tools to convert between image types (e.g., DICOM, NIfTI) and segmentation types (e.g., DICOM-RT, NIfTI, and DSO). There is also a tool if a user wishes to validate DICOM files to ensure all required DICOM tags are present prior to processing.<xref rid="r25" ref-type="bibr"><sup>25</sup></xref></p>
      </sec>
      <sec id="sec3.5.2">
        <label>3.5.2</label>
        <title>Segmentation</title>
        <p>Currently, there are several segmentation algorithms implemented as Docker tools available on the QIFP. There are two-dimensional (2-D) (2D LesionSeg) and three-dimensional (3-D) (3D LesionSeg) level set-based tumor segmentation tools, which take as input the image and a polygon or long axis line within the lesion.<xref rid="r26" ref-type="bibr"><sup>26</sup></xref><sup>,</sup><xref rid="r27" ref-type="bibr"><sup>27</sup></xref> There is also a Chest Imaging Platform (CIP) Lesion Segmentation tool for DICOM or NIfTI files written by the Applied Chest Imaging Laboratory (Brigham and Women’s Hospital). This tool takes an input image and one or more seed points on the lesion and outputs a segmentation of the lesion of interest.<xref rid="r28" ref-type="bibr"><sup>28</sup></xref> AIM files are required to provide these inputs and specific example templates are provided on QIFP when running the workflow.</p>
      </sec>
      <sec id="sec3.5.3">
        <label>3.5.3</label>
        <title>Feature extraction</title>
        <p>There are several different feature extraction modules that are currently available within the QIFP system. Stanford’s Quantitative Image Feature Extraction (QIFE) tool allows for the extraction of size, shape, intensity, texture, and law’s features.<xref rid="r29" ref-type="bibr"><sup>29</sup></xref>
<xref ref-type="fig" rid="f4">Figure 4</xref> shows an example of how to configure a workflow containing this tool. Users can view, edit, and upload their own configuration file or manually select workflow options through the checkboxes provided. <xref ref-type="fig" rid="f5">Figure 5</xref> shows a completed workflow. All the files produced by the workflow are available for download through a link provided at the bottom of the results, including the log file, the resultant feature file in a comma-separated values (CSV) format, and the configuration file used for that run.</p>
        <fig id="f4" orientation="portrait" position="float">
          <label>Fig. 4</label>
          <caption>
            <p>Example feature extraction workflow using the QIFE. Users can upload a configuration file or manually select configuration options in the interface shown.</p>
          </caption>
          <graphic xlink:href="JMI-007-042803-g004"/>
        </fig>
        <fig id="f5" orientation="portrait" position="float">
          <label>Fig. 5</label>
          <caption>
            <p>Example of a completed feature extraction workflow. The output components are displayed at the bottom. The files for this workflow include (A) the extracted features, (B) the log file describing the results of the workflow, and (C) the configuration file used to run the workflow. Clicking on any of the file names will allow the user to view and/or download them.</p>
          </caption>
          <graphic xlink:href="JMI-007-042803-g005"/>
        </fig>
        <fig id="f6" orientation="portrait" position="float">
          <label>Fig. 6</label>
          <caption>
            <p>Example clinical file required in all LASSO training workflows.</p>
          </caption>
          <graphic xlink:href="JMI-007-042803-g006"/>
        </fig>
        <p>The PyRadiomics tool is another feature extraction engine that has the option to extract higher order wavelet features along with the traditional features on the original images.<xref rid="r16" ref-type="bibr"><sup>16</sup></xref> Additional feature extraction tools include 2-D Riesz features<xref rid="r30" ref-type="bibr"><sup>30</sup></xref> and scale-invariant feature transform (SIFT) features.<xref rid="r31" ref-type="bibr"><sup>31</sup></xref> In general, each feature extraction module has its own set of user-configurable parameters to ensure the workflow is configured to best suit the required data type and analysis.</p>
        <p>Different feature extraction modules may compute radiomic features differently, and for this reason may arrive at different values for what might appear to be the same feature.<xref rid="r32" ref-type="bibr"><sup>32</sup></xref> Common differences may be specifications for directional sampling of voxels for texture features, algorithms used for surface area calculations, and intensity discretization. We refer the user to the manuscripts describing QIFE<xref rid="r29" ref-type="bibr"><sup>29</sup></xref> and PyRadiomics<xref rid="r16" ref-type="bibr"><sup>16</sup></xref> for feature definitions. Also, since each Docker tool will contain a version of the tools from a specific point of time, the version code and Docker ID for each instance of the tool is recorded for each output workflow.</p>
      </sec>
      <sec id="sec3.5.4">
        <label>3.5.4</label>
        <title>Machine learning tools</title>
        <p>Machine learning tools allow for the use of radiomic features with or without clinical features to predict an outcome or clinical parameter of choice (e.g., overall survival, specific gene mutation). The QIFP contains a least absolute shrinkage and selection operator (LASSO)<xref rid="r33" ref-type="bibr"><sup>33</sup></xref> tool, written using the open-source R software<xref rid="r34" ref-type="bibr"><sup>34</sup></xref> (Vienna, Austria). This tool can be configured for training and/or testing classification and regression models. To run a machine learning workflow, the user must also upload the corresponding clinical data and indicate the clinical parameter that they want to predict. An example file demonstrating how the clinical data should be organized is provided when the user sets up a workflow (<xref ref-type="fig" rid="f6">Fig. 6</xref>). As with the feature extraction modules, each machine learning module has its own set of user-defined configuration parameters that can be used to customize the workflow. For example, the configuration file can specify the model type (binomial, Cox, etc.), the elastic-net mixing parameter alpha (LASSO to ridge), feature standardization, and number of folds for cross-validation. Future work is ongoing to add additional classifiers, hyperparameter tuning methods, and unsupervised machine learning techniques.</p>
      </sec>
    </sec>
    <sec id="sec3.6">
      <label>3.6</label>
      <title>Workflows Menu</title>
      <p>This menu allows the user to visualize all currently available workflows, which are also categorized according to their type, including feature extraction, segmentation, or prediction workflows (<xref rid="t002" ref-type="table">Table 2</xref>). User can also create a new workflow from modifications of any existing workflow. Workflows have already been created to run individual or combinations of Docker tools. For example, there is a workflow to run only the PyRadiomics feature extractor and another that will first run PyRadiomics followed by a machine learning engine. Workflows can be customized to include any of the tools listed in <xref rid="t001" ref-type="table">Table 1</xref>.</p>
      <table-wrap id="t002" orientation="portrait" position="float">
        <label>Table 2</label>
        <caption>
          <p>Workflows currently available on the QIFP.</p>
        </caption>
        <!--OASIS TABLE HERE-->
        <table frame="hsides" rules="groups">
          <colgroup>
            <col/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th valign="top">Workflow type</th>
              <th align="left" valign="top">Workflow descriptions</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>QIFE 3-D/2-D features</td>
              <td align="left">All workflows that include the Stanford feature extraction code (QIFE)</td>
            </tr>
            <tr>
              <td>PyRadiomics 3-D features</td>
              <td align="left">All workflows that include the PyRadiomics feature extraction code</td>
            </tr>
            <tr>
              <td>Other 3-D features</td>
              <td align="left">All workflows that include feature extraction code other than QIFE and PyRadiomics</td>
            </tr>
            <tr>
              <td>2-D features</td>
              <td align="left">All workflows that include feature extraction code for 2-D images</td>
            </tr>
            <tr>
              <td>Prediction</td>
              <td align="left">All workflows that include the LASSO prediction tools</td>
            </tr>
            <tr>
              <td>Image conversion</td>
              <td align="left">All workflows that include an image and/or segmentation conversion tool</td>
            </tr>
            <tr>
              <td>Segmentation</td>
              <td align="left">All workflows that contain a segmentation tool</td>
            </tr>
            <tr>
              <td>Other workflows</td>
              <td align="left">All workflows that do not fall into one of the above categories (e.g., semantic features)</td>
            </tr>
            <tr>
              <td>All workflows</td>
              <td align="left">All workflows available on the QIFP</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <sec id="sec3.6.1">
        <label>3.6.1</label>
        <title>Creating and customizing workflows</title>
        <p>The simplest way to create a new workflow for existing Dockers tools on the QIFP is to run the existing “user configurable workflow.” This workflow allows users to manually select the Docker Tools to run, with the option to add/remove components. The configured workflow can then be saved with a new name for future use. Any existing workflow can also be customized by clicking on the “Modify Workflow” bottom beside the workflow name (an example is shown in <xref ref-type="fig" rid="f4">Fig. 4</xref>).</p>
      </sec>
    </sec>
    <sec id="sec3.7">
      <label>3.7</label>
      <title>Other Menus</title>
      <p>The remaining buttons will provide access to user profile information and preferences (Profile) as well as usage statistics and event logs for completed QIFP actions (System Status/Statistics).</p>
      <fig id="f7" orientation="portrait" position="float">
        <label>Fig. 7</label>
        <caption>
          <p>Example of how to add a new Docker Tool. After selecting the Docker Tool menu, click on the “New” option at the top of the left-hand (indicated by the arrow) to add the requirements for a user-supplied tool.</p>
        </caption>
        <graphic xlink:href="JMI-007-042803-g007"/>
      </fig>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>User-Supplied Tools</title>
    <sec id="sec4.1">
      <label>4.1</label>
      <title>Creating and Uploading Tools</title>
      <p>Users are able to upload their own tools to the QIFP by encapsulating them in a Docker container and storing them on DockerHub. For each Docker tool added to the QIFP, a Linux command should be indicated to describe the required inputs and outputs of the tool. For ease of incorporation into the QIFP system, tools can be created in two formats. The first being a tool that works on a single patient/series and a single segmentation or annotation. This type is simpler to implement, since the program does not need to figure out which segmentation refers to which series or to aggregate features/results for all patients. When using this type of tool on a whole cohort, the QIFP system will call the Docker image multiple times and run a separate Docker image for each case. The second option is to create a tool that processes multiple series/segmentations (i.e., an entire cohort). Each patient could have one or more series, and each series could have one or more segmentations/annotations. This type is more difficult to implement and requires using index files that contains file references and the feature results, if any, need to be aggregated into a single file. However, this type of tool is more efficient since only one Docker image is run for the entire data set. <xref ref-type="fig" rid="f7">Figure 7</xref> demonstrates how to upload a Docker tool and what information is required.</p>
    </sec>
  </sec>
  <sec id="sec5">
    <label>5</label>
    <title>Example Workflow</title>
    <p>This section provides a step-by-step example of how to run a workflow on the QIFP using publicly available data on the TCIA: the “NSCLC Radiogenomics” dataset<xref rid="r35" ref-type="bibr"><sup>35</sup></xref> processed by the PyRadiomics feature extraction module, followed by an LASSO predictive modeler. The goal of this example is to predict recurrence (a binary outcome) in this cohort of lung cancer patients. The clinical information for this example can be directly downloaded from the TCIA website.<xref rid="r36" ref-type="bibr"><sup>36</sup></xref> Users can request an account on the main login page for the QIFP.<xref rid="r37" ref-type="bibr"><sup>37</sup></xref></p>
    <sec id="sec5.1">
      <label>5.1</label>
      <title>Selecting the Cohort and Workflow</title>
      <p>To run a workflow, the user must first select the cohort that they would like to analyze by clicking on “Images” menu at the top, choosing the “TCIA” source, and then clicking on the “tcia:NSCLC Radiogenomics” cohort. This example will process 75 patients (R01-001 through ROI-075). To select these, click on the checkbox next to R01-001 then scroll down to R01-075 then press and hold SHIFT while clicking on the checkbox next to R01-075. Next, click on “run workflow” at the top left-hand corner of the window and select the workflow of interest, in this example “PyRadiomics 3D Features → PyRadiomics, LASSO Train” workflow (<xref ref-type="fig" rid="f8">Fig. 8</xref>). This workflow will first perform feature extraction using the PyRadiomics Docker tool and then perform LASSO training to build a predictive model.</p>
      <fig id="f8" orientation="portrait" position="float">
        <label>Fig. 8</label>
        <caption>
          <p>Example execution of a feature extraction and machine learning training workflow using the TCIA Radiogenomics cohort.</p>
        </caption>
        <graphic xlink:href="JMI-007-042803-g008"/>
      </fig>
    </sec>
    <sec id="sec5.2">
      <label>5.2</label>
      <title>Configuring and Running the Workflow</title>
      <p><xref ref-type="fig" rid="f9">Figure 9</xref> shows what information must be provided to successfully run the workflow. Specific text is shown to the left of the block diagram with arrows indicating which of these inputs are required for each specific tool. This includes uploading a file of clinical data, which includes the outcome of interest and a link to the image patient ID. For this example, extract columns A (case ID) and AE (recurrence) from the TCIA clinical data file. All rows for cases beginning with “AMC” can be deleted, so that only “R01” cases remain. The “Case ID” header must also be renamed to “Patient ID,” then these two columns can be saved as a new CSV file. This will be the clinical data file uploaded to run this workflow. In this case, the CSV file must be transposed, and the target feature “recurrence” can be selected from the drop-down menu provided. Configuration files for the feature extraction component and prediction engines can also be uploaded here; however, this example will use default configuration options. Users are also provided with output and processing options for each workflow. Selecting “retain data in local DB” will save the data in a local cohort named “tcia-NSCLC_Radiogenomics” and this will avoid having to redownload the images from TCIA for future processing. After all the selections have been made, clicking on “Upload and Run Workflow” will start the workflow. The status of the workflow can be tracked under the “Pipeline Results” menu. An email will be sent to the user once the workflow has completed with a direct link to the results of the workflow.</p>
      <fig id="f9" orientation="portrait" position="float">
        <label>Fig. 9</label>
        <caption>
          <p>Input screen for the feature extraction and machine learning training workflow applied to the TCIA cohort. The required input requirements are shown to the left of the block diagram. Users have the ability to upload these required files and are provided with example files to illustrate formatting and default files used.</p>
        </caption>
        <graphic xlink:href="JMI-007-042803-g009"/>
      </fig>
    </sec>
    <sec id="sec5.3">
      <label>5.3</label>
      <title>Saving the Prediction Model</title>
      <p>The resultant model parameters can be found in the model.csv file provide as output from the training workflow (<xref ref-type="fig" rid="f10">Fig. 10</xref>). In this example, LASSO selected six features for the final model. <xref ref-type="fig" rid="f10">Figure 10</xref> shows the output from the workflow, including the configuration files, feature extraction, and model results. To save the model and test it on a new cohort, users must go to the “Models” menu and click on “New” on the top of the left-hand panel. Users can name the model and select the appropriate workflow instance and tool instance from the training workflow that was just completed. Once a model is saved, it will appear under the “Models” menu on the left-hand side (<xref ref-type="fig" rid="f11">Fig. 11</xref>).</p>
      <fig id="f10" orientation="portrait" position="float">
        <label>Fig. 10</label>
        <caption>
          <p>Output screen for the feature extraction and machine learning training workflow applied to the TCIA cohort (<xref ref-type="fig" rid="f9">Fig. 9</xref>).</p>
        </caption>
        <graphic xlink:href="JMI-007-042803-g010"/>
      </fig>
      <fig id="f11" orientation="portrait" position="float">
        <label>Fig. 11</label>
        <caption>
          <p>Output screen allowing the saving of the model produced by the workflow applied to the TCIA cohort.</p>
        </caption>
        <graphic xlink:href="JMI-007-042803-g011"/>
      </fig>
    </sec>
    <sec id="sec5.4">
      <label>5.4</label>
      <title>Testing the Prediction Model</title>
      <p>To test this model on a new cohort of patients, patients are selected as described in Sec. <xref ref-type="sec" rid="sec5.1">5.1</xref>. For this example, testing will be done on 25 different patients from the same TCIA NSCLC radiogenomics cohort (patients R01-076 through R01-100) by running the workflow “PyRadiomics 3D Features → PyRadiomics and Lasso Test.” The same clinical data file can be uploaded that was used for testing, since it includes all patients in the TCIA dataset (<xref ref-type="fig" rid="f12">Fig. 12</xref>). Once again, the default feature extraction and prediction configurations will be used. The model that was saved in Sec. <xref ref-type="sec" rid="sec5.3">5.3</xref> above can be selected from the drop-down menu and then the workflow can be started. Once the workflow has completed, the output files will be displayed (<xref ref-type="fig" rid="f13">Fig. 13</xref>), including a list of the resulting model’s features and their coefficients, and an area under the receiver operating characteristic (ROC) curve describing performance (<xref ref-type="fig" rid="f14">Fig. 14</xref>).</p>
    </sec>
  </sec>
  <sec id="sec6">
    <label>6</label>
    <title>Limitations and Future Work</title>
    <p>Although the QIFP is equipped with several preprocessing and feature extraction tools, there is a limited number of machine learning tools available. Future work will include the addition of new feature selection methods and classifiers, including unsupervised machine learning techniques, as well as methods for hyperparameter tuning. Another limitation is that there are currently no cross-validation modules, including random sampling or bootstrapping; however, this is an area of ongoing work. There are also no deep learning tools available on the QIFP; however, since Docker easily allows for sharing algorithms, it would be relatively easy to dockize a pretrained neural net. Finally, the QIFP does not have any built-in data visualization or harmonization tools, important in quantitative imaging and, therefore, another area of future work.</p>
  </sec>
  <sec id="sec7">
    <label>7</label>
    <title>Conclusions</title>
    <p>The QIFP is an open-source, web-based platform that allows users to access, share, and build configurable quantitative image processing pipelines for both planar and volumetric medical images. The QIFP gives researchers the tools and infrastructure for the assessment and evaluation of new imaging biomarkers in single and multicenter clinical and virtual clinical trials. This includes performing all aspects of quantitative imaging, from segmentation to feature extraction and machine learning. The QIFP currently has 68 registered users across 18 institutions and companies in the United States, Canada, and Europe. Any researcher can request an account on the QIFP system using the link provided on the QIFP login page. A detailed user guide is also available on the QIFP website.<xref rid="r38" ref-type="bibr"><sup>38</sup></xref></p>
    <fig id="f12" orientation="portrait" position="float">
      <label>Fig. 12</label>
      <caption>
        <p>Input screen for the feature extraction and machine learning testing workflow applied to the TCIA cohort.</p>
      </caption>
      <graphic xlink:href="JMI-007-042803-g012"/>
    </fig>
    <fig id="f13" orientation="portrait" position="float">
      <label>Fig. 13</label>
      <caption>
        <p>Output screen for the feature extraction and machine learning testing workflow applied to the TCIA cohort (<xref ref-type="fig" rid="f12">Fig. 12</xref>).</p>
      </caption>
      <graphic xlink:href="JMI-007-042803-g013"/>
    </fig>
    <fig id="f14" orientation="portrait" position="float">
      <label>Fig. 14</label>
      <caption>
        <p>Resulting features and area under the ROC curve produced by the feature extraction and machine learning testing workflow applied to the TCIA cohort (<xref ref-type="fig" rid="f13">Fig. 13</xref>). (a) The output pyradiomics.csv file displays all features in the rows and all images in the columns. The first six rows identify the annotation, rows 7 to 262 contain metadata and were removed from the figure. Quantitative features start at row 263 and a subset of 20 of the 900 features is shown. (b) The area under the ROC curve is 0.55. Note that this example is for illustrative purposes only, the cohorts have not been preselected, standardized, or balanced for the outcome of interest, and the performance of the classifier has not been optimized for this dataset, including assessing performance with time-to-event analysis.</p>
      </caption>
      <graphic xlink:href="JMI-007-042803-g014"/>
    </fig>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>The authors would like to acknowledge grant funding (U01 CA187947 and U01 CA190214) from the National Institutes of Health (NIH) National Cancer Institute (NCI). They would also like to acknowledge the Natural Sciences and Engineering Research Council of Canada (NSERC) Postdoctoral Fellowship.</p>
  </ack>
  <notes notes-type="conflict-of-interest">
    <title>Disclosures</title>
    <p>Dr. Sandy Napel is on the Medical Advisory Board for Fovia Inc., a Scientific Advisor for EchoPixel Inc., and a Scientific Advisor for RADLogics Inc. There are no other potential conflicts of interest to disclose.</p>
  </notes>
  <ref-list>
    <title>References</title>
    <ref id="r1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillies</surname><given-names>R. J.</given-names></name><name><surname>Kinahan</surname><given-names>P. E.</given-names></name><name><surname>Hricak</surname><given-names>H.</given-names></name></person-group>, “<article-title>Radiomics: images are more than pictures, they are data</article-title>,” <source>Radiology</source>
<volume>278</volume>(<issue>2</issue>), <fpage>563</fpage>–<lpage>577</lpage> (<year>2015</year>).<pub-id pub-id-type="coden">RADLAX</pub-id><issn>0033-8419</issn><pub-id pub-id-type="doi">10.1148/radiol.2015151169</pub-id><pub-id pub-id-type="pmid">26579733</pub-id></mixed-citation>
    </ref>
    <ref id="r2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambin</surname><given-names>P.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Radiomics: extracting more information from medical images using advanced feature analysis</article-title>,” <source>Eur. J. Cancer</source>
<volume>48</volume>(<issue>4</issue>), <fpage>441</fpage>–<lpage>446</lpage> (<year>2012</year>).<pub-id pub-id-type="coden">EJCAEL</pub-id><issn>0959-8049</issn><pub-id pub-id-type="doi">10.1016/j.ejca.2011.11.036</pub-id><pub-id pub-id-type="pmid">22257792</pub-id></mixed-citation>
    </ref>
    <ref id="r3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Napel</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Quantitative imaging of cancer in the postgenomic era: radio(geno)mics, deep learning, and habitats</article-title>,” <source>Cancer</source>
<volume>124</volume>(<issue>24</issue>), <fpage>4633</fpage>–<lpage>4649</lpage> (<year>2018</year>).<pub-id pub-id-type="coden">CANCAR</pub-id><issn>0008-543X</issn><pub-id pub-id-type="doi">10.1002/cncr.31630</pub-id><pub-id pub-id-type="pmid">30383900</pub-id></mixed-citation>
    </ref>
    <ref id="r4">
      <label>4.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>R.</given-names></name><etal>et al.</etal></person-group>, <source>Radiomics and Radiogenomics: Technical Basis and Clinical Applications</source>, <publisher-name>CRC Press</publisher-name>, <publisher-loc>New York</publisher-loc> (<year>2019</year>).</mixed-citation>
    </ref>
    <ref id="r5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yip</surname><given-names>S. S.</given-names></name><name><surname>Aerts</surname><given-names>H. J.</given-names></name></person-group>, “<article-title>Applications and limitations of radiomics</article-title>,” <source>Phys. Med. Biol.</source>
<volume>61</volume>(<issue>13</issue>), <fpage>R150</fpage> (<year>2016</year>).<pub-id pub-id-type="coden">PHMBA7</pub-id><issn>0031-9155</issn><pub-id pub-id-type="doi">10.1088/0031-9155/61/13/R150</pub-id><pub-id pub-id-type="pmid">27269645</pub-id></mixed-citation>
    </ref>
    <ref id="r6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giger</surname><given-names>M. L.</given-names></name></person-group>, “<article-title>Machine learning in medical imaging</article-title>,” <source>J. Am. Coll. Radiol.</source>
<volume>15</volume>(<issue>3</issue>), <fpage>512</fpage>–<lpage>520</lpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1016/j.jacr.2017.12.028</pub-id><pub-id pub-id-type="pmid">29398494</pub-id></mixed-citation>
    </ref>
    <ref id="r7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parmar</surname><given-names>C.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Machine learning methods for quantitative radiomic biomarkers</article-title>,” <source>Sci. Rep.</source>
<volume>5</volume>, <fpage>13087</fpage> (<year>2015</year>).<pub-id pub-id-type="coden">SRCEC3</pub-id><issn>2045-2322</issn><pub-id pub-id-type="doi">10.1038/srep13087</pub-id><pub-id pub-id-type="pmid">26278466</pub-id></mixed-citation>
    </ref>
    <ref id="r8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sala</surname><given-names>E.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Unravelling tumour heterogeneity using next-generation imaging: radiomics, radiogenomics, and habitat imaging</article-title>,” <source>Clin. Radiol.</source>
<volume>72</volume>(<issue>1</issue>), <fpage>3</fpage>–<lpage>10</lpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1016/j.crad.2016.09.013</pub-id><pub-id pub-id-type="pmid">27742105</pub-id></mixed-citation>
    </ref>
    <ref id="r9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thawani</surname><given-names>R.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Radiomics and radiogenomics in lung cancer: a review for the clinician</article-title>,” <source>Lung Cancer</source>
<volume>115</volume>, <fpage>34</fpage>–<lpage>41</lpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1016/j.lungcan.2017.10.015</pub-id><pub-id pub-id-type="pmid">29290259</pub-id></mixed-citation>
    </ref>
    <ref id="r10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Radiomics and radiogenomics for precision radiotherapy</article-title>,” <source>J. Radiat. Res.</source>
<volume>59</volume>(<issue>Suppl. 1</issue>), <fpage>i25</fpage>–<lpage>i31</lpage> (<year>2018</year>).<pub-id pub-id-type="coden">JRARAX</pub-id><issn>0449-3060</issn><pub-id pub-id-type="doi">10.1093/jrr/rrx102</pub-id><pub-id pub-id-type="pmid">29385618</pub-id></mixed-citation>
    </ref>
    <ref id="r11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y.-Q.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Development and validation of a radiomics nomogram for preoperative prediction of lymph node metastasis in colorectal cancer</article-title>,” <source>J. Clin. Oncol.</source>
<volume>34</volume>(<issue>18</issue>), <fpage>2157</fpage>–<lpage>2164</lpage> (<year>2016</year>).<pub-id pub-id-type="doi">10.1200/JCO.2015.65.9128</pub-id><pub-id pub-id-type="pmid">27138577</pub-id></mixed-citation>
    </ref>
    <ref id="r12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aerts</surname><given-names>H. J.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach</article-title>,” <source>Nat. Commun.</source>
<volume>5</volume>, <fpage>4006</fpage> (<year>2014</year>).<pub-id pub-id-type="coden">NCAOBW</pub-id><issn>2041-1723</issn><pub-id pub-id-type="doi">10.1038/ncomms5006</pub-id><pub-id pub-id-type="pmid">24892406</pub-id></mixed-citation>
    </ref>
    <ref id="r13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Radiomics signature: a potential biomarker for the prediction of disease-free survival in early-stage (I or II) nonsmall cell lung cancer</article-title>,” <source>Radiology</source>
<volume>281</volume>(<issue>3</issue>), <fpage>947</fpage>–<lpage>957</lpage> (<year>2016</year>).<pub-id pub-id-type="coden">RADLAX</pub-id><issn>0033-8419</issn><pub-id pub-id-type="doi">10.1148/radiol.2016152234</pub-id><pub-id pub-id-type="pmid">27347764</pub-id></mixed-citation>
    </ref>
    <ref id="r14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallières</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, “<article-title>A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities</article-title>,” <source>Phys. Med. Biol.</source>
<volume>60</volume>(<issue>14</issue>), <fpage>5471</fpage>–<lpage>5496</lpage> (<year>2015</year>).<pub-id pub-id-type="coden">PHMBA7</pub-id><issn>0031-9155</issn><pub-id pub-id-type="doi">10.1088/0031-9155/60/14/5471</pub-id><pub-id pub-id-type="pmid">26119045</pub-id></mixed-citation>
    </ref>
    <ref id="r15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattonen</surname><given-names>S. A.</given-names></name><etal>et al.</etal></person-group>, “<article-title>[18F] FDG positron emission tomography (PET) tumor and penumbra imaging features predict recurrence in non-small cell lung cancer</article-title>,” <source>Tomography</source>
<volume>5</volume>(<issue>1</issue>), <fpage>145</fpage>–<lpage>153</lpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.18383/j.tom.2018.00026</pub-id><pub-id pub-id-type="pmid">30854452</pub-id></mixed-citation>
    </ref>
    <ref id="r16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Griethuysen</surname><given-names>J. J.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Computational radiomics system to decode the radiographic phenotype</article-title>,” <source>Cancer Res.</source>
<volume>77</volume>(<issue>21</issue>), <fpage>e104</fpage>–<lpage>e107</lpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1158/0008-5472.CAN-17-0339</pub-id><pub-id pub-id-type="pmid">29092951</pub-id></mixed-citation>
    </ref>
    <ref id="r17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demšar</surname><given-names>J.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Orange: data mining toolbox in Python</article-title>,” <source>J. Mach. Learn. Res.</source>
<volume>14</volume>(<issue>1</issue>), <fpage>2349</fpage>–<lpage>2353</lpage> (<year>2013</year>).</mixed-citation>
    </ref>
    <ref id="r18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berthold</surname><given-names>M. R.</given-names></name><etal>et al.</etal></person-group>, “<article-title>KNIME-the Konstanz information miner: version 2.0 and beyond</article-title>,” <source>ACM SIGKDD Explor. Newsletter</source>
<volume>11</volume>(<issue>1</issue>), <fpage>26</fpage>–<lpage>31</lpage> (<year>2009</year>).<pub-id pub-id-type="doi">10.1145/1656274.1656280</pub-id></mixed-citation>
    </ref>
    <ref id="r19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boettiger</surname><given-names>C.</given-names></name></person-group>, “<article-title>An introduction to Docker for reproducible research</article-title>,” <source>ACM SIGOPS Oper. Syst. Rev.</source>
<volume>49</volume>(<issue>1</issue>), <fpage>71</fpage>–<lpage>79</lpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1145/2723872</pub-id></mixed-citation>
    </ref>
    <ref id="r20">
      <label>20.</label>
      <mixed-citation publication-type="web"><person-group person-group-type="author"><name><surname>Napel</surname><given-names>S.</given-names></name><name><surname>Rubin</surname><given-names>D. L.</given-names></name><name><surname>Gude</surname><given-names>D.</given-names></name></person-group>, “<article-title>Quantitative imaging feature pipeline</article-title>,” <ext-link ext-link-type="uri" xlink:href="http://qifp.stanford.edu/">http://qifp.stanford.edu/</ext-link> (accessed <day>3</day>
<month>3</month>
<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="r21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>D. L.</given-names></name><etal>et al.</etal></person-group>, “<article-title>ePAD: an image annotation and analysis platform for quantitative imaging</article-title>,” <source>Tomography</source>
<volume>5</volume>(<issue>1</issue>), <fpage>170</fpage>–<lpage>183</lpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.18383/j.tom.2018.00055</pub-id><pub-id pub-id-type="pmid">30854455</pub-id></mixed-citation>
    </ref>
    <ref id="r22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>K.</given-names></name><etal>et al.</etal></person-group>, “<article-title>The cancer imaging archive (TCIA): maintaining and operating a public information repository</article-title>,” <source>J. Digital Imaging</source>
<volume>26</volume>(<issue>6</issue>), <fpage>1045</fpage>–<lpage>1057</lpage> (<year>2013</year>).<pub-id pub-id-type="coden">JDIMEW</pub-id><pub-id pub-id-type="doi">10.1007/s10278-013-9622-7</pub-id></mixed-citation>
    </ref>
    <ref id="r23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Channin</surname><given-names>D. S.</given-names></name><etal>et al.</etal></person-group>, “<article-title>The caBIG™ annotation and image markup project</article-title>,” <source>J. Digital Imaging</source>
<volume>23</volume>(<issue>2</issue>), <fpage>217</fpage>–<lpage>225</lpage> (<year>2010</year>).<pub-id pub-id-type="coden">JDIMEW</pub-id><pub-id pub-id-type="doi">10.1007/s10278-009-9193-9</pub-id></mixed-citation>
    </ref>
    <ref id="r24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>R.</given-names></name><etal>et al.</etal></person-group>, “<article-title>A (sort of) new image data format standard: NIfTI-1: WE 150</article-title>,” <source>NeuroImage</source>
<volume>22</volume>, <fpage>e1440</fpage> (<year>2004</year>).<pub-id pub-id-type="coden">NEIMEF</pub-id><issn>1053-8119</issn></mixed-citation>
    </ref>
    <ref id="r25">
      <label>25.</label>
      <mixed-citation publication-type="web"><person-group person-group-type="author"><name><surname>Clunie</surname><given-names>D.</given-names></name></person-group>, “<article-title>PixelMed Java DICOM Toolkit</article-title>,” <year>2015</year>, <ext-link ext-link-type="uri" xlink:href="http://www.dclunie.com/pixelmed/software/">http://www.dclunie.com/pixelmed/software/</ext-link>.</mixed-citation>
    </ref>
    <ref id="r26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoogi</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Adaptive local window for level set segmentation of CT and MRI liver lesions</article-title>,” <source>Med. Image Anal.</source>
<volume>37</volume>, <fpage>46</fpage>–<lpage>55</lpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1016/j.media.2017.01.002</pub-id><pub-id pub-id-type="pmid">28157660</pub-id></mixed-citation>
    </ref>
    <ref id="r27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoogi</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Adaptive estimation of active contour parameters using convolutional neural networks and texture analysis</article-title>,” <source>IEEE Trans. Med. Imaging</source>
<volume>36</volume>(<issue>3</issue>), <fpage>781</fpage>–<lpage>791</lpage> (<year>2017</year>).<pub-id pub-id-type="coden">ITMID4</pub-id><issn>0278-0062</issn><pub-id pub-id-type="doi">10.1109/TMI.2016.2628084</pub-id><pub-id pub-id-type="pmid">28113927</pub-id></mixed-citation>
    </ref>
    <ref id="r28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnan</surname><given-names>K.</given-names></name><etal>et al.</etal></person-group>, “<article-title>An open-source toolkit for the volumetric measurement of CT lung lesions</article-title>,” <source>Opt. Express</source>
<volume>18</volume>(<issue>14</issue>), <fpage>15256</fpage>–<lpage>15266</lpage> (<year>2010</year>).<pub-id pub-id-type="coden">OPEXFF</pub-id><issn>1094-4087</issn><pub-id pub-id-type="doi">10.1364/OE.18.015256</pub-id><pub-id pub-id-type="pmid">20640012</pub-id></mixed-citation>
    </ref>
    <ref id="r29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Echegaray</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Quantitative image feature engine (QIFE): an open-source, modular engine for 3D quantitative feature extraction from volumetric medical images</article-title>,” <source>J. Digital Imaging</source>
<volume>31</volume>(<issue>4</issue>), <fpage>403</fpage>–<lpage>414</lpage> (<year>2018</year>).<pub-id pub-id-type="coden">JDIMEW</pub-id><pub-id pub-id-type="doi">10.1007/s10278-017-0019-x</pub-id></mixed-citation>
    </ref>
    <ref id="r30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Depeursinge</surname><given-names>A.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Rotation–covariant texture learning using steerable Riesz wavelets</article-title>,” <source>IEEE Trans. Image Process.</source>
<volume>23</volume>(<issue>2</issue>), <fpage>898</fpage>–<lpage>908</lpage> (<year>2014</year>).<pub-id pub-id-type="coden">IIPRE4</pub-id><issn>1057-7149</issn><pub-id pub-id-type="doi">10.1109/TIP.2013.2295755</pub-id><pub-id pub-id-type="pmid">26270926</pub-id></mixed-citation>
    </ref>
    <ref id="r31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rister</surname><given-names>B.</given-names></name><name><surname>Horowitz</surname><given-names>M. A.</given-names></name><name><surname>Rubin</surname><given-names>D. L.</given-names></name></person-group>, “<article-title>Volumetric image registration from invariant keypoints</article-title>,” <source>IEEE Trans. Image Process.</source>
<volume>26</volume>(<issue>10</issue>), <fpage>4900</fpage>–<lpage>4910</lpage> (<year>2017</year>).<pub-id pub-id-type="coden">IIPRE4</pub-id><issn>1057-7149</issn><pub-id pub-id-type="doi">10.1109/TIP.2017.2722689</pub-id><pub-id pub-id-type="pmid">28682256</pub-id></mixed-citation>
    </ref>
    <ref id="r32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNitt-Gray</surname><given-names>M.</given-names></name><etal>et al.</etal></person-group>, “<article-title>Standardization in quantitative imaging: a multi-center comparison of radiomic features from different software packages on digital reference objects and patient datasets</article-title>,” <source>Tomography</source> in press (Minor revisions requested) (<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="r33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group>, “<article-title>Regression shrinkage and selection via the lasso</article-title>,” <source>J. R. Stat. Soc. Ser. B</source>
<volume>58</volume>(<issue>1</issue>), <fpage>267</fpage>–<lpage>288</lpage> (<year>1996</year>).<pub-id pub-id-type="coden">JSTBAJ</pub-id><issn>0035-9246</issn></mixed-citation>
    </ref>
    <ref id="r34">
      <label>34.</label>
      <mixed-citation publication-type="other"><collab>R Core Team</collab>, “<article-title>R: a language and environment for statistical computing</article-title>,” R Foundation for Statistical Computing, Vienna, Austria, <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="r35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakr</surname><given-names>S.</given-names></name><etal>et al.</etal></person-group>, “<article-title>A radiogenomic dataset of non-small cell lung cancer</article-title>,” <source>Sci. Data</source>
<volume>5</volume>, <fpage>180202</fpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1038/sdata.2018.202</pub-id><pub-id pub-id-type="pmid">30325352</pub-id></mixed-citation>
    </ref>
    <ref id="r36">
      <label>36.</label>
      <mixed-citation publication-type="web"><person-group person-group-type="author"><name><surname>Napel</surname><given-names>S.</given-names></name><name><surname>Plevritis</surname><given-names>S. K.</given-names></name></person-group>, “<article-title>NSCLC Radiogenomics</article-title>,” <ext-link ext-link-type="uri" xlink:href="https://wiki.cancerimagingarchive.net/display/Public/NSCLC+Radiogenomics" specific-use="print">https://wiki.cancerimagingarchive.net/display/Public/NSCLC+Radiogenomics</ext-link> (accessed <day>3</day>
<month>3</month>
<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="r37">
      <label>37.</label>
      <mixed-citation publication-type="web"><person-group person-group-type="author"><name><surname>Napel</surname><given-names>S.</given-names></name><name><surname>Rubin</surname><given-names>D. L.</given-names></name><name><surname>Gude</surname><given-names>D.</given-names></name></person-group>, “<article-title>Quantitative imaging feature pipeline—system login</article-title>,” <ext-link ext-link-type="uri" xlink:href="https://qifp.stanford.edu/qifp/" specific-use="print">https://qifp.stanford.edu/qifp/</ext-link> (accessed <day>3</day>
<month>3</month>
<year>2020</year>).</mixed-citation>
    </ref>
    <ref id="r38">
      <label>38.</label>
      <mixed-citation publication-type="web"><person-group person-group-type="author"><name><surname>Napel</surname><given-names>S.</given-names></name><name><surname>Rubin</surname><given-names>D. L.</given-names></name><name><surname>Gude</surname><given-names>D.</given-names></name></person-group>, “<article-title>Quantitative imaging feature pipeline—documentation</article-title>,” <ext-link ext-link-type="uri" xlink:href="http://qifp.stanford.edu/index.php/2017/03/16/documentation/" specific-use="print">http://qifp.stanford.edu/index.php/2017/03/16/documentation/</ext-link> (accessed <day>3</day>
<month>3</month>
<year>2020</year>).</mixed-citation>
    </ref>
  </ref-list>
  <bio id="b1">
    <p><bold>Sarah A. Mattonen</bold> is an assistant professor at Western University in the Departments of Medical Biophysics and Oncology. She received her PhD in Medical Biophysics from Western University in 2016 and completed her postdoctoral training at Stanford University in the Department of Radiology in 2019. She has received funding from the Natural Sciences and Engineering Research Council of Canada. Her research interests include radiomics and machine learning for translational cancer imaging.</p>
  </bio>
  <bio id="b2">
    <p><bold>Dev Gude</bold> is a research software developer at Stanford University working on the Quantitative Imaging Feature Pipeline project. He received his MSEE degree from the University of Houston and BTech degree from the Indian Institute of Technology, Bombay. He has been a consultant developing software for enterprise web applications for many years.</p>
  </bio>
  <bio id="b3">
    <p><bold>Sebastian Echegaray</bold> received his BS and MS degrees from St. Mary’s University in San Antonio Texas in 2008 and 2010, respectively, and his PhD in EE from Stanford University in 2017. He was formerly a postdoctoral fellow at Stanford, working under Dr. Sandy Napel. He is currently the head of technology at Listo Unlimited Inc., where he is leading the development of new tools and algorithms to provide better financial services to the underserved population.</p>
  </bio>
  <bio id="b4">
    <p><bold>Shaimaa Bakr</bold> received her BSc degree from the American University in Cairo and her MS degree in electrical engineering from Rensselaer Polytechnic Institute, advised by Professor Richard Radke. She is currently a PhD candidate at Stanford University in the Radiological Image and Information Processing Laboratory (RIIPL), led by Professor Sandy Napel.</p>
  </bio>
  <bio id="b5">
    <p><bold>Daniel L. Rubin</bold> received his MD degree from Stanford University in 1985 and his MS degree in biomedical informatics in 2000 and is currently a professor of biomedical data science, radiology, and medicine (biomedical informatics) and, by courtesy, of computer science and ophthalmology at Stanford University. He is director of biomedical informatics of the Stanford Cancer Institute and leads the Laboratory of Quantitative Imaging and Artificial Intelligence (QIAI), where his team develops methods and tools to integrate imaging and other nonimage data and leverage them to create applications that enable precision medicine and precision health.</p>
  </bio>
  <bio id="b6">
    <p><bold>Sandy Napel</bold> received his BSES degree from SUNY Stony Brook in 1974 and his MSEE and PhD degrees in EE from Stanford University in 1976 and 1981, respectively. He was formerly VP of engineering at Imatron Inc., and is currently professor of radiology and, by courtesy, of electrical engineering and medicine (Biomedical Informatics Research) at Stanford University. He coleads the Stanford Radiology 3D and Quantitative Imaging Lab and leads the Radiology Department’s Division of Integrative Biomedical Imaging Informatics, where he is developing techniques for linkage of image features to molecular properties of disease.</p>
  </bio>
</back>
