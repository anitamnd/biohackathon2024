<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Plant Cell</journal-id>
    <journal-id journal-id-type="iso-abbrev">Plant Cell</journal-id>
    <journal-id journal-id-type="hwp">plantcell</journal-id>
    <journal-id journal-id-type="publisher-id">aspb</journal-id>
    <journal-title-group>
      <journal-title>The Plant Cell</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1040-4651</issn>
    <issn pub-type="epub">1532-298X</issn>
    <publisher>
      <publisher-name>American Society of Plant Biologists</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7721323</article-id>
    <article-id pub-id-type="pmid">33037149</article-id>
    <article-id pub-id-type="publisher-id">202000318R2</article-id>
    <article-id pub-id-type="doi">10.1105/tpc.20.00318</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Large-Scale Biology Articles</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ARADEEPOPSIS, an Automated Workflow for Top-View Plant Phenomics using Semantic Segmentation of Leaf States</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3315-2484</contrib-id>
        <name>
          <surname>Hüther</surname>
          <given-names>Patrick</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>a</sup>
        </xref>
        <xref rid="fn1" ref-type="author-notes">
          <sup>1</sup>
        </xref>
        <xref rid="cor1" ref-type="corresp">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3099-7860</contrib-id>
        <name>
          <surname>Schandry</surname>
          <given-names>Niklas</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>a</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>b</sup>
        </xref>
        <xref rid="cor1" ref-type="corresp">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0161-582X</contrib-id>
        <name>
          <surname>Jandrasits</surname>
          <given-names>Katharina</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>a</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-0420-4329</contrib-id>
        <name>
          <surname>Bezrukov</surname>
          <given-names>Ilja</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">
          <sup>c</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3406-4670</contrib-id>
        <name>
          <surname>Becker</surname>
          <given-names>Claude</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>a</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>b</sup>
        </xref>
        <xref rid="fn1" ref-type="author-notes">
          <sup>1</sup>
        </xref>
      </contrib>
      <aff id="aff1"><label>a</label>Gregor Mendel Institute of Molecular Plant Biology (GMI), Austrian Academy of Sciences, Vienna BioCenter (VBC), 1030 Vienna, Austria</aff>
      <aff id="aff2"><label>b</label>Genetics, Faculty of Biology, Ludwig-Maximilians-University München, 82152 Martinsried, Germany</aff>
      <aff id="aff3"><label>c</label>Department of Molecular Biology, Max Planck Institute of Developmental Biology, 72076 Tübingen, Germany</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>2</label>Address correspondence to <email>patrick.huether@gmi.oeaw.ac.at</email> or <email>claude.becker@gmi.oeaw.ac.at</email>.</corresp>
      <fn fn-type="other" id="fn1">
        <label>1</label>
        <p>These authors contributed equally to this work.</p>
      </fn>
      <fn id="afn2" specific-use="misc-author">
        <p>The author responsible for distribution of materials integral to the findings presented in this article in accordance with the policy described in the Instructions for Authors (<ext-link xlink:href="http://www.plantcell.org" ext-link-type="uri">www.plantcell.org</ext-link>) is: Claude Becker (<email>claude.becker@gmi.oeaw.ac.at</email>).</p>
      </fn>
      <fn fn-type="other" id="afn1" specific-use="uri">
        <p>
          <ext-link xlink:href="http://www.plantcell.org/cgi/doi/10.1105/tpc.20.00318" ext-link-type="uri">www.plantcell.org/cgi/doi/10.1105/tpc.20.00318</ext-link>
        </p>
      </fn>
    </author-notes>
    <!--Fake ppub date generated by PMC from publisher
					pub-date/@pub-type='epub-ppub'-->
    <pub-date pub-type="ppub">
      <month>12</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>9</day>
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>9</day>
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <volume>32</volume>
    <issue>12</issue>
    <fpage>3674</fpage>
    <lpage>3688</lpage>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>4</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>28</day>
        <month>9</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>07</day>
        <month>10</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 American Society of Plant Biologists. All rights reserved.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution 4.0 License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:role="icon" xlink:href="TPC_202000318R2_ic1.gif"/>
    <abstract abstract-type="precis">
      <p>ARADEEPOPSIS is a robust, open-source, and easy-to-use pipeline to extract plant phenotypic traits and to classify leaf states from large-scale image data.</p>
    </abstract>
    <abstract>
      <p>Linking plant phenotype to genotype is a common goal to both plant breeders and geneticists. However, collecting phenotypic data for large numbers of plants remain a bottleneck. Plant phenotyping is mostly image based and therefore requires rapid and robust extraction of phenotypic measurements from image data. However, because segmentation tools usually rely on color information, they are sensitive to background or plant color deviations. We have developed a versatile, fully open-source pipeline to extract phenotypic measurements from plant images in an unsupervised manner. ARADEEPOPSIS (<ext-link xlink:href="https://github.com/Gregor-Mendel-Institute/aradeepopsis" ext-link-type="uri">https://github.com/Gregor-Mendel-Institute/aradeepopsis</ext-link>) uses semantic segmentation of top-view images to classify leaf tissue into three categories: healthy, anthocyanin rich, and senescent. This makes it particularly powerful at quantitative phenotyping of different developmental stages, mutants with aberrant leaf color and/or phenotype, and plants growing in stressful conditions. On a panel of 210 natural Arabidopsis (<italic toggle="yes">Arabidopsis thaliana</italic>) accessions, we were able to not only accurately segment images of phenotypically diverse genotypes but also to identify known loci related to anthocyanin production and early necrosis in genome-wide association analyses. Our pipeline accurately processed images of diverse origin, quality, and background composition, and of a distantly related Brassicaceae. ARADEEPOPSIS is deployable on most operating systems and high-performance computing environments and can be used independently of bioinformatics expertise and resources.</p>
    </abstract>
    <funding-group>
      <award-group id="sp1">
        <funding-source>
          <institution-wrap>
            <institution>EC | European Research Council (ERC</institution>
            <institution-id institution-id-type="DOI">http://dx.doi.org/10.13039/501100000781</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id rid="sp1">716823</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group id="sp2">
        <funding-source>
          <institution-wrap>
            <institution>Austrian Academy of Sciences (Österreichische Akademie der Wissenschaften)</institution>
            <institution-id institution-id-type="DOI">http://dx.doi.org/10.13039/501100001822</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group id="sp3">
        <funding-source>
          <institution-wrap>
            <institution>Max-Planck-Gesellschaft (Max Planck Society)</institution>
            <institution-id institution-id-type="DOI">http://dx.doi.org/10.13039/501100004189</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="8"/>
      <table-count count="0"/>
      <equation-count count="5"/>
      <ref-count count="44"/>
      <page-count count="15"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="introduction" id="s1">
    <title>Introduction</title>
    <sec id="s2">
      <title>The Phenotyping Bottleneck in Plant -Omics Studies</title>
      <p>Over the last decades, molecular techniques have steadily increased in throughput while decreasing in cost. A prime example is nucleic acid sequencing, which has followed a trend analogous to Moore’s law in computer science. However, phenotyping methods aimed at determining the physical shape of an organism and measuring morphological parameters have not kept up with this pace, which has caused a “phenotyping bottleneck” (<xref rid="bib16" ref-type="bibr">Furbank and Tester, 2011</xref>) in the design and execution of scientific studies. This phenotyping bottleneck constitutes a major challenge in plant biology as well, due to two major issues: image acquisition and data analysis. Data acquisition in most plant phenotyping scenarios involves acquiring standardized images of plants growing in a controlled environment. Plant phenotyping requires space and a dedicated infrastructure that can accommodate the developmental and growth transitions that occur over the life of a plant. Moreover, plant development needs to be phenotyped over relatively long periods of time. For example, in the case of the model plant Arabidopsis (<italic toggle="yes">Arabidopsis thaliana</italic>), a relatively small and fast-growing species, a phenotyping experiment typically runs for several weeks or months, depending on the phenotype of interest and growth conditions. In their vegetative phase, that is, before they produce shoots, flowers, and seeds, Arabidopsis plants grow as relatively small, flat rosettes and can therefore be considered two-dimensional. Because the entire plant is visible from the top during this phase, top-view phenotyping is a straightforward method to determine size, growth rate, and leaf development. While this type of high-throughput image acquisition has become almost trivial, robustly and faithfully extracting meaningful data from these images has not.</p>
      <fig position="float" fig-type="figure" id="fx1" specific-use="box">
        <graphic xlink:href="TPC_202000318R2_fx1" position="float"/>
      </fig>
      <p>Indeed, overcoming the data acquisition challenge, by means of a dedicated plant phenotyping facility for example, does not mitigate the second cause of the phenotyping bottleneck: data analysis. In the context of image-based phenotyping data, analysis includes automated image processing for object detection, object segmentation, and extraction of quantitative phenotypic traits.</p>
    </sec>
    <sec id="s3">
      <title>Challenges in Image Analysis</title>
      <p>The first key step from images to quantitative phenotypic data is also the most difficult: defining the area in the image that depicts the object of interest, in this case a plant. On a small set of images, this segmentation can be done manually by delineating the plant object using software such as <italic toggle="yes">ImageJ</italic> (<xref rid="bib33" ref-type="bibr">Schindelin et al., 2012</xref>; <xref rid="bib34" ref-type="bibr">Schneider et al., 2012</xref>). On data sets consisting of hundreds or thousands of images, which are typical for experiments from phenotyping platforms, this segmentation task must be automated, both to speed up the process and to neutralize user bias. Commonly used software for segmenting plant objects from digital images rely on color information. In simple terms, such color information is stored in tables, with information on each pixel stored in a corresponding cell. While grayscale images are stored in a single table, where each cell contains the grayscale value of the corresponding pixel, color image information is stored in several such tables. For images using the very common additive Red-Green-Blue (RGB) color model, three separate tables store information for the red (R), green (G), and blue (B) color channel intensities for each pixel. A very simple approach to differentiate plants from background is to assume that, since plants are green, one can assign all pixels that pass a certain threshold in the green channel to the object ‘plant’ and ignore all other pixels. This approach is called binary thresholding and works well if, and only if, the assumption is correct that all plants in the experiment are green and that the remaining area of the image is not. In reality, this assumption is often violated: plants can produce high quantities of anthocyanins under certain circumstances, which give plants a red or purple hue. Plants may also develop chlorosis or become senescent, in which case they turn yellow or brown and, ultimately, white. Moreover, as they grow larger over the course of an experiment, the leaves of neighboring plants can protrude into the image area of the monitored plant, thus creating additional green areas that should not be taken into account. Even the background color can fluctuate, either because of constraints in the experimental setup, or because other organisms, such as algae, start growing on the soil surface. In summary, color-based segmentation is sensitive to image parameters and developmental stage, which are difficult to control in a biological experiment, and therefore often needs to be verified by laborious and time-consuming visual inspection of the individual images to identify and correct erroneous segmentation and artifacts. We therefore sought to develop an alternative, unsupervised, high-throughput approach that is robust against variation in color parameters and image characteristics.</p>
    </sec>
    <sec id="s4">
      <title>Machine Learning Methods in Plant Phenotyping</title>
      <p>Alternative approaches to object segmentation that solve the aforementioned problems of color-based segmentation are available: they are based on machine learning methods and include Gaussian mixture models (<xref rid="bib30" ref-type="bibr">Rother et al., 2004</xref>) or Naïve Bayes classifiers (<xref rid="bib17" ref-type="bibr">Gehan et al., 2017</xref>). While these approaches are highly flexible, their correct implementation on new data sets requires substantial programming knowledge and deeper understanding of the underlying statistical principles.</p>
      <p>More recently, successful alternative approaches to segment particular classes of objects from images have come from the deep learning field. Convolutional neural networks have proven invaluable for image classification and segmentation tasks (<xref rid="bib23" ref-type="bibr">Lecun et al., 1998</xref>; <xref rid="bib20" ref-type="bibr">Krizhevsky et al., 2012</xref>; <xref rid="bib18" ref-type="bibr">He et al., 2016</xref>; <xref rid="bib10" ref-type="bibr">Chollet, 2017</xref>). They perform well on data that has a local structure, which is inherently the case in images where values of neighboring pixels tend to be highly correlated and contain recurring structures such as corners and edges. These models are supervised, meaning that they are provided with a set of manually annotated images containing the “ground truth” for each pixel, from which the model will attempt to derive rules for the classification of pixels based on the training data set. This training process is iterative and usually performed over many thousands of iterations, during which the model attempts to map input images to their corresponding ground truth annotations. In brief, during training, a loss function is calculated to estimate the error between input and output, and the model subsequently tries to minimize this error via back-propagation (<xref rid="bib32" ref-type="bibr">Rumelhart et al., 1986</xref>). Weight matrices throughout the network layers are updated along a gradient, following the chain rule to calculate partial derivatives of the loss function with regard to the layer weights. Due to the nonconvex nature of the loss function, error minimization typically reaches only local minima, requiring careful selection of model parameters and a large and diverse set of training data to avoid overfitting. The latter is usually scarce because ground truth data for supervised learning have to be generated by meticulous manual annotation. Depending on the nature of the desired feature to be extracted, such a task is typically labor intensive and time consuming due to the large number of data points required to train a well-generalizing deep learning model de novo, and hence exceeds the capacity of many experimental research labs.</p>
      <p>Such models have been successfully applied to plant phenotyping for both aerial and root tissue (<xref rid="bib28" ref-type="bibr">Pound et al., 2017</xref>; <xref rid="bib39" ref-type="bibr">Ubbens et al., 2018</xref>; <xref rid="bib44" ref-type="bibr">Yasrab et al., 2019</xref>; <xref rid="bib43" ref-type="bibr">Wang et al., 2020</xref>). The Deep Plant Phenomics (DPP) platform provides a framework to either build models suitable for phenotyping tasks or use pretrained models (<xref rid="bib40" ref-type="bibr">Ubbens and Stavness, 2017</xref>). Because these models are trained from scratch, their ability to generalize to unseen data is limited by the amount of annotations contained in the training set, even if data augmentation such as random cropping, contrast, or brightness adjustments are performed. Despite the lowered bar of entry provided by DPP, training of specialized models still requires programming knowledge, which may prevent nonexpert users from applying these tools to their data.</p>
      <p>Here, we use an alternative approach toward the application of convolutional neural networks in plant phenomics. Our aim was to lower the amount of required training data, while at the same time reaching high accuracy across images sourced from different imaging platforms. To this end, we make use of a well-established large network architecture that had already been trained on the ImageNet (<xref rid="bib14" ref-type="bibr">Deng et al., 2009</xref>) data set, consisting of millions of annotated images. Our approach, named ARADEEPOPSIS (ARAbidopsis DEEP-learning-based OPtimal Semantic Image Segmentation), is an image analysis pipeline for unsupervised plant segmentation and phenotypic trait extraction from large image data sets. Rather than developing a new model for plant segmentation, we used transfer learning by centering our pipeline around the well-established deep-learning model DeepLabV3+, which we re-trained for segmentation of Arabidopsis rosettes in top view. Besides accurately identifying image areas containing green plants (rosettes), our pipeline also faithfully phenotypes very late developmental stages, in which plants often undergo color changes. ARADEEPOPSIS can discriminate three different health states of leaves (green/healthy, high in anthocyanins, and senescent) during image segmentation, during which it is able to accurately segment and quantify plants showing altered phenotypes arising from developmental transitions or physiological responses to environmental stresses. We show how ARADEEPOPSIS can be applied to reliably segment Arabidopsis rosettes independent of shape, age, health state, and background, which is not possible with color-based segmentation algorithms. By noninvasively extracting color index data from the segmented leaf area, our tool delivered highly resolved quantitative data that we successfully applied to a genome-wide association (GWA) analysis of anthocyanin content. ARADEEPOPSIS succeeds in delivering quantitative data on the notoriously difficult phenotype of plant senescence from the analysis of plant images, from which we identified genetic variants that drive premature senescence. Because Arabidopsis is a widely used model species in molecular plant biology and genetics, we centered our pipeline on this species. Additionally, we show that it can accurately segment other species.</p>
      <p>ARADEEPOPSIS requires minimal user input compared to other tools that offer semi-automated analysis; it is open source and available for free. ARADEEPOPSIS provides a convenient all-in-one solution for machine learning-based image analysis and trait-extraction and significantly lowers the bar to nonbioinformaticians who may wish to apply machine learning for plant phenotyping. Because the pipeline is written in <italic toggle="yes">Nextflow</italic> and supports containerization, researchers with little computational background can install and run it on their personal computer, cloud infrastructure, or in a high-performance computing environment. ARADEEPOPSIS is available at <ext-link xlink:href="https://github.com/Gregor-Mendel-Institute/aradeepopsis" ext-link-type="uri">https://github.com/Gregor-Mendel-Institute/aradeepopsis</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s5">
    <title>Results</title>
    <sec id="s6">
      <title>Color-Guided Segmentation Can be Misguided by Image and Object Parameters</title>
      <p>We wished to explore the performance and accuracy of classical color-based segmentation and of a self-learning algorithm for segmenting Arabidopsis rosettes from a large, automated phenotyping experiment. We monitored the growth of 210 Arabidopsis accessions from the 1001 genomes panel (<xref rid="bib36" ref-type="bibr">The 1001 Genomes Consortium, 2016</xref>) in natural soil, under climate-controlled conditions, with six replicates per genotype, from seed to flowering stage. Using an automated phenotyping system, we recorded top-view images twice per day, cropped them to frames containing single pots, and subjected them to rosette area measurements. We first used <italic toggle="yes">LemnaGrid</italic> (Lemnatec), which, similar to other integrated pipelines that perform (semi)-supervised segmentation, relies on color channel information to segment plants in top-view images and is commonly used for Arabidopsis phenotyping (<xref rid="bib3" ref-type="bibr">Arvidsson et al., 2011</xref>; <xref rid="bib27" ref-type="bibr">Minervini et al., 2017</xref>). The implementation of this pipeline resulted in the accurate segmentation of young and healthy plants that were mainly composed of green tissue (<xref rid="fig1" ref-type="fig">Figure 1</xref>). However, because the plants in our data set were grown over a long period in natural soil, many accumulated high levels of anthocyanins in their leaves, either because of genetic determinants or as a general stress response to the natural soil and its abiotic or biotic composition; others showed an onset of senescence at later stages of the experiment. For such images, segmentation often failed completely or resulted in only partial or inaccurate segmentation of the plant (<xref rid="fig1" ref-type="fig">Figure 1</xref>), showing that color-based segmentation is sensitive to deviations from the expected color composition of the object to be detected.</p>
      <fig position="float" fig-type="figure" id="fig1">
        <label>Figure 1.</label>
        <caption>
          <p>Performance of Color-Based Versus Semantic Segmentation.</p>
          <p>The left-most column shows the original RGB images of a representative Arabidopsis plant from our phenotyping experiment at three different developmental stages. The second column shows the same images transformed into the` Lab color space, which enhances contrast between green and other colors. Binary thresholding (third column), based on Lab input and used for color-based segmentation, results in difficulties to correctly segment older or anthocyanin-containing plants. Semantic segmentation by our models A and C is insensitive to color and contrast of the plant (two right-most columns). For an explanation of the model C color code, see <xref rid="fig2" ref-type="fig">Figure 2</xref>. Scale bar = 1 cm.</p>
        </caption>
        <graphic xlink:href="TPC_202000318R2_f1" position="float"/>
      </fig>
    </sec>
    <sec id="s7">
      <title>Training of a Proof-of-Principle Model</title>
      <p>We hypothesized that self-learning algorithms would achieve higher accuracy while at the same time be more robust against shifts in color patterns. In a process referred to as transfer learning, trained model architectures that do well at pixel classification can be re-trained on new data that contain new classes while retaining already trained weights in layers that extract low-level features such as shapes and edges. In other words, a model may be re-trained on plant rosettes even when the initial data set the model was trained on does not contain plant rosettes. Model design is a complex task, and experts in the field of machine learning have invested considerable effort into establishing sophisticated model architectures that can be built upon. Moreover, because re-training a model mainly consists of updating the last layer of the model, transfer learning on a model that already performs well at image segmentation is much faster and requires less training data than designing and training a new model from scratch, since the established model is already able to extract features from image data and is merely learning a new classification output (<xref rid="bib29" ref-type="bibr">Razavian et al., 2014</xref>).</p>
      <p>To test whether transfer learning of the well-established segmentation model DeepLabV3+ would result in a model that could faithfully extract Arabidopsis rosettes from top-view images, independently of developmental stage or phenotype, we generated a 'ground-truth' training set. We generated a small data set of 300 images, in which we manually annotated the rosette area, deliberately excluding senescent leaves. From these 300 annotations, we randomly selected 80% (240 images) for model training and kept the remaining 20% for evaluating the accuracy of the new model.</p>
      <p>After training, this model (referred to hereafter as model A) performed well on plants at various developmental stages and with different leaf coloring (<xref rid="fig1" ref-type="fig">Figure 1</xref>), which encouraged us to generate a much larger ground-truth data set for more fine-grained models (<ext-link xlink:href="https://doi.org/10.5281/zenodo.3946393" ext-link-type="uri">https://doi.org/10.5281/zenodo.3946393</ext-link>).</p>
    </sec>
    <sec id="s8">
      <title>Advanced Models for Differentiated Plant Area Classification of Individual Leaves</title>
      <p>From the example images shown in <xref rid="fig1" ref-type="fig">Figure 1</xref>, it is apparent that plant health changes not only in the context of the whole rosette but also between individual leaves or parts of leaves, with some accumulating high anthocyanin levels and others entering senescence. We therefore asked if we could train a model that would semantically extract such features from leaves and assign them to different classes, depending on their phenotypic appearance. Not only would such a model allow finer resolution when assessing the overall health of the plant, it would also enable segmenting differently colored areas.</p>
      <p>Reasoning that it should be possible to segment senescent leaves separately, we generated a second, larger, and more finely annotated training data set, consisting of 1375 manually annotated top-view images of Arabidopsis plants from the phenotyping experiment described above, from which we again withheld 20% for later validation. The images used for annotation were selected semi-randomly such that they covered various developmental stages and included healthy-looking as well as stressed plants from different genotypes that exhibited altered color profiles due to anthocyanin accumulation or senescence, and that were diverse in leaf-morphology. Instead of manually annotating the entire rosette, as we had done for the initial training set, we annotated all leaves of each rosette individually and manually assigned each leaf to one of three different classes, depending on its appearance: green; anthocyanin-rich; senescent or dead. From these annotations, we generated image masks for two additional models complementing the initial one-class model A. For the two-class model B, we classified senescent (class_senesc) versus nonsenescent leaves (class_norm), whereas the three-class model C was trained to differentiate between senescent (class_senesc), anthocyanin-rich (class_antho), and green (class_norm) areas (<xref rid="fig2" ref-type="fig">Figure 2A</xref>). We then used these three annotated sets of masks for transfer learning of a publicly available <italic toggle="yes">xception65</italic> (<xref rid="bib10" ref-type="bibr">Chollet, 2017</xref>) based DeepLabV3+ checkpoint that had been pretrained on the ImageNet data set (see Methods and https://doi.org/10.5281/zenodo.3946618; <xref rid="bib8" ref-type="bibr">Chen et al., 2017</xref>; <xref rid="bib9" ref-type="bibr">Chen et al., 2018</xref>).</p>
      <fig position="float" fig-type="figure" id="fig2">
        <label>Figure 2.</label>
        <caption>
          <p>The Three Default Models Available Through ARADEEPOPSIS.</p>
          <p><bold>(A)</bold> Example of segmentation results from the one-class model A, the two-class model B, and the three-class model C on a single Arabidopsis individual.</p>
          <p><bold>(B)</bold> Comparison of the percentage of classified pixels across all 148,792 images between the three available models (colors as in C; black = background).</p>
          <p><bold>(C)</bold> Relative number of pixels classified as green, anthocyanin-rich, or senescent by model C over time. The mean percentage of pixels assigned to each class is shown per day. Error bars indicate 95% confidence intervals.</p>
          <p><bold>(D)</bold> Scatterplots showing pairwise correlations between measurements of plant area (includes all leaf states except senescence) between models A, B, and C.</p>
        </caption>
        <graphic xlink:href="TPC_202000318R2_f2" position="float"/>
      </fig>
      <p>After training of all three models had completed, we assessed their performance according to the mean intersection over union (mIoU) for all pixel-level classifications, which is defined as the mean fraction of true positives divided by the sum of true positives, false positives, and false negatives over all annotated classes. When applied to the validation data set, models A, B, and C reached an mIoU of 96.8%, 84.0%, and 83.1%, respectively.</p>
      <p>Next, we compared how each model performed at segmenting the 148,792 rosette images comprising our data set. First, we asked how much the different classes contributed to the area segmented as 'plant'. Averaged across all images, model A, which ignores senescent leaves, classified 12.5% of image area as plant tissue (<xref rid="fig2" ref-type="fig">Figure 2B</xref>). Models B and C both classified ∼14% of image area as plant tissue (including senescent areas ignored by model A; <xref rid="fig2" ref-type="fig">Figure 2B</xref>). The fraction of class_senesc segmentation was almost identical in models B and C. To further test whether our most complex model, model C, performed well, we analyzed the ratio of the three classes over the course of the experiment. As expected, we saw a sharp increase in anthocyanin-rich area beginning 30 d after sowing, followed by an increase in senescent segmentations 10 d later (<xref rid="fig2" ref-type="fig">Figure 2C</xref>). Relative classification of green tissue decreased accordingly. This observation demonstrated that model C successfully captured an increase in senescence as plants grew older, accurately reflecting plant development.</p>
      <p>Next, we determined whether the three models behaved differently in segmenting the combined rosette area. Using the total pixel area classified as leaves from each image, we found that models B (two-class) and C (three-class) displayed the strongest correlation (R<sup>2</sup> = 0.998; <xref rid="fig2" ref-type="fig">Figure 2D</xref>). Model A, our one-class model trained on annotations of whole rosettes rather than individual leaves, correlated slightly less well with models B and C (R<sup>2</sup> = 0.985 and 0.982, respectively) and had a tendency to segment larger areas than either of the other two models (<xref rid="fig2" ref-type="fig">Figure 2D</xref>). Visual inspection of overlays of the segmentations produced by model A with the original image revealed that segmentations produced by model A frequently included areas between leaves that were ignored by models B and C, explaining the larger rosette sizes measured by model A (see exemplary segmentations in <xref rid="fig2" ref-type="fig">Figure 2A</xref>). We believe that these differences are related to the different annotation strategies (whole rosettes in model A vs. more refined per-leaf annotations for the larger data sets in models B and C). When generating the ground-truth training set for models B and C, we noticed how nontrivial it was to make consistent decisions as to when a leaf should be annotated as senescent or rich in anthocyanins, as the transitions are gradual, hence classification can become subjective. This lack of total precision during annotation of the training set might also explain the decrease in mIoU scores that we observed with the two- and three-class models. We attempted to mitigate user bias by having several individuals contributing to generating the annotations for a subset of images, thus ensuring that the model would faithfully learn the features of interest.</p>
    </sec>
    <sec id="s9">
      <title>Extraction of Morphometric and Color Index Traits from Segmented Images</title>
      <p>Next, we used the segmentation masks obtained from the models created by transfer learning to extract morphometric and color index traits associated with each plant (<xref rid="fig3" ref-type="fig">Figure 3</xref>), using the Python library <italic toggle="yes">scikit-image</italic> (<xref rid="bib41" ref-type="bibr">van der Walt et al., 2014</xref>; <xref rid="bib13" ref-type="bibr">Del Valle et al., 2018</xref>). Besides the total number of pixels annotated as 'plant', the pipeline returns the equivalent diameter, that is, the diameter of a circle with the same number of pixels as the plant area. The major and minor axes of an object-surrounding ellipse are a measure of the aspect ratio of the plant and inform on whether the rosette is rather round or elongated along one axis (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). The area of a convex hull surrounding the object is a measure of plant size; the solidity of the plant object is calculated by dividing the area of the plant by the area of the convex hull. Finally, dividing the plant area by the area of its bounding box, that is, a minimal box enclosing the object, is representative of the overall extent of the plant (<xref rid="fig3" ref-type="fig">Figure 3A</xref>). Depending on the type of downstream analyses, these indirect object parameters can be used as proxies for overall or specific growth.</p>
      <fig position="float" fig-type="figure" id="fig3">
        <label>Figure 3.</label>
        <caption>
          <p>Morphometric and Color Index Measurements Extracted from Segmentation Masks.</p>
          <p><bold>(A)</bold> Morphometric traits extracted using the Python library <italic toggle="yes">scikit-image</italic> (<xref rid="bib41" ref-type="bibr">van der Walt et al., 2014</xref>).</p>
          <p><bold>(B)</bold> Separation of the segmented plant image into red <bold>(R)</bold>, green <bold>(G)</bold>, and blue <bold>(B)</bold> color channels.</p>
          <p><bold>(C)</bold> Color channel indices calculated as described in <xref rid="bib13" ref-type="bibr">Del Valle et al. (2018)</xref>. The differently colored plants represent mean values of the different color channels; see <bold>(B)</bold>.</p>
        </caption>
        <graphic xlink:href="TPC_202000318R2_f3" position="float"/>
      </fig>
      <p>The segmentation masks are also used to extract color index information from the plant area (<xref rid="fig3" ref-type="fig">Figure 3B</xref>). These traits are based on the intensity of each RGB color channel per pixel. Simple color index traits are, for example, the average intensity of the green channel over all pixels classified as 'plant.' We have implemented the color index traits described by <xref rid="bib13" ref-type="bibr">Del Valle et al. (2018)</xref> in our pipeline (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). These traits are calculated for each class individually and for a compound class termed “plant_region,” which contains both class_norm and class_antho (only for model C). Details on the color index traits are shown in <xref rid="fig3" ref-type="fig">Figure 3C</xref> and provided in Methods.</p>
    </sec>
    <sec id="s10">
      <title>The ARADEEPOPSIS Pipeline</title>
      <p>The ARADEEPOPSIS (ARAbidopsis DEEP-learning-based OPtimal Semantic Image Segmentation) pipeline integrates the unsupervised image segmentation with the trait extraction outlined above. The segmentation results are easily browsable with a custom <italic toggle="yes">Shiny</italic> application, allowing for straightforward quality assessment of the obtained results. ARADEEPOPSIS is written in <italic toggle="yes">Nextflow</italic> (<xref rid="bib15" ref-type="bibr">Di Tommaso et al., 2017</xref>), a high-level data-science language used to implement bioinformatics workflows and geared toward increasing portability of these workflows across different hardware and scheduling systems. The pipeline is fully open-source and licensed under GPLv3; the technical details are laid out in the “Methods” sections. Briefly, the pipeline takes a folder of images as an input, splits the total image set into batches of equal size, and performs semantic segmentation on the images using a model of choice (see section Advanced Models for Differentiated Plant Area Classification of Individual Leaves). The semantic segmentation output is then used to extract morphometric and color index traits from each individual image (see section Extraction of Morphometric and Color Index Traits from Segmented Images). Quality control and exploratory analyses can be performed after the pipeline has finished by launching a bundled <italic toggle="yes">Shiny</italic> (<xref rid="bib31" ref-type="bibr">RStudio, 2014</xref>) application. In addition to offering some straightforward visualization of the results, <italic toggle="yes">Shiny</italic> also provides an interface to merge metadata and ARADEEPOPSIS output for downstream analysis (<xref rid="fig4" ref-type="fig">Figure 4</xref>; <xref rid="bib15" ref-type="bibr">Di Tommaso et al., 2017</xref>). ARADEEPOPSIS is built modularly, and trait extraction works independently of the model: that is, although our pipeline comes with the three described models for plant segmentation (models A, B, and C; <xref rid="fig2" ref-type="fig">Figure 2</xref>), it offers flexibility for experienced users, via the option to feed in custom <italic toggle="yes">TensorFlow</italic> models (e.g., created using DPP). Alternatively, users can choose to use only the trait extraction and visualization steps of the pipeline by providing their own segmentation masks along with the original images.</p>
      <fig position="float" fig-type="figure" id="fig4">
        <label>Figure 4.</label>
        <caption>
          <p>The ARADEEPOPSIS Pipeline.</p>
          <p>A folder containing image files is passed to the pipeline. The total number of images is split into batches of a fixed size (indicated by different background colors). Batches are processed in parallel: first, the segmentation is performed on each batch, then traits are extracted from the segmented images. The output from all batches is collected in one final table, the results table. In addition, the pipeline produces diagnostic images that show the segmentation results overlaid on the original image, color-coded segmentation masks, and background-subtracted plant rosettes. These diagnostic images can be explored in a <italic toggle="yes">Shiny</italic> (<xref rid="bib31" ref-type="bibr">RStudio, 2014</xref>) app, which is launched at the end of an ARADEEPOPSIS run, such that users can visually inspect the segmentations and verify their accuracy.</p>
        </caption>
        <graphic xlink:href="TPC_202000318R2_f4" position="float"/>
      </fig>
      <p>ARADEEPOPSIS is designed to segment plants from images that contain only a single plant, and we do not recommend usage on images containing multiple plants. In case the phenotyping system records whole or partial trays of plant pots, as is common for many automated phenotyping platforms, users will need to preprocess these images and divide them into sub-images, each containing a single individual. Unless explicitly stated otherwise, we use ARADEEPOPSIS to refer to the pipeline using model C (<xref rid="fig2" ref-type="fig">Figure 2</xref>) from here on.</p>
    </sec>
    <sec id="s11">
      <title>Validation and Generalization to Public Phenotyping Data Sets and Comparison to Other Models</title>
      <p>To validate the accuracy of our pipeline, we re-analyzed publicly available images and compared the output of ARADEEPOPSIS to the published analyses. This validation step served two purposes: first, we wished to confirm that the rosette features extracted by ARADEEPOPSIS segmentation were accurately reproducing published data. Second, we wanted to test whether our pipeline would remain robust when using data generated on different phenotyping platforms, with different image recording systems, pot sizes and shapes, and background composition. We used images from three published studies (<xref rid="bib6" ref-type="bibr">Baute et al., 2017</xref>; <xref rid="bib7" ref-type="bibr">Capovilla et al., 2018</xref>; <xref rid="bib5" ref-type="bibr">Barragan et al., 2019</xref>), generated on either the RAPA platform (<xref rid="bib42" ref-type="bibr">Vasseur et al., 2018</xref>) at the Max Planck Institute for Developmental Biology in Tübingen, Germany (<xref rid="bib7" ref-type="bibr">Capovilla et al., 2018</xref>; <xref rid="bib5" ref-type="bibr">Barragan et al., 2019</xref>), or the Weighing, Imaging and Watering Machine platform (<ext-link xlink:href="https://www.psb.ugent.be/phenotyping-platforms" ext-link-type="uri">https://www.psb.ugent.be/phenotyping-platforms</ext-link>) at the VIB in Ghent, Belgium (<xref rid="bib6" ref-type="bibr">Baute et al., 2017</xref>; <xref rid="bib12" ref-type="bibr">Coppens et al., 2017</xref>), and re-analyzed them using ARADEEPOPSIS.</p>
      <p>Correlation analysis of the published rosette area derived from the RAPA system and our own measurements resulted in R<sup>2</sup> values of 0.996 (<xref rid="bib7" ref-type="bibr">Capovilla et al., 2018</xref>) and 0.979 (<xref rid="bib5" ref-type="bibr">Barragan et al., 2019</xref>), respectively (<xref rid="fig5" ref-type="fig">Figures 5A and 5B</xref>). Despite this overall very strong correlation, we noticed individual images in which our segmentation disagreed with the published record. When inspecting the respective images and segmentation masks from both analyses, we discovered that the respective rosette segmentations in the original analysis were inaccurate or incomplete, and that ARADEEPOPSIS segmentation was in strong agreement with the actual rosette in the image, even when plants had strong aberrant phenotypes or grew on low-contrast background (<xref rid="fig5" ref-type="fig">Figures 5A and 5B</xref>). We also used the pretrained <italic toggle="yes">vegetationSegmentation</italic> model from DPP to segment these two data sets and noticed a markedly lower to between DPP-derived and published measurements (<ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Figure 1A</ext-link>) when compared with ARADEEPOPSIS.</p>
      <fig position="float" fig-type="figure" id="fig5">
        <label>Figure 5.</label>
        <caption>
          <p>Validation of ARADEEPOPSIS Output Against Published Data.</p>
          <p><bold>(A)</bold> Validation against data from <xref rid="bib7" ref-type="bibr">Capovilla et al. (2018)</xref>. The left-most panel shows the correlation between values produced by ARADEEPOPSIS against published data; the boxed area is magnified in the second panel to highlight disagreements. The third panel shows the original image corresponding to the red data point in panel two. Panel four shows the original segmentation, panel five the segmentation by ARADEEPOPSIS.</p>
          <p><bold>(B)</bold> Validation against <xref rid="bib5" ref-type="bibr">Barragan et al. (2019)</xref> with the same order as in <bold>(A)</bold>.</p>
          <p><bold>(C)</bold> Validation against the A1 (top row), A2 (middle), and A4 (bottom) Arabidopsis image series (A1, A2, and A4) from the IPPN data set (<xref rid="bib26" ref-type="bibr">Minervini et al., 2016</xref>). The ground-truth mask corresponds to the manually annotated foreground mask provided by IPPN.</p>
        </caption>
        <graphic xlink:href="TPC_202000318R2_f5" position="float"/>
      </fig>
      <p>Re-analysis of the Weighing, Imaging and Watering Machine data set (<xref rid="bib6" ref-type="bibr">Baute et al., 2017</xref>) also showed high correlation between the published measurements and our analysis (<ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Figure 1B</ext-link>). Unfortunately, original segmentations were not available for these images, but when inspecting some of the outliers, we again noticed highly accurate segmentation by ARADEEPOPSIS relative to the actual image (<ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Figure 1A</ext-link>).</p>
      <p>To further validate the ARADEEPOPSIS pipeline, we assessed its performance on the International Plant Phenotyping Network (IPPN) data set (<ext-link xlink:href="https://www.plant-phenotyping.org/data" ext-link-type="uri">https://www.plant-phenotyping.org/data</ext-link> sets-download; <xref rid="bib26" ref-type="bibr">Minervini et al., 2016</xref>), which contains diverse images of plants with a range of backgrounds and varying image qualities. Importantly, this data set was not part of the training data set used during transfer learning. We used the ground-truth annotation masks of the IPPN data set with ARADEEPOPSIS to extract the respective ground-truth traits. We found that the models provided by ARADEEPOPSIS generalized well to this data set, although we noticed that some of the models (B and C) occasionally segmented background into classes corresponding to anthocyanin-rich or senescent, or misclassified algae as leaf tissue (<ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Figure 2B</ext-link>). Such artifacts can be reduced by running ARADEEPOPSIS with multi-scale inference, which improves segmentation accuracy by scaling the input image by different scale factors before model prediction and taking the average over these multiple scales for classification of individual pixels. With multi-scale inference, the segmentations of green plant tissue (ARADEEPOPSIS model C) agreed with the ground-truth annotations available for the IPPN data set (<xref rid="fig5" ref-type="fig">Figure 5C</xref>) and correlated better than those obtained when running our pipeline with the pretrained DPP vegetation segmentation model (<ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Figure 3</ext-link>). Because multi-scale inference results in higher computational demand, single-scale inference is the default setting; users will need to choose based on the availability of computational resources.</p>
    </sec>
    <sec id="s12">
      <title>Validation by GWA Studies</title>
      <sec id="s13">
        <title>Validation of Color Index Traits</title>
        <p>Often, the ultimate purpose of phenotypic measurements is to relate them to genetic data to understand the underlying genetic determinants. We therefore wanted to test whether ARADEEPOPSIS provided sufficiently high-quality data to identify genetic associations in Arabidopsis by GWA studies, relying on the 1001 genomes variant information (<xref rid="bib36" ref-type="bibr">The 1001 Genomes Consortium, 2016</xref>). Instead of morphometric traits such as size and growth rate, which are usually highly polygenic and therefore difficult to test, we wondered if we could make use of additional layers of image information provided by ARADEEPOPSIS as a proxy for physiological parameters. We hypothesized that RGB color information of the plant, that is, the respective pixel intensities in the three color channels, would provide information on anthocyanin content and hence on the physiological state of the plant, as it was previously shown that tissue color highly correlates with anthocyanin content (<xref rid="bib13" ref-type="bibr">Del Valle et al., 2018</xref>). Using the ARADEEPOPSIS segmentations, which accurately reflect the area covered by the rosette, we extracted color information by collecting data from the different RGB channels from the segmented object area, as explained above (<xref rid="fig3" ref-type="fig">Figures 3B and 3C</xref>). We then used <italic toggle="yes">LIMIX</italic> (<xref rid="bib24" ref-type="bibr">Lippert et al., 2014</xref>) to perform GWA analysis on the “chroma ratio” of the rosette area 37 d after sowing. The “chroma ratio” is calculated by dividing the mean intensities from the green channel by half of the sum of the mean intensities from the blue and red channels (<xref rid="fig3" ref-type="fig">Figure 3C</xref>). The chroma ratio is therefore inversely correlated with anthocyanin accumulation and thus increases when anthocyanin content is low. We observed strong associations with regions on chromosomes 1, 2, 4, and 5 (<xref rid="fig6" ref-type="fig">Figure 6A</xref>). When ranked by -log<sub>10</sub>(p), the second-highest ranking Single Nucleotide Polymorphism (SNP) mapped to chromosome 4, with the closest annotated gene being <italic toggle="yes">ANTHOCYANINLESS2</italic> (<italic toggle="yes">ANL2;</italic> At4g00730). <italic toggle="yes">ANL2</italic> encodes a homeobox transcription factor that has been implicated in controlling anthocyanin accumulation in sub-epidermal tissues (<xref rid="bib21" ref-type="bibr">Kubo et al., 1999</xref>). Mutants in <italic toggle="yes">ANL2</italic> accumulate less anthocyanins in sub-epidermal leaf tissue. In our data, accessions carrying the alternative allele near <italic toggle="yes">ANL2</italic> displayed a higher chroma ratio, indicative of lower anthocyanin accumulation (<xref rid="fig6" ref-type="fig">Figure 6B</xref>). This result demonstrates that ARADEEPOPSIS can noninvasively extract quantitative color index data from whole rosettes that can be applied to genetic association analysis.</p>
        <fig position="float" fig-type="figure" id="fig6">
          <label>Figure 6.</label>
          <caption>
            <p>GWA Analyses based on ARADEEPOPSIS Output.</p>
            <p><bold>(A)</bold> Results of GWA on the trait “chroma ratio” 37 d after sowing. The log<sub>10</sub>-transformed <italic toggle="yes">p</italic>-value for each SNP is plotted (only SNPs with minor allele frequency &gt;5% are shown). SNPs that are significant after Bonferroni correction are shown in blue. For the two highest-ranking SNPs, the closest gene is indicated.</p>
            <p><bold>(B)</bold> Chroma ratio of plants 37 d after sowing, split by accessions carrying the reference and alternative alleles of the SNP close to At2g00730 (<italic toggle="yes">ANTHOCYANINLESS2</italic>, <italic toggle="yes">ANL2</italic>), highlighted in <bold>(A)</bold>.</p>
            <p><bold>(C)</bold> Same as <bold>(A)</bold> for the trait “relative senescence” 25 d after sowing. At4g14400 corresponds to <italic toggle="yes">ACCELERATED CELL DEATH6</italic> (<italic toggle="yes">ACD6</italic>, see text).</p>
            <p><bold>(D)</bold> Relative senescence 25 d after sowing, split by accessions carrying the reference and alternative alleles of the SNP close to <italic toggle="yes">ACD6</italic>, highlighted in <bold>(C)</bold>.</p>
          </caption>
          <graphic xlink:href="TPC_202000318R2_f6" position="float"/>
        </fig>
      </sec>
      <sec id="s14">
        <title>Validation of Pixel-Level Classifications</title>
        <p>Having validated our hypothesis that color profiles of the whole plant area can be used to extract informative traits, we asked if ARADEEPOPSIS would allow the identification of genetic variants significantly associated with the relative amount of senescent or dead tissue during early development. We conducted a GWA analysis, using the relative amount of senescent tissue at 25 d after sowing as the phenotypic trait. This time point is fairly early during the vegetative phase of plant development, and the relative senescent plant area is small compared to later stages (see <xref rid="fig2" ref-type="fig">Figure 2C</xref>). We identified several genomic regions that showed significant associations with this phenotype, the most striking of which was located on chromosome 4. The highest-ranking SNP in this region was located within the coding region of <italic toggle="yes">ACCELERATED CELL DEATH 6</italic> (<italic toggle="yes">ACD6;</italic> At4g14400; <xref rid="fig6" ref-type="fig">Figure 6C</xref>). <italic toggle="yes">ACD6</italic> has been extensively studied in Arabidopsis and was identified as being associated with vegetative growth, microbial resistance, and necrosis (<xref rid="bib4" ref-type="bibr">Atwell et al., 2010</xref>; <xref rid="bib38" ref-type="bibr">Todesco et al., 2010</xref>; <xref rid="bib45" ref-type="bibr">Zhu et al., 2018</xref>). Our plants were grown on natural, nonsterilized soil from a field site near Zurich, Switzerland (<xref rid="bib19" ref-type="bibr">Hu et al., 2018</xref>); it is therefore fair to assume that the microbial load and potential biotic stress levels were higher than in standard potting soil. We observed an ∼8-fold difference in median relative senescent tissue per plant (<xref rid="fig6" ref-type="fig">Figure 6D</xref>) from ∼0.5% of total segmented pixels in accessions carrying the reference allele to around 4% in those carrying the alternative allele. This result is in line with published studies, which highlighted the role of certain <italic toggle="yes">ACD6</italic> alleles in the early onset of senescence and necrosis in Arabidopsis (<xref rid="bib38" ref-type="bibr">Todesco et al., 2010</xref>).</p>
      </sec>
    </sec>
    <sec id="s15">
      <title>Analysis of Other Plant Species</title>
      <p>While our specific goal was to faithfully segment Arabidopsis rosettes, we wanted to see whether our method would generalize to other plant species with similar overall morphology. We therefore tested ARADEEPOPSIS on top-view images of pennycress (<italic toggle="yes">Thlaspi arvense</italic>), another member of the Brassicaceae, which has a similar leaf shape and phyllotaxis as Arabidopsis but does not form a flat rosette on the ground. Without training a model on images of <italic toggle="yes">T. arvense</italic>, that is, by using the model trained on Arabidopsis, segmentation masks accurately matched the original plant (<xref rid="fig7" ref-type="fig">Figure 7A</xref>), showing that ARADEEPOPSIS is robust against variations in plant morphology, as long as these do not deviate from the general architecture that was used in the training set. We also ran the pipeline on images from the IPPN data set (<ext-link xlink:href="https://www.plant-phenotyping.org/data" ext-link-type="uri">https://www.plant-phenotyping.org/data</ext-link> sets-download; <xref rid="bib26" ref-type="bibr">Minervini et al., 2016</xref>) on tobacco (<italic toggle="yes">Nicotiana tabacum</italic>), a member of the Solanaceae, which is morphologically quite distinct from Arabidopsis. Segmentations were accurate for most plants and correlated well with the ground-truth masks from the IPPN data set (<xref rid="fig7" ref-type="fig">Figure 7B</xref>). We did observe some exceptions for larger plants with substantial shaded leaf areas (<xref rid="fig7" ref-type="fig">Figure 7B</xref>; <ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Figure 2C</ext-link>).</p>
      <fig position="float" fig-type="figure" id="fig7">
        <label>Figure 7.</label>
        <caption>
          <p>Segmentation of Other Plant Species.</p>
          <p><bold>(A)</bold> Segmentation of <italic toggle="yes">Thlaspi arvense</italic>. The left-most panel shows the original image of a <italic toggle="yes">T. arvense</italic> individual. The middle panel shows the segmented mask and convex hull area. The rightmost panel shows the plant accurately cropped from the original image using the mask generated by ARADEEPOPSIS.</p>
          <p><bold>(B)</bold> Segmentation of a tobacco (<italic toggle="yes">Nicotiana tabacum</italic>) plant (<ext-link xlink:href="https://www.plant-phenotyping.org/data" ext-link-type="uri">https://www.plant-phenotyping.org/data</ext-link> sets-download). From top left to center right, panels show the original image, the segmentation mask, the convex hull area, the segmentation, and the overlay of the segmentation and the original image of a representative plant. The scatterplot shows the correlation between ARADEEPOPSIS segmentation and the original ground-truth masks.</p>
        </caption>
        <graphic xlink:href="TPC_202000318R2_f7" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s16">
    <title>Discussion</title>
    <p>Here, we present ARADEEPOPSIS, a versatile pipeline to extract phenotypic measurements from small or large sets of plant images in an unsupervised manner. ARADEEPOPSIS can faithfully identify, segment, and measure plants from top-view images of rosettes, such as those of Arabidopsis, using deep learning methodology. Our tool is easy to use, runs on personal computers as well as high-performance computing environments, is robust against variations in overall plant appearance, image quality, and background composition, and generates tabular output, making unsupervised high-throughput image-analysis accessible to a broad community of plant researchers.</p>
    <sec id="s17">
      <title>A Fast and Easy-to-Use Segmentation Tool</title>
      <p>Our models were trained on sets of 240 (model A) and &gt;1,000 (models B and C) images, respectively, with manually annotated 'ground-truth' segmentations, which required substantial but manageable manual labor. We believe that these investments were warranted, because ARADEEPOPSIS produced accurate segmentations and morphometric measurements that correlated very well with published data; occasional deviations from the originally reported data were in most cases in favor of our pipeline, due to more accurate segmentation by ARADEEPOPSIS. In addition to these pretrained models, ARADEEPOPSIS also allows the use of custom models or sets of presegmented masks for use with the pipeline trait extraction module. Besides being accurate and requiring limited labor, the method is also fast: the fully automated analysis of 100 randomly selected images from our data set took 12 minutes on a laptop computer (24 Gbytes RAM, Intel i5-8250U 1.6 GHz). Depending on available resources such as memory and number of cores, the implementation in <italic toggle="yes">Nextflow</italic> enables straightforward parallelization, allowing for a significant increase in speed when deployed in high-performance computing environments. At the same time, <italic toggle="yes">Nextflow</italic> is agnostic to the computational infrastructure being used, making it straightforward to deploy ARADEEPOPSIS on any type of computer or cloud infrastructure, as long as it fulfills the minimum requirement of 6 Gbytes and 12 Gbytes of memory for single-scale and multi-scale inference, respectively. While training of the model greatly benefitted from the availability of graphics processing units (GPUs), image predictions can also be performed using central processing units in a time-efficient manner.</p>
    </sec>
    <sec id="s18">
      <title>Accurate Determination of Plant Health State</title>
      <p>ARADEEPOPSIS not only segments rosettes and extracts morphometric parameters, it also allows for the extraction of color channel information, which can be used to make assessments on the anthocyanin content and overall health status of the plant. We have shown that these results can be used, for example, for quantitative genetics approaches (<xref rid="fig6" ref-type="fig">Figure 6</xref>).</p>
      <p>Besides a simple one-class model (model A) that can segment nonsenescent leaves, ARADEEPOPSIS includes two models that allow segmentation of anthocyanin-rich and/or senescent areas, depending on their color composition. As such, ARADEEPOPSIS is capable of reliably classifying senescent leaves and of distinguishing healthy, green leaves from stressed, anthocyanin-containing leaves. Our models may also be extended to additional classes, depending on the specific needs of researchers and the phenotypes of interest, by including the corresponding training sets.</p>
    </sec>
    <sec id="s19">
      <title>Limitations and Shortcomings of the Proposed Models</title>
      <p>Many of the images in our training data set had blue plastic meshes as background. This may raise the concern that the model might have learned to classify pixels belonging to the background rather than the plant of interest, that is, to segment the plant as “nonbackground,” which would render the tool unreliable when using it on images with a different background composition. However, by testing ARADEEPOPSIS on images acquired in other phenotyping environments using different backgrounds, and on images of potted plants with substantial algal growth surrounding the Arabidopsis plant of interest, we determined that segmentation of leaves remained accurate and was agnostic to the background (see <xref rid="fig5" ref-type="fig">Figure 5A</xref>). Ideally, ARADEEPOPSIS should be used with images that are homogeneous within one set regarding light intensity, exposure time, etc. Segmentation still works for images with various light intensities (<ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Figure 4</ext-link>), and plant size and geometry can still be analyzed, but quantification of color intensities based on the original image is no longer comparable in that case.</p>
      <p>Our trained models have restrictions with regard to the angle at which the images are recorded and currently perform best for top-view images.</p>
      <p>Although we found our models to generalize well to several publicly available data sets (<xref rid="fig5" ref-type="fig">Figure 5</xref>), there are also cases where the models misclassify parts of the background (<ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Figure 2</ext-link>). Such cases may effectively be avoided by re-training and fine-tuning the model, after adding a small number of annotated images representative of changed image parameters to the training data set. Such training set extensions may either be sourced from manual annotation or by selecting correctly segmented masks produced by the model. We will aim to further diversify the training set for a future iteration of model training and also encourage researchers to use and build upon the training code (<ext-link xlink:href="https://github.com/phue/models/tree/aradeepopsis_manuscript/research/deeplab" ext-link-type="uri">https://github.com/phue/models/tree/aradeepopsis_manuscript/research/deeplab</ext-link>) and ground-truth annotations (<ext-link xlink:href="https://doi.org/10.5281/zenodo.3946393" ext-link-type="uri">https://doi.org/10.5281/zenodo.3946393</ext-link>) we deposited for that purpose.</p>
      <p>When designing the pipeline, we placed a strong emphasis on the ability for the user to visually assess quality of segmentation at a glance, which we implemented using the <italic toggle="yes">Shiny</italic> framework (<xref rid="bib31" ref-type="bibr">RStudio, 2014</xref>). This way, users can identify misclassifications early on. Because our pipeline calculates traits independently for each segmentation class, it can extract accurate information for some classes even when others are affected by misclassifications. We demonstrate this in <xref rid="fig5" ref-type="fig">Figure 5C</xref>, which shows that ARADEEPOPSIS measurements of class_norm_area correlate well with the area extracted from the IPPN CVPPP2017 annotations (<ext-link xlink:href="https://www.plant-phenotyping.org/data" ext-link-type="uri">https://www.plant-phenotyping.org/data</ext-link> sets-download; <xref rid="bib26" ref-type="bibr">Minervini et al., 2016</xref>), despite occasional misclassification of class_senesc, which partially occurred in the A1 image series of the data set (<ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Figure 2B</ext-link>).</p>
    </sec>
  </sec>
  <sec id="s20">
    <title>Outlook and Perspective</title>
    <p>Our study shows that transfer learning is a valuable approach to overcome the phenotyping bottleneck. The model was designed for the analysis of Arabidopsis rosettes but was also able to segment images from other species. We believe that providing a modular pipeline that can be used with different models provides valuable flexibility. For experimental conditions where our models do not perform well, when interest is in other species, or for analysis of other developmental traits than the ones implemented here, researchers can train their own models and simply integrate them into our pipeline to perform trait extraction. These adjustments will require expert annotation of the corresponding images but can be performed by an experienced researcher and trained helpers in a reasonable amount of time. We therefore anticipate that the straightforward usage of ARADEEPOPSIS for trait-extraction with either the provided or custom models combined with direct visualization of the outputs will make semantic segmentation more easily accessible to plant scientists. Moreover, we would like to propose that pre-existing annotations from different research groups and various species be collected into a centralized repository that would serve as a powerful resource for phenomics in plant science and breeding.</p>
    <p>ImageNet, the data set that was used to pretrain the baseline model we built upon, contains images of dogs, airplanes, bicycles, etc., but no detailed annotations of plant species. It is therefore remarkable that ImageNet pretrained models already enabled high accuracy for plant segmentation when re-training the last layer with a relatively small set of 240 manually annotated images. Ultimately, the adaptability displayed by ImageNet highlights the potential such publicly available databases and models hold for research and suggests that such resources should be exploited more frequently and more extensively.</p>
  </sec>
  <sec sec-type="methods" id="s21">
    <title>Methods</title>
    <sec id="s22">
      <title>Plant Growth and Image Acquisition</title>
      <p>Plants (210 genotypes with 6 individuals each; 1260 plants total) were grown in a custom-built growth chamber (YORK Refrigeration), equipped with an environmental control unit (Johnson Controls) and a climate control unit (Incotec), on natural soil (<xref rid="bib19" ref-type="bibr">Hu et al., 2018</xref>) in long-day conditions (16-h light [21°C], 8-h dark [16°C], 140 µE/m<sup>2</sup>s) with 60% relative humidity and watered twice per week. As light sources, we used R-LMR-5001 light emitting diode panels (Rhenac); the spectrum is shown in <ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Figure 5</ext-link>. Seeds were surface-sterilized with chlorine gas for 1 h and sown on September 20, 2018, stratified for 4 d by cooling the chamber to 4°C, and from this point on plants were imaged two times per day. The images were acquired using an RGB camera (IDS uEye UI-548xRE-C; 5 MP) using an automated x-y-z sensor-to-plant system (<underline>Hesotech GmbH</underline>). Plants were monitored daily for emerging inflorescences, and flowering accessions were removed from the growth chamber to prevent emerging flowers from covering other plants in the top-down images. Plants that had not flowered by December 11, 2018 (12 weeks after release from stratification) were subjected to vernalization for 6 weeks: the chamber was cooled to 4°C (ramped over 8 h). During watering, the temperature was raised to 5°C. This treatment ended on January 21, 2019, after which the plants were exposed to the previous long-day growth conditions. In total 148,792 plant images were acquired and analyzed.</p>
    </sec>
    <sec id="s23">
      <title>Training and Validation</title>
      <p>We used the <italic toggle="yes">Computer Vision Annotation Tool</italic> (<xref rid="bib35" ref-type="bibr">Sekachev et al., 2019</xref>) for manual image annotation; custom scripts were used to produce annotation masks from the XML output. The training data are available at <ext-link xlink:href="https://doi.org/10.5281/zenodo.3946393" ext-link-type="uri">https://doi.org/10.5281/zenodo.3946393</ext-link>. The publicly available <italic toggle="yes">DeepLabV3+</italic> (<xref rid="bib8" ref-type="bibr">Chen et al., 2017</xref>; <xref rid="bib9" ref-type="bibr">Chen et al., 2018</xref>) code was modified to enable model training on our own annotated training sets. The code used for training, as well as download links for our annotated training data sets, are available here: <ext-link xlink:href="https://github.com/phue/models/tree/aradeepopsis_manuscript/research/deeplab" ext-link-type="uri">https://github.com/phue/models/tree/aradeepopsis_manuscript/research/deeplab</ext-link>.</p>
      <p>For model evaluation, we split the annotated sets 80:20: 80% of the images were used to train the model and 20% for evaluation.</p>
      <p>We used a transfer learning strategy by using a DeepLabV3+ (based on the <italic toggle="yes">xception65</italic> architecture) model checkpoint (<xref rid="bib10" ref-type="bibr">Chollet, 2017</xref>) that had been pretrained on the <italic toggle="yes">ImageNet</italic> data set (<xref rid="bib14" ref-type="bibr">Deng et al., 2009</xref>) as a starting point. Training was then implemented in an asynchronous manner using between-graph replication on our in-house <italic toggle="yes">Slurm</italic> cluster, allowing the use of 16 Nvidia Tesla V100 GPUs across 4 compute nodes. We performed model training according to published protocols (<xref rid="bib8" ref-type="bibr">Chen et al., 2017</xref>; <xref rid="bib9" ref-type="bibr">Chen et al., 2018</xref>) with the following changes: to account for the number of GPUs, the training batch size was set to 128 and a base learning rate of 0.1 was applied and decayed according to a polynomial function after a burn-in period of 2000 training steps. Images were randomly cropped to 321×321 pixels, and training was stopped after 75,000 iterations. The models were exported to support single- or multiscale inference and are available at <ext-link xlink:href="https://doi.org/10.5281/zenodo.3946618" ext-link-type="uri">https://doi.org/10.5281/zenodo.3946618</ext-link>.</p>
    </sec>
    <sec id="s24">
      <title>Implementation</title>
      <p>Based on the trained models, we implemented an image analysis pipeline in <italic toggle="yes">Nextflow</italic> (<xref rid="bib15" ref-type="bibr">Di Tommaso et al., 2017</xref>) that is available at <ext-link xlink:href="https://github.com/Gregor-Mendel-Institute/aradeepopsis" ext-link-type="uri">https://github.com/Gregor-Mendel-Institute/aradeepopsis</ext-link>. The workflow is outlined in <xref rid="fig4" ref-type="fig">Figure 4</xref>. <italic toggle="yes">Nextflow</italic> allows external pipeline dependencies to be packaged in a <italic toggle="yes">Docker</italic> container, which we provide at <ext-link xlink:href="https://hub.docker.com/r/beckerlab/aradeepopsis/" ext-link-type="uri">https://hub.docker.com/r/beckerlab/aradeepopsis/</ext-link>. The container can be run using <italic toggle="yes">Docker</italic> (<xref rid="bib25" ref-type="bibr">Merkel, 2014</xref>), <italic toggle="yes">podman</italic> (<xref rid="bib11" ref-type="bibr">Containers Organisation, 2020</xref>), or <italic toggle="yes">Singularity</italic> (<xref rid="bib22" ref-type="bibr">Kurtzer et al., 2017</xref>). Alternatively, dependencies can be automatically installed into a <italic toggle="yes">Conda</italic> (<xref rid="bib2" ref-type="bibr">Anaconda Inc., 2020</xref>) environment.</p>
      <p>The pipeline follows a scatter-gather strategy to scatter the input data into equally sized batches of arbitrary size, allowing for parallel processing, after which the results are again collected into a single output table. After the initial splitting, all images in one batch are first converted to TFrecord files using <italic toggle="yes">TensorFlow</italic> (<xref rid="bib1" ref-type="bibr">Abadi et al., 2016</xref>). The records are then served to the trained <italic toggle="yes">TensorFlow</italic> model of choice in to generate segmentation masks, containing pixelwise classification.</p>
      <p>In the next step, we extracted morphometric traits from the segmented images using the <italic toggle="yes">regionprops</italic> function of the Python library <italic toggle="yes">scikit-image</italic> (<xref rid="bib41" ref-type="bibr">van der Walt et al., 2014</xref>) on a per-class basis. Additionally, color channel information was extracted in the form of mean pixel intensities for each of the channels in the original RGB image, within both the region determined by the segmentation mask and for each class. Based on the color channel means, we calculated chroma indices as follows (<xref rid="bib13" ref-type="bibr">Del Valle et al., 2018</xref>):<disp-formula><mml:math id="me1" overflow="scroll"><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:math></disp-formula><disp-formula><mml:math id="me2" overflow="scroll"><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula><disp-formula><mml:math id="me3" overflow="scroll"><mml:mi>C</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mfrac></mml:math></disp-formula><disp-formula><mml:math id="me4" overflow="scroll"><mml:mi>B</mml:mi><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:math></disp-formula><disp-formula><mml:math id="me5" overflow="scroll"><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula></p>
      <p>The results from these measurements were then collected from all processed batches, ultimately resulting in a single table containing 78 traits, listed in the <ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental Data Set</ext-link>. Based on the segmentation masks in combination with their corresponding input images, the pipeline also produces diagnostic images showing a color-coded mask, the cropped plant region, the convex hull, and an overlay image between the original and the mask.</p>
      <p>These single-plant diagnostics are then, in an optional step, merged to produce summary diagnostics using <italic toggle="yes">ImageMagick</italic> (ImageMagick Development Team, 2020), or can be viewed in an interactive <italic toggle="yes">Shiny</italic> (<xref rid="bib31" ref-type="bibr">RStudio, 2014</xref>) application specifically designed for the purpose of quality control of ARADEEPOPSIS output, allowing for fine-grained inspection of segmentation quality, pixel classification, correlations between traits, as well as time-resolved data visualization if appropriate metadata is provided.</p>
    </sec>
    <sec id="s25">
      <title>GWA Studies</title>
      <p>We performed GWA analysis on the traits produced by ARADEEPOPSIS using <italic toggle="yes">LIMIX</italic> (<xref rid="bib24" ref-type="bibr">Lippert et al., 2014</xref>). We used the average of each trait per accession per d and performed GWA analysis by fitting a linear mixed model using <italic toggle="yes">LIMIX</italic>. To associate phenotype and single nucleotide polymorphisms, we used the 1135 genotype SNP matrix and the corresponding kinship matrix, subset to those accessions for which we had trait information. We screened the results for interesting trait-date combinations and followed these using Arabidopsis (<italic toggle="yes">Arabidopsis thaliana</italic>)-specific R tools developed in-house (<ext-link xlink:href="https://github.com/Gregor-Mendel-Institute/gwaR" ext-link-type="uri">https://github.com/Gregor-Mendel-Institute/gwaR</ext-link>). The analysis is detailed in the <ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri">Supplemental File</ext-link>.</p>
    </sec>
    <sec id="s26">
      <title>Supplemental Material</title>
      <list list-type="simple" id="L1">
        <list-item>
          <p><ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri"><bold>Supplemental Figure 1.</bold></ext-link> Validation of morphometric measurements against published data.</p>
        </list-item>
        <list-item>
          <p><ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri"><bold>Supplemental Figure 2.</bold></ext-link> Segmentations produced by ARADEEPOPSIS model C.</p>
        </list-item>
        <list-item>
          <p><ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri"><bold>Supplemental Figure 3.</bold></ext-link> Comparison of ARADEEPOPSIS to a dedicated small-scale model.</p>
        </list-item>
        <list-item>
          <p><ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri"><bold>Supplemental Figure 4.</bold></ext-link> ARADEEPOPSIS is tolerant to changes in image parameters.</p>
        </list-item>
        <list-item>
          <p><ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri"><bold>Supplemental Figure 5.</bold></ext-link> Light spectrum applied during the phenotyping experiment.</p>
        </list-item>
        <list-item>
          <p><ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri"><bold>Supplemental Data Set.</bold></ext-link> List of traits resulting from the ARADEEPOPSIS pipeline.</p>
        </list-item>
        <list-item>
          <p><ext-link xlink:href="http://www.plantcell.org/cgi/content/full/tpc.20.00318/DC1" ext-link-type="uri"><bold>Supplemental File.</bold></ext-link> R-scripts used for the analysis of GWAS results.</p>
        </list-item>
        <list-item>
          <p>https://syncandshare.lrz.de/getlink/fiLvKLBUsPKMrj4dLiGckiMa/araDeepopsis_supplement.html</p>
        </list-item>
      </list>
    </sec>
  </sec>
  <sec id="divesec1">
    <title>DIVE Curated Terms</title>
    <p specific-use="curated-terms">The following phenotypic, genotypic, and functional terms are of significance to the work described in this paper:<named-content content-type="protein"><list list-type="simple"><list-item><p><ext-link xlink:href="http://tools.gramene.org/search?query=AT4G00730" ext-link-type="uri" specific-use="Gramene" xlink:title="ANL2">ANL2 Gramene: AT4G00730</ext-link></p></list-item><list-item><p><ext-link xlink:href="http://bar.utoronto.ca/thalemine/keywordSearchResults.do?searchTerm=AT4G00730&amp;searchSubmit=GO" ext-link-type="uri" specific-use="Araport" xlink:title="ANL2">ANL2 Araport: AT4G00730</ext-link></p></list-item></list></named-content><named-content content-type="protein"><list list-type="simple"><list-item><p><ext-link xlink:href="http://tools.gramene.org/search?query=AT4G14400" ext-link-type="uri" specific-use="Gramene" xlink:title="ACD6">ACD6 Gramene: AT4G14400</ext-link></p></list-item><list-item><p><ext-link xlink:href="http://bar.utoronto.ca/thalemine/keywordSearchResults.do?searchTerm=AT4G14400&amp;searchSubmit=GO" ext-link-type="uri" specific-use="Araport" xlink:title="ACD6">ACD6 Araport: AT4G14400</ext-link></p></list-item></list></named-content></p>
  </sec>
</body>
<back>
  <ack>
    <title>AUTHOR CONTRIBUTIONS</title>
    <p>P.H., N.S., and C.B. designed the research and wrote the article; P.H., N.S., and K.J. performed the research; P.H. wrote the pipeline; N.S. and K.J. performed the phenotyping experiment and generated the image data; I.B. provided additional image and segmentation data; P.H. and N.S. analyzed the data.</p>
  </ack>
  <ack>
    <p>We thank J. Matthew Watson for critical reading of the manuscript and valuable comments. Karina Weiser Lobão helped in the manual image annotation. We thank Dario Galanti and Oliver Bossdorf for providing images of <italic toggle="yes">T. arvense</italic>. Klaus Schlaeppi and Selma Cadot provided the field soil. We thank Núria Serra Serra and Jorge Isaac Rodriguez for testing the analysis pipeline. The computational results presented were obtained using the CLIP cluster at VBC. Erich Birngruber gave advice on distributed model training on a Slurm cluster. We thank Stijn Dhondt for providing access to the Arabidopsis images through the BrAPI endpoint of the PIPPA information system (<ext-link xlink:href="https://pippa.psb.ugent.be/" ext-link-type="uri">https://pippa.psb.ugent.be/</ext-link>). Arabidopsis image acquisition was performed with support of the Plant Sciences Facility at Vienna BioCenter Core Facilities GmbH (VBCF), a member of the Vienna BioCenter (VBC), Austria. This work was supported by the <ext-link xlink:href="http://dx.doi.org/10.13039/501100000781" ext-link-type="uri">EC | European Research Council (ERC</ext-link>; grant 716823, “FEAR-SAP” to C.B.), by the <ext-link xlink:href="http://dx.doi.org/10.13039/501100001822" ext-link-type="uri">Austrian Academy of Sciences (Österreichische Akademie der Wissenschaften)</ext-link>, and the <ext-link xlink:href="http://dx.doi.org/10.13039/501100004189" ext-link-type="uri">Max-Planck-Gesellschaft (Max Planck Society)</ext-link>.</p>
  </ack>
  <ref-list>
    <ref id="bib1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Abadi</surname>, <given-names>M.</given-names></string-name>, <etal/></person-group> (<year>2016</year>). TensorFlow: A system for large-scale machine learning. Conference Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI ’16); November 2–4, 2016; Savannah, GA.</mixed-citation>
    </ref>
    <ref id="bib2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Anaconda Inc</collab></person-group> (<year>2020</year>). <article-title>Anaconda Software Distribution v. 2-2.4.0. Anaconda Documentation</article-title>. <ext-link xlink:href="https://docs.anaconda.com/" ext-link-type="uri">https://docs.anaconda.com/</ext-link></mixed-citation>
    </ref>
    <ref id="bib3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arvidsson</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Pérez-Rodríguez</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Mueller-Roeber</surname>, <given-names>B.</given-names></string-name></person-group>(<year>2011</year>). <article-title>A growth phenotyping pipeline for <italic toggle="yes">Arabidopsis thaliana</italic> integrating image analysis and rosette area modeling for robust quantification of genotype effects</article-title>. <source>New Phytol.</source><volume>191</volume>: <fpage>895</fpage>–<lpage>907</lpage>.<pub-id pub-id-type="pmid">21569033</pub-id></mixed-citation>
    </ref>
    <ref id="bib4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Atwell</surname>, <given-names>S.</given-names></string-name>, <etal/></person-group>(<year>2010</year>). <article-title>Genome-wide association study of 107 phenotypes in Arabidopsis thaliana inbred lines</article-title>. <source>Nature</source><volume>465</volume>: <fpage>627</fpage>–<lpage>631</lpage>.<pub-id pub-id-type="pmid">20336072</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Barragan</surname>, <given-names>C.A.</given-names></string-name>, <etal/></person-group>(<year>2019</year>). <article-title>RPW8/HR repeats control NLR activation in <italic toggle="yes">Arabidopsis thaliana</italic></article-title>. <source>PLoS Genet.</source><volume>15</volume>: <fpage>e1008313</fpage>.<pub-id pub-id-type="pmid">31344025</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baute</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Polyn</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>De Block</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Blomme</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Van Lijsebettens</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Inzé</surname>, <given-names>D.</given-names></string-name></person-group>(<year>2017</year>). <article-title>F-Box protein FBX92 affects leaf size in <italic toggle="yes">Arabidopsis thaliana</italic></article-title>. <source>Plant Cell Physiol.</source><volume>58</volume>: <fpage>962</fpage>–<lpage>975</lpage>.<pub-id pub-id-type="pmid">28340173</pub-id></mixed-citation>
    </ref>
    <ref id="bib7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Capovilla</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Delhomme</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Collani</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Shutava</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Bezrukov</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Symeonidi</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>de Francisco Amorim</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Laubinger</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schmid</surname>, <given-names>M.</given-names></string-name></person-group>(<year>2018</year>). <article-title>PORCUPINE regulates development in response to temperature through alternative splicing</article-title>. <source>Nat. Plants</source><volume>4</volume>: <fpage>534</fpage>–<lpage>539</lpage>.<pub-id pub-id-type="pmid">29988152</pub-id></mixed-citation>
    </ref>
    <ref id="bib8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>L.-C.</given-names></string-name>, <string-name><surname>Papandreou</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Schroff</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Adam</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2017</year>). <article-title>Rethinking atrous convolution for semantic image segmentation</article-title>. <source>ArXiv.org</source>: <comment>arXiv:1706.05587</comment>.</mixed-citation>
    </ref>
    <ref id="bib9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Chen</surname>, <given-names>L.-C.</given-names></string-name>, <string-name><surname>Zhu</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Papandreou</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Schroff</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Adam</surname>, <given-names>H.</given-names></string-name></person-group> (<year>2018</year>). <article-title>Encoder-cecoder with atrous separable convolution for semantic image segmentation. ECCV 2018 – Computer Vision</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Ferrari</surname><given-names>V.</given-names></string-name>, <string-name><surname>Hebert</surname><given-names>M.</given-names></string-name>, <string-name><surname>Sminchisescu</surname><given-names>C.</given-names></string-name>, <string-name><surname>Weiss</surname><given-names>Y.</given-names></string-name></person-group> (eds). <source>Computer Vision – ECCV 2018. ECCV 2018. Lecture Notes in Computer Science, vol 11211</source>. <publisher-name>Springer</publisher-name>, <publisher-loc>Cham, Switzerland</publisher-loc>. <ext-link xlink:href="http://doi-org-443.webvpn.fjmu.edu.cn/10.1007/978-3-030-01234-2_49" ext-link-type="uri">http://doi-org-443.webvpn.fjmu.edu.cn/10.1007/978-3-030-01234-2_49</ext-link><comment>European Conference on Computer Vision</comment>.</mixed-citation>
    </ref>
    <ref id="bib10">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chollet</surname>, <given-names>F.</given-names></string-name></person-group> (<year>2017</year>). <comment>Xception: Deep learning with depthwise separable convolutions. 2017 IEEE Conf Comp Vis Pattern Recogn (CVPR); Honolulu, HI; 2017; 1800–1807</comment>.</mixed-citation>
    </ref>
    <ref id="bib11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Containers Organisation</collab></person-group> (<year>2020</year>). <article-title>Podman</article-title>. <ext-link xlink:href="https://podman.io/" ext-link-type="uri">https://podman.io/</ext-link></mixed-citation>
    </ref>
    <ref id="bib12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Coppens</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Wuyts</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Inzé</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Dhondt</surname>, <given-names>S.</given-names></string-name></person-group>(<year>2017</year>). <article-title>Unlocking the potential of plant phenotyping data through integration and data-driven approaches</article-title>. <source>Curr. Opin. Syst. Biol.</source><volume>4</volume>: <fpage>58</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">32923745</pub-id></mixed-citation>
    </ref>
    <ref id="bib13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Del Valle</surname>, <given-names>J.C.</given-names></string-name>, <string-name><surname>Gallardo-López</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Buide</surname>, <given-names>M.L.</given-names></string-name>, <string-name><surname>Whittall</surname>, <given-names>J.B.</given-names></string-name>, <string-name><surname>Narbona</surname>, <given-names>E.</given-names></string-name></person-group>(<year>2018</year>). <article-title>Digital photography provides a fast, reliable, and noninvasive method to estimate anthocyanin pigment concentration in reproductive and vegetative plant tissues</article-title>. <source>Ecol. Evol.</source><volume>8</volume>: <fpage>3064</fpage>–<lpage>3076</lpage>.<pub-id pub-id-type="pmid">29607006</pub-id></mixed-citation>
    </ref>
    <ref id="bib14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Deng</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Dong</surname>, <given-names>W.</given-names></string-name>, <string-name><surname>Socher</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Fei-Fei</surname>, <given-names>L.</given-names></string-name></person-group> (<year>2009</year>). <comment>ImageNet: A large-scale hierarchical image database. IEEE Conference on Computer Vision and Pattern Recognition; Miami, FL; 2009; 248–255</comment>.</mixed-citation>
    </ref>
    <ref id="bib15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Tommaso</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Chatzou</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Floden</surname>, <given-names>E.W.</given-names></string-name>, <string-name><surname>Barja</surname>, <given-names>P.P.</given-names></string-name>, <string-name><surname>Palumbo</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Notredame</surname>, <given-names>C.</given-names></string-name></person-group>(<year>2017</year>). <article-title>Nextflow enables reproducible computational workflows</article-title>. <source>Nat. Biotechnol.</source><volume>35</volume>: <fpage>316</fpage>–<lpage>319</lpage>.<pub-id pub-id-type="pmid">28398311</pub-id></mixed-citation>
    </ref>
    <ref id="bib16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Furbank</surname>, <given-names>R.T.</given-names></string-name>, <string-name><surname>Tester</surname>, <given-names>M.</given-names></string-name></person-group>(<year>2011</year>). <article-title>Phenomics--technologies to relieve the phenotyping bottleneck</article-title>. <source>Trends Plant Sci.</source><volume>16</volume>: <fpage>635</fpage>–<lpage>644</lpage>.<pub-id pub-id-type="pmid">22074787</pub-id></mixed-citation>
    </ref>
    <ref id="bib17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gehan</surname>, <given-names>M.A.</given-names></string-name>, <etal/></person-group>(<year>2017</year>). <article-title>PlantCV v2: Image analysis software for high-throughput plant phenotyping</article-title>. <source>PeerJ</source><volume>5</volume>: <fpage>e4088</fpage>.<pub-id pub-id-type="pmid">29209576</pub-id></mixed-citation>
    </ref>
    <ref id="bib18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>He</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ren</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Sun</surname>, <given-names>J.</given-names></string-name></person-group> (<year>2016</year>). <comment>Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); 2016; 770–778</comment>.</mixed-citation>
    </ref>
    <ref id="bib19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Robert</surname>, <given-names>C.A.M.</given-names></string-name>, <string-name><surname>Cadot</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Zhang</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Ye</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Manzo</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Chervet</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Steinger</surname>, <given-names>T.</given-names></string-name>, <string-name><surname>van der Heijden</surname>, <given-names>M.G.A.</given-names></string-name>, <string-name><surname>Schlaeppi</surname>, <given-names>K.</given-names></string-name>, <string-name><surname>Erb</surname>, <given-names>M.</given-names></string-name></person-group>(<year>2018</year>). <article-title>Root exudate metabolites drive plant-soil feedbacks on growth and defense by shaping the rhizosphere microbiota</article-title>. <source>Nat. Commun.</source><volume>9</volume>: <fpage>2738</fpage>.<pub-id pub-id-type="pmid">30013066</pub-id></mixed-citation>
    </ref>
    <ref id="bib20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Krizhevsky</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Sutskever</surname>, <given-names>I.</given-names></string-name>, <string-name><surname>Hinton</surname>, <given-names>G.E.</given-names></string-name></person-group>(<year>2012</year>). <article-title>ImageNet classification with deep convolutional neural networks</article-title>. <source>Nips</source><volume>12</volume>: <fpage>1097</fpage>–<lpage>1105</lpage>.</mixed-citation>
    </ref>
    <ref id="bib21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kubo</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Peeters</surname>, <given-names>A.J.</given-names></string-name>, <string-name><surname>Aarts</surname>, <given-names>M.G.</given-names></string-name>, <string-name><surname>Pereira</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Koornneef</surname>, <given-names>M.</given-names></string-name></person-group>(<year>1999</year>). <article-title>ANTHOCYANINLESS2, a homeobox gene affecting anthocyanin distribution and root development in Arabidopsis</article-title>. <source>Plant Cell</source><volume>11</volume>: <fpage>1217</fpage>–<lpage>1226</lpage>.<pub-id pub-id-type="pmid">10402424</pub-id></mixed-citation>
    </ref>
    <ref id="bib22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kurtzer</surname>, <given-names>G.M.</given-names></string-name>, <string-name><surname>Sochat</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Bauer</surname>, <given-names>M.W.</given-names></string-name></person-group>(<year>2017</year>). <article-title>Singularity: Scientific containers for mobility of compute</article-title>. <source>PLoS One</source><volume>12</volume>: <fpage>e0177459</fpage>.<pub-id pub-id-type="pmid">28494014</pub-id></mixed-citation>
    </ref>
    <ref id="bib23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lecun</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Bottou</surname>, <given-names>L.</given-names></string-name>, <string-name><surname>Bengio</surname>, <given-names>Y.</given-names></string-name>, <string-name><surname>Haffner</surname>, <given-names>P.</given-names></string-name></person-group>(<year>1998</year>). <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proc. IEEE</source><volume>86</volume>: <fpage>2278</fpage>–<lpage>2324</lpage>.</mixed-citation>
    </ref>
    <ref id="bib24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lippert</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Casale</surname>, <given-names>F.P.</given-names></string-name>, <string-name><surname>Rakitsch</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Stegle</surname>, <given-names>O.</given-names></string-name></person-group> (<year>2014</year>). <article-title>LIMIX: Genetic analysis of multiple traits.</article-title>
<source>bioRxiv</source>: https://doi.org/<pub-id pub-id-type="doi">10.1101/003905</pub-id></mixed-citation>
    </ref>
    <ref id="bib25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Merkel</surname>, <given-names>D.</given-names></string-name></person-group>(<year>2014</year>). <article-title>Docker: Lightweight linux containers for consistent development and deployment</article-title>. <source>Linux journal</source><volume>2014</volume>: <fpage>2</fpage>.</mixed-citation>
    </ref>
    <ref id="bib26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Minervini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Fischbach</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Scharr</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Tsaftaris</surname>, <given-names>S.A.</given-names></string-name></person-group>(<year>2016</year>). <article-title>Finely-grained annotated datasets for image-based plant phenotyping</article-title>. <source>Pattern Recognit. Lett.</source><volume>81</volume>: <fpage>80</fpage>–<lpage>89</lpage>.</mixed-citation>
    </ref>
    <ref id="bib27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Minervini</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Giuffrida</surname>, <given-names>M.V.</given-names></string-name>, <string-name><surname>Perata</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Tsaftaris</surname>, <given-names>S.A.</given-names></string-name></person-group>(<year>2017</year>). <article-title>Phenotiki: An open software and hardware platform for affordable and easy image-based phenotyping of rosette-shaped plants</article-title>. <source>Plant J.</source><volume>90</volume>: <fpage>204</fpage>–<lpage>216</lpage>.<pub-id pub-id-type="pmid">28066963</pub-id></mixed-citation>
    </ref>
    <ref id="bib28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pound</surname>, <given-names>M.P.</given-names></string-name>, <string-name><surname>Atkinson</surname>, <given-names>J.A.</given-names></string-name>, <string-name><surname>Townsend</surname>, <given-names>A.J.</given-names></string-name>, <string-name><surname>Wilson</surname>, <given-names>M.H.</given-names></string-name>, <string-name><surname>Griffiths</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Jackson</surname>, <given-names>A.S.</given-names></string-name>, <string-name><surname>Bulat</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Tzimiropoulos</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Wells</surname>, <given-names>D.M.</given-names></string-name>, <string-name><surname>Murchie</surname>, <given-names>E.H.</given-names></string-name>, <string-name><surname>Pridmore</surname>, <given-names>T.P.</given-names></string-name>, <string-name><surname>French</surname>, <given-names>A.P.</given-names></string-name></person-group>(<year>2017</year>). <article-title>Deep machine learning provides state-of-the-art performance in image-based plant phenotyping</article-title>. <source>Gigascience</source><volume>6</volume>: <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="bib29">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Razavian</surname>, <given-names>A.S.</given-names></string-name>, <string-name><surname>Azizpour</surname>, <given-names>H.</given-names></string-name>, <string-name><surname>Sullivan</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Carlsson</surname>, <given-names>S.</given-names></string-name></person-group> (<year>2014</year>). <comment>CNN features off-the-shelf: an astounding baseline for recognition. 2014 IEEE Conf Comp Vis Pattern Recogn; 2016; 512–519</comment>.</mixed-citation>
    </ref>
    <ref id="bib30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Rother</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Kolmogorov</surname>, <given-names>V.</given-names></string-name>, <string-name><surname>Blake</surname>, <given-names>A.</given-names></string-name></person-group>(<year>2004</year>). <part-title>“GrabCut”: Interactive foreground extraction using iterated graph cuts</part-title>. In <source>ACM SIGGRAPH 2004 Papers</source>. (<publisher-loc>Los Angeles, California</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>), pp. <fpage>309</fpage>–<lpage>314</lpage>.</mixed-citation>
    </ref>
    <ref id="bib31">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>RStudio</collab></person-group> (<year>2014</year>). <article-title>Shiny: Easy web applications in R. R package</article-title>. <ext-link xlink:href="https://rstudio.com/products/shiny/" ext-link-type="uri">https://rstudio.com/products/shiny/</ext-link></mixed-citation>
    </ref>
    <ref id="bib32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rumelhart</surname>, <given-names>D.E.</given-names></string-name>, <string-name><surname>Hinton</surname>, <given-names>G.E.</given-names></string-name>, <string-name><surname>Williams</surname>, <given-names>R.J.</given-names></string-name></person-group>(<year>1986</year>). <article-title>Learning representations by back-propagating errors</article-title>. <source>Nature</source><volume>323</volume>: <fpage>533</fpage>–<lpage>536</lpage>.</mixed-citation>
    </ref>
    <ref id="bib33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schindelin</surname>, <given-names>J.</given-names></string-name>, <etal/></person-group>(<year>2012</year>). <article-title>Fiji: An open-source platform for biological-image analysis</article-title>. <source>Nat. Methods</source><volume>9</volume>: <fpage>676</fpage>–<lpage>682</lpage>.<pub-id pub-id-type="pmid">22743772</pub-id></mixed-citation>
    </ref>
    <ref id="bib34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schneider</surname>, <given-names>C.A.</given-names></string-name>, <string-name><surname>Rasband</surname>, <given-names>W.S.</given-names></string-name>, <string-name><surname>Eliceiri</surname>, <given-names>K.W.</given-names></string-name></person-group>(<year>2012</year>). <article-title>NIH Image to ImageJ: 25 Years of image analysis</article-title>. <source>Nat. Methods</source><volume>9</volume>: <fpage>671</fpage>–<lpage>675</lpage>.<pub-id pub-id-type="pmid">22930834</pub-id></mixed-citation>
    </ref>
    <ref id="bib35">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Sekachev</surname>, <given-names>B.</given-names></string-name>, <string-name><surname>Zhavoronkov</surname>, <given-names>A.</given-names></string-name>, <string-name><surname>Manovich</surname>, <given-names>N.</given-names></string-name></person-group> (<year>2019</year>). <article-title>Computer vision annotation tool: A universal approach to data annotation</article-title>. software.intel.com.</mixed-citation>
    </ref>
    <ref id="bib36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>The 1001 Genomes Consortium.</collab></person-group> (<year>2016</year>). <article-title>1,135 Genomes reveal the global pattern of polymorphism in Arabidopsis thaliana</article-title>. <source>Cell</source>
<volume>166</volume>: <fpage>481</fpage>–<lpage>491</lpage>.<pub-id pub-id-type="pmid">27293186</pub-id></mixed-citation>
    </ref>
    <ref id="bib38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Todesco</surname>, <given-names>M.</given-names></string-name>, <etal/></person-group>(<year>2010</year>). <article-title>Natural allelic variation underlying a major fitness trade-off in <italic toggle="yes">Arabidopsis thaliana</italic></article-title>. <source>Nature</source><volume>465</volume>: <fpage>632</fpage>–<lpage>636</lpage>.<pub-id pub-id-type="pmid">20520716</pub-id></mixed-citation>
    </ref>
    <ref id="bib39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ubbens</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Cieslak</surname>, <given-names>M.</given-names></string-name>, <string-name><surname>Prusinkiewicz</surname>, <given-names>P.</given-names></string-name>, <string-name><surname>Stavness</surname>, <given-names>I.</given-names></string-name></person-group>(<year>2018</year>). <article-title>The use of plant models in deep learning: An application to leaf counting in rosette plants</article-title>. <source>Plant Methods</source><volume>14</volume>: <fpage>6</fpage>.<pub-id pub-id-type="pmid">29375647</pub-id></mixed-citation>
    </ref>
    <ref id="bib40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ubbens</surname>, <given-names>J.R.</given-names></string-name>, <string-name><surname>Stavness</surname>, <given-names>I.</given-names></string-name></person-group>(<year>2017</year>). <article-title>Deep Plant Phenomics: A deep learning platform for complex plant phenotyping tasks</article-title>. <source>Front Plant Sci</source><volume>8</volume>: <fpage>1190</fpage>.<pub-id pub-id-type="pmid">28736569</pub-id></mixed-citation>
    </ref>
    <ref id="bib41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van der Walt</surname>, <given-names>S.</given-names></string-name>, <string-name><surname>Schonberger</surname>, <given-names>J.L.</given-names></string-name>, <string-name><surname>Nunez-Iglesias</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Boulogne</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Warner</surname>, <given-names>J.D.</given-names></string-name>, <string-name><surname>Yager</surname>, <given-names>N.</given-names></string-name>, <string-name><surname>Gouillart</surname>, <given-names>E.</given-names></string-name>, <string-name><surname>Yu</surname>, <given-names>T.</given-names></string-name>; <collab>scikit-image contributors.</collab></person-group> (<year>2014</year>). <article-title>scikit-image: image processing in Python</article-title>. <source>PeerJ</source>
<volume>2</volume>: <fpage>e453</fpage>.<pub-id pub-id-type="pmid">25024921</pub-id></mixed-citation>
    </ref>
    <ref id="bib42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vasseur</surname>, <given-names>F.</given-names></string-name>, <string-name><surname>Bresson</surname>, <given-names>J.</given-names></string-name>, <string-name><surname>Wang</surname>, <given-names>G.</given-names></string-name>, <string-name><surname>Schwab</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Weigel</surname>, <given-names>D.</given-names></string-name></person-group>(<year>2018</year>). <article-title>Image-based methods for phenotyping growth dynamics and fitness components in <italic toggle="yes">Arabidopsis thaliana</italic></article-title>. <source>Plant Methods</source><volume>14</volume>: <fpage>63</fpage>.<pub-id pub-id-type="pmid">30065776</pub-id></mixed-citation>
    </ref>
    <ref id="bib43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname>, <given-names>C.</given-names></string-name>, <string-name><surname>Li</surname>, <given-names>X.</given-names></string-name>, <string-name><surname>Caragea</surname>, <given-names>D.</given-names></string-name>, <string-name><surname>Bheemanahallia</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Jagadish</surname>, <given-names>S.V.K.</given-names></string-name></person-group>(<year>2020</year>). <article-title>Root anatomy based on root cross-section image analysis with deep learning</article-title>. <source>Comp Electr Agricult</source><volume>175</volume>: <fpage>105549</fpage>.</mixed-citation>
    </ref>
    <ref id="bib44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yasrab</surname>, <given-names>R.</given-names></string-name>, <string-name><surname>Atkinson</surname>, <given-names>J.A.</given-names></string-name>, <string-name><surname>Wells</surname>, <given-names>D.M.</given-names></string-name>, <string-name><surname>French</surname>, <given-names>A.P.</given-names></string-name>, <string-name><surname>Pridmore</surname>, <given-names>T.P.</given-names></string-name>, <string-name><surname>Pound</surname>, <given-names>M.P.</given-names></string-name></person-group>(<year>2019</year>). <article-title>RootNav 2.0: Deep learning for automatic navigation of complex plant root architectures</article-title>. <source>Gigascience</source><volume>8</volume>: <fpage>8</fpage>.</mixed-citation>
    </ref>
    <ref id="bib45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhu</surname>, <given-names>W.</given-names></string-name>, <etal/></person-group>(<year>2018</year>). <article-title>Modulation of ACD6 dependent hyperimmunity by natural alleles of an <italic toggle="yes">Arabidopsis thaliana</italic> NLR resistance gene</article-title>. <source>PLoS Genet.</source><volume>14</volume>: <fpage>e1007628</fpage>.<pub-id pub-id-type="pmid">30235212</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
