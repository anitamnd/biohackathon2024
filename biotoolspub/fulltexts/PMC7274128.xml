<?DTDIdentifier.IdentifierValue http://null/schema/dtds/document/fulltext/xcr/xocs-article.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName xocs-article.xsd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName ftrr2jats.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Methods Programs Biomed</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Methods Programs Biomed</journal-id>
    <journal-title-group>
      <journal-title>Computer Methods and Programs in Biomedicine</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0169-2607</issn>
    <issn pub-type="epub">1872-7565</issn>
    <publisher>
      <publisher-name>Elsevier B.V.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7274128</article-id>
    <article-id pub-id-type="pii">S0169-2607(20)31414-0</article-id>
    <article-id pub-id-type="doi">10.1016/j.cmpb.2020.105581</article-id>
    <article-id pub-id-type="publisher-id">105581</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CoroNet: A deep neural network for detection and diagnosis of COVID-19 from chest x-ray images</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0001">
        <name>
          <surname>Khan</surname>
          <given-names>Asif Iqbal</given-names>
        </name>
        <xref rid="aff0001" ref-type="aff">a</xref>
        <xref rid="cor0001" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au0002">
        <name>
          <surname>Shah</surname>
          <given-names>Junaid Latief</given-names>
        </name>
        <xref rid="aff0002" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au0003">
        <name>
          <surname>Bhat</surname>
          <given-names>Mohammad Mudasir</given-names>
        </name>
        <xref rid="aff0003" ref-type="aff">c</xref>
      </contrib>
      <aff id="aff0001"><label>a</label>Department of Computer Science, Jamia Millia Islamia, New Delhi, India</aff>
      <aff id="aff0002"><label>b</label>Higher Education Department, J&amp;K, India</aff>
      <aff id="aff0003"><label>c</label>Lelafe IT Solutions, J&amp;K, India</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor0001"><label>⁎</label>Corresponding author.</corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>5</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>11</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>5</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <volume>196</volume>
    <fpage>105581</fpage>
    <lpage>105581</lpage>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>4</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>5</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 Elsevier B.V. All rights reserved.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder/>
      <license>
        <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
      </license>
    </permissions>
    <abstract id="abs0002">
      <sec>
        <title>Background and Objective</title>
        <p>The novel Coronavirus also called COVID-19 originated in Wuhan, China in December 2019 and has now spread across the world. It has so far infected around 1.8 million people and claimed approximately 114,698 lives overall. As the number of cases are rapidly increasing, most of the countries are facing shortage of testing kits and resources. The limited quantity of testing kits and increasing number of daily cases encouraged us to come up with a Deep Learning model that can aid radiologists and clinicians in detecting COVID-19 cases using chest X-rays.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>In this study, we propose CoroNet, a Deep Convolutional Neural Network model to automatically detect COVID-19 infection from chest X-ray images. The proposed model is based on Xception architecture pre-trained on ImageNet dataset and trained end-to-end on a dataset prepared by collecting COVID-19 and other chest pneumonia X-ray images from two different publically available databases.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>CoroNet has been trained and tested on the prepared dataset and the experimental results show that our proposed model achieved an overall accuracy of 89.6%, and more importantly the precision and recall rate for COVID-19 cases are 93% and 98.2% for 4-class cases (COVID vs Pneumonia bacterial vs pneumonia viral vs normal). For 3-class classification (COVID vs Pneumonia vs normal), the proposed model produced a classification accuracy of 95%. The preliminary results of this study look promising which can be further improved as more training data becomes available.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>CoroNet achieved promising results on a small prepared dataset which indicates that given more data, the proposed model can achieve better results with minimum pre-processing of data. Overall, the proposed model substantially advances the current radiology based methodology and during COVID-19 pandemic, it can be very helpful tool for clinical practitioners and radiologists to aid them in diagnosis, quantification and follow-up of COVID-19 cases.</p>
      </sec>
    </abstract>
    <kwd-group id="keys0001">
      <title>Keywords</title>
      <kwd>Coronavirus</kwd>
      <kwd>COVID-19, Pneumonia viral</kwd>
      <kwd>Pneumonia bacterial</kwd>
      <kwd>Convolutional Neural Network</kwd>
      <kwd>Deep learning</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec0001">
    <label>1</label>
    <title>Introduction</title>
    <p id="para0011">The 2019 novel Coronavirus or COVID-19, first reported in Wuhan, China in December 2019 belongs to the family of viruses “Coronavirus” (CoV) was called “Severe Acute Respiratory Syndrome Coronavirus 2” (SARS-CoV-2) before it was named COVID-19 by World Health Organization (WHO) in February 2020. The outbreak was declared a Public Health Emergency of International Concern on 30 January 2020 <xref rid="bib0001" ref-type="bibr">[1]</xref> and finally on March 11, 2020, WHO declared COVID-19 as Pandemic. After the outbreak, the number of daily cases began to increase exponentially and reached 1.8 million cases and around 114,698 deaths globally by 12 April 2020. The virus has engulfed more than 210 countries among which USA, Spain and Italy are severely hit with 560,433, 166,831 and 156,363 active cases and 22,115, 17,209 and 19,899 deaths respectively <xref rid="bib0002" ref-type="bibr">[2]</xref>.</p>
    <p id="para0012">Once infected, a COVID-19 patient may develop various symptoms and signs of infection which include fever, cough and respiratory illness (like flu). In severe cases, the infection may cause pneumonia, difficulty breathing, multi-organ failure and death <xref rid="bib0002" ref-type="bibr">[2</xref>,<xref rid="bib0003" ref-type="bibr">3]</xref>. Due to the rapid and increasing growth rate of the COVID-19 cases, the health system of many advanced countries has come to the point of collapse. They are now facing shortage of ventilators and testing kits. Many countries have declared total lockdown and asked its population to stay indoors and strictly avoid gatherings.</p>
    <p id="para0013">A critical and important step in fighting COVID-19 is effective screening of infected patients, such that positive patients can be isolated and treated. Currently, the main screening method used for detecting COVID-19 is real-time reverse transcription polymerase chain reaction (rRT-PCR) <xref rid="bib0004" ref-type="bibr">[4</xref>,<xref rid="bib0005" ref-type="bibr">5]</xref>. The test is done on respiratory samples of the patient and the results can be available within few hours to 2 days. An alternate method to PCR screening method can be based on chest radiography images. Various research articles published in Radiology journal <xref rid="bib0006" ref-type="bibr">[6</xref>,<xref rid="bib0007" ref-type="bibr">7]</xref> indicate that chest scans might be useful in detecting COVID-19. Researchers found that the lungs of patients with COVID-19 symptoms have some visual marks like ground-glass opacities—hazy darkened spots that can differentiate COVID-19 infected patients from non COVID-19 infected ones <xref rid="bib0008" ref-type="bibr">[8</xref>,<xref rid="bib0009" ref-type="bibr">9]</xref>. The researchers believe that chest radiology based system can be an effective tool in detection, quantification and follow-up of COVID-19 cases.</p>
    <p id="para0014">A chest radiology image based detection system can have many advantages over conventional method. It can be fast, analyze multiple cases simultaneously, have greater availability and more importantly, such system can be very useful in hospitals with no or limited number of testing kits and resources. Moreover, given the importance of radiography in modern health care system, radiology imaging systems are available in every hospital, thus making radiography based approach more convenient and easily available.</p>
    <p id="para0015">Today, researchers from all around the world, from various different fields are working day and night to fight this pandemic. Many researchers have published series of preprint papers demonstrating approaches for COVID-19 detection from chest radiography images <xref rid="bib0010" ref-type="bibr">[10</xref>,<xref rid="bib0011" ref-type="bibr">11]</xref>. These approaches have achieved promising results on a small dataset but by no means are production ready solutions. These approaches still need rigorous testing and improvement before putting them in use. Subsequently, a large number of researchers and data scientists are working together to build highly accurate and reliable deep learning based approaches for detection and management of COVID-19 disease. Researchers are focusing on deep learning techniques to detect any specific features from chest radiography images of COVID-19 patients. In recent past, deep learning has been very successful in various visual tasks which include medical image analysis as well. Deep learning has revolutionized automatic disease diagnosis and management by accurately analyzing, identifying, classifying patterns in medical images. The reason behind such success is that deep learning techniques do not rely on manual handcrafted features but these algorithms learn features automatically from data itself <xref rid="bib0012" ref-type="bibr">[12]</xref>. In the past, deep learning has had success in disease classification using chest radiography image. ChexNet <xref rid="bib0013" ref-type="bibr">[13]</xref> is a deep neural network model that detects Pneumonia from chest X-ray image. ChexNet achieved exceptional results exceeding average radiologist performance. Another similar approach called ChestNet <xref rid="bib0014" ref-type="bibr">[14]</xref> is a deep neural network model designed to diagnose thorax diseases on chest radiography images.</p>
    <p id="para0016">The success of AI based techniques in automatic diagnosis in the medical field and rapid rise in COVID-10 cases have necessitated the need of AI based automatic detection and diagnosis system. Recently, many researchers have used radiology images for COVD-19 detection. A deep learning model for COVID-19 detection (COVID-Net) proposed by Wang and Wong <xref rid="bib0010" ref-type="bibr">[10]</xref> obtained 83.5% accuracy in classifying COVID-19, normal, pneumonia-bacterial and pneumonia-viral classes. Hemdan et al. <xref rid="bib0015" ref-type="bibr">[15]</xref> used various deep learning models to diagnose COVID-19 from cheat X-ray images and proposed a COVIDX-Net model comprising seven CNN models. Apostolopoulos and Mpesiana <xref rid="bib0016" ref-type="bibr">[16]</xref> trained different pre-trained deep learning models on a dataset comprising of 224 confirmed COVID-19 images and achieved 98.75% and 93.48% accuracy for two and three classes, respectively. Narin et al. <xref rid="bib0011" ref-type="bibr">[11]</xref> trained ResNet50 model using chest X-ray images and achieved a 98% COVID-19 detection accuracy for two classes. However, the performance for multi class classification is not known. Sethy and Behera <xref rid="bib0017" ref-type="bibr">[17]</xref> used various convolutional neural network (CNN) models along support vector machine (SVM) classifier for COVID-19 classification. Their study states that the ResNet50 model with SVM classifier provided the best performance. Most recently Ozturk et al. <xref rid="bib0018" ref-type="bibr">[18]</xref> proposed a deep network based on DarkNet model. Their model consists of 17 convolution layers with Leaky RelU as activation function. Their model achieved an accuracy of 98.08% for binary classes and 87.02% for multi-class cases. All these techniques except COVID-Net <xref rid="bib0010" ref-type="bibr">[10]</xref> either perform binary classification (normal vs COVID-19) or 3–class classification (normal vs pneumonia vs COVID-19). Other than COVID-Net, none of the methods discussed above treat pneumonia bacterial and pneumonia viral as separate classes.</p>
    <p id="para0017">In this study, we present a deep learning based approach to detect COVID-19 infection from chest X-ray images. We propose a deep convolutional neural network (CNN) model to classify three different types of Pneumonia; bacterial pneumonia, viral pneumonia and COVID-19 pneumonia. We also implemented binary and 3-class versions of our proposed model and compared the results with other studies in the literature. The proposed model is called CoroNet and will help us identifying the difference between three types of pneumonia infections and how COVID-19 is different from other infections. A model that can identify COVID-19 infection from chest radiography images can be very helpful to doctors in the triage, quantification and follow-up of positive cases. Even if this model does not completely replace the existing testing method, it can still be used to bring down the number of cases that need immediate testing or further review from experts.</p>
  </sec>
  <sec id="sec0002">
    <label>2</label>
    <title>Dataset</title>
    <p id="para0018">Deep learning is all about data which serves as fuel in these learning models. Since COVID-19 is a new disease, there is no appropriate sized dataset available that can be used for this study. Therefore, we had to create a dataset by collecting chest X-ray images from two different publically available image databases. COVID-19 X-ray images are available at an open source Github repository by Joseph et al. <xref rid="bib0019" ref-type="bibr">[19]</xref>. The authors have compiled the radiology images from various authentic sources (Radiological Society of North America (RSNA), Radiopaedia etc.) of COVID-19 cases for research purpose and most of the studies on COVID-19 use images from this source. The repository contains an open database of COVID-19 cases with chest X-ray or CT images and is being updated regularly. At the time of writing this paper, the database contained around 290 COVID-19 chest radiography images. Pneumonia bacterial, Pneumonia viral and normal chest X-ray images were obtained from Kaggle repository “Chest X-Ray Images (Pneumonia)” <xref rid="bib0020" ref-type="bibr">[20]</xref>. The dataset consists of 1203 normal, 660 bacterial Pneumonia and 931 viral Pneumonia cases. We collected a total of 1300 images from these two sources. We then resized all the images to the dimension of 224 × 224 pixels with a resolution of 72 dpi. <xref rid="tbl0001" ref-type="table">Table 1</xref>
below shows the summary of the prepared dataset. <xref rid="fig0001" ref-type="fig">Fig. 1</xref>
below shows some samples of chest X-ray images from the prepared dataset.<table-wrap position="float" id="tbl0001"><label>Table 1</label><caption><p>Dataset summary.</p></caption><alt-text id="alt0009">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Disease</th><th valign="top">No. of Images</th></tr></thead><tbody><tr><td valign="top">Normal</td><td valign="top">310</td></tr><tr><td valign="top">Pneumonia Bacterial</td><td valign="top">330</td></tr><tr><td valign="top">Pneumonia Viral</td><td valign="top">327</td></tr><tr><td valign="top">COVID-19</td><td valign="top">284</td></tr></tbody></table></table-wrap><fig id="fig0001"><label>Fig. 1</label><caption><p>Samples of chest x-ray images from prepared dataset (a) Normal (b) Pneumonia bacterial (c) Pneumonia viral (d) COVID-19.</p></caption><alt-text id="alt0001">Fig. 1</alt-text><graphic xlink:href="gr1_lrg"/></fig></p>
    <p id="para0019">In order to overcome the unbalanced data problem, we used resampling technique called random under-sampling which involves randomly deleting examples from the majority class until the dataset becomes balanced. We used only 310 normal, 330 pneumonia-bacterial and 327 Pneumonia-viral X-ray images randomly from this chest X-ray pneumonia database.</p>
  </sec>
  <sec id="sec0003">
    <label>3</label>
    <title>Methodology</title>
    <p id="para0020">In this section, we will discuss the work methodology for the proposed technique, model architecture, implementation and training. The work methodology is also illustrated in <xref rid="fig0002" ref-type="fig">Fig. 2</xref>
.<fig id="fig0002"><label>Fig. 2</label><caption><p>Overview of the proposed methodology.</p></caption><alt-text id="alt0002">Fig. 2</alt-text><graphic xlink:href="gr2_lrg"/></fig></p>
    <sec id="sec0004">
      <label>3.1</label>
      <title>Convolutional neural network (CNN)</title>
      <p id="para0021">Convolutional Neural Network also known as CNN is a deep learning technique that consists of multiple layers stacked together which uses local connections known as local receptive field and weight-sharing for better performance and efficiency. The deep architecture helps these networks learn many different and complex features which a simple neural network cannot learn. Convolutional neural networks are powering core of computer vision that has many applications which include self-driving cars, robotics, and treatments for the visually impaired. The main concept of CNN is to obtain local features from input (usually an image) at higher layers and combine them into more complex features at the lower layers <xref rid="bib0021" ref-type="bibr">[21</xref>,<xref rid="bib0022" ref-type="bibr">22]</xref>.</p>
      <p id="para0022">A typical Convolutional Neural Network architecture consists of the following layers:<list list-type="simple" id="celist0002"><list-item id="celistitem0007"><label>(a)</label><p id="para0023"><bold>Convolutional Layer</bold></p></list-item></list>
<list list-type="simple" id="celist0003"><list-item id="celistitem0008"><p id="para0024">Convolution layer is the core building block of a Convolutional Neural Network which uses convolution operation (represented by *) in place of general matrix multiplication. Its parameters consist of a set of learnable filters also known as kernels. The main task of the convolutional layer is to detect features found within local regions of the input image that are common throughout the dataset and mapping their appearance to a feature map. The convolution operation is given as<disp-formula id="eqn0001"><label>(1)</label><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo linebreak="badbreak">*</mml:mo><mml:mi>K</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.16em"/><mml:munder><mml:mo>∑</mml:mo><mml:mi>m</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mi>n</mml:mi></mml:munder><mml:mi>I</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p></list-item><list-item id="celistitem0009"><p id="para0025">Where <italic>I</italic> is the input matrix (image), K is the 2D filter of size <italic>m x n</italic> and F represents the output 2D feature map. Here the input <italic>I</italic> is convolved with the filter <italic>K</italic> and produces the feature map F. This convolution operation is denoted by I*K.</p></list-item><list-item id="celistitem0010"><p id="para0026">The output of each convolutional layer is fed to an activation function to introduce non-linearity. There are number of activation functions available but the one which is recognized for deep learning is Rectified Linear Unit (ReLU). ReLU simply computes the activation by thresholding the input at zero. In other words, ReLU outputs 0 if the input is less than 0, and raw output otherwise. It is mathematically given as:<disp-formula id="eqn0002"><label>(2)</label><mml:math id="M2" altimg="si2.svg"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>max</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
</p></list-item></list>
<list list-type="simple" id="celist0004"><list-item id="celistitem0011"><label>(b)</label><p id="para0027"><bold>Subsampling (Pooling) Layer</bold></p></list-item></list>
<list list-type="simple" id="celist0005"><list-item id="celistitem0012"><p id="para0028">In CNN, the sequence of convolution layer is followed by an optional pooling or down sampling layer to reduce the spatial size of the input and thus reducing the number of parameters in the network. A pooling layer takes each feature map output from the convolutional layer and down samples it i.e. pooling layer summarizes a region of neurons in the convolution layer. There most common pooling technique is Max Pooling which simply outputs the maximum value in the input region. Other pooling options are average pooling and L2-norm pooling.</p></list-item></list>
<list list-type="simple" id="celist0006"><list-item id="celistitem0013"><label>(c)</label><p id="para0029"><bold>Fully connected layer</bold></p></list-item></list>
<list list-type="simple" id="celist0007"><list-item id="celistitem0014"><p id="para0030">In fully connected layer each neuron from previous layer is connected to every neuron in the next layer and every value contributes in predicting how strongly a value matches a particular class. The output of last fully connected layer is then forwarded to an activation function which outputs the class scores. Softmax and Support Vector Machines (SVM) are the two main classifiers used in CNN. Softmax function which computes the probability distribution of the n output classes is given as<disp-formula id="eqn0003"><label>(3)</label><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:msup><mml:mi>Z</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:msup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
</p></list-item><list-item id="celistitem0015"><p id="para0031">Where x is the input vector and Z is the output vector. The sum of all outputs (Z) equals to 1. The proposed model CoroNet uses Softmax, to predict the class to which the input X-ray image belongs to.</p></list-item></list>
</p>
      <p id="para0032">All the layers discussed above are stacked up to make a full CNN architecture. In addition to these main layers mentioned above, CNN may include optional layers like batch normalization layer to improve the training time and dropout layer to address the overfitting issue.</p>
    </sec>
    <sec id="sec0005">
      <label>3.2</label>
      <title>Model architecture and development</title>
      <p id="para0033">CoroNet is a CNN architecture tailored for detection of COVID-19 infection from chest X-ray images. It is based on Xception CNN architecture <xref rid="bib0023" ref-type="bibr">[23]</xref>. Xception which stands for Extreme version of Inception <xref rid="bib0024" ref-type="bibr">[24]</xref> (its predecessor model) is a 71 layers deep CNN architecture pre-trained on ImageNet dataset. Xception uses depthwise separable convolution layers with residual connections instead of classical convolutions. Depthwise Separable Convolution replaces classic <italic>n x n x k</italic> convolution operation with <italic>1 × 1 x k</italic> point-wise convolution operation followed by channel-wise <italic>n x n</italic> spatial convolution operation. This way the number of operations are reduced by a factor proportional to 1/k.</p>
      <p id="para0034">Residual connections are 'skip connections' which allow gradients to flow through a network directly, without passing through non-linear activation functions and thus avoiding the problem of vanishing gradients. In residual connections, output of a weight layer series is added to the original input and then passed through non-linear activation function as shown in <xref rid="fig0003" ref-type="fig">Fig. 3</xref>
.<fig id="fig0003"><label>Fig. 3</label><caption><p>Residual connection.</p></caption><alt-text id="alt0003">Fig. 3</alt-text><graphic xlink:href="gr3_lrg"/></fig></p>
      <p id="para0035">CoroNet uses Xception as base model with a dropout layer and two fully-connected layers added at the end. CoroNet has 33,969,964 parameters in total out of which 33,969,964 trainable and 54,528 are non-trainable parameters. Architecture details, layer-wise parameters and output shape of CoroNet model are shown in <xref rid="tbl0002" ref-type="table">Table 2</xref>
. To initialize the model parameters, we used Transfer Learning to overcome the problem of overfitting as the training data was not sufficient.<table-wrap position="float" id="tbl0002"><label>Table 2</label><caption><p>Details of CoroNet architecture.</p></caption><alt-text id="alt0010">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Layer (type)</th><th valign="top">Output Shape</th><th valign="top">Param #</th></tr></thead><tbody><tr><td valign="top">Xception (Model)</td><td valign="top">5 × 5 × 2048</td><td valign="top">20,861,480</td></tr><tr><td valign="top">flatten (Flatten)</td><td valign="top">51,200</td><td valign="top">0</td></tr><tr><td valign="top">dropout (Dropout)</td><td valign="top">51,200</td><td valign="top">0</td></tr><tr><td valign="top">dense (Dense)</td><td valign="top">256</td><td valign="top">13,107,456</td></tr><tr><td valign="top">dense_1 (Dense)</td><td valign="top">4</td><td valign="top">1028</td></tr><tr><td valign="top">Total Parameters: 33,969,964</td><td valign="top"/><td valign="top"/></tr><tr><td valign="top">Trainable Parameters: 33,915,436</td><td valign="top"/><td valign="top"/></tr><tr><td valign="top">Non-trainable Parameters: 54,528</td><td valign="top"/><td valign="top"/></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec0006">
      <label>3.3</label>
      <title>Implementation and training</title>
      <p id="para0036">We implemented three scenarios of the proposed model to detect COVID-19 from chest X-ray images. First model is the main multi-class model (4-class CoroNet) which is trained to classify chest X-ray images into four categories: <italic>COVID-19, Normal, Pneumonia-bacterial</italic> and <italic>Pneumonia-viral</italic>. The other two models 3-class CoroNet (<italic>COVID-19, Normal</italic> and <italic>Pneumonia</italic>) and binary 2-class CoroNet model (<italic>COVID-19, Normal</italic> and <italic>Pneumonia</italic>) are modifications of the main multi-class model.</p>
      <p id="para0037">The proposed model, CoroNet was implemented in Keras on top of Tensorflow 2.0. The model was pre-trained on ImageNet dataset and then retrained end-to-end on prepared dataset using Adam optimizer with learning rate of 0.0001, batch size of 10 and epoch value of 80. For training, data shuffling was enabled which involves shuffling the data before each epoch. All the experiment and training was done on Google Colaboratory Ubuntu server equipped with Tesla K80 graphics card. We used 4-fold cross-validation <xref rid="bib0025" ref-type="bibr">[25]</xref> approach to assess the performance of our main 4-class model. The training set was randomly divided into 4 equal sets. Three out of four sets were used to train the CNN model while the remaining set was used for validation. This strategy was repeated 4 times by shifting the validation and training sets. Final performance of the model was reported by averaging values obtained from each fold. Plots of accuracy and loss on the training and validation sets over training epochs for Fold 4 are shown in <xref rid="fig0004" ref-type="fig">Fig. 4</xref>
.<fig id="fig0004"><label>Fig. 4</label><caption><p>Plots of accuracy and loss on training and validation sets over training epochs for fold 4.</p></caption><alt-text id="alt0004">Fig. 4</alt-text><graphic xlink:href="gr4_lrg"/></fig></p>
    </sec>
  </sec>
  <sec id="sec0007">
    <label>4</label>
    <title>Results</title>
    <p id="para0038">The multi-class classification result of the proposed model was recorded for each Fold and then average numbers were calculated. The performance of the proposed CoroNet on each fold is presented in the form of Confusion matrix (CM) in <xref rid="fig0005" ref-type="fig">Fig. 5</xref>
. Overall Accuracy, precision, recall and F-measure computed for each fold by formulae given below are summarized in <xref rid="tbl0003" ref-type="table">Table 3</xref>
.<disp-formula id="ueqn0004"><mml:math id="M4" altimg="si4.svg"><mml:mrow><mml:mtext>Accuracy</mml:mtext><mml:mspace width="0.16em"/><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mo>.</mml:mo><mml:mspace width="0.16em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.16em"/><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.16em"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>l</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.16em"/><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.16em"/><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mo>.</mml:mo><mml:mspace width="0.16em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.16em"/><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="ueqn0005"><mml:math id="M5" altimg="si5.svg"><mml:mrow><mml:mtext>Precision</mml:mtext><mml:mspace width="-0.16em"/><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="-0.16em"/><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.16em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.16em"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.16em"/><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.16em"/><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.16em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.16em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.16em"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.16em"/><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.16em"/><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.16em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.16em"/><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.16em"/><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.16em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="ueqn0006"><mml:math id="M6" altimg="si6.svg"><mml:mrow><mml:mtext>Recall</mml:mtext><mml:mspace width="0.28em"/><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.28em"/><mml:mfrac><mml:mrow><mml:mi>S</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.16em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.16em"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.16em"/><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.16em"/><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.16em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>u</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.16em"/><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.16em"/><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.16em"/><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.16em"/><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.16em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.16em"/><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.16em"/><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.16em"/><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<disp-formula id="ueqn0007"><mml:math id="M7" altimg="si7.svg"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mtext>measure</mml:mtext></mml:mrow><mml:mspace width="0.28em"/><mml:mo linebreak="goodbreak">=</mml:mo><mml:mspace width="0.28em"/><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>*</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>*</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
<fig id="fig0005"><label>Fig. 5</label><caption><p>Confusion matrices of 4-class classification task (a) Fold 1 CM (b) Fold 2 CM (c) Fold 3 CM (d) Fold 4 CM.</p></caption><alt-text id="alt0005">Fig. 5</alt-text><graphic xlink:href="gr5_lrg"/></fig><table-wrap position="float" id="tbl0003"><label>Table 3</label><caption><p>Performance of the CoroNet on each fold.</p></caption><alt-text id="alt0011">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Folds</th><th valign="top">Precision (%)</th><th valign="top">Recall (%)</th><th valign="top">Specificity (%)</th><th valign="top">F-measure (%)</th><th valign="top">Accuracy (%)</th></tr></thead><tbody><tr><td valign="top">Fold 1</td><td valign="top">88</td><td valign="top">87.7</td><td valign="top">95.7</td><td valign="top">87.6</td><td valign="top">87.3</td></tr><tr><td valign="top">Fold 2</td><td valign="top">90.8</td><td valign="top">90.7</td><td valign="top">96.7</td><td valign="top">90.7</td><td valign="top">90</td></tr><tr><td valign="top">Fold 3</td><td valign="top">88.9</td><td valign="top">89</td><td valign="top">96.2</td><td valign="top">88.9</td><td valign="top">89.1</td></tr><tr><td valign="top">Fold 4</td><td valign="top">92.5</td><td valign="top">92.2</td><td valign="top">97.3</td><td valign="top">92.1</td><td valign="top">92.26</td></tr><tr><td valign="top">Average</td><td valign="top">90</td><td valign="top">89.92</td><td valign="top">96.4</td><td valign="top">89.8</td><td valign="top">89.6</td></tr></tbody></table></table-wrap></p>
    <p id="para0039">The aforementioned performance metrics are the top metrics used to measure the performance of classification algorithms. The proposed model CoroNet achieved an average accuracy of 89.6%, while as average accuracy, precision, recall and F-measure (F1-Score) for COVID-19 class are 96.6%, 93.17%, 98.25% and 95.6% respectively. The class wise performance of CoroNet is presented in <xref rid="tbl0004" ref-type="table">Table 4</xref>
. The performance for Non COVID-19 pneumonia classes (pneumonia-bacterial and pneumonia-viral) are comparatively lower than other two classes and contributes to the lower overall accuracy. If we combine the pneumonia-bacterial and pneumonia-viral into one single class as Pneumonia class, then the overall accuracy increases significantly. We did slight modification to same 4-class CoroNet model and fine-tuned it for three classes only. In this way we did not have to retrain the model from end-to-end. After fine-tuning, the model was tested on test set comprising of 29 COVID-19, 72 normal and 120 pneumonia cases. The Confusion matrix of 3-class CoroNet is presented in <xref rid="fig0006" ref-type="fig">Fig. 6</xref>
(a). After combining the two non-COVID-19 pneumonia infections, the overall accuracy of CoroNet increased from 89.5% to 94.59%. Finally, the confusion matrix for the binary classification problem in detecting COVID-19 positive is presented in <xref rid="fig0006" ref-type="fig">Fig. 6</xref>(b). In addition, precision, recall, F-measure, and accuracy results for all three classification tasks are given in <xref rid="tbl0005" ref-type="table">Table 5</xref>
.<table-wrap position="float" id="tbl0004"><label>Table 4</label><caption><p>Average class-wise precision, recall, F-measure of 4-class CoroNet.</p></caption><alt-text id="alt0012">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Class</th><th valign="top">Precision (%)</th><th valign="top">Recall (%)</th><th valign="top">Specificity (%)</th><th valign="top">F-measure (%)</th></tr></thead><tbody><tr><td valign="top">COVID-19</td><td valign="top">93.17</td><td valign="top">98.25</td><td valign="top">97.9</td><td valign="top">95.61</td></tr><tr><td valign="top">Normal</td><td valign="top">95.25</td><td valign="top">93.5</td><td valign="top">98.1</td><td valign="top">94.3</td></tr><tr><td valign="top">Pneumonia Bacterial</td><td valign="top">86.85</td><td valign="top">85.9</td><td valign="top">95</td><td valign="top">86.3</td></tr><tr><td valign="top">Pneumonia Viral</td><td valign="top">84.1</td><td valign="top">82.1</td><td valign="top">94.8</td><td valign="top">83.1</td></tr></tbody></table></table-wrap><fig id="fig0006"><label>Fig. 6</label><caption><p>Confusion matrix results of CoroNet a) 3-class Classification and b) binary classification.</p></caption><alt-text id="alt0006">Fig. 6</alt-text><graphic xlink:href="gr6_lrg"/></fig><table-wrap position="float" id="tbl0005"><label>Table 5</label><caption><p>Performance of 4-class, 3-class and binary CoroNet.</p></caption><alt-text id="alt0013">Table 5</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Model</th><th valign="top">Precision (%)</th><th valign="top">Recall (%)</th><th valign="top">Specificity (%)</th><th valign="top">F-measure (%)</th><th valign="top">Accuracy (%)</th></tr></thead><tbody><tr><td valign="top">4-class CoroNet</td><td valign="top">90</td><td valign="top">89.92</td><td valign="top">96.4</td><td valign="top">89.8</td><td valign="top">89.6</td></tr><tr><td valign="top">3-Class CoroNet</td><td valign="top">95</td><td valign="top">96.9</td><td valign="top">97.5</td><td valign="top">95.6</td><td valign="top">95</td></tr><tr><td valign="top">Binary CoroNet</td><td valign="top">98.3</td><td valign="top">99.3</td><td valign="top">98.6</td><td valign="top">98.5</td><td valign="top">99</td></tr></tbody></table></table-wrap></p>
    <sec id="sec0008">
      <label>4.1</label>
      <title>Classification accuracy on Dataset-2</title>
      <p id="para0040">To check the robustness, we tested our proposed model on another dataset prepared by Ozturk et al. <xref rid="bib0018" ref-type="bibr">[18]</xref>. The dataset-2 contains around 500 normal, 500 pneumonia and 157 COVID-19 chest X-ray images. This dataset contains same COVID-19 X-ray images as in our prepared dataset, however normal and pneumonia X-ray images were collected from ChestX-ray database provided by Wang et al. <xref rid="bib0026" ref-type="bibr">[26]</xref>. After slight modification and fine-tuning, our proposed model achieved an overall accuracy of 90%, The results are illustrated in <xref rid="tbl0006" ref-type="table">Table 6</xref>
and corresponding confusion matrix is given in <xref rid="fig0007" ref-type="fig">Fig. 7</xref>
. <xref rid="tbl0006" ref-type="table">Table 6</xref> shows accuracy, precision, recall and F-measure of CoroNet on Dataset-2.<table-wrap position="float" id="tbl0006"><label>Table 6</label><caption><p>Performance of the CoroNet on Dataset-2 <xref rid="bib0018" ref-type="bibr">[18]</xref>.</p></caption><alt-text id="alt0014">Table 6</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Class</th><th valign="top">Precision (%)</th><th valign="top">Recall (%)</th><th valign="top">Specificity (%)</th><th valign="top">F-measure (%)</th></tr></thead><tbody><tr><td valign="top">COVID-19</td><td valign="top">97</td><td valign="top">89</td><td valign="top">99.6</td><td valign="top">93</td></tr><tr><td valign="top">Normal</td><td valign="top">92</td><td valign="top">85</td><td valign="top">97.7</td><td valign="top">89</td></tr><tr><td valign="top">Pneumonia Bacterial</td><td valign="top">87</td><td valign="top">95</td><td valign="top">88.7</td><td valign="top">91</td></tr><tr><td valign="top">Average</td><td valign="top">92</td><td valign="top">90</td><td valign="top">95.3</td><td valign="top">91</td></tr><tr><td valign="top">Overall Accuracy</td><td colspan="4" align="left" valign="top">90.21%</td></tr></tbody></table></table-wrap><fig id="fig0007"><label>Fig. 7</label><caption><p>Confusion matrix result of CoroNet on Dataset-2 <xref rid="bib0018" ref-type="bibr">[18]</xref>.</p></caption><alt-text id="alt0007">Fig. 7</alt-text><graphic xlink:href="gr7_lrg"/></fig></p>
    </sec>
  </sec>
  <sec id="sec0009">
    <label>5</label>
    <title>Discussion</title>
    <p id="para0041">In this study, we proposed a deep model based on Xception architecture to detect COVID-19 cases from chest X-ray images. The proposed model was tested on two datasets and performed exceptionally well on both of them. Our model achieved an accuracy of 89.5%, 94.59% and 99% for 4-classes, 3-classes and binary class classification tasks respectively. Furthermore, our model also achieved an accuracy of 90% on dataset-2. Another positive observation from the results is the precision (PPV) and recall (Sensitivity) for COVID-19 cases. Higher recall value means low false negative (FN) cases and low number of FN is an encouraging result. This is important because minimizing the missed COVID-19 cases as much as possible is the main aim of this research.</p>
    <p id="para0042">The results obtained by our proposed model are superior compared to other studies in the literature. <xref rid="tbl0007" ref-type="table">Table 7</xref>
presents a summary of studies conducted in the automated diagnosis of COVID-19 from chest X-ray images and their comparison with our proposed model CoroNet. <xref rid="fig0008" ref-type="fig">Fig. 8</xref>
shows CoroNet results on some sample images from the test set.<table-wrap position="float" id="tbl0007"><label>Table 7</label><caption><p>Comparison of the proposed CoroNet with other existing deep learning methods.</p></caption><alt-text id="alt0015">Table 7</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top">Study</th><th valign="top">Architecture</th><th valign="top">Accuracy 3-class (%)</th><th valign="top">Accuracy 2-class (%)</th><th valign="top"># Params (in million)</th></tr></thead><tbody><tr><td valign="top">Ioannis et al. [43]</td><td valign="top">VGG19</td><td valign="top">93.48</td><td valign="top">98.75</td><td valign="top">143</td></tr><tr><td valign="top">Ioannis et al. [43]</td><td valign="top">Xception</td><td valign="top">92.85</td><td valign="top">85.57</td><td valign="top">33</td></tr><tr><td valign="top">Wang and Wong [42]</td><td valign="top">Covid-Net (Residual Arch)</td><td valign="top">NA</td><td valign="top">92.4</td><td valign="top">116</td></tr><tr><td valign="top">Sethy and Behra [45]</td><td valign="top">ResNet-50</td><td valign="top">NA</td><td valign="top">95.38</td><td valign="top">36</td></tr><tr><td valign="top">Hemdan et al. [41]</td><td valign="top">VGG19</td><td valign="top">NA</td><td valign="top">90</td><td valign="top">143</td></tr><tr><td valign="top">Narin et al. [44]</td><td valign="top">ResNet-50</td><td valign="top">NA</td><td valign="top">98</td><td valign="top">36</td></tr><tr><td valign="top">Narin et al. [44]</td><td valign="top">InceptionV3</td><td valign="top">NA</td><td valign="top">97</td><td valign="top">26</td></tr><tr><td valign="top">Ozturk et al. <xref rid="bib0018" ref-type="bibr">[18]</xref></td><td valign="top">DarkNet</td><td valign="top">87.02</td><td valign="top">98.08</td><td valign="top">1.1</td></tr><tr><td valign="top">Proposed CoroNet</td><td valign="top">CoroNet (Xception)</td><td valign="top">89.6</td><td valign="top">99</td><td valign="top">33</td></tr></tbody></table></table-wrap><fig id="fig0008"><label>Fig. 8</label><caption><p>Some images evaluated by CoroNet.</p></caption><alt-text id="alt0008">Fig. 8</alt-text><graphic xlink:href="gr8_lrg"/></fig></p>
    <p id="para0043">Wang and Wong <xref rid="bib0010" ref-type="bibr">[10]</xref> presented a residual deep architecture called COVID-Net for detection of COVID-19 from chest X-ray images. COVID-Net is one of the early works done on COVID-19 which uses deep neural network to classify chest X-ray images into four categories (COVID, Normal, Pneumonia bacterial and Pneumonia Viral). COVID-Net achieved an accuracy of 83.5% for four classes. <xref rid="tbl0008" ref-type="table">Table 8</xref>
presents performance comparison of COVID-Net and our proposed model CoroNet on 4-class classification task.<table-wrap position="float" id="tbl0008"><label>Table 8</label><caption><p>4-class performance comparison between Covid-Net and Proposed CoroNet.</p></caption><alt-text id="alt0016">Table 8</alt-text><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th colspan="3" align="left" valign="top">COVID-Net <xref rid="bib0010" ref-type="bibr">[10]</xref><hr/></th><th colspan="3" align="left" valign="top">CoroNet<hr/></th></tr><tr><th valign="top">Class</th><th valign="top">Precision (%)</th><th valign="top">Recall (%)</th><th valign="top">F-measure (%)</th><th valign="top">Precision (%)</th><th valign="top">Recall (%)</th><th valign="top">F-measure (%)</th></tr></thead><tbody><tr><td valign="top">COVID-19</td><td valign="top">80</td><td valign="top">100</td><td valign="top">88.8</td><td valign="top">93.17</td><td valign="top">98.25</td><td valign="top">95.61</td></tr><tr><td valign="top">Normal</td><td valign="top">95.1</td><td valign="top">73.9</td><td valign="top">83.17</td><td valign="top">95.25</td><td valign="top">93.5</td><td valign="top">94.3</td></tr><tr><td valign="top">Pneumonia Bacterial</td><td valign="top">87.1</td><td valign="top">93.1</td><td valign="top">90</td><td valign="top">86.85</td><td valign="top">85.9</td><td valign="top">86.3</td></tr><tr><td valign="top">Pneumonia Viral</td><td valign="top">67.0</td><td valign="top">81.9</td><td valign="top">73.7</td><td valign="top">84.1</td><td valign="top">82.1</td><td valign="top">83.1</td></tr><tr><td valign="top"># of Parameters</td><td colspan="3" align="left" valign="top">116 million</td><td colspan="3" align="left" valign="top">33 million</td></tr><tr><td valign="top">Accuracy</td><td colspan="3" align="left" valign="top">83.5%</td><td colspan="3" align="left" valign="top">89.6%</td></tr></tbody></table></table-wrap></p>
    <p id="para0044">Apostolopoulos and Mpesiana <xref rid="bib0016" ref-type="bibr">[16]</xref> evaluated various state-of-the-art deep architectures on chest X-ray images. With transfer learning their best model VGG19 managed to achieve an accuracy of 93.48% and 98.75% for 3-class and 2-class classification tasks respectively on a dataset consisting of 224 COVID-19, 700 pneumonia and 504 normal X-ray images.</p>
    <p id="para0045">Narin et al. <xref rid="bib0011" ref-type="bibr">[11]</xref> performed same experiment with three different CNN models (ResNet50, InceptionV3, and InceptionResNetV2) and ResNet50 pre-trained on ImageNet database achieved best accuracy of 98% for 2-class classification. Since, they did not include pneumonia cases in their experiment, it is unknown how well their model would distinguish between COVID-19 and other pneumonia cases.</p>
    <p id="para0046">CNNs are deep models which perform automatic feature extraction from input data and based on these extracted features, a classifier like Softmax performs classification. Softmax classifier is a default but noncompulsory choice for CNN's and can be replaced by any good classifier like Support Vector Machine (SVM). One such experiment was done by Sethy and Behera <xref rid="bib0017" ref-type="bibr">[17]</xref>. They employed ResNet50 CNN model along with SVM for detection of COVID-19 cases from chest X-ray images. CNN model acts as feature extractor and SVM serves the purpose of classifier. Their model achieved an accuracy of 95.38% on 2-class problem.</p>
    <p id="para0047">Ozturk et al. <xref rid="bib0018" ref-type="bibr">[18]</xref> proposed a CNN model based on DarkNet architecture to detect and classify COVID-19 cases from X-ray images. Their model achieved binary and 3-class classification accuracy of 98.08% and 87.02%, respectively on a dataset consisting of 125 COVID-19, 500 Pneumonia and 500 normal chest X-ray images.</p>
    <p id="para0048">The promising and encouraging results of deep learning models in detection of COVID-19 from radiography images indicate that deep learning has a greater role to play in fighting this pandemic in near future. Some limitation of this study can be overcome with more in depth analysis which is possible once more patient data (both symptomatic and asymptomatic patients) becomes available.</p>
  </sec>
  <sec id="sec0010">
    <label>6</label>
    <title>Conclusion</title>
    <p id="para0049">As the cases of COVID-19 pandemic are increasing daily, many countries are facing shortage of resources. During this health emergency, it is important that not even a single positive case goes unidentified. With this thing in mind, we proposed a deep learning approach to detect COVID-19 cases from chest radiography images. The proposed method (CoroNet) is a convolutional neural network designed to identify COVID-19 cases using chest X-ray images. The model has been trained and tested on a small dataset of few hundred images prepared by obtaining chest X-ray images of various pneumonia cases and COVID-19 cases from different publically available databases. CoroNet is computationally less expensive and achieved promising results on the prepared dataset. The performance can further be improved once more training data becomes available. Notwithstanding the encouraging results, CoroNet still needs clinical study and testing but with higher accuracy and sensitivity for COVID-19 cases, CoroNet can still be beneficial for radiologists and health experts to gain deeper understandings into critical aspects associated with COVID-19 cases.</p>
  </sec>
  <sec id="sec0011">
    <title>Source code and dataset</title>
    <p id="para0050">For further research in this area we have made the source code, trained model and dataset available at <ext-link ext-link-type="uri" xlink:href="https://github.com/drkhan107/CoroNet" id="interref0001">https://github.com/drkhan107/CoroNet</ext-link>
</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="para0052">The authors have no conflict of interest to disclose.</p>
  </sec>
</body>
<back>
  <ref-list id="cebibl1">
    <title>References</title>
    <ref id="bib0001">
      <label>1</label>
      <mixed-citation publication-type="other" id="othref0001">WHO updates on COVID-19 <ext-link ext-link-type="uri" xlink:href="https://www.who.int/emergencies/diseases/novel-coronavirus-2019/events-as-they-happen" id="interref0003">https://www.who.int/emergencies/diseases/novel-coronavirus-2019/events-as-they-happen</ext-link>(Last Accessed: 3 Apr 2020)</mixed-citation>
    </ref>
    <ref id="bib0002">
      <label>2</label>
      <mixed-citation publication-type="other" id="othref0002">E. Mahase, "Coronavirus: Covid-19 Has Killed More People Than SARS and MERS Combined, Despite Lower Case Fatality Rate." (2020). doi: <pub-id pub-id-type="doi">10.1136/bmj.m641</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib0003">
      <label>3</label>
      <mixed-citation publication-type="other" id="othref0003">COVID-19 symptoms <ext-link ext-link-type="uri" xlink:href="https://www.who.int/health-topics/coronavirus#tab=tab_3" id="interref0004">https://www.who.int/health-topics/coronavirus#tab=tab_3</ext-link>Last Accessed: 3 Apr 2020</mixed-citation>
    </ref>
    <ref id="bib0004">
      <label>4</label>
      <element-citation publication-type="journal" id="sbref0001">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Detection of SARS-CoV-2 in different types of clinical specimens</article-title>
        <source>JAMA</source>
        <volume>323</volume>
        <issue>18</issue>
        <year>2020 May 12</year>
        <fpage>1843</fpage>
        <lpage>1844</lpage>
        <pub-id pub-id-type="pmid">32159775</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0005">
      <label>5</label>
      <element-citation publication-type="journal" id="sbref0002">
        <person-group person-group-type="author">
          <name>
            <surname>Corman</surname>
            <given-names>V.M.</given-names>
          </name>
          <name>
            <surname>Landt</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Kaiser</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Molenkamp</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Meijer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chu</surname>
            <given-names>D.K.</given-names>
          </name>
          <name>
            <surname>Bleicker</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Brünink</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Schneider</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>M.L.</given-names>
          </name>
          <name>
            <surname>Mulders</surname>
            <given-names>D.G</given-names>
          </name>
        </person-group>
        <article-title>Detection of 2019 novel coronavirus (2019-nCoV) by real-time RT-PCR</article-title>
        <source>Eurosurveillance.</source>
        <volume>25</volume>
        <issue>3</issue>
        <year>2020 Jan 23</year>
        <object-id pub-id-type="publisher-id">2000045</object-id>
      </element-citation>
    </ref>
    <ref id="bib0006">
      <label>6</label>
      <mixed-citation publication-type="other" id="othref0004">A. Bernheim, X. Mei, M. Huang, Y. Yang, Z.A. Fayad, N. Zhang, K. Diao, B. Lin, X. Zhu, K. Li, S. Li, Chest CT findings in coronavirus disease-19 (COVID-19): relationship to duration of infection. Radiology. 2020 Feb 20:200463.</mixed-citation>
    </ref>
    <ref id="bib0007">
      <label>7</label>
      <mixed-citation publication-type="other" id="othref0005">X. Xie, Z. Zhong, W. Zhao, C. Zheng, F. Wang, J. Liu, Chest CT for typical 2019-nCoV pneumonia: relationship to negative RT-PCR testing. Radiology. 2020 Feb 12:200343.</mixed-citation>
    </ref>
    <ref id="bib0008">
      <label>8</label>
      <mixed-citation publication-type="other" id="othref0006">Y. Fang, H. Zhang, J. Xie, M. Lin, L. Ying, P. Pang, W. Ji, Sensitivity of Chest CT for COVID-19: comparison to RT-PCR. Radiology2020 Feb. 19:200432. [Epub ahead of print], doi:<ext-link ext-link-type="uri" xlink:href="https://pubs.rsna.org/doi/full/10.1148/radiol.2020200432" id="interref0003c">https://pubs.rsna.org/doi/full/10.1148/radiol.2020200432</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0009">
      <label>9</label>
      <mixed-citation publication-type="other" id="othref0007">X. Xie, Z. Zhong, W. Zhao, C. Zheng, F. Wang, J. Liu, Chest CT for typical 2019-nCoV pneumonia: relationship to negative RT-PCR testing. Radiology 2020 Feb. 7 (Epub ahead of print), doi:<ext-link ext-link-type="uri" xlink:href="https://pubs.rsna.org/doi/10.1148/radiol.2020200343" id="interref0003b">https://pubs.rsna.org/doi/10.1148/radiol.2020200343</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0010">
      <label>10</label>
      <mixed-citation publication-type="other" id="othref0008">L. Wang, A. Wong, COVID-Net: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest Radiography Images. arXiv preprint arXiv:<ext-link ext-link-type="uri" xlink:href="arxiv:2003.09871" id="interref00008g">2003.09871</ext-link>. 2020 Mar 22.</mixed-citation>
    </ref>
    <ref id="bib0011">
      <label>11</label>
      <mixed-citation publication-type="other" id="othref0009">A. Narin, C. Kaya, Z. Pamuk, Automatic Detection of Coronavirus Disease (COVID-19) Using X-ray Images and Deep Convolutional Neural Networks. arXiv preprint arXiv:<ext-link ext-link-type="uri" xlink:href="arxiv:2003.10849" id="interref0008a">2003.10849</ext-link>. 2020 Mar 24.</mixed-citation>
    </ref>
    <ref id="bib0012">
      <label>12</label>
      <element-citation publication-type="journal" id="sbref0003">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Suk</surname>
            <given-names>H.I</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in medical image analysis</article-title>
        <source>Annu. Rev. Biomed. Eng.</source>
        <volume>19</volume>
        <year>2017 Jun 21</year>
        <fpage>221</fpage>
        <lpage>248</lpage>
        <pub-id pub-id-type="pmid">28301734</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0013">
      <label>13</label>
      <mixed-citation publication-type="other" id="othref0010">P. Rajpurkar, J. Irvin, K. Zhu, B. Yang, H. Mehta, T. Duan, D. Ding, A. Bagul, C. Langlotz, K. Shpanskaya, M.P. Lungren, Chexnet: radiologist-level pneumonia detection on chest X-rays with deep learning. arXiv preprint arXiv:<ext-link ext-link-type="uri" xlink:href="arxiv:1711.05225" id="interref0008b">1711.05225</ext-link>. 2017 Nov 14.</mixed-citation>
    </ref>
    <ref id="bib0014">
      <label>14</label>
      <mixed-citation publication-type="other" id="othref0010a">H. Wang, Y. Xia. Chestnet: A deep neural network for classification of thoracic diseases on chest radiography. arXiv preprint arXiv:<ext-link ext-link-type="uri" xlink:href="arxiv:1807.03058" id="interref0008z">1807.03058</ext-link>. 2018 Jul 9.</mixed-citation>
    </ref>
    <ref id="bib0015">
      <label>15</label>
      <mixed-citation publication-type="other" id="othref0012">E.E. Hemdan, M.A. Shouman, M.E. Karar, Covidx-net: a framework of deep learning classifiers to diagnose covid-19 in X-ray images. arXiv preprint arXiv:<ext-link ext-link-type="uri" xlink:href="arxiv:2003.11055" id="interref0008d">2003.11055</ext-link>. 2020 Mar 24.</mixed-citation>
    </ref>
    <ref id="bib0016">
      <label>16</label>
      <mixed-citation publication-type="other" id="othref0013">Apostolopoulos ID, Mpesiana TA. Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks. Phys. Eng. Sci. Med.. 2020 Apr 6:1</mixed-citation>
    </ref>
    <ref id="bib0017">
      <label>17</label>
      <element-citation publication-type="journal" id="sbref0004">
        <person-group person-group-type="author">
          <name>
            <surname>Sethy</surname>
            <given-names>P.K.</given-names>
          </name>
          <name>
            <surname>Behera</surname>
            <given-names>S.K.</given-names>
          </name>
        </person-group>
        <article-title>Detection of coronavirus disease (covid-19) based on deep features</article-title>
        <source>Preprints</source>
        <volume>2020030300</volume>
        <year>2020 Mar 19</year>
        <fpage>2020</fpage>
      </element-citation>
    </ref>
    <ref id="bib0018">
      <label>18</label>
      <mixed-citation publication-type="other" id="othref0014">T. Ozturk, M. Talo, E.A. Yildirim, U.B. Baloglu, O. Yildirim, U.R. Acharya, Automated detection of COVID-19 cases using deep neural networks with X-ray images. Comput. Biol. Med.. 2020 Apr 28:103792</mixed-citation>
    </ref>
    <ref id="bib0019">
      <label>19</label>
      <mixed-citation publication-type="other" id="othref0015">J.P. Cohen, P. Morrison and L. Dao, COVID-19 image data collection, arXiv:<ext-link ext-link-type="uri" xlink:href="arxiv:2003.11597" id="interref0008e">2003.11597</ext-link>, 2020 <ext-link ext-link-type="uri" xlink:href="https://github.com/ieee8023/covid-chestxray-dataset" id="interref0011">https://github.com/ieee8023/covid-chestxray-dataset</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0020">
      <label>20</label>
      <mixed-citation publication-type="other" id="othref0016">Chest X-ray images (pneumonia). <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia" id="interref0012">https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia</ext-link>. Last Accessed:1 Apr 2020</mixed-citation>
    </ref>
    <ref id="bib0021">
      <label>21</label>
      <element-citation publication-type="book" id="sbref0005">
        <person-group person-group-type="author">
          <name>
            <surname>Wan</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Hoi</surname>
            <given-names>S.C.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <part-title>Deep learning for content-based image retrieval: a comprehensive study</part-title>
        <source>Proceedings of the 22nd ACM international conference on Multimedia</source>
        <year>2014 Nov 3</year>
        <fpage>157</fpage>
        <lpage>166</lpage>
      </element-citation>
    </ref>
    <ref id="bib0022">
      <label>22</label>
      <element-citation publication-type="book" id="sbref0006">
        <person-group person-group-type="author">
          <name>
            <surname>Wani</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Bhat</surname>
            <given-names>F.A.</given-names>
          </name>
          <name>
            <surname>Afzal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>A.I</given-names>
          </name>
        </person-group>
        <part-title>Advances in Deep Learning</part-title>
        <year>2020</year>
        <publisher-name>Springer</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib0023">
      <label>23</label>
      <mixed-citation publication-type="other" id="othref0017">F. Chollet, Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2017 (pp. 1251–1258).</mixed-citation>
    </ref>
    <ref id="bib0024">
      <label>24</label>
      <mixed-citation publication-type="other" id="othref0018">C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna, Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2016 (pp. 2818–2826).</mixed-citation>
    </ref>
    <ref id="bib0025">
      <label>25</label>
      <element-citation publication-type="book" id="sbref0007">
        <person-group person-group-type="author">
          <name>
            <surname>Duda</surname>
            <given-names>R.O.</given-names>
          </name>
          <name>
            <surname>Hart</surname>
            <given-names>P.E.</given-names>
          </name>
          <name>
            <surname>Stork</surname>
            <given-names>D.G.</given-names>
          </name>
        </person-group>
        <part-title>Pattern Classification</part-title>
        <edition>2nd edition</edition>
        <year>2001</year>
        <publisher-name>John Wiley and Sons</publisher-name>
        <publisher-loc>New York</publisher-loc>
      </element-citation>
    </ref>
    <ref id="bib0026">
      <label>26</label>
      <mixed-citation publication-type="other" id="othref0019">X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, R.M. Summers, Chestx-ray8: hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition 2017 (pp. 2097–2106).</mixed-citation>
    </ref>
  </ref-list>
  <sec id="sec0013" sec-type="supplementary-material">
    <label>Appendix</label>
    <title>Supplementary materials</title>
    <p id="para0051a">
      <supplementary-material content-type="local-data" id="ecom0001">
        <media xlink:href="mmc1.xml">
          <alt-text>Image, application 1</alt-text>
        </media>
      </supplementary-material>
    </p>
  </sec>
  <fn-group>
    <fn id="sec0012" fn-type="supplementary-material">
      <p id="para0011a">Supplementary material associated with this article can be found, in the online version, at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cmpb.2020.105581" id="interref0002">doi:10.1016/j.cmpb.2020.105581</ext-link>.</p>
    </fn>
  </fn-group>
</back>
