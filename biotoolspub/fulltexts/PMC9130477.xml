<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neurol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neurol</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neurol.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neurology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-2295</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9130477</article-id>
    <article-id pub-id-type="doi">10.3389/fneur.2022.663200</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neurology</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>IE-Vnet: Deep Learning-Based Segmentation of the Inner Ear's Total Fluid Space</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Ahmadi</surname>
          <given-names>Seyed-Ahmad</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <xref rid="fn002" ref-type="author-notes">
          <sup>†</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Frei</surname>
          <given-names>Johann</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="fn002" ref-type="author-notes">
          <sup>†</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Vivar</surname>
          <given-names>Gerome</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff5" ref-type="aff">
          <sup>5</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1220679/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dieterich</surname>
          <given-names>Marianne</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff6" ref-type="aff">
          <sup>6</sup>
        </xref>
        <xref rid="aff7" ref-type="aff">
          <sup>7</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/19445/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Kirsch</surname>
          <given-names>Valerie</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff6" ref-type="aff">
          <sup>6</sup>
        </xref>
        <xref rid="c002" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/276077/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>German Center for Vertigo and Balance Disorders, University Hospital, Ludwig-Maximilians-Universität</institution>, <addr-line>Munich</addr-line>, <country>Germany</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Department of Neurology, University Hospital, Ludwig-Maximilians-Universität</institution>, <addr-line>Munich</addr-line>, <country>Germany</country></aff>
    <aff id="aff3"><sup>3</sup><institution>NVIDIA GmbH</institution>, <addr-line>Munich</addr-line>, <country>Germany</country></aff>
    <aff id="aff4"><sup>4</sup><institution>IT-Infrastructure for Translational Medical Research, University of Augsburg</institution>, <addr-line>Augsburg</addr-line>, <country>Germany</country></aff>
    <aff id="aff5"><sup>5</sup><institution>Computer Aided Medical Procedures (CAMP), Technical University of Munich (TUM)</institution>, <addr-line>Munich</addr-line>, <country>Germany</country></aff>
    <aff id="aff6"><sup>6</sup><institution>Graduate School of Systemic Neuroscience (GSN), Ludwig-Maximilians-Universität</institution>, <addr-line>Munich</addr-line>, <country>Germany</country></aff>
    <aff id="aff7"><sup>7</sup><institution>Munich Cluster for Systems Neurology (SyNergy)</institution>, <addr-line>Munich</addr-line>, <country>Germany</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Joel Alan Goebel, Washington University in St. Louis, United States</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Marc van Hoof, Maastricht University Medical Centre, Netherlands; Michael Eliezer, Hôpital Lariboisière, France</p>
      </fn>
      <corresp id="c001">*Correspondence: Seyed-Ahmad Ahmadi <email>ahmadi@cs.tum.edu</email></corresp>
      <corresp id="c002">Valerie Kirsch <email>vkirsch@med.lmu.de</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Neuro-Otology, a section of the journal Frontiers in Neurology</p>
      </fn>
      <fn fn-type="equal" id="fn002">
        <p>†These authors have contributed equally to this work</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>663200</elocation-id>
    <history>
      <date date-type="received">
        <day>02</day>
        <month>2</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>04</day>
        <month>4</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Ahmadi, Frei, Vivar, Dieterich and Kirsch.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Ahmadi, Frei, Vivar, Dieterich and Kirsch</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <sec>
        <title>Background</title>
        <p><italic>In-vivo</italic> MR-based high-resolution volumetric quantification methods of the endolymphatic hydrops (ELH) are highly dependent on a reliable segmentation of the inner ear's total fluid space (TFS). This study aimed to develop a novel open-source inner ear TFS segmentation approach using a dedicated deep learning (DL) model.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>The model was based on a V-Net architecture (IE-Vnet) and a multivariate (MR scans: T1, T2, FLAIR, SPACE) training dataset (D1, 179 consecutive patients with peripheral vestibulocochlear syndromes). Ground-truth TFS masks were generated in a semi-manual, atlas-assisted approach. IE-Vnet model segmentation performance, generalizability, and robustness to domain shift were evaluated on four heterogenous test datasets (D2-D5, <italic>n</italic> = 4 × 20 ears).</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>The IE-Vnet model predicted TFS masks with consistently high congruence to the ground-truth in all test datasets (Dice overlap coefficient: 0.9 ± 0.02, Hausdorff maximum surface distance: 0.93 ± 0.71 mm, mean surface distance: 0.022 ± 0.005 mm) without significant difference concerning side (two-sided Wilcoxon signed-rank test, <italic>p</italic>&gt;0.05), or dataset (Kruskal-Wallis test, <italic>p</italic>&gt;0.05; <italic>post-hoc</italic> Mann-Whitney U, FDR-corrected, all <italic>p</italic>&gt;0.2). Prediction took 0.2 s, and was 2,000 times faster than a state-of-the-art atlas-based segmentation method.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>IE-Vnet TFS segmentation demonstrated high accuracy, robustness toward domain shift, and rapid prediction times. Its output works seamlessly with a previously published open-source pipeline for automatic ELS segmentation. IE-Vnet could serve as a core tool for high-volume trans-institutional studies of the inner ear. Code and pre-trained models are available free and open-source under <ext-link xlink:href="https://github.com/pydsgz/IEVNet" ext-link-type="uri">https://github.com/pydsgz/IEVNet</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group>
      <kwd>MRI</kwd>
      <kwd>deep learning</kwd>
      <kwd>endolymphatic hydros</kwd>
      <kwd>endolymphatic and perilymphatic space</kwd>
      <kwd>convolutional neural network CNN</kwd>
      <kwd>VNet</kwd>
      <kwd>segmentation (image processing)</kwd>
      <kwd>inner ear imaging</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn003">
          <institution-wrap>
            <institution>Bundesministerium fÃ¼r Bildung und Forschung</institution>
            <institution-id institution-id-type="doi">10.13039/501100002347</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id award-type="contract" rid="cn003">01 EO 0901</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>Deutsche Stiftung Neurologie</institution>
            <institution-id institution-id-type="doi">10.13039/501100012686</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group>
        <funding-source id="cn002">
          <institution-wrap>
            <institution>Medizinischen FakultÃ¤t, Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen</institution>
            <institution-id institution-id-type="doi">10.13039/501100009401</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="3"/>
      <equation-count count="0"/>
      <ref-count count="90"/>
      <page-count count="16"/>
      <word-count count="11551"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p><italic>In-vivo</italic> non-invasive verification of endolymphatic hydrops (ELH) via intravenous delayed gadolinium (Gd) enhanced magnetic resonance imaging of the inner ear (iMRI) is increasingly becoming an essential standard clinical diagnostic tool to distinguish leading causes of peripheral vestibulocochlear syndromes (<xref rid="B1" ref-type="bibr">1</xref>, <xref rid="B2" ref-type="bibr">2</xref>). In this context, a fast and easily reproducible, yet more importantly, comparable and standardized quantification method of the endolymphatic space (ELS) is a prerequisite in any setting, be it clinical or research (<xref rid="B3" ref-type="bibr">3</xref>). Unfortunately, such a quantification method is not entirely available yet despite many efforts.</p>
    <p>At first glance, clinical radiology approaches offer fast and easily applicable visual semi-quantitative (SQ) ELH classifications (<xref rid="B4" ref-type="bibr">4</xref>–<xref rid="B9" ref-type="bibr">9</xref>). Nevertheless, given the plurality of visual SQ ELH classification approaches that may vary in wording, resolution (3- or 4-point ordinal scale), or evaluation level (anatomical fixpoint), and can be sensitive to human bias, published results cannot be considered inherently reproducible, comparable or standardized (<xref rid="B10" ref-type="bibr">10</xref>). Already an improvement in comparability, manual measurement of the ELS area in 2D within one MR-layer (<xref rid="B11" ref-type="bibr">11</xref>, <xref rid="B12" ref-type="bibr">12</xref>), or better yet, the entire ELS volume 3D over multiple MR-layers (<xref rid="B13" ref-type="bibr">13</xref>, <xref rid="B14" ref-type="bibr">14</xref>) remain dependent on human decisions.</p>
    <p>Similar to optimizing entire iMR sequences in use to date (<xref rid="B15" ref-type="bibr">15</xref>–<xref rid="B17" ref-type="bibr">17</xref>), automatic ELS quantification is predetermined by two methodical sticking points (<xref rid="B18" ref-type="bibr">18</xref>): The first obstacle is to distinguish between total fluid space (TFS) within the entire inner ears bony labyrinth from the surrounding petrosal bone structures (<xref rid="B19" ref-type="bibr">19</xref>–<xref rid="B21" ref-type="bibr">21</xref>). The second difficulty is distinguishing the two different fluid spaces within the TFS (<xref rid="B22" ref-type="bibr">22</xref>, <xref rid="B23" ref-type="bibr">23</xref>), namely ELS within the membranous labyrinth and the surrounding perilymphatic space (PLS) within the bony labyrinth. Current semi-automatic (<xref rid="B24" ref-type="bibr">24</xref>–<xref rid="B26" ref-type="bibr">26</xref>) or automatic (<xref rid="B27" ref-type="bibr">27</xref>, <xref rid="B28" ref-type="bibr">28</xref>) 3D ELS quantification methods have mostly concentrated on ELS differentiation within TFS.</p>
    <p>Most available 3D TFS segmentation approaches are either manual (<xref rid="B24" ref-type="bibr">24</xref>, <xref rid="B26" ref-type="bibr">26</xref>), or atlas-based (<xref rid="B29" ref-type="bibr">29</xref>, <xref rid="B30" ref-type="bibr">30</xref>). However, atlas-based segmentation uses deformable image registration that entails several challenges (<xref rid="B31" ref-type="bibr">31</xref>). On the one hand, careful parameterization and run-times between minutes to hours of computation to obtain accurate segmentation prohibit interactive analysis. Another challenge and important motivation for this study are that the thin structures of the TFS, particularly the semi-circular canals, often lead to misregistration, despite the usage of multi-resolution registration.</p>
    <p>A promising alternative tool is machine learning algorithms based on deep neural networks (DNN, or deep learning). Recently, an automated 2D measurement of hydrops ratio using a three-layer convolutional neural network (CNN) based segmentation (<xref rid="B32" ref-type="bibr">32</xref>) and a deep learning algorithm for fully automated 3D segmentation of the inner ear (<xref rid="B33" ref-type="bibr">33</xref>) were proposed. However, to the best of our knowledge, these algorithms are not accessible to the public at large.</p>
    <p>This work proposes an open-source approach for inner ear TFS segmentation based on deep learning and using a specialized V-Net architecture (IE-Vnet) that will be made available to the scientific community. The discussion includes a comprehensive comparison of the currently available deep learning algorithms for 3D volumetric inner ear segmentation. In addition, we aimed to investigate the following questions:</p>
    <list list-type="simple">
      <list-item>
        <p>(i) Is the training of the IE-Vnet on semi-manual, atlas-based pre-segmentations of inner ear TFS possible from a large cohort with comparatively little manual segmentation effort?</p>
      </list-item>
      <list-item>
        <p>(ii) Is the IE-Vnet able to generalize across domain shift differences in MRI scanner hardware and sequence settings, or patient pathology without significant loss of segmentation accuracy, given appropriate augmentation techniques during training?</p>
      </list-item>
    </list>
  </sec>
  <sec sec-type="materials and methods" id="s2">
    <title>2. Materials and Methods</title>
    <sec>
      <title>2.1. Setting and Institutional Review Board Approval</title>
      <p>This work was conducted at the interdisciplinary German Center for Vertigo and Balance Disorders (DSGZ) and the Neurology Department of the Munich University Hospital (LMU) between 2015 and 2019. This study used previously published datasets (<xref rid="B10" ref-type="bibr">10</xref>, <xref rid="B27" ref-type="bibr">27</xref>, <xref rid="B30" ref-type="bibr">30</xref>, <xref rid="B34" ref-type="bibr">34</xref>, <xref rid="B35" ref-type="bibr">35</xref>). Institutional Review Board approval was obtained before the initiation of the study (no. 094-10 and no. 641-15). All participants provided informed oral and written consent in accordance with the Declaration of Helsinki before inclusion in the study. The inclusion criterion was age between 18 and 80 years. The exclusion criteria were other (than vestibular) neurological or psychiatric disorders, as well as any MR-related contraindications (<xref rid="B36" ref-type="bibr">36</xref>), poor image quality, or missing MR sequences.</p>
    </sec>
    <sec>
      <title>2.2. Datasets and Cohorts</title>
      <p>The study included five different real-life datasets, denoted as D1–D5. Dataset 1 (D1, training dataset) was used to train the deep neural network model. Datasets 2–5 (D2–D5, test datasets) were used to investigate the model's out-of-sample performance due to MR scanner, MR sequence, or cohort and pathology. A detailed description of the domain differences between D1 and D2-5 is given in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Domain differences between training (D1) and test (D2-D5) datasets.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>MR scanner</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold># Channels</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>ELH</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Vestibulocochlear syndrome</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Domain difference</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">D1</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Skyra</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes/No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">D2</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Skyra</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">ELH, pathology</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">D3</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Skyra</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">D4</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Verio</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Unknown, but improbable</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Scanner, coil, site, ELH, pathology</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">D5</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Verio</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Unknown, but possible</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Scanner, coil, site</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>Test datasets with various properties were included to examine the robustness of the network's segmentation performance toward domain shift. This shift was caused either by changes in population (endolymphatic hydrops present or not, determined by an ELH grade ≥1; pathologies present or not), or by changes in the imaging hardware and sequence parameters (scanner model, number of channels in the head RF coil), or both</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <sec>
        <title>2.2.1. Training Dataset D1</title>
        <p>D1 included 358 ears of 179 consecutive patients (102 female= 56.9%; aged 19–80 years, mean age 52.2 ± 15.7 years) with peripheral vestibulocochlear syndromes that underwent iMRI for exclusion or verification of ELH (51 without ELH, 49 with unilateral ELH, 79 with bilateral ELH). Vestibulocochlear syndromes comprised Meniere's disease (<italic>n</italic> = 78), vestibular migraine (<italic>n</italic> = 69), acute unilateral vestibulopathy (<italic>n</italic> = 14), vestibular paroxysmia (<italic>n</italic> = 11), bilateral vestibulopathy (<italic>n</italic> = 5), and benign paroxysmal positional vertigo (<italic>n</italic> = 2). Patients were clinically diagnosed according to the respective international guidelines, such as the brny Society (<ext-link xlink:href="http://jvr-web.org/ICVD.html" ext-link-type="uri">www.jvr_web.org/ICVD.html</ext-link> or <ext-link xlink:href="https://www.baranysociety.nl" ext-link-type="uri">https://www.baranysociety.nl</ext-link>) when diagnosing vestibular migraine (<xref rid="B37" ref-type="bibr">37</xref>, <xref rid="B38" ref-type="bibr">38</xref>), Menires disease (<xref rid="B39" ref-type="bibr">39</xref>), vestibular paroxysmia (<xref rid="B40" ref-type="bibr">40</xref>), bilateral vestibulopathy (<xref rid="B41" ref-type="bibr">41</xref>), acute unilateral vestibulopathy/vestibular neuritis (<xref rid="B42" ref-type="bibr">42</xref>) and benign paroxysmal positional vertigo (<xref rid="B43" ref-type="bibr">43</xref>). A detailed description of the diagnostic work-up of all cohorts can be found in the <xref rid="SM1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
      </sec>
      <sec>
        <title>2.2.2. Test Dataset D2 and D3</title>
        <p>In comparison to D1, these test datasets have the same acquisition parameters (D2, D3) but differences in population (D2). <bold>D2</bold> included 20 ears of 10 consecutive Department of Neurology inpatients (7 female= 70%; aged 24–45 years, mean age 33.1 ± 6.7 years) without symptoms or underlying pathologies of the peripheral and central audio-vestibular system that underwent MRI with a contrast agent as part of their diagnostic workup and agreed to undergo iMRI sequences after 4 h without any indication of ELH. Patients were admitted into the clinic due to movement disorders (<italic>n</italic> = 3), epilepsy (<italic>n</italic> = 2), trigeminal neuralgia (<italic>n</italic> = 2), viral meningitis (<italic>n</italic> = 1), subdural hematoma (<italic>n</italic> = 1), and decompensated esophoria (<italic>n</italic> = 1). D2 underwent audio-vestibular testing confirmed the soundness of their peripheral end organs. <bold>D3</bold> included 20 ears of 10 consecutive patients (6 female= 60%; aged 20–58 years, mean age 37.8 ± 13.6 years) with peripheral vestibulocochlear syndromes that underwent iMRI for verification of ELH (7 with unilateral ELH, 3 with bilateral ELH). Pathologies comprehended patients with Meniere's disease (<italic>n</italic> = 3), vestibular migraine (<italic>n</italic> = 3), acute unilateral vestibulopathy (<italic>n</italic> = 2), vestibular paroxysmia (<italic>n</italic> = 1), and bilateral vestibulopathy (<italic>n</italic> = 1).</p>
      </sec>
      <sec>
        <title>2.2.3. Test Dataset D4 and D5</title>
        <p>In comparison to D1, these datasets differ regarding MR acquisition parameters (D4, D5) and population (D4). <bold>D4</bold> included 20 ears of 10 consecutive healthy controls (HC; 7 female= 70%; aged 25–52 years, mean age 36.6 ± 9.1 years). <bold>D5</bold> included 20 ears of 10 consecutive patients (4 female= 40%; aged 27–44 years, mean age 37.5 ± 5.6 years) with bilateral vestibulopathy. Measured MR sequences in D4 and D5 only distinguished between TFS within the entire inner ears bony labyrinth from the surrounding petrosal bone structure, but not between ELS and PLS within the TFS. The existence of an ELH cannot be excluded, but is unlikely in D4 and possible in D5.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3. MR Imaging Data Acquisition</title>
      <sec>
        <title>2.3.1. Datasets D1-3</title>
        <p>Four hours after intravenous injection of a standard dose (0.1 mmol/kg body weight) of Gadobutrol (Gadovist<sup>Ⓡ</sup>, Bayer, Leverkusen, Germany), MR imaging data was acquired in a whole-body 3 Tesla MR scanner (Magnetom Skyra, Siemens Healthcare, Erlangen, Germany) with a 20-channel head coil. Head movements were minimalized in all three axes using a head positioning system for MRI (Crania Adult 01, Pearl Technology AG, Schlieren, Switzerland). A 3D-FLAIR (fluid-attenuated inversion recovery) sequence was used to differentiate ELS from PLS within TFS, and a spin-echo 3D-SPACE (three-dimensional sampling perfection with application-optimized contrasts by using different flip angle evolutions) sequence to delineate the TFS from the surrounding bone. ELH was classified on 3D-FLAIR images as enlarged negative-signal spaces within TFS, according to a previously reported convention (<xref rid="B8" ref-type="bibr">8</xref>, <xref rid="B10" ref-type="bibr">10</xref>). The 3D-FLAIR had the following parameters: TE 134 ms, TR 6,000 ms, TI 2240 ms, FA 180°, FOV 160 × 160 <italic>mm</italic><sup>2</sup>, 36 slices, base resolution 320, averages 1, acceleration factor of 2 using a parallel imaging technique with a generalized auto-calibrating partially parallel acquisition (GRAPPA) algorithm, slice thickness 0.5 mm, 0.5 × 0.5 × 0.5 <italic>mm</italic><sup>3</sup> spatial resolution.</p>
        <p>The spin-echo 3D-SPACE sequence had the following parameters: TE 133 ms, TR 1000 ms, FA 100°, FOV 192 × 192 <italic>mm</italic><sup>2</sup>, 56 slices, base resolution 384, averages 4, acceleration factor of 2 using GRAPPA algorithm, 0.5 mm slice thickness, 0.5 × 0.5 × 0.5 <italic>mm</italic><sup>3</sup> spatial resolution. Further structural sequences included a T2-weighted sequence (TE 89 ms, TR 4,540 ms, FOV 250 × 250 <italic>mm</italic><sup>2</sup>, 42 slices, base resolution 364, averages 1, acceleration factor of 2 using GRAPPA algorithm, slice thickness 3 mm, voxel size 0.7 × 0.7 × 3 <italic>mm</italic><sup>3</sup>) and a T1-weighted magnetization-prepared rapid gradient echo (MP-RAGE) sequence with an isotropic spatial resolution of 1.0 × 1.0 × 1.0 <italic>mm</italic><sup>3</sup> (TE 4.37 ms, TR 2,100 ms, FOV 256 × 256 <italic>mm</italic><sup>2</sup>, 160 slices).</p>
      </sec>
      <sec>
        <title>2.3.2. Datasets D4-5</title>
        <p>MR imaging data were acquired in a whole-body 3.0 Tesla MR scanner (Magnetom Verio, Siemens Healthcare, Erlangen, Germany) with a 32-channel head coil. Head movements were minimalized in all three axes using a head positioning system for MRI (Crania Adult 01, Pearl Technology AG, Schlieren, Switzerland). A spin-echo 2D-SPACE sequence was used to delineate the bony labyrinth (TR 1,000 ms, TE 138 ms, FA 110°, FOV 180 × 180 <italic>mm</italic><sup>2</sup>, 60 slices, base resolution 384, averages 2, slice thickness 0.5 mm, 0.5 × 0.5 × 0.5 <italic>mm</italic><sup>3</sup> spatial resolution). Further structural sequences included a T2-weighted sequence (TE 94 ms, TR 4,000 ms, FOV 230 × 230 <italic>mm</italic><sup>2</sup>, 40 slices, base resolution 364, averages 1, acceleration factor 2 using GRAPPA algorithm, slice thickness 3 mm, voxel size 0.7 × 0.7 × 3 <italic>mm</italic><sup>3</sup>) and a T1-weighted magnetization-prepared rapid gradient echo (MP-RAGE) sequence with a field-of-view of 256 mm and an isotropic spatial resolution of 1.0 × 1.0 × 1.0 <italic>mm</italic><sup>3</sup> (TE 4.37 ms, TR 2,100 ms, 160 slices).</p>
      </sec>
    </sec>
    <sec>
      <title>2.4. Creation of Ground Truth Using Atlas-Based Segmentation</title>
      <p>The ground-truth (or gold standard) segmentation for D1-5 was created using the T2 and SPACE MRI volumes in a semi-manual process, with the assistance of automatic, atlas-based segmentation. 2D- or 3D-SPACE MRI volumes served as input to the IE-Vnet model. A flowchart of the (semi-)manual ground-truth segmentation can be viewed in <xref rid="F1" ref-type="fig">Figure 1</xref>. <xref rid="F2" ref-type="fig">Figure 2</xref> depicts an exemplary T2 volume along with a ground-truth segmentation mask. First, two custom templates and atlases were created from scratch, specifically for automated pre-segmentation of the inner ear. Then, registrations were performed using linear affine and non-linear Symmetric Normalization [SyN, (<xref rid="B44" ref-type="bibr">44</xref>)] as well as Optimal Template Building [OTB, (<xref rid="B45" ref-type="bibr">45</xref>)], which are part of the Advanced Normalization Toolkit (ANTs)<xref rid="fn0001" ref-type="fn"><sup>1</sup></xref>. Also, all subjects T1, T2 and FLAIR volumes were spatially co-aligned with the SPACE volume via intra-subject rigid registration.</p>
      <fig position="float" id="F1">
        <label>Figure 1</label>
        <caption>
          <p>Flowchart of the inner ear's auto-segmentation. Auto-segmentation of the inner ear (IE) involved data preparation and manual ground-truth annotation of the IEs total fluid space (TFS) masks in training (D1, grey shading) and test (D2-D5, white shading) datasets. First, pre-segmentations (orange boxes) were obtained in D1-D5 via a custom-built full-head template (FHT) and an inner-ear template (IET). Then, manual quality control (QC), followed by manual refinement of IE segmentations (purple boxes), trained and examined the IE-Vnet model. Finally, its predictions were validated under various forms of domain shift in the test datasets D2-D5 (cf. <xref rid="T1" ref-type="table">Table 1</xref>).</p>
        </caption>
        <graphic xlink:href="fneur-13-663200-g0001" position="float"/>
      </fig>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>Inner ear MR example case. Depiction of an exemplary SPACE volume along with its ground-truth segmentation masks. <bold>(A)</bold> Depiction of ten axial slices from the SPACE MRI sequence volume, through the right inner ear, from caudal to cranial, covering a range of 12.4 mm (~1.4 mm slice distance). <bold>(B)</bold> Like <bold>(A)</bold>, but with the manually segmented total fluid space (TFS) mask (colored in red). <bold>(C)</bold> Volume rendering of the right inner ear ROI, which serves as input to the IE-Vnet model. <bold>(D)</bold> Like <bold>(C)</bold>, but the manually segmented TFS surface overlaid in red.</p>
        </caption>
        <graphic xlink:href="fneur-13-663200-g0002" position="float"/>
      </fig>
      <p>The first atlas localized the inner ears inside full-head (FH) or limited FOV (field-of-view) MRI scans. To this end, a full-head template (FHT) was created from T2 volumes using ANTs OTB, and the inner ear structures' central location was annotated with a single landmark for each side, respectively. Finally, FHT plus annotations, i.e., the full-head atlas, were non-linearly registered to all subjects' volumes. Thus, left and right inner ears could be located in all participants's heads.</p>
      <p>The second atlas enabled automatic pre-segmentation of the inner ear. Therefore, inner ear localization landmarks were transferred from the FH T2-FLAIR scans to the narrow FOV SPACE scans. Here, inner ears were cropped using a 4 × 3 × 2 cm region-of-interest (ROI) that contained the entire inner ear structure and a sufficient margin of 5–10 mm to all sides to account for slight localization errors. Inside the ROI, SPACE voxel intensities were resampled at 0.2 mm isotropic resolution (i.e., 200 × 150 × 100 voxels). All ROI cubes were geometrically centered to the origin ([0, 0, 0]) coordinate. At the origin, right-sided inner ears were re-oriented onto the left inner ears through horizontal flipping. A single inner ear template (IET) using ANTs OTB was computed from this uni-directed set of inner ears. This template was annotated with manual segmentation of the total fluid space (TFS), first by intensity thresholding with Otsu's method (<xref rid="B46" ref-type="bibr">46</xref>), followed by manual refinement with various 3D mask editing tools “Segment Editor Module”, mainly 3D brush, eraser, and scissor tool in 3D Slicer <xref rid="fn0002" ref-type="fn"><sup>2</sup></xref> (<xref rid="B47" ref-type="bibr">47</xref>).</p>
      <p>All inner ears in training (D1) and testing (D2-5) were pre-segmented using two atlas registrations; first, an inner ear localization with the FHT, followed by TFS segmentation with the IET. Then, an automatic refinement step was performed post-registration by intersecting an Otsu-thresholded mask with a 0.5 mm dilated atlas mask to account for patient-wise shape- and intensity- variations. Despite this automatic refinement, every automatic segmentation needed to be quality-controlled (QC) and corrected for mistakes in an additional manual process. Two different QC and correction strategies were implemented in the training dataset (D1) and test datasets (D2–D5) to balance the amount of manual annotation effort and the TFS masks criticality. The automatic segmentation underwent a visual QC check in each of the 358 training inner ears (D1). Inner ear localization worked very robustly, without any inner ears being missed or mislocalized. In contrast, the atlas-based segmentation was not as robust, with severe mis-segmentations (e.g., partially incomplete or entirely missed semi-circular canals or cochlear turns) in 64 out of 358 training inner ear ROIs (17.9%). These were manually refined before network training, while the remaining 302 inner ears were used for training, even if minor visual errors in the atlas auto-segmentations were present. In contrast, atlas-segmentation in the test datasets (D2–D5) was not only visually inspected, but all 80 inner ears were thoroughly error-corrected and manually refined with the aforementioned 3D Slicer mask editing tools. Manual refinement of a single inner ear, for an experienced annotator familiar with the 3D Slicer user interface, took on the order of 5–15 min.</p>
      <p>The pre-processing steps necessary for inner ear segmentation in new MRI volumes are limited to localizing the left and right inner ear. This can be achieved automatically using a full-head registration (performed in this work) and requires no manual interaction. Alternatively, the inner ears can also be manually localized using landmark annotation. Depending on the workstation hardware and registration parametrization, a fully automatic inner ear ROI localization can be performed in 1–2 min. However, a manual localization is much faster and requires two clicks, which can be performed in seconds.</p>
    </sec>
    <sec>
      <title>2.5. IE-Vnet Neural Network Architecture and Training</title>
      <sec>
        <title>2.5.1. Architecture and Loss Function</title>
        <p>The deep learning architectures for volumetric 3D segmentation were based on a V-Net model (<xref rid="B48" ref-type="bibr">48</xref>), which is a variant of the 3D U-Net family of architectures (<xref rid="B49" ref-type="bibr">49</xref>). The basic idea of these fully convolutional architectures is to extract hierarchical image features using learnable convolutional filters at an increasingly coarse resolution and image representation. The down-sampling and up-sampling operations are achieved via pooling/un-pooling operations (<xref rid="B49" ref-type="bibr">49</xref>) or forward/transpose convolutions (<xref rid="B48" ref-type="bibr">48</xref>). In this work, the network was designed as a variant of a V-Net architecture, with four down-sampling levels, with [16, 32, 64, 128, 256] 3D-convolutional filters at each level (kernel size: 3 × 3 × 3 voxels), and with residual blocks spreading two convolutional layers each within each level. Each convolutional layer is followed by Instance Normalization (<xref rid="B50" ref-type="bibr">50</xref>), channel-wise random dropout (<italic>p</italic> = 0.5), and non-linear activation with Parametrized Rectified Linear Units (PReLU) (<xref rid="B51" ref-type="bibr">51</xref>). The loss function used for training was the Dice loss (<xref rid="B48" ref-type="bibr">48</xref>). The recently published cross-institutional and open-source deep learning framework “Medical Open Network for AI” (MONAI) (<xref rid="B52" ref-type="bibr">52</xref>)<xref rid="fn0003" ref-type="fn"><sup>3</sup></xref> was used to implement the network, pre-processing, augmentation and optimization. <xref rid="F3" ref-type="fig">Figure 3</xref> visualizes the architecture.</p>
        <fig position="float" id="F3">
          <label>Figure 3</label>
          <caption>
            <p>Schematic representation of the IE-Vnet architecture. The IE-Vnet architecture is designed as a variant of a V-Net architecture with four down-sampling levels, [16, 32, 64, 128, 256] 3D-convolutional filters at each level (kernel size: 3 × 3 × 3 voxels), and with residual blocks spreading two convolutional layers each within each level. Each convolutional layer is followed by Instance Normalization (<xref rid="B50" ref-type="bibr">50</xref>), channel-wise random dropout (<italic>p</italic> = 0.5), and non-linear activation with Parametrized Rectified Linear Units (PReLU) (<xref rid="B51" ref-type="bibr">51</xref>). The convolutions aim to extract features from the data and, at the end of each stage, reduce its resolution by using appropriate stride. The left part of the network consists of a compression path, while the right part decompresses the signal until its original size is reached. A more detailed description can be found in Milletari et al. (<xref rid="B48" ref-type="bibr">48</xref>).</p>
          </caption>
          <graphic xlink:href="fneur-13-663200-g0003" position="float"/>
        </fig>
      </sec>
      <sec>
        <title>2.5.2. Pre-processing and Augmentation Scheme</title>
        <p>All volumes in D1–D5 were pre-processed with simple spatial padding to a volume size [208, 160, 112], and intensity scaling to the range [0…1]. The dataset D1 was split into 90% training data (<italic>N</italic> = 161 subjects, 322 inner ears) and 10% validation data (<italic>N</italic> = 18 subjects, 36 inner ears). Random image augmentation was used to enlarge the training set size artificially, since fully convolutional segmentation networks require large amounts of training data for robust and accurate prediction. Augmentation steps included random contrast adjustment (gamma range: [0.3, …1.5], probability of occurrence <italic>p</italic><sub><italic>o</italic></sub> = 0.9), addition of random Gaussian noise (μ = 0, σ = 1.0, <italic>p</italic><sub><italic>o</italic></sub> = 0.5), random horizontal flipping (<italic>p</italic><sub><italic>o</italic></sub> = 0.5), and random affine-elastic transformation (<italic>p</italic><sub><italic>o</italic></sub> = 0.75; 3D translation: 15% of ROI dimensions; 3D rotation: 20°; scaling: ±15%; grid deformation: magnitude range [5…100], sigma range: [5…8]).</p>
      </sec>
      <sec>
        <title>2.5.3. Optimization</title>
        <p>Adam stochastic optimization algorithm (<xref rid="B53" ref-type="bibr">53</xref>) at a learning rate of 3<italic>e</italic>−4 was used to train the network weights.</p>
      </sec>
    </sec>
    <sec>
      <title>2.6. Validation Parameters</title>
      <p>Segmentation accuracy was quantified using spatial overlap indexes, such as Dice overlap coefficient (<xref rid="B54" ref-type="bibr">54</xref>, <xref rid="B55" ref-type="bibr">55</xref>), Hausdorff distance (<xref rid="B56" ref-type="bibr">56</xref>, <xref rid="B57" ref-type="bibr">57</xref>), and mean surface distance (<xref rid="B58" ref-type="bibr">58</xref>).</p>
      <p>Localized performance issues within the inner ear were visually assessed using a semi-quantitative five-point Likert-type response scale (<xref rid="B59" ref-type="bibr">59</xref>, <xref rid="B60" ref-type="bibr">60</xref>). Therefore, the level of agreement in the segmentation outcome of the cochlea, sacculus, utriculus, the anterior semi-circular canal (aSCC), posterior SCC (pSCC), and horizontal SCC (hSCC), respectively, were quantified using the following categories: 5-Strongly agree (no structure missing, no false-positive segmentation, clean contour), 4-Agree (no structure missing, no false-positive segmentation, ≤ 1 unclean contour), 3-Neither agree nor disagree, (no structure missing, ≤ 1 false-positive segmentation, &gt; 1 unclean contour), 2-Disagree ( ≤ 1 missing structure, &gt; 1 false-positive segmentation, clean or unclean contour), and 1-Strongly disagree (&gt; 1 missing structure, &gt; 1 false-positive segmentation, clean or unclean contour).</p>
    </sec>
    <sec>
      <title>2.7. Statistical Testing</title>
      <p>Normal distribution of Dice overlap measures across datasets was determined using Shapiro and Wilk testing (<xref rid="B61" ref-type="bibr">61</xref>) and homoskedastic across datasets was determined using Bartlett and Fowler testing (<xref rid="B62" ref-type="bibr">62</xref>) before statistical analysis. Consequently non-parametric testing was further applied. Given their ordinal nature (<xref rid="B63" ref-type="bibr">63</xref>), non-parametric testing was also applied to the Likert-type expert ratings.</p>
      <p>Statistical hypothesis tests were then performed to investigate two questions: First, the sidedness of the network was checked, i.e., whether there was a statistically significant difference in segmentation accuracy (Dice overlap coefficients, Likert ratings) between left and right inner ears. To this end, a non-parametric Wilcoxon signed-rank test was applied to the Dice, and Likert outcomes, paired between the left and right inner ears of each test subject. Second, the null-hypothesis was verified, i.e., that the Dice overlap median outcomes of the four test datasets D2-D5 were equal. The purpose of this was to investigate the generalization capability of the network, i.e., whether a shift in population or imaging parameters or both (cf. <xref rid="T1" ref-type="table">Table 1</xref>) led to a measurable deterioration of segmentation performance. To this end, a non-parametric Kruskal-Wallis test for independent samples was employed with the concatenated left and right Dice and Likert outcomes as the dependent variable and the test set indicator (D2–D5) as the independent variable. <italic>Post-hoc</italic>, a non-parametric tests [Mann-Whitney U (<xref rid="B64" ref-type="bibr">64</xref>)] between Dice and Likert outcomes was performed in all pairs of test datasets D2–D5. All statistical analyses were applied using the open-source libraries Scipy Stats (<xref rid="B65" ref-type="bibr">65</xref>), Statsmodels (<xref rid="B66" ref-type="bibr">66</xref>), and Pingouin (<xref rid="B67" ref-type="bibr">67</xref>). Values are presented as means ± standard deviations.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>3. Results</title>
    <p>Results are presented separately for the training and testing stage, followed by statistical comparisons.</p>
    <sec>
      <title>3.1. Training Results</title>
      <p><xref rid="F4" ref-type="fig">Figure 4</xref> shows the evolution of Dice loss for model training and the corresponding Dice metric on the withheld validation set. The maximum validation Dice overlap metric of 0.944 was obtained at epoch 113, and this best-performing model was saved for forwarding inference on the withheld test datasets D2-D5 (cf. Sections 3.2, 3.3), as well as for open-source dissemination. Notably, the loss curve showed a steady convergence toward the minimum obtained at the final iteration. Simultaneously, the validation metric showed a steady convergence without any signs of overfitting throughout the entire optimization procedure. The total training time took around 11 h on a consumer-level workstation (AMD Ryzen Threadripper 1950X 8-core CPU, 32 GB RAM, Nvidia 1080 Ti GPU).</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>IE-Vnet training loss and validation metrics. IE-Vnet training loss <bold>(left graphic)</bold> and validation <bold>(right graphic)</bold> metrics were observed during 120 epochs of training. In addition, the maximum validation Dice overlap metric of 0.944 was obtained at epoch 113. Notably, the loss curve showed a steady convergence toward the minimum obtained at the final iteration. At the same time, the validation metric showed a steady convergence without any signs of overfitting throughout the entire optimization procedure.</p>
        </caption>
        <graphic xlink:href="fneur-13-663200-g0004" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.2. Test Results</title>
      <p>The total inference time for 80 samples was 15.2 s, i.e., on average 0.19 s ± 0.047 s for each cropped and up-sampled inner ear volume at 0.2 mm isotropic resolution (i.e., 200 × 150 × 100 voxels). The agreement of TFS segmentation between manual ground-truth and the networks prediction was quantified by three metrics: Dice overlap coefficient “Dice”, maximum Hausdorff surface distance “HDmax”, mean surface distance “SDmean”, along with five-point Likert-type response scale “LS”. These metrics are illustrated with boxplots in <xref rid="F5" ref-type="fig">Figure 5</xref>, and summarized numerically in <xref rid="T2" ref-type="table">Table 2</xref>.</p>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>IE-Vnet segmentation quality control. Alignment between ground-truth and prediction in the four test datasets D2-D5 (20 IE each, 80 IE altogether) was measured by the quantitative metrics of Dice overlap coefficient <bold>(upper row)</bold>, Hausdorff maximum surface distance <bold>(middle row)</bold>, and average surface distance <bold>(lower row)</bold>. Results of left ears (blue) are depicted on the left (L), while results of the right ears (black) are shown on the right (R). In most cases across D2-5, congruence between model prediction and manual ground-truth was high.</p>
        </caption>
        <graphic xlink:href="fneur-13-663200-g0005" position="float"/>
      </fig>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>IE-Vnet segmentation results on four test datasets D2-D5 (20 inner ears each) compared to manual ground-truth.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Dataset</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>D2</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>D3</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>D4</bold>
              </th>
              <th valign="top" align="center" colspan="2" style="border-bottom: thin solid #000000;" rowspan="1">
                <bold>D5</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Mean</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Mean</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Mean</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>SD</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Mean</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>SD</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" colspan="9" rowspan="1">
                <bold>(A) Accuracy</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Dice</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.906</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.012</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.895</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.021</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.904</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.014</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.894</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.026</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">HDmax</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.949</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.862</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.804</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.476</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1.170</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.774</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.811</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.565</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">SDmean</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.020</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.003</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.023</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.005</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.021</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.003</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.023</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.006</td>
            </tr>
            <tr>
              <td valign="top" align="left" colspan="9" rowspan="1">
                <bold>(B) Performance</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Cochlea</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.950</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.224</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.900</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.308</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.900</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.308</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.950</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.224</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sacculus</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.950</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.224</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.000</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Utriculus</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5.000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.000</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">aSCC</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.800</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.616</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.800</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.616</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.900</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.308</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.750</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.716</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">pSCC</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.900</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.447</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.900</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.447</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.850</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.671</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.900</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.308</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">hSCC</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.950</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.224</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.850</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.671</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.850</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.671</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4.800</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.696</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>Segmentation accuracy (A) was measured by the quantitative metrics of Dice overlap coefficient (“Dice”), Hausdorff maximum surface distance (“HDmax,” in [mm]), and average surface distance (“SDmean,” in [mm]). Localized performance (B) issues within the inner ear were assessed using a semi-quantitative five-point Likert-type response scale for the cochlea, sacculus, utriculus, the anterior semi-circular canal (aSCC), posterior SCC (pSCC), and horizontal SCC (hSCC) respectively. A detailed description of the used categories can be found in Section 2.6</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>Several points are noteworthy. On average, across all left and right inner ears and in all four test datasets, the Dice overlap coefficient showed a mean value of 0.900 ± 0.020, the Hausdorff maximum surface distance a mean value of 0.93 ± 0.71 mm), and the mean surface distance a mean value of 0.022 ± 0.005 mm). Thus, the segmentation performance seems quantitatively consistent across the test datasets D2–D5 (cf. <xref rid="F5" ref-type="fig">Figure 5</xref> and <xref rid="T2" ref-type="table">Table 2A</xref>), which was further confirmed by statistical analyses (cf. Section 3.3). The mean Likert scales of the inner ear structures were altogether consistently high (4.913 ± 0.337) across both inner ears and in all four test datasets. However, depending on the location, shape and intricacy of the separate inner ear structures, Likert scores consistently differed in performance success (cf. <xref rid="T2" ref-type="table">Table 2B</xref>) with the most robust results in the vestibulum (sacculus: 4.988 ± 0.112, utriculus: 5.000 ± 0.000), intermediate results in cochlea (4.925 ± 0.265) and posterior SCC (4.888 ± 0.477), and least robust results in the anterior (4.813 ± 0.576) and horizontal SCC (4.863 ± 0.590). The mentioned pattern can be verified in the several outliers, in particular in the Hausdorff distance values in both right and left inner ears. Two cases with outlier Hausdorff distances on the order on 3 mm and above are presented in <xref rid="F6" ref-type="fig">Figures 6C,D</xref>. Visual inspection reveals that these comparatively high surface errors stem either from challenging cases, which were also difficult in manual ground truth segmentation in the horizontal and posterior SCC (panel D), or minor prediction artifacts in the anterior SCC such as isolated blobs, rather than gross mis-segmentations (panel C). Such artifacts could be filtered away through minor post-processing like connected-components filters. In most cases, it is noteworthy that surface congruence between model prediction and manual ground truth was very high, with mean surface distances on the order on 0.02 mm, and with very few cases of surface distances above 0.03 mm. This is also reflected in the form of visual agreement between ground truth and prediction, as visible in two cases in <xref rid="F6" ref-type="fig">Figures 6A,B</xref>.</p>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>IE-Vnet 3D segmentation predictions visualized. Visualization of 3D segmentations predicted by the IE-Vnet model (color: cyan), overlaid onto manual ground-truth 3D surface contours (color: red). <bold>(A)</bold> Exemplary case of a highly accurate segmentation Dice &gt;0.92, Hausdorff maximum surface distance &lt;0.5 mm, mean surface distance &lt;0.02 mm, 5/5 Likert scale (LS) for cochlea, sacculus, utriculus, anterior, posterior and horizontal SCC. Manual ground-truth and prediction are in high agreement over the entire surface. <bold>(B)</bold> Another example of a highly accurate segmentation. The arrow denotes a location with hypo-intense semi-circular canal voxels, as marked manually in red, and the model prediction predicts a thinning at the same location. LS was 5/5 for cochlea, sacculus, utriculus, anterior, posterior and horizontal SCC. <bold>(C)</bold> An exemplary case of an overall accurate segmentation, but with a high Hausdorff surface distance of &gt;4 mm. LS was 5/5 for cochlea, sacculus, utriculus, posterior and horizontal SCC, and 2/5 for anterior SCC. Visual inspection reveals two isolated blobs, which could be removed with simple post-processing, such as connected components analysis and removal of small islands. <bold>(D)</bold> A failure case. Notably, manual ground-truth creation in this sample was challenging as well. LS was 5/5 for utriculus and anterior SCC, 4/5 for cochlea and sacculus, 2/5 for horizontal SCC, and 1/5 for posterior SCC. Low-contrast and high noise in SPACE sequences, with further potential challenges like motion artifacts, can lead to irregular ground-truth contours (left arrow) or unusually thin semi-circular canal segments (middle and right arrow). In such cases, model predictions may result in in-contiguous semi-circular canals and noisy surface contours.</p>
        </caption>
        <graphic xlink:href="fneur-13-663200-g0006" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.3. Impact of Side and Domain Shift</title>
      <p>We investigated whether the IE-Vnet segmentation model is affected by a side bias and whether its segmentation performance is affected by variance of the population or imaging parameters (cf. <xref rid="T1" ref-type="table">Table 1</xref>). The first test was performed in a paired manner between Dice overlap measures in the left and right inner ears for all 40 test subjects (D2–D5). This analysis yielded no significant difference between sides (two-sided Wilcoxon signed-rank test: <italic>p</italic> = 0.061; normality rejected, Shapiro-Wilk test: <italic>p</italic> &lt; 0.001). Further, we examined whether differences in image acquisition led to domain shifts across the four test datasets that impacted our model's segmentation performance. This test yielded no significant difference between Dice overlap outcomes across datasets D2–D5 (Kruskal-Wallis test: <italic>p</italic> = 0.146; homoscedasticity rejected, Bartlett test: <italic>p</italic> &lt; 0.01). Further, the pair-wise <italic>post-hoc</italic> tests between datasets D2-D5 yielded no significant differences in Dice overlaps (Mann-Whitney U, all BH-FDR corrected <italic>p</italic>-values at <italic>p</italic>&gt;0.20). Equivalent results were obtained for qualitative expert ratings of segmentation results upon visual inspection. No significant differences in Likert scale ratings were found across any of the rated regions (cochlea, sacculus, utriculus, anterior, posterior, and horizontal SCC), neither between sides (Wilcoxon signed-rank test, all <italic>p</italic>-values above <italic>p</italic> = 0.162), nor between group-medians across D2–D5 (Kruskal-Wallis test: all <italic>p</italic>-values above <italic>p</italic> = 0.392), nor pair-wise across D2–D5 (<italic>post-hoc</italic>, Mann-Whitney U, all BH-FDR corrected <italic>p</italic>-values at <italic>p</italic>&gt;0.860).</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>The current work proposes a novel inner ear TFS segmentation approach using a dedicated deep learning (DL) model based on a V-Net architecture (IE-Vnet). A variant of a V-Net deep convolutional neural network architecture was trained to perform segmentation inference on inner ear volumes. During training, various image augmentation techniques were used to account for expected variations in out-of-sample datasets, such as image contrast and intensity, noise, or affine/deformable distortions of geometry. The training dataset was constructed through atlas-based pre-segmentations with comparatively minor manual correction and segmentation effort (aim i). As a result, the inferred IE-Vnet segmentations on four testing datasets were free from side bias and robust to various domain shift sources, such as MRI scanner hardware and sequences and patient pathology (aim ii). Compared to atlas-based segmentation, the novel model was roughly 2,000 times faster and managed to avoid gross mis-segmentations in more than 20% of test cases, especially in high-volume datasets. In the following, IE-Vnet, compared to currently available neural network algorithms used for MR inner ear segmentation, its technical and clinical implications, methodical limitations, and future work will be discussed.</p>
    <sec>
      <title>4.1. Technical Implications</title>
      <sec>
        <title>4.1.1. Accuracy of Segmentation</title>
        <p>The average Dice values during testing (0.900) are noticeably lower when compared to training (0.944). This effect can be attributed to the fact that the TFS ground-truth regions were manually refined in every test sample with considerably more effort than the training set. Nevertheless, these indicate accurate segmentation (<xref rid="B31" ref-type="bibr">31</xref>), especially in structures like the semi-circular canals, where the Dice metric is known to degrade quickly for small regions or regions with fine-grained protrusions (<xref rid="B68" ref-type="bibr">68</xref>). Furthermore, the overall low surface distance of 0.02 mm can be attributed to the fact that the volumes were bi-cubically up-sampled to a resolution of 0.2 mm before inference. Therefore, the manual ground-truths predicted outer surfaces are smooth and consistent even in the presence of fine-grained details.</p>
      </sec>
      <sec>
        <title>4.1.2. Generalization</title>
        <p>The validation metric showed a steady convergence without any signs of overfitting throughout the entire optimization procedure that points to a well-parameterized network and data augmentation scheme. Furthermore, the results from our statistical analyses on Dice overlap in both inner ears imply the models freedom from side bias. Further, Dice overlap comparisons (group- and pair-wise) across the four testing datasets show no measurable difference in segmentation performance, indicating that the trained network is robust to variations in scanner hardware, image sequence parameters, and population characteristics. When discussing generalization, it is important to also consider whether quantitative metrics are sufficient to obtain trustworthy and interpretable results. A recent study (<xref rid="B69" ref-type="bibr">69</xref>) on chest X-ray classification for computer-aided diagnosis of COVID-19 cases has shown that it is vital to incorporate expert validation into the validation of results. Otherwise, it is possible that AI models learn to classify disease statuses based on confounding factors, rather than based on true pathology image content. In particular, image segmentation suffers less from the danger of spurious correlations than image classification: the segmentation output can be overlaid with the source image, and the model predictions become inherently interpretable. However, apart from quantitative metrics like Dice overlap score, or Hausdorff surface distance, a model validation can benefit from additional, expert-based qualitative ratings of the segmentation result. Hence, a differentiated Likert scale rating for the different inner ear structures (cochlea, sacculus, utriculus, anterior, posterior and horizontal semi-circular canal) was incorporated and obtained further insight into the model's performance. In particular, a performance pattern became evident in which, in decreasing order, the most robust results were found in the vestibulum (sacculus, utriculus), while cochlea and posterior SCC performed moderately well. Horizontal SCC and anterior SCC were most susceptible to segmentation errors. Notably, the lack of statistically significant differences in Likert ratings confirms that our model generalizes well. Ideally, these results should be corroborated in further prospective studies and larger cohorts.</p>
      </sec>
      <sec>
        <title>4.1.3. Inference Speed Compared to Atlas</title>
        <p>On average, the segmentation of a single volume with IE-Vnet took 0.19 ± 0.047 s, including volume loading and pre-processing, and 0.093 s for inference alone. The average segmentation time for inner ears was 377.0 ± 36.9 s using deformable registration. In total, the segmentation was about 2,000 times faster than a state-of-the-art atlas-based method. However, atlas registration is computed on the CPU, while the inference is fully GPU accelerated; hence the comparison is not entirely fair. It is worth noting that GPU-accelerated deformable registration libraries were introduced recently with speedups in the order of 10–100 times (<xref rid="B70" ref-type="bibr">70</xref>). Moreover, deep models for deformable (<xref rid="B71" ref-type="bibr">71</xref>) and diffeomorphic (<xref rid="B72" ref-type="bibr">72</xref>) image registration were recently proposed, allowing for registration times comparable to those of our model. However, deep models for registration are trained with dataset sizes in the order of a few thousand sample volumes (<xref rid="B71" ref-type="bibr">71</xref>, <xref rid="B72" ref-type="bibr">72</xref>). Furthermore, atlas-based registration was less robust than IE-Vnet segmentation, as all test dataset volumes required manual correction after atlas pre-segmentation. Hence, our IE-Vnet model was not only trained on TFS contours obtained from registration. Instead, our segmentation model learned patient-wise adaptations, including individual threshold-based refinements and entire manual corrections of atlas auto-segmentations. Patient-specific prediction of the TFS contour, along with the fast inference in the order of milliseconds, makes deep convolutional network models like IE-Vnet attractive for large-scale studies in clinical and neuroscientific imaging-based studies of the inner ear.</p>
      </sec>
      <sec>
        <title>4.1.4. Robustness Compared to Atlas</title>
        <p>Atlas-based auto-segmentation in our datasets led to severe mis-segmentations (e.g., incomplete or missing semi-circular canals or cochlear turns), which occurred in 17.9% of cases in the training dataset (D1) and 22% of all cases in the test datasets (D2-5), and almost all cases in D2-5 required minor manual corrections along the entire TFS surface. Therefore, the actual speedup is probably much higher regarding automated post-processing or manual refinement steps necessary to fix atlas segmentation failures. The exact reason for the high rate of atlas mis-segmentations is unclear. It cannot be excluded that a better parameterization of the deformable registration could improve the success ratio. As mentioned, the very thin and, at times, low-contrast semi-circular canals would remain a challenge for atlas registration.</p>
      </sec>
    </sec>
    <sec>
      <title>4.2. Comparison to Currently Available Neural Network Algorithms for MR Inner Ear Segmentation</title>
      <p>In recent years, deep learning has revolutionized medical image analysis, particularly segmentation (<xref rid="B73" ref-type="bibr">73</xref>). Among an ever-growing number of architectures and approaches proposed for volumetric segmentation, two of the most popular and successful methods (<xref rid="B74" ref-type="bibr">74</xref>, <xref rid="B75" ref-type="bibr">75</xref>) are 3D U-Net (<xref rid="B49" ref-type="bibr">49</xref>) and the previously proposed V-Net (<xref rid="B48" ref-type="bibr">48</xref>). In addition, the latest published suggestions for inner ear segmentation can also be seen in this development- whether for CT (<xref rid="B76" ref-type="bibr">76</xref>–<xref rid="B78" ref-type="bibr">78</xref>) or MRI (<xref rid="B32" ref-type="bibr">32</xref>, <xref rid="B33" ref-type="bibr">33</xref>). In the following, currently available neural network algorithms used for MR inner ear (IE) segmentation will be compared (see <xref rid="T3" ref-type="table">Table 3</xref> for an overview).</p>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Overview of MR IE deep learning segmentation algorithms in comparison.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>IE-Vnet</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>IE-Unet</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>INHEARIT</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Machine learning technique</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Deep learning</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Deep learning</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Deep learning</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Network structure</td>
              <td valign="top" align="left" rowspan="1" colspan="1">3D Vnet (<xref rid="B48" ref-type="bibr">48</xref>)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">3D Unet (<xref rid="B49" ref-type="bibr">49</xref>)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2D CNN based on VGG-19 (<xref rid="B79" ref-type="bibr">79</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Input</td>
              <td valign="top" align="left" rowspan="1" colspan="1">T2-weighted sequences</td>
              <td valign="top" align="left" rowspan="1" colspan="1">T2-weighted sequences</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Hydrops-Mi2 (<xref rid="B80" ref-type="bibr">80</xref>)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Output</td>
              <td valign="top" align="left" rowspan="1" colspan="1">3D TFS segmentation</td>
              <td valign="top" align="left" rowspan="1" colspan="1">3D TFS segmentation</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2D hydrops ratio</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Output resolution</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.2 x 0.2 x 0.2 mm<sup>3</sup></td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.45 x 0.45 x 0.45 mm<sup>3</sup></td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.5 x 0.5 mm<sup>2</sup></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>(A) Training and testing parameters</bold>
              </td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Ground truth</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Semi-manual atlas-based segmentation</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Manual segmentation</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Manual segmentation</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Training dataset</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Mono-centric (n=179)</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Mono-centric (n=944)</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Mono-centric (n=124)</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Features</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>3T, multi-scanner, multi-scale</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>1.5T, 3T, multi-vendor, multi-scale</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>3T, 1 scanner, 1 scale</italic>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Participants</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Vestibular pathologies and HC</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>IE pathologies</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>MD, VM, VN</italic>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Test dataset</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Mono-centric (n=80)</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Multi-(n=3)-centric (n=276)</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>
                  <bold>5-fold cross validation of</bold>
                </italic>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Features</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>3T, multi-scanner, multi-scale</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>1.5T, 3T, multi-vendor, multi-scale</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>
                  <bold>Training dataset</bold>
                </italic>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Participants</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Vestibular pathologies and HC</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>IE pathologies</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>see above</italic>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>(B) Model performance</bold>
              </td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Accuracy (Dice)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.90 ± 0.02</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.87 (CI 0.87-0.88)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.83 ± 0.04</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>Robustness</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>100% in test sets D2-5</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>98.3% in test centers B-D</bold>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1"><bold>n.r</bold>.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>To artifacts</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>n.a</italic>.</td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Yes</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>n.r</italic>.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>To outliers</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Yes</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Yes</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>n.r</italic>.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>To noise</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Yes</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>Yes</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>n.r</italic>.</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Speed (localization/segmentation)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">25s / 0.19 s</td>
              <td valign="top" align="left" rowspan="1" colspan="1">n.r. / 6.5 s</td>
              <td valign="top" align="left" rowspan="1" colspan="1">n.r. / within 1 s</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Ability to segment diseased IE</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Full automatization</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Manual intervention needed</td>
              <td valign="top" align="left" rowspan="1" colspan="1">IE localization</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Data preparation</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Data availability</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Model availability</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Source availability</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="TN1">
            <p><italic>In the following the current study is referred to as “IE-Vnet.” The approach of Vaidyanathan et al. (<xref rid="B33" ref-type="bibr">33</xref>) is referred to as “IE-Unet.” Cho et al. (<xref rid="B32" ref-type="bibr">32</xref>) called their approach “INHEARIT” and are referred to as such. INHEARIT offers an automatic 2D area ELH (endolymphatic hydrops) ratio segmentation customized to the needs of a clinical radiologist, while IE-Vnet and IE-Unet enable 3D volumetric TFS segmentation with broad usability. The comparison considers a) parameters of the training and testing of the models, as well as their b) performance. IE-Vnet and Unet represent a similar approach to the same problem and can be complementary. However, while Unet offers a large dataset, IE-Vnet operates at a more than twice higher resolution, and its pre-trained model and accompanying codebase will be published open-source. CI, confidence interval 95%; ELH, Endolymphatic hydrops; HC, Healthy controls; Hydrops-Mi2, HYbriD of Reversed image Of Positive endolymph signal and native image of positive perilymph Signal- Multiplied with heavily T2-weighted MR cisternography; IDL, idiopathic hearing loss; IE, inner ear; INHEARIT, INner ear Hydrops Estimation via ARtificial InTelligence; MD, Morbus Mnire; MRC, MR cisternography; n.a., not analyzed; n.r., not reported; TFS, Total fluid space; VM, Vestibular migraine; VN, Vestibular neuritis</italic>.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>To the best of our knowledge, there are two machine learning MR IE segmentation proposals to date. First, Cho et al. (<xref rid="B32" ref-type="bibr">32</xref>) developed an automated measurement of 2D cochlea and vestibulum hydrops ratio from iMRI using CNN-based segmentation. Its primary difference is its usage of 2D data and focused usability on ELH area ratios in cochlea and vestibule. This tool should prove helpful to make ELH classifications (<xref rid="B4" ref-type="bibr">4</xref>, <xref rid="B5" ref-type="bibr">5</xref>, <xref rid="B8" ref-type="bibr">8</xref>, <xref rid="B9" ref-type="bibr">9</xref>) more objective and comparable for clinical radiologists during the diagnostic assessment. For research purposes, ELH classification and 2D- or 3D- quantification methods were reliable and valuable for diagnosing endolymphatic hydrops (<xref rid="B25" ref-type="bibr">25</xref>). However, the reliability increases from ELH classification to 2D- and again to 3D-quantification methods (<xref rid="B10" ref-type="bibr">10</xref>). A model for complete 3D segmentation of TFS, including semi-circular canals (SCC), not only enables 3D volumetric analyses but gives it a substantially wider application area, e.g., IE surgical planning.</p>
      <p>Second, Vaidyanathan et al. (<xref rid="B33" ref-type="bibr">33</xref>) recently suggested a fully automated segmentation of the inner ears TFS based on deep learning similar to our current approach. There are many overlaps in methodology and application, e.g., a similar network architecture. In the following, it will be referred to as IE-Unet. Compared to IE-Vnet, IE-Unet does not need to localize the inner ears in a separate pre-processing step. On the other hand, IE-Vnet operates at a more than twice higher resolution (0.2 mm isotropic vs. 0.45 mm), which leads to smoother surface boundaries of the output segmentation and can better deal with partial volume effects due to low voxel resolution in MRI. Notably, both solutions follow a similar approach to the same problem (IE MR TFS segmentation), which highlights their relevance and value compared to the method of Cho et al., whose usability is limited to the hydrops ratio in cochlea and vestibulum. Most importantly, though, both IE-Vnet and IE-Unet are highly complementary, making both trained models highly valuable. Therefore, we are choosing to publish our pre-trained model and accompanying code for training and inference open-source replication in other centers and alleviate similar studies in the community.</p>
    </sec>
    <sec>
      <title>4.3. Clinical Implications</title>
      <p>Deep learning models for medical image analysis have reached a maturity (<xref rid="B74" ref-type="bibr">74</xref>) that makes them relevant for further clinical and research-based investigations of the inner ear in the neuro-otological and vestibular domain. Once released, the proposed inner ear TFS segmentation approach using a dedicated deep learning (DL) model based on a V-Net architecture (IE-Vnet) has the potential to become a core tool for high-volume trans-institutional studies in vestibulocochlear research, such as on the endolymphatic hydrops (ELH).</p>
      <p>IE-Vnet bridges the current gap existing for available automatic 3D ELS quantification methods. In particular, its input can be seamlessly combined with a previously published open-source pipeline for automatic iMRI ELS segmentation (<xref rid="B27" ref-type="bibr">27</xref>) via the TOMAAT module (<xref rid="B81" ref-type="bibr">81</xref>) in 3DSlicer (<xref rid="B82" ref-type="bibr">82</xref>).</p>
    </sec>
    <sec>
      <title>4.4. Limitations and Future Work</title>
      <p>There are methodical limitations in the current study that need to be considered in interpreting the data. One limitation of IE-Vnet in its current form is its reliance on a pre-localization and cropping of a cubical inner ear ROI obtained via deformable registration of the FHT and a transfer of the inner ear annotations. Their computational time was not considered in the discussion since both IE-Vnet, and the IET atlas-segmentation assume a previous localization and ROI cropping of the inner ear. The pre-processing steps are limited to localizing the left and right inner ear in the present work. This can be achieved fully automatically using a full-head registration and requires no manual interaction (other than, e.g., a post-registration visual inspection of whether the cropped ROI indeed contains the inner ear). In the current study, inner ear localization was successful for all 100% of inner ears. This can be achieved fully automatically using a full-head registration and requires no manual interaction (other than e.g., a post-registration visual inspection whether the cropped ROI indeed contains the inner ear). In this study, inner ear localization was successful for all 100% of inner ears. Given that IE-Vnet is trained to be robust toward a localization uncertainty of ~1 cm (cf. augmentations in Section 2.5.2) this registration can be parametrized at a reasonably low resolution (e.g., deformation fields at 5 mm resolution). Consequently, in our study, inner ear localization via deformable registration was comparatively fast and took 25 s for both inner ears of each subject on a commodity laptop with 4a CPU. Alternatively, the inner ears can also be manually localized using landmark annotation. Depending on the workstation hardware and registration parametrization, a fully automatic inner ear ROI localization could be performed in 1–2 min. A manual localization is much faster and requires two clicks, which can be performed in the order of seconds. However, it would be attractive to incorporate this step into the deep learning architecture itself, either via a cascaded setup of two networks (<xref rid="B83" ref-type="bibr">83</xref>), one for ROI localization and one for segmentation (IE-Vnet), or via a sliding-window inference approach (<xref rid="B84" ref-type="bibr">84</xref>). Both approaches are exciting avenues for future work. Another issue is that rare cases with strong artifacts can still lead to mis-segmentations (e.g., <xref rid="F6" ref-type="fig">Figure 6D</xref>). However, such cases are statistically rare (long-tail problem) and challenging to solve. Instead, prior knowledge of the shape and topology of the inner ears TFS could be incorporated into the regularization model, e.g., through statistical shape models (<xref rid="B85" ref-type="bibr">85</xref>, <xref rid="B86" ref-type="bibr">86</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>5. Conclusion</title>
    <p>The current work proposes a novel volumetric MR image segmentation approach for the inner ears total fluid space (TFS) using a dedicated deep learning (DL) model based on V-Net architecture (IE-Vnet). IE-Vnet demonstrated high accuracy, speedy prediction times, and robustness toward domain shifts. Furthermore, its output can be seamlessly combined with a previously published open-source pipeline for automatic iMRI ELS segmentation. Taken together, IE-Vnet has the potential to become a core tool for high-volume trans-institutional studies of the inner ear in vestibular research and will also be released as a free and open-source toolkit.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability Statement</title>
    <p>The original contributions presented in the study are included in the article's <xref rid="SM1" ref-type="supplementary-material">Supplementary Material</xref>, further inquiries can be directed to the corresponding authors.</p>
  </sec>
  <sec id="s7">
    <title>Ethics Statement</title>
    <p>The studies involving human participants were reviewed and approved by Ethics Commission of the medical faculty of the Ludwig-Maximilians-Universität, Munich, Germany. The patients/participants provided their written informed consent to participate in this study.</p>
  </sec>
  <sec id="s8">
    <title>Author Contributions</title>
    <p>S-AA and JF: conception, design of the study, analysis of the data, drafting the manuscript, and providing funding. GV: acquisition and analysis of the data. MD: conception and design of the study, drafting the manuscript, and providing funding. VK: conception, design of the study, acquisition, analysis of the data, drafting the manuscript, and providing funding. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s9">
    <title>Funding</title>
    <p>This work was partially funded by the German Foundation of Neurology (Deutsche Stiftung Neurologie, DSN), Verein zur Förderung von Wissenschaft und Forschung an der Medizinischen Fakultät der LMU (Association for the Promotion of Science and Research at the LMU Medical Faculty), and the German Federal Ministry of Education and Research (BMBF) via the German Center for Vertigo and Balance Disorders (DSGZ, Grant No. 01 EO 0901).</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>S-AA was employed by NVIDIA GmbH. The remaining authors declare that the research was conducted in the absence of any commercial orfinancial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s10">
    <title>Publisher's Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We thank B. Ertl-Wagner and O. Dietrich for insightful discussions and their unwavering support during this project, and K. Göttlinger for copy-editing the script.</p>
  </ack>
  <fn-group>
    <fn id="fn0001">
      <p><sup>1</sup>ANTs open-source code and binaries: <ext-link xlink:href="https://stnava.github.io/ANTs/" ext-link-type="uri">https://stnava.github.io/ANTs/</ext-link>.</p>
    </fn>
    <fn id="fn0002">
      <p><sup>2</sup>3D Slicer open-source code and binaries: <ext-link xlink:href="https://www.slicer.org/" ext-link-type="uri">https://www.slicer.org/</ext-link>.</p>
    </fn>
    <fn id="fn0003">
      <p><sup>3</sup>Project MONAI documentation and code: <ext-link xlink:href="https://monai.io/" ext-link-type="uri">https://monai.io/</ext-link>.</p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s11">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fneur.2022.663200/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fneur.2022.663200/full#supplementary-material</ext-link></p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="Data_Sheet_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <sec>
      <title> Datasets D1-5: Measurement of the Auditory, Semicircular Canal, and Otolith Functions</title>
      <p>Diagnostic work-up included a careful neurological (e.g., history-taking, clinical examination), and neuro-orthoptic assessment with, e.g., Frenzel goggles, fundus photography, adjustments of the subjective visual vertical (SVV), video-oculography (VOG) during caloric stimulation and head-impulse test (HIT), as well as pure tone audiometry (PTA). A tilt of the SVV is a sensitive sign of a graviceptive vestibular tone imbalance. SVV was assessed when sitting in an upright position in front of a half-spherical dome with the head fixed on a chin rest (<xref rid="B87" ref-type="bibr">87</xref>). A mean deviation of &gt; 2.5 from the true vertical was considered a pathological tilt of SVV (<xref rid="B87" ref-type="bibr">87</xref>). The impairment of vestibulo-ocular reflex (VOR) in higher frequencies was measured by using high-frame-rate VOG with EyeSeeCam ((<xref rid="B88" ref-type="bibr">88</xref>), EyeSeeTech, Munich, Germany). A median gain during head impulses &lt;0.6 (eye velocity in °/s divided by head velocity in °/s) was considered a pathological VOR (<xref rid="B89" ref-type="bibr">89</xref>). Furthermore, horizontal semicircular canal responsiveness in lower frequencies was assessed by caloric testing with VOG. This was done for both ears with 30°C cold and 44° C warm water. Vestibular paresis was defined as &gt;25% asymmetry between the right- and left-sided responses (<xref rid="B90" ref-type="bibr">90</xref>).</p>
    </sec>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Strupp</surname><given-names>M</given-names></name><name><surname>Brandt</surname><given-names>T</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name></person-group>. <source>Vertigo and Dizziness: Common Complaints. 3rd Edn</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Springer</publisher-name> (<year>2022</year>).</mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandt</surname><given-names>T</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name></person-group>. <article-title>The dizzy patient: don't forget disorders of the central vestibular system</article-title>. <source>Nat Rev Neurol</source>. (<year>2017</year>) <volume>13</volume>:<fpage>352</fpage>–<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1038/nrneurol.2017.58</pub-id><?supplied-pmid 28429801?><pub-id pub-id-type="pmid">28429801</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Pyykkö</surname><given-names>I</given-names></name><name><surname>Zou</surname><given-names>J</given-names></name><name><surname>Gürkov</surname><given-names>R</given-names></name><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Nakashima</surname><given-names>T</given-names></name></person-group>. <article-title>Imaging of temporal bone</article-title>. In: Lea J, Pothier D, editors. <source>Advances in Oto-Rhino-Laryngology, vol. 82</source>. S. Karger AG (<year>2019</year>). p. <fpage>12</fpage>–<lpage>31</lpage>. Available online at: <ext-link xlink:href="https://www.karger.com/Article/FullText/490268" ext-link-type="uri">https://www.karger.com/Article/FullText/490268</ext-link>.</mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakashima</surname><given-names>T</given-names></name><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Pyykko</surname><given-names>I</given-names></name><name><surname>Gibson</surname><given-names>WPR</given-names></name><name><surname>Sone</surname><given-names>M</given-names></name><name><surname>Nakata</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>Grading of endolymphatic hydrops using magnetic resonance imaging</article-title>. <source>Acta Otolaryngol Suppl</source>. (<year>2009</year>) <volume>560</volume>:<fpage>5</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1080/00016480902729827</pub-id><?supplied-pmid 19221900?><pub-id pub-id-type="pmid">19221900</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gürkov</surname><given-names>R</given-names></name><name><surname>Flatz</surname><given-names>W</given-names></name><name><surname>Louza</surname><given-names>J</given-names></name><name><surname>Strupp</surname><given-names>M</given-names></name><name><surname>Ertl-Wagner</surname><given-names>B</given-names></name><name><surname>Krause</surname><given-names>E</given-names></name></person-group>. <article-title>In vivo visualized endolymphatic hydrops and inner ear functions in patients with electrocochleographically confirmed Ménière's disease</article-title>. <source>Otol Neurotol</source>. (<year>2012</year>) <volume>33</volume>:<fpage>1040</fpage>–<lpage>5</lpage>. <pub-id pub-id-type="doi">10.1097/MAO.0b013e31825d9a95</pub-id><?supplied-pmid 22772006?><pub-id pub-id-type="pmid">22772006</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baráth</surname><given-names>K</given-names></name><name><surname>Schuknecht</surname><given-names>B</given-names></name><name><surname>Naldi</surname><given-names>AM</given-names></name><name><surname>Schrepfer</surname><given-names>T</given-names></name><name><surname>Bockisch</surname><given-names>CJ</given-names></name><name><surname>Hegemann</surname><given-names>SCA</given-names></name></person-group>. <article-title>Detection and grading of endolymphatic hydrops in Menière disease using MR imaging</article-title>. <source>AJNR Am J Neuroradiol</source>. (<year>2014</year>) <volume>35</volume>:<fpage>1387</fpage>–<lpage>92</lpage>. <pub-id pub-id-type="doi">10.3174/ajnr.A3856</pub-id><?supplied-pmid 24524921?><pub-id pub-id-type="pmid">24524921</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attyé</surname><given-names>A</given-names></name><name><surname>Eliezer</surname><given-names>M</given-names></name><name><surname>Boudiaf</surname><given-names>N</given-names></name><name><surname>Tropres</surname><given-names>I</given-names></name><name><surname>Chechin</surname><given-names>D</given-names></name><name><surname>Schmerber</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>MRI of endolymphatic hydrops in patients with Meniere's disease: a case-controlled study with a simplified classification based on saccular morphology</article-title>. <source>Eur Radiol</source>. (<year>2017</year>) <volume>27</volume>:<fpage>3138</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1007/s00330-016-4701-z</pub-id><?supplied-pmid 27999985?><pub-id pub-id-type="pmid">27999985</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirsch</surname><given-names>V</given-names></name><name><surname>Becker-Bense</surname><given-names>S</given-names></name><name><surname>Berman</surname><given-names>A</given-names></name><name><surname>Kierig</surname><given-names>E</given-names></name><name><surname>Ertl-Wagner</surname><given-names>B</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name></person-group>. <article-title>Transient endolymphatic hydrops after an attack of vestibular migraine: a longitudinal single case study</article-title>. <source>J Neurol</source>. (<year>2018</year>) <volume>265</volume>:<fpage>51</fpage>–<lpage>3</lpage>. <pub-id pub-id-type="doi">10.1007/s00415-018-8870-3</pub-id><?supplied-pmid 29696496?><pub-id pub-id-type="pmid">29696496</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernaerts</surname><given-names>A</given-names></name><name><surname>Vanspauwen</surname><given-names>R</given-names></name><name><surname>Blaivie</surname><given-names>C</given-names></name><name><surname>van Dinther</surname><given-names>J</given-names></name><name><surname>Zarowski</surname><given-names>A</given-names></name><name><surname>Wuyts</surname><given-names>FL</given-names></name><etal/></person-group>. <article-title>The value of four stage vestibular hydrops grading and asymmetric perilymphatic enhancement in the diagnosis of Menière's disease on MRI</article-title>. <source>Neuroradiology</source>. (<year>2019</year>) <volume>61</volume>:<fpage>421</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1007/s00234-019-02155-7</pub-id><?supplied-pmid 30719545?><pub-id pub-id-type="pmid">30719545</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boegle</surname><given-names>R</given-names></name><name><surname>Gerb</surname><given-names>J</given-names></name><name><surname>Kierig</surname><given-names>E</given-names></name><name><surname>Becker-Bense</surname><given-names>S</given-names></name><name><surname>Ertl-Wagner</surname><given-names>B</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name><etal/></person-group>. <article-title>Intravenous delayed gadolinium-enhanced MR imaging of the endolymphatic space: a methodological comparative study</article-title>. <source>Front Neurol</source>. (<year>2021</year>) <volume>12</volume>:<fpage>647296</fpage>. <pub-id pub-id-type="doi">10.3389/fneur.2021.647296</pub-id><?supplied-pmid 33967941?><pub-id pub-id-type="pmid">33967941</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Kanou</surname><given-names>M</given-names></name><name><surname>Ohashi</surname><given-names>T</given-names></name><name><surname>Kuno</surname><given-names>K</given-names></name><name><surname>Sone</surname><given-names>M</given-names></name></person-group>. <article-title>Simple estimation of the endolymphatic volume ratio after intravenous administration of a single-dose of gadolinium contrast</article-title>. <source>Magn Reson Med Sci</source>. (<year>2016</year>) <volume>15</volume>:<fpage>379</fpage>–<lpage>85</lpage>. <pub-id pub-id-type="doi">10.2463/mrms.mp.2015-0175</pub-id><?supplied-pmid 27001396?><pub-id pub-id-type="pmid">27001396</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>S</given-names></name><name><surname>Zhu</surname><given-names>H</given-names></name><name><surname>Zhu</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><etal/></person-group>. <article-title>Correlations between the degree of endolymphatic hydrops and symptoms and audiological test results in patients with menière's disease: a reevaluation</article-title>. <source>Otol Neurotol</source>. (<year>2018</year>) <volume>39</volume>:<fpage>351</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1097/MAO.0000000000001675</pub-id><?supplied-pmid 29287037?><pub-id pub-id-type="pmid">29287037</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inui</surname><given-names>H</given-names></name><name><surname>Sakamoto</surname><given-names>T</given-names></name><name><surname>Ito</surname><given-names>T</given-names></name><name><surname>Kitahara</surname><given-names>T</given-names></name></person-group>. <article-title>Volumetric measurements of the inner ear in patients with Meniere's disease using three-dimensional magnetic resonance imaging</article-title>. <source>Acta Otolaryngol</source>. (<year>2016</year>) <volume>136</volume>:<fpage>888</fpage>–<lpage>93</lpage>. <pub-id pub-id-type="doi">10.3109/00016489.2016.1168940</pub-id><?supplied-pmid 27187035?><pub-id pub-id-type="pmid">27187035</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>T</given-names></name><name><surname>Inui</surname><given-names>H</given-names></name><name><surname>Miyasaka</surname><given-names>T</given-names></name><name><surname>Shiozaki</surname><given-names>T</given-names></name><name><surname>Matsuyama</surname><given-names>S</given-names></name><name><surname>Yamanaka</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>Three-Dimensional magnetic resonance imaging reveals the relationship between the control of vertigo and decreases in endolymphatic hydrops after endolymphatic sac drainage with steroids for meniere's disease</article-title>. <source>Front Neurol</source>. (<year>2019</year>) <volume>10</volume>:<fpage>46</fpage>. <pub-id pub-id-type="doi">10.3389/fneur.2019.00046</pub-id><?supplied-pmid 30778329?><pub-id pub-id-type="pmid">30778329</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Kawai</surname><given-names>H</given-names></name><name><surname>Taoka</surname><given-names>T</given-names></name><name><surname>Sone</surname><given-names>M</given-names></name></person-group>. <article-title>Improved HYDROPS: imaging of endolymphatic hydrops after intravenous administration of gadolinium</article-title>. <source>Magn Reson Med Sci</source>. (<year>2017</year>) <volume>16</volume>:<fpage>357</fpage>–<lpage>61</lpage>. <pub-id pub-id-type="doi">10.2463/mrms.tn.2016-0126</pub-id><?supplied-pmid 29515085?><pub-id pub-id-type="pmid">28529249</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohashi</surname><given-names>T</given-names></name><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Takeuchi</surname><given-names>A</given-names></name><name><surname>Katagiri</surname><given-names>T</given-names></name><name><surname>Kuno</surname><given-names>K</given-names></name></person-group>. <article-title>Quantification of endolymphatic space volume after intravenous administration of a single dose of gadolinium-based contrast agent: 3D-real inversion recovery versus HYDROPS-Mi2</article-title>. <source>Magn Reson Med Sci</source>. (<year>2019</year>) <volume>19</volume>:<fpage>119</fpage>–<lpage>24</lpage>. <pub-id pub-id-type="doi">10.2463/mrms.mp.2019-0013</pub-id><?supplied-pmid 31061269?><pub-id pub-id-type="pmid">31061269</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Nakamichi</surname><given-names>R</given-names></name><name><surname>Ichikawa</surname><given-names>K</given-names></name><name><surname>Kawamura</surname><given-names>M</given-names></name><name><surname>Kawai</surname><given-names>H</given-names></name><name><surname>Yoshida</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>MR imaging of endolymphatic hydrops: utility of iHYDROPS-Mi2 combined with deep learning reconstruction denoising</article-title>. <source>Magn Reson Med Sci</source>. (<year>2020</year>) <volume>20</volume>:<fpage>272</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.2463/mrms.mp.2020-0082</pub-id><?supplied-pmid 32830173?><pub-id pub-id-type="pmid">32830173</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakashima</surname><given-names>T</given-names></name><name><surname>Pyykkö</surname><given-names>I</given-names></name><name><surname>Arroll</surname><given-names>MA</given-names></name><name><surname>Casselbrant</surname><given-names>ML</given-names></name><name><surname>Foster</surname><given-names>CA</given-names></name><name><surname>Manzoor</surname><given-names>NF</given-names></name><etal/></person-group>. <article-title>Meniere's disease</article-title>. <source>Nat Rev Dis Primers</source>. (<year>2016</year>) <volume>2</volume>:<fpage>16028</fpage>. <pub-id pub-id-type="doi">10.1038/nrdp.2016.28</pub-id><?supplied-pmid 27170253?><pub-id pub-id-type="pmid">27170253</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakker</surname><given-names>CJ</given-names></name><name><surname>Bhagwandien</surname><given-names>R</given-names></name><name><surname>Moerland</surname><given-names>MA</given-names></name><name><surname>Ramos</surname><given-names>LM</given-names></name></person-group>. <article-title>Simulation of susceptibility artifacts in 2D and 3D Fourier transform spin-echo and gradient-echo magnetic resonance imaging</article-title>. <source>Magn Reson Imaging</source>. (<year>1994</year>) <volume>12</volume>:<fpage>767</fpage>–<lpage>74</lpage>. <pub-id pub-id-type="doi">10.1016/0730-725X(94)92201-2</pub-id><?supplied-pmid 7934663?><pub-id pub-id-type="pmid">7934663</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Yamakawa</surname><given-names>K</given-names></name><name><surname>Fukatsu</surname><given-names>H</given-names></name><name><surname>Ishigaki</surname><given-names>T</given-names></name><name><surname>Nakashima</surname><given-names>T</given-names></name><name><surname>Sugimoto</surname><given-names>H</given-names></name><etal/></person-group>. <article-title>High-resolution T2-weighted MR imaging of the inner ear using a long echo-train-length 3D fast spin-echo sequence</article-title>. <source>Eur Radiol</source>. (<year>1996</year>) <volume>6</volume>:<fpage>369</fpage>–<lpage>74</lpage>. <pub-id pub-id-type="doi">10.1007/BF00180615</pub-id><?supplied-pmid 8798008?><pub-id pub-id-type="pmid">8798008</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>T</given-names></name><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Fukatsu</surname><given-names>H</given-names></name><name><surname>Ishiguchi</surname><given-names>T</given-names></name><name><surname>Ishigaki</surname><given-names>T</given-names></name><name><surname>Kobayashi</surname><given-names>M</given-names></name><etal/></person-group>. <article-title>High-resolution MR images of inner ear internal anatomy using a local gradient coil at 1</article-title>.5 Tesla: correlation with histological specimen. <source>Radiat Med</source>. (<year>1999</year>) <volume>17</volume>:<fpage>343</fpage>–<lpage>7</lpage>.<?supplied-pmid 10593283?><pub-id pub-id-type="pmid">10593283</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Yamazaki</surname><given-names>M</given-names></name><name><surname>Kawai</surname><given-names>H</given-names></name><name><surname>Bokura</surname><given-names>K</given-names></name><name><surname>Sone</surname><given-names>M</given-names></name><name><surname>Nakashima</surname><given-names>T</given-names></name></person-group>. <article-title>Imaging of endolymphatic and perilymphatic fluid at 3T after intratympanic administration of gadolinium-diethylene-triamine pentaacetic acid</article-title>. <source>Magn Reson Med Sci</source>. (<year>2012</year>) <volume>29</volume>:<fpage>7</fpage>. <pub-id pub-id-type="doi">10.3174/ajnr.A0894</pub-id><?supplied-pmid 18184846?><pub-id pub-id-type="pmid">18184846</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Yamazaki</surname><given-names>M</given-names></name><name><surname>Kawai</surname><given-names>H</given-names></name><name><surname>Bokura</surname><given-names>K</given-names></name><name><surname>Sone</surname><given-names>M</given-names></name><name><surname>Nakashima</surname><given-names>T</given-names></name></person-group>. <article-title>Imaging of ménière's disease after intravenous administration of single-dose gadodiamide: utility of subtraction images with different inversion time</article-title>. <source>Magn Reson Med Sci</source>. (<year>2012</year>) <volume>11</volume>:<fpage>7</fpage>. <pub-id pub-id-type="doi">10.2463/mrms.11.213</pub-id><?supplied-pmid 23037568?><pub-id pub-id-type="pmid">23037568</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gürkov</surname><given-names>R</given-names></name><name><surname>Berman</surname><given-names>A</given-names></name><name><surname>Dietrich</surname><given-names>O</given-names></name><name><surname>Flatz</surname><given-names>W</given-names></name><name><surname>Jerin</surname><given-names>C</given-names></name><name><surname>Krause</surname><given-names>E</given-names></name><etal/></person-group>. <article-title>MR volumetric assessment of endolymphatic hydrops</article-title>. <source>Eur Radiol</source>. (<year>2015</year>) <volume>25</volume>:<fpage>585</fpage>–<lpage>595</lpage>. <pub-id pub-id-type="doi">10.1007/s00330-014-3414-4</pub-id><?supplied-pmid 25319347?><pub-id pub-id-type="pmid">25319347</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Homann</surname><given-names>G</given-names></name><name><surname>Vieth</surname><given-names>V</given-names></name><name><surname>Weiss</surname><given-names>D</given-names></name><name><surname>Nikolaou</surname><given-names>K</given-names></name><name><surname>Heindel</surname><given-names>W</given-names></name><name><surname>Notohamiprodjo</surname><given-names>M</given-names></name><etal/></person-group>. <article-title>Semi-quantitative vs. volumetric determination of endolymphatic space in Menière's disease using endolymphatic hydrops 3T-HR-MRI after intravenous gadolinium injection</article-title>. <source>PLoS ONE</source>. (<year>2015</year>) <volume>10</volume>:<fpage>e0120357</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0120357</pub-id><?supplied-pmid 25768940?><pub-id pub-id-type="pmid">25768940</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirsch</surname><given-names>V</given-names></name><name><surname>Ertl-Wagner</surname><given-names>B</given-names></name><name><surname>Berman</surname><given-names>A</given-names></name><name><surname>Gerb</surname><given-names>J</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name><name><surname>Becker-Bense</surname><given-names>S</given-names></name></person-group>. <article-title>High-resolution MRI of the inner ear enables syndrome differentiation and specific treatment of cerebellar downbeat nystagmus and secondary endolymphatic hydrops in a postoperative ELST patient</article-title>. <source>J Neurol</source>. (<year>2018</year>) <volume>265</volume>:<fpage>48</fpage>–<lpage>50</lpage>. <pub-id pub-id-type="doi">10.1007/s00415-018-8858-z</pub-id><?supplied-pmid 29644399?><pub-id pub-id-type="pmid">29644399</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerb</surname><given-names>J</given-names></name><name><surname>Ahmadi</surname><given-names>SA</given-names></name><name><surname>Kierig</surname><given-names>E</given-names></name><name><surname>Ertl-Wagner</surname><given-names>B</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name><name><surname>Kirsch</surname><given-names>V</given-names></name></person-group>. <article-title>VOLT: a novel open-source pipeline for automatic segmentation of endolymphatic space in inner ear MRI</article-title>. <source>J Neurol</source>. (<year>2020</year>) <volume>267</volume>:<fpage>185</fpage>–<lpage>96</lpage>. <pub-id pub-id-type="doi">10.1007/s00415-020-10062-8</pub-id><?supplied-pmid 32666134?><pub-id pub-id-type="pmid">32666134</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>SY</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>BN</given-names></name><name><surname>Boegle</surname><given-names>R</given-names></name><name><surname>Kang</surname><given-names>JJ</given-names></name><name><surname>Lee</surname><given-names>NR</given-names></name><etal/></person-group>. <article-title>Endolymphatic hydrops in patients with vestibular migraine and concurrent Meniere's disease</article-title>. <source>Front Neurol</source>. (<year>2021</year>) <volume>12</volume>:<fpage>594481</fpage>. <pub-id pub-id-type="doi">10.3389/fneur.2021.594481</pub-id><?supplied-pmid 33776877?><pub-id pub-id-type="pmid">33776877</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadi</surname><given-names>SA</given-names></name><name><surname>Raiser</surname><given-names>TM</given-names></name><name><surname>Ru¨hl</surname><given-names>RM</given-names></name><name><surname>Flanagin</surname><given-names>VL</given-names></name><name><surname>zu Eulenburg</surname><given-names>P</given-names></name></person-group>. <article-title>IE-Map: a novel <italic>in-vivo</italic> atlas and template of the human inner ear</article-title>. <source>Sci Rep</source>. (<year>2021</year>) <volume>11</volume>:<fpage>3293</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-021-82716-0</pub-id><?supplied-pmid 33558581?><pub-id pub-id-type="pmid">33558581</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirsch</surname><given-names>V</given-names></name><name><surname>Nejatbakhshesfahani</surname><given-names>F</given-names></name><name><surname>Ahmadi</surname><given-names>SA</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name><name><surname>Ertl-Wagner</surname><given-names>B</given-names></name></person-group>. <article-title>A probabilistic atlas of the human inner ear's bony labyrinth enables reliable atlas-based segmentation of the total fluid space</article-title>. <source>J Neurol</source>. (<year>2019</year>) <volume>266</volume>:<fpage>52</fpage>–<lpage>61</lpage>. <pub-id pub-id-type="doi">10.1007/s00415-019-09488-6</pub-id><?supplied-pmid 31422454?><pub-id pub-id-type="pmid">31422454</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Ardekani</surname><given-names>BA</given-names></name><name><surname>Ashburner</surname><given-names>J</given-names></name><name><surname>Avants</surname><given-names>B</given-names></name><name><surname>Chiang</surname><given-names>MC</given-names></name><etal/></person-group>. <article-title>Evaluation of 14 nonlinear deformation algorithms applied to human brain MRI registration</article-title>. <source>Neuroimage</source>. (<year>2009</year>) <volume>46</volume>:<fpage>786</fpage>–<lpage>802</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.12.037</pub-id><?supplied-pmid 19195496?><pub-id pub-id-type="pmid">19195496</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>YS</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Park</surname><given-names>CJ</given-names></name><name><surname>Chung</surname><given-names>MJ</given-names></name><name><surname>Kim</surname><given-names>JH</given-names></name><name><surname>Kim</surname><given-names>K</given-names></name><etal/></person-group>. <article-title>Automated measurement of hydrops ratio from MRI in patients with Ménière's disease using CNN-based segmentation</article-title>. <source>Sci Rep</source>. (<year>2020</year>) <volume>10</volume>:<fpage>7003</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-020-63887-8</pub-id><?supplied-pmid 32332804?><pub-id pub-id-type="pmid">32332804</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaidyanathan</surname><given-names>A</given-names></name><name><surname>van der Lubbe</surname><given-names>MFJA</given-names></name><name><surname>Leijenaar</surname><given-names>RTH</given-names></name><name><surname>van Hoof</surname><given-names>M</given-names></name><name><surname>Zerka</surname><given-names>F</given-names></name><name><surname>Miraglio</surname><given-names>B</given-names></name><etal/></person-group>. <article-title>Deep learning for the fully automated segmentation of the inner ear on MRI</article-title>. <source>Sci Rep</source>. (<year>2021</year>) <volume>11</volume>:<fpage>2885</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-021-82289-y</pub-id><?supplied-pmid 33536451?><pub-id pub-id-type="pmid">33536451</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirsch</surname><given-names>V</given-names></name><name><surname>Keeser</surname><given-names>D</given-names></name><name><surname>Hergenroeder</surname><given-names>T</given-names></name><name><surname>Erat</surname><given-names>O</given-names></name><name><surname>Ertl-Wagner</surname><given-names>B</given-names></name><name><surname>Brandt</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>Structural and functional connectivity mapping of the vestibular circuitry from human brainstem to cortex</article-title>. <source>Brain Struct Funct</source>. (<year>2016</year>) <volume>221</volume>:<fpage>1291</fpage>–<lpage>1308</lpage>. <pub-id pub-id-type="doi">10.1007/s00429-014-0971-x</pub-id><?supplied-pmid 25552315?><pub-id pub-id-type="pmid">25552315</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirsch</surname><given-names>V</given-names></name><name><surname>Boegle</surname><given-names>R</given-names></name><name><surname>Keeser</surname><given-names>D</given-names></name><name><surname>Kierig</surname><given-names>E</given-names></name><name><surname>Ertl-Wagner</surname><given-names>B</given-names></name><name><surname>Brandt</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>Handedness-dependent functional organizational patterns within the bilateral vestibular cortical network revealed by fMRI connectivity based parcellation</article-title>. <source>Neuroimage</source>. (<year>2018</year>) <volume>178</volume>:<fpage>224</fpage>–<lpage>37</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.05.018</pub-id><?supplied-pmid 29787866?><pub-id pub-id-type="pmid">29787866</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dill</surname><given-names>T</given-names></name></person-group>. <article-title>Contraindications to magnetic resonance imaging</article-title>. <source>Heart</source>. (<year>2008</year>) <volume>94</volume>:<fpage>943</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.1136/hrt.2007.125039</pub-id><?supplied-pmid 18552230?><pub-id pub-id-type="pmid">18552230</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lempert</surname><given-names>T</given-names></name><name><surname>Olesen</surname><given-names>J</given-names></name><name><surname>Furman</surname><given-names>J</given-names></name><name><surname>Waterston</surname><given-names>J</given-names></name><name><surname>Seemungal</surname><given-names>B</given-names></name><name><surname>Carey</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>Vestibular migraine: diagnostic criteria</article-title>. <source>J Vestib Res</source>. (<year>2012</year>) <volume>22</volume>:<fpage>167</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.3233/VES-2012-0453</pub-id><?supplied-pmid 23142830?><pub-id pub-id-type="pmid">23142830</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dieterich</surname><given-names>M</given-names></name><name><surname>Obermann</surname><given-names>M</given-names></name><name><surname>Celebisoy</surname><given-names>N</given-names></name></person-group>. <article-title>Vestibular migraine: the most frequent entity of episodic vertigo</article-title>. <source>J Neurol</source>. (<year>2016</year>) <volume>263</volume>:<fpage>82</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1007/s00415-015-7905-2</pub-id><?supplied-pmid 27083888?><pub-id pub-id-type="pmid">27083888</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez-Escamez</surname><given-names>JA</given-names></name><name><surname>Carey</surname><given-names>J</given-names></name><name><surname>Chung</surname><given-names>WH</given-names></name><name><surname>Goebel</surname><given-names>JA</given-names></name><name><surname>Magnusson</surname><given-names>M</given-names></name><name><surname>Mandalà</surname><given-names>M</given-names></name><etal/></person-group>. <article-title>Diagnostic criteria for Méenièe's disease</article-title>. <source>J Vestib Res</source>. (<year>2015</year>) <volume>25</volume>:<fpage>1</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.3233/VES-150549</pub-id><?supplied-pmid 25882471?><pub-id pub-id-type="pmid">25882471</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strupp</surname><given-names>M</given-names></name><name><surname>Lopez-Escamez</surname><given-names>JA</given-names></name><name><surname>Kim</surname><given-names>JS</given-names></name><name><surname>Straumann</surname><given-names>D</given-names></name><name><surname>Jen</surname><given-names>JC</given-names></name><name><surname>Carey</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>Vestibular paroxysmia: diagnostic criteria</article-title>. <source>J Vestib Res</source>. (<year>2016</year>) <volume>26</volume>:<fpage>409</fpage>–<lpage>15</lpage>. <pub-id pub-id-type="doi">10.3233/VES-160589</pub-id><?supplied-pmid 28262641?><pub-id pub-id-type="pmid">28262641</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strupp</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>JS</given-names></name><name><surname>Murofushi</surname><given-names>T</given-names></name><name><surname>Straumann</surname><given-names>D</given-names></name><name><surname>Jen</surname><given-names>JC</given-names></name><name><surname>Rosengren</surname><given-names>SM</given-names></name><etal/></person-group>. <article-title>Bilateral vestibulopathy: diagnostic criteria consensus document of the classification committee of the bãrãny society</article-title>. <source>J Vestib Res</source>. (<year>2017</year>) <volume>27</volume>:<fpage>177</fpage>–<lpage>89</lpage>. <pub-id pub-id-type="doi">10.3233/VES-170619</pub-id><?supplied-pmid 29081426?><pub-id pub-id-type="pmid">29081426</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strupp</surname><given-names>M</given-names></name><name><surname>Brandt</surname><given-names>T</given-names></name></person-group>. <article-title>Vestibular neuritis</article-title>. <source>Seminars Neurol</source>. (<year>2009</year>) <volume>29</volume>:<fpage>509</fpage>–<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1055/s-0029-1241040</pub-id><?supplied-pmid 19834862?><pub-id pub-id-type="pmid">19834862</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Brevern</surname><given-names>M</given-names></name><name><surname>Bertholon</surname><given-names>P</given-names></name><name><surname>Brandt</surname><given-names>T</given-names></name><name><surname>Fife</surname><given-names>T</given-names></name><name><surname>Imai</surname><given-names>T</given-names></name><name><surname>Nuti</surname><given-names>D</given-names></name><etal/></person-group>. <article-title>Benign paroxysmal positional vertigo: diagnostic criteria consensus document of the committee for the classification of vestibular disorders of the bárány society</article-title>. <source>Acta Otorrinolaringol Espanola</source>. (<year>2017</year>) <volume>68</volume>:<fpage>349</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1016/j.otorri.2017.02.007</pub-id><?supplied-pmid 29056234?><pub-id pub-id-type="pmid">29056234</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Epstein</surname><given-names>CL</given-names></name><name><surname>Grossman</surname><given-names>M</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group>. <article-title>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</article-title>. <source>Med Image Anal</source>. (<year>2008</year>) <volume>12</volume>:<fpage>26</fpage>–<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id><?supplied-pmid 17659998?><pub-id pub-id-type="pmid">17659998</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Yushkevich</surname><given-names>P</given-names></name><name><surname>Pluta</surname><given-names>J</given-names></name><name><surname>Minkoff</surname><given-names>D</given-names></name><name><surname>Korczykowski</surname><given-names>M</given-names></name><name><surname>Detre</surname><given-names>J</given-names></name><etal/></person-group>. <article-title>The optimal template effect in hippocampus studies of diseased populations</article-title>. <source>Neuroimage</source>. (<year>2010</year>) <volume>49</volume>:<fpage>2457</fpage>–<lpage>66</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.09.062</pub-id><?supplied-pmid 19818860?><pub-id pub-id-type="pmid">19818860</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otsu</surname><given-names>N</given-names></name></person-group>. <article-title>A threshold selection method from gray level histograms</article-title>. <source>IEEE Trans Syst Man Cybern</source>. (<year>1979</year>) <volume>9</volume>:<fpage>62</fpage>–<lpage>6</lpage>. <pub-id pub-id-type="doi">10.1109/TSMC.1979.4310076</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <label>47.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kikinis</surname><given-names>R</given-names></name><name><surname>Pieper</surname><given-names>SD</given-names></name><name><surname>Vosburgh</surname><given-names>KG</given-names></name></person-group>. <article-title>3D Slicer: a platform for subject-specific image analysis, visualization, and clinical support</article-title>. In: Jolesz FA. editor. <source>Intraoperative Imaging and Image-Guided Therapy</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer New York</publisher-name> (<year>2014</year>). p. <fpage>277</fpage>–<lpage>89</lpage>.</mixed-citation>
    </ref>
    <ref id="B48">
      <label>48.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Milletari</surname><given-names>F</given-names></name><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Ahmadi</surname><given-names>SA</given-names></name></person-group>. <article-title>V-Net: fully convolutional neural networks for volumetric medical image segmentation</article-title>. In: <source>2016 Fourth International Conference on 3D Vision (3DV)</source>. <publisher-loc>Stanford, CA</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2016</year>). p. <fpage>565</fpage>–<lpage>71</lpage>.</mixed-citation>
    </ref>
    <ref id="B49">
      <label>49.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Çiçek</surname><given-names>O</given-names></name><name><surname>Abdulkadir</surname><given-names>A</given-names></name><name><surname>Lienkamp</surname><given-names>SS</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name></person-group>. <article-title>3D U-net: learning dense volumetric segmentation from sparse annotation</article-title>. In: Ourselin S, Joskowicz L, Sabuncu MR, Unal G, Wells W, editors. <source>Medical Image Computing and Computer-Assisted Intervention-MICCAI 2016. Vol. 9901 of Lecture Notes in Computer Science</source>. <publisher-loc>Athens, Greece</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name> (<year>2016</year>). p. <fpage>424</fpage>–<lpage>32</lpage>.</mixed-citation>
    </ref>
    <ref id="B50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulyanov</surname><given-names>D</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Lempitsky</surname><given-names>V</given-names></name></person-group>. <article-title>Instance normalization: the missing ingredient for fast stylization</article-title>. <source>arXiv:160708022 [cs]</source>. (<year>2017</year>) ArXiv: 1607.08022.</mixed-citation>
    </ref>
    <ref id="B51">
      <label>51.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group>. <article-title>Delving deep into rectifiers: surpassing human-level performance on imagenet classification</article-title>. In: <source>2015 IEEE International Conference on Computer Vision (ICCV)</source>. <publisher-loc>Santiago</publisher-loc>: <publisher-name>IEEE</publisher-name> (<year>2015</year>). p. <fpage>1026</fpage>–<lpage>34</lpage>.</mixed-citation>
    </ref>
    <ref id="B52">
      <label>52.</label>
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>N</given-names></name><name><surname>Wenqi</surname><given-names>Li</given-names></name><name><surname>Brown</surname><given-names>R</given-names></name><name><surname>Yiheng</surname></name><name><surname>Wang</surname></name><name><surname>Behrooz Gorman</surname><given-names>B</given-names></name><collab>Project-MONAI/MONAI: Medical Open Network for AI in Medicine Deep Learning in Healthcare Imaging: v0.5.3. Zenodo</collab></person-group> (<year>2021</year>). Available online at: <ext-link xlink:href="https://zenodo.org/record/4323058" ext-link-type="uri">https://zenodo.org/record/4323058</ext-link>.</mixed-citation>
    </ref>
    <ref id="B53">
      <label>53.</label>
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group>. <article-title>Adam: a method for stochastic optimization</article-title>. In: <source>3rd International Conference for Learning Representations (ICLR)</source>. San Diego, CA (<year>2015</year>). Availalble online at: <ext-link xlink:href="https://arxiv.org/abs/1412.6980" ext-link-type="uri">https://arxiv.org/abs/1412.6980</ext-link></mixed-citation>
    </ref>
    <ref id="B54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dice</surname><given-names>LR</given-names></name></person-group>. <article-title>Measures of the amount of ecologic association between species</article-title>. <source>Ecology</source>. (<year>1945</year>) <volume>26</volume>:<fpage>297</fpage>–<lpage>302</lpage>. <pub-id pub-id-type="doi">10.2307/1932409</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kh</surname><given-names>Z</given-names></name><name><surname>Sk</surname><given-names>W</given-names></name><name><surname>A</surname><given-names>B</given-names></name><name><surname>Cm</surname><given-names>T</given-names></name><name><surname>Mr</surname><given-names>K</given-names></name><name><surname>Sj</surname><given-names>H</given-names></name><etal/></person-group>. <article-title>Statistical validation of image segmentation quality based on a spatial overlap index</article-title>. <source>Acad Radiol</source>. (<year>2004</year>) <volume>11</volume>:<fpage>178</fpage>–<lpage>89</lpage>. <pub-id pub-id-type="doi">10.1016/S1076-6332(03)00671-8</pub-id><?supplied-pmid 14974593?><pub-id pub-id-type="pmid">14974593</pub-id></mixed-citation>
    </ref>
    <ref id="B56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huttenlocher</surname><given-names>DP</given-names></name><name><surname>Klanderman</surname><given-names>GA</given-names></name><name><surname>Rucklidge</surname><given-names>WA</given-names></name></person-group>. <article-title>Comparing images using the hausdorff distance</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. (<year>1993</year>) <volume>15</volume>:<fpage>850</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1109/34.232073</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taha</surname><given-names>AA</given-names></name><name><surname>Hanbury</surname><given-names>A</given-names></name></person-group>. <article-title>An efficient algorithm for calculating the exact hausdorff distance</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. (<year>2015</year>) <volume>37</volume>:<fpage>2153</fpage>–<lpage>63</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2015.2408351</pub-id><?supplied-pmid 26440258?><pub-id pub-id-type="pmid">26440258</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maurer</surname><given-names>CR</given-names></name><name><surname>Qi</surname><given-names>R</given-names></name><name><surname>Raghavan</surname><given-names>V</given-names></name></person-group>. <article-title>A linear time algorithm for computing exact Euclidean distance transforms of binary images in arbitrary dimensions</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. (<year>2003</year>) <volume>25</volume>:<fpage>265</fpage>–<lpage>70</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2003.1177156</pub-id></mixed-citation>
    </ref>
    <ref id="B59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Likert</surname><given-names>R</given-names></name></person-group>. <article-title>A technique for the measurement of attitudes</article-title>. (<year>1932</year>) <source>Arch Psychol</source>. <volume>140</volume>:<fpage>55</fpage>. <pub-id pub-id-type="doi">10.2307/297087</pub-id></mixed-citation>
    </ref>
    <ref id="B60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jebb</surname><given-names>AT</given-names></name><name><surname>Ng</surname><given-names>V</given-names></name><name><surname>Tay</surname><given-names>L</given-names></name></person-group>. <article-title>A review of key likert scale development advances: 1995–2019</article-title>. <source>Front Psychol</source>. (<year>2021</year>) <volume>12</volume>:<fpage>637547</fpage>. <pub-id pub-id-type="doi">10.3389/fpsyg.2021.637547</pub-id><?supplied-pmid 34017283?><pub-id pub-id-type="pmid">34017283</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <label>61.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shapiro</surname><given-names>SS</given-names></name><name><surname>Wilk</surname><given-names>MB</given-names></name></person-group>. <article-title>An analysis of variance test for normality (complete samples)</article-title>. <source>Biometrika</source>. (<year>1965</year>) <volume>52</volume>:<fpage>591</fpage>. <pub-id pub-id-type="doi">10.2307/2333709</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartlett</surname><given-names>MS</given-names></name><name><surname>Fowler</surname><given-names>RH</given-names></name></person-group>. <article-title>Properties of sufficiency and statistical tests</article-title>. <source>Proc R Soc Lond A Math Phys Sci</source>. (<year>1937</year>) <volume>160</volume>:<fpage>268</fpage>–<lpage>82</lpage>. <pub-id pub-id-type="doi">10.1098/rspa.1937.0109</pub-id></mixed-citation>
    </ref>
    <ref id="B63">
      <label>63.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mircioiu</surname><given-names>C</given-names></name><name><surname>Atkinson</surname><given-names>J</given-names></name></person-group>. <article-title>A comparison of parametric and non-parametric methods applied to a likert scale</article-title>. <source>Pharmacy</source>. (<year>2017</year>) <volume>5</volume>:<fpage>26</fpage>. <pub-id pub-id-type="doi">10.3390/pharmacy5020026</pub-id><?supplied-pmid 28970438?><pub-id pub-id-type="pmid">28970438</pub-id></mixed-citation>
    </ref>
    <ref id="B64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>HB</given-names></name><name><surname>Whitney</surname><given-names>DR</given-names></name></person-group>. <article-title>On a test of whether one of two random variables is stochastically larger than the other</article-title>. <source>Ann Math Stat</source>. (<year>1947</year>) <volume>18</volume>:<fpage>50</fpage>–<lpage>60</lpage>. <pub-id pub-id-type="doi">10.1214/aoms/1177730491</pub-id></mixed-citation>
    </ref>
    <ref id="B65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><etal/></person-group>. <article-title>SciPy 1</article-title>.0: fundamental algorithms for scientific computing in python. <source>Nat Methods</source>. (<year>2020</year>) <volume>17</volume>:<fpage>261</fpage>–<lpage>72</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-020-0772-5</pub-id><?supplied-pmid 32094914?><pub-id pub-id-type="pmid">32015543</pub-id></mixed-citation>
    </ref>
    <ref id="B66">
      <label>66.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Seabold</surname><given-names>S</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name></person-group>. <article-title>Statsmodels: econometric and statistical modeling with python</article-title>. In: <source>9th Python in Science Conference</source>. <publisher-loc>San Diego, CA</publisher-loc> (<year>2010</year>). p. <fpage>92</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="B67">
      <label>67.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vallat</surname><given-names>R</given-names></name></person-group>. <article-title>Pingouin: statistics in Python</article-title>. <source>J Open Source Softw</source>. (<year>2018</year>) <volume>3</volume>:<fpage>1026</fpage>. <pub-id pub-id-type="doi">10.21105/joss.01026</pub-id></mixed-citation>
    </ref>
    <ref id="B68">
      <label>68.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crum</surname><given-names>WR</given-names></name><name><surname>Camara</surname><given-names>O</given-names></name><name><surname>Hill</surname><given-names>DLG</given-names></name></person-group>. <article-title>Generalized overlap measures for evaluation and validation in medical image analysis</article-title>. <source>IEEE Trans Med Imaging</source>. (<year>2006</year>) <volume>25</volume>:<fpage>1451</fpage>–<lpage>61</lpage>. <pub-id pub-id-type="doi">10.1109/TMI.2006.880587</pub-id><?supplied-pmid 17117774?><pub-id pub-id-type="pmid">17117774</pub-id></mixed-citation>
    </ref>
    <ref id="B69">
      <label>69.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeGrave</surname><given-names>AJ</given-names></name><name><surname>Janizek</surname><given-names>JD</given-names></name><name><surname>Lee</surname><given-names>SI</given-names></name></person-group>. <article-title>AI for radiographic COVID-19 detection selects shortcuts over signal</article-title>. <source>Nat Mach Intell</source>. (<year>2021</year>) <volume>3</volume>:<fpage>610</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1038/s42256-021-00338-7</pub-id><?supplied-pmid 32995822?><pub-id pub-id-type="pmid">32995822</pub-id></mixed-citation>
    </ref>
    <ref id="B70">
      <label>70.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Tang</surname><given-names>X</given-names></name></person-group>. <article-title>A large deformation diffeomorphic framework for fast brain image registration via parallel computing and optimization</article-title>. <source>Neuroinformatics</source>. (<year>2020</year>) <volume>18</volume>:<fpage>251</fpage>–<lpage>66</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-019-09438-7</pub-id><?supplied-pmid 31701342?><pub-id pub-id-type="pmid">31701342</pub-id></mixed-citation>
    </ref>
    <ref id="B71">
      <label>71.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dalca</surname><given-names>AV</given-names></name><name><surname>Balakrishnan</surname><given-names>G</given-names></name><name><surname>Guttag</surname><given-names>J</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name></person-group>. <article-title>Unsupervised learning for fast probabilistic diffeomorphic registration</article-title>. In: <source>Medical Image Computing and Computer Assisted Intervention – MICCAI 2018</source>. <volume>vol. 11070</volume>. <publisher-loc>Granada</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name> (<year>2018</year>). p. <fpage>729</fpage>–<lpage>38</lpage>.<?supplied-pmid 31351389?><pub-id pub-id-type="pmid">31351389</pub-id></mixed-citation>
    </ref>
    <ref id="B72">
      <label>72.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalca</surname><given-names>AV</given-names></name><name><surname>Balakrishnan</surname><given-names>G</given-names></name><name><surname>Guttag</surname><given-names>J</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name></person-group>. <article-title>Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces</article-title>. <source>Med Image Anal</source>. (<year>2019</year>) <volume>57</volume>:<fpage>226</fpage>–<lpage>36</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2019.07.006</pub-id><?supplied-pmid 31351389?><pub-id pub-id-type="pmid">31351389</pub-id></mixed-citation>
    </ref>
    <ref id="B73">
      <label>73.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litjens</surname><given-names>G</given-names></name><name><surname>Kooi</surname><given-names>T</given-names></name><name><surname>Bejnordi</surname><given-names>BE</given-names></name><name><surname>Setio</surname><given-names>AAA</given-names></name><name><surname>Ciompi</surname><given-names>F</given-names></name><name><surname>Ghafoorian</surname><given-names>M</given-names></name><etal/></person-group>. <article-title>A survey on deep learning in medical image analysis</article-title>. <source>Med Image Anal</source>. (<year>2017</year>) <volume>42</volume>:<fpage>60</fpage>–<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1016/j.media.2017.07.005</pub-id><?supplied-pmid 33901992?><pub-id pub-id-type="pmid">28778026</pub-id></mixed-citation>
    </ref>
    <ref id="B74">
      <label>74.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hesamian</surname><given-names>MH</given-names></name><name><surname>Jia</surname><given-names>W</given-names></name><name><surname>He</surname><given-names>X</given-names></name><name><surname>Kennedy</surname><given-names>P</given-names></name></person-group>. <article-title>Deep learning techniques for medical image segmentation: achievements and challenges</article-title>. <source>J Digit Imaging</source>. (<year>2019</year>) <volume>32</volume>:<fpage>582</fpage>–<lpage>96</lpage>. <pub-id pub-id-type="doi">10.1007/s10278-019-00227-x</pub-id><?supplied-pmid 31144149?><pub-id pub-id-type="pmid">31144149</pub-id></mixed-citation>
    </ref>
    <ref id="B75">
      <label>75.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lei</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Wan</surname><given-names>Y</given-names></name><name><surname>Du</surname><given-names>X</given-names></name><name><surname>Meng</surname><given-names>H</given-names></name><name><surname>Nandi</surname><given-names>AK</given-names></name></person-group>. <article-title>Medical image segmentation using deep learning: a survey</article-title>. <source>arXiv:200913120</source>. (<year>2020</year>).<?supplied-pmid 34919522?><pub-id pub-id-type="pmid">34919522</pub-id></mixed-citation>
    </ref>
    <ref id="B76">
      <label>76.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heutink</surname><given-names>F</given-names></name></person-group>. <article-title>Multi-Scale deep learning framework for cochlea localization, segmentation and analysis on clinical ultra-high-resolution CT images</article-title>. <source>Comput Methods Progr Biomed</source>. (<year>2020</year>) <volume>191</volume>:<fpage>105387</fpage>. <pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105387</pub-id><?supplied-pmid 32109685?><pub-id pub-id-type="pmid">32109685</pub-id></mixed-citation>
    </ref>
    <ref id="B77">
      <label>77.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hussain</surname><given-names>R</given-names></name><name><surname>Lalande</surname><given-names>A</given-names></name><name><surname>Girum</surname><given-names>KB</given-names></name><name><surname>Guigou</surname><given-names>C</given-names></name><name><surname>Bozorg Grayeli</surname><given-names>A</given-names></name></person-group>. <article-title>Automatic segmentation of inner ear on CT-scan using auto-context convolutional neural network</article-title>. <source>Sci Rep</source>. (<year>2021</year>) <volume>11</volume>:<fpage>4406</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-021-83955-x</pub-id><?supplied-pmid 33623074?><pub-id pub-id-type="pmid">33623074</pub-id></mixed-citation>
    </ref>
    <ref id="B78">
      <label>78.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nikan</surname><given-names>S</given-names></name><name><surname>Osch</surname><given-names>KV</given-names></name><name><surname>Bartling</surname><given-names>M</given-names></name><name><surname>Allen</surname><given-names>DG</given-names></name><name><surname>Rohani</surname><given-names>SA</given-names></name><name><surname>Connors</surname><given-names>B</given-names></name><etal/></person-group>. <article-title>PWD-3DNet: a deep learning-based fully-automated segmentation of multiple structures on temporal bone CT scans</article-title>. <source>IEEE Trans Image Process</source>. (<year>2021</year>) <volume>30</volume>:<fpage>15</fpage>. <pub-id pub-id-type="doi">10.1109/TIP.2020.3038363</pub-id><?supplied-pmid 33226942?><pub-id pub-id-type="pmid">33035163</pub-id></mixed-citation>
    </ref>
    <ref id="B79">
      <label>79.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group>. <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. <source>arXiv:14091556 [cs]</source>. (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="B80">
      <label>80.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naganawa</surname><given-names>S</given-names></name><name><surname>Yamazaki</surname><given-names>M</given-names></name><name><surname>Kawai</surname><given-names>H</given-names></name><name><surname>Bokura</surname><given-names>K</given-names></name><name><surname>Sone</surname><given-names>M</given-names></name><name><surname>Nakashima</surname><given-names>T</given-names></name></person-group>. <article-title>Imaging of Ménière's disease after intravenous administration of single-dose gadodiamide: utility of multiplication of MR cisternography and HYDROPS image</article-title>. <source>Magn Reson Med Sci</source>. (<year>2013</year>) <volume>12</volume>:<fpage>63</fpage>–<lpage>8</lpage>. <pub-id pub-id-type="doi">10.2463/mrms.2012-0027</pub-id><?supplied-pmid 23474961?><pub-id pub-id-type="pmid">23474961</pub-id></mixed-citation>
    </ref>
    <ref id="B81">
      <label>81.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milletari</surname><given-names>F</given-names></name><name><surname>Frei</surname><given-names>J</given-names></name><name><surname>Aboulatta</surname><given-names>M</given-names></name><name><surname>Vivar</surname><given-names>G</given-names></name><name><surname>Ahmadi</surname><given-names>SA</given-names></name></person-group>. <article-title>Cloud deployment of high-resolution medical image analysis with TOMAAT</article-title>. <source>IEEE J Biomed Health Inform</source>. (<year>2019</year>) <volume>23</volume>:<fpage>969</fpage>–<lpage>77</lpage>. <pub-id pub-id-type="doi">10.1109/JBHI.2018.2885214</pub-id><?supplied-pmid 30530377?><pub-id pub-id-type="pmid">30530377</pub-id></mixed-citation>
    </ref>
    <ref id="B82">
      <label>82.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorov</surname><given-names>A</given-names></name><name><surname>Beichel</surname><given-names>R</given-names></name><name><surname>Kalpathy-Cramer</surname><given-names>J</given-names></name><name><surname>Finet</surname><given-names>J</given-names></name><name><surname>Fillion-Robin</surname><given-names>JC</given-names></name><name><surname>Pujol</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>3D slicer as an image computing platform for the quantitative imaging network</article-title>. <source>Magn Reson Imaging</source>. (<year>2012</year>). <volume>30</volume>:<fpage>1323</fpage>–<lpage>41</lpage>. <pub-id pub-id-type="doi">10.1016/j.mri.2012.05.001</pub-id><?supplied-pmid 22770690?><pub-id pub-id-type="pmid">22770690</pub-id></mixed-citation>
    </ref>
    <ref id="B83">
      <label>83.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Christ</surname><given-names>PF</given-names></name><name><surname>Elshaer</surname><given-names>MEA</given-names></name><name><surname>Ettlinger</surname><given-names>F</given-names></name><name><surname>Tatavarty</surname><given-names>S</given-names></name><name><surname>Bickel</surname><given-names>M</given-names></name><name><surname>Bilic</surname><given-names>P</given-names></name><etal/></person-group>. <article-title>Automatic liver and lesion segmentation in CT using cascaded fully convolutional neural networks and 3D conditional random fields</article-title>. In: <source>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2016. vol. 9901</source>. <publisher-loc>Athens</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name> (<year>2016</year>). p. <fpage>415</fpage>–<lpage>23</lpage>.</mixed-citation>
    </ref>
    <ref id="B84">
      <label>84.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>He</surname><given-names>K</given-names></name><name><surname>Dollar</surname><given-names>P</given-names></name></person-group>. <article-title>TensorMask: a foundation for dense object segmentation</article-title>. In: <source>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</source>. (<year>2019</year>). p. <fpage>2061</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="B85">
      <label>85.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ahmadi</surname><given-names>SA</given-names></name><name><surname>Baust</surname><given-names>M</given-names></name><name><surname>Karamalis</surname><given-names>A</given-names></name><name><surname>Plate</surname><given-names>A</given-names></name><name><surname>Boetzel</surname><given-names>K</given-names></name><name><surname>Klein</surname><given-names>T</given-names></name><etal/></person-group>. <article-title>Midbrain segmentation in transcranial 3D ultrasound for parkinson diagnosis</article-title>. In: Fichtinger G, Martel A, Peters T, editors. <source>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2011 Lecture Notes in Computer Science</source>. <publisher-loc>Berlin; Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name> (<year>2011</year>). p. <fpage>362</fpage>–<lpage>9</lpage>.<?supplied-pmid 22003720?></mixed-citation>
    </ref>
    <ref id="B86">
      <label>86.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutiãrrez-Becker</surname><given-names>B</given-names></name><name><surname>Sarasua</surname><given-names>I</given-names></name><name><surname>Wachinger</surname><given-names>C</given-names></name></person-group>. <article-title>Discriminative and generative models for anatomical shape analysis on point clouds with deep neural networks</article-title>. <source>Med Image Anal</source>. (<year>2021</year>) <volume>67</volume>:<fpage>101852</fpage>. <pub-id pub-id-type="doi">10.1016/j.media.2020.101852</pub-id><?supplied-pmid 33129154?><pub-id pub-id-type="pmid">33129154</pub-id></mixed-citation>
    </ref>
    <ref id="B87">
      <label>87.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dieterich</surname><given-names>M</given-names></name><name><surname>Brandt</surname><given-names>T</given-names></name></person-group>. <article-title>Ocular torsion and tilt of subjective visual vertical are sensitive brainstem signs</article-title>. <source>Ann Neurol</source>. (<year>1993</year>) <volume>33</volume>:<fpage>292</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1002/ana.410330311</pub-id><?supplied-pmid 8498813?><pub-id pub-id-type="pmid">8498813</pub-id></mixed-citation>
    </ref>
    <ref id="B88">
      <label>88.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>E</given-names></name><name><surname>Villgrattner</surname><given-names>T</given-names></name><name><surname>Vockeroth</surname><given-names>J</given-names></name><name><surname>Bartl</surname><given-names>K</given-names></name><name><surname>Kohlbecher</surname><given-names>S</given-names></name><name><surname>Bardins</surname><given-names>S</given-names></name><etal/></person-group>. <article-title>EyeSeeCam: an eye movement-driven head camera for the examination of natural visual exploration</article-title>. <source>Ann N Y Acad Sci</source>. (<year>2009</year>) <volume>1164</volume>:<fpage>461</fpage>–<lpage>7</lpage>. <pub-id pub-id-type="doi">10.1111/j.1749-6632.2009.03858.x</pub-id><?supplied-pmid 19645949?><pub-id pub-id-type="pmid">19645949</pub-id></mixed-citation>
    </ref>
    <ref id="B89">
      <label>89.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halmagyi</surname><given-names>GM</given-names></name><name><surname>Curthoys</surname><given-names>IS</given-names></name></person-group>. <article-title>A clinical sign of canal paresis</article-title>. <source>Arch Neurol</source>. (<year>1988</year>) <volume>45</volume>:<fpage>737</fpage>–<lpage>739</lpage>. <pub-id pub-id-type="doi">10.1001/archneur.1988.00520310043015</pub-id><?supplied-pmid 3390028?><pub-id pub-id-type="pmid">3390028</pub-id></mixed-citation>
    </ref>
    <ref id="B90">
      <label>90.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jongkees</surname><given-names>LB</given-names></name><name><surname>Maas</surname><given-names>JP</given-names></name><name><surname>Philipszoon</surname><given-names>AJ</given-names></name></person-group>. <article-title>Clinical nystagmography. A detailed study of electro-nystagmography in 341 patients with vertigo</article-title>. <source>Practica Otorhinolaryngol</source>. (<year>1962</year>) <volume>24</volume>:<fpage>65</fpage>–<lpage>93</lpage>. <pub-id pub-id-type="doi">10.1159/000274383</pub-id><?supplied-pmid 14452374?><pub-id pub-id-type="pmid">14452374</pub-id></mixed-citation>
    </ref>
  </ref-list>
  <glossary>
    <def-list>
      <title>Abbreviations</title>
      <def-item>
        <term>±</term>
        <def>
          <p>Standard deviation</p>
        </def>
      </def-item>
      <def-item>
        <term>2D</term>
        <def>
          <p>Two-dimensional</p>
        </def>
      </def-item>
      <def-item>
        <term>3D</term>
        <def>
          <p>Three-dimensional</p>
        </def>
      </def-item>
      <def-item>
        <term>ANTS</term>
        <def>
          <p>Advanced Normalization Toolkit</p>
        </def>
      </def-item>
      <def-item>
        <term>aSCC</term>
        <def>
          <p>anterior semi-circular canal</p>
        </def>
      </def-item>
      <def-item>
        <term>D1, Dataset 1</term>
        <def>
          <p>Training dataset</p>
        </def>
      </def-item>
      <def-item>
        <term>D2, Dataset 2</term>
        <def>
          <p>Test dataset</p>
        </def>
      </def-item>
      <def-item>
        <term>D3, Dataset 3</term>
        <def>
          <p>Test dataset</p>
        </def>
      </def-item>
      <def-item>
        <term>D4, Dataset 4</term>
        <def>
          <p>Test dataset</p>
        </def>
      </def-item>
      <def-item>
        <term>D5, Dataset 5</term>
        <def>
          <p>Test dataset</p>
        </def>
      </def-item>
      <def-item>
        <term>DL</term>
        <def>
          <p>Deep learning</p>
        </def>
      </def-item>
      <def-item>
        <term>ELH</term>
        <def>
          <p>Endolymphatic hydrops</p>
        </def>
      </def-item>
      <def-item>
        <term>ELS</term>
        <def>
          <p>Endolymphatic space</p>
        </def>
      </def-item>
      <def-item>
        <term>FLAIR</term>
        <def>
          <p>Fluid-attenuated inversion recovery</p>
        </def>
      </def-item>
      <def-item>
        <term>FH</term>
        <def>
          <p>Full-head</p>
        </def>
      </def-item>
      <def-item>
        <term>FHT</term>
        <def>
          <p>Full-head template</p>
        </def>
      </def-item>
      <def-item>
        <term>FOV</term>
        <def>
          <p>Field-of-view</p>
        </def>
      </def-item>
      <def-item>
        <term>GBCA</term>
        <def>
          <p>Gadolinium-based contrast agent</p>
        </def>
      </def-item>
      <def-item>
        <term>Gd</term>
        <def>
          <p>Gadolinium</p>
        </def>
      </def-item>
      <def-item>
        <term>GRAPPA</term>
        <def>
          <p>Generalized auto-calibrating partially parallel acquisition</p>
        </def>
      </def-item>
      <def-item>
        <term>HC</term>
        <def>
          <p>Healthy control</p>
        </def>
      </def-item>
      <def-item>
        <term>hSCC</term>
        <def>
          <p>horizontal semi-circular canal</p>
        </def>
      </def-item>
      <def-item>
        <term>IET</term>
        <def>
          <p>Inner ear template</p>
        </def>
      </def-item>
      <def-item>
        <term>IE-Vnet</term>
        <def>
          <p>Inner ear dedicated deep learning model based on a V-Net architecture</p>
        </def>
      </def-item>
      <def-item>
        <term>iMRI</term>
        <def>
          <p>Delayed intravenous gadolinium-enhanced MRI of the inner ear</p>
        </def>
      </def-item>
      <def-item>
        <term>iv</term>
        <def>
          <p>Intravenous</p>
        </def>
      </def-item>
      <def-item>
        <term>L</term>
        <def>
          <p>Left</p>
        </def>
      </def-item>
      <def-item>
        <term>MRI</term>
        <def>
          <p>Magnetic resonance imaging</p>
        </def>
      </def-item>
      <def-item>
        <term>n</term>
        <def>
          <p>Number</p>
        </def>
      </def-item>
      <def-item>
        <term>OTB</term>
        <def>
          <p>Optimal Template Building</p>
        </def>
      </def-item>
      <def-item>
        <term>PLS</term>
        <def>
          <p>Perlymphatic space</p>
        </def>
      </def-item>
      <def-item>
        <term>pSCC</term>
        <def>
          <p>posterior semi-circular canal</p>
        </def>
      </def-item>
      <def-item>
        <term>QC</term>
        <def>
          <p>Quality-control (led)</p>
        </def>
      </def-item>
      <def-item>
        <term>R</term>
        <def>
          <p>Right</p>
        </def>
      </def-item>
      <def-item>
        <term>ROI</term>
        <def>
          <p>Region-of-interest</p>
        </def>
      </def-item>
      <def-item>
        <term>SCC</term>
        <def>
          <p>Semi-circular canal</p>
        </def>
      </def-item>
      <def-item>
        <term>SQ</term>
        <def>
          <p>Semi-quantitative</p>
        </def>
      </def-item>
      <def-item>
        <term>SPACE</term>
        <def>
          <p>Sampling perfection with application-optimized contrasts by using different flip angle evolutions</p>
        </def>
      </def-item>
      <def-item>
        <term>SyN</term>
        <def>
          <p>Symmetric Normalization</p>
        </def>
      </def-item>
      <def-item>
        <term>TFS</term>
        <def>
          <p>Total fluid space.</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
</back>
