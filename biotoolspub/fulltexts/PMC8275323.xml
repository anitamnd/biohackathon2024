<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8275323</article-id>
    <article-id pub-id-type="pmid">34252922</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab294</article-id>
    <article-id pub-id-type="publisher-id">btab294</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Macromolecular Sequence, Structure, and Function</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>TITAN: T-cell receptor specificity prediction with bimodal attention networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Weber</surname>
          <given-names>Anna</given-names>
        </name>
        <aff><institution>IBM Research Europe</institution>, 8803 Rüschlikon, <country country="CH">Switzerland</country></aff>
        <aff><institution>ETH Zurich, Department of Biosystems Science and Engineering (D-BSSE)</institution>, 4058 Basel, <country country="CH">Switzerland</country></aff>
        <xref rid="btab294-cor1" ref-type="corresp"/>
        <!--wbr@zurich.ibm.com-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Born</surname>
          <given-names>Jannis</given-names>
        </name>
        <aff><institution>IBM Research Europe</institution>, 8803 Rüschlikon, <country country="CH">Switzerland</country></aff>
        <aff><institution>ETH Zurich, Department of Biosystems Science and Engineering (D-BSSE)</institution>, 4058 Basel, <country country="CH">Switzerland</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Rodriguez Martínez</surname>
          <given-names>María</given-names>
        </name>
        <aff><institution>IBM Research Europe</institution>, 8803 Rüschlikon, <country country="CH">Switzerland</country></aff>
        <xref rid="btab294-cor1" ref-type="corresp"/>
        <!--mrm@zurich.ibm.com-->
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btab294-cor1">To whom correspondence should be addressed. <email>wbr@zurich.ibm.com</email> and <email>mrm@zurich.ibm.com</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-07-12">
      <day>12</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <volume>37</volume>
    <issue>Suppl 1</issue>
    <issue-title>ISMB/ECCB 2021 Proceedings</issue-title>
    <fpage>i237</fpage>
    <lpage>i244</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab294.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The activity of the adaptive immune system is governed by T-cells and their specific T-cell receptors (TCR), which selectively recognize foreign antigens. Recent advances in experimental techniques have enabled sequencing of TCRs and their antigenic targets (epitopes), allowing to research the missing link between TCR sequence and epitope binding specificity. Scarcity of data and a large sequence space make this task challenging, and to date only models limited to a small set of epitopes have achieved good performance. Here, we establish a <italic toggle="yes">k</italic>-nearest-neighbor (K-NN) classifier as a strong baseline and then propose Tcr epITope bimodal Attention Networks (TITAN), a bimodal neural network that explicitly encodes both TCR sequences and epitopes to enable the independent study of generalization capabilities to unseen TCRs and/or epitopes.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>By encoding epitopes at the atomic level with SMILES sequences, we leverage transfer learning and data augmentation to enrich the input data space and boost performance. TITAN achieves high performance in the prediction of specificity of unseen TCRs (ROC-AUC 0.87 in 10-fold CV) and surpasses the results of the current state-of-the-art (ImRex) by a large margin. Notably, our Levenshtein-based K-NN classifier also exhibits competitive performance on unseen TCRs. While the generalization to unseen epitopes remains challenging, we report two major breakthroughs. First, by dissecting the attention heatmaps, we demonstrate that the sparsity of available epitope data favors an implicit treatment of epitopes as classes. This may be a general problem that limits unseen epitope performance for sufficiently complex models. Second, we show that TITAN nevertheless exhibits significantly improved performance on unseen epitopes and is capable of focusing attention on chemically meaningful molecular structures.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The code as well as the dataset used in this study is publicly available at <ext-link xlink:href="https://github.com/PaccMann/TITAN" ext-link-type="uri">https://github.com/PaccMann/TITAN</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>European Union’s Horizon 2020 research and innovation programme</institution>
          </institution-wrap>
          <!-- oupReleaseDelayRemoved from OA Article (00|0) -->
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Marie Sklodowska-Curie</institution>
          </institution-wrap>
          <!-- oupReleaseDelayRemoved from OA Article (00|0) -->
        </funding-source>
        <award-id>813545</award-id>
        <award-id>826121</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>T-cells are an integral part of the adaptive immune system, whose survival, proliferation, activation and function are all governed by the interaction of their T-cell receptor (TCR) with immunogenic peptides (epitopes) presented on major histocompatibility complex molecules (MHC). A large repertoire of T-cell receptors with different specificity is needed to provide protection against a wide range of pathogens. This repertoire is generated using stochastic gene recombination and can theoretically produce diversities of 10<sup>15</sup>–10<sup>20</sup> different receptors (<xref rid="btab294-B22" ref-type="bibr">Laydon et al., 2015</xref>) in an individual, each with unique binding capabilities. Due to this diversity, reliably predicting the binding specificity of a TCR from its sequence and understanding the mechanisms underlying TCR–pMHC interaction is highly challenging. At the same time, it has enormous potential to transform the field of immunology. A reliable prediction tool could unlock the wealth of information encoded in a patients’ TCR repertoire, which reflects their immune history and could inform about past and current infectious diseases, vaccine effectiveness or autoimmune reactions. Additionally, it could empower the application of therapeutic T-cells for cancer treatment, allowing the study of effectiveness and potential cross-reactivity risks <italic toggle="yes">in silico</italic>.</p>
    <p>Recent advances in high-throughput sequencing techniques have led to the generation of an increasing amount of datasets linking TCR sequences to the epitopes they bind. However, the available data is still extremely sparse compared to the high dimensionality of the search space created by the TCR theoretical diversity. Moreover, current experimental settings typically link many TCRs to a single epitope, which leads to datasets that contain information about tens of thousands of TCRs, but only a few hundred different epitopes.</p>
    <p>Nevertheless, several studies have attempted the prediction of TCR specificity from sequence using machine learning (for a review see <xref rid="btab294-B28" ref-type="bibr">Mösch et al., 2019</xref>). The most successful approaches so far are categorical epitope models, which exploit the relative abundance of TCR sequences to learn patterns of TCRs binding to the same epitope. Various machine learning concepts were applied to this task, including decision trees (De <xref rid="btab294-B8" ref-type="bibr">Neuter et al., 2018</xref>; <xref rid="btab294-B13" ref-type="bibr">Gielis et al., 2019</xref>), a range of different clustering approaches (<xref rid="btab294-B7" ref-type="bibr">Dash et al., 2017</xref>; <xref rid="btab294-B15" ref-type="bibr">Glanville et al., 2017</xref>; <xref rid="btab294-B19" ref-type="bibr">Jokinen et al., 2019</xref>) and Variational Autoencoders (<xref rid="btab294-B30" ref-type="bibr">Sidhom et al., 2021</xref>). Many of these can successfully predict which one of a small set of epitopes a given TCR will most likely bind to. However, these approaches are inherently incapable of predicting specificity to epitopes not contained in the training set (unseen epitopes), which fundamentally limits their applicability.</p>
    <p>The next milestone toward this challenging goal are generic models, which explicitly encode both the TCRs and the epitopes. These have the potential to predict binding of any TCR–epitope pair, opening the door to the development of models that can generalize to both, unseen TCRs and epitopes. Current models show moderate performance on test data containing epitopes already encountered in training, but cannot extrapolate to unseen epitopes (<xref rid="btab294-B20" ref-type="bibr">Jurtz et al., 2018</xref>; <xref rid="btab294-B27" ref-type="bibr">Moris et al., 2020</xref>; <xref rid="btab294-B31" ref-type="bibr">Springer et al., 2019</xref>).</p>
    <p>Tcr epITope bimodal Attention Networks (TITAN) exploits a bimodal neural network architecture to explicitly encode both TCR and epitope sequences. More specifically, TITAN uses convolutions to aggregate local information and fuses the modalities, using an interpretable attention mechanism from which binding probabilities are predicted.</p>
    <p>Since interpretability is paramount in healthcare applications of machine learning, the use of context attention is central to our model, as it allows us to explain the choices of the algorithm and to analyze which amino acids or even atoms the model focuses on. These highlighted entities can be interpreted in the context of the underlying biochemical processes. We explore different encoding strategies for the epitopes such as SMILES (<xref rid="btab294-B33" ref-type="bibr">Weininger et al., 1989</xref>), a string-based, atom-level representation of molecules. SMILES are ubiquitously utilized in chemoinformatics for a wide range of applications, from predicting the chemical or pharmacological properties of molecules (<xref rid="btab294-B16" ref-type="bibr">Goh et al., 2017</xref>; <xref rid="btab294-B24" ref-type="bibr">Manica et al., 2019</xref>; <xref rid="btab294-B29" ref-type="bibr">Schwaller et al., 2018</xref>) to generative modeling (<xref rid="btab294-B17" ref-type="bibr">Gómez-Bombarelli et al., 2018</xref>). Using SMILES effectively results in a re-formulation of the TCR–epitope binding problem as the more general compound protein interaction (CPI) task, thus enabling the usage of large databases of protein-ligand binding affinity for pretraining, e.g. BindingDB including &gt;1 M labeled samples (<xref rid="btab294-B14" ref-type="bibr">Gilson et al., 2016</xref>).</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Data</title>
      <p>In order to assemble a larger and more diverse dataset, we combine data collected in the VDJ database (<xref rid="btab294-B1" ref-type="bibr">Bagaev et al., 2020</xref>) with a recently published COVID-19 specific dataset published by the ImmuneCODE project (<xref rid="btab294-B9" ref-type="bibr">Dines et al., 2020</xref>). Since paired chain data is still rare, we restrict ourselves to TCR<italic toggle="yes">β</italic> chain sequences.</p>
      <p>We use all human TCR<italic toggle="yes">β</italic> sequences downloaded from the VDJ database (December 7, 2020), i.e. 40 438 TCR sequences assigned to 191 peptides. Since this dataset is highly imbalanced, we exclude epitopes with less than 15 associated TCR sequences and downsample to a limit of 400 TCRs per epitope. After these preprocessing steps, we are left with a dataset of 10 599 examples on 87 epitopes. We refer to this dataset as <italic toggle="yes">VDJ</italic>.</p>
      <p>The COVID-19 dataset (published July 25, 2020) originally contained 154 320 examples associated with 269 different epitopes or groups of epitopes. To avoid ambiguity, we keep only samples associated with a single unique epitope and exclude unproductive sequences. Then we apply the same preprocessing steps as for the VDJ dataset, downsampling to 400 TCRs/epitope and excluding epitopes with less than 15 associated TCRs to arrive at a dataset of 12 996 examples.</p>
      <p>We refer to the combined dataset with samples from VDJ and the COVID-19 dataset as <italic toggle="yes">VDJ+COVID-19</italic>.</p>
      <p>Since these primary datasets contain only positive examples, we need to generate negative data. This can be achieved by shuffling the sequences, thereby associating TCRs with epitopes that they have not been shown to bind. Due to the low probability of a randomly drawn TCR binding a specific epitope, this manner of generating negative samples is established in the field (<xref rid="btab294-B11" ref-type="bibr">Fischer et al., 2020</xref>; <xref rid="btab294-B27" ref-type="bibr">Moris et al., 2020</xref>). It has also been shown to limit overestimation of performances in comparison to adding additional naive TCR sequences from other sources (<xref rid="btab294-B27" ref-type="bibr">Moris et al., 2020</xref>). Furthermore, by shuffling the pairing of TCRs and epitopes, we can match the number of negative examples to that of positive examples for each TCR, avoiding unbalanced datasets. With this procedure, we build a training dataset of 46 290 examples, 50% of which are positive, encompassing 192 different epitopes.</p>
      <p>To ensure a fair comparison to the state of the art model ImRex, we also downloaded the publicly available dataset that ImRex was trained on. This dataset is based on the VDJ database and contains 13 404 samples for 118 different epitopes, with 50% negative samples. We use it to train all models for the final comparison (see Section 3.4). To evaluate the performances, we use an independent test set generated from the McPAS database (<xref rid="btab294-B32" ref-type="bibr">Tickotsky et al., 2017</xref>) (downloaded on November 3, 2020). We exclude non-human TCRs and remove all samples with TCRs contained in the ImRex training data. Then we split the McPas data into two test sets: one including all samples with epitopes contained in the ImRex training set (seen epitope test set) and one with epitopes not contained in the ImRex dataset (unseen epitope test set). For both test sets, 50% negative data is generated by shuffling. The final seen epitope test set contains 9740 samples, the unseen epitopes test set contains 1458 samples.</p>
    </sec>
    <sec>
      <title>2.2 Models</title>
      <sec>
        <title>2.2.1 Problem formulation</title>
        <p>We are interested in learning a mapping <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mo>Φ</mml:mo></mml:math></inline-formula> between the space of receptors <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mi mathvariant="script">T</mml:mi></mml:math></inline-formula> and the space of epitopes <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mi mathvariant="script">E</mml:mi></mml:math></inline-formula> to the space of affinity scores <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mi mathvariant="script">A</mml:mi></mml:math></inline-formula>, i.e <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mo>Φ</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="script">E</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="script">A</mml:mi></mml:mrow></mml:math></inline-formula>. <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mo>Φ</mml:mo></mml:math></inline-formula> is learned with a training dataset <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> where <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">E</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> is a binary label indicating whether binding occurred.</p>
      </sec>
      <sec>
        <title>2.2.2 K-NN baseline</title>
        <p>Our baseline model for the presented TCR-epitope binding prediction is constituted by a <italic toggle="yes">k</italic>-nearest-neighbor (K-NN) classifier. As a distance metric between samples, we utilize the sum of the length-normalized Levenshtein distance of the respective epitope and TCR protein primary sequences.</p>
        <p>More formally, for the training data <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, we choose <italic toggle="yes">e<sub>i</sub></italic> and <italic toggle="yes">t<sub>i</sub></italic> to be epitope and TCR sequences, respectively (<italic toggle="yes">t<sub>i</sub></italic> is the CDR3 region). Let <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> denote an unseen sample from the test dataset <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Test</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Test</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. With the goal of predicting <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to approximate the unknown <italic toggle="yes">a<sub>j</sub></italic>, we first retrieve the subset of training data <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> containing the <italic toggle="yes">k</italic> nearest neighbors using the distance measure
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Lev</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">Lev</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mo>|</mml:mo><mml:mo>·</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> denotes sequence length and <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">Lev</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the Levenshtein distance (<xref rid="btab294-B23" ref-type="bibr">Levenshtein, 1966</xref>), i.e. a string-based distance measure that measures the number of single-AA changes required to transform one sequence into the other. Then, the prediction <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is trivially computed by <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> with <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. We evaluate the model on all odd <italic toggle="yes">k</italic> (to avoid ties), s.t. <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>, and choose the value for <italic toggle="yes">k</italic> leading to the best ROC-AUC score for comparisons. Note that this model is non-parametric.</p>
      </sec>
    </sec>
    <sec>
      <title>2.3 Model architecture</title>
      <p><xref rid="btab294-F1" ref-type="fig">Figure 1</xref> shows an overview of the algorithmic steps of TITAN. To predict binding, we devise a bimodal architecture that separately ingests both the TCR and the peptide sequence.</p>
      <fig position="float" id="btab294-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Overview of TITAN architecture. (<bold>a</bold>) Our model ingests a TCR and an epitope sequence, which get encoded using BLOSUM62 for amino acid sequences or learned embeddings for SMILES. Then, 1D convolutions of varying kernel sizes are performed on both input streams before context attention layers generate attention weights for each amino acid of the TCR sequence <italic toggle="yes">given an epitope</italic> and vice versa. Finally, a stack of dense layers outputs the binding probability. Conceptually, this architecture is identical to the one proposed in <xref rid="btab294-B3" ref-type="bibr">Born <italic toggle="yes">et al.</italic> (2021</xref>) (cf. <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S3</xref>) but our visualization here is more fine-grained. (<bold>b</bold>) The linchpin of the model is the bimodal context attention mechanism. It ingests the convolved TCR and epitope encodings, treating one as <italic toggle="yes">reference</italic> and the binding partner as <italic toggle="yes">context</italic>. A series of transformations combines the modalities and yields an attention vector over the reference sequence (driven by the context) that can be overlayed with the molecule like a heatmap</p>
        </caption>
        <graphic xlink:href="btab294f1" position="float"/>
      </fig>
      <p>The TCR sequences are encoded with the BLOSUM62 matrix (<xref rid="btab294-B18" ref-type="bibr">Henikoff and Henikoff, 1992</xref>), which is based on evolutionary similarity of amino acids and has been widely applied in TCR specificity prediction tools (<xref rid="btab294-B20" ref-type="bibr">Jurtz et al., 2018</xref>; <xref rid="btab294-B19" ref-type="bibr">Jokinen et al., 2019</xref>). Either the full sequence or only the CDR3 region was used as input. Since the antigenic peptides are small molecules, we explore two options to encode them: one that uses an amino acid-wise encoding such as BLOSUM62, and one that uses an atom-level encoding with SMILES. All sequences were padded to the same length of 500 tokens. Advantageously, SMILES representations of a molecule are not unique, thus facilitating data augmentation (<xref rid="btab294-B2" ref-type="bibr">Bjerrum, 2017</xref>).</p>
      <p>The remaining architecture is inspired by <xref rid="btab294-B24" ref-type="bibr">Manica et al. (2019)</xref> and almost identical to the compound-protein-interaction (CPI) model presented in <xref rid="btab294-B3" ref-type="bibr">Born <italic toggle="yes">et al.</italic> (2021</xref>). In case of pretraining on CPI data (see below), the models are identical, otherwise, the SMILES-encoding ligand input stream is replaced with an AA-encoding epitope stream. Three parallel channels with convolutions of kernel sizes 3, 5 and 11 are employed on the input sequences to combine information from local neighborhoods of varying spatial extend. A fourth channel has a residual connection without convolutions (see <xref rid="btab294-F1" ref-type="fig">Fig. 1a</xref>). For each of the four channels, we utilize two attention layers, where one modality is used as a context to compute attention scores over the other (see <xref rid="btab294-F1" ref-type="fig">Fig. 1b</xref>). This allows the model to use information from the binding partner, i.e the context, to learn the importance of each token in the input sequence, i.e. the reference. The attention weights <italic toggle="yes">α<sub>i</sub></italic> are computed as:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mi>j</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mo> </mml:mo><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo> </mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mtext>where</mml:mtext><mml:mo> </mml:mo><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">u</mml:mi><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="true">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></disp-formula></p>
      <p>We call <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> the <italic toggle="yes">reference</italic> input, where <italic toggle="yes">T</italic> is the sequence length and <italic toggle="yes">H</italic> is the number of convolutional filters. Further, <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the <italic toggle="yes">context</italic> input, where <italic toggle="yes">U</italic> and <italic toggle="yes">K</italic> are sequence length and number of convolutional filters in the other modality, respectively. <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>×</mml:mo><mml:mi>U</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> are learnable parameters. Intuitively, both inputs are projected into a common attention space <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>A</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> with <italic toggle="yes">A </italic>=<italic toggle="yes"> </italic>16 and then summed up, which enables the layer to take the context into account for determining feature relevance. <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> combines the information through a dot product, the output of which is fed to a softmax layer to obtain the attention weights <italic toggle="yes">α<sub>i</sub></italic>, which are used to filter the inputs. Finally, both TCR and peptide information gets passed to two dense layers with 368 and 184 nodes, respectively, which output the binding probability.</p>
    </sec>
    <sec>
      <title>2.4 Pretraining</title>
      <p>By using SMILES encodings of the epitopes, predicting epitope receptor binding affinity can be seen as a CPI prediction task. We utilize BindingDB (<xref rid="btab294-B14" ref-type="bibr">Gilson et al., 2016</xref>), as of April 2020, to pretrain our model. To reduce the problem complexity and potential biases associated with the different experimental platforms to measure affinity, we binarize the binding data and define all entries in the database as binding, ignoring continuous affinity measurements. We generate an equal amount of negative examples by randomly assigning ligands to proteins (<xref rid="btab294-B3" ref-type="bibr">Born et al., 2021</xref>). In order to avoid high discrepancy between sequence lengths, ligands with a length &gt; 250 SMILES tokens and proteins larger than 1028 amino acids are discarded. This results in 325 688 ligands and 3351 proteins and a total of 471 017 pairs from which 90% (423 915) are used for training and the rest for validation. To adapt the model size to the larger available dataset, we changed the layer sizes for pretraining by padding TCR sequences to 1028, setting the attention space <italic toggle="yes">A </italic>=<italic toggle="yes"> </italic>256, using four convolutional channels with kernel sizes 3, 7, 9 and 13 for epitopes and 3, 7, 13, 19 for TCR and using three final dense layers with 2048, 1024 and 512 nodes.</p>
    </sec>
    <sec>
      <title>2.5 Data splitting</title>
      <p>We evaluate our models on a 10-fold cross-validation split. To determine the generalization capabilities of the models toward unseen TCRs and unseen epitopes separately, we use two different split methods. In the first, we ensure that each TCR is restricted to only one fold, which ensures that the validation datasets do not contain TCRs which were shown in training. The epitopes, however, are distributed randomly over the folds, so that most of the epitopes in the validation dataset were also shown during training. We refer to this split as the <italic toggle="yes">TCR split</italic>. Additionally, we generate a <italic toggle="yes">strict split</italic>, where we ensure that each TCR and each epitope is restricted to a single fold, ensuring that neither TCRs nor epitopes contained in the validation dataset were shown during training. To ensure the separation of TCRs and epitopes in their folds, we generate negative data by shuffling within each fold. A UMap (<xref rid="btab294-B25" ref-type="bibr">McInnes et al., 2018</xref>) visualization of all samples of the dataset is presented in <xref rid="btab294-F2" ref-type="fig">Figure 2</xref> and shows the ramifications of the splitting strategy. The feature space for the UMap dimensionality reduction was generated by embedding the amino acid sequences using a pretrained protein language model (<xref rid="btab294-B10" ref-type="bibr">Elnaggar et al., 2020</xref>).</p>
      <fig position="float" id="btab294-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>UMap Visualization of the VDJ dataset colored by the fold each sample belongs to in the TCR split (left) and strict split (right). The UMap projection leads to clear clusters of the samples. Coloring by fold in the TCR split reveals no connection between clusters and folds. However, coloring by fold in the strict split reveals that all the samples in a cluster belong to the same fold. This suggests that clusters correspond to distinct epitopes, highlighting their heterogeneity and the challenge of good generalization for the strict split.</p>
        </caption>
        <graphic xlink:href="btab294f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.6 Model training</title>
      <p>All described architectures were implemented in PyTorch 1.4 and used the pytoda package for data handling and preprocessing. The models optimized binary cross entropy loss with Adam (<xref rid="btab294-B21" ref-type="bibr">Kingma and Ba, 2015</xref>) (<italic toggle="yes">β</italic><sub>1</sub> = 0.9, <italic toggle="yes">β</italic><sub>2</sub> = 0.999, <italic toggle="yes">ϵ</italic>  =  1e-8) and a learning rate of 0.0001. In the convolutional and dense layers, we employed dropout (<italic toggle="yes">P </italic>=<italic toggle="yes"> </italic>0.5) and ReLU activation. All models were trained with a batch size of 512 on a cluster equipped with POWER8 processors and a single NVIDIA Tesla P100. The learning rate was tuned using the VDJ dataset and the remaining hyperparameters were chosen carefully based on previous experience.</p>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Performance on TCR split</title>
      <p>In <xref rid="btab294-F3" ref-type="fig">Figure 3a</xref>, we compare 10-fold crossvalidation performances of different TITAN settings on the TCR split scenario, which allows us to gauge the generalization capabilities of the algorithm toward unseen TCR sequences. We explored several options to input the sequence information of TCR and epitope in our model. Following the well-established concept of word embeddings (<xref rid="btab294-B26" ref-type="bibr">Mikolov et al., 2013</xref>), we can encode the amino acids using a fixed-length vector representation (in our case 32-dimensional), that is initialized randomly and then learned through training. Alternatively, we can represent each amino acid as a vector containing biophysical properties (e.g. molecular weight, residue weight, pKa, pKb, pKx, pI and hydrophobicity at pH2). Finally, we explored using evolutionary substitution matrices like BLOSUM62. Each row in the BLOSUM matrix represents the probability for an amino acid to be substituted by any of the other amino acids and can be used as a 26-dimensional vector representation of that amino acid.</p>
      <fig position="float" id="btab294-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Performance comparison of different TITAN model settings trained or fine-tuned on the VDJ+Covid dataset. (<bold>a</bold>) ROC-AUC scores and balanced accuracy on a 10-fold crossvalidation TCR split (validation and training data share epitopes, but not TCRs). (<bold>b</bold>) ROC-AUC scores and balanced accuracy on a strict 10-fold crossvalidation split, where validation and training data share neither epitopes, nor TCRs. All boxplots: The center of each boxplot marks the sample median, and the box extends from lower to upper quartile. <italic toggle="yes">K-NN</italic> refers to the baseline model. Other abbreviations denote different settings under which TITAN was trained. <italic toggle="yes">AA CDR3:</italic> amino acid encoding of epitopes, only CDR3 sequence input for TCRs; <italic toggle="yes">AA full:</italic> amino acid encoding of epitopes, full sequence input for TCRs; <italic toggle="yes">SMI CDR3:</italic> SMILES encoding of epitopes, only CDR3 sequence input for TCRs; <italic toggle="yes">SMI full:</italic> SMILES encoding of epitopes, full sequence input for TCRs; <italic toggle="yes">Pretrained:</italic> SMILES encoding of epitopes, full sequence input for TCRs, model pretrained on BindingDB; <italic toggle="yes">Pretrained semifrozen:</italic> SMILES encoding of epitopes, full sequence input for TCRs, model pretrained on BindingDB, weights in epitope channel fixed during fine-tuning; <italic toggle="yes">Pretrained aug:</italic> SMILES encoding of epitopes, full sequence input for TCRs, model pretrained on BindingDB with data augmentation; <italic toggle="yes">Pretrained semifrozen aug:</italic> SMILES encoding of epitopes, full sequence input for TCRs, model pretrained on BindingDB with data augmentation, weights in epitope channel fixed during fine-tuning</p>
        </caption>
        <graphic xlink:href="btab294f3" position="float"/>
      </fig>
      <p>An initial comparison of these embedding options showed no clear preference for either of them (See <xref rid="btab294-T1" ref-type="table">Table 1</xref>). Since a learned embedding is less reproducible and biophysical feature choices can be debated, we decided to use the BLOSUM62 matrix to embed amino acid sequences in all model settings.</p>
      <table-wrap position="float" id="btab294-T1">
        <label>Table 1.</label>
        <caption>
          <p>Comparison of amino acid embeddings</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Embedding</th>
              <th rowspan="1" colspan="1">ROC-AUC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Biophysical features</td>
              <td rowspan="1" colspan="1">0.76 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Learned embedding</td>
              <td rowspan="1" colspan="1">0.75 ± 0.01</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">BLOSUM62 matrix</td>
              <td rowspan="1" colspan="1">0.75 ± 0.01</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: Mean and standard deviation of the <italic toggle="yes">AA CDR3</italic> model configuration on 10-fold TCR split of the VDJ dataset. All tested amino acid embeddings show a similar performance.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>For TCRs, we have the options to either focus solely on the hypervariable CDR3 loop, which is known to be the main peptide binding region, or to consider the full variable region of the TCR<italic toggle="yes">β</italic> sequence, which includes the V, D and J segments and encompasses all three hypervariable loops CDR1, CDR2 and CDR3. <xref rid="btab294-F3" ref-type="fig">Figure 3a</xref> shows that the use of the full sequence information boosts the model performance, indicating that valuable information is contained in regions outside of the CDR3 loop.</p>
      <p>Since epitopes are relatively short (5 to 15 amino acids), we can represent them in a fine-grained, atom-wise manner using SMILES strings. We can see in <xref rid="btab294-F3" ref-type="fig">Figure 3a</xref> that the SMILES representation of epitopes further improves performance compared to an amino acid encoding.</p>
      <p>The combination of SMILES for epitopes and full sequence encoding of the TCR leads to a mean ROC-AUC of 0.77 ± 0.006 and a mean balanced accuracy of 0.72 ± 0.005. However, we see that even this improved model does not outperform the simple K-NN baseline model that we included as a comparison. With <italic toggle="yes">k </italic>=<italic toggle="yes"> </italic>13, the K-NN baseline achieves the best results with a ROC-AUC of 0.78 ± 0.007 and a balanced accuracy of 0.71 ± 0.008 (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S1</xref>). This highlights the importance of including an appropriate baseline model, which is so far rarely observed in the field, as simple models may outperform complex ones in a sparse data setting. We emphasize that this is not an argument <italic toggle="yes">against</italic> our neural networks, but <italic toggle="yes">for</italic> the K-NN baseline model, which also outperforms the state of the art model, ImRex, a recent approach that uses 2D CNNs (<xref rid="btab294-B27" ref-type="bibr">Moris et al., 2020</xref>) (see Section 3.4 for a more detailed comparison of TITAN and ImRex). We also note that an approach similar to our K-NN baseline, TCRMatch (<xref rid="btab294-B5" ref-type="bibr">Chronister et al., 2020</xref>), was recently presented in a preprint. TCRMatch predicts TCR specificity using only sequence similarities to previously characterized receptors.</p>
      <p>All model comparisons on the TCR split are also summarized in <xref rid="btab294-T2" ref-type="table">Table 2</xref>. For better comparability to previously published models, we also include scores obtained on the dataset excluding the samples gathered from the COVID-19 dataset.</p>
      <table-wrap position="float" id="btab294-T2">
        <label>Table 2.</label>
        <caption>
          <p>Tenfold cross validation performance on TCR split</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">ROC-AUC</th>
              <th rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">VDJ</th>
              <th rowspan="1" colspan="1">VDJ+COVID-19</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">K-NN (Baseline)</td>
              <td rowspan="1" colspan="1">0.79 ± 0.01</td>
              <td rowspan="1" colspan="1">0.78 ± 0.007</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">AA CDR3</td>
              <td rowspan="1" colspan="1">0.75 ± 0.02</td>
              <td rowspan="1" colspan="1">0.74 ± 0.007</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">AA full</td>
              <td rowspan="1" colspan="1">0.76 ± 0.007</td>
              <td rowspan="1" colspan="1">0.75 ± 0.007</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SMI CDR3</td>
              <td rowspan="1" colspan="1">0.73 ± 0.007</td>
              <td rowspan="1" colspan="1">0.76 ± 0.008</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SMI full</td>
              <td rowspan="1" colspan="1">0.75 ± 0.006</td>
              <td rowspan="1" colspan="1">0.77 ± 0.006</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained</td>
              <td rowspan="1" colspan="1">0.81 ± 0.01</td>
              <td rowspan="1" colspan="1">0.84 ± 0.005</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained aug.</td>
              <td rowspan="1" colspan="1">0.80 ± 0.01</td>
              <td rowspan="1" colspan="1">0.86 ± 0.004</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained semifrozen</td>
              <td rowspan="1" colspan="1">0.80 ± 0.01</td>
              <td rowspan="1" colspan="1">0.84 ± 0.005</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained semifrozen aug.</td>
              <td align="center" rowspan="1" colspan="1">
                <inline-formula id="IE27">
                  <mml:math id="IM27" display="inline" overflow="scroll">
                    <mml:mn>0.82</mml:mn>
                    <mml:mo>±</mml:mo>
                    <mml:mn>0.01</mml:mn>
                  </mml:math>
                </inline-formula>
              </td>
              <td align="center" rowspan="1" colspan="1">
                <inline-formula id="IE28">
                  <mml:math id="IM28" display="inline" overflow="scroll">
                    <mml:mn>0.87</mml:mn>
                    <mml:mo>±</mml:mo>
                    <mml:mn>0.005</mml:mn>
                  </mml:math>
                </inline-formula>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic toggle="yes">Note</italic>: Mean and standard deviation of each model configuration on the VDJ dataset and the VDJ + COVID-19 dataset. Best performance marked in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Furthermore, the results in <xref rid="btab294-T2" ref-type="table">Table 2</xref> also indicate that the available interaction data may be too sparse to enable the models to learn the complex interaction patterns governing TCR–epitope binding. However, using the SMILES encoding for epitopes and the full sequence encoding for TCRs, we have effectively re-formulated the task as a compound protein interaction, which allows us to use BindingDB (<xref rid="btab294-B14" ref-type="bibr">Gilson et al., 2016</xref>) to pretrain the model, before we fine-tune it on the TCR–epitope interaction data. The pretrained model performed well on the held-out data from BindingDB (ROC-AUC 0.895).</p>
      <p>Regarding the pretraining, we tested two different settings, one where all weights could be adapted during fine-tuning, and a <italic toggle="yes">semifrozen</italic> setting, where we only allowed weight changes in the TCR channel and the final dense layers of the epitope. This was done to prevent the model from ‘unlearning’ to recognize relevant SMILES features due to the extremely low number of different epitopes in the fine-tuning dataset. <xref rid="btab294-F3" ref-type="fig">Figure 3</xref> shows that pretraining severely improves model performance in both settings. We further improved model performance by exploiting the non-uniqueness of SMILES strings to perform data augmentation. Augmentation is achieved by randomly generating a valid SMILES representation of the epitope on the fly at each training step (<xref rid="btab294-B4" ref-type="bibr">Born et al., 2021b</xref>). The best overall model performance was achieved by the semifrozen pretrained model with augmentation, with a mean ROC-AUC of 0.87 ± 0.005 and a mean balanced accuracy of 0.79 ± 0.005, clearly outperforming the K-NN baseline by a large margin.</p>
      <p>The high performance on validation data that does not contain TCR sequences used in training shows that the model successfully generalizes to unseen TCRs. Comparing the performance across groups of TCRs with different similarities to the training data, we find that while the model performs better for TCRs that are highly similar to their closest partners in the training set, the performance on the TCRs with highest distance from the training data is still high (ROC AUC 0.84 ± 0.016, balanced accuracy 0.71 ± 0.008, see <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S3</xref>). Moreover, it is interesting to observe that models pretrained on the BindingDB (<xref rid="btab294-B14" ref-type="bibr">Gilson et al., 2016</xref>) performed better on the larger and more heterogeneous dataset VDJ + COVID-19, which suggests that pretraining might not only increase performance, but also enable the model to better generalize to different datasets.</p>
    </sec>
    <sec>
      <title>3.2 Analysis of attention layers</title>
      <p>We can investigate the decision processes of TITAN using the information contained in the attention layers. <xref rid="btab294-F4" ref-type="fig">Figure 4a</xref> shows the attention scores of TITAN in the <italic toggle="yes">AA CDR3</italic> setting—a setting that clearly outperforms previous work (see Section 3.4)—over a number of exemplary CDR3 sequences. We see that while there is some preference to focus on certain positions, the model adapts the attention to the different sequences. The mean inter-TCR variance per token is at <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mn>4.3</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Moreover, we find that the attention also adapts to the context (i.e. the epitope), for which we want to predict the interaction. The mean variance per token within the same TCR interacting with different epitopes is <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mn>1.3</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. This demonstrates that the model is capable of adapting the attention layer to both the input and the context.</p>
      <fig position="float" id="btab294-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>Analysis of the context attention layers. The attention weights <italic toggle="yes">α<sub>i</sub></italic> on each token are extracted from the context attention layer and represented as a colormap. In (<bold>a</bold>) and (<bold>b</bold>), examples of the <italic toggle="yes">AA CDR3</italic> setting are shown, where epitopes are input as amino acid sequences and only CDR3 sequences are input in the TCR channel. 0 denotes the padding and 1 the &lt;START&gt; and &lt;STOP&gt; token. (a) The attention scores over three exemplary CDR3 sequences. The model adapts the attention scores based on the different inputs. (b) The attention on three exemplary epitopes. The model fails to adapt the attention scores to different inputs. (<bold>c</bold>) Attention scores over an exemplary epitope that was not included in the training dataset. The model was pretrained on BindingDB and fine-tuned on the training data with a frozen epitope input channel with SMILES augmentations to enrich the data. The lower bound of the color scale was set to one standard deviation above the mean attention on the padding tokens.</p>
        </caption>
        <graphic xlink:href="btab294f4" position="float"/>
      </fig>
      <p><xref rid="btab294-F4" ref-type="fig">Figure 4b</xref> shows the attention of the same model (<italic toggle="yes">AA CDR3</italic> setting) on several exemplary epitopes. We can clearly see that the model chooses to focus heavily on the same positions on each epitope. The preferred positions are independent of the sequence of both the input epitope and the context TCR. Comparing the attention scores across epitopes, we find that both the inter-epitope and the intra-epitope variance of the attention are extremely small, at <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mn>4.9</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> respectively. This behavior indicates that TITAN fails to learn meaningful patterns in the epitope sequences from the limited diversity of epitopes in the dataset. We conjectured that the model finds a way to internally generate classes of epitopes represented by meaningless—but unique—vectors and predict specificity of TCRs toward these. This hypothesis is supported by the observation that compressing each epitope sequence to the chain of amino acids with attention scores <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> yields only a very moderate reduction in the number of unique sequences. Crucially, these shortened epitope sequences are still unique for 185 out of 192 epitopes in the dataset (96%). By focusing on these fixed, invariant positions the model circumnavigates learning generic representations of epitopes and instead internally classifies epitopes, at the cost of losing the power to differentiate 19 epitopes.</p>
      <p>Pretraining the models on the compound interaction task greatly increased the diversity of sequences that were seen by the epitope channel of the model. While most of the ligands in BindingDB are not peptides, and may therefore exhibit structures and chemical properties that differ strongly from the epitopes, the model may nevertheless infer general rules of chemical interaction from them. This enables the model to learn more meaningful attention weights, which may be the reason for the performance boost we observe for pretrained models. In <xref rid="btab294-F4" ref-type="fig">Figure 4b</xref>, we show a visual representation of the attention scores over one exemplary epitope as a case study of TITAN in the <italic toggle="yes">pretrained semifrozen augmented</italic> setting. The chosen epitope CINGVCWTV was not shown during training. The attention scores therefore reflect the transferable knowledge the model has gained during pretraining. We see that the attention patterns align well with our expectations. The attention is high on many of the oxygens, as well as on the two thiol groups of the cysteines. Nevertheless, we see that while the attention layers may extract some chemically relevant structures, they fail to capture others. Specifically, large and hydrophobic amino acid sidechains tend to get low attention scores, although they may be of high importance for molecule interactions.</p>
    </sec>
    <sec>
      <title>3.3 Performance on strict split</title>
      <p><xref rid="btab294-F3" ref-type="fig">Figure 3b</xref> compares the performances of the different TITAN settings on the strict epitope split, which measures the generalization capabilities to unseen epitopes. As expected, model performance drops severely across all settings. Moreover, we find that all TITAN settings perform similarly, with mean ROC-AUC scores around 0.6, while the K-NN baseline model shows a mean ROC-AUC of only 0.54 ± 0.03 for a choice of <italic toggle="yes">k </italic>=<italic toggle="yes"> </italic>25 (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S2</xref> for comparison). Compared to the unseen TCR performance, we also see an increase in the standard deviation of the scores, with ROC-AUC scores of over 0.7 for some folds, and 0.5 for others. All mean ROC-AUC values are summarized in <xref rid="btab294-T3" ref-type="table">Table 3</xref>.</p>
      <table-wrap position="float" id="btab294-T3">
        <label>Table 3.</label>
        <caption>
          <p>Tenfold cross validation performance on strict split</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="±" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">ROC-AUC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">K-NN (Baseline)</td>
              <td rowspan="1" colspan="1">0.54 ± 0.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">AA CDR3</td>
              <td rowspan="1" colspan="1">0.60 ± 0.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">AA full</td>
              <td rowspan="1" colspan="1">0.59 ± 0.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SMI CDR3</td>
              <td rowspan="1" colspan="1">0.60 ± 0.06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SMI full</td>
              <td rowspan="1" colspan="1">0.59 ± 0.06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained</td>
              <td rowspan="1" colspan="1">0.56 ± 0.04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained + Aug.</td>
              <td rowspan="1" colspan="1">0.59 ± 0.03</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained semifrozen</td>
              <td rowspan="1" colspan="1">0.58 ± 0.06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained semifrozen + Aug.</td>
              <td align="center" rowspan="1" colspan="1">
                <inline-formula id="IE34">
                  <mml:math id="IM34" display="inline" overflow="scroll">
                    <mml:mn>0.62</mml:mn>
                    <mml:mo>±</mml:mo>
                    <mml:mn>0.06</mml:mn>
                  </mml:math>
                </inline-formula>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <p><italic toggle="yes">Note</italic>: Mean and standard deviation of each model configuration on the VDJ + COVID-19 dataset. Best performance marked in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>The best score overall is still achieved by the <italic toggle="yes">pretrained semifrozen augmented</italic> model, with a mean ROC-AUC of 0.62 ± 0.06 and a mean balanced accuracy of 0.61 ± 0.05. However, its superiority over other TITAN setting is not as large as in the TCR split scheme. We can also see that pretraining does not strongly improve model performance on unseen epitopes.</p>
      <p>Another surprising result is the comparably good performance of the TITAN <italic toggle="yes">AA CDR3</italic> setting on unseen epitopes. The analysis of the attention layer in <xref rid="btab294-F4" ref-type="fig">Figure 4a</xref> shows that in this setting, TITAN uses an attention mask to reduce the task to a TCR classification problem. This should prevent the model to generalize to new epitopes. A close look at <xref rid="btab294-F5" ref-type="fig">Figure 5</xref> reveals that the best performance of the <italic toggle="yes">AA CDR3</italic> model on the strict split is achieved during the first 10 to 20 epochs of training. During this time, the attention is still uniformly distributed over all input tokens, because many training epochs are required for the model to build the static attention mask described in Section 3.2.</p>
      <fig position="float" id="btab294-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>Training dynamics for both splitting strategies. A key distinction between the training on the TCR and strict splits is that the validation performance steadily converges only in the TCR split case. In the strict split case, no significant improvement is achieved even after training for more than 140 epochs. This indicates that in the strict split scenario, training and validation data may be too distinct to enable a proper generalization. The AA CDR3 setting was used for this plot.</p>
        </caption>
        <graphic xlink:href="btab294f5" position="float"/>
      </fig>
      <p><xref rid="btab294-F5" ref-type="fig">Figure 5</xref> also shows that while the ROC-AUC increases continuously over the training epochs in the TCR split cross-validation, it stagnates during training on the strict split. We hypothesize that the underlying factor causing this phenomenon is the violation of the i.i.d. assumption across training and validation data. This behavior was common during training on the strict split and again highlights the challenge for models to generalize to unseen epitopes from the sparsity of the current datasets.</p>
      <p>While TITAN’s generalization capabilities to unseen epitopes are still limited, the performance is moderately good. This suggests that, despite their sparsity, the currently available datasets do contain enough information to enable generalization to unseen epitopes to a certain degree.</p>
    </sec>
    <sec>
      <title>3.4 Comparison to ImRex model on independent test set</title>
      <p>Moris <italic toggle="yes">et al.</italic> recently published ImRex (<xref rid="btab294-B27" ref-type="bibr">Moris et al., 2020</xref>), an image-based generic TCR specificty prediction model, which explicitly encodes epitopes and can make predictions for unseen epitopes. As a direct comparison to ImRex, we trained TITAN with different settings on the ImRex training data and tested performance on an independent test set generated from the McPAS database (see Section 2.1). To judge both generalization capabilities to unseen TCRs and unseen epitopes, we used two different subsets of the McPAS data, one where all samples containing TCRs from the ImRex training dataset were excluded (<italic toggle="yes">seen epitopes</italic> test set) and one where all samples including epitopes contained in the ImRex training dataset were removed (<italic toggle="yes">unseen epitopes</italic> test set). ROC-AUC scores are compared in <xref rid="btab294-T4" ref-type="table">Table 4</xref>. We emphasize that this is not a cross-validation setting, i.e. we train on the full training set and record performance on an independent test set.</p>
      <table-wrap position="float" id="btab294-T4">
        <label>Table 4.</label>
        <caption>
          <p>Comparison of TITAN with prior work</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">ROC-AUC</th>
              <th rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Seen epitopes</th>
              <th rowspan="1" colspan="1">Unseen epitopes</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">ImRex</td>
              <td rowspan="1" colspan="1">0.61</td>
              <td rowspan="1" colspan="1">0.50</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">K-NN (Baseline)</td>
              <td rowspan="1" colspan="1">0.79</td>
              <td rowspan="1" colspan="1">0.37</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">AA CDR3</td>
              <td rowspan="1" colspan="1">0.83</td>
              <td rowspan="1" colspan="1">0.69</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">AA full</td>
              <td rowspan="1" colspan="1">0.81</td>
              <td rowspan="1" colspan="1">0.64</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SMI CDR3</td>
              <td rowspan="1" colspan="1">0.85</td>
              <td rowspan="1" colspan="1">0.72</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SMI full</td>
              <td rowspan="1" colspan="1">0.86</td>
              <td rowspan="1" colspan="1">0.64</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained</td>
              <td rowspan="1" colspan="1">0.79</td>
              <td rowspan="1" colspan="1">
                <bold>0.78</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained + Aug.</td>
              <td rowspan="1" colspan="1">0.83</td>
              <td rowspan="1" colspan="1">0.65</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained semifrozen</td>
              <td rowspan="1" colspan="1">0.77</td>
              <td rowspan="1" colspan="1">0.69</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pretrained semifrozen + Aug.</td>
              <td rowspan="1" colspan="1">
                <bold>0.87</bold>
              </td>
              <td rowspan="1" colspan="1">0.60</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn4">
            <p><italic toggle="yes">Note</italic>: All but the ImRex model (shaded in gray) are contributions of this work. Models were trained on identical data and tested on an independent test set to ensure a fair comparison. Best performance marked in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We find that all of our model settings outperform ImRex on both independent test sets by a large margin. Moreover, our K-NN baseline model clearly outperforms ImRex on seen epitopes.</p>
      <p>Moreover, we see that the base models (<italic toggle="yes">AA CDR3, AA full, SMI CDR3, SMI full</italic>) all perform better on the independent <italic toggle="yes">seen epitopes</italic> test data than during 10-fold crossvalidation, while for the pretrained model settings, performance is comparable to the crossvalidation. For the <italic toggle="yes">unseen epitopes</italic> test set, we observe a similar trend as above in the strict split cross-validation. All models show performances clearly better than chance, with the pretrained semifrozen model with augmentation even achieving a ROC-AUC of 0.78. However, one needs to keep in mind, that the sample size for the <italic toggle="yes">unseen epitopes</italic> test is at only 1500, making especially high (or low) scores likely to be statistical outliers.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>In this work, we present TITAN, a generic, bimodal, sequence-based neural network for predicting TCR–epitope binding probability that significantly outperforms the state-of-the-art. We compare several settings of TITAN that differ in their inputs for TCRs and epitopes. While we restrict ourselves to the TCR<italic toggle="yes">β</italic> chain, we find that inputting the complete variable TCR sequence boosts performance compared to scenarios where only the CDR3 region is used.</p>
    <p>Notably, we are, to the best of our knowledge, the first to formulate TCR-epitope binding prediction as a subclass of the commonly investigated task of predicting compound-protein-interaction. The concomitant change of representing epitopes as SMILES (an atomic, more granular representation) instead of amino acid sequences improves the predictive power of TITAN. This reformulation not only enables data augmentation—by exploiting the non-uniqueness of SMILES (<xref rid="btab294-B2" ref-type="bibr">Bjerrum, 2017</xref>)—to enrich the training data but also positions us to leverage large-scale datasets such as BindingDB for pretraining. Pretrained TITAN models achieve considerably higher scores than all TITAN base models. Freezing the weights of the epitope input channel coupled with SMILES augmentation gives us the best performance with a mean ROC-AUC of 0.87 ± 0.005 on the TCR split.</p>
    <p>To assess model performance in a more rigorous fashion, we designed a K-NN classifier based on the Levenshtein distance as a baseline model. Surprisingly, this simple model achieves a mean ROC-AUC score of 0.79 ± 0.007 on the TCR split crossvalidation. This model surpassed the performance of complex neural networks from previous publications (<xref rid="btab294-B27" ref-type="bibr">Moris et al., 2020</xref>) and is only outperformed by our pretrained TITAN models, which highlights the importance of including relevant baseline models.</p>
    <p>As a final assessment of model performance, we compare TITAN to the current state of the art for general TCR specificity prediction, ImRex. To ensure fair comparison, we train our models on the ImRex training data and evaluate the performance on an independent test set derived from a different database. We demonstrate that both pretrained and base TITAN models, as well as the K-NN baseline, outperform ImRex by a large margin. The best result is again achieved by the pretrained semifrozen augmented TITAN model, with a ROC-AUC of 0.87 on epitopes included in the training data.</p>
    <p>Finally, the main challenge for generic TCR–epitope interaction prediction remains the generalization to unseen epitopes. Here, we report two major breakthroughs. First, we demonstrate that TITAN exhibits moderate performance on unseen epitopes, where the best TITAN model achieves a ROC-AUC of 0.62 ± 0.05 on a strict 10-fold crossvalidation split, and ROC-AUC of 0.78 on an independent test set of unseen epitopes. Second, by dissecting the attention heatmaps we were able to identify a possible explanation for the observed poor unseen epitope generalization capabilities. We demonstrate that TITAN reduces the general TCR–epitope prediction task to the simpler task of TCR classification, by internally treating the epitopes as classes instead of focusing on their properties. This shortcut could present a general problem for sufficiently complex models, as long as the extreme sparsity of sampling of the epitope sequence space is not remedied. Until then, our future endeavors might include using an enriched training dataset consisting of TCR-epitope pairs as well as compound-protein interaction pairs from BindingDB, or further improving the amino acid and SMILES embeddings by training on diverse databases (UniProt <xref rid="btab294-B6" ref-type="bibr">Consortium, 2020</xref>; <xref rid="btab294-B12" ref-type="bibr">Gaulton et al., 2017</xref>). In general, our results indicate that approaches with a focus on leveraging transfer learning techniques to enrich the input data space may be promising to tackle the daunting task of unseen epitope-TCR specificity prediction.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab294_Supplementary_Data</label>
      <media xlink:href="btab294_supplementary_data.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors thank Matteo Manica and Joris Cadow for building up much of the model architecture used, as well as for many helpful discussions.</p>
    <p><italic toggle="yes">Financial Support</italic>: The authors acknowledge funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie [813545 and 826121].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab294-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bagaev</surname><given-names>D.V.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>VDJdb in 2019: database extension, new analysis infrastructure and a T-cell receptor motif compendium</article-title>. <source>Nucleic Acids Res</source>., <volume>48</volume>, <fpage>D1057</fpage>–<lpage>D1062</lpage>.<pub-id pub-id-type="pmid">31588507</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bjerrum</surname><given-names>E.J.</given-names></string-name></person-group> (<year>2017</year>) SMILES enumeration as data augmentation for neural network modeling of molecules. <ext-link xlink:href="http://arxiv.org/abs/1703.07076" ext-link-type="uri">http://arxiv.org/abs/1703.07076</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab294-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Born</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Data-driven molecular design for discovery and synthesis of novel ligands – a case study on sars-cov-2</article-title>. <source>Mach. Learn. Sci. Technol</source>., <volume>2</volume>, <fpage>025024</fpage>.</mixed-citation>
    </ref>
    <ref id="btab294-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Born</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>PaccMann<sup>RL</sup>: de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning</article-title>. <source>iScience</source>, <volume>24</volume>, <fpage>102269</fpage>.<pub-id pub-id-type="pmid">33851095</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chronister</surname><given-names>W.D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>TCRmatch: Predicting T-cell Receptor Specificity based on Sequence Similarity to Previously Characterized Receptors</article-title>. <source>Front. immunol., 12, 673</source>.</mixed-citation>
    </ref>
    <ref id="btab294-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Consortium</surname><given-names>T.U.</given-names></string-name></person-group> (<year>2020</year>) <article-title>UniProt: the universal protein knowledgebase in 2021</article-title>. <source>Nucleic Acids Res</source>., <volume>49</volume>, <fpage>D480</fpage>–<lpage>D489</lpage>.</mixed-citation>
    </ref>
    <ref id="btab294-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dash</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Quantifiable predictive features define epitope-specific T cell receptor repertoires</article-title>. <source>Nature</source>, <volume>547</volume>, <fpage>89</fpage>–<lpage>93</lpage>.<pub-id pub-id-type="pmid">28636592</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Neuter</surname><given-names>N.D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>On the feasibility of mining CD8+ T cell receptor patterns underlying immunogenic peptide recognition</article-title>. <source>Immunogenetics</source>, <volume>70</volume>, <fpage>159</fpage>–<lpage>168</lpage>.<pub-id pub-id-type="pmid">28779185</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dines</surname><given-names>J.N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>The immunerace study: a prospective multicohort study of immune response action to covid-19 events with the immunecode<sup>TM</sup> open access database</article-title>. <source>medRxiv</source>.</mixed-citation>
    </ref>
    <ref id="btab294-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elnaggar</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Prottrans: towards cracking the language of life’s code through self-supervised deep learning and high performance computing</article-title>. CoRR, abs/2007.06225, <ext-link xlink:href="https://arxiv.org/abs/2007.06225" ext-link-type="uri">https://arxiv.org/abs/2007.06225</ext-link></mixed-citation>
    </ref>
    <ref id="btab294-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fischer</surname><given-names>D.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Predicting antigen specificity of single t cells based on TCR cdr3 regions</article-title>. <source>Mol. Syst. Biol</source>., <volume>16</volume>, <fpage>e9416</fpage>.<pub-id pub-id-type="pmid">32779888</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gaulton</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>The ChEMBL database in</article-title>. <source>Nucleic Acids Res</source>., <volume>45</volume>, <fpage>D945</fpage>–<lpage>D954</lpage>.<pub-id pub-id-type="pmid">27899562</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gielis</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Detection of enriched T cell epitope specificity in full T cell receptor sequence repertoires</article-title>. <source>Front Immunol</source>., <volume>10</volume>, <fpage>2820</fpage>.<pub-id pub-id-type="pmid">31849987</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gilson</surname><given-names>M.K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Bindingdb in 2015: a public database for medicinal chemistry, computational chemistry and systems pharmacology</article-title>. <source>Nucleic Acids Res</source>., <volume>44</volume>, <fpage>D1045</fpage>–<lpage>D1053</lpage>.<pub-id pub-id-type="pmid">26481362</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Glanville</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Identifying specificity groups in the T cell receptor repertoire</article-title>. <source>Nature</source>, <volume>547</volume>, <fpage>94</fpage>–<lpage>98</lpage>.<pub-id pub-id-type="pmid">28636589</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Goh</surname><given-names>G.B.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) SMILES2Vec: an interpretable general-purpose deep neural network for predicting chemical properties. CoRR, abs/1712.02034, <ext-link xlink:href="http://arxiv.org/abs/1712.02034" ext-link-type="uri">http://arxiv.org/abs/1712.02034</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab294-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gómez-Bombarelli</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>automatic chemical design using a data-driven continuous representation of molecules</article-title>. <source>ACS Central Sci</source>., <volume>4</volume>, <fpage>268</fpage>–<lpage>276</lpage>.</mixed-citation>
    </ref>
    <ref id="btab294-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Henikoff</surname><given-names>S.</given-names></string-name>, <string-name><surname>Henikoff</surname><given-names>J.G.</given-names></string-name></person-group> (<year>1992</year>) <article-title>Amino acid substitution matrices from protein blocks</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>89</volume>, <fpage>10915</fpage>–<lpage>10919</lpage>.<pub-id pub-id-type="pmid">1438297</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Jokinen</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) TCRGP: determining epitope specificity of T cell receptors. PLoS Comput Biol, 17(3), e1008814.</mixed-citation>
    </ref>
    <ref id="btab294-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jurtz</surname><given-names>V.I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>NetTCR: sequence-based prediction of TCR binding to peptide-MHC complexes using convolutional neural networks</article-title>. <source>bioRxiv</source>. 433706, doi: 10.1101/433706.</mixed-citation>
    </ref>
    <ref id="btab294-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kingma</surname><given-names>D.P.</given-names></string-name>, <string-name><surname>Ba</surname><given-names>J.L.</given-names></string-name></person-group> (<year>2015</year>) Adam: a method for stochastic optimization. In: <italic toggle="yes">3rd International Conference on Learning Representations</italic>, San Diego, CA, USA, ICLR 2015 – Conference Track Proceedings. <ext-link xlink:href="https://arxiv.org/abs/1412.6980v9" ext-link-type="uri">https://arxiv.org/abs/1412.6980v9</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab294-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Laydon</surname><given-names>D.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <article-title>Estimating T-cell repertoire diversity: limitations of classical estimators and a new approach</article-title>. <source>Philos. Trans. R. Soc. B Biol. Sci</source>., <volume>370</volume>, <fpage>20140291</fpage>.</mixed-citation>
    </ref>
    <ref id="btab294-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Levenshtein</surname><given-names>V.</given-names></string-name></person-group> (<year>1966</year>) <article-title>Binary codes capable of correcting deletions, insertions, and reversals</article-title>. <source>Soviet Physics Doklady</source>, <volume>10</volume>, <fpage>707</fpage>–<lpage>710</lpage>.</mixed-citation>
    </ref>
    <ref id="btab294-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manica</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Toward explainable anticancer compound sensitivity prediction via multimodal attention-based convolutional encoders</article-title>. <source>Mol. Pharm</source>., <volume>16</volume>, <fpage>4797</fpage>–<lpage>4806</lpage>.<pub-id pub-id-type="pmid">31618586</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McInnes</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Umap: uniform manifold approximation and projection for dimension reduction</article-title>. CoRR, abs/1802.03426, <ext-link xlink:href="https://arxiv.org/abs/1802.03426" ext-link-type="uri">https://arxiv.org/abs/1802.03426</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab294-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mikolov</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) <article-title>Efficient estimation of word representations in vector space</article-title>. In: <source>1st International Conference on Learning Representations</source>, Scottsdale, Arizona, USA, ICLR 2013 - Workshop Track Proceedings. <ext-link xlink:href="https://arxiv.org/abs/1301.3781" ext-link-type="uri">https://arxiv.org/abs/1301.3781</ext-link>.</mixed-citation>
    </ref>
    <ref id="btab294-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moris</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Current challenges for unseen-epitope TCR interaction prediction and a new perspective derived from image classification</article-title>. <source>Brief. Bioinf</source>., bbaa<volume>318</volume>, <fpage>1477</fpage>–<lpage>4054</lpage></mixed-citation>
    </ref>
    <ref id="btab294-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mösch</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) Machine learning for cancer immunotherapies based on epitope recognition by T cell receptors. <italic toggle="yes">Front. genet</italic>., 10, 1141.</mixed-citation>
    </ref>
    <ref id="btab294-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwaller</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Found in translation: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models</article-title>. <source>Chem. Sci</source>., <volume>9</volume>, <fpage>6091</fpage>–<lpage>6098</lpage>.<pub-id pub-id-type="pmid">30090297</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sidhom</surname><given-names>J.-W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>DeepTCR is a deep learning framework for revealing sequence concepts within T-cell repertoires</article-title>. <source>Nat. Commun</source>., <volume>12</volume>, <fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">33397941</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Springer</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Prediction of specific TCR-peptide binding from large dictionaries of TCR-peptide pairs</article-title>. <source>Front. immunol.</source>, <volume>11</volume>, <fpage>1803</fpage>.<pub-id pub-id-type="pmid">32983088</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tickotsky</surname><given-names>N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Mcpas-TCR: a manually curated catalogue of pathology-associated t cell receptor sequences</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>2924</fpage>–<lpage>2929</lpage>.<pub-id pub-id-type="pmid">28481982</pub-id></mixed-citation>
    </ref>
    <ref id="btab294-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Weininger</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>1989</year>) <article-title>SMILES. 2. algorithm for generation of unique SMILES notation</article-title>. <source>J. Chem. Inf. Comput. Sci</source>., <volume>29</volume>, <fpage>97</fpage>–<lpage>101</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
