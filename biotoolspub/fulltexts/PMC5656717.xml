<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Adv Struct Chem Imaging</journal-id>
    <journal-id journal-id-type="iso-abbrev">Adv Struct Chem Imaging</journal-id>
    <journal-title-group>
      <journal-title>Advanced Structural and Chemical Imaging</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2198-0926</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5656717</article-id>
    <article-id pub-id-type="publisher-id">48</article-id>
    <article-id pub-id-type="doi">10.1186/s40679-017-0048-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A streaming multi-GPU implementation of image simulation algorithms for scanning transmission electron microscopy</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Pryor</surname>
          <given-names>Alan</given-names>
          <suffix>Jr.</suffix>
        </name>
        <address>
          <email>apryor6@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ophus</surname>
          <given-names>Colin</given-names>
        </name>
        <address>
          <email>cophus@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Miao</surname>
          <given-names>Jianwei</given-names>
        </name>
        <address>
          <email>miao@physics.ucla.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9632 6718</institution-id><institution-id institution-id-type="GRID">grid.19006.3e</institution-id><institution>Department of Physics and Astronomy and California NanoSystems Institute, </institution><institution>University of California at Los Angeles, </institution></institution-wrap>Los Angeles, CA 90095 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2231 4551</institution-id><institution-id institution-id-type="GRID">grid.184769.5</institution-id><institution>National Center for Electron Microscopy, Molecular Foundry, Lawrence Berkeley National Laboratory, </institution></institution-wrap>Berkeley, CA 94720 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>10</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>25</day>
      <month>10</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2017</year>
    </pub-date>
    <volume>3</volume>
    <issue>1</issue>
    <elocation-id>15</elocation-id>
    <history>
      <date date-type="received">
        <day>6</day>
        <month>7</month>
        <year>2017</year>
      </date>
      <date date-type="accepted">
        <day>13</day>
        <month>10</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Simulation of atomic-resolution image formation in scanning transmission electron microscopy can require significant computation times using traditional methods. A recently developed method, termed plane-wave reciprocal-space interpolated scattering matrix (PRISM), demonstrates potential for significant acceleration of such simulations with negligible loss of accuracy. Here, we present a software package called <italic>Prismatic</italic> for parallelized simulation of image formation in scanning transmission electron microscopy (STEM) using both the PRISM and multislice methods. By distributing the workload between multiple CUDA-enabled GPUs and multicore processors, accelerations as high as 1000 × for PRISM and 15 × for multislice are achieved relative to traditional multislice implementations using a single 4-GPU machine. We demonstrate a potentially important application of <italic>Prismatic</italic>, using it to compute images for atomic electron tomography at sufficient speeds to include in the reconstruction pipeline. <italic>Prismatic</italic> is freely available both as an open-source CUDA/C++ package with a graphical user interface and as a Python package, <italic>PyPrismatic</italic>.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Scanning transmission electron microscopy</kwd>
      <kwd>PRISM</kwd>
      <kwd>Multislice</kwd>
      <kwd>GPU</kwd>
      <kwd>CUDA</kwd>
      <kwd>Electron scattering</kwd>
      <kwd>Imaging simulation</kwd>
      <kwd>High performance computing</kwd>
      <kwd>Atomic electron tomography</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000015</institution-id>
            <institution>U.S. Department of Energy</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DE-AC02-05CH11231</award-id>
        <principal-award-recipient>
          <name>
            <surname>Pryor</surname>
            <given-names>Alan</given-names>
            <suffix>Jr.</suffix>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DMR 1548924</award-id>
        <principal-award-recipient>
          <name>
            <surname>Pryor</surname>
            <given-names>Alan</given-names>
            <suffix>Jr.</suffix>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2017</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par2">Scanning transmission electron microscopy (STEM) has had a major impact on materials science [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>], especially for atomic-resolution imaging since the widespread adoption of hardware aberration correction [<xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR5">5</xref>]. Many large scale STEM experimental techniques are routinely validated using imaging or diffraction simulations. Examples include electron ptychography [<xref ref-type="bibr" rid="CR6">6</xref>], 3D atomic reconstructions using dynamical scattering [<xref ref-type="bibr" rid="CR7">7</xref>], high precision surface atom position measurements on catalytic particles [<xref ref-type="bibr" rid="CR8">8</xref>], de-noising routines [<xref ref-type="bibr" rid="CR9">9</xref>], phase contrast imaging with phase plates [<xref ref-type="bibr" rid="CR10">10</xref>], new dynamical atomic contrast models [<xref ref-type="bibr" rid="CR11">11</xref>], atomic electron tomography (AET) [<xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR16">16</xref>], and many others. The most commonly employed simulation algorithm for STEM simulation is the multislice algorithm introduced by Cowlie and Moodie [<xref ref-type="bibr" rid="CR17">17</xref>]. This method consists of two main steps. The first is calculation of the projected potentials from all atoms into a series of 2D slices. Second, the electron wave is initialized and propagated through the sample. The multislice method is straightforward to implement and is quite efficient for plane-wave or single-probe diffraction simulations [<xref ref-type="bibr" rid="CR18">18</xref>].<table-wrap id="Tab1"><label>Table 1</label><caption><p>A non-exhaustive list of electron microscopy simulation codes</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Code(s)</th><th align="left">Author(s)</th><th align="left">Reference(s)</th><th align="left">Comments</th><th align="left">Links</th></tr></thead><tbody><tr><td align="left"><italic>xHREM</italic></td><td align="left">Ishizuka</td><td align="left">[<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]</td><td align="left"/><td align="left">HREM Simulation Suite</td></tr><tr><td align="left"><italic>computem</italic></td><td align="left">Kirkland</td><td align="left">[<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR21">21</xref>]</td><td align="left">CPU parallelized</td><td align="left">Computem Repo</td></tr><tr><td align="left"><italic>EMS, JEMS</italic></td><td align="left">Stadelmann</td><td align="left">[<xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR23">23</xref>]</td><td align="left"/><td align="left">JEMS website</td></tr><tr><td align="left"><italic>MacTempas</italic></td><td align="left">Kilaas</td><td align="left">[<xref ref-type="bibr" rid="CR24">24</xref>]</td><td align="left"/><td align="left">MacTempasX website</td></tr><tr><td align="left"><italic>QSTEM</italic></td><td align="left">Koch</td><td align="left">[<xref ref-type="bibr" rid="CR25">25</xref>]</td><td align="left"/><td align="left">QSTEM website</td></tr><tr><td align="left"><italic>CTEMsoft</italic></td><td align="left">De Graef</td><td align="left">[<xref ref-type="bibr" rid="CR26">26</xref>]</td><td align="left"/><td align="left">CTEMsoft repo</td></tr><tr><td align="left"><italic>Web-EMAPS</italic></td><td align="left">Zuo et al.</td><td align="left">[<xref ref-type="bibr" rid="CR27">27</xref>]</td><td align="left">Deprecated</td><td align="left">Status page</td></tr><tr><td align="left"><italic>STEM_CELL</italic></td><td align="left">Carlino, Grillo et al.</td><td align="left">[<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR29">29</xref>]</td><td align="left">CPU parallelized</td><td align="left">STEM_CELL website</td></tr><tr><td align="left"><italic>STEMSIM</italic></td><td align="left">Rosenauer and Schowalter</td><td align="left">[<xref ref-type="bibr" rid="CR30">30</xref>]</td><td align="left"/><td align="left">STEMSIM webpage</td></tr><tr><td align="left"><italic>MALTS</italic></td><td align="left">Walton et al.</td><td align="left">[<xref ref-type="bibr" rid="CR31">31</xref>]</td><td align="left">Lorentz TEM</td><td align="left"/></tr><tr><td align="left"><italic>Dr. Probe</italic></td><td align="left">Barthel and Houben</td><td align="left">[<xref ref-type="bibr" rid="CR32">32</xref>]</td><td align="left"/><td align="left">Dr. Probe website</td></tr><tr><td align="left"><italic>MULTEM</italic></td><td align="left">Lobato and Van Dyck</td><td align="left">[<xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>]</td><td align="left">GPU par., many modes</td><td align="left">MULTEM repo</td></tr><tr><td align="left"><italic>FDES</italic></td><td align="left">Van den Broek et al.</td><td align="left">[<xref ref-type="bibr" rid="CR35">35</xref>]</td><td align="left">Multi-GPU parallelized</td><td align="left">FDES repo</td></tr><tr><td align="left"><italic>μSTEM</italic></td><td align="left">D’Alfonso et al.</td><td align="left">[<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR37">37</xref>]</td><td align="left">GPU par., inelastic</td><td align="left">μSTEM website</td></tr><tr><td align="left"><italic>STEMsalabim</italic></td><td align="left">Oelerich et al.</td><td align="left">[<xref ref-type="bibr" rid="CR38">38</xref>]</td><td align="left">CPU parallelized</td><td align="left">STEMsalabim website</td></tr><tr><td align="left"><italic>Prismatic</italic></td><td align="left">Pryor Jr. and Ophus</td><td align="left">[<xref ref-type="bibr" rid="CR39">39</xref>], this work</td><td align="left">Multi-GPU streaming</td><td align="left"><italic>Prismatic</italic> website, repo</td></tr></tbody></table></table-wrap>
</p>
    <p id="Par3">A large number of electron microscopy simulation codes are available, summarized in Table <xref rid="Tab1" ref-type="table">1</xref>. Most of these codes use the multislice method, and many have implemented parallel processing algorithms for both central processing units (CPUs) and graphics processing units (GPUs). Recently, some authors have begun using hybrid CPU + GPU codes for multislice simulation [<xref ref-type="bibr" rid="CR40">40</xref>]. Multislice simulation relies heavily on the fast Fourier transform (FFT) which can be computed using heavily optimized packages for both CPUs [<xref ref-type="bibr" rid="CR41">41</xref>] and GPUs [<xref ref-type="bibr" rid="CR42">42</xref>]. The other primary computational requirement of multislice calculations is large element-wise matrix arithmetic, which GPUs are very well-suited to perform [<xref ref-type="bibr" rid="CR43">43</xref>]. Parallelization is important because STEM experiments may record full probe images or integrated values from thousands or even millions of probe positions [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR44">44</xref>]. Performing STEM simulations on the same scale as these experiments are very challenging because in the conventional multislice algorithm the propagation of each STEM probe through the sample is computed separately. Furthermore, if additional simulation parameters are explored, the number of required simulations can become even larger, requiring very large computation times even using a modern, parallelized implementation. To address this issue, we introduced a new algorithm called PRISM which offers a substantial speed increase for STEM image simulations [<xref ref-type="bibr" rid="CR39">39</xref>].</p>
    <p id="Par4">In this manuscript, we introduce a highly-optimized multi-GPU simulation code that can perform both multislice and PRISM simulations of extremely large structures called <italic>Prismatic</italic>. We will briefly describe the multislice and PRISM algorithms, and describe the implementation details for our parallelized CPU and CPU + GPU codes. We perform timing benchmarks to compare both algorithms under a variety of conditions. Finally, we demonstrate the utility of our new code with typical use cases and compare with the popular packages <italic>computem</italic> and <italic>MULTEM</italic> [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>]. <italic>Prismatic</italic> includes a graphical user interface (GUI) and uses the cross-platform build system CMake [<xref ref-type="bibr" rid="CR45">45</xref>]. All of the source code is freely available. Throughout this manuscript, we use the NVIDIA convention of referring to the CPU and GPU(s) as the host and device(s), respectively.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Description of algorithms</title>
      <p id="Par5">A flow chart of the steps performed in <italic>Prismatic</italic> is given in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Both multislice and PRISM share the same initial steps, where the sample is divided into slices which are used to compute the projected potential from the atomic scattering factors give in [<xref ref-type="bibr" rid="CR21">21</xref>]. This step is shown schematically in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, b, and is implemented by using a precomputed lookup table for each atom type [<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR39">39</xref>].<fig id="Fig1"><label>Fig. 1</label><caption><p>Flow chart of STEM simulation algorithm steps.<bold> a</bold> All atoms are separated into slices at different positions along the beam direction, and<bold> b</bold> atomic scattering factors are used to compute projected potential of each slice.<bold> c</bold> Multislice algorithm, where each converged probe is initialized,<bold> d</bold> propagated through each of the sample slices defined in (<bold>b</bold>), and then<bold> e</bold> output either as images, or radially integrated detectors.<bold> f</bold> PRISM algorithm where<bold> g</bold> converged probes are defined in coordinate system downsampled by factor <italic>f</italic> as a set of plane waves.<bold> h</bold> Each required plane wave is propagated through the sample slices defined in (<bold>b</bold>).<bold> i</bold> Output probes are computed by cropping subset of plane waves multiplied by probe complex coefficients, and<bold> j</bold> summed to form output probe,<bold> k</bold> which is then saved</p></caption><graphic xlink:href="40679_2017_48_Fig1_HTML" id="MO1"/></fig>
</p>
      <p id="Par6">Figure <xref rid="Fig1" ref-type="fig">1</xref>c–e show the steps in a multislice STEM simulation. First the complex electron wave <inline-formula id="IEq3"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Psi$$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="normal">Ψ</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq3.gif"/></alternatives></inline-formula> representing the initial converged probe is defined, typically as an Airy disk function shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>c. This choice of probe represents that of an idealized instrument with perfect, aberration-free lenses. This probe is positioned at the desired location on the sample surface in real space, as in Fig. <xref rid="Fig1" ref-type="fig">1</xref>d. Next, this probe is propagated through the sample’s potential slices defined in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b. This propagation is achieved by alternating two steps. The first step is a transmission through a given potential slice <inline-formula id="IEq4"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$V_p^\mathrm{{2D}}$$\end{document}</tex-math><mml:math id="M4"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq4.gif"/></alternatives></inline-formula> over the real space coordinates <inline-formula id="IEq5"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{r}$$\end{document}</tex-math><mml:math id="M6"><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq5.gif"/></alternatives></inline-formula>
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \psi _{p+1}(\overrightarrow {r}) = \psi _p(\overrightarrow{r}) \exp \left[ {i} \sigma V_p^\mathrm{{2D}} \left(\overrightarrow{r}\right) \right] , \end{aligned}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="italic">ψ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="italic">ψ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>exp</mml:mo><mml:mfenced close="]" open="[" separators=""><mml:mi>i</mml:mi><mml:mi mathvariant="italic">σ</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:msubsup><mml:mfenced close=")" open="(" separators=""><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mfenced></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="40679_2017_48_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq6"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M10"><mml:mi mathvariant="italic">σ</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq6.gif"/></alternatives></inline-formula> is the beam-sample interaction constant. Next, the electron wave is propagated over the distance <italic>t</italic> to the next sample potential slice, which is done in Fourier space over the Fourier coordinates <inline-formula id="IEq7"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{q}$$\end{document}</tex-math><mml:math id="M12"><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq7.gif"/></alternatives></inline-formula>
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \Psi _{p+1}(\overrightarrow{q}) = \Psi _p(\overrightarrow{q}) \exp (- {i} \pi \lambda |\overrightarrow{q} \, |^2 t), \end{aligned}$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="normal">Ψ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>exp</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>-</mml:mo><mml:mi>i</mml:mi><mml:mi mathvariant="italic">π</mml:mi><mml:mi mathvariant="italic">λ</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mover accent="true"><mml:mi>q</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mspace width="0.166667em"/><mml:mrow><mml:msup><mml:mo stretchy="false">|</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="40679_2017_48_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\lambda$$\end{document}</tex-math><mml:math id="M16"><mml:mi mathvariant="italic">λ</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq8.gif"/></alternatives></inline-formula> is the electron wavelength. These steps are alternated until the electron probe has been propagated through the entire sample. Next, the simulated output is computed, which is typically a subset of the probe’s intensity summed in Fourier space as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>e. For more details on the multislice method, we refer readers to Kirkland [<xref ref-type="bibr" rid="CR21">21</xref>]. The steps given in Fig. <xref rid="Fig1" ref-type="fig">1</xref>c–e are repeated for the desired probe positions, typically a 2D grid. Thus the simulation produces a 2D diffraction pattern at each position in the 2D scan grid, resulting in a 4D output, the size of which can become considerable. Many simulations only require counting the scattered electrons within some angular range, and optionally each of the 2D diffraction patterns can be integrated once azimuthally into radial bins representing the electrons scattered between a corresponding inner and outer angle at a given probe position, forming a 3D output. The 3D output is conceptually the same as having a large number of evenly-spaced virtual annular detectors. The 3D output can be further integrated radially between some inner and outer angle to produce a 2D output where a single pixel value is recorded for each probe position representing the total number of electrons scattered between the inner and outer virtual detector position, allowing formation of multiple simulated 2D images such a bright field (BF), high-angle annular dark field (HAADF), etc. The 4D, 3D, and 2D outputs can be independently turned on/off by the user.</p>
      <p id="Par7">The PRISM simulation method for STEM images is outlined in Fig. <xref rid="Fig1" ref-type="fig">1</xref>f–k. This method exploits the fact that an electron scattering simulation can be decomposed into an orthogonal basis set, as in the Bloch wave method [<xref ref-type="bibr" rid="CR21">21</xref>]. If we compute the electron scattering for a set of plane waves that forms a complete basis, these waves can each be multiplied by a complex scalar value and summed to give a desired electron probe. A detailed description of the PRISM algorithm is given in [<xref ref-type="bibr" rid="CR39">39</xref>].</p>
      <p id="Par8">The first step of PRISM is to compute the sample potential slices as in Fig.<xref rid="Fig1" ref-type="fig">1</xref>a, b. Next, a maximum input probe semi-angle and an interpolation factor <italic>f</italic> is defined for the simulation. Figure <xref rid="Fig1" ref-type="fig">1</xref>g shows how these two variables specify the plane wave calculations required for PRISM, where every<italic> f</italic>th plane wave in both spatial dimensions inside the maximum scattering angle is required. Each of these plane waves must be propagated through the sample using the multislice method given above, shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>h. Once all of these plane waves have been propagated through the sample, together they form the desired basis set we refer to as the compact <inline-formula id="IEq10"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}$$\end{document}</tex-math><mml:math id="M18"><mml:mi mathvariant="bold">S</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq10.gif"/></alternatives></inline-formula>-matrix. Next, we define the location of all desired STEM probes. For each probe, a subset of all plane waves is cut out around the maximum value of the input STEM probe. The size length of the subset regions is <italic>d</italic>/<italic>f</italic>, where <italic>d</italic> is the simulation cell length. The probe coefficients for all plane waves are complex values that define the center position of the STEM probe, and coherent wave aberrations such as defocus or spherical aberration. Each STEM probe is computed by multiplying every plane wave subset by the appropriate coefficient and summing all wave subsets. This is equivalent to using Fourier interpolation to approximate the electron probe wavefunction. In real space, this operation corresponds to cropping the probe from the full field of view, and as long as this subset region is large enough to encompass the vast majority of the probe intensity, the error in this approximation will be negligible [<xref ref-type="bibr" rid="CR39">39</xref>]. Thus, the key insight in the PRISM algorithm is that the real space probe decays to approximately zero rapidly and under many imaging simulation conditions is oversampled. Finally, the output signal is computed for all probes as above, giving a 2D, 3D or 4D output array. As will be shown below, STEM simulations using the PRISM method can be significantly faster than using the multislice method.</p>
    </sec>
  </sec>
  <sec id="Sec4">
    <title>Implementation details</title>
    <sec id="Sec5">
      <title>Computational model</title>
      <p id="Par9">Wherever possible, parallelizable calculations in <italic>Prismatic</italic> are divided into individual tasks and performed using a pool of CPU and GPU worker threads that asynchronously consume the work on the host or the device, respectively. We refer to a GPU worker thread as a host thread that manages work dispatched to a single device context. Whenever one of these worker threads is available, it queries a mutex-synchronized dispatcher that returns a unique work ID or range of IDs. The corresponding work is then consumed, and the dispatcher required until no more work remains. This computational model, depicted visually in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, provides maximal load balancing at essentially no cost, as workers are free to independently obtain work as often as they become available. Therefore, machines with faster CPUs may observe more work being performed on the host, and if multiple GPU models are installed in the same system their relative performance is irrelevant to the efficiency of work dispatch. The GPU workers complete most types of tasks used by <italic>Prismatic</italic> well over an order of magnitude faster than the CPU on modern hardware, and if a CPU worker is dispatched one of the last pieces of work then the entire program may be forced to unnecessarily wait on the slower worker to complete. Therefore, an adjustable early stopping mechanism is provided for the CPU workers.<fig id="Fig2"><label>Fig. 2</label><caption><p>Visualization of the computation model used repeatedly in the <italic>Prismatic</italic> software package, whereby a pool of GPU and CPU workers are assigned batches of work by querying a synchronized work dispatcher. Once the assignment is complete, the worker requests more work until no more exists. All workers record completed simulation outputs in parallel</p></caption><graphic xlink:href="40679_2017_48_Fig2_HTML" id="MO4"/></fig>
</p>
      <p id="Par10">GPU calculations in <italic>Prismatic</italic> are performed using a fully asynchronous memory transfer and computational model driven by CUDA streams. By default, kernel launches and calls to the CUDA runtime API for transferring memory occur on what is known as the default stream and subsequently execute in order. This serialization does not fully utilize the hardware, as it is possible to simultaneously perform a number of operations such as memory transfer from the host to the device, memory transfer from the device to the host, and kernel execution concurrently. This level of concurrency can be achieved using CUDA streams. Each CUDA stream represents an independent queue of tasks using a single device that execute internally in exact order, but that can be scheduled to run concurrently irrespective of other streams if certain conditions are met. This streaming model combined with the multithreaded work dispatch approach described previously allow for concurrent two-way host/device memory transfers and simultaneous data processing. A snapshot of the output produced by the NVIDA Visual Profiler for a single device context during a streaming multislice simulation similar to those described later in this work verifies that <italic>Prismatic</italic> is indeed capable of such concurrency (Fig. <xref rid="Fig3" ref-type="fig">3</xref>).<fig id="Fig3"><label>Fig. 3</label><caption><p><bold>a</bold> Sample profile of the GPU activities on a single NVIDIA GTX 1070 during a multislice simulation in streaming mode with<bold> b</bold> enlarged inset containing a window where computation is occurring on streams #1 and #5 while three separate arrays are simultaneously being copied on streams #2–4</p></caption><graphic xlink:href="40679_2017_48_Fig3_HTML" id="MO5"/></fig>
</p>
      <p id="Par11">To achieve maximum overlap of work, each CUDA-enabled routine in <italic>Prismatic</italic> begins with an initialization phase where relevant data on the host-side is copied into page-locked (also called “pinned”) memory, which provides faster transfer times to the device and is necessary for asynchronous memory copying as the system can bypass internal staging steps that would be necessary for pageable memory [<xref ref-type="bibr" rid="CR46">46</xref>]. CUDA streams and data buffers are then allocated on each device and copied to asynchronously. Read-only memory is allocated once per device, and read/write memory is allocated once per stream. It is important to perform all memory allocations initially, as any later calls to <italic>cudaMalloc</italic> will implicitly force synchronization of the streams. Once the initialization phase is over, a host thread is spawned for each unique CUDA stream and begins to consume work.</p>
    </sec>
    <sec id="Sec6">
      <title>Calculation of the projected potentials</title>
      <p id="Par12">Both PRISM and multislice require dividing the atomic coordinates into thin slices and computing the projected potential for each. The calculation details are described by Kirkland and require evaluation of modified Bessel functions of the second kind, which are computationally expensive [<xref ref-type="bibr" rid="CR21">21</xref>]. This barrier is overcome by precomputing the result for each unique atomic species and assembling a lookup table. Each projected potential is calculated on an 8 × supersampled grid, integrated, and cached. Currently, this grid is defined as a regularly spaced rectangular grid, but in future releases additional grid selections may become available. The sample volume is then divided into slices, and the projected potential for each slice is computed on separate CPU threads using the cached potentials. In principle, this step could be GPU accelerated, but even for a large sample with several hundred thousand atoms the computation time is on the order of seconds and is considered negligible.</p>
    </sec>
    <sec id="Sec7">
      <title>PRISM probe simulations</title>
      <p id="Par13">Following calculation of the projected potential, the next step of PRISM is to compute the compact <inline-formula id="IEq11"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}$$\end{document}</tex-math><mml:math id="M20"><mml:mi mathvariant="bold">S</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq11.gif"/></alternatives></inline-formula>-matrix. Each plane wave component is repeatedly transmitted and propagated through each slice of the potential until it has passed through the entire sample, at which point the complex-valued output wave is stored in real space to form a single layer of the compact <inline-formula id="IEq12"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}$$\end{document}</tex-math><mml:math id="M22"><mml:mi mathvariant="bold">S</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq12.gif"/></alternatives></inline-formula>-matrix. This step of PRISM is highly analogous to multislice except whereas multislice requires propagating/transmitting the entire probe simultaneously, in PRISM each initial Fourier component is propagated/transmitted individually. The advantage is that in PRISM this calculation must only be performed once per Fourier component for the entire calculation, while in multislice it must be repeated entirely at every probe position. Thus, in many sample geometries the PRISM algorithm can significantly out-perform multislice despite the overhead of the <inline-formula id="IEq13"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}$$\end{document}</tex-math><mml:math id="M24"><mml:mi mathvariant="bold">S</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq13.gif"/></alternatives></inline-formula>-matrix calculation [<xref ref-type="bibr" rid="CR39">39</xref>].</p>
      <p id="Par14">The propagation step requires a convolution operation which can be performed efficiently through use of the FFT. Our implementation uses the popular FFTW and cuFFT libraries for the CPU and GPU implementations, respectively [<xref ref-type="bibr" rid="CR41">41</xref>, <xref ref-type="bibr" rid="CR42">42</xref>]. Both of these libraries support batch FFTs, whereby multiple Fourier transforms of the same size can be computed simultaneously. This allows for reuse of intermediate twiddle factors, resulting in a faster overall computation than performing individual transforms one-by-one at the expense of requiring a larger block of memory to hold the multiple arrays. <italic>Prismatic</italic> uses this batch FFT method with both PRISM and multislice, and thus each worker thread will actually propagate a number of plane waves or probes simultaneously. This number, called the <italic>batch_size</italic>, may be tuned by the user to potentially enhance performance at the cost of using additional memory, but sensible defaults are provided.</p>
      <p id="Par15">In the final step of PRISM, a 2D output is produced for each probe position by applying coefficients, one for each plane wave, to the elements of the compact <inline-formula id="IEq14"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}$$\end{document}</tex-math><mml:math id="M26"><mml:mi mathvariant="bold">S</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq14.gif"/></alternatives></inline-formula>-matrix and summing along the dimension corresponding to the different plane waves. These coefficients correspond to Fourier phase shifts that scale and translate each plane wave to the relevant location on the sample in real space. The phase coefficients, which are different for each plane wave but constant for a given probe position, are precomputed and stored in global memory. Each threadblock on the device first reads the coefficients from global memory into shared memory, where they can be reused throughout the lifetime of the threadblock. Components of the compact <inline-formula id="IEq15"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}$$\end{document}</tex-math><mml:math id="M28"><mml:mi mathvariant="bold">S</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq15.gif"/></alternatives></inline-formula>-matrix for a given output wave position are then read from global memory, multiplied by the relevant coefficient, and stored in fast shared memory, where the remaining summation is performed. This parallel sum-reduction is performed using a number of well-established optimization techniques including reading multiple global values per thread, loop unrolling through template specialization, and foregoing of synchronization primitives when the calculation has been reduced to the single-warp level [<xref ref-type="bibr" rid="CR47">47</xref>]. Once the real space exit-wave has been computed, the modulus squared of its FFT yields the calculation result at the detector plane.</p>
    </sec>
    <sec id="Sec8">
      <title>Multislice probe simulations</title>
      <p id="Par16">The implementation of multislice is fairly straightforward. The initial probe is translated to the probe position of interest, and then is alternately transmitted and propagated through the sample. In practice, this is accomplished by alternating forward and inverse Fourier transforms with an element-wise complex multiplication in between each with either the transmission or propagation functions. Upon propagation through the entire sample, the squared intensity of the Fourier transform of the exit-wave provides the final result of the calculation at the detector plane for that probe position. For additional speed, the FFTs of many probes are computed simultaneously in batch mode. Thus in practice <italic>batch_size</italic> probes are transmitted, followed by a batch FFT, then propagated, followed by a batch inverse FFT, etc.</p>
    </sec>
    <sec id="Sec9">
      <title>Streaming data for very large simulations</title>
      <p id="Par17">The preferred way to perform PRISM and multislice simulations is to transfer large data structures such as the projected potential array or the compact <inline-formula id="IEq16"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}$$\end{document}</tex-math><mml:math id="M30"><mml:mi mathvariant="bold">S</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq16.gif"/></alternatives></inline-formula>-matrix to each GPU only once, where they can then be read from repeatedly over the course of the calculation. However, this requires that the arrays fit into limited GPU memory. For simulations that are too large, we have implemented an asynchronous streaming version of both PRISM and multislice. Instead of allocating and transferring a single read-only copy of large arrays, buffers are allocated to each stream large enough to hold only the relevant subset of the data for the current step in the calculation, and the job itself triggers asynchronous streaming of the data it requires for the next step. For example, in the streaming implementation of multislice, each stream possesses a buffer to hold a single slice of the potential array and after transmission through that slice, the transfer of the next slice is requested. The use of asynchronous memory copies and CUDA streams permits the partial hiding of memory transfer latencies behind computation (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Periodically, an individual stream must wait on data transfer before it can continue, but if another stream is ready to perform work the device is effectively kept busy. Doing so is critical for performance, as the amount of time needed to transfer data can become significant relative to the total calculation. By default, <italic>Prismatic</italic> uses an automatic setting to determine whether to use the single-transfer or streaming memory model whereby the input parameters are used to estimate how much memory will be consumed on the device, and if this estimate is too large compared with the available device memory then streaming mode is used. This estimation is conservative and is intended for convenience, but users can also forcibly set either memory mode.</p>
    </sec>
    <sec id="Sec10">
      <title>Launch configuration</title>
      <p id="Par18">All CUDA kernels are accompanied by a launch configuration that determines how the calculation will be carried out [<xref ref-type="bibr" rid="CR46">46</xref>]. The launch configuration specifies the amount of shared memory needed, on which CUDA stream to execute the computation, and defines a 3D grid of threadblocks, each of which contains a 3D arrangement of CUDA threads. It is this arrangement of threads and threadblocks that must be managed in software to perform the overall calculation. The choice of launch configuration can have a significant impact on the overall performance of a CUDA application as certain GPU resources, such as shared memory, are limited. If too many resources are consumed by individual threadblocks, the total number of blocks that run concurrently can be negatively affected, reducing overall concurrency. This complexity of CUDA cannot be overlooked in a performance-critical application, and we found that the speed difference in a suboptimal and well-tuned launch configuration could be as much as 2–3 x.</p>
      <p id="Par19">In the reduction step of PRISM, there are several competing factors that must be considered when choosing a launch configuration. The first of these is the threadblock size. The compact <inline-formula id="IEq17"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}$$\end{document}</tex-math><mml:math id="M32"><mml:mi mathvariant="bold">S</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq17.gif"/></alternatives></inline-formula>-matrix is arranged in memory such that the fastest changing dimension, considered to be the<italic> x</italic>-axis, lies along the direction of the different plane waves. Therefore to maximize memory coalescence, threadblocks are chosen to be as large as possible in the <italic> x</italic>-direction. Usually the result will be threadblocks that are effectively 1D, with <inline-formula id="IEq18"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BlockSize_y$$\end{document}</tex-math><mml:math id="M34"><mml:mrow><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq18.gif"/></alternatives></inline-formula> and <inline-formula id="IEq19"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BlockSize_z$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq19.gif"/></alternatives></inline-formula> equal to one; however, in cases where very few plane waves need to be computed, the blocks may be extended in<italic> y</italic> and<italic> z</italic> to prevent underutilization of the device. To perform the reduction, two arrays of shared memory are used. The first is dynamically sized and contains as many elements as there are plane waves. This array is used to cache the phase shift coefficients to prevent unnecessary reads from global memory, which are slow. The second array has <inline-formula id="IEq20"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BlockSize_x$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq20.gif"/></alternatives></inline-formula> * <inline-formula id="IEq21"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BlockSize_y$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq21.gif"/></alternatives></inline-formula> * <inline-formula id="IEq22"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BlockSize_z$$\end{document}</tex-math><mml:math id="M42"><mml:mrow><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq22.gif"/></alternatives></inline-formula> elements and is where the actual reduction is performed. Each block of threads steps through the array of phase shifts once and reads them into shared memory. Then the block contiguously steps through the elements of the compact <inline-formula id="IEq23"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathbf {S}$$\end{document}</tex-math><mml:math id="M44"><mml:mi mathvariant="bold">S</mml:mi></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq23.gif"/></alternatives></inline-formula>-matrix for a different exit-wave position at each<italic> y</italic> and<italic> z</italic> index, reading values from global memory, multiplying them by the associated coefficient, and accumulating them in the second shared memory array. Once all of the plane waves have been accessed, the remaining reduction occurs quickly as all remaining operations occur in fast shared memory. Each block of threads will repeat this process for many exit-wave positions which allows efficient reuse of the phase coefficients from shared memory. The parallel reduction is performed by repeatedly splitting each array in half and adding one half to the other until only one value remains. Consequently, if the launch configuration specifies too many threads along the<italic> x</italic>-direction, then many of them will become idle as the reduction proceeds, which wastes work. Conversely, choosing <inline-formula id="IEq24"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BlockSize_x$$\end{document}</tex-math><mml:math id="M46"><mml:mrow><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq24.gif"/></alternatives></inline-formula> to be too small is problematic for shared memory usage, as the amount of shared memory per block for the phase coefficients is constant regardless of the block size. In this case, the amount of shared memory available will rapidly become the limiting factor to the achievable occupancy. A suitably balanced block size produces the best results.</p>
      <p id="Par20">The second critical component of the launch configuration is the number of blocks to launch. Each block globally reads the phase coefficients once and then reuses them, which favors using fewer blocks and having each compute more exit-wave positions. However, if too few blocks are launched, the device may not reach full occupancy. The theoretically optimal solution would be to launch the minimal amount of blocks needed to saturate the device and no more.</p>
      <p id="Par21">Considering these many factors, <italic>Prismatic</italic> uses the following heuristic to choose a good launch configuration. At runtime, the properties of the available devices are queried, which includes the maximum number of threads per threadblock, the total amount of shared memory, and the total number of streaming multiprocessors. <inline-formula id="IEq25"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BlockSize_x$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq25.gif"/></alternatives></inline-formula> is chosen to be either the largest power of two smaller than the number of plane waves or the maximum number of threads per block, whichever is smaller. The total number of threadblocks that can run concurrently on a single streaming multiprocessor is then estimated using <inline-formula id="IEq26"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BlockSize_x$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>k</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:msub><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq26.gif"/></alternatives></inline-formula>, the limiting number of threads per block, and the limiting number of threadblocks per streaming multiprocessor. The total number of threadblocks across the entire device is then estimated as this number times the total number of streaming multiprocessors, and then the grid dimensions of the launch configuration are set to create three times this many blocks, where the factor of three is a fudge factor that we found produces better results.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Benchmarks</title>
    <sec id="Sec12">
      <title>Algorithm comparison</title>
      <p id="Par22">
        <fig id="Fig4">
          <label>Fig. 4</label>
          <caption>
            <p>Comparison of the CPU/GPU implementations of the PRISM and multislice algorithms described in this work. A 100 × 100 × 100 Å amorphous carbon cell was divided slices of varying thickness and sampled with progressively smaller pixels in real space corresponding to digitized probes of array size 256 × 256, 512 × 512, 1024 × 1024, and 2048 × 2048, respectively. Two different PRISM simulations are shown, a more accurate case where the interpolation factor <inline-formula id="IEq28"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=4$$\end{document}</tex-math><mml:math id="M52"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq28.gif"/></alternatives></inline-formula> (left), and a faster case with <inline-formula id="IEq29"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=16$$\end{document}</tex-math><mml:math id="M54"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq29.gif"/></alternatives></inline-formula> (right). The multislice simulation is the same for both columns. Power laws were fit of the form <inline-formula id="IEq30"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A + B \, {q_\mathrm{{max}}}^n$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mspace width="0.166667em"/><mml:msup><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="normal">max</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq30.gif"/></alternatives></inline-formula> where possible. The asymptotic power laws for higher scattering angles are shown on the right of each curve</p>
          </caption>
          <graphic xlink:href="40679_2017_48_Fig4_HTML" id="MO6"/>
        </fig>
      </p>
      <p id="Par23">A total of four primary algorithms are implemented <italic>Prismatic</italic>, as there are optimized CPU and GPU implementations of both PRISM and multislice simulation. To visualize the performance of the different algorithms, we performed a number of benchmarking simulations spanning a range of sample thicknesses, sizes, and with varying degrees of sampling. Using the average density of amorphous carbon, an atomic model corresponding to a 100 × 100 × 100 Å carbon cell was constructed and used for image simulation with various settings for slice thickness and pixel sampling. The results of this analysis are summarized in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. These benchmarks are plotted as a function of the maximum scattering angle <inline-formula id="IEq32"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$q_\mathrm{{max}}$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="normal">max</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq32.gif"/></alternatives></inline-formula>, which varies inversely to the pixel size.</p>
      <p id="Par24">The difference in computation time <italic>t</italic> shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref> between traditional CPU multislice and GPU PRISM is stark, approximately four orders of magnitude for the “fast” setting where <inline-formula id="IEq33"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=16$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq33.gif"/></alternatives></inline-formula>, and still more than a factor of 500 for the more accurate case of <inline-formula id="IEq34"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=4$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq34.gif"/></alternatives></inline-formula>. For both PRISM and multislice, the addition of GPU acceleration increases speed by at least an order of magnitude. Note that as the thickness of the slices is decreased, the relative gap between PRISM and multislice grows, as probe calculation in PRISM does not require additional propagation through the sample. We have also fit curves of the form<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} t = A + B \, {q_\mathrm{{max}}}^n, \end{aligned}$$\end{document}</tex-math><mml:math id="M64" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mspace width="0.166667em"/><mml:msup><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="normal">max</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="40679_2017_48_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> and <italic>B</italic> are prefactors and <italic>n</italic> is the asymptotic power law for high scattering angles. We observed that most of the simulation types approximately approach <inline-formula id="IEq35"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n=2$$\end{document}</tex-math><mml:math id="M66"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq35.gif"/></alternatives></inline-formula>, which is unsurprising for both PRISM and multislice. The limiting operation in PRISM is matrix-scalar multiplication, which depends on the array size and varies as <inline-formula id="IEq36"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${q_\mathrm{{max}}}^2$$\end{document}</tex-math><mml:math id="M68"><mml:msup><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="normal">max</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq36.gif"/></alternatives></inline-formula>. For multislice, the computation is a combination of multiplication operations and FFTs, and the theoretical <inline-formula id="IEq37"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal {O}(n\log {}n)$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>log</mml:mo><mml:mrow/><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq37.gif"/></alternatives></inline-formula> scaling of the latter is only slightly larger than 2, and thus the trendline is an approximate lower bound. The only cases that fall significantly outside the <inline-formula id="IEq38"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n=2$$\end{document}</tex-math><mml:math id="M72"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq38.gif"/></alternatives></inline-formula> regime were the multislice GPU simulations with the largest slice separation (20 Å) and the “fast” PRISM GPU simulations where <inline-formula id="IEq40"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=16$$\end{document}</tex-math><mml:math id="M74"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq40.gif"/></alternatives></inline-formula>. These calculations are sufficiently fast that the relatively small overhead required to compute the projected potential slices, allocate data, etc., is actually a significant portion of the calculation, resulting in apparent scaling better than <inline-formula id="IEq41"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${q_\mathrm{{max}}}^2$$\end{document}</tex-math><mml:math id="M76"><mml:msup><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="normal">max</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq41.gif"/></alternatives></inline-formula>. For the <inline-formula id="IEq42"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=16$$\end{document}</tex-math><mml:math id="M78"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq42.gif"/></alternatives></inline-formula> PRISM case, we observed approximately <inline-formula id="IEq43"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${q_\mathrm{{max}}}^{0.6}$$\end{document}</tex-math><mml:math id="M80"><mml:msup><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="normal">max</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mn>0.6</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq43.gif"/></alternatives></inline-formula> scaling, which translates into sub-millisecond calculation times per probe even with small pixel sizes and slice thicknesses.</p>
      <p id="Par25">To avoid unnecessarily long computation times for the many simulations, particularly multislice, different numbers of probe positions were calculated for each algorithm, and thus we report the benchmark as time per probe. Provided enough probe positions are calculated to obviate overhead of computing the projected potential and setting up the remainder of the calculation, there is a linear relationship between the number of probe positions calculated and the calculation time for all of the algorithms, and computing more probes will not change the time per probe significantly. Here, this overhead is only on the order of 10 s or fewer, and the reported results were obtained by computing 128 × 128 probes for PRISM CPU and multislice CPU, 512 × 512 for multislice GPU, and 2048 × 2048 for PRISM GPU. All of these calculations used the single-transfer memory implementations and were run on compute nodes with dual Intel Xeon E5-2650 processors, four Tesla K20 GPUs, and 64GB RAM from the VULCAN cluster within the Lawrence Berkeley National Laboratory Supercluster.</p>
    </sec>
    <sec id="Sec13">
      <title>Hardware scaling</title>
      <p id="Par26">Modern high performance computing is dominated by parallelization. At the time of this writing, virtually all desktop CPUs contain at least four cores, and high end server CPUs can have as many as twenty or more [<xref ref-type="bibr" rid="CR48">48</xref>]. Even mobile phones have begun to routinely ship with multicore processors [<xref ref-type="bibr" rid="CR49">49</xref>]. In addition to powerful CPUs, GPUs, and other types of coprocessors such as the Xeon Phi [<xref ref-type="bibr" rid="CR50">50</xref>] can be used to accelerate parallel algorithms. It, therefore, is becoming increasingly important to write parallel software that fully utilizes the available computing resources.</p>
      <p id="Par27">To demonstrate how the algorithms implemented in <italic>Prismatic</italic> scale with hardware, we performed the following simulation. Simulated images of a 100 × 100 × 100 Å amorphous carbon cell were produced with both PRISM and multislice using 5 Å thick slices, pixel size 0.1 Å, 20 mrad probe convergence semi-angle, and 80 keV electrons. This simulation was repeated using varying numbers of CPU threads and GPUs. As before, a varying number of probes were computed for each algorithm, specifically 2048 × 2048 for GPU PRISM, 512 × 512 for CPU PRISM and GPU multislice, and 64 × 64 for CPU multislice. This simulation utilized the same 4-GPU VULCAN nodes described previously. The results of this simulation are summarized in Fig. <xref rid="Fig5" ref-type="fig">5</xref>.</p>
      <p id="Par28">The ideal behavior for the CPU-only codes would be to scale as 1/<italic>x</italic> with the number of CPU cores utilized such that doubling the number of cores also approximately doubles the calculation speed. Provided that the number of CPU threads spawned is not greater than the number of cores, the number of CPU threads can effectively be considered the number of CPU cores utilized, and this benchmark indicates that both CPU-only PRISM and multislice possess close to ideal scaling behavior with number of CPU cores available.</p>
      <p id="Par29">The addition of a single GPU improves both algorithms by approximately a factor of 8 in this case, but in general, the relative improvement varies depending on the quality and number of the CPUs vs GPUs. The addition of a second GPU improves the calculation speed by a further factor of 1.8–1.9 with 14 threads, and doubling the number of GPUs to four total improves again by a similar factor. The reason that this factor is less than two is because the CPU is doing a nontrivial amount of work alongside the GPU. This claim is supported by the observation that when only using two threads the relative performance increase is almost exactly a factor of two when doubling the number of GPUs. We conclude that our implementations of both algorithms scale very well with available hardware, and potential users should be confident that investing in additional hardware, particularly GPUs, will benefit them accordingly.<fig id="Fig5"><label>Fig. 5</label><caption><p>Comparison of the implementations of multislice and PRISM for varying combinations of CPU threads and GPUs. The simulation was performed on a 100 × 100 × 100 Å amorphous carbon cell with 5 Å thick slices, 0.1 Å pixel size, and 20 mrad probe convergence semi-angle. All simulations were performed on compute nodes with dual Intel Xeon E5-2650 processors, four Tesla K20 GPUs, and 64 GB RAM. Calculation time of rightmost data point is labeled for all curves</p></caption><graphic xlink:href="40679_2017_48_Fig5_HTML" id="MO8"/></fig>
</p>
    </sec>
    <sec id="Sec14">
      <title>Data streaming/single-transfer benchmark</title>
      <p id="Par30">For both PRISM and multislice, <italic>Prismatic</italic> implements two different memory models, a single-transfer method where all data is copied to the GPU a single time before the main computation begins and a streaming mode where asynchronous copying of the required data is triggered across multiple CUDA streams as it is needed throughout the computation. Streaming mode reduces the peak memory required on the device at the cost of redundant copies; however, the computational cost of this extra copying can be reduced by hiding the transfer latency behind compute kernels and other copies (Fig. <xref rid="Fig3" ref-type="fig">3</xref>).</p>
      <p id="Par31">To compare the implementations of these two memory models in <italic>Prismatic</italic>, a number of amorphous carbon cells of increasing sizes were used as input to simulations using 80 keV electrons, 20 mrad probe convergence semi-angle, 0.1 Å pixel size, 4 Å slice thickness, and 0.4 Å probe steps. Across a range of simulation cell sizes, the computation time of the streaming vs. single-transfer versions of each code are extremely similar while the peak memory may be reduced by an order of magnitude or more (Fig. <xref rid="Fig6" ref-type="fig">6</xref>). For the streaming calculations, memory copy operations may become significant relative to the computational work (Fig. <xref rid="Fig3" ref-type="fig">3</xref>); however, this can be alleviated by achieving multi-stream concurrency.<fig id="Fig6"><label>Fig. 6</label><caption><p>Comparison of <bold>a</bold> relative performance and<bold> b</bold> peak memory consumption for single-transfer and streaming implementations of PRISM and multislice</p></caption><graphic xlink:href="40679_2017_48_Fig6_HTML" id="MO9"/></fig>
</p>
    </sec>
    <sec id="Sec15">
      <title>Comparison to existing methods</title>
      <p id="Par32">
        <fig id="Fig7">
          <label>Fig. 7</label>
          <caption>
            <p>Comparison of simulation results produced by <bold>a</bold> <italic>computem</italic>, <bold>b</bold>
<italic>MULTEM</italic>, and <bold>c</bold>–<bold>g</bold> <italic>Prismatic</italic>. The sample is composed of 27 × 27 × 27 pseudocubic perovskite unit cells, and images were simulated using 80 keV electrons, a 20 mrad probe convergence semi-angle, 0 Å defocus, and 1520 × 1536 pixel sampling for the probe and projected potential. A total of 512 × 512 probe positions were computed and the final images are an average over 64 frozen phonon configurations. Separate PRISM simulations were performed with interpolation factors 4, 8, 12, and 16. Line scans corresponding to the positions of the red/blue arrows are shown in the right-hand column. As the various simulations produce results with differing absolute intensity scales, all images were scaled to have the same mean intensity as <italic>Prismatic</italic> multislice</p>
          </caption>
          <graphic xlink:href="40679_2017_48_Fig7_HTML" id="MO10"/>
        </fig>
      </p>
      <p id="Par33">All previous benchmarks in this work have measured the speed of the various algorithms included in <italic>Prismatic</italic> against each other; however, relative metrics are largely meaningless without an external reference both in terms of overall speed and resulting image quality. To this end, we also performed STEM simulations of significant size and compare the results produced by the algorithms in <italic>Prismatic</italic>, the popular CPU package <italic>computem</italic>, and a newer GPU multislice code, <italic>MULTEM</italic>. [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>].</p>
      <p id="Par34">We have chosen a simulation cell typical of those used in structural atomic-resolution STEM studies, a complex Ruddlesden–Popper (RP) layered oxide. The RP structure we used contains nine pseudocubic unit cells of perovskite strontium titanate structure, with two stacking defects every 4.5 1 × 1 cells that modify the composition and atomic coordinates. The atomic coordinates of this cell were refined using density functional theory and were used for very large scale STEM image simulations [<xref ref-type="bibr" rid="CR51">51</xref>]. This 9 × 1 × 1 unit cell was tiled 3 × 27 × 27 times resulting in final sample approximately 10.5 nm cubed with more than 850,000 atoms.</p>
      <p id="Par35">Simulations were performed with multislice as implemented in <italic>computem</italic> (specifically using the <italic>autostem</italic> module), multislice in <italic>MULTEM</italic>, multislice in <italic>Prismatic</italic>, and the PRISM method with <italic>f</italic> values of 4, 8, 12, and 16 using 80 keV electrons, 1520 × 1536 pixel sampling, 20 mrad probe convergence semi-angle, and 5 Å thick potential slices. All simulations used Kirkland’s method for calculating the projected potential. A total of 512 × 512 evenly-spaced probes were computed for each simulation, and a total of 64 frozen phonon configurations were averaged to produce the final images, which are summarized in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. The <italic>Prismatic</italic> and <italic>MULTEM</italic> simulations were run on the VULCAN GPU nodes while <italic>computem</italic> simulations utilized better VULCAN CPU nodes with dual Intel Xeon E5-2670v2 CPUs and 64 GB RAM.</p>
      <p id="Par36">The mean computation time per frozen phonon for the <italic>computem</italic> simulations were 18.2 h resulting in a total computation time of 48.5 days. The acceleration made with GPU usage in <italic>MULTEM</italic> may seem to be fairly modest, but this is mostly due to the nature of the hardware and deserves some clarification. The version of <italic>MULTEM</italic> available at the time of this writing only can utilize one GPU and does not simultaneously use the CPU. On a quad-core desktop workstation, one may expect a single GPU to calculate FFTs somewhere between 4 and 10 × faster than on the CPU, but the server nodes used for these simulations possess up to 20 cores, which somewhat closes the gap between the two hardware types. On a workstation, one would expect <italic>MULTEM</italic> to perform better relative to <italic>computem</italic>. We note that this is through no fault of <italic>computem</italic>, which is itself a well-optimized code. It simply runs without the benefit of GPU acceleration. <italic>MULTEM</italic> is an ongoing project and provides additional flexibility such as alternate methods of computing the projected potential, and our intention is not to discount the value of these other simulation packages based purely on performance metrics.</p>
      <p id="Par37">As described previously, <italic>Prismatic</italic> is capable of utilizing multiple GPUs and CPU cores simultaneously, and the use of <italic>Prismatic</italic> CPU + GPU multislice code here provides an acceleration of about 11 × relative to <italic>computem</italic>, reducing the computation from 7 weeks to just over 4 days. The PRISM <inline-formula id="IEq55"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=4$$\end{document}</tex-math><mml:math id="M82"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq55.gif"/></alternatives></inline-formula> simulation is almost indistinguishable from the multislice results, and gives a 13 × speed-up over our GPU multislice simulation. For the <inline-formula id="IEq56"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=8$$\end{document}</tex-math><mml:math id="M84"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq56.gif"/></alternatives></inline-formula> PRISM simulation an additional 6 × improvement is achieved, requiring just over an hour of computation time with very similar resulting image quality. The <inline-formula id="IEq57"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=12$$\end{document}</tex-math><mml:math id="M86"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq57.gif"/></alternatives></inline-formula> and <inline-formula id="IEq58"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=16$$\end{document}</tex-math><mml:math id="M88"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq58.gif"/></alternatives></inline-formula> PRISM results show moderate and substantial intensity deviations from the ideal result, respectively, but require just tens of seconds per frozen phonon configuration. The intensity differences may be quantitatively visualized in the line scans on the right column of Fig. <xref rid="Fig7" ref-type="fig">7</xref>. The total difference in acceleration from CPU multislice to the fastest PRISM simulation shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref> is over three orders of magnitude. The importance of choosing a suitable value for the PRISM interpolation factor is evident by the artifacts introduced for <inline-formula id="IEq59"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=12$$\end{document}</tex-math><mml:math id="M90"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq59.gif"/></alternatives></inline-formula> and <inline-formula id="IEq60"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=16$$\end{document}</tex-math><mml:math id="M92"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq60.gif"/></alternatives></inline-formula> where the real space probe is cropped too heavily. The <italic>Prismatic</italic> GUI provides an interactive way to compute individual probes with PRISM and multislice to tune the parameters before running a full calculation. Ultimately, the user’s purpose dictates what balance of speed and accuracy is appropriate, but the important point is that calculations that previously required days or weeks on a computer cluster may now be performed on a single workstation in a fraction of the time.</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Application to atomic electron tomography</title>
    <p id="Par38">
      <fig id="Fig8">
        <label>Fig. 8</label>
        <caption>
          <p>Images from one projection of an atomic electron tomography tilt series of a FePt nanoparticle [<xref ref-type="bibr" rid="CR14">14</xref>], from<bold> a</bold> experiment,<bold> b</bold> linear projection of the reconstruction,<bold> c</bold> multislice simulation, and<bold> d</bold>–<bold>f</bold> PRISM simulations for <inline-formula id="IEq61"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=8$$\end{document}</tex-math><mml:math id="M94"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq61.gif"/></alternatives></inline-formula>, 16, and 32, respectively.<bold> g</bold> Relative root-mean-square error of the images in (<bold>b</bold>–<bold>f</bold>) relative to (<bold>a</bold>).<bold> h</bold> Calculation times per frozen phonon configuration for (<bold>c</bold>–<bold>f</bold>). All simulations performed with <italic>Prismatic</italic>
</p>
        </caption>
        <graphic xlink:href="40679_2017_48_Fig8_HTML" id="MO11"/>
      </fig>
    </p>
    <p id="Par39">One potentially important application of STEM image simulations is AET experiments. One of the ADF-STEM images from an atomic-resolution tilt series of a FePt nanoparticle [<xref ref-type="bibr" rid="CR14">14</xref>] is shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>a, with the corresponding linear projection from the 3D reconstruction shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>b. In this study and others, we have used multislice simulations to validate the tomographic reconstructions and estimate both the position and chemical identification errors [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>]. One such multislice simulation is given in Fig. <xref rid="Fig8" ref-type="fig">8</xref>c. This simulation was performed at 300 kV using a 30 mrad STEM probe, with a simulation pixel size of 0.0619 Å and a spacing between adjacent probes of 0.3725 Å. The image results shown are for 16 frozen phonon configurations using a 41–159 mrad annular dark field detector. This experimental dataset includes some postprocessing and was obtained freely online [<xref ref-type="bibr" rid="CR14">14</xref>].</p>
    <p id="Par40">The 3D reconstruction algorithm we have used, termed GENeralized Fourier Iterative REconstruction (GENFIRE), assumes that the projection images are linearly related to the potential of the reconstruction [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR52">52</xref>]. This assumption was sufficient for atomic-resolution tomographic reconstruction, but the measured intensity has some non-linear dependence on the atomic potentials, due to effects such as exponential decrease of electrons in the unscattered STEM probe, channeling effects along atomic columns, coherent diffraction at low scattering angles, and other related effects [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR53">53</xref>–<xref ref-type="bibr" rid="CR58">58</xref>]. These effects can be seen in the differences between the images shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>b, c. The multislice simulation image shows sharper atomic columns, likely due to the channeling effect along atomic columns that are aligned close to the beam direction [<xref ref-type="bibr" rid="CR55">55</xref>]. Additionally, there are mean intensity differences between the center part of the particle (thickest region) and the regions closed to the surfaces in projection (thinnest regions). Including these dynamical scattering effects in the reconstruction algorithm would increase the accuracy of the reconstruction.</p>
    <p id="Par41">However, Fig. <xref rid="Fig8" ref-type="fig">8</xref>h shows that the computation time for the multislice simulation is prohibitively high. Even using the <italic>Prismatic</italic> GPU code, each frozen phonon configuration for multislice require almost 7 h. Using 16 configurations and simulating all 65 projection angles would require months of simulation time, or massively parallel simulation on a super cluster. An alternative is to use the PRISM algorithm for the image simulations, shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>d, e and f for interpolation factors of <inline-formula id="IEq64"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=8$$\end{document}</tex-math><mml:math id="M96"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq64.gif"/></alternatives></inline-formula>, 16 and 32, respectively. Figure <xref rid="Fig8" ref-type="fig">8</xref>g shows the relative errors of Fig. <xref rid="Fig8" ref-type="fig">8</xref>b–f, where the error is defined by the root-mean-square of the intensity difference with the experimental image in Fig. <xref rid="Fig8" ref-type="fig">8</xref>a, divided by the root-mean-square of the experimental image. Unsurprisingly, the linear projection shows the lowest error since it was calculated directly from the 3D reconstruction built using the experimental data. The multislice and PRISM <inline-formula id="IEq65"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=8$$\end{document}</tex-math><mml:math id="M98"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq65.gif"/></alternatives></inline-formula> and <inline-formula id="IEq66"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=16$$\end{document}</tex-math><mml:math id="M100"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq66.gif"/></alternatives></inline-formula> simulations show essentially the same errors within the noise level of the experiment. The PRISM <inline-formula id="IEq67"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=32$$\end{document}</tex-math><mml:math id="M102"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq67.gif"/></alternatives></inline-formula> has a higher error, and obvious image artifacts are visible in Fig. <xref rid="Fig8" ref-type="fig">8</xref>f. Thus, we conclude that using an interpolation factor <inline-formula id="IEq68"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f=16$$\end{document}</tex-math><mml:math id="M104"><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="40679_2017_48_Article_IEq68.gif"/></alternatives></inline-formula> produces an image of sufficient accuracy. This calculation required only 90 s per frozen phonon calculation, and therefore computing 16 configurations for all 65 tilt angles would require only 26 h. One could therefore imagine integrating this simulation routine into the final few tomography reconstruction iterations to account for dynamical scattering effects and to improve the reconstruction quality.</p>
  </sec>
  <sec id="Sec17">
    <title>Conclusions</title>
    <p id="Par42">We have presented <italic>Prismatic</italic>, an asynchronous, streaming multi-GPU implementation of the PRISM and multislice algorithms for image formation in scanning transmission electron microscopy. Both multislice and PRISM algorithms were described in detail as well as our approach to implementing them in a parallel framework. Our benchmarks demonstrate that this software may be used to simulate STEM images up to several orders of magnitude faster than using traditional methods, allowing users to simulate complex systems on a GPU workstation without the need for a computer cluster. <italic>Prismatic</italic> is freely available as an open-source C++/CUDA package with a graphical interface that contains convenience features such as allowing users to interactively view the projected potential slices, compute/compare individual probe positions with both PRISM and multislice, and dynamically adjust positions of virtual detectors. A command line interface and a Python package, <italic>PyPrismatic</italic>, are also available. We have demonstrated one potential application of the <italic>Prismatic</italic> code, using it to compute STEM images to improve the accuracy in atomic electron tomography. We hope that the speed of this code as well as the convenience of the user interface will have significant impact for users in the EM community.</p>
  </sec>
</body>
<back>
  <ack>
    <title>Authors' contributions</title>
    <p>AP designed the software, implemented the CUDA/C++ versions of PRISM and multislice, programmed the graphical user interface and command line interface, and performed the simulations in this paper. CO conceived of the PRISM algorithm, and wrote the original MATLAB implementations. AP and CO wrote the manuscript. JM advised the project. All authors commented on the manuscript. All authors read and approved the final manuscript.</p>
    <sec id="d29e2493">
      <title>Acknowledgements</title>
      <p>We kindly thank Yongsoo Yang for his help using <italic>computem</italic> and with the FePt simulations, and Ivan Lobato for his help using <italic>MULTEM</italic>. This work was supported by STROBE: A National Science Foundation Science &amp; Technology Center under Grant No. DMR 1548924, the Office of Basic Energy Sciences of the US DOE (DE-SC0010378), and the NSF DMREF program (DMR-1437263).The computations were supported by a User Project at the Molecular Foundry using its compute cluster (VULCAN), managed by the High Performance Computing Services Group, at Lawrence Berkeley National Laboratory (LBNL).</p>
    </sec>
    <sec id="d29e2504">
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="d29e2509">
      <title>Availability of data and materials</title>
      <p>The <italic>Prismatic</italic> source code, installers, and documentation with tutorials are freely available at <ext-link ext-link-type="uri" xlink:href="http://www.prism-em.com">http://www.prism-em.com</ext-link>.</p>
    </sec>
    <sec id="d29e2522">
      <title>Consent for publication</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="d29e2527">
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="d29e2532">
      <title>Funding</title>
      <p>This work was supported by STROBE: A National Science Foundation Science &amp; Technology Center under Grant No. DMR 1548924, the Office of Basic Energy Sciences of the US DOE (DE-SC0010378), and the NSF DMREF program (DMR-1437263). Work at the Molecular Foundry was supported by the Office of Science, Office of Basic Energy Sciences, of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.</p>
    </sec>
    <sec id="d29e2537">
      <title>Publisher's Note</title>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Crewe</surname>
            <given-names>AV</given-names>
          </name>
        </person-group>
        <article-title>Scanning transmission electron microscopy</article-title>
        <source>J. Microsc.</source>
        <year>1974</year>
        <volume>100</volume>
        <issue>3</issue>
        <fpage>247</fpage>
        <lpage>259</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1365-2818.1974.tb03937.x</pub-id>
        <pub-id pub-id-type="pmid">4599319</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Nellist</surname>
            <given-names>PD</given-names>
          </name>
        </person-group>
        <source>Scanning transmission electron microscopy</source>
        <year>2007</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>65</fpage>
        <lpage>132</lpage>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Batson</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Dellby</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Krivanek</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Sub-ångstrom resolution using aberration corrected electron optics</article-title>
        <source>Nature</source>
        <year>2002</year>
        <volume>418</volume>
        <issue>6898</issue>
        <fpage>617</fpage>
        <lpage>620</lpage>
        <pub-id pub-id-type="doi">10.1038/nature00972</pub-id>
        <pub-id pub-id-type="pmid">12167855</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Muller</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <article-title>Structure and bonding at the atomic scale by scanning transmission electron microscopy</article-title>
        <source>Nat. Mater.</source>
        <year>2009</year>
        <volume>8</volume>
        <issue>4</issue>
        <fpage>263</fpage>
        <lpage>270</lpage>
        <pub-id pub-id-type="doi">10.1038/nmat2380</pub-id>
        <pub-id pub-id-type="pmid">19308085</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Pennycook, S.J.: The impact of stem aberration correction on materials science. Ultramicroscopy. <bold>180</bold>, 22–33 (2017). doi:10.1016/j.ultramic.2017.03.020</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Pelz, P.M., Qiu, W.X., Bücker, R., Kassier, G., Miller, R.: Low-dose cryo electron ptychography via non-convex bayesian optimization. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1702.05732">arXiv:1702.05732</ext-link> (2017)</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van den Broek</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Koch</surname>
            <given-names>CT</given-names>
          </name>
        </person-group>
        <article-title>Method for retrieval of the three-dimensional object potential by inversion of dynamical electron scattering</article-title>
        <source>Phys. Rev. Lett.</source>
        <year>2012</year>
        <volume>109</volume>
        <issue>24</issue>
        <fpage>245502</fpage>
        <pub-id pub-id-type="doi">10.1103/PhysRevLett.109.245502</pub-id>
        <pub-id pub-id-type="pmid">23368342</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yankovich</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Berkels</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Dahmen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Binev</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sanchez</surname>
            <given-names>SI</given-names>
          </name>
          <name>
            <surname>Bradley</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Szlufarska</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Voyles</surname>
            <given-names>PM</given-names>
          </name>
        </person-group>
        <article-title>Picometre-precision analysis of scanning transmission electron microscopy images of platinum nanocatalysts</article-title>
        <source>Nat. Commun.</source>
        <year>2014</year>
        <volume>5</volume>
        <fpage>4155</fpage>
        <pub-id pub-id-type="doi">10.1038/ncomms5155</pub-id>
        <pub-id pub-id-type="pmid">24916914</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mevenkamp</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Binev</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Dahmen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Voyles</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Yankovich</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Berkels</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Poisson noise removal from high-resolution stem images based on periodic block matching</article-title>
        <source>Adv. Struct. Chem. Imaging.</source>
        <year>2015</year>
        <volume>1</volume>
        <issue>1</issue>
        <fpage>3</fpage>
        <pub-id pub-id-type="doi">10.1186/s40679-015-0004-8</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Ophus, C., Ciston, J., Pierce, J., Harvey, T.R., Chess, J., McMorran, B.J., Czarnik, C., Rose, H.H., Ercius, P.: Efficient linear phase contrast in scanning transmission electron microscopy with matched illumination and detector interferometry. Nat. Commun. <bold>7</bold> (2016). doi:10.1038/ncomms10719</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van den Bos</surname>
            <given-names>KH</given-names>
          </name>
          <name>
            <surname>De Backer</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Martinez</surname>
            <given-names>GT</given-names>
          </name>
          <name>
            <surname>Winckelmans</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Bals</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Nellist</surname>
            <given-names>PD</given-names>
          </name>
          <name>
            <surname>Van Aert</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Unscrambling mixed elements using high angle annular dark field scanning transmission electron microscopy</article-title>
        <source>Phys. Rev. Lett.</source>
        <year>2016</year>
        <volume>116</volume>
        <issue>24</issue>
        <fpage>246101</fpage>
        <pub-id pub-id-type="doi">10.1103/PhysRevLett.116.246101</pub-id>
        <pub-id pub-id-type="pmid">27367396</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ercius</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Billinge</surname>
            <given-names>SJL</given-names>
          </name>
        </person-group>
        <article-title>Atomic electron tomography: 3D structures without crystals</article-title>
        <source>Science</source>
        <year>2016</year>
        <volume>353</volume>
        <issue>6306</issue>
        <fpage>2157</fpage>
        <lpage>2157</lpage>
        <pub-id pub-id-type="doi">10.1126/science.aaf2157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>C-C</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ophus</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bartels</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ramezani-Dakhel</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Sawaya</surname>
            <given-names>MR</given-names>
          </name>
        </person-group>
        <article-title>Three-dimensional coordinates of individual atoms in materials revealed by electron tomography</article-title>
        <source>Nat. Mater.</source>
        <year>2015</year>
        <volume>14</volume>
        <issue>11</issue>
        <fpage>1099</fpage>
        <lpage>1103</lpage>
        <pub-id pub-id-type="doi">10.1038/nmat4426</pub-id>
        <pub-id pub-id-type="pmid">26390325</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>C-C</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ophus</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pryor</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Deciphering chemical order/disorder and material properties at the single-atom level</article-title>
        <source>Nature</source>
        <year>2017</year>
        <volume>542</volume>
        <issue>7639</issue>
        <fpage>75</fpage>
        <lpage>79</lpage>
        <pub-id pub-id-type="doi">10.1038/nature21042</pub-id>
        <pub-id pub-id-type="pmid">28150758</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scott</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>CC</given-names>
          </name>
          <name>
            <surname>Mecklenburg</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ercius</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Dahmen</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Regan</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Miao</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Electron tomography at 2.4-angstrom resolution</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>483</volume>
        <issue>7390</issue>
        <fpage>444</fpage>
        <lpage>447</lpage>
        <pub-id pub-id-type="doi">10.1038/nature10934</pub-id>
        <pub-id pub-id-type="pmid">22437612</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>C-C</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>ER</given-names>
          </name>
          <name>
            <surname>Chiu</surname>
            <given-names>C-Y</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>MC</given-names>
          </name>
          <name>
            <surname>Regan</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Marks</surname>
            <given-names>LD</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Miao</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Three-dimensional imaging of dislocations in a nanoparticle at atomic resolution</article-title>
        <source>Nature</source>
        <year>2013</year>
        <volume>496</volume>
        <issue>7443</issue>
        <fpage>74</fpage>
        <lpage>77</lpage>
        <pub-id pub-id-type="doi">10.1038/nature12009</pub-id>
        <pub-id pub-id-type="pmid">23535594</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cowley</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Moodie</surname>
            <given-names>AF</given-names>
          </name>
        </person-group>
        <article-title>The scattering of electrons by atoms and crystals. I. A new theoretical approach</article-title>
        <source>Acta Crystallogr.</source>
        <year>1957</year>
        <volume>10</volume>
        <issue>10</issue>
        <fpage>609</fpage>
        <lpage>619</lpage>
        <pub-id pub-id-type="doi">10.1107/S0365110X57002194</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kirkland</surname>
            <given-names>EJ</given-names>
          </name>
          <name>
            <surname>Loane</surname>
            <given-names>RF</given-names>
          </name>
          <name>
            <surname>Silcox</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Simulation of annular dark field stem images using a modified multislice method</article-title>
        <source>Ultramicroscopy</source>
        <year>1987</year>
        <volume>23</volume>
        <issue>1</issue>
        <fpage>77</fpage>
        <lpage>96</lpage>
        <pub-id pub-id-type="doi">10.1016/0304-3991(87)90229-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ishizuka</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Uyeda</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>A new theoretical and practical approach to the multislice method</article-title>
        <source>Acta Crystallogr. Sect. A Cryst. Phys. Diffr Theor. Gen. Crystallogr.</source>
        <year>1977</year>
        <volume>33</volume>
        <issue>5</issue>
        <fpage>740</fpage>
        <lpage>749</lpage>
        <pub-id pub-id-type="doi">10.1107/S0567739477001879</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ishizuka</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>A practical approach for stem image simulation based on the FFT multislice method</article-title>
        <source>Ultramicroscopy.</source>
        <year>2002</year>
        <volume>90</volume>
        <issue>2</issue>
        <fpage>71</fpage>
        <lpage>83</lpage>
        <pub-id pub-id-type="doi">10.1016/S0304-3991(01)00145-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Kirkland, E.J.: Advanced computing in electron microscopy, Second edition. Springer, New York (2010)</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stadelmann</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Ems-a software package for electron diffraction analysis and hrem image simulation in materials science</article-title>
        <source>Ultramicroscopy</source>
        <year>1987</year>
        <volume>21</volume>
        <issue>2</issue>
        <fpage>131</fpage>
        <lpage>145</lpage>
        <pub-id pub-id-type="doi">10.1016/0304-3991(87)90080-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stadelmann</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Image analysis and simulation software in transmission electron microscopy</article-title>
        <source>Microsc. Microanal.</source>
        <year>2003</year>
        <volume>9</volume>
        <issue>S03</issue>
        <fpage>60</fpage>
        <lpage>61</lpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Kilaas, R.: MacTempas a program for simulating high resolution TEM images and diffraction patterns. <ext-link ext-link-type="uri" xlink:href="http://www.totalresolution.com/">http://www.totalresolution.com/</ext-link></mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Koch, C.T.: Determination of core structure periodicity and point defect density along dislocations. Arizona State University (2002). <ext-link ext-link-type="uri" xlink:href="http://adsabs.harvard.edu/abs/2002PhDT........50K">http://adsabs.harvard.edu/abs/2002PhDT........50K</ext-link></mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>De Graef</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <source>Introduction to conventional transmission electron microscopy</source>
        <year>2003</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Cambridge University Press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zuo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mabon</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Web-based electron microscopy application software: Web-emaps</article-title>
        <source>Microsc. Microanal.</source>
        <year>2004</year>
        <volume>10</volume>
        <issue>S02</issue>
        <fpage>1000</fpage>
        <pub-id pub-id-type="doi">10.1017/S1431927604884319</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carlino</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Grillo</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Palazzari</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Accurate and fast multislice simulations of haadf image contrast by parallel computing</article-title>
        <source>Microsc. Semicond. Mater.</source>
        <year>2008</year>
        <volume>2007</volume>
        <fpage>177</fpage>
        <lpage>180</lpage>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grillo</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Rotunno</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>STEM_CELL: a software tool for electron microscopy: part I-simulations</article-title>
        <source>Ultramicroscopy.</source>
        <year>2013</year>
        <volume>125</volume>
        <fpage>97</fpage>
        <lpage>111</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ultramic.2012.10.016</pub-id>
        <pub-id pub-id-type="pmid">23265085</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rosenauer</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Schowalter</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Stemsim—new software tool for simulation of stem haadf z-contrast imaging</article-title>
        <source>Microsc. Semicond. Mater.</source>
        <year>2008</year>
        <volume>2007</volume>
        <fpage>170</fpage>
        <lpage>172</lpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Walton</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Zeissler</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Branford</surname>
            <given-names>WR</given-names>
          </name>
          <name>
            <surname>Felton</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Malts: a tool to simulate lorentz transmission electron microscopy from micromagnetic simulations</article-title>
        <source>IEEE Trans. Magn.</source>
        <year>2013</year>
        <volume>49</volume>
        <issue>8</issue>
        <fpage>4795</fpage>
        <lpage>4800</lpage>
        <pub-id pub-id-type="doi">10.1109/TMAG.2013.2247410</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bar-Sadan</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Barthel</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shtrikman</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Houben</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Direct imaging of single au atoms within gaas nanowires</article-title>
        <source>Nano Lett.</source>
        <year>2012</year>
        <volume>12</volume>
        <issue>5</issue>
        <fpage>2352</fpage>
        <lpage>2356</lpage>
        <pub-id pub-id-type="doi">10.1021/nl300314k</pub-id>
        <pub-id pub-id-type="pmid">22497234</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lobato</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Van Dyck</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Multem: a new multislice program to perform accurate and fast electron diffraction and imaging simulations using graphics processing units with cuda</article-title>
        <source>Ultramicroscopy.</source>
        <year>2015</year>
        <volume>156</volume>
        <fpage>9</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ultramic.2015.04.016</pub-id>
        <pub-id pub-id-type="pmid">25965576</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lobato</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Van Aert</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Verbeeck</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Progress and new advances in simulating electron microscopy datasets using multem</article-title>
        <source>Ultramicroscopy.</source>
        <year>2016</year>
        <volume>168</volume>
        <fpage>17</fpage>
        <lpage>27</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ultramic.2016.06.003</pub-id>
        <pub-id pub-id-type="pmid">27323350</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van den Broek</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Koch</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>FDES, a GPU-based multislice algorithm with increased efficiency of the computation of the projected potential</article-title>
        <source>Ultramicroscopy.</source>
        <year>2015</year>
        <volume>158</volume>
        <fpage>89</fpage>
        <lpage>97</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ultramic.2015.07.005</pub-id>
        <pub-id pub-id-type="pmid">26233822</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cosgriff</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>D’Alfonso</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Allen</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Findlay</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kirkland</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Nellist</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Three-dimensional imaging in double aberration-corrected scanning confocal electron microscopy, part I: elastic scattering</article-title>
        <source>Ultramicroscopy.</source>
        <year>2008</year>
        <volume>108</volume>
        <issue>12</issue>
        <fpage>1558</fpage>
        <lpage>1566</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ultramic.2008.05.009</pub-id>
        <pub-id pub-id-type="pmid">18639381</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Forbes</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Findlay</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>D’alfonso</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Allen</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Quantum mechanical model for phonon excitation in electron diffraction and imaging using a born-oppenheimer approximation</article-title>
        <source>Phys. Rev. B.</source>
        <year>2010</year>
        <volume>82</volume>
        <issue>10</issue>
        <fpage>104103</fpage>
        <pub-id pub-id-type="doi">10.1103/PhysRevB.82.104103</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Oelerich</surname>
            <given-names>JO</given-names>
          </name>
          <name>
            <surname>Duschek</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Belz</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Beyer</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Baranovskii</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Volz</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Stemsalabim: a high-performance computing cluster friendly code for scanning transmission electron microscopy image simulations of thin specimens</article-title>
        <source>Ultramicroscopy.</source>
        <year>2017</year>
        <volume>177</volume>
        <fpage>91</fpage>
        <lpage>96</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ultramic.2017.03.010</pub-id>
        <pub-id pub-id-type="pmid">28334576</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ophus</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>A fast image simulation algorithm for scanning transmission electron microscopy</article-title>
        <source>Adv. Struct. Chem. Imaging.</source>
        <year>2017</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>13</fpage>
        <pub-id pub-id-type="doi">10.1186/s40679-017-0046-1</pub-id>
        <pub-id pub-id-type="pmid">28546904</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ge</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Stem image simulation with hybrid cpu/gpu programming</article-title>
        <source>Ultramicroscopy.</source>
        <year>2016</year>
        <volume>166</volume>
        <fpage>1</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ultramic.2016.04.001</pub-id>
        <pub-id pub-id-type="pmid">27093687</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Frigo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>SG</given-names>
          </name>
        </person-group>
        <article-title>The design and implementation of FFTW3</article-title>
        <source>Proc. IEEE.</source>
        <year>2005</year>
        <volume>93</volume>
        <issue>2</issue>
        <fpage>216</fpage>
        <lpage>231</lpage>
        <pub-id pub-id-type="doi">10.1109/JPROC.2004.840301</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">NVIDIA: cuFFT. <ext-link ext-link-type="uri" xlink:href="https://developer.nvidia.com/cufft">https://developer.nvidia.com/cufft</ext-link></mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Volkov, V., Demmel, J.W.: Benchmarking gpus to tune dense linear algebra. In: High Performance Computing, Networking, Storage and Analysis, 2008. SC 2008. International Conference For, pp. 1–11 (2008)</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Rutte</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Simson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sagawa</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ryll</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Huth</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pennycook</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Green</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Soltau</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Simultaneous atomic-resolution electron ptychography and z-contrast imaging of light and heavy elements in complex nanostructures</article-title>
        <source>Nat. Commun.</source>
        <year>2016</year>
        <volume>7</volume>
        <fpage>12532</fpage>
        <pub-id pub-id-type="doi">10.1038/ncomms12532</pub-id>
        <pub-id pub-id-type="pmid">27561914</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Martin</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Hoffman</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <source>Mastering CMake: a cross-platform build system</source>
        <year>2010</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Kitware</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <mixed-citation publication-type="other">NVIDIA: CUDA C Programming Guide. <ext-link ext-link-type="uri" xlink:href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/">http://docs.nvidia.com/cuda/cuda-c-programming-guide/</ext-link></mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Harris, M.: “Optimizing parallel reduction in CUDA”. Presentation included in the CUDA Toolkit released by NVIDIA. <ext-link ext-link-type="uri" xlink:href="http://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86%5fwebsite/projects/reduction/doc/reduction.pdf">http://developer.download.nvidia.com/compute/cuda/1.1-Beta/x86_website/projects/reduction/doc/reduction.pdf</ext-link> (2007)</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Intel: E5-4669v4. <ext-link ext-link-type="uri" xlink:href="https://ark.intel.com/products/93805/Intel-Xeon-Processor-E5-4669-v4-55M-Cache-2%5f20-GHz">https://ark.intel.com/products/93805/Intel-Xeon-Processor-E5-4669-v4-55M-Cache-2_20-GHz</ext-link></mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <mixed-citation publication-type="other">Sakran, N., Yuffe, M., Mehalel, M., Doweck, J., Knoll, E., Kovacs, A.: The implementation of the 65nm dual-core 64b merom processor. In: Solid-State Circuits Conference, 2007. ISSCC 2007. Digest of Technical Papers. IEEE International, pp. 106–590 (2007)</mixed-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Jeffers</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Reinders</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>Intel Xeon Phi coprocessor high performance programming</source>
        <year>2013</year>
        <edition>1</edition>
        <publisher-loc>San Francisco</publisher-loc>
        <publisher-name>Morgan Kaufmann Publishers Inc.</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Stone, G., Ophus, C., Birol, T., Ciston, J., Lee, C.-H., Wang, K., Fennie, C.J., Schlom, D.G., Alem, N., Gopalan, V.: Atomic scale imaging of competing polar states in a Ruddlesden-Popper layered oxide. Nat. Commun. <bold>7</bold> (2016)</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pryor</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Rana</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gallagher-Jones</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lo</surname>
            <given-names>YH</given-names>
          </name>
          <name>
            <surname>Melinte</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Chiu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Rodriguez</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Miao</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>GENFIRE: a generalized Fourier iterative reconstruction algorithm for high-resolution 3D imaging</article-title>
        <source>Sci. Rep.</source>
        <year>2017</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>10409</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-09847-1</pub-id>
        <pub-id pub-id-type="pmid">28874736</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Muller</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Nakagawa</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Ohtomo</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Grazul</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Hwang</surname>
            <given-names>HY</given-names>
          </name>
        </person-group>
        <article-title>Atomic-scale imaging of nanoengineered oxygen vacancy profiles in SrTiO3</article-title>
        <source>Nature</source>
        <year>2004</year>
        <volume>430</volume>
        <issue>7000</issue>
        <fpage>657</fpage>
        <lpage>661</lpage>
        <pub-id pub-id-type="doi">10.1038/nature02756</pub-id>
        <pub-id pub-id-type="pmid">15295595</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeBeau</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Findlay</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Allen</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Stemmer</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Quantitative atomic resolution scanning transmission electron microscopy</article-title>
        <source>Phys. Rev. Lett.</source>
        <year>2008</year>
        <volume>100</volume>
        <issue>20</issue>
        <fpage>206101</fpage>
        <pub-id pub-id-type="doi">10.1103/PhysRevLett.100.206101</pub-id>
        <pub-id pub-id-type="pmid">18518557</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Findlay</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Shibata</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sawada</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Okunishi</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kondo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ikuhara</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Dynamics of annular bright field imaging in scanning transmission electron microscopy</article-title>
        <source>Ultramicroscopy.</source>
        <year>2010</year>
        <volume>110</volume>
        <issue>7</issue>
        <fpage>903</fpage>
        <lpage>923</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ultramic.2010.04.004</pub-id>
        <pub-id pub-id-type="pmid">20434265</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kourkoutis</surname>
            <given-names>LF</given-names>
          </name>
          <name>
            <surname>Parker</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vaithyanathan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Schlom</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Muller</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Direct measurement of electron channeling in a crystal using scanning transmission electron microscopy</article-title>
        <source>Phys. Rev. B.</source>
        <year>2011</year>
        <volume>84</volume>
        <issue>7</issue>
        <fpage>075485</fpage>
        <pub-id pub-id-type="doi">10.1103/PhysRevB.84.075485</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Woehl</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Keller</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Dark-field image contrast in transmission scanning electron microscopy: effects of substrate thickness and detector collection angle</article-title>
        <source>Ultramicroscopy.</source>
        <year>2016</year>
        <volume>171</volume>
        <fpage>166</fpage>
        <lpage>176</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ultramic.2016.08.008</pub-id>
        <pub-id pub-id-type="pmid">27690347</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <mixed-citation publication-type="other">Cui, J., Yao, Y., Wang, Y., Shen, X., Yu, R.: The origin of atomic displacements in HAADF images of the tilted specimen. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1704.07524">arXiv:1704.07524</ext-link> (2017)</mixed-citation>
    </ref>
  </ref-list>
</back>
