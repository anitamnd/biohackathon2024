<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr J Mach Learn Res?>
<?submitter-system nihms?>
<?submitter-userid 10531256?>
<?submitter-authority eRA?>
<?submitter-login hua_zhou?>
<?submitter-name Hua Zhou?>
<?domain nihpa?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">101262635</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">32661</journal-id>
    <journal-id journal-id-type="nlm-ta">J Mach Learn Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Mach Learn Res</journal-id>
    <journal-title-group>
      <journal-title>Journal of machine learning research : JMLR</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1532-4435</issn>
    <issn pub-type="epub">1533-7928</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6812563</article-id>
    <article-id pub-id-type="manuscript">nihpa1053815</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Proximal Distance Algorithms: Theory and Practice</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Keys</surname>
          <given-names>Kevin L.</given-names>
        </name>
        <aff id="A1">Department of Medicine, University of California, San Francisco, CA 94158, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Hua</given-names>
        </name>
        <aff id="A2">Department of Biostatistics, University of California, Los Angeles, CA 90095-1772, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lange</surname>
          <given-names>Kenneth</given-names>
        </name>
        <aff id="A3">Departments of Biomathematics, Human Genetics, and Statistics, University of California, Los Angeles, CA 90095-1766, USA</aff>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="CR1">
        <email>KEVIN.KEYS@UCSF.EDU</email>
      </corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>9</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>4</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>66</elocation-id>
    <permissions>
      <license license-type="open-access">
        <license-p>License: CC-BY 4.0, see <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>. Attribution requirements are provided at <ext-link ext-link-type="uri" xlink:href="http://jmlr.org/papers/v20/17-687.html">http://jmlr.org/papers/v20/17-687.html</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">Proximal distance algorithms combine the classical penalty method of constrained minimization with distance majorization. If <italic>f</italic>(<italic>x</italic>) is the loss function, and <italic>C</italic> is the constraint set in a constrained minimization problem, then the proximal distance principle mandates minimizing the penalized loss <inline-formula><mml:math display="inline" id="M1" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and following the solution <italic>x</italic><sub><italic>ρ</italic></sub> to its limit as <italic>ρ</italic> tends to ∞. At each iteration the squared Euclidean distance dist(<italic>x,C</italic>)<sup>2</sup> is majorized by the spherical quadratic ‖<italic>x</italic>− <italic>P</italic><sub><italic>C</italic></sub>(<italic>x</italic><sub><italic>k</italic></sub>)‖<sup>2</sup>, where <italic>P</italic><sub><italic>C</italic></sub>(<italic>x</italic><sub><italic>k</italic></sub>) denotes the projection of the current iterate <italic>x</italic><sub><italic>k</italic></sub> onto <italic>C</italic>. The minimum of the surrogate function <inline-formula><mml:math display="inline" id="M2" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is given by the proximal map prox<sub><italic>ρ</italic></sub>−<sub>1<italic>f</italic></sub>[<italic>P</italic><sub><italic>C</italic></sub>(<italic>x</italic><sub><italic>k</italic></sub>)]. The next iterate <italic>x</italic><sub><italic>k</italic>+1</sub> automatically decreases the original penalized loss for fixed <italic>ρ</italic>. Since many explicit projections and proximal maps are known, it is straightforward to derive and implement novel optimization algorithms in this setting. These algorithms can take hundreds if not thousands of iterations to converge, but the simple nature of each iteration makes proximal distance algorithms competitive with traditional algorithms. For convex problems, proximal distance algorithms reduce to proximal gradient algorithms and therefore enjoy well understood convergence properties. For nonconvex problems, one can attack convergence by invoking Zangwill’s theorem. Our numerical examples demonstrate the utility of proximal distance algorithms in various high-dimensional settings, including a) linear programming, b) constrained least squares, c) projection to the closest kinship matrix, d) projection onto a second-order cone constraint, e) calculation of Horn’s copositive matrix index, f) linear complementarity programming, and g) sparse principal components analysis. The proximal distance algorithm in each case is competitive or superior in speed to traditional methods such as the interior point method and the alternating direction method of multipliers (ADMM). Source code for the numerical examples can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/klkeys/proxdist">https://github.com/klkeys/proxdist</ext-link>.</p>
    </abstract>
    <kwd-group>
      <kwd>constrained optimization</kwd>
      <kwd>EM algorithm</kwd>
      <kwd>majorization</kwd>
      <kwd>projection</kwd>
      <kwd>proximal operator</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P2">The solution of constrained optimization problems is part science and part art. As mathematical scientists explore the largely uncharted territory of high-dimensional nonconvex problems, it is imperative to consider new methods. The current paper studies a class of optimization algorithms that combine Courant’s penalty method of optimization (<xref rid="R6" ref-type="bibr">Beltrami, 1970</xref>; <xref rid="R23" ref-type="bibr">Courant, 1943</xref>) with the notion of a proximal operator (<xref rid="R4" ref-type="bibr">Bauschke and Combettes, 2011</xref>; <xref rid="R61" ref-type="bibr">Moreau, 1962</xref>; <xref rid="R67" ref-type="bibr">Parikh and Boyd, 2013</xref>). The classical penalty method turns constrained minimization of a function <italic>f</italic>(<italic>x</italic>) over a closed set <italic>C</italic> into unconstrained minimization. The general idea is to seek the minimum point of a penalized version <italic>f</italic>(<italic>x</italic>)+<italic>ρq</italic>(<italic>x</italic>) of <italic>f</italic>(<italic>x</italic>), where the penalty <italic>q</italic>(<italic>x</italic>) is nonnegative and vanishes precisely on <italic>C</italic>. If one follows the solution vector <italic>x</italic><sub><italic>ρ</italic></sub> as <italic>ρ</italic> tends to ∞, then in the limit one recovers the constrained solution. The penalties of choice in the current paper are squared Euclidean distances dist(<italic>x,C</italic>)<sup>2</sup> = inf<sub><italic>y</italic>∈<italic>C</italic></sub> ‖<italic>x</italic>−<italic>y</italic>‖<sup>2</sup>.</p>
    <p id="P3">The formula
<disp-formula id="FD1"><label>(1)</label><mml:math display="block" id="M4" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mtext>prox</mml:mtext></mml:mrow><mml:mi>f</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
defines the proximal map of a function <italic>f</italic>(<bold><italic>x</italic></bold>). Here ‖ · ‖ is again the standard Euclidean norm, and <italic>f</italic>(<bold><italic>x</italic></bold>) is typically assumed to be closed and convex. Projection onto a closed convex set <italic>C</italic> is realized by choosing <italic>f</italic>(<bold><italic>x</italic></bold>) to be the 0<italic>/</italic>∞ indicator <italic>δ</italic><sub><italic>C</italic></sub>(<bold><italic>x</italic></bold>) of <italic>C</italic>. It is possible to drop the convexity assumption if <italic>f</italic>(<bold><italic>x</italic></bold>) is nonnegative or coercive. In so doing, prox<sub><italic>f</italic></sub>(<italic>y</italic>) may become multi-valued. For example, the minimum distance from a nonconvex set to an exterior point may be attained at multiple boundary points. The point <bold><italic>x</italic></bold> in the definition <xref rid="FD1" ref-type="disp-formula">(1)</xref> can be restricted to a subset <italic>S</italic> of Euclidean space by replacing <italic>f</italic>(<bold><italic>x</italic></bold>) by <italic>f</italic>(<bold><italic>x</italic></bold>) + <italic>δ</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold>), where <italic>δ</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold>) is the indicator of <italic>S</italic>.</p>
    <p id="P4">One of the virtues of exploiting proximal operators is that they have been thoroughly investigated. For a large number of functions <italic>f</italic>(<bold><italic>x</italic></bold>), the map prox<sub><italic>cf</italic></sub>(<bold><italic>y</italic></bold>) for <italic>c &gt;</italic> 0 is either given by an exact formula or calculable by an efficient algorithm. The known formulas tend to be highly accurate. This is a plus because the classical penalty method suffers from ill conditioning for large values of the penalty constant. Although the penalty method seldom delivers exquisitely accurate solutions, moderate accuracy suffices for many problems.</p>
    <p id="P5">There are ample precedents in the optimization literature for the proximal distance principle. Proximal gradient algorithms have been employed for many years in many contexts, including projected Landweber, alternating projection onto the intersection of two or more closed convex sets, the alternating-direction method of multipliers (ADMM), and fast iterative shrinkage thresholding algorithms (FISTA) (<xref rid="R5" ref-type="bibr">Beck and Teboulle, 2009</xref>; <xref rid="R22" ref-type="bibr">Combettes and Pesquet, 2011</xref>; <xref rid="R48" ref-type="bibr">Landweber, 1951</xref>). Applications of distance majorization are more recent (<xref rid="R21" ref-type="bibr">Chi et al., 2014</xref>; <xref rid="R51" ref-type="bibr">Lange and Keys, 2014</xref>; <xref rid="R75" ref-type="bibr">Xu et al., 2017</xref>). The overall strategy consists of replacing the distance penalty dist(<italic>x,C</italic>)<sup>2</sup> by the spherical quadratic ‖<bold><italic>x</italic></bold> − <bold><italic>y</italic></bold><sub><italic>k</italic></sub>‖<sup>2</sup>, where <bold><italic>y</italic></bold><sub><italic>k</italic></sub> is the projection of the <italic>k</italic>th iterate <bold><italic>x</italic></bold><sub><italic>k</italic></sub> onto <italic>C</italic>. To form the next iterate, one then sets
<disp-formula id="FD2"><mml:math display="block" id="M5" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>prox</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>     with     </mml:mtext><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The MM (majorization-minimization) principle guarantees that <italic>x</italic><sub><italic>k</italic>+1</sub> decreases the penalized loss. We call the combination of Courant’s penalty method with distance majorization the <italic>proximal distance principle</italic>. Algorithms constructed according to the principle are <italic>proximal distance algorithms</italic>.</p>
    <p id="P6">The current paper extends and deepens our previous preliminary treatments of the proximal distance principle. Details of implementation such as Nesterov acceleration matter in performance. We have found that squared distance penalties tend to work better than exact penalties. In the presence of convexity, it is now clear that every proximal distance algorithm reduces to a proximal gradient algorithm. Hence, convergence analysis can appeal to a venerable body of convex theory. This does not imply that the proximal distance algorithm is limited to convex problems. In fact, its most important applications may well be to nonconvex problems. A major focus of this paper is on practical exploration of the proximal distance algorithm.</p>
    <p id="P7">In addition to reviewing the literature, the current paper presents some fresh ideas. Among the innovations are: a) recasting proximal distance algorithms with convex losses as concave-convex programs, b) providing new perspectives on convergence for both convex and nonconvex proximal distance algorithms, c) demonstrating the virtue of folding constraints into the domain of the loss, and d) treating in detail seven interesting examples. It is noteworthy that some our new convergence theory is pertinent to more general MM algorithms.</p>
    <p id="P8">It is our sincere hope to enlist other mathematical scientists in expanding and clarifying this promising line of research. The reviewers of the current paper have correctly pointed out that we do not rigorously justify our choices of the penalty constant sequence <italic>ρ</italic><sub><italic>k</italic></sub>. The recent paper by <xref rid="R53" ref-type="bibr">Li et al. (2017)</xref> may be a logical place to start in filling this theoretical gap. They deal with the problem of minimizing <italic>f</italic>(<bold><italic>x</italic></bold>) subject to <bold><italic>Ax</italic></bold> = <bold><italic>b</italic></bold> through the quadratic penalized objective <inline-formula><mml:math display="inline" id="M6" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. For the right choices of the penalty sequence <italic>ρ</italic><sub><italic>k</italic></sub>, their proximal gradient algorithm achieves a <italic>O</italic>(<italic>k</italic><sup>−1</sup>) rate of convergence for <italic>f</italic>(<bold><italic>x</italic></bold>) strongly convex. As a substitute, we explore the classical problem of determining how accurately the solution <bold><italic>y</italic></bold><sub><italic>ρ</italic></sub> of the problem <inline-formula><mml:math display="inline" id="M7" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>q</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> approximates the solution <bold><italic>y</italic></bold> of the constrained problem min<sub><italic>x</italic>∈<italic>C</italic></sub>
<italic>f</italic>(<italic>x</italic>). <xref rid="R69" ref-type="bibr">Polyak (1971)</xref> demonstrates that <italic>f</italic>(<bold><italic>y</italic></bold>)−<italic>f</italic>(<bold><italic>y</italic></bold><sub><italic>ρ</italic></sub>) = <italic>O</italic>(<bold><italic>ρ</italic></bold><sup>−1</sup>) for a penalty function <italic>q</italic>(<bold><italic>x</italic></bold>) that vanishes precisely on <italic>C</italic>. Polyak’s proof relies on strong differentiability assumptions. Our proof for the case <italic>q</italic>(<bold><italic>x</italic></bold>) = dist(<bold><italic>x</italic></bold><italic>,C</italic>) relies on convexity and is much simpler.</p>
    <p id="P9">As a preview, let us outline the remainder of our paper. <xref rid="S2" ref-type="sec">Section 2</xref> briefly sketches the underlying MM principle. We then show how to construct proximal distance algorithms from the MM principle and distance majorization. The section concludes with the derivation of a few broad categories proximal distance algorithms. <xref rid="S3" ref-type="sec">Section 3</xref> covers convergence theory for convex problems, while <xref rid="S4" ref-type="sec">Section 4</xref> provides a more general treatment of convergence for nonconvex problems. To avoid breaking the flow of our exposition, all proofs are relegated to the <xref rid="APP1" ref-type="app">Appendix</xref>. <xref rid="S5" ref-type="sec">Section 5</xref> discusses our numerical experiments on various convex and nonconvex problems. <xref rid="S13" ref-type="sec">Section 6</xref> closes by indicating some future research directions.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Derivation</title>
    <p id="P10">The derivation of our proximal distance algorithms exploits the majorization-minimization (MM) principle (<xref rid="R37" ref-type="bibr">Hunter and Lange, 2004</xref>; <xref rid="R49" ref-type="bibr">Lange, 2010</xref>). In minimizing a function <italic>f</italic>(<bold><italic>x</italic></bold>), the MM principle exploits a surrogate function <italic>g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) that majorizes <italic>f</italic>(<bold><italic>x</italic></bold>) around the current iterate <bold><italic>x</italic></bold><sub><italic>k</italic></sub>. Majorization mandates both domination <italic>g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) ≥ <italic>f</italic>(<bold><italic>x</italic></bold>) for all feasible <bold><italic>x</italic></bold> and tangency <italic>g</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) = <italic>f</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) at the anchor <bold><italic>x</italic></bold><sub><italic>k</italic></sub>. If <bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub> minimizes <italic>g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>), then the descent property <italic>f</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub>) ≤ <italic>f</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) follows from the string of inequalities and equalities
<disp-formula id="FD3"><mml:math display="block" id="M8" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Clever selection of the surrogate <italic>g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub>) can lead to a simple algorithm with an explicit update that requires little computation per iterate. The number of iterations until convergence of an MM algorithm depends on how tightly <italic>g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) hugs <italic>f</italic>(<bold><italic>x</italic></bold>). Constraint satisfaction is built into any MM algorithm. If maximization of <italic>f</italic>(<bold><italic>x</italic></bold>) is desired, then the objective <italic>f</italic>(<bold><italic>x</italic></bold>) should dominate the surrogate <italic>g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) subject to the tangency condition. The next iterate <bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub> is then chosen to maximize <italic>g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>). The minorization-maximization version of the MM principle guarantees the ascent property.</p>
    <p id="P11">The constraint set <italic>C</italic> over which the loss <italic>f</italic>(<bold><italic>x</italic></bold>) is minimized can usually be expressed as an intersection <inline-formula><mml:math display="inline" id="M9" overflow="scroll"><mml:mrow><mml:msubsup><mml:mstyle><mml:mo>∩</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of closed sets. It is natural to define the penalty
<disp-formula id="FD4"><mml:math display="block" id="M10" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
using a convex combination of the squared distances. The neutral choice <inline-formula><mml:math display="inline" id="M11" overflow="scroll"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> is one we prefer in practice. Distance majorization gives the surrogate function
<disp-formula id="FD5"><mml:math display="block" id="M12" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>
for an irrelevant constant <italic>c</italic><sub><italic>k</italic></sub>. If we put <inline-formula><mml:math display="inline" id="M13" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle stretchy="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mstyle><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, then by definition the minimum of the surrogate <italic>g</italic><sub><italic>ρ</italic></sub>(<italic>x</italic> | <italic>x</italic><sub><italic>k</italic></sub>) occurs at the proximal point
<disp-formula id="FD6"><label>(2)</label><mml:math display="block" id="M14" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>prox</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
We call this MM algorithm the proximal distance algorithm. The penalty <italic>q</italic>(<bold><italic>x</italic></bold>) is generally smooth because
<disp-formula id="FD7"><mml:math display="block" id="M15" overflow="scroll"><mml:mrow><mml:mo>∇</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
at any point <bold><italic>x</italic></bold> where the projection <italic>P</italic><sub><italic>C</italic></sub>(<bold><italic>x</italic></bold>) is single valued (<xref rid="R14" ref-type="bibr">Borwein and Lewis, 2006</xref>; <xref rid="R50" ref-type="bibr">Lange, 2016</xref>). This is always true for convex sets and almost always true for nonconvex sets. For the moment, we will ignore the possibility that <italic>P</italic><sub><italic>C</italic></sub>(<bold><italic>x</italic></bold>) is multi-valued.</p>
    <p id="P12">For the special case of projection of an external point <italic>z</italic> onto the intersection <italic>C</italic> of the closed sets <italic>C</italic><sub><italic>i</italic></sub>, one should take <inline-formula><mml:math display="inline" id="M16" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. The proximal distance iterates then obey the explicit formula
<disp-formula id="FD8"><mml:math display="block" id="M17" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Linear programming with arbitrary convex constraints is another example. Here the loss is <italic>f</italic>(<bold><italic>x</italic></bold>) = <bold><italic>v</italic></bold><sup><italic>t</italic></sup><bold><italic>x</italic></bold>, and the update reduces to
<disp-formula id="FD9"><mml:math display="block" id="M18" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>ρ</mml:mi></mml:mfrac><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
If the proximal map is impossible to calculate, but <italic>f</italic>(<bold><italic>x</italic></bold>) is <italic>L</italic>-smooth (∇<italic>f</italic>(<bold><italic>x</italic></bold>) is Lipschitz with constant <italic>L</italic>), then one can substitute the standard majorization
<disp-formula id="FD10"><mml:math display="block" id="M19" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
for <italic>f</italic>(<bold><italic>x</italic></bold>). Minimizing the sum of the loss majorization plus the penalty majorization leads to the MM update
<disp-formula id="FD11"><label>(3)</label><mml:math display="block" id="M20" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>L</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo>∇</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
This is a gradient descent algorithm without an intervening proximal map.</p>
    <p id="P13">In moderate-dimensional problems, local quadratic approximation of <italic>f</italic>(<bold><italic>x</italic></bold>) can lead to a viable algorithm. For instance, in generalized linear statistical models, <xref rid="R75" ref-type="bibr">Xu et al. (2017)</xref> suggest replacing the observed information matrix by the expected information matrix. The latter matrix has the advantage of being positive semidefinite. In our notation, if <bold><italic>A</italic></bold><sub><italic>k</italic></sub> ≈ <italic>d</italic><sup>2</sup><italic>f</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>), then an approximate quadratic surrogate is
<disp-formula id="FD12"><mml:math display="block" id="M21" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The natural impulse is to update <italic>x</italic> by the Newton step
<disp-formula id="FD13"><label>(4)</label><mml:math display="block" id="M22" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
This choice does not necessarily decrease <italic>f</italic>(<bold><italic>x</italic></bold>). Step halving or another form of backtracking restores the descent property.</p>
    <p id="P14">A more valid concern is the effort expended in matrix inversion. If <bold><italic>A</italic></bold><sub><italic>k</italic></sub> is dense and constant, then extracting the spectral decomposition <bold><italic>VDV</italic></bold><sup><italic>t</italic></sup> of <bold><italic>A</italic></bold> reduces formula <xref rid="FD13" ref-type="disp-formula">(4)</xref> to
<disp-formula id="FD14"><mml:math display="block" id="M23" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
which can be implemented as a sequence of matrix-vector multiplications. Alternatively, one can take just a few terms of the series
<disp-formula id="FD15"><mml:math display="block" id="M24" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula>
when <italic>ρ</italic> is sufficiently large. For a generalized linear model, parameter updating involves solving the linear system
<disp-formula id="FD16"><label>(5)</label><mml:math display="block" id="M25" overflow="scroll"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>
for <bold><italic>W</italic></bold><sub><italic>k</italic></sub> a diagonal matrix with positive diagonal entries. This task is equivalent to minimizing the least squares criterion
<disp-formula id="FD17"><label>(6)</label><mml:math display="block" id="M26" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msqrt><mml:mi>ρ</mml:mi></mml:msqrt><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msqrt><mml:mi>ρ</mml:mi></mml:msqrt><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
In the unweighted case, extracting the singular value decomposition <bold><italic>Z</italic></bold> = <bold><italic>USV</italic></bold><sup><italic>T</italic></sup> facilitates solving the system of <xref rid="FD16" ref-type="disp-formula">equations (5)</xref>. The svd decomposition is especially cheap if there is a substantial mismatch between the number rows and columns of <bold><italic>Z</italic></bold>. For sparse <bold><italic>Z</italic></bold>, the conjugate gradient algorithm adapted to least squares (<xref rid="R66" ref-type="bibr">Paige and Saunders, 1982b</xref>) is subject to much less ill conditioning than the standard conjugate gradient algorithm. Indeed, the algorithm LSQR and its sparse version LSMR (<xref rid="R28" ref-type="bibr">Fong and Saunders, 2011</xref>) perform well even when the matrix <inline-formula><mml:math display="inline" id="M27" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msubsup><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msqrt><mml:mi>ρ</mml:mi></mml:msqrt><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is ill conditioned.</p>
    <p id="P15">The proximal distance principle also applies to unconstrained problems. For example, consider the problem of minimizing a penalized loss ℓ(<bold><italic>x</italic></bold>)+<italic>p</italic>(<bold><italic>Ax</italic></bold>). The presence of the linear transformation <bold><italic>Ax</italic></bold> in the penalty complicates optimization. The strategy of parameter splitting introduces a new variable <bold><italic>y</italic></bold> and minimizes ℓ(<bold><italic>x</italic></bold>) + <italic>p</italic>(<bold><italic>y</italic></bold>) subject to the constraint <bold><italic>y</italic></bold> = <bold><italic>Ax</italic></bold>. If <italic>P</italic><sub><italic>M</italic></sub>(<italic>z</italic>) denotes projection onto the manifold
<disp-formula id="FD18"><mml:math display="block" id="M28" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
then the constrained problem can be solved approximately by minimizing the function
<disp-formula id="FD19"><mml:math display="block" id="M29" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">l</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
for large <italic>ρ</italic>. If <italic>P</italic><sub><italic>M</italic></sub>(<italic>z</italic><sub><italic>k</italic></sub>) consists of two subvectors <bold><italic>u</italic></bold><sub><italic>k</italic></sub> and <bold><italic>v</italic></bold><sub><italic>k</italic></sub> corresponding to <bold><italic>x</italic></bold><sub><italic>k</italic></sub> and <bold><italic>y</italic></bold><sub><italic>k</italic></sub>, then the proximal distance updates are
<disp-formula id="FD20"><mml:math display="block" id="M30" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>prox</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="script">l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>    and    </mml:mtext><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>prox</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
    <p id="P16">Given the matrix <bold><italic>A</italic></bold> is <italic>n</italic> × <italic>p</italic>, one can attack the projection by minimizing the function
<disp-formula id="FD21"><mml:math display="block" id="M31" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
This leads to the solution
<disp-formula id="FD22"><mml:math display="block" id="M32" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>+</mml:mo><mml:mstyle><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext>    and    </mml:mtext><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
If <italic>n &lt; p</italic>, then the Woodbury formula
<disp-formula id="FD23"><mml:math display="block" id="M33" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow></mml:math></disp-formula>
reduces the expense of matrix inversion.</p>
    <p id="P17">Traditionally, convex constraints have been posed as inequalities <italic>C</italic> = {<bold><italic>x</italic></bold> : <italic>a</italic>(<bold><italic>x</italic></bold>) ≤ <italic>t</italic>}. <xref rid="R67" ref-type="bibr">Parikh and Boyd (2013)</xref> point out how to project onto such sets. The relevant Lagrangian for projecting an external point <bold><italic>y</italic></bold> amounts to
<disp-formula id="FD24"><mml:math display="block" id="M34" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
with <italic>λ</italic> ≥ 0. The corresponding stationarity condition
<disp-formula id="FD25"><label>(7)</label><mml:math display="block" id="M35" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mo>∇</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
can be interpreted as <italic>a</italic>[prox<sub><italic>λa</italic></sub>(<bold><italic>y</italic></bold>)] = <italic>t</italic>. One can solve this one-dimensional equation for <italic>λ</italic> by bisection. Once <italic>λ</italic> is available, <bold><italic>x</italic></bold> = prox<sub><italic>λa</italic></sub>(<bold><italic>y</italic></bold>) is available as well. <xref rid="R67" ref-type="bibr">Parikh and Boyd (2013)</xref> note that the value <italic>a</italic>[prox<sub><italic>λa</italic></sub>(<bold><italic>y</italic></bold>)] is decreasing in <italic>λ</italic>. One can verify their claim by implicit differentiation of <xref rid="FD25" ref-type="disp-formula">equation (7)</xref>. This gives
<disp-formula id="FD26"><mml:math display="block" id="M36" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∇</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
and consequently the chain rule inequality
<disp-formula id="FD27"><mml:math display="block" id="M37" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:mfrac><mml:mi>a</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mtext>prox</mml:mtext></mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∇</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:math></disp-formula></p>
  </sec>
  <sec id="S3">
    <label>3.</label>
    <title>Convergence: Convex Case</title>
    <p id="P18">In the presence of convexity, the proximal distance algorithm reduces to a proximal gradient algorithm. This follows from the representation
<disp-formula id="FD28"><mml:math display="block" id="M38" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mo>∇</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
involving the penalty <italic>q</italic>(<bold><italic>x</italic></bold>). Thus, the proximal distance algorithm can be expressed as
<disp-formula id="FD29"><mml:math display="block" id="M39" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>prox</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mo>∇</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
In this regard, there is the implicit assumption that <italic>q</italic>(<bold><italic>x</italic></bold>) is 1-smooth. This is indeed the case. According to the Moreau decomposition (<xref rid="R4" ref-type="bibr">Bauschke and Combettes, 2011</xref>), for a single closed convex set <italic>C</italic>
<disp-formula id="FD30"><mml:math display="block" id="M40" overflow="scroll"><mml:mrow><mml:mo>∇</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>prox</mml:mtext></mml:mrow><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mi>C</mml:mi><mml:mo>⋆</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math display="inline" id="M41" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mi>C</mml:mi><mml:mo>⋆</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the Fenchel conjugate of the indicator function
<disp-formula id="FD31"><mml:math display="block" id="M42" overflow="scroll"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mi>∞</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mstyle><mml:mo>∉</mml:mo><mml:mi>C</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
Because proximal operators of closed convex functions are nonexpansive (<xref rid="R4" ref-type="bibr">Bauschke and Combettes, 2011</xref>), the result follows for a single set. For the general penalty <italic>q</italic>(<bold><italic>x</italic></bold>) with <italic>m</italic> sets, the Lipschitz constants are scaled by the convex coefficients <italic>α</italic><sub><italic>i</italic></sub> and added to produce an overall Lipschitz constant of 1.</p>
    <p id="P19">It is enlightening to view the proximal distance algorithm through the lens of concaveconvex programming. Recall that the function
<disp-formula id="FD32"><label>(8)</label><mml:math display="block" id="M43" overflow="scroll"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>∈</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mstyle><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mstyle><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
is closed and convex for any nonempty closed set <italic>C</italic>. Danskin’s theorem (<xref rid="R50" ref-type="bibr">Lange, 2016</xref>) justifies the directional derivative expression
<disp-formula id="FD33"><mml:math display="block" id="M44" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">v</mml:mi></mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>∈</mml:mo><mml:mtext>conv</mml:mtext><mml:msub><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
This equality allows us to identify the subdifferential <italic>∂s</italic>(<bold><italic>x</italic></bold>) as the convex hull conv<italic>P</italic><sub><italic>C</italic></sub>(<italic>x</italic>). For any <bold><italic>y</italic></bold> ∈ <italic>∂s</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>), the supporting hyperplane inequality entails
<disp-formula id="FD34"><mml:math display="block" id="M45" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>d</italic> is a constant not depending on <bold><italic>x</italic></bold>. The same majorization can be generated by rearranging the majorization
<disp-formula id="FD35"><mml:math display="block" id="M46" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
when <bold><italic>y</italic></bold> is the convex combination <inline-formula><mml:math display="inline" id="M47" overflow="scroll"><mml:mrow><mml:munder><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of vectors <italic>p</italic><sub><italic>i</italic></sub> from <italic>P</italic><sub><italic>C</italic></sub>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>). These facts demonstrate that the proximal distance algorithm minimizing
<disp-formula id="FD36"><mml:math display="block" id="M48" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
is a special case of concave-convex programming when <italic>f</italic>(<bold><italic>x</italic></bold>) is convex. It is worth emphasizing that <inline-formula><mml:math display="inline" id="M49" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is often strongly convex regardless of whether <italic>f</italic>(<bold><italic>x</italic></bold>) itself is convex. If we replace the penalty dist(<bold><italic>x</italic></bold><italic>,C</italic>)<sup>2</sup> by the penalty dist(<bold><italic>Dx</italic></bold><italic>,C</italic>)<sup>2</sup> for a matrix <bold><italic>D</italic></bold>, then the function <italic>s</italic>(<bold><italic>Dx</italic></bold>) is still closed and convex, and minimization of <inline-formula><mml:math display="inline" id="M50" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> can also be viewed as an exercise in concave-convex programming.</p>
    <p id="P20">In the presence of convexity, the proximal distance algorithm is guaranteed to converge. Our exposition relies on well-known operator results (<xref rid="R4" ref-type="bibr">Bauschke and Combettes, 2011</xref>). Proximal operators in general and projection operators in particular are nonexpansive and averaged. By definition an averaged operator
<disp-formula id="FD37"><mml:math display="block" id="M51" overflow="scroll"><mml:mrow><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
is a convex combination of a nonexpansive operator <italic>N</italic>(<bold><italic>x</italic></bold>) and the identity operator <bold><italic>I</italic></bold>. The averaged operators on <inline-formula><mml:math display="inline" id="M52" overflow="scroll"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> with <italic>α</italic> ∈ (0,1) form a convex set closed under functional composition. Furthermore, <italic>M</italic>(<bold><italic>x</italic></bold>) and the base operator <italic>N</italic>(<bold><italic>x</italic></bold>) share their fixed points. The celebrated theorem of <xref rid="R46" ref-type="bibr">Krasnosel’skii (1955)</xref> and <xref rid="R58" ref-type="bibr">Mann (1953)</xref> says that if an averaged operator <italic>M</italic>(<bold><italic>x</italic></bold>) = <italic>α</italic><bold><italic>x</italic></bold> + (1 − <italic>α</italic>)<italic>N</italic>(<bold><italic>x</italic></bold>) possesses one or more fixed points, then the iteration scheme <bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub> = <italic>M</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) converges to a fixed point.</p>
    <p id="P21">These results immediately apply to minimization of the penalized loss
<disp-formula id="FD38"><label>(9)</label><mml:math display="block" id="M53" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Given the choice <inline-formula><mml:math display="inline" id="M54" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle stretchy="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mstyle><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the algorithm map <bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub> = prox<sub><italic>ρ</italic></sub>−<sub>1<italic>f</italic></sub>(<bold><italic>y</italic></bold><sub><italic>k</italic></sub>) is an averaged operator, being the composition of two averaged operators. Hence, the Krasnosel’skiiMann theorem guarantees convergence to a fixed point if one or more exist. Now <italic>z</italic> is a fixed point if and only if
<disp-formula id="FD39"><mml:math display="block" id="M55" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
for all <bold><italic>x</italic></bold>. In the presence of convexity, this is equivalent to the directional derivative inequality
<disp-formula id="FD40"><mml:math display="block" id="M56" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">v</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">v</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
for all <bold><italic>v</italic></bold>, which is in turn equivalent to <italic>z</italic> minimizing <italic>h</italic><sub><italic>ρ</italic></sub>(<bold><italic>x</italic></bold>). Hence, if <italic>h</italic><sub><italic>ρ</italic></sub>(<bold><italic>x</italic></bold>) attains its minimum value, then the proximal distance iterates converge to a minimum point.</p>
    <p id="P22">Convergence of the overall proximal distance algorithm is tied to the convergence of the classical penalty method (<xref rid="R6" ref-type="bibr">Beltrami, 1970</xref>). In our setting, the loss is <italic>f</italic>(<bold><italic>x</italic></bold>), and the penalty is <inline-formula><mml:math display="inline" id="M57" overflow="scroll"><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mstyle stretchy="false"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup></mml:mstyle><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. Assuming the objective <italic>f</italic>(<bold><italic>x</italic></bold>) + <italic>ρq</italic>(<bold><italic>x</italic></bold>) is coercive for all <italic>ρ</italic> ≥ 0, the theory mandates that the solution path <bold><italic>x</italic></bold><sub><italic>ρ</italic></sub> is bounded and any limit point of the path attains the minimum value of <italic>f</italic>(<bold><italic>x</italic></bold>) subject to the constraints. Furthermore, if <italic>f</italic>(<bold><italic>x</italic></bold>) is coercive and possesses a unique minimum point in the constraint set <italic>C</italic>, then the path <bold><italic>x</italic></bold><sub><italic>ρ</italic></sub> converges to that point.</p>
    <p id="P23">Proximal distance algorithms often converge at a painfully slow rate. Following <xref rid="R57" ref-type="bibr">Mairal (2013)</xref>, one can readily exhibit a precise bound.</p>
    <p id="P24"><bold>Proposition 1</bold><italic>Suppose C is closed and convex and f</italic>(<bold><italic>x</italic></bold>) <italic>is convex. If the point z minimizes</italic><inline-formula><mml:math display="inline" id="M58" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, <italic>then the proximal distance iterates satisfy</italic><disp-formula id="FD41"><mml:math display="block" id="M59" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p>
    <p id="P25">The <italic>O</italic>(<italic>ρk</italic><sup>−1</sup>) convergence rate of the proximal distance algorithm suggests that one should slowly send <italic>ρ</italic> to ∞ and refuse to wait until convergence occurs for any given <italic>ρ</italic>. It also suggests that Nesterov acceleration may vastly improve the chances for convergence. Nesterov acceleration for the general proximal gradient algorithm with loss <italic>ℓ</italic>(<bold><italic>x</italic></bold>) and penalty <italic>p</italic>(<bold><italic>x</italic></bold>) takes the form
<disp-formula id="FD42"><mml:math display="block" id="M60" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD43"><label>(10)</label><mml:math display="block" id="M61" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>prox</mml:mtext></mml:mrow><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="script">l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∇</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>L</italic> is the Lipschitz constant for ∇<italic>p</italic>(<italic>x</italic>) and <italic>d</italic> is typically chosen to be 3. Nesterov acceleration achieves an <italic>O</italic>(<italic>k</italic><sup>−2</sup>) convergence rate (<xref rid="R71" ref-type="bibr">Su et al., 2014</xref>), which is vastly superior to the <italic>O</italic>(<italic>k</italic><sup>−1</sup>) rate achieved by proximal gradient descent. The Nesterov update possesses the further desirable property of preserving affine constraints. In other words, if <bold><italic>Ax</italic></bold><sub><italic>k</italic>−1</sub> = <bold><italic>b</italic></bold> and <bold><italic>Ax</italic></bold><sub><italic>k</italic></sub> = <bold><italic>b</italic></bold>, then <bold><italic>Az</italic></bold><sub><italic>k</italic></sub> = <bold><italic>b</italic></bold> as well. In subsequent examples, we will accelerate our proximal distance algorithms by applying the algorithm map <italic>M</italic>(<bold><italic>x</italic></bold>) given by <xref rid="FD6" ref-type="disp-formula">equation (2)</xref> to the shifted point <italic>z</italic><sub><italic>k</italic></sub> of <xref rid="FD43" ref-type="disp-formula">equation (10)</xref>, yielding the accelerated update <bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub> = <italic>M</italic>(<italic>z</italic><sub><italic>k</italic></sub>). Algorithm 1 provides a schematic of a proximal distance algorithm with Nesterov acceleration. The recent paper of <xref rid="R30" ref-type="bibr">Ghadimi and Lan (2015)</xref> extends Nestorov acceleration to nonconvex settings.</p>
    <p id="P26">In ideal circumstances, one can prove linear convergence of function values in the framework of <xref rid="R42" ref-type="bibr">Karimi et al. (2016)</xref>.</p>
    <p id="P27"><bold>Proposition 2</bold><italic>Suppose C is closed and convex and f</italic>(<bold><italic>x</italic></bold>) <italic>is L-smooth and μ-strongly convex. Then</italic><inline-formula><mml:math display="inline" id="M62" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula><italic>possesses a unique minimum point</italic><bold><italic>y</italic></bold><italic>, and the proximal</italic>
distance iterates <bold>x</bold><sub>k</sub> satisfy
<disp-formula id="FD44"><mml:math display="block" id="M63" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
    <p id="P28">We now turn to convergence of the penalty function iterates as the penalty constants <italic>ρ</italic><sub><italic>k</italic></sub> tends to ∞. To simplify notation, we restrict attention to a single closed constraint set <italic>S</italic>. Let us start with a proposition requiring no convexity assumptions.</p>
    <p id="P29"><bold>Proposition 3</bold><italic>If f</italic>(<bold><italic>x</italic></bold>) <italic>is continuous and coercive and S is compact, then the proximal distance iterates</italic><bold><italic>x</italic></bold><sub><italic>k</italic></sub><italic>are bounded and the distance to the constraint set satisfies</italic><disp-formula id="FD45"><mml:math display="block" id="M64" overflow="scroll"><mml:mrow><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:mfrac><mml:mi>c</mml:mi><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><italic>for some constant c. If in addition f</italic>(<bold><italic>x</italic></bold>) <italic>is continuously differentiable, then</italic><disp-formula id="FD46"><mml:math display="block" id="M65" overflow="scroll"><mml:mrow><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>
for some further constant d. Similar claims hold for the solutions <bold>y</bold><sub>k</sub> of the penalty problem <inline-formula><mml:math display="inline" id="M66" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> except that the assumption that S is compact can be dropped.</p>
    <p id="P30">As a corollary, if the penalty sequence <italic>ρ</italic><sub><italic>k</italic></sub> tends to ∞, then all limit points of <bold><italic>x</italic></bold><sub><italic>k</italic></sub> must obey the constraint. Proposition 3 puts us into position to prove the next important result.</p>
    <p id="P31"><bold>Proposition 4</bold><italic>If f</italic>(<bold><italic>x</italic></bold>) <italic>is continuously differentiable and coercive and S is convex, then the penalty function iterates defined by</italic><inline-formula><mml:math display="inline" id="M67" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><italic>satisfy</italic><disp-formula id="FD47"><mml:math display="block" id="M68" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <bold>y</bold> attains the constrained minimum and d is the constant identified in Proposition 3.</p>
  </sec>
  <sec id="S4">
    <label>4.</label>
    <title>Convergence: General Case</title>
    <p id="P32">Our strategy for addressing convergence in nonconvex problems fixes <italic>ρ</italic> and relies on Zangwill’s global convergence theorem (<xref rid="R56" ref-type="bibr">Luenberger and Ye, 1984</xref>). This result depends in turn on the notion of a closed multi-valued map <bold><italic>N</italic></bold>(<bold><italic>x</italic></bold>). If <bold><italic>x</italic></bold><sub><italic>k</italic></sub> converges to <bold><italic>x</italic></bold><sub>∞</sub> and <bold><italic>y</italic></bold><sub><italic>k</italic></sub> ∈ <italic>N</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) converges to <bold><italic>y</italic></bold><sub>∞</sub>, then for <bold><italic>N</italic></bold>(<bold><italic>x</italic></bold>) to be closed, we must have <bold><italic>y</italic></bold><sub>∞</sub> ∈ <italic>N</italic>(<bold><italic>x</italic></bold><sub>∞</sub>). The next proposition furnishes a prominent example.</p>
    <p id="P33"><bold>Proposition 5</bold><italic>If S is a closed nonempty set in</italic><inline-formula><mml:math display="inline" id="M69" overflow="scroll"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <italic>then the projection operator P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold>) <italic>is closed. Furthermore, if the sequence</italic><bold><italic>x</italic></bold><sub><italic>k</italic></sub><italic>is bounded, then the set</italic> ∪<sub><italic>k</italic></sub><italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) <italic>is bounded as well</italic>.</p>
    <p id="P34">
      <graphic xlink:href="nihms-1053815-f0001.jpg" position="float" orientation="portrait"/>
    </p>
    <p id="P35">Zangwill’s global convergence theorem is phrased in terms of an algorithm map <italic>M</italic>(<bold><italic>x</italic></bold>) and a real-valued objective <italic>h</italic>(<bold><italic>x</italic></bold>). The theorem requires a critical set Γ outside which <italic>M</italic>(<bold><italic>x</italic></bold>) is closed. Furthermore, all iterates <bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub> ∈ <italic>M</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) must fall within a compact set. Finally, the descent condition <italic>h</italic>(<bold><italic>y</italic></bold>) ≤ <italic>h</italic>(<bold><italic>x</italic></bold>) should hold for all <bold><italic>y</italic></bold> ∈ <italic>M</italic>(<bold><italic>x</italic></bold>), with strict inequality when <bold><italic>x</italic></bold> ∉ Γ. If these conditions are valid, then every convergent subsequence of <bold><italic>x</italic></bold><sub><italic>k</italic></sub> tends to a point in Γ. In the proximal distance context, we define the complement of Γ to consist of the points <bold><italic>x</italic></bold> with
<disp-formula id="FD48"><mml:math display="block" id="M70" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>&lt;</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
for all <bold><italic>y</italic></bold> ∈ <italic>M</italic>(<bold><italic>x</italic></bold>). This definition plus the monotonic nature of the proximal distance algorithm
<disp-formula id="FD49"><mml:math display="block" id="M71" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle><mml:mo>∪</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
force the satisfaction of Zangwill’s final requirement. Note that if <italic>f</italic>(<bold><italic>x</italic></bold>) is differentiable, then a point <bold><italic>x</italic></bold> belongs to Γ whenever <bold>0</bold> ∈ ∇<italic>f</italic>(<italic>x</italic>) + <italic>ρx</italic> − <italic>ρP</italic><sub><italic>S</italic></sub>(<italic>x</italic>).</p>
    <p id="P36">In general, the algorithm map <italic>M</italic>(<italic>x</italic>) is multi-valued in two senses. First, for a given <italic>z</italic><sub><italic>k</italic></sub> ∈ <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>), the minimum may be achieved at multiple points. This contingency is ruled out if the proximal map of <italic>f</italic>(<bold><italic>x</italic></bold>) is unique. Second, because <italic>S</italic> may be nonconvex, the projection may be multi-valued. This sounds distressing, but the points <bold><italic>x</italic></bold><sub><italic>k</italic></sub> where this occurs are exceptionally rare. Accordingly, it makes no practical difference that we restrict the anchor points <italic>z</italic><sub><italic>k</italic></sub> to lie in <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) rather than in conv<italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>).</p>
    <p id="P37"><bold>Proposition 6</bold><italic>If S is a closed nonempty set in</italic><inline-formula><mml:math display="inline" id="M72" overflow="scroll"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <italic>then the projection operator P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold>) <italic>is single valued except on a set of Lebesgue measure</italic> 0.</p>
    <p id="P38">In view of the preceding results, one can easily verify the next proposition.</p>
    <p id="P39"><bold>Proposition 7</bold><italic>The algorithm map M</italic>(<italic>x</italic>) <italic>is everywhere closed</italic>.</p>
    <p id="P40">To apply Zangwill’s global convergence theory, we must in addition prove that the iterates <italic>x</italic><sub><italic>k</italic>+1</sub> = <italic>M</italic>(<italic>x</italic><sub><italic>k</italic></sub>) remain within a compact set. This is true whenever the objective is coercive since the algorithm is a descent algorithm. As noted earlier, the coercivity of <italic>f</italic>(<bold><italic>x</italic></bold>) is a sufficient condition. One can readily concoct other sufficient conditions. For example, if <italic>f</italic>(<bold><italic>x</italic></bold>) is bounded below, say nonnegative, and <italic>S</italic> is compact, then the objective is also coercive. Indeed, if <italic>S</italic> is contained in the ball of radius <italic>r</italic> about the origin, then
<disp-formula id="FD50"><mml:math display="block" id="M73" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mtext>dist</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
which proves that dist(<bold><italic>x</italic></bold><italic>,S</italic>) is coercive. The next proposition summarizes these findings.</p>
    <p id="P41"><bold>Proposition 8</bold><italic>If S is closed and nonempty, the objective</italic><inline-formula><mml:math display="inline" id="M74" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula><italic>is coercive, and the proximal operator</italic> prox<sub><italic>ρ</italic></sub>−<sub>1f</sub>(<italic>x</italic>) <italic>is everywhere nonempty, then all limit points of the iterates</italic>
<bold><italic>x</italic></bold><sub>k+1</sub> ∈ <italic>M</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) <italic>of the proximal distance algorithm occur in the critical set</italic> Γ.</p>
    <p id="P42">This result is slightly disappointing. A limit point <bold><italic>x</italic></bold> could potentially exist with improvement in the objective for some but not all <bold><italic>y</italic></bold> ∈ conv<italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold>). This fault is mitigated by the fact that <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold>) is almost always single valued. In common with other algorithms in nonconvex optimization, we also cannot rule out convergence to a local minimum or a saddlepoint. One can improve on Proposition 8 by assuming that the surrogates <italic>g</italic><sub><italic>ρ</italic></sub>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) are all <italic>μ</italic>-strongly convex. This is a small concession to make because <italic>ρ</italic> is typically large. If <italic>f</italic>(<bold><italic>x</italic></bold>) is convex, then <italic>g</italic><sub><italic>ρ</italic></sub>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) is <italic>ρ</italic>-strongly convex by definition. It is also worth noting that any convex MM surrogate <italic>g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) can be made <italic>μ</italic>-strongly convex by adding the viscosity penalty <inline-formula><mml:math display="inline" id="M75" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> majorizing 0. The addition of a viscosity penalty seldom complicates finding the next iterate <italic>x</italic><sub><italic>n</italic>+1</sub> and has little impact on the rate of convergence when <italic>μ &gt;</italic> 0 is small.</p>
    <p id="P43"><bold>Proposition 9</bold><italic>Under the μ-strongly convexity assumption on the surrogates g</italic><sub><italic>ρ</italic></sub>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>)<italic>, the proximal distance iterates satisfy</italic> lim<sub>k→∞</sub>‖<italic>x</italic><sub>k+1</sub> −<italic>x</italic><sub><italic>k</italic></sub>‖ = 0. <italic>As a consequence, the set of limit points is connected as well as closed. Furthermore, if each limit point is isolated, then the iterates converge to a critical point</italic>.</p>
    <p id="P44">Further progress requires even more structure. Fortunately, what we now pursue applies to generic MM algorithms. We start with the concept of a Fréchet subdifferential (<xref rid="R47" ref-type="bibr">Kruger, 2003</xref>). If <italic>h</italic>(<bold><italic>x</italic></bold>) is a function mapping <inline-formula><mml:math display="inline" id="M76" overflow="scroll"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> into <inline-formula><mml:math display="inline" id="M77" overflow="scroll"><mml:mrow><mml:mi>ℝ</mml:mi><mml:mo>∪</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, then its Fréchet subdifferential at <bold><italic>x</italic></bold> ∈ dom <italic>f</italic> is the set
<disp-formula id="FD51"><mml:math display="block" id="M78" overflow="scroll"><mml:mrow><mml:msup><mml:mo>∂</mml:mo><mml:mi>F</mml:mi></mml:msup><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>:</mml:mo><mml:munder><mml:mrow><mml:mtext>liminf</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The set <italic>∂</italic><sup><italic>F</italic></sup><italic>h</italic>(<bold><italic>x</italic></bold>) is closed, convex, and possibly empty. If <italic>h</italic>(<bold><italic>x</italic></bold>) is convex, then <italic>∂</italic><sup><italic>F</italic></sup><italic>h</italic>(<bold><italic>x</italic></bold>) reduces to its convex subdifferential. If <italic>h</italic>(<bold><italic>x</italic></bold>) is differentiable, then <italic>∂</italic><sup><italic>F</italic></sup><italic>h</italic>(<bold><italic>x</italic></bold>) reduces to its ordinary differential. At a local minimum <bold><italic>x</italic></bold>, Fermat’s rule <bold>0</bold> ∈ <italic>∂</italic><sup><italic>F</italic></sup><italic>h</italic>(<italic>x</italic>) holds.</p>
    <p id="P45"><bold>Proposition 10</bold><italic>In an MM algorithm, suppose that h</italic>(<bold><italic>x</italic></bold>) <italic>is coercive, that the surrogates g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) <italic>are differentiable, and that the algorithm map M</italic>(<bold><italic>x</italic></bold>) <italic>is closed. Then every limit point z of the MM sequence</italic>
<bold><italic>x</italic></bold><sub><italic>k</italic></sub>
<italic>is critical in the sense that</italic>
<bold>0</bold> ∈ <italic>∂</italic><sup><italic>F</italic></sup> (−<italic>h</italic>)(<italic>z</italic>).</p>
    <p id="P46">We will also need to invoke Łojasiewicz’s inequality. This deep result depends on some rather arcane algebraic geometry (<xref rid="R11" ref-type="bibr">Bierstone and Milman, 1988</xref>; <xref rid="R12" ref-type="bibr">Bochnak et al., 2013</xref>). It applies to semialgebraic functions and their more inclusive cousins semianalytic functions and subanalytic functions. For simplicity we focus on semialgebraic functions. The class of semialgebraic subsets of <inline-formula><mml:math display="inline" id="M79" overflow="scroll"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is the smallest class that:
<list list-type="alpha-lower" id="L2"><list-item><p id="P47">contains all sets of the form {<bold><italic>x</italic></bold> : <italic>q</italic>(<bold><italic>x</italic></bold>) &gt; 0} for a polynomial <italic>q</italic>(<bold><italic>x</italic></bold>) in <italic>p</italic> variables,</p></list-item><list-item><p id="P48">is closed under the formation of finite unions, finite intersections, and set complementation.</p></list-item></list>
A function <italic>a</italic> : <inline-formula><mml:math display="inline" id="M80" overflow="scroll"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>p</mml:mi></mml:msup><mml:mo>↦</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>r</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is said to be semialgebraic if its graph is a semialgebraic set of <inline-formula><mml:math display="inline" id="M81" overflow="scroll"><mml:mrow><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The class of real-valued semialgebraic functions contains all polynomials <italic>p</italic>(<bold><italic>x</italic></bold>) and all 0/1 indicators of algebraic sets. It is closed under the formation of sums, products, absolute values, reciprocals when <italic>a</italic>(<bold><italic>x</italic></bold>) 6≠ 0, <italic>n</italic>th roots when <italic>a</italic>(<italic>x</italic>) ≥ 0, and maxima max{<italic>a</italic>(<bold><italic>x</italic></bold>)<italic>, b</italic>(<bold><italic>x</italic></bold>)} and minima min{<italic>a</italic>(<bold><italic>x</italic></bold>)<italic>, b</italic>(<bold><italic>x</italic></bold>)}. For our purposes, it is important to note that dist(<bold><italic>x</italic></bold><italic>,S</italic>) is a semialgebraic function whenever <italic>S</italic> is a semialgebraic set.</p>
    <p id="P50">Łojasiewicz’s inequality in its modern form (<xref rid="R13" ref-type="bibr">Bolte et al., 2007</xref>) requires a function <italic>h</italic>(<bold><italic>x</italic></bold>) to be closed (lower semicontinuous) and subanalytic with a closed domain. If <italic>z</italic> is a critical point of <italic>h</italic>(<bold><italic>x</italic></bold>), then
<disp-formula id="FD52"><mml:math display="block" id="M82" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mi>θ</mml:mi></mml:msup><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
for all <italic>x</italic> ∈ <italic>B</italic><sub><italic>r</italic></sub>(<italic>z</italic>)∩dom<italic>∂</italic><sup><italic>F</italic></sup><italic>h</italic> satisfying <italic>h</italic>(<bold><italic>x</italic></bold>) &gt; <italic>h</italic>(<bold><italic>z</italic></bold>) and all <bold><italic>v</italic></bold> in <italic>∂</italic><sup><italic>F</italic></sup><italic>h</italic>(<bold><italic>x</italic></bold>). Here the exponent <italic>θ</italic> ∈ [0,1), the radius <italic>r</italic>, and the constant <italic>c</italic> depend on <italic>z</italic>. This inequality is valid for semialgebraic functions since they are automatically subanalytic. We will apply Łojasiewicz’s inequality to the limit points of an MM algorithm. The next proposition is an elaboration and expansion of known results (<xref rid="R3" ref-type="bibr">Attouch et al., 2010</xref>; <xref rid="R13" ref-type="bibr">Bolte et al., 2007</xref>; <xref rid="R24" ref-type="bibr">Cui et al., 2018</xref>; <xref rid="R41" ref-type="bibr">Kang et al., 2015</xref>; <xref rid="R52" ref-type="bibr">Le Thi et al., 2009</xref>).</p>
    <p id="P51"><bold>Proposition 11</bold><italic>In an MM algorithm suppose the objective h</italic>(<bold><italic>x</italic></bold>) <italic>is coercive, continuous, and subanalytic and all surrogates g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) <italic>are continuous, μ-strongly convex, and satisfy the L-smoothness condition</italic>
<disp-formula id="FD53"><mml:math display="block" id="M83" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mo>∇</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mo>∇</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<italic>on the compact set</italic> {<italic>x</italic> : <italic>h</italic>(<bold><italic>x</italic></bold>) ≤ <italic>h</italic>(<bold><italic>x</italic></bold><sub>0</sub>)}. <italic>Then the MM iterates</italic>
<bold><italic>x</italic></bold><sub>k+1</sub> = argmin<sub><bold><italic>x</italic></bold></sub><italic>g</italic>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) <italic>converge to a critical point</italic>.</p>
    <p id="P52">The last proposition applies to proximal distance algorithms. The loss <italic>f</italic>(<bold><italic>x</italic></bold>) must be subanalytic and differentiable with a locally Lipschitz gradient. Furthermore, all surrogates <inline-formula><mml:math display="inline" id="M84" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> should be coercive and <italic>μ</italic>-strongly convex. Finally, the constraints sets <italic>S</italic><sub><italic>i</italic></sub> should be subanalytic. Semialgebraic sets and functions will do. Under these conditions and regardless of how the projected points <italic>P</italic><sub><italic>Si</italic></sub>(<bold><italic>x</italic></bold>) are chosen, the MM iterates are guaranteed to converge to a critical point.</p>
  </sec>
  <sec id="S5">
    <label>5.</label>
    <title>Examples</title>
    <p id="P53">The following examples highlight the versatility of proximal distance algorithms in a variety of convex and nonconvex settings. Programming details matter in solving these problems. Individual programs are not necessarily long, but care must be exercised in projecting onto constraints, choosing tuning schedules, folding constraints into the domain of the loss, implementing acceleration, and declaring convergence. All of our examples are coded in the Julia programming language. Whenever possible, competing software was run in the Julia environment via the Julia module MathProgBase (<xref rid="R26" ref-type="bibr">Dunning et al., 2017</xref>; <xref rid="R55" ref-type="bibr">Lubin and Dunning, 2015</xref>). The sparse PCA problem relies on the software of Witten et al. (<xref rid="R74" ref-type="bibr">Witten et al., 2009</xref>), which is coded in R. Convergence is tested at iteration <italic>k</italic> by the two criteria
<disp-formula id="FD54"><mml:math display="block" id="M85" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mtext>     and     dist</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>ϵ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where ϵ<sub>1</sub>=10<sup>−6</sup> and ϵ<sub>2</sub>=10<sup>−4</sup> are typical values. The number of iterations until convergence is about 1000 in most examples. This handicap is offset by the simplicity of each stereotyped update. Our code is available as <xref rid="SD1" ref-type="supplementary-material">supplementary material</xref> to this paper. Readers are encouraged to try the code and adapt it to their own examples.</p>
    <sec id="S6">
      <label>5.1</label>
      <title>Linear Programming</title>
      <p id="P54">Two different tactics suggest themselves for constructing a proximal distance algorithm. The first tactic rolls the standard affine constraints <bold><italic>Ax</italic></bold> = <bold><italic>b</italic></bold> into the domain of the loss function <italic>v</italic><sup><italic>t</italic></sup><italic>x</italic>. The standard nonnegativity requirement <bold><italic>x</italic></bold> ≥ <bold>0</bold> is achieved by penalization. Let <italic>x</italic><sub><italic>k</italic></sub> be the current iterate and <bold><italic>y</italic></bold><sub><italic>k</italic></sub> = (<bold><italic>x</italic></bold><sub><italic>k</italic></sub>)<sub>+</sub> be its projection onto <inline-formula><mml:math display="inline" id="M86" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>ℝ</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Derivation of the proximal distance algorithm relies on the Lagrangian
<disp-formula id="FD55"><mml:math display="block" id="M87" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold">λ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
One can multiply the corresponding stationarity equation
<disp-formula id="FD56"><mml:math display="block" id="M88" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold">λ</mml:mi></mml:mrow></mml:math></disp-formula>
by <bold><italic>A</italic></bold> and solve for the Lagrange multiplier <bold><italic>λ</italic></bold> in the form
<disp-formula id="FD57"><label>(11)</label><mml:math display="block" id="M89" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">λ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:mi mathvariant="bold-italic">A</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
assuming <bold><italic>A</italic></bold> has full row rank. Inserting this value into the stationarity equation gives the MM update
<disp-formula id="FD58"><label>(12)</label><mml:math display="block" id="M90" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>ρ</mml:mi></mml:mfrac><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>ρ</mml:mi></mml:mfrac><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <bold><italic>A</italic></bold><sup>−</sup> = <bold><italic>A</italic></bold><sup><italic>t</italic></sup>(<bold><italic>AA</italic></bold><sup><italic>t</italic></sup>)<sup>−1</sup> is the pseudo-inverse of <bold><italic>A</italic></bold>.</p>
      <p id="P55">The second tactic folds the nonnegativity constraints into the domain of the loss. Let <bold><italic>p</italic></bold><sub><italic>k</italic></sub> denote the projection of <bold><italic>x</italic></bold><sub><italic>k</italic></sub> onto the affine constraint set <bold><italic>Ax</italic></bold> = <bold><italic>b</italic></bold>. Fortunately, the surrogate function <inline-formula><mml:math display="inline" id="M91" overflow="scroll"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> splits the parameters. Minimizing one component at a time gives the update <italic>x</italic><sub><italic>k</italic>+1</sub> with components
<disp-formula id="FD59"><label>(13)</label><mml:math display="block" id="M92" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>max</mml:mtext><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mi>ρ</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The projection <bold><italic>p</italic></bold><sub><italic>k</italic></sub> can be computed via
<disp-formula id="FD60"><label>(14)</label><mml:math display="block" id="M93" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <bold><italic>A</italic></bold><sup>−</sup> is again the pseudo-inverse of <bold><italic>A</italic></bold>.</p>
      <p id="P56"><xref rid="T1" ref-type="table">Table 1</xref> compares the accelerated versions of these two proximal distance algorithms to two efficient solvers. The first is the open-source Splitting Cone Solver (SCS) (<xref rid="R64" ref-type="bibr">O’Donoghue et al., 2016</xref>), which relies on a fast implementation of ADMM. The second is the commercial Gurobi solver, which ships with implementations of both the simplex method and a barrier (interior point) method; in this example, we use its barrier algorithm. The first seven rows of the table summarize linear programs with dense data <bold><italic>A</italic></bold>, <bold><italic>b</italic></bold>, and <italic>v</italic>. The bottom six rows rely on random sparse matrices <bold><italic>A</italic></bold> with sparsity level 0.01. For dense problems, the proximal distance algorithms start the penalty constant <italic>ρ</italic> at 1 and double it every 100 iterations. Because we precompute and cache the pseudoinverse <bold><italic>A</italic></bold><sup>−</sup> of <bold><italic>A</italic></bold>, the updates <xref rid="FD58" ref-type="disp-formula">(12)</xref> and <xref rid="FD59" ref-type="disp-formula">(13)</xref> reduce to vector additions and matrix-vector multiplications.</p>
      <p id="P57">For sparse problems the proximal distance algorithms update <italic>ρ</italic> by a factor of 1.5 every 50 iterations. To avoid computing large pseudoinverses, we appeal to the LSQR variant of the conjugate gradient method (<xref rid="R66" ref-type="bibr">Paige and Saunders, 1982b</xref>,<xref rid="R65" ref-type="bibr">a</xref>) to solve the linear systems <xref rid="FD57" ref-type="disp-formula">(11)</xref> and <xref rid="FD60" ref-type="disp-formula">(14)</xref>. The optima of all four methods agree to about 4 digits of accuracy. It is hard to declare an absolute winner in these comparisons. Gurobi and SCS clearly perform better on low-dimensional problems, but the proximal distance algorithms are competitive as dimensions increase. PD1, the proximal distance algorithm over an affine domain, tends to be more accurate than PD2. If high accuracy is not a concern, then the proximal distance algorithms are easily accelerated with a more aggressive update schedule for <italic>ρ</italic>.</p>
    </sec>
    <sec id="S7">
      <label>5.2.</label>
      <title>Constrained Least Squares</title>
      <p id="P58">Constrained least squares programming subsumes constrained quadratic programming. A typical quadratic program involves minimizing the quadratic <inline-formula><mml:math display="inline" id="M94" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:math></inline-formula> subject to <italic>x</italic> ∈ <italic>C</italic> for a positive definite matrix <italic>Q</italic>. Quadratic programming can be reformulated as least squares by taking the Cholesky decomposition <italic>Q</italic> = <italic>LL</italic><sup><italic>t</italic></sup> of <italic>Q</italic> and noting that
<disp-formula id="FD61"><mml:math display="block" id="M95" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">Q</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">p</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">p</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The constraint <italic>x</italic> ∈ <italic>C</italic> applies in both settings. It is particularly advantageous to reframe a quadratic program as a least squares problem when <italic>Q</italic> is already presented in factored form or when it is nearly singular (<xref rid="R7" ref-type="bibr">Bemporad, 2018</xref>). To simplify subsequent notation, we replace <bold><italic>L</italic></bold><sup><italic>t</italic></sup> by the rectangular matrix <bold><italic>A</italic></bold> and <bold><italic>L</italic></bold><sup>−1</sup><bold><italic>p</italic></bold> by <bold><italic>y</italic></bold>. The key to solving constrained least squares is to express the proximal distance surrogate as
<disp-formula id="FD62"><mml:math display="block" id="M96" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msqrt><mml:mi>ρ</mml:mi></mml:msqrt><mml:msub><mml:mi>P</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msqrt><mml:mi>ρ</mml:mi></mml:msqrt><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
as in <xref rid="FD17" ref-type="disp-formula">equation (6)</xref>. As noted earlier, in sparse problems the update <bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub> can be found by a fast stable conjugate gradient solver.</p>
      <p id="P59"><xref rid="T2" ref-type="table">Table 2</xref> compares the performance of the proximal distance algorithm for least squares estimation with probability-simplex constraints to the open source nonlinear interior point solver Ipopt (<xref rid="R72" ref-type="bibr">Wächter and Biegler, 2005</xref>, <xref rid="R73" ref-type="bibr">2006</xref>) and the interior point method of Gurobi. Simplex constrained problems arise in hyperspectral imaging (<xref rid="R33" ref-type="bibr">Heylen et al., 2011</xref>; <xref rid="R43" ref-type="bibr">Keshava, 2003</xref>), portfolio optimization (<xref rid="R59" ref-type="bibr">Markowitz, 1952</xref>), and density estimation (<xref rid="R17" ref-type="bibr">Bunea et al., 2010</xref>). Test problems were generated by filling an <italic>n</italic>×<italic>p</italic> matrix <bold><italic>A</italic></bold> and an <italic>n</italic>-vector <bold><italic>y</italic></bold> with standard normal deviates. For sparse problems we set the sparsity level of <bold><italic>A</italic></bold> to be 10<italic>/p</italic>. Our setup ensures that <bold><italic>A</italic></bold> has full rank and that the quadratic program has a solution. For the proximal distance algorithm, we start <italic>ρ</italic> at 1 and multiply it by 1.5 every 200 iterations. <xref rid="T2" ref-type="table">Table 2</xref> suggests that the proximal distance algorithm and the interior point solvers perform equally well on small dense problems. However, in high-dimensional and low-accuracy environments, the proximal distance algorithm provides much better scalability.</p>
    </sec>
    <sec id="S8">
      <label>5.3.</label>
      <title>Closest Kinship Matrix</title>
      <p id="P60">In genetics studies, kinship is measured by the fraction of genes two individuals share identical by descent. For a given pedigree, the kinship coefficients for all pairs of individuals appear as entries in a symmetric kinship matrix <bold><italic>Y</italic></bold>. This matrix possesses three crucial properties: a) it is positive semidefinite, b) its entries are nonnegative, and c) its diagonal entries are <inline-formula><mml:math display="inline" id="M97" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> unless some pedigree members are inbred. Inbreeding is the exception rather than the rule. Kinship matrices can be estimated empirically from single nucleotide polymorphism (SNP) data, but there is no guarantee that the three highlighted properties are satisfied. Hence, it helpful to project <bold><italic>Y</italic></bold> to the nearest qualifying matrix.</p>
      <p id="P61">This projection problem is best solved by folding the positive semidefinite constraint into the domain of the Frobenius loss function <inline-formula><mml:math display="inline" id="M98" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:msubsup><mml:mo stretchy="false">‖</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. As we shall see, the alternative of imposing two penalties rather than one is slower and less accurate. Projection onto the constraints implied by conditions b) and c) is trivial. All diagonal entries <italic>x</italic><sub><italic>ii</italic></sub> of <bold><italic>X</italic></bold> are reset to <inline-formula><mml:math display="inline" id="M99" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>, and all off-diagonal entries <italic>x</italic><sub><italic>ij</italic></sub> are reset to max{<italic>x</italic><sub><italic>ij</italic></sub>,0}. If <italic>P</italic>(<bold><italic>X</italic></bold><sub><italic>k</italic></sub>) denotes the current projection, then the proximal distance algorithm minimizes the surrogate
<disp-formula id="FD63"><mml:math display="block" id="M100" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>c</italic><sub><italic>k</italic></sub> is an irrelevant constant. The minimum is found by extracting the spectral decomposition <bold><italic>UDU</italic></bold><sup><italic>t</italic></sup> of <inline-formula><mml:math display="inline" id="M101" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and truncating the negative eigenvalues. This gives the update <bold><italic>X</italic></bold><sub><italic>k</italic>+1</sub> = <bold><italic>UD</italic></bold><sub>+</sub><bold><italic>U</italic></bold><sup><italic>t</italic></sup> in obvious notation. This proximal distance algorithm and its Nesterov acceleration are simple to implement in a numerically oriented language such as Julia. The most onerous part of the calculation is clearly the repeated eigen-decompositions.</p>
      <p id="P62"><xref rid="T3" ref-type="table">Table 3</xref> compares three versions of the proximal distance algorithm to Dykstra’s algorithm (<xref rid="R16" ref-type="bibr">Boyle and Dykstra, 1986</xref>). Higham proposed Dykstra’s algorithm for the related problem of finding the closest correlation matrix <xref rid="R34" ref-type="bibr">Higham (2002)</xref>. In <xref rid="T3" ref-type="table">Table 3</xref> algorithm PD1 is the unadorned proximal distance algorithm, PD2 is the accelerated proximal distance, and PD3 is the accelerated proximal distance algorithm with the positive semidefinite constraints folded into the domain of the loss. On this demanding problem, these algorithms are comparable to Dykstra’s algorithm in speed but slightly less accurate. Acceleration of the proximal distance algorithm is effective in reducing both execution time and error. Folding the positive semidefinite constraint into the domain of the loss function leads to further improvements. The data matrices <bold><italic>M</italic></bold> in these trials were populated by standard normal deviates and then symmetrized by averaging opposing off-diagonal entries. In algorithm PD1 we set <italic>ρ</italic><sub><italic>k</italic></sub> = max{1.2<sup><italic>k</italic></sup>,2<sup>22</sup>}. In the accelerated versions PD2 and PD3 we started <italic>ρ</italic> at 1 and multiplied it by 5 every 100 iterations. At the expense of longer compute times, better accuracy can be achieved by all three proximal distance algorithms with a less aggressive update schedule.</p>
    </sec>
    <sec id="S9">
      <label>5.4.</label>
      <title>Projection onto a Second-Order Cone Constraint</title>
      <p id="P63">Second-order cone programming is one of the unifying themes of convex analysis (<xref rid="R2" ref-type="bibr">Alizadeh and Goldfarb, 2003</xref>; <xref rid="R54" ref-type="bibr">Lobo et al., 1998</xref>). It revolves around conic constraints of the form {<bold><italic>u</italic></bold> : ‖<italic>Au</italic> + <italic>b</italic>‖ ≤ <italic>c</italic><sup><italic>t</italic></sup><bold><italic>u</italic></bold> + <italic>d</italic>}. Projection of a vector <bold><italic>x</italic></bold> onto such a constraint is facilitated by parameter splitting. In this setting parameter splitting introduces a vector <bold><italic>w</italic></bold>, a scalar <italic>r</italic>, and the two affine constraints <bold><italic>w</italic></bold> = <bold><italic>Au</italic></bold> + <bold><italic>b</italic></bold> and <italic>r</italic> = <bold><italic>c</italic></bold><sup><italic>t</italic></sup><bold><italic>u</italic></bold> + <italic>d</italic>. The conic constraint then reduces to the Lorentz cone constraint ‖<italic>w</italic>‖ ≤ <italic>r</italic>, for which projection is straightforward (<xref rid="R15" ref-type="bibr">Boyd and Vandenberghe, 2009</xref>). If we concatenate the parameters into the single vector
<disp-formula id="FD64"><mml:math display="block" id="M102" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mi>r</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
and define <italic>L</italic> = {<bold><italic>y</italic></bold> : ‖<italic>w</italic>‖ ≤ <italic>r</italic>} and <italic>M</italic> = {<bold><italic>y</italic></bold> : <bold><italic>w</italic></bold> = <bold><italic>Au</italic></bold> + <bold><italic>b</italic></bold> and <italic>r</italic> = <bold><italic>c</italic></bold><sup><italic>t</italic></sup><bold><italic>u</italic></bold> + <italic>d</italic>}, then we can rephrase the problem as minimizing <inline-formula><mml:math display="inline" id="M103" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> subject to <bold><italic>y</italic></bold> ∈ <italic>L</italic> ∩ <italic>M</italic>. This is a fairly typical set projection problem except that the <bold><italic>w</italic></bold> and <italic>r</italic> components of <italic>y</italic> are missing in the loss function.</p>
      <p id="P64">Taking a cue from Example 5.1, we incorporate the affine constraints in the domain of the objective function. If we represent projection onto <italic>L</italic> by
<disp-formula id="FD65"><mml:math display="block" id="M104" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
then the Lagrangian generated by the proximal distance algorithm amounts to
<disp-formula id="FD66"><mml:math display="block" id="M105" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>‖</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo>‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold">λ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>θ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
This gives rise to a system of three stationarity equations
<disp-formula id="FD67"><label>(15)</label><mml:math display="block" id="M106" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold">λ</mml:mi><mml:mo>+</mml:mo><mml:mi>θ</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD68"><label>(16)</label><mml:math display="block" id="M107" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="bold">λ</mml:mi></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD69"><label>(17)</label><mml:math display="block" id="M108" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Solving for the multipliers <bold><italic>λ</italic></bold> and <italic>θ</italic> in <xref rid="FD68" ref-type="disp-formula">equations (16)</xref> and <xref rid="FD69" ref-type="disp-formula">(17)</xref> and substituting their values in <xref rid="FD67" ref-type="disp-formula">equation (15)</xref> yield
<disp-formula id="FD70"><mml:math display="block" id="M109" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
This leads to the MM update
<disp-formula id="FD71"><label>(18)</label><mml:math display="block" id="M110" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi><mml:msup><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">w</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>r</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The updates <bold><italic>w</italic></bold><sub><italic>k</italic>+1</sub> = <bold><italic>Au</italic></bold><sub><italic>k</italic>+1</sub> + <bold><italic>b</italic></bold> and <italic>r</italic><sub><italic>k</italic>+1</sub> = <bold><italic>c</italic></bold><sup><italic>t</italic></sup><bold><italic>u</italic></bold><sub><italic>k</italic>+1</sub> + <italic>d</italic> follow from the constraints.</p>
      <p id="P65"><xref rid="T4" ref-type="table">Table 4</xref> compares the proximal distance algorithm to SCS and Gurobi. Echoing previous examples, we tailor the update schedule for <italic>ρ</italic> differently for dense and sparse problems. Dense problems converge quickly and accurately when we set <italic>ρ</italic><sub>0</sub> = 1 and double <italic>ρ</italic> every 100 iterations. Sparse problems require a greater range and faster updates of <italic>ρ</italic>, so we set <italic>ρ</italic><sub>0</sub> = 0.01 and then multiply <italic>ρ</italic> by 2.5 every 10 iterations. For dense problems, it is clearly advantageous to cache the spectral decomposition of <bold><italic>A</italic></bold><sup><italic>t</italic></sup><bold><italic>A</italic></bold> + <bold><italic>cc</italic></bold><sup><italic>t</italic></sup> as suggested in Example 5.2. In this regime, the proximal distance algorithm is as accurate as Gurobi and nearly as fast. SCS is comparable to Gurobi in speed but notably less accurate.</p>
      <p id="P66">With a large sparse constraint matrix <bold><italic>A</italic></bold>, extraction of its spectral decomposition becomes prohibitive. If we let <bold><italic>E</italic></bold> = (<italic>ρ</italic><sup>−1<italic>/</italic>2</sup><bold><italic>I A</italic></bold><sup><italic>t</italic></sup>
<italic>c</italic>), then we must solve a linear system of equations defined by the Gramian matrix <bold><italic>G</italic></bold> = <bold><italic>EE</italic></bold><sup><italic>t</italic></sup>. There are three reasonable options for solving this system. The first relies on computing and caching a sparse Cholesky decomposition of <bold><italic>G</italic></bold>. The second computes the QR decomposition of the sparse matrix <bold><italic>E</italic></bold>. The R part of the QR decomposition coincides with the Cholesky factor. Unfortunately, every time <italic>ρ</italic> changes, the Cholesky or QR decomposition must be redone. The third option is the conjugate gradient algorithm. In our experience the QR decomposition offers superior stability and accuracy. When <bold><italic>E</italic></bold> is very sparse, the QR decomposition is often much faster than the Cholesky decomposition because it avoids forming the dense matrix <bold><italic>A</italic></bold><sup><italic>t</italic></sup><bold><italic>A</italic></bold>. Even when only 5% of the entries of <bold><italic>A</italic></bold> are nonzero, 90% of the entries of <bold><italic>A</italic></bold><sup><italic>t</italic></sup><bold><italic>A</italic></bold> can be nonzero. If exquisite accuracy is not a concern, then the conjugate gradient method provides the fastest update. <xref rid="T4" ref-type="table">Table 4</xref> reflects this choice.</p>
    </sec>
    <sec id="S10">
      <label>5.5.</label>
      <title>Copositive Matrices</title>
      <p id="P67">A symmetric matrix <bold><italic>M</italic></bold> is copositive if its associated quadratic form <bold><italic>x</italic></bold><sup><italic>t</italic></sup><bold><italic>Mx</italic></bold> is nonnegative for all <italic>x</italic> ≥ <bold>0</bold>. Copositive matrices find applications in numerous branches of the mathematical sciences (<xref rid="R8" ref-type="bibr">Berman and Plemmons, 1994</xref>). All positive semidefinite matrices and all matrices with nonnegative entries are copositive. The variational index
<disp-formula id="FD72"><mml:math display="block" id="M111" overflow="scroll"><mml:mrow><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow></mml:math></disp-formula>
is one key to understanding copositive matrices (<xref rid="R35" ref-type="bibr">Hiriart-Urruty and Seeger, 2010</xref>). The constraint set <italic>S</italic> is the intersection of the unit sphere and the nonnegative cone <inline-formula><mml:math display="inline" id="M112" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>ℝ</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>. Projection of an external point <bold><italic>y</italic></bold> onto <italic>S</italic> splits into three cases. When all components of <bold><italic>y</italic></bold> are negative, then <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>y</italic></bold>) = <italic>e</italic><sub><italic>i</italic></sub>, where <italic>y</italic><sub><italic>i</italic></sub> is the least negative component of <bold><italic>y</italic></bold>, and <bold><italic>e</italic></bold><sub><italic>i</italic></sub> is the standard unit vector along coordinate direction <italic>i</italic>. The origin <bold>0</bold> is equidistant from all points of <italic>S</italic>. If any component of <bold><italic>y</italic></bold> is positive, then the projection is constructed by setting the negative components of <bold><italic>y</italic></bold> equal to 0, and standardizing the truncated version of <bold><italic>y</italic></bold> to have Euclidean norm 1.</p>
      <p id="P68">As a test case for the proximal distance algorithm, consider the Horn matrix (<xref rid="R32" ref-type="bibr">Hall and Newman, 1963</xref>)
<disp-formula id="FD73"><mml:math display="block" id="M113" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable columnalign="right"><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="right"><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The value <italic>μ</italic>(<italic>M</italic>) = 0 is attained for the vectors <inline-formula><mml:math display="inline" id="M114" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math display="inline" id="M115" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mn>6</mml:mn></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, and equivalent vectors with their entries permuted. Matrices in higher dimensions with the same Horn pattern of 1’s and −1’s are copositive as well (<xref rid="R38" ref-type="bibr">Johnson and Reams, 2008</xref>). A Horn matrix of odd dimension cannot be written as a positive semidefinite matrix, a nonnegative matrix, or a sum of two such matrices.</p>
      <p id="P69">The proximal distance algorithm minimizes the criterion
<disp-formula id="FD74"><mml:math display="block" id="M116" overflow="scroll"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
and generates the updates
<disp-formula id="FD75"><mml:math display="block" id="M117" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>ρ</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
It takes a gentle tuning schedule to get decent results. The choice <italic>ρ</italic><sub><italic>k</italic></sub> = 1.2<sup><italic>k</italic></sup> converges in 600 to 700 iterations from random starting points and reliably yields objective values below 10<sup>−5</sup> for Horn matrices. The computational burden per iteration is significantly eased by exploiting the cached spectral decomposition of <bold><italic>M</italic></bold>. <xref rid="T5" ref-type="table">Table 5</xref> compares the performance of the proximal distance algorithm to the Mosek solver on a range of Horn matrices. Mosek uses semidefinite programming to decide whether <bold><italic>M</italic></bold> can be decomposed into a sum of a positive semidefinite matrix and a nonnegative matrix. If not, Mosek declares the problem infeasible. Nesterov acceleration improves the final loss for the proximal distance algorithm, but it does not decrease overall computing time.</p>
      <p id="P70">Testing for copositivity is challenging because neither the loss function nor the constraint set is convex. The proximal distance algorithm offers a fast screening device for checking whether a matrix is copositive. On random 1000×1000 symmetric matrices <bold><italic>M</italic></bold>, the method invariably returns a negative index in less than two seconds of computing time. Because the vast majority of symmetric matrices are not copositive, accurate estimation of the minimum is not required. <xref rid="T6" ref-type="table">Table 6</xref> summarizes a few random trials with lower-dimensional symmetric matrices. In higher dimensions, Mosek becomes non-competitive, and Nesterov acceleration is of dubious value.</p>
    </sec>
    <sec id="S11">
      <label>5.6.</label>
      <title>Linear Complementarity Problem</title>
      <p id="P71">The linear complementarity problem (<xref rid="R62" ref-type="bibr">Murty and Yu, 1988</xref>) consists of finding vectors <italic>x</italic> and <italic>y</italic> with nonnegative components such that <bold><italic>x</italic></bold><sup><italic>t</italic></sup><bold><italic>y</italic></bold> = 0 and <bold><italic>y</italic></bold> = <bold><italic>A</italic></bold><italic>x</italic> + <bold><italic>b</italic></bold> for a given square matrix <italic>A</italic> and vector <italic>b</italic>. The natural loss function is <inline-formula><mml:math display="inline" id="M118" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. To project a vector pair (<bold><italic>u</italic></bold>,<bold><italic>v</italic></bold>) onto the nonconvex constraint set, one considers each component pair (<italic>u</italic><sub><italic>i</italic></sub><italic>, v</italic><sub><italic>i</italic></sub>) in turn. If <italic>u</italic><sub><italic>i</italic></sub> ≥ max{<italic>v</italic><sub><italic>i</italic></sub>,0}, then the nearest pair (<bold><italic>x</italic></bold>,<bold><italic>y</italic></bold>) has components (<italic>x</italic><sub><italic>i</italic></sub><italic>, y</italic><sub><italic>i</italic></sub>) = (<italic>u</italic><sub><italic>i</italic></sub>,0). If <italic>v</italic><sub><italic>i</italic></sub> ≥ max{<italic>u</italic><sub><italic>i</italic></sub>,0}, then the nearest pair has components (<italic>x</italic><sub><italic>i</italic></sub><italic>, y</italic><sub><italic>i</italic></sub>) = (0<italic>, v</italic><sub><italic>i</italic></sub>). Otherwise, (<italic>x</italic><sub><italic>i</italic></sub><italic>, y</italic><sub><italic>i</italic></sub>) = (0,0). At each iteration the proximal distance algorithm minimizes the criterion
<disp-formula id="FD76"><mml:math display="block" id="M119" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where (<inline-formula><mml:math display="inline" id="M120" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math display="inline" id="M121" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>) is the projection of (<bold><italic>x</italic></bold><sub><italic>k</italic></sub>, <bold><italic>y</italic></bold><sub><italic>k</italic></sub>) onto the constraint set. The stationarity equations become
<disp-formula id="FD77"><mml:math display="block" id="M122" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD78"><mml:math display="block" id="M123" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Substituting the value of <bold><italic>y</italic></bold> from the second equation into the first equation leads to the updates
<disp-formula id="FD79"><label>(19)</label><mml:math display="block" id="M124" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD80"><mml:math display="block" id="M125" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The linear system <xref rid="FD79" ref-type="disp-formula">(19)</xref> can be solved in low to moderate dimensions by computing and caching the spectral decomposition of <bold><italic>A</italic></bold><sup><italic>t</italic></sup><bold><italic>A</italic></bold> and in high dimensions by the conjugate gradient method. <xref rid="T7" ref-type="table">Table 7</xref> compares the performance of the proximal gradient algorithm to the Gurobi solver on some randomly generated problems.</p>
    </sec>
    <sec id="S12">
      <label>5.7.</label>
      <title>Sparse Principal Components Analysis</title>
      <p id="P72">Let <bold><italic>X</italic></bold> be an <italic>n</italic> × <italic>p</italic> data matrix gathered on <italic>n</italic> cases and <italic>p</italic> predictors. Assume the columns of <bold><italic>X</italic></bold> are centered to have mean 0. Principal component analysis (PCA) (<xref rid="R36" ref-type="bibr">Hotelling, 1933</xref>; <xref rid="R68" ref-type="bibr">Pearson, 1901</xref>) operates on the sample covariance matrix <inline-formula><mml:math display="inline" id="M126" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:math></inline-formula>. Here we formulate a proximal distance algorithm for sparse PCA (SPCA), which has attracted substantial interest in the machine learning community (<xref rid="R10" ref-type="bibr">Berthet and Rigollet, 2013b</xref>,<xref rid="R9" ref-type="bibr">a</xref>; <xref rid="R25" ref-type="bibr">D’Aspremont et al., 2007</xref>; <xref rid="R39" ref-type="bibr">Johnstone and Lu, 2009</xref>; <xref rid="R40" ref-type="bibr">Journée et al., 2010</xref>; <xref rid="R74" ref-type="bibr">Witten et al., 2009</xref>; <xref rid="R76" ref-type="bibr">Zou et al., 2006</xref>). According to a result of Ky Fan (<xref rid="R27" ref-type="bibr">Fan, 1949</xref>), the first <italic>q</italic> principal components (PCs) <bold><italic>u</italic></bold><sub>1</sub>,…,<bold><italic>u</italic></bold><sub><italic>q</italic></sub> can be extracted by maximizing the function tr(<bold><italic>U</italic></bold><sup><italic>t</italic></sup><bold><italic>SU</italic></bold>) subject to the matrix constraint <bold><italic>U</italic></bold><sup><italic>t</italic></sup><bold><italic>U</italic></bold> = <bold><italic>I</italic></bold><sub><italic>q</italic></sub>, where <bold><italic>u</italic></bold><sub><italic>i</italic></sub> is the <italic>i</italic>th column of the <italic>p</italic>×<italic>q</italic> matrix <bold><italic>U</italic></bold>. This constraint set is called a Stiefel manifold. One can impose sparsity by insisting that any given column <bold><italic>u</italic></bold><sub><italic>i</italic></sub> have at most <italic>r</italic> nonzero entries. Alternatively, one can require the entire matrix <italic>U</italic> to have at most <italic>r</italic> nonzero entries. The latter choice permits sparsity to be distributed non-uniformly across columns.</p>
      <p id="P73">Extraction of sparse PCs is difficult for three reasons. First, the Stiefel manifold <bold><italic>M</italic></bold><sub><italic>q</italic></sub> and both sparsity sets are nonconvex. Second, the objective function is concave rather than convex. Third, there is no simple formula or algorithm for projecting onto the intersection of the two constraint sets. Fortunately, it is straightforward to project onto each separately. Let <inline-formula><mml:math display="inline" id="M127" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denote the projection of <bold><italic>U</italic></bold> onto the Stiefel manifold. It is well known that <inline-formula><mml:math display="inline" id="M128" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> can be calculated by extracting a partial singular value decomposition <bold><italic>U</italic></bold> = <bold><italic>V</italic>Σ<italic>W</italic></bold><sup><italic>t</italic></sup> of <bold><italic>U</italic></bold> and setting <italic>P</italic><sub><italic>Mq</italic></sub>(<bold><italic>U</italic></bold>) = <bold><italic>VW</italic></bold><sup><italic>t</italic></sup> (<xref rid="R31" ref-type="bibr">Golub and Van Loan, 2012</xref>). Here <bold><italic>V</italic></bold> and <bold><italic>W</italic></bold> are orthogonal matrices of dimension <italic>p</italic>×<italic>q</italic> and <italic>q</italic>×<italic>q</italic>, respectively, and <bold>Σ</bold> is a diagonal matrix of dimension <italic>q</italic> × <italic>q</italic>. Let <inline-formula><mml:math display="inline" id="M129" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denote the projection of <bold><italic>U</italic></bold> onto the sparsity set
<disp-formula id="FD81"><mml:math display="block" id="M130" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:mn>0</mml:mn><mml:mtext> for at most </mml:mtext><mml:mi>r</mml:mi><mml:mtext> entries of each column </mml:mtext><mml:msub><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Because <inline-formula><mml:math display="inline" id="M131" overflow="scroll"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> operates column by column, it suffices to project each column vector <bold><italic>u</italic></bold><sub><italic>i</italic></sub> to sparsity. This entails nothing more than sorting the entries of <bold><italic>u</italic></bold><sub><italic>i</italic></sub> by magnitude, saving the <italic>r</italic> largest, and sending the remaining <italic>p</italic>−<italic>r</italic> entries to 0. If the entire matrix <bold><italic>U</italic></bold> must have at most <italic>r</italic> nonzero entries, then <bold><italic>U</italic></bold> can be treated as a concatenated vector during projection.</p>
      <p id="P74">The key to a good algorithm is to incorporate the Stiefel constraints into the domain of the objective function (<xref rid="R44" ref-type="bibr">Kiers, 1990</xref>; <xref rid="R45" ref-type="bibr">Kiers and ten Berge, 1992</xref>) and the sparsity constraints into the distance penalty. Thus, we propose decreasing the criterion
<disp-formula id="FD82"><mml:math display="block" id="M132" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
at each iteration subject to the Stiefel constraints. The loss can be majorized via
<disp-formula id="FD83"><mml:math display="block" id="M133" overflow="scroll"><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:mo>−</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">S</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mi>S</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
because <bold><italic>S</italic></bold> is positive semidefinite. The penalty is majorized by
<disp-formula id="FD84"><mml:math display="block" id="M134" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>
up to an irrelevant constant <italic>c</italic><sub><italic>k</italic></sub> since the squared Frobenius norm satisfies the relation <inline-formula><mml:math display="inline" id="M135" overflow="scroll"><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:msubsup><mml:mo stretchy="false">‖</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula> on the Stiefel manifold. It now follows that <italic>f</italic>(<bold><italic>U</italic></bold>) is majorized by
<disp-formula id="FD85"><mml:math display="block" id="M136" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">S</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula>
up to an irrelevant constant. Accordingly, the Stiefel projection
<disp-formula id="FD86"><mml:math display="block" id="M137" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
provides the next MM iterate.</p>
      <p id="P75"><xref rid="F1" ref-type="fig">Figures 1</xref> and <xref rid="F2" ref-type="fig">2</xref> compare the proximal distance algorithm to the SPC function from the R package PMA (<xref rid="R74" ref-type="bibr">Witten et al., 2009</xref>). The breast cancer data from PMA provide the data matrix <bold><italic>X</italic></bold>. The data consist of <italic>p</italic> = 19,672 RNA measurements on <italic>n</italic> = 89 patients. The two figures show computation times and the proportion of variance explained (PVE) by the <italic>p</italic> × <italic>q</italic> loading matrix <bold><italic>U</italic></bold>. For sparse PCA, PVE is defined as <inline-formula><mml:math display="inline" id="M138" overflow="scroll"><mml:mrow><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>q</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mtext>tr</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <bold><italic>X</italic></bold><sub><italic>q</italic></sub> = <bold><italic>XU</italic></bold>(<bold><italic>U</italic></bold><sup><italic>t</italic></sup><bold><italic>U</italic></bold>)<sup>−1</sup><bold><italic>U</italic></bold><sup><italic>t</italic></sup> (<xref rid="R70" ref-type="bibr">Shen and Huang, 2008</xref>). When the loading vectors of <bold><italic>U</italic></bold> are orthogonal, this criterion reduces to the familiar definition tr(<bold><italic>U</italic></bold><sup><italic>t</italic></sup><bold><italic>X</italic></bold><sup><italic>t</italic></sup><bold><italic>XU</italic></bold>)<italic>/</italic>tr(<bold><italic>X</italic></bold><sup><italic>t</italic></sup><bold><italic>X</italic></bold>) of PVE for ordinary PCA. The proximal distance algorithm enforces either matrix-wise or column-wise sparsity. In contrast, SPC enforces only column-wise sparsity via the constraint ‖<italic>u</italic><sub><italic>i</italic></sub>‖<sub>1</sub> ≤ <italic>c</italic> for each column <bold><italic>u</italic></bold><sub><italic>i</italic></sub> of <bold><italic>U</italic></bold>. We take <italic>c</italic> = 8. The number of nonzeroes per loading vector output by SPC dictates the sparsity level for the column-wise version of the proximal distance algorithm. Summing these counts across all columns dictates the sparsity level for the matrix version of the proximal distance algorithm.</p>
      <p id="P76"><xref rid="F1" ref-type="fig">Figures 1</xref> and <xref rid="F2" ref-type="fig">2</xref> demonstrate the superior PVE and computational speed of both proximal distance algorithms versus SPC. The type of projection does not appear to affect the computational performance of the proximal distance algorithm, as both versions scale equally well with <italic>q</italic>. However, the matrix projection, which permits the algorithm to more freely assign nonzeroes to the loadings, attains better PVE than the more restrictive column-wise projection. For both variants of the proximal distance algorithm, Nesterov acceleration improves both fitting accuracy and computational speed, especially as the number of PCs <italic>q</italic> increases.</p>
    </sec>
  </sec>
  <sec id="S13">
    <label>6.</label>
    <title>Discussion</title>
    <p id="P77">The proximal distance algorithm applies to a host of problems. In addition to the linear and quadratic programming examples considered here, our previous paper (<xref rid="R51" ref-type="bibr">Lange and Keys, 2014</xref>) derives and tests algorithms for binary piecewise-linear programming, <italic>ℓ</italic><sub>0</sub> regression, matrix completion (<xref rid="R18" ref-type="bibr">Cai et al., 2010</xref>; <xref rid="R19" ref-type="bibr">Candès and Tao, 2010</xref>; <xref rid="R20" ref-type="bibr">Chen et al., 2012</xref>; <xref rid="R60" ref-type="bibr">Mazumder et al., 2010</xref>), and sparse precision matrix estimation (<xref rid="R29" ref-type="bibr">Friedman et al., 2008</xref>). Other potential applications immediately come to mind. An integer linear program in standard form can be expressed as minimizing <italic>c</italic><sup><italic>t</italic></sup><italic>x</italic> subject to <bold><italic>Ax</italic></bold> + <bold><italic>s</italic></bold> = <bold><italic>b</italic></bold>, <bold><italic>s</italic></bold> ≥ <bold>0</bold>, and <inline-formula><mml:math display="inline" id="M139" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℤ</mml:mi><mml:mi>p</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The latter two constraints can be combined in a single constraint for which projection is trivial. The affine constraints should be folded into the domain of the objective. Integer programming is NP hard, so that the proximal distance algorithm just sketched is merely heuristic. Integer linear programming includes traditional NP hard problems such as the traveling salesman problem, the vertex cover problem, set packing, and Boolean satisfiability. It will be interesting to see if the proximal distance principle is competitive in meeting these challenges. Our experience with the closest lattice point problem (<xref rid="R1" ref-type="bibr">Agrell et al., 2002</xref>) and the eight queens problem suggests that the proximal distance algorithm can be too greedy for hard combinatorial problems. The nonconvex problems solved in this paper are in some vague sense easy combinatorial problems.</p>
    <p id="P78">The behavior of a proximal distance algorithm depends critically on a sensible tuning schedule for increasing <italic>ρ</italic>. Starting <italic>ρ</italic> too high puts too much stress on satisfying the constraints. Incrementing <italic>ρ</italic> too quickly causes the algorithm to veer off the solution path guaranteed by the penalty method. Given the chance of roundoff error even with double precision arithmetic, it is unwise to take <italic>ρ</italic> all the way to ∞. Trial and error can help in deciding whether a given class of problems will benefit from an aggressive update schedule and strict or loose convergence criteria. In problems with little curvature such as linear programming, more conservative updates are probably prudent. The linear programming, closest kinship matrix, and SPCA problems document the value of folding constraints into the domain of the loss. In the same spirit it is wise to minimize the number of constraints. A single penalty for projecting onto the intersection of two constraint sets is almost always preferable to the sum of two penalties for their separate projections. Exceptions to this rule obviously occur when projection onto the intersection is intractable. The integer linear programming problem mentioned previously illustrates these ideas.</p>
    <p id="P79">Our earlier proximal distance algorithms ignored acceleration. In many cases the solutions produced had very low accuracy. The realization that convex proximal distance algorithms can be phrased as proximal gradient algorithms convinced us to try Nesterov acceleration. We now do this routinely on the subproblems with <italic>ρ</italic> fixed. This typically forces tighter path following and a reduction in overall computing times. Our examples generally bear out the contention that Nesterov acceleration is useful in nonconvex problems (<xref rid="R30" ref-type="bibr">Ghadimi and Lan, 2015</xref>). It is noteworthy that the value of acceleration often lies in improving the quality of a solution as much as it does in increasing the rate of convergence. Of course, acceleration cannot prevent convergence to an inferior local minimum.</p>
    <p id="P80">On both convex and nonconvex problems, proximal distance algorithms enjoy global convergence guarantees. On nonconvex problems, one must confine attention to subanalytic sets and subanalytic functions. This minor restriction is not a handicap in practice. Determining local convergence rates is a more vexing issue. For convex problems, we review existing theory for a fixed penalty constant <italic>ρ</italic>. The classical results buttress an <italic>O</italic>(<italic>ρk</italic><sup>−1</sup>) sublinear rate for general convex problems. Better results require restrictive smoothness assumptions on both the objective function and the constraint sets. For instance, when <italic>f</italic>(<bold><italic>x</italic></bold>) is <italic>L</italic>-smooth and strongly convex, linear convergence can be demonstrated. When <italic>f</italic>(<bold><italic>x</italic></bold>) equals a difference of convex functions, proximal distance algorithms reduce to concave-convex programming. <xref rid="R52" ref-type="bibr">Le Thi et al. (2009)</xref> attack convergence in this setting.</p>
    <p id="P81">We hope readers will sense the potential of the proximal distance principle. This simple idea offers insight into many existing algorithms and a straightforward path in devising new ones. Effective proximal and projection operators usually spell the difference between success and failure. The number and variety of such operators is expanding quickly as the field of optimization relinquishes it fixation on convexity. The current paper research leaves many open questions about tuning schedules, rates of convergence, and acceleration in the face of nonconvexity. We welcome the contributions of other mathematical scientists in unraveling these mysteries and in inventing new proximal distance algorithms.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="SD1">
      <label>code</label>
      <media xlink:href="NIHMS1053815-supplement-code.zip" orientation="portrait" xlink:type="simple" id="d36e12879" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S14">
    <title>Acknowledgments</title>
    <p id="P82">We thank Joong-Ho Won for many insightful discussions. In partiular, he pointed out the utility of the least squares criterion (6). Hua Zhou and Kenneth Lange were supported by grants from the National Human Genome Research Institute (HG006139) and the National Institute of General Medical Sciences (GM053275). Kevin Keys was supported by a National Science Foundation Graduate Research Fellowship (DGE-0707424), a Predoctoral Training Grant (HG002536) from the National Human Genome Research Institute, a National Heart, Lung, Blood Institute grant (R01HL135156), the UCSF Bakar Computational Health Sciences Institute, the Gordon and Betty Moore Foundation grant GBMF3834, and the Alfred P. Sloan Foundation grant 2013-10-27 to UC Berkeley through the Moore-Sloan Data Sciences Environment initiative at the Berkeley Institute for Data Science (BIDS).</p>
  </ack>
  <app-group>
    <app id="APP1">
      <label>Appendix A.</label>
      <title>Proofs of the Stated Propositions</title>
      <sec id="S15">
        <label>A.1.</label>
        <title>Proposition 1</title>
        <p id="P83">We first observe that the surrogate function <italic>g</italic><sub><italic>ρ</italic></sub>(<italic>x</italic> | <italic>x</italic><sub><italic>k</italic></sub>) is <italic>ρ</italic>-strongly convex. Consequently, the stationarity condition <inline-formula><mml:math display="inline" id="M140" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mn>0</mml:mn></mml:mstyle><mml:mo>∈</mml:mo><mml:mo>∂</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> implies
<disp-formula id="FD87"><label>(20)</label><mml:math display="block" id="M141" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
for all <italic>x</italic>. In the notation <xref rid="FD38" ref-type="disp-formula">(9)</xref>, the difference
<disp-formula id="FD88"><mml:math display="block" id="M142" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
is <italic>ρ</italic>-smooth because
<disp-formula id="FD89"><mml:math display="block" id="M143" overflow="scroll"><mml:mrow><mml:mo>∇</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ρ</mml:mi><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The tangency conditions <italic>d</italic><sub><italic>ρ</italic></sub>(<italic>x</italic><sub><italic>k</italic></sub> | <italic>x</italic><sub><italic>k</italic></sub>) = 0 and ∇<italic>d</italic><sub><italic>ρ</italic></sub>(<italic>x</italic><sub><italic>k</italic></sub> | <italic>x</italic><sub><italic>k</italic></sub>) = <bold>0</bold> therefore yield
<disp-formula id="FD90"><label>(21)</label><mml:math display="block" id="M144" overflow="scroll"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>∇</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
for all <italic>x</italic>. Combining inequalities <xref rid="FD87" ref-type="disp-formula">(20)</xref> and <xref rid="FD90" ref-type="disp-formula">(21)</xref> gives
<disp-formula id="FD91"><mml:math display="block" id="M145" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Adding the result
<disp-formula id="FD92"><mml:math display="block" id="M146" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
over <italic>k</italic> and invoking the descent property <italic>h</italic><sub><italic>ρ</italic></sub>(<bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub>) ≤ <italic>h</italic><sub><italic>ρ</italic></sub>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) produce the error bound
<disp-formula id="FD93"><mml:math display="block" id="M147" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mfrac><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Setting <italic>x</italic> equal to a minimal point <italic>z</italic> gives the stated result. ■</p>
      </sec>
      <sec id="S16">
        <label>A.2.</label>
        <title>Proposition 2</title>
        <p id="P84">The existence and uniqueness of <bold><italic>y</italic></bold> are obvious. The remainder of the proof hinges on the assumptions that <italic>h</italic><sub><italic>ρ</italic></sub>(<bold><italic>x</italic></bold>) is <italic>μ</italic>-strongly convex and the surrogate <italic>g</italic><sub><italic>ρ</italic></sub>(<bold><italic>x</italic></bold> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) is <italic>L</italic> + <italic>ρ</italic> smooth. The latter assumption yields
<disp-formula id="FD94"><label>(22)</label><mml:math display="block" id="M148" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:mo>∇</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The strong convexity condition <inline-formula><mml:math display="inline" id="M149" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mo>∇</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> implies
<disp-formula id="FD95"><mml:math display="block" id="M150" overflow="scroll"><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">‖</mml:mo><mml:mtext>    </mml:mtext><mml:mo>≥</mml:mo><mml:mo>−</mml:mo><mml:mo>∇</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
It follows that <inline-formula><mml:math display="inline" id="M151" overflow="scroll"><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:mo>≥</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:math></inline-formula>. This last inequality and inequality <xref rid="FD94" ref-type="disp-formula">(22)</xref> produce the Polyak-Łojasiewicz bound
<disp-formula id="FD96"><mml:math display="block" id="M152" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>≥</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Taking <inline-formula><mml:math display="inline" id="M153" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mo>∇</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mo>∇</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the Polyak-Łojasiewicz bound gives
<disp-formula id="FD97"><mml:math display="block" id="M154" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac><mml:mo>∇</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mo>∇</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Rearranging this inequality yields
<disp-formula id="FD98"><mml:math display="block" id="M155" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≤</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
which can be iterated to give the stated bound. ■</p>
      </sec>
      <sec id="S17">
        <label>A.3.</label>
        <title>Proposition 3</title>
        <p id="P85">Consider first the proximal distance iterates. The inequality
<disp-formula id="FD99"><mml:math display="block" id="M156" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:munder><mml:mrow><mml:mtext>sup</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
plus the coerciveness of <italic>f</italic>(<bold><italic>x</italic></bold>) imply that <bold><italic>x</italic></bold><sub><italic>k</italic></sub> is a bounded sequence. The claimed bound now holds for <italic>c</italic> equal to the finite supremum of the sequence 2[sup<sub><italic>x</italic>∈<italic>S</italic></sub>
<italic>f</italic>(<bold><italic>x</italic></bold>) − <italic>f</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>)]. If in addition, <italic>f</italic>(<bold><italic>x</italic></bold>) is continuously differentiable, then the stationarity equation
<disp-formula id="FD100"><mml:math display="block" id="M157" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
and the Cauchy-Schwarz inequality give
<disp-formula id="FD101"><mml:math display="block" id="M158" overflow="scroll"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Dividing this by ‖<italic>x</italic><sub><italic>k</italic></sub> − <italic>P</italic><sub><italic>S</italic></sub>(<italic>x</italic><sub><italic>k</italic>−1</sub>) ‖ and squaring further yield
<disp-formula id="FD102"><mml:math display="block" id="M159" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:msubsup><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Taking <italic>d</italic> = sup<sub><italic>k</italic></sub> ‖∇<italic>f</italic>(<italic>x</italic><sub><italic>k</italic></sub>) ‖<sup>2</sup> over the bounded sequence <bold><italic>x</italic></bold><sub><italic>k</italic></sub> completes the proof. For the penalty method iterates, the bound
<disp-formula id="FD103"><mml:math display="block" id="M160" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>≤</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>
is valid by definition, where <bold><italic>y</italic></bold> attains the constrained minimum. Thus, coerciveness implies that the sequence <bold><italic>y</italic></bold><sub><italic>k</italic></sub> is bounded. When <italic>f</italic>(<bold><italic>x</italic></bold>) is continuously differentiable, the proof of the second claim also applies if we substitute <bold><italic>y</italic></bold><sub><italic>k</italic></sub> for <bold><italic>x</italic></bold><sub><italic>k</italic></sub> and <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>y</italic></bold><sub><italic>k</italic></sub>) for <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold><sub><italic>k</italic>−1</sub>). ■</p>
      </sec>
      <sec id="S18">
        <label>A.4.</label>
        <title>Proposition 4</title>
        <p id="P86">Because the function <inline-formula><mml:math display="inline" id="M161" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is convex and has the value <italic>f</italic>(<bold><italic>y</italic></bold>) and gradient ∇<italic>f</italic>(<bold><italic>y</italic></bold>) at a constrained minimum <bold><italic>y</italic></bold>, the supporting hyperplane principle says
<disp-formula id="FD104"><mml:math display="block" id="M162" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>≥</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The first-order optimality condition ∇<italic>f</italic>(<bold><italic>y</italic></bold>)<sup><italic>t</italic></sup>[<italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>y</italic></bold><sub><italic>k</italic></sub>) − <bold><italic>y</italic></bold>] ≥ 0 holds given <bold><italic>y</italic></bold> is a constrained minimum. Hence, the Cauchy-Schwarz inequality and Proposition 3 imply
<disp-formula id="FD105"><mml:math display="block" id="M163" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≤</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:mo>⋅</mml:mo><mml:mtext>dist</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msqrt><mml:mi>d</mml:mi></mml:msqrt><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>ρ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
■</p>
      </sec>
      <sec id="S19">
        <label>A.5.</label>
        <title>Proposition 5</title>
        <p id="P87">Let <bold><italic>x</italic></bold><sub><italic>k</italic></sub> converge to <bold><italic>x</italic></bold><sub>∞</sub> and <bold><italic>y</italic></bold><sub><italic>k</italic></sub> ∈ <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) converge to <bold><italic>y</italic></bold><sub>∞</sub>. For an arbitrary <bold><italic>y</italic></bold> ∈ <italic>S</italic>, taking limits in the inequality ‖<bold><italic>x</italic></bold><sub><italic>k</italic></sub> −<bold><italic>y</italic></bold><sub><italic>k</italic></sub>‖ ≤ ‖<bold><italic>x</italic></bold><sub><italic>k</italic></sub>–<bold><italic>y</italic></bold>‖ yields ‖<bold><italic>x</italic></bold><sub>∞</sub> −<bold><italic>y</italic></bold><sub>∞</sub>‖ ≤ ‖<italic>x</italic><sub>∞</sub> −<italic>y</italic>‖; consequently, <bold><italic>y</italic></bold><sub>∞</sub> ∈ <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold><sub>∞</sub>). To prove the second assertion, take <bold><italic>y</italic></bold><sub><italic>k</italic></sub>∈ <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) and observe that
<disp-formula id="FD106"><mml:math display="block" id="M164" overflow="scroll"><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>≤</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mspace linebreak="newline"/><mml:mo>≤</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>+</mml:mo><mml:mtext>dist</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
which is bounded above by the constant dist(<bold><italic>x</italic></bold><sub>1</sub><italic>,S</italic>) + 3<sup>sup</sup><sub><italic>m</italic>≥1</sub> ‖<italic>x</italic><sub><italic>m</italic></sub>‖. ■</p>
      </sec>
      <sec id="S20">
        <label>A.6.</label>
        <title>Proposition 6</title>
        <p id="P88">In fact, a much stronger result holds. Since the function <italic>s</italic>(<bold><italic>x</italic></bold>) of <xref rid="FD32" ref-type="disp-formula">equation (8)</xref> is convex and finite, Alexandrov’s theorem (<xref rid="R63" ref-type="bibr">Niculescu and Persson, 2006</xref>) implies that it is almost everywhere twice differentiable. In view of the identities <inline-formula><mml:math display="inline" id="M165" overflow="scroll"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math display="inline" id="M166" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∇</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mtext>dist</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> where <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold>) is single valued, it follows that <italic>P</italic><sub><italic>S</italic></sub>(<bold><italic>x</italic></bold>) = ∇<italic>s</italic>(<bold><italic>x</italic></bold>) is almost everywhere differentiable. ■</p>
      </sec>
      <sec id="S21">
        <label>A.7.</label>
        <title>Proposition 8</title>
        <p id="P89">See the discussion just prior to the statement of the proposition. ■</p>
      </sec>
      <sec id="S22">
        <label>A.8.</label>
        <title>Proposition 9</title>
        <p id="P90">The strong-convexity inequality
<disp-formula id="FD107"><mml:math display="block" id="M167" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula>
and the tangency and domination properties of the algorithm imply
<disp-formula id="FD108"><label>(23)</label><mml:math display="block" id="M168" overflow="scroll"><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi>ρ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mfrac><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Since the difference in function values tends to 0, this validates the stated limit. The remaining assertions follow from Propositions 7.3.3 and 7.3.5 of (<xref rid="R50" ref-type="bibr">Lange, 2016</xref>). ■</p>
      </sec>
      <sec id="S23">
        <label>A.9.</label>
        <title>Proposition 10</title>
        <p id="P91">Let the subsequence <inline-formula><mml:math display="inline" id="M169" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> of the MM sequence <italic>x</italic><sub><italic>k</italic>+1</sub> ∈ <italic>M</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub>) converge to <italic>z</italic>. By passing to a subsubsequence if necessary, we may suppose that <bold><italic>x</italic></bold><sub><italic>km</italic>+1</sub> converges to <bold><italic>y</italic></bold>. Owing to our closedness assumption, <bold><italic>y</italic></bold> ∈ <italic>M</italic>(<italic>z</italic>). Given that <italic>h</italic>(<bold><italic>y</italic></bold>) = <italic>h</italic>(<italic>z</italic>), it is obvious that <italic>z</italic> also minimizes <italic>g</italic>(<bold><italic>x</italic></bold> | <italic>z</italic>) and that <bold>0</bold> = ∇<italic>g</italic>(<italic>z</italic> | <italic>z</italic>). Since the difference Δ(<italic>x</italic> | <italic>z</italic>) = <italic>g</italic>(<italic>x</italic> | <italic>z</italic>) − <italic>h</italic>(<italic>x</italic>) achieves its minimum at <italic>x</italic> = <italic>z</italic>, the Fréchet subdifferential <italic>∂</italic><sup><italic>F</italic></sup> Δ(<italic>x</italic> | <italic>z</italic>) satisfies
<disp-formula id="FD109"><mml:math display="block" id="M170" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>∈</mml:mo><mml:msup><mml:mo>∂</mml:mo><mml:mi>F</mml:mi></mml:msup><mml:mi>Δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>∇</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mo>∂</mml:mo><mml:mi>F</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
It follows that <bold>0</bold> ∈ <italic>∂</italic><sup><italic>F</italic></sup> (−<italic>h</italic>)(<bold><italic>z</italic></bold>). ■</p>
      </sec>
      <sec id="S24">
        <label>A.10.</label>
        <title>Proposition 11</title>
        <p id="P92">Because Δ(<bold><italic>x</italic></bold> | <bold><italic>y</italic></bold>) = <italic>g</italic>(<bold><italic>x</italic></bold> | <bold><italic>y</italic></bold>) − <italic>h</italic>(<bold><italic>x</italic></bold>) achieves its minimum at <bold><italic>x</italic></bold> = <bold><italic>y</italic></bold>, the Fréchet subdifferential <italic>∂</italic><sup><italic>F</italic></sup> Δ(<bold><italic>x</italic></bold> | <bold><italic>y</italic></bold>) satisfies
<disp-formula id="FD110"><mml:math display="block" id="M171" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>∈</mml:mo><mml:msup><mml:mo>∂</mml:mo><mml:mi>F</mml:mi></mml:msup><mml:mi>Δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>∇</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mo>∂</mml:mo><mml:mi>F</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
It follows that −∇<italic>g</italic>(<bold><italic>y</italic></bold> | <bold><italic>y</italic></bold>) ∈ <italic>∂</italic><sup><italic>F</italic></sup> (−<italic>h</italic>)(<bold><italic>y</italic></bold>). Furthermore, by assumption
<disp-formula id="FD111"><mml:math display="block" id="M172" overflow="scroll"><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mo>∇</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mo>≤</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:math></disp-formula>
for all relevant <bold><italic>a</italic></bold> and <bold><italic>b</italic></bold> and <bold><italic>x</italic></bold><sub><italic>k</italic></sub>. In particular, because ∇<italic>g</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic>+1</sub> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>) = <bold>0</bold>, we have
<disp-formula id="FD112"><label>(24)</label><mml:math display="block" id="M173" overflow="scroll"><mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mo>≤</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p id="P93">Let <italic>W</italic> denote the set of limit points. The objective <italic>h</italic>(<bold><italic>x</italic></bold>) is constant on <italic>W</italic> with value <inline-formula><mml:math display="inline" id="M174" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msub><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. According to the Łojasiewicz inequality applied for the subanalytic function <inline-formula><mml:math display="inline" id="M175" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, for each <bold><italic>z</italic></bold> ∈ <italic>W</italic> there exists an open ball <italic>B</italic><sub><italic>r</italic>(<italic>z</italic>)</sub>(<italic>z</italic>) of radius <italic>r</italic>(<italic>z</italic>) around <bold><italic>z</italic></bold> and an exponent <italic>θ</italic>(<bold><italic>z</italic></bold>) ∈ [0,1) such that
<disp-formula id="FD113"><mml:math display="block" id="M176" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">‖</mml:mo><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:math></disp-formula>
for all <bold><italic>u</italic></bold> ∈ <italic>B</italic><sub><italic>r</italic>(<italic>z</italic>)</sub>(<italic>z</italic>) and all <inline-formula><mml:math display="inline" id="M177" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mo>∂</mml:mo><mml:mi>F</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mo>∂</mml:mo><mml:mi>F</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We will apply this inequality to <bold><italic>u</italic></bold> = <bold><italic>x</italic></bold><sub><italic>k</italic></sub> and <bold><italic>v</italic></bold> = −∇<italic>g</italic>(<bold><italic>x</italic></bold><sub><italic>k</italic></sub> | <bold><italic>x</italic></bold><sub><italic>k</italic></sub>). In so doing, we would like to assume that the exponent <italic>θ</italic>(<italic>z</italic>) and constant <italic>c</italic>(<bold><italic>z</italic></bold>) do not depend on <bold><italic>z</italic></bold>. With this end in mind, cover the compact set <italic>W</italic> by a finite number of balls <italic>B</italic><sub><italic>r</italic>(<italic>zi</italic></sub>)(<italic>z</italic><sub><italic>i</italic></sub>) and take <italic>θ</italic> = max<sub><italic>i</italic></sub>
<italic>θ</italic>(<italic>z</italic><sub><italic>i</italic></sub>) &lt; 1 and <italic>c</italic> = max<sub><italic>i</italic></sub>
<italic>c</italic>(<bold><italic>z</italic></bold><sub><italic>i</italic></sub>). For a sufficiently large <italic>K</italic>, every <bold><italic>x</italic></bold><sub><italic>k</italic></sub> with <italic>k</italic> ≥ <italic>K</italic> falls within one of these balls and satisfies <inline-formula><mml:math display="inline" id="M178" overflow="scroll"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Without loss of generality assume <italic>K</italic> = 0. The Łojasiewicz inequality reads
<disp-formula id="FD114"><label>(25)</label><mml:math display="block" id="M179" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mi>θ</mml:mi></mml:msup><mml:mo>≤</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
In combination with the concavity of the function <italic>t</italic><sup>1−<italic>θ</italic></sup> on [0,∞), inequalities <xref rid="FD108" ref-type="disp-formula">(23)</xref>, <xref rid="FD112" ref-type="disp-formula">(24)</xref>, and <xref rid="FD114" ref-type="disp-formula">(25)</xref> imply
<disp-formula id="FD115"><mml:math display="block" id="M180" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msup><mml:mo>≥</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>θ</mml:mi></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mspace linebreak="newline"/><mml:mo>≥</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">‖</mml:mo><mml:mo>∇</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">‖</mml:mo></mml:mrow></mml:mfrac><mml:mfrac><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">‖</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mspace linebreak="newline"/><mml:mo>≥</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>c</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Rearranging this inequality and summing over <italic>k</italic> yield
<disp-formula id="FD116"><mml:math display="block" id="M181" overflow="scroll"><mml:mrow><mml:munderover><mml:mstyle><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mo stretchy="false">‖</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">‖</mml:mo><mml:mo>≤</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>c</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>h</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula>
Thus, the sequence <bold><italic>x</italic></bold><sub><italic>k</italic></sub> is a fast Cauchy sequence and converges to a unique limit in <italic>W</italic>. ■</p>
      </sec>
    </app>
  </app-group>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <mixed-citation publication-type="journal"><name><surname>Agrell</surname><given-names>Erik</given-names></name>, <name><surname>Eriksson</surname><given-names>Thomas</given-names></name>, <name><surname>Vardy</surname><given-names>Alexander</given-names></name>, and <name><surname>Zeger</surname><given-names>Kenneth</given-names></name>. <article-title>Closest point search in lattices</article-title>. <source>IEEE Transactions on Information Theory</source>, <volume>48</volume>(<issue>8</issue>):<fpage>2201</fpage>–<lpage>2214</lpage>, <year>2002</year>.</mixed-citation>
    </ref>
    <ref id="R2">
      <mixed-citation publication-type="journal"><name><surname>Alizadeh</surname><given-names>Farid</given-names></name> and <name><surname>Goldfarb</surname><given-names>Donald</given-names></name>. <article-title>Second-order cone programming</article-title>. <source>Mathematical Programming</source>, <volume>95</volume>:<fpage>3</fpage>–<lpage>51</lpage>, <year>2003</year>.</mixed-citation>
    </ref>
    <ref id="R3">
      <mixed-citation publication-type="journal"><name><surname>Attouch</surname><given-names>Hédy</given-names></name>, <name><surname>Bolte</surname><given-names>Jérôme</given-names></name>, <name><surname>Redont</surname><given-names>Patrick</given-names></name>, and <name><surname>Soubeyran</surname><given-names>Antoine</given-names></name>. <article-title>Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-Łojasiewicz inequality</article-title>. <source>Mathematics of Operations Research</source>, <volume>35</volume>(<issue>2</issue>):<fpage>438</fpage>–<lpage>457</lpage>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R4">
      <mixed-citation publication-type="book"><name><surname>Bauschke</surname><given-names>Heinz H</given-names></name> and <name><surname>Combettes</surname><given-names>Patrick L</given-names></name>. <source>Convex Analysis and Monotone Operator Theory in Hilbert Spaces</source>. <publisher-name>Springer</publisher-name>, <year>2011</year>.</mixed-citation>
    </ref>
    <ref id="R5">
      <mixed-citation publication-type="journal"><name><surname>Beck</surname><given-names>Amir</given-names></name> and <name><surname>Teboulle</surname><given-names>Marc</given-names></name>. <article-title>A fast iterative shrinkage-thresholding algorithm for linear inverse problems</article-title>. <source>SIAM Journal on Imaging Sciences</source>, <volume>2</volume>(<issue>1</issue>):<fpage>183</fpage>–<lpage>202</lpage>, <year>2009</year>.</mixed-citation>
    </ref>
    <ref id="R6">
      <mixed-citation publication-type="book"><name><surname>Beltrami</surname><given-names>Edward J</given-names></name>. <source>An Algorithmic Approach to Nonlinear Analysis and Optimization</source>. <publisher-name>Academic Press</publisher-name>, <year>1970</year>.</mixed-citation>
    </ref>
    <ref id="R7">
      <mixed-citation publication-type="journal"><name><surname>Bemporad</surname><given-names>Alberto</given-names></name>. <article-title>A numerically stable solver for positive semidefinite quadratic programs based on nonnegative least squares</article-title>. <source>IEEE Transactions on Automatic Control</source>, <volume>63</volume>(<issue>2</issue>): <fpage>525</fpage>–<lpage>531</lpage>, <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R8">
      <mixed-citation publication-type="book"><name><surname>Berman</surname><given-names>Abraham</given-names></name> and <name><surname>Plemmons</surname><given-names>Robert J</given-names></name>. <source>Nonnegative Matrices in the Mathematical Sciences</source>. <chapter-title>Classics in Applied Mathematics</chapter-title>
<publisher-name>SIAM</publisher-name>, <year>1994</year>.</mixed-citation>
    </ref>
    <ref id="R9">
      <mixed-citation publication-type="book"><name><surname>Berthet</surname><given-names>Quentin</given-names></name> and <name><surname>Rigollet</surname><given-names>Philippe</given-names></name>. <chapter-title>Complexity theoretic lower bounds for sparse principal component detection</chapter-title> In <source>Conference on Learning Theory</source>, pages <fpage>1046</fpage>–<lpage>1066</lpage>, <year>2013a</year>.</mixed-citation>
    </ref>
    <ref id="R10">
      <mixed-citation publication-type="journal"><name><surname>Berthet</surname><given-names>Quentin</given-names></name> and <name><surname>Rigollet</surname><given-names>Philippe</given-names></name>. <article-title>Optimal detection of sparse principal components in high dimension</article-title>. <source>The Annals of Statistics</source>, <volume>41</volume>(<issue>4</issue>):<fpage>1780</fpage>–<lpage>1815</lpage>, <year>2013b</year>.</mixed-citation>
    </ref>
    <ref id="R11">
      <mixed-citation publication-type="journal"><name><surname>Bierstone</surname><given-names>Edward</given-names></name> and <name><surname>Milman</surname><given-names>Pierre D</given-names></name>. <article-title>Semianalytic and subanalytic sets</article-title>. <source>Publications Mathématiques de l’Institut des Hautes Études Scientifiques</source>, <volume>67</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>42</lpage>, <year>1988</year>.</mixed-citation>
    </ref>
    <ref id="R12">
      <mixed-citation publication-type="book"><name><surname>Bochnak</surname><given-names>Jacek</given-names></name>, <name><surname>Coste</surname><given-names>Michel</given-names></name>, and <name><surname>Roy</surname><given-names>Marie-Françoise</given-names></name>. <source>Real Algebraic Geometry</source>, volume <volume>36</volume><publisher-name>Springer Science &amp; Business Media</publisher-name>, <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="R13">
      <mixed-citation publication-type="journal"><name><surname>Bolte</surname><given-names>Jérôme</given-names></name>, <name><surname>Daniilidis</surname><given-names>Aris</given-names></name>, and <name><surname>Lewis</surname><given-names>Adrian</given-names></name>. <article-title>The Łojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems</article-title>. <source>SIAM Journal on Optimization</source>, <volume>17</volume>(<issue>4</issue>):<fpage>1205</fpage>–<lpage>1223</lpage>, <year>2007</year>.</mixed-citation>
    </ref>
    <ref id="R14">
      <mixed-citation publication-type="book"><name><surname>Borwein</surname><given-names>Jonathan M</given-names></name> and <name><surname>Lewis</surname><given-names>Adrian S</given-names></name>. <source>Convex Analysis and Nonlinear Optimization: Theory and Examples</source>. <chapter-title>CMS Books in Mathematics</chapter-title>
<publisher-name>Springer</publisher-name>, <publisher-loc>New York</publisher-loc>, <edition>2nd edition</edition>, <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="R15">
      <mixed-citation publication-type="book"><name><surname>Boyd</surname><given-names>Stephen</given-names></name> and <name><surname>Vandenberghe</surname><given-names>Lieven</given-names></name>. <source>Convex Optimization</source>. <publisher-name>Cambridge University Press</publisher-name>, <year>2009</year>.</mixed-citation>
    </ref>
    <ref id="R16">
      <mixed-citation publication-type="book"><name><surname>Boyle</surname><given-names>James P</given-names></name> and <name><surname>Dykstra</surname><given-names>Richard L</given-names></name>. <chapter-title>A method for finding projections onto the intersection of convex sets in Hilbert spaces</chapter-title> In <source>Advances in Order Restricted Statistical Inference</source>, pages <fpage>28</fpage>–<lpage>47</lpage>. <publisher-name>Springer</publisher-name>, <year>1986</year>.</mixed-citation>
    </ref>
    <ref id="R17">
      <mixed-citation publication-type="journal"><name><surname>Bunea</surname><given-names>Florentina</given-names></name>, <name><surname>Tsybakov</surname><given-names>Alexandre B</given-names></name>, <name><surname>Wegkamp</surname><given-names>Marten H</given-names></name>, and <name><surname>Barbu</surname><given-names>Adrian</given-names></name>. <article-title>Spades and mixture models</article-title>. <source>The Annals of Statistics</source>, <volume>38</volume>(<issue>4</issue>):<fpage>2525</fpage>–<lpage>2558</lpage>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R18">
      <mixed-citation publication-type="journal"><name><surname>Cai</surname><given-names>Jian-Feng</given-names></name>, <name><surname>Candès</surname><given-names>Emmanuel J</given-names></name>, and <name><surname>Shen</surname><given-names>Zuowei</given-names></name>. <article-title>A singular value thresholding algorithm for matrix completion</article-title>. <source>SIAM Journal on Optimization</source>, <volume>20</volume>:<fpage>1956</fpage>–<lpage>1982</lpage>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R19">
      <mixed-citation publication-type="journal"><name><surname>Candès</surname><given-names>Emmanuel J</given-names></name> and <name><surname>Tao</surname><given-names>Terence</given-names></name>. <article-title>The power of convex relaxation: near-optimal matrix completion</article-title>. <source>IEEE Transactions on Information Theory</source>, <volume>56</volume>:<fpage>2053</fpage>–<lpage>2080</lpage>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R20">
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>Caihua</given-names></name>, <name><surname>He</surname><given-names>Bingsheng</given-names></name>, and <name><surname>Yuan</surname><given-names>Xiaoming</given-names></name>. <article-title>Matrix completion via an alternating direction method</article-title>. <source>IMA Journal of Numerical Analysis</source>, <volume>32</volume>:<fpage>227</fpage>–<lpage>245</lpage>, <year>2012</year>.</mixed-citation>
    </ref>
    <ref id="R21">
      <mixed-citation publication-type="journal"><name><surname>Chi</surname><given-names>Eric C</given-names></name>, <name><surname>Zhou</surname><given-names>Hua</given-names></name>, and <name><surname>Lange</surname><given-names>Kenneth</given-names></name>. <article-title>Distance majorization and its applications</article-title>. <source>Mathematical Programming Series A</source>, <volume>146</volume>:<fpage>409</fpage>–<lpage>436</lpage>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="R22">
      <mixed-citation publication-type="book"><name><surname>Combettes</surname><given-names>Patrick L</given-names></name> and <name><surname>Pesquet</surname><given-names>Jean-Christophe</given-names></name>. <chapter-title>Proximal splitting methods in signal processing</chapter-title> In <source>Fixed-Point Algorithms for Inverse Problems in Science and Engineering</source>, pages <fpage>185</fpage>–<lpage>212</lpage>. <publisher-name>Springer</publisher-name>, <year>2011</year>.</mixed-citation>
    </ref>
    <ref id="R23">
      <mixed-citation publication-type="journal"><name><surname>Courant</surname><given-names>Richard</given-names></name>. <article-title>Variational methods for the solution of problems of equilibrium and vibrations</article-title>. <source>Bulletin of the American Mathematical Society</source>, <volume>49</volume>:<fpage>1</fpage>–<lpage>23</lpage>, <year>1943</year>.</mixed-citation>
    </ref>
    <ref id="R24">
      <mixed-citation publication-type="journal"><name><surname>Cui</surname><given-names>Ying</given-names></name>, <name><surname>Pang</surname><given-names>Jong-Shi</given-names></name>, and <name><surname>Sen</surname><given-names>Bodhisattva</given-names></name>. <article-title>Composite difference-max programs for modern statistical estimation problems</article-title>. <source>SIAM Journal on Optimization</source>, <volume>28</volume>(<issue>4</issue>):<fpage>3344</fpage>–<lpage>3374</lpage>, <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="R25">
      <mixed-citation publication-type="journal"><name><surname>D’Aspremont</surname><given-names>Alexandre</given-names></name>, <name><surname>Ghaoui</surname><given-names>Laurent El</given-names></name>, <name><surname>Jordan</surname><given-names>Michael I</given-names></name>, and <name><surname>Lanckriet</surname><given-names>Gert R G</given-names></name>. <article-title>A direct formulation for sparse PCA using semidefinite programming</article-title>. <source>SIAM Review</source>, <volume>49</volume>(<issue>3</issue>): <fpage>434</fpage>–<lpage>448</lpage>, <year>2007</year>.</mixed-citation>
    </ref>
    <ref id="R26">
      <mixed-citation publication-type="journal"><name><surname>Dunning</surname><given-names>Iain</given-names></name>, <name><surname>Huchette</surname><given-names>Joey</given-names></name>, and <name><surname>Lubin</surname><given-names>Miles</given-names></name>. <article-title>JuMP: A modeling language for mathematical optimization</article-title>. <source>SIAM Review</source>, <volume>59</volume>(<issue>2</issue>):<fpage>295</fpage>–<lpage>320</lpage>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="R27">
      <mixed-citation publication-type="journal"><name><surname>Fan</surname><given-names>Ky</given-names></name>. <article-title>On a theorem of Weyl concerning eigenvalues of linear transformations I</article-title>. <source>Proceedings of the National Academy of Sciences of the United States of America</source>, <volume>35</volume>:<fpage>652</fpage>–<lpage>655</lpage>, <year>1949</year>.<pub-id pub-id-type="pmid">16578320</pub-id></mixed-citation>
    </ref>
    <ref id="R28">
      <mixed-citation publication-type="journal"><name><surname>Fong</surname><given-names>David Chin-Lung</given-names></name> and <name><surname>Saunders</surname><given-names>Michael</given-names></name>. <article-title>Lsmr: An iterative algorithm for sparse least-squares problems</article-title>. <source>SIAM Journal on Scientific Computing</source>, <volume>33</volume>(<issue>5</issue>):<fpage>2950</fpage>–<lpage>2971</lpage>, <year>2011</year>.</mixed-citation>
    </ref>
    <ref id="R29">
      <mixed-citation publication-type="journal"><name><surname>Friedman</surname><given-names>Jerome</given-names></name>, <name><surname>Hastie</surname><given-names>Trevor</given-names></name>, and <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>. <article-title>Sparse inverse covariance estimation with the graphical lasso</article-title>. <source>Biostatistics</source>, <volume>9</volume>:<fpage>432</fpage>–<lpage>441</lpage>, <year>2008</year><comment>ISSN 1468–4357.</comment><pub-id pub-id-type="pmid">18079126</pub-id></mixed-citation>
    </ref>
    <ref id="R30">
      <mixed-citation publication-type="journal"><name><surname>Ghadimi</surname><given-names>Saeed</given-names></name> and <name><surname>Lan</surname><given-names>Guanghui</given-names></name>. <article-title>Accelerated gradient methods for nonconvex nonlinear and stochastic programming</article-title>. <source>Mathematical Programming</source>, <volume>156</volume>(<issue>1</issue>):<fpage>59</fpage>–<lpage>99</lpage>, <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="R31">
      <mixed-citation publication-type="book"><name><surname>Golub</surname><given-names>Gene H</given-names></name> and <name><surname>Van Loan</surname><given-names>Charles F</given-names></name>. <source>Matrix Computations</source>. <publisher-name>JHU Press</publisher-name>, <edition>3rd edition</edition>, <year>2012</year>.</mixed-citation>
    </ref>
    <ref id="R32">
      <mixed-citation publication-type="book"><name><surname>Hall</surname><given-names>Marshall</given-names></name> and <name><surname>Newman</surname><given-names>Morris</given-names></name>. <chapter-title>Copositive and completely positive quadratic forms</chapter-title> In <source>Mathematical Proceedings of the Cambridge Philosophical Society</source>, volume <volume>59</volume>, pages <fpage>329</fpage>–<lpage>339</lpage>. <publisher-name>Cambridge Univ Press</publisher-name>, <year>1963</year>.</mixed-citation>
    </ref>
    <ref id="R33">
      <mixed-citation publication-type="journal"><name><surname>Heylen</surname><given-names>Rob</given-names></name>, <name><surname>Burazerović</surname><given-names>Dževdet</given-names></name>, and <name><surname>Scheunders</surname><given-names>Paul</given-names></name>. <article-title>Fully constrained least squares spectral unmixing by simplex projection</article-title>. <source>IEEE Transactions on Geoscience and Remote Sensing</source>, <volume>49</volume>(<issue>11</issue>):<fpage>4112</fpage>–<lpage>4122</lpage>, <month>11</month><year>2011</year>.</mixed-citation>
    </ref>
    <ref id="R34">
      <mixed-citation publication-type="journal"><name><surname>Higham</surname><given-names>Nicholas J</given-names></name>. <article-title>Computing the nearest correlation matrix - a problem from finance</article-title>. <source>IMA Journal of Numerical Analysis</source>, <volume>22</volume>(<issue>3</issue>):<fpage>329</fpage>–<lpage>343</lpage>, <year>2002</year>.</mixed-citation>
    </ref>
    <ref id="R35">
      <mixed-citation publication-type="journal"><name><surname>Hiriart-Urruty</surname><given-names>Jean-Baptiste</given-names></name> and <name><surname>Seeger</surname><given-names>Alberto</given-names></name>. <article-title>A variational approach to copositive matrices</article-title>. <source>SIAM Review</source>, <volume>52</volume>:<fpage>593</fpage>–<lpage>629</lpage>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R36">
      <mixed-citation publication-type="journal"><name><surname>Hotelling</surname><given-names>Harold</given-names></name>. <article-title>Analysis of a complex of statistical variables into principle components</article-title>. <source>Journal of Educational Psychology</source>, <volume>24</volume>:<fpage>417</fpage>–<lpage>441</lpage>, <year>1933</year>.</mixed-citation>
    </ref>
    <ref id="R37">
      <mixed-citation publication-type="journal"><name><surname>Hunter</surname><given-names>David R</given-names></name> and <name><surname>Lange</surname><given-names>Kenneth</given-names></name>. <article-title>A tutorial on MM algorithms</article-title>. <source>American Statistician</source>, <volume>58</volume>:<fpage>30</fpage>–<lpage>37</lpage>, <year>2004</year>.</mixed-citation>
    </ref>
    <ref id="R38">
      <mixed-citation publication-type="journal"><name><surname>Johnson</surname><given-names>Charles R</given-names></name> and <name><surname>Reams</surname><given-names>Robert</given-names></name>. <article-title>Constructing copositive matrices from interior matrices</article-title>. <source>Electronic Journal of Linear Algebra</source>, <volume>17</volume>:<fpage>9</fpage>–<lpage>20</lpage>, <year>2008</year>.</mixed-citation>
    </ref>
    <ref id="R39">
      <mixed-citation publication-type="journal"><name><surname>Johnstone</surname><given-names>Iain M</given-names></name> and <name><surname>Lu</surname><given-names>Arthur Yu</given-names></name>. <article-title>On consistency and sparsity for principal components analysis in high dimensions</article-title>. <source>Journal of the American Statistical Association</source>, <volume>104</volume>(<issue>486</issue>): <fpage>682</fpage>–<lpage>693</lpage>, <year>2009</year>.<pub-id pub-id-type="pmid">20617121</pub-id></mixed-citation>
    </ref>
    <ref id="R40">
      <mixed-citation publication-type="journal"><name><surname>Journée</surname><given-names>Michel</given-names></name>, <name><surname>Nesterov</surname><given-names>Yurii</given-names></name>, <name><surname>Richtárik</surname><given-names>Peter</given-names></name>, and <name><surname>Sepulchre</surname><given-names>Rodolphe</given-names></name>. <article-title>Generalized power method for sparse principal component analysis</article-title>. <source>Journal of Machine Learning Research</source>, <volume>11</volume>:<fpage>517</fpage>–<lpage>553</lpage>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R41">
      <mixed-citation publication-type="journal"><name><surname>Kang</surname><given-names>Yangyang</given-names></name>, <name><surname>Zhang</surname><given-names>Zhihua</given-names></name>, and <name><surname>Li</surname><given-names>Wu-Jun</given-names></name>. <article-title>On the global convergence of majorization minimization algorithms for nonconvex optimization problems</article-title>. <source>arXiv preprint arXiv:1504.07791</source>, <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="R42">
      <mixed-citation publication-type="book"><name><surname>Karimi</surname><given-names>Hamed</given-names></name>, <name><surname>Nutini</surname><given-names>Julie</given-names></name>, and <name><surname>Schmidt</surname><given-names>Mark</given-names></name>. <chapter-title>Linear convergence of gradient and proximal-gradient methods under the Polyak-Łojasiewicz condition</chapter-title> In <source>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</source>, pages <fpage>795</fpage>–<lpage>811</lpage>. <publisher-name>Springer</publisher-name>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="R43">
      <mixed-citation publication-type="journal"><name><surname>Keshava</surname><given-names>Nirmal</given-names></name>. <article-title>A survey of spectral unmixing algorithms</article-title>. <source>Lincoln Laboratory Journal</source>, <volume>14</volume> (<issue>1</issue>):<fpage>55</fpage>–<lpage>78</lpage>, <year>2003</year>.</mixed-citation>
    </ref>
    <ref id="R44">
      <mixed-citation publication-type="journal"><name><surname>Kiers</surname><given-names>Henk AL</given-names></name>. <article-title>Majorization as a tool for optimizing a class of matrix functions</article-title>. <source>Psychometrika</source>, <volume>55</volume>:<fpage>417</fpage>–<lpage>428</lpage>, <year>1990</year>.</mixed-citation>
    </ref>
    <ref id="R45">
      <mixed-citation publication-type="journal"><name><surname>Kiers</surname><given-names>Henk AL</given-names></name> and <name><surname>ten Berge</surname><given-names>Jos MF</given-names></name>. <article-title>Minimization of a class of matrix trace functions by means of refined majorization</article-title>. <source>Psychometrika</source>, <volume>57</volume>:<fpage>371</fpage>–<lpage>382</lpage>, <year>1992</year>.</mixed-citation>
    </ref>
    <ref id="R46">
      <mixed-citation publication-type="journal"><name><surname>Krasnosel’skii</surname><given-names>Mark Aleksandrovich</given-names></name>. <article-title>Two remarks on the method of successive approximations</article-title>. <source>Uspekhi Matematicheskikh Nauk</source>, <volume>10</volume>(<issue>1</issue>):<fpage>123</fpage>–<lpage>127</lpage>, <year>1955</year>.</mixed-citation>
    </ref>
    <ref id="R47">
      <mixed-citation publication-type="journal"><name><surname>Kruger</surname><given-names>Alexander Ya</given-names></name>. <article-title>On Fréchet subdifferentials</article-title>. <source>Journal of Mathematical Sciences</source>, <volume>116</volume> (<issue>3</issue>):<fpage>3325</fpage>–<lpage>3358</lpage>, <year>2003</year>.</mixed-citation>
    </ref>
    <ref id="R48">
      <mixed-citation publication-type="journal"><name><surname>Landweber</surname><given-names>Louis</given-names></name>. <article-title>An iteration formula for Fredholm integral equations of the first kind</article-title>. <source>American Journal of Mathematics</source>, <volume>73</volume>(<issue>3</issue>):<fpage>615</fpage>–<lpage>624</lpage>, <year>1951</year>.</mixed-citation>
    </ref>
    <ref id="R49">
      <mixed-citation publication-type="book"><name><surname>Lange</surname><given-names>Kenneth</given-names></name>. <source>Optimization</source>. <publisher-name>Springer</publisher-name>, <collab>New York</collab>, <edition>2nd edition</edition>, <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="R50">
      <mixed-citation publication-type="book"><name><surname>Lange</surname><given-names>Kenneth</given-names></name>. <source>MM Optimization Algorithms</source>. <publisher-name>SIAM</publisher-name>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="R51">
      <mixed-citation publication-type="book"><name><surname>Lange</surname><given-names>Kenneth</given-names></name> and <name><surname>Keys</surname><given-names>Kevin L</given-names></name>. <chapter-title>The proximal distance algorithm</chapter-title> In <source>Proceedings of the 2014 International Congress of Mathematicians</source>, pages <fpage>95</fpage>–<lpage>116</lpage>. <publisher-name>Kyung Moon</publisher-name>, <month>8</month>
<year>2014</year>.</mixed-citation>
    </ref>
    <ref id="R52">
      <mixed-citation publication-type="journal"><name><surname>Le Thi</surname><given-names>Hoai An</given-names></name>, <name><surname>Van Ngai</surname><given-names>Huynh</given-names></name>, and <name><surname>Dinh</surname><given-names>Tao Pham</given-names></name>. <article-title>Convergence analysis of DC algorithm for DC programming with subanalytic data</article-title>. <source>Annals of Operation Research Technical Report, LMI, INSA-Rouen</source>, <year>2009</year>.</mixed-citation>
    </ref>
    <ref id="R53">
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>Huan</given-names></name>, <name><surname>Fang</surname><given-names>Cong</given-names></name>, and <name><surname>Lin</surname><given-names>Zhouchen</given-names></name>. <article-title>Convergence rates analysis of the quadratic penalty method and its applications to decentralized distributed optimization</article-title>. <source>arXiv preprint arXiv:1711.10802</source>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="R54">
      <mixed-citation publication-type="journal"><name><surname>Lobo</surname><given-names>Miguel Sousa</given-names></name>, <name><surname>Vandenberghe</surname><given-names>Lieven</given-names></name>, <name><surname>Boyd</surname><given-names>Stephen</given-names></name>, and <name><surname>Lebret</surname><given-names>Hervé</given-names></name>. <article-title>Applications of second-order cone programming</article-title>. <source>Linear Algebra and its Applications</source>, <volume>284</volume>:<fpage>193</fpage>–<lpage>228</lpage>, <year>1998</year>.</mixed-citation>
    </ref>
    <ref id="R55">
      <mixed-citation publication-type="journal"><name><surname>Lubin</surname><given-names>Miles</given-names></name> and <name><surname>Dunning</surname><given-names>Iain</given-names></name>. <article-title>Computing in operations research using Julia</article-title>. <source>INFORMS Journal on Computing</source>, <volume>27</volume>(<issue>2</issue>):<fpage>238</fpage>–<lpage>248</lpage>, <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="R56">
      <mixed-citation publication-type="book"><name><surname>Luenberger</surname><given-names>David G</given-names></name> and <name><surname>Ye</surname><given-names>Yinyu</given-names></name>. <source>Linear and Nonlinear Programming</source>, volume <volume>2</volume>
<publisher-name>Springer</publisher-name>, <year>1984</year>.</mixed-citation>
    </ref>
    <ref id="R57">
      <mixed-citation publication-type="book"><name><surname>Mairal</surname><given-names>Julien</given-names></name>. <chapter-title>Optimization with first-order surrogate functions</chapter-title> In <source>International Conference on Machine Learning</source>, pages <fpage>783</fpage>–<lpage>791</lpage>, <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="R58">
      <mixed-citation publication-type="journal"><name><surname>Mann</surname><given-names>W Robert</given-names></name>. <article-title>Mean value methods in iteration</article-title>. <source>Proceedings of the American Mathematical Society</source>, <volume>4</volume>(<issue>3</issue>):<fpage>506</fpage>–<lpage>510</lpage>, <year>1953</year>.</mixed-citation>
    </ref>
    <ref id="R59">
      <mixed-citation publication-type="journal"><name><surname>Markowitz</surname><given-names>Harry</given-names></name>. <article-title>Portfolio selection</article-title>. <source>The Journal of Finance</source>, <volume>7</volume>(<issue>1</issue>):<fpage>77</fpage>–<lpage>91</lpage>, <year>1952</year>.</mixed-citation>
    </ref>
    <ref id="R60">
      <mixed-citation publication-type="journal"><name><surname>Mazumder</surname><given-names>Rahul</given-names></name>, <name><surname>Hastie</surname><given-names>Trevor</given-names></name>, and <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>. <article-title>Spectral regularization algorithms for learning large incomplete matrices</article-title>. <source>The Journal of Machine Learning Research</source>, <volume>11</volume>: <fpage>2287</fpage>–<lpage>2322</lpage>, <year>2010</year>.<pub-id pub-id-type="pmid">21552465</pub-id></mixed-citation>
    </ref>
    <ref id="R61">
      <mixed-citation publication-type="journal"><name><surname>Moreau</surname><given-names>Jean-Jacques</given-names></name>. <article-title>Fonctions convexes duales et points proximaux dans un espace hilbertien</article-title>. <source>Comptes Rendus de l’Académie des Sciences de Paris A</source>, <volume>255</volume>:<fpage>2897</fpage>–<lpage>2899</lpage>, <year>1962</year>.</mixed-citation>
    </ref>
    <ref id="R62">
      <mixed-citation publication-type="book"><name><surname>Murty</surname><given-names>Katta G</given-names></name> and <name><surname>Yu</surname><given-names>Feng-Tien</given-names></name>. <source>Linear Complementarity, Linear and Nonlinear Programming</source>. <publisher-name>Heldermann Verlag</publisher-name>, <publisher-loc>West Berlin</publisher-loc>, <year>1988</year>.</mixed-citation>
    </ref>
    <ref id="R63">
      <mixed-citation publication-type="book"><name><surname>Niculescu</surname><given-names>Constantin</given-names></name> and <name><surname>Persson</surname><given-names>Lars-Erik</given-names></name>. <source>Convex Functions and their Applications: A Contemporary Approach</source>. <publisher-name>Springer</publisher-name>, <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="R64">
      <mixed-citation publication-type="journal"><name><surname>O’Donoghue</surname><given-names>Brendan</given-names></name>, <name><surname>Chu</surname><given-names>Eric</given-names></name>, <name><surname>Parikh</surname><given-names>Neal</given-names></name>, and <name><surname>Boyd</surname><given-names>Stephen</given-names></name>. <article-title>Conic optimization via operator splitting and homogeneous self-dual embedding</article-title>. <source>Journel of Optimization Theory and Applications</source>, pages <fpage>1</fpage>–<lpage>27</lpage>, <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="R65">
      <mixed-citation publication-type="journal"><name><surname>Paige</surname><given-names>Christopher C</given-names></name> and <name><surname>Saunders</surname><given-names>Michael A</given-names></name>. <article-title>LSQR: An algorithm for sparse linear equations and sparse least squares</article-title>. <source>ACM Transactions on Mathematical Software (TOMS)</source>, <volume>8</volume> (<issue>1</issue>):<fpage>43</fpage>–<lpage>71</lpage>, <year>1982a</year>.</mixed-citation>
    </ref>
    <ref id="R66">
      <mixed-citation publication-type="journal"><name><surname>Paige</surname><given-names>Christopher C</given-names></name> and <name><surname>Saunders</surname><given-names>Michael A</given-names></name>. <article-title>Algorithm 583: LSQR: Sparse linear equations and least squares problems</article-title>. <source>ACM Transactions on Mathematical Software (TOMS)</source>, <volume>8</volume>(<issue>2</issue>): <fpage>195</fpage>–<lpage>209</lpage>, <year>1982b</year>.</mixed-citation>
    </ref>
    <ref id="R67">
      <mixed-citation publication-type="journal"><name><surname>Parikh</surname><given-names>Neal</given-names></name> and <name><surname>Boyd</surname><given-names>Stephen</given-names></name>. <article-title>Proximal algorithms</article-title>. <source>Foundations and Trends in Optimization</source>, <volume>1</volume>(<issue>3</issue>):<fpage>123</fpage>–<lpage>231</lpage>, <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="R68">
      <mixed-citation publication-type="journal"><name><surname>Pearson</surname><given-names>Karl</given-names></name>. <article-title>On lines and planes of closest fit to systems of points in space</article-title>. <source>Philosophical Magazine</source>, <volume>2</volume>(<issue>11</issue>):<fpage>559</fpage>–<lpage>572</lpage>, <year>1901</year>.</mixed-citation>
    </ref>
    <ref id="R69">
      <mixed-citation publication-type="journal"><name><surname>Polyak</surname><given-names>Boris T</given-names></name>. <article-title>The convergence rate of the penalty function method</article-title>. <source>USSR Computational Mathematics and Mathematical Physics</source>, <volume>11</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>12</lpage>, <year>1971</year>.</mixed-citation>
    </ref>
    <ref id="R70">
      <mixed-citation publication-type="journal"><name><surname>Shen</surname><given-names>Haipeng</given-names></name> and <name><surname>Huang</surname><given-names>Jianhua Z</given-names></name>. <article-title>Sparse principal component analysis via regularized low rank matrix approximation</article-title>. <source>Journal of Multivariate Analysis</source>, <volume>99</volume>:<fpage>1015</fpage>–<lpage>1034</lpage>, <year>2008</year>.</mixed-citation>
    </ref>
    <ref id="R71">
      <mixed-citation publication-type="book"><name><surname>Su</surname><given-names>Weijie</given-names></name>, <name><surname>Boyd</surname><given-names>Stephen</given-names></name>, and <name><surname>Candès</surname><given-names>Emmanuel</given-names></name>. <chapter-title>A differential equation for modeling Nesterov’s accelerated gradient method: Theory and insights</chapter-title> In <source>Advances in Neural Information Processing Systems</source>, pages <fpage>2510</fpage>–<lpage>2518</lpage>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="R72">
      <mixed-citation publication-type="journal"><name><surname>Wächter</surname><given-names>Andreas</given-names></name> and <name><surname>Biegler</surname><given-names>Lorenz T</given-names></name>. <article-title>Line search filter methods for nonlinear programming: Motivation and global convergence</article-title>. <source>SIAM Journal on Optimization</source>, <volume>16</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>31</lpage>, <year>2005</year>.</mixed-citation>
    </ref>
    <ref id="R73">
      <mixed-citation publication-type="journal"><name><surname>Wächter</surname><given-names>Andreas</given-names></name>and <name><surname>Biegler</surname><given-names>Lorenz T</given-names></name>. <article-title>On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming</article-title>. <source>Mathematical Programming</source>, <volume>106</volume>(<issue>1</issue>):<fpage>25</fpage>–<lpage>57</lpage>, <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="R74">
      <mixed-citation publication-type="journal"><name><surname>Witten</surname><given-names>Daniela M</given-names></name>, <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>, and <name><surname>Hastie</surname><given-names>Trevor</given-names></name>. <article-title>A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis</article-title>. <source>Biostatistics</source>, <volume>10</volume>(<issue>3</issue>):<fpage>515</fpage>–<lpage>534</lpage>, <year>2009</year>.<pub-id pub-id-type="pmid">19377034</pub-id></mixed-citation>
    </ref>
    <ref id="R75">
      <mixed-citation publication-type="book"><name><surname>Xu</surname><given-names>Jason</given-names></name>, <name><surname>Chi</surname><given-names>Eric</given-names></name>, and <name><surname>Lange</surname><given-names>Kenneth</given-names></name>. <chapter-title>Generalized linear model regression under distanceto-set penalties</chapter-title> In <source>Advances in Neural Information Processing Systems</source>, pages <fpage>1385</fpage>–<lpage>1394</lpage>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="R76">
      <mixed-citation publication-type="journal"><name><surname>Zou</surname><given-names>Hui</given-names></name>, <name><surname>Hastie</surname><given-names>Trevor</given-names></name>, and <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>. <article-title>Sparse principal components analysis</article-title>. <source>Journal of Computational and Graphical Statistics</source>, <volume>15</volume>(<issue>2</issue>):<fpage>262</fpage>–<lpage>282</lpage>, <year>2006</year>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="F1" orientation="portrait" position="float">
    <label>Figure 1:</label>
    <caption>
      <p id="P94">Proportion of variance explained by <italic>q</italic> PCs for each algorithm. Here PD1 is the accelerated proximal distance algorithm enforcing matrix sparsity, PD2 is the accelerated proximal distance algorithm enforcing column-wise sparsity, and SPC is the orthogonal sparse PCA method from PMA.</p>
    </caption>
    <graphic xlink:href="nihms-1053815-f0002"/>
  </fig>
  <fig id="F2" orientation="portrait" position="float">
    <label>Figure 2:</label>
    <caption>
      <p id="P95">Computation times for <italic>q</italic> PCs for each algorithm. Here PD1 is the accelerated proximal distance algorithm enforcing matrix sparsity, PD2 is the accelerated proximal distance algorithm enforcing column-wise sparsity, and SPC is the orthogonal sparse PCA method from PMA.</p>
    </caption>
    <graphic xlink:href="nihms-1053815-f0003"/>
  </fig>
  <table-wrap id="T1" position="float" orientation="portrait">
    <label>Table 1:</label>
    <caption>
      <p id="P96">CPU times and optima for linear programming. Here <italic>m</italic> is the number of constraints, <italic>n</italic> is the number of variables, PD1 is the proximal distance algorithm over an affine domain, PD2 is the proximal distance algorithm over a nonnegative domain, SCS is the Splitting Cone Solver, and Gurobi is the Gurobi solver. After <italic>m</italic> = 512 the constraint matrix <bold><italic>A</italic></bold> is initialized to be sparse with sparsity level <italic>s</italic> = 0.01.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Dimensions</th>
          <th colspan="4" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Optima</th>
          <th colspan="4" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">CPU Times (secs)</th>
        </tr>
        <tr>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic>m</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic>n</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD1</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD2</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">SCS</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Gurobi</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD1</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD2</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">SCS</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Gurobi</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.2629</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.2629</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.2629</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.2629</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0142</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0010</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0034</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0038</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.0455</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.0457</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.0456</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.0455</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0212</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0021</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0009</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0011</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.4513</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.4515</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.4514</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.4513</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0361</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0048</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0018</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0029</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.4226</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.4231</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.4225</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.4223</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0847</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0104</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0090</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0036</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6.2398</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6.2407</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6.2397</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6.2398</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1428</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0151</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0140</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0055</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">128</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">14.671</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">14.674</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">14.671</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">14.671</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.2117</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0282</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0587</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0088</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">128</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">256</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">27.116</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">27.125</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">27.116</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">27.116</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.3993</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0728</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.8436</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0335</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">256</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">512</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">58.501</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">58.512</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">58.494</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">58.494</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.7426</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1538</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.5409</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1954</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">512</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1024</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">135.35</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">135.37</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">135.34</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">135.34</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.6413</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.5799</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.0648</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.7179</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">1024</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2048</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">254.50</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">254.55</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">254.47</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">254.48</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.9541</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.2127</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.9433</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.6787</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">2048</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4096</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">533.29</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">533.35</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">533.23</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">533.23</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">7.3669</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">17.318</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">25.614</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.2475</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">4096</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8192</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">991.78</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">991.88</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">991.67</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">991.67</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">30.799</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">95.974</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">98.347</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">46.957</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">8192</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">16384</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2058.8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2059.1</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2058.5</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2058.5</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">316.44</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">623.42</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">454.23</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">400.59</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T2" position="float" orientation="portrait">
    <label>Table 2:</label>
    <caption>
      <p id="P97">CPU times and optima for simplex-constrained least squares. Here <inline-formula><mml:math display="inline" id="M3" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, PD is the proximal distance algorithm, IPOPT is the Ipopt solver, and Gurobi is the Gurobi solver. After <italic>n</italic> = 1024, the predictor matrix <italic>A</italic> is sparse.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Dimensions</th>
          <th colspan="3" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Optima</th>
          <th colspan="3" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">CPU Times</th>
        </tr>
        <tr>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic>n</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic>p</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">IPOPT</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Gurobi</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">IPOPT</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Gurobi</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.1515</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.1515</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.1515</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0038</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0044</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0010</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10.8225</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10.8225</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10.8225</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0036</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0039</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0010</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">29.6218</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">29.6218</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">29.6218</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0079</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0079</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0019</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">128</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">43.2626</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">43.2626</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">43.2626</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0101</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0078</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0033</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">256</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">128</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">111.7642</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">111.7642</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">111.7642</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0872</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0151</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0136</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">512</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">256</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">231.6455</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">231.6454</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">231.6454</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1119</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0710</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0619</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">1024</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">512</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">502.1276</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">502.1276</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">502.1276</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.2278</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.4013</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.2415</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">2048</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1024</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">994.2447</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">994.2447</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">994.2447</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.2575</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.3346</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.1682</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">4096</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2048</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2056.8381</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2056.8381</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2056.8381</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.3253</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">15.2214</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">7.4971</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">8192</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4096</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4103.4611</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4103.4611</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4103.4611</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.0289</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">146.1604</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">49.7411</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">16384</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8192</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8295.2136</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8295.2136</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8295.2136</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6.8739</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">732.1039</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">412.3612</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T3" position="float" orientation="portrait">
    <label>Table 3:</label>
    <caption>
      <p id="P98">CPU times and optima for the closest kinship matrix problem. Here the kinship matrix is <italic>n</italic> × <italic>n</italic>, PD1 is the proximal distance algorithm, PD2 is the accelerated proximal distance, PD3 is the accelerated proximal distance algorithm with the positive semidefinite constraints folded into the domain of the loss, and Dykstra is Dykstra’s adaptation of alternating projections. All times are in seconds.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">Size</th>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">PD1</th>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">PD2</th>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">PD3</th>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Dykstra</th>
        </tr>
        <tr>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic>n</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Loss</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Time</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Loss</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Time</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Loss</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Time</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Loss</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Time</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.36</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.00</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.86</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.10</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.86</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.86</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.86</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.00</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">18.77</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.21</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">18.78</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.03</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">18.78</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.03</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">18.78</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.00</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">45.10</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.84</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">45.12</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.18</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">45.12</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.12</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">45.12</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.02</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">169.58</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.36</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">169.70</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.61</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">169.70</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.52</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">169.70</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.37</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">837.85</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">16.77</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">838.44</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.90</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">838.43</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.63</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">838.42</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.32</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">128</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3276.41</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">91.94</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3279.44</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">18.00</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3279.25</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">14.83</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3279.23</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">19.73</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">256</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">14029.07</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">403.59</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">14045.30</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">89.58</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">14043.59</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">64.89</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">14043.46</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">72.79</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T4" position="float" orientation="portrait">
    <label>Table 4:</label>
    <caption>
      <p id="P99">CPU times and optima for the second-order cone projection. Here <italic>m</italic> is the number of constraints, <italic>n</italic> is the number of variables, PD is the accelerated proximal distance algorithm, SCS is the Splitting Cone Solver, and Gurobi is the Gurobi solver. After <italic>m</italic> = 512 the constraint matrix <bold><italic>A</italic></bold> is initialized with sparsity level 0.01.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Dimensions</th>
          <th colspan="3" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Optima</th>
          <th colspan="3" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">CPU Seconds</th>
        </tr>
        <tr>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic>m</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic>n</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">SCS</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Gurobi</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">SCS</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Gurobi</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">2</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.10598</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.10607</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.10598</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0043</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0103</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0026</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.00000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.00000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.00000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0003</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0009</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0022</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.88988</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.88991</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.88988</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0557</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0011</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0027</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.16514</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.16520</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.16514</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0725</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0012</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0040</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.03855</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.03864</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.03853</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0952</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0019</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0094</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">128</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.86894</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.86962</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4.86895</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1225</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0065</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0403</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">128</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">256</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10.5863</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10.5843</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">10.5863</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1975</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0810</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0868</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">256</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">512</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">31.1039</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">31.0965</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">31.1039</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.5463</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.3995</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.3405</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">512</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1024</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">27.0483</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">27.0475</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">27.0483</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.7667</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.6692</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.0189</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">1024</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2048</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.45578</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.45569</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.45569</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.5352</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.3691</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.5489</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">2048</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">4096</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.22936</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.22930</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.22921</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.0845</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.4531</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.5521</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">4096</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">8192</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.72306</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.72202</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.72209</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.1404</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">17.272</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">15.204</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">8192</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">16384</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.36191</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.36116</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.36144</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">13.979</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">133.25</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">88.024</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T5" position="float" orientation="portrait">
    <label>Table 5:</label>
    <caption>
      <p id="P100">CPU times (seconds) and optima for approximating the Horn variational index of a Horn matrix. Here <italic>n</italic> is the size of Horn matrix, PD is the proximal distance algorithm, aPD is the accelerated proximal distance algorithm, and Mosek is the Mosek solver.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">Dimension</th>
          <th colspan="3" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Optima</th>
          <th colspan="3" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">CPU Seconds</th>
        </tr>
        <tr>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic>n</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">aPD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Mosek</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">aPD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Mosek</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">feasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.5555</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0124</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.7744</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">5</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0039</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0086</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0276</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000021</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">feasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0059</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0083</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0050</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">9</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000045</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0055</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0072</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0082</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000377</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000001</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">feasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0204</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0237</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0185</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">17</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000441</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000001</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0204</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0378</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0175</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.001610</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000007</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">feasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0288</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0288</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1211</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">33</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.002357</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000009</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0242</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0346</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1294</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.054195</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000026</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">feasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0415</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0494</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.6284</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">65</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.006985</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000026</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0431</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0551</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.7862</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T6" position="float" orientation="portrait">
    <label>Table 6:</label>
    <caption>
      <p id="P101">CPU times and optima for testing the copositivity of random symmetric matrices. Here <italic>n</italic> is the size of matrix, PD is the proximal distance algorithm, aPD is the accelerated proximal distance algorithm, and Mosek is the Mosek solver.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">Dimension</th>
          <th colspan="3" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Optima</th>
          <th colspan="3" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">CPU Seconds</th>
        </tr>
        <tr>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic>n</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">aPD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Mosek</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">aPD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Mosek</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−0.391552</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−0.391561</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0029</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0031</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0024</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−0.911140</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−2.050316</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0037</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0044</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0045</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−1.680697</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−1.680930</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0199</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0272</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0062</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−2.334520</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−2.510781</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0261</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0242</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0441</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.821927</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.628060</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0393</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0437</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.6559</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">128</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−5.473609</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−5.475879</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0792</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0798</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">38.3919</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">256</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−7.956365</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−7.551814</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">infeasible</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1632</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.1797</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">456.1500</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="T7" position="float" orientation="portrait">
    <label>Table 7:</label>
    <caption>
      <p id="P102">CPU times (seconds) and optima for the linear complementarity problem with randomly generated data. Here <italic>n</italic> is the size of matrix, PD is the accelerated proximal distance algorithm, and Gurobi is the Gurobi solver.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1" colspan="1">Dimension</th>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">Optima</th>
          <th colspan="2" align="center" valign="middle" style="border-bottom: solid 1px" rowspan="1">CPU Seconds</th>
        </tr>
        <tr>
          <th align="center" valign="middle" rowspan="1" colspan="1">
            <italic>n</italic>
          </th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Mosek</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">PD</th>
          <th align="center" valign="middle" rowspan="1" colspan="1">Mosek</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">4</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0230</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0266</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">8</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0062</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0079</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">16</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0269</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0052</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">32</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.0996</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.4303</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000074</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.000000</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.6846</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">360.5183</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
