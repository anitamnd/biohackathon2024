<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Brain Topogr</journal-id>
    <journal-id journal-id-type="iso-abbrev">Brain Topogr</journal-id>
    <journal-title-group>
      <journal-title>Brain Topography</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0896-0267</issn>
    <issn pub-type="epub">1573-6792</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10014671</article-id>
    <article-id pub-id-type="pmid">36575327</article-id>
    <article-id pub-id-type="publisher-id">935</article-id>
    <article-id pub-id-type="doi">10.1007/s10548-022-00935-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>fMRIflows: A Consortium of Fully Automatic Univariate and Multivariate fMRI Processing Pipelines</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <name>
          <surname>Notter</surname>
          <given-names>Michael P.</given-names>
        </name>
        <address>
          <email>michaelnotter@hotmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Herholz</surname>
          <given-names>Peer</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Da Costa</surname>
          <given-names>Sandra</given-names>
        </name>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gulban</surname>
          <given-names>Omer F.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Isik</surname>
          <given-names>Ayse Ilkay</given-names>
        </name>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gaglianese</surname>
          <given-names>Anna</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Murray</surname>
          <given-names>Micah M.</given-names>
        </name>
        <address>
          <email>micah.murray@chuv.ch</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff8">8</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.8515.9</institution-id><institution-id institution-id-type="ISNI">0000 0001 0423 4662</institution-id><institution>The Laboratory for Investigative Neurophysiology (The LINE), Department of Radiology, </institution><institution>Lausanne University Hospital and University of Lausanne, </institution></institution-wrap>Lausanne, Switzerland </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.14848.31</institution-id><institution-id institution-id-type="ISNI">0000 0001 2292 3357</institution-id><institution>International Laboratory for Brain, Music and Sound Research, </institution><institution>Université de Montréal &amp; McGill University, </institution></institution-wrap>Montreal, Canada </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.14709.3b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8649</institution-id><institution>McConnell Brain Imaging Centre, Montréal Neurological Institute, </institution><institution>McGill University, </institution></institution-wrap>Montreal, Canada </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.433220.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 0390 8241</institution-id><institution>CIBM Center for Biomedical Imaging, </institution></institution-wrap>Lausanne, Switzerland </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.5012.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 0481 6099</institution-id><institution>Department of Cognitive Neuroscience, </institution><institution>Maastricht University, </institution></institution-wrap>Maastricht, The Netherlands </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.432498.0</institution-id><institution>Brain Innovation B.V., </institution></institution-wrap>Maastricht, The Netherlands </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.461782.e</institution-id><institution-id institution-id-type="ISNI">0000 0004 1795 8610</institution-id><institution>Department of Neuroscience, </institution><institution>Max Planck Institute for Empirical Aesthetics, </institution></institution-wrap>Frankfurt, Germany </aff>
      <aff id="Aff8"><label>8</label>The Sense Innovation and Research Center, Lausanne and Sion, Switzerland </aff>
    </contrib-group>
    <author-notes>
      <fn fn-type="com">
        <p>Handling Editor: Christoph Michel.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2023</year>
    </pub-date>
    <volume>36</volume>
    <issue>2</issue>
    <fpage>172</fpage>
    <lpage>191</lpage>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>12</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">How functional magnetic resonance imaging (fMRI) data are analyzed depends on the researcher and the toolbox used. It is not uncommon that the processing pipeline is rewritten for each new dataset. Consequently, code transparency, quality control and objective analysis pipelines are important for improving reproducibility in neuroimaging studies. Toolboxes, such as Nipype and fMRIPrep, have documented the need for and interest in automated pre-processing analysis pipelines. Recent developments in data-driven models combined with high resolution neuroimaging dataset have strengthened the need not only for a standardized preprocessing workflow, but also for a reliable and comparable statistical pipeline. Here, we introduce fMRIflows: a consortium of fully automatic neuroimaging pipelines for fMRI analysis, which performs standard preprocessing, as well as 1st- and 2nd-level univariate and multivariate analyses. In addition to the standardized pre-processing pipelines, fMRIflows provides flexible temporal and spatial filtering to account for datasets with increasingly high temporal resolution and to help appropriately prepare data for advanced machine learning analyses, improving signal decoding accuracy and reliability. This paper <italic>first</italic> describes fMRIflows’ structure and functionality, <italic>then</italic> explains its infrastructure and access, and <italic>lastly</italic> validates the toolbox by comparing it to other neuroimaging processing pipelines such as fMRIPrep, FSL and SPM. This validation was performed on three datasets with varying temporal sampling and acquisition parameters to prove its flexibility and robustness. fMRIflows is a fully automatic fMRI processing pipeline which uniquely offers univariate and multivariate single-subject and group analyses as well as pre-processing.</p>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1007/s10548-022-00935-8.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Python</kwd>
      <kwd>Neuroimaging</kwd>
      <kwd>Data processing</kwd>
      <kwd>Pipeline</kwd>
      <kwd>Reproducible research</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id>
            <institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution>
          </institution-wrap>
        </funding-source>
        <award-id>169206</award-id>
        <award-id>169206</award-id>
        <principal-award-recipient>
          <name>
            <surname>Notter</surname>
            <given-names>Michael P.</given-names>
          </name>
          <name>
            <surname>Murray</surname>
            <given-names>Micah M.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100010785</institution-id>
            <institution>Canada First Research Excellence Fund</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
            <institution>National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>NIH-NIBIB P41 EB019936</award-id>
        <principal-award-recipient>
          <name>
            <surname>Herholz</surname>
            <given-names>Peer</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id>
            <institution>National Institute of Mental Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R01MH096906</award-id>
        <principal-award-recipient>
          <name>
            <surname>Herholz</surname>
            <given-names>Peer</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id>
            <institution>European Research Council</institution>
          </institution-wrap>
        </funding-source>
        <award-id>DVL-894612</award-id>
        <principal-award-recipient>
          <name>
            <surname>Gaglianese</surname>
            <given-names>Anna</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>University of Lausanne</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open access funding provided by University of Lausanne</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Science+Business Media, LLC, part of Springer Nature 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Functional magnetic resonance imaging (fMRI) is a well-established neuroimaging method used to analyze activation patterns in order to understand brain function. A full fMRI analysis includes preprocessing of the data, followed by statistical analysis and inference on the results, usually separated into 1st-level analysis (the statistical analysis within subjects) and 2nd-level analysis (the group analysis between subjects). The goal of preprocessing is to identify and remove nuisance sources, measure confounds, apply temporal and spatial filters and to spatially realign and normalize images to make them spatially conform (Caballero-Gaudes and Reynolds <xref ref-type="bibr" rid="CR10">2017</xref>). A good preprocessing pipeline should improve the signal-to-noise ratio (SNR) of the data, ensure validity of inference and interpretability of results (Ashburner <xref ref-type="bibr" rid="CR2">2009</xref>), reduce false positive and false negative errors in the statistical analysis and therefore improve the statistical power.</p>
    <p id="Par3">Even though the consequences of inappropriate preprocessing and statistical inference are well documented (Strother <xref ref-type="bibr" rid="CR49">2006</xref>; Power et al. <xref ref-type="bibr" rid="CR42">2017b</xref>), most fMRI analysis pipelines are still established ad-hoc, subjectively customized by researchers to each new dataset (Carp <xref ref-type="bibr" rid="CR11">2012</xref>). This usage can be explained by the circumstance that most researchers, by habit or lack of time, stick with the neuroimaging software at-hand or reuse and modify scripts and code snippets from colleagues and previous projects, and do not always adapt their processing pipelines to the newest standard in neuroimaging processing. Rehashing processing pipelines is associated with problems like persisting bugs in the code and delays in updating individual analysis steps to the most recent standards. This can lead to far reaching consequences. Of course, the constant updating of pipelines to newest standards and softwares also bears the risk of introducing new bugs and might lead to the pitfall of blindly trusting new untested procedures.</p>
    <p id="Par4">One solution to tackle this issue will require code transparency, good quality control and a collective development of well-tested objective analysis pipelines (Gorgolewski et al. <xref ref-type="bibr" rid="CR23">2016</xref>). Recent years have brought some important reformations to the neuroimaging community that go in this direction.</p>
    <p id="Par5"><italic>First</italic>, the introduction of Nipype (Gorgolewski et al. <xref ref-type="bibr" rid="CR21">2011</xref>) made it easier for researchers to switch between different neuroimaging toolboxes, such as AFNI (Cox and Hyde <xref ref-type="bibr" rid="CR12">1997</xref>), ANTs (Avants et al. <xref ref-type="bibr" rid="CR3">2011</xref>), FreeSurfer (Fischl <xref ref-type="bibr" rid="CR17">2012</xref>), FSL (Jenkinson et al. <xref ref-type="bibr" rid="CR29">2012</xref>), and SPM (Friston et al. <xref ref-type="bibr" rid="CR19">2006</xref>). Nipype together with other software packages such as Nibabel (Brett et al. <xref ref-type="bibr" rid="CR8">2018</xref>) and Nilearn (Abraham et al. <xref ref-type="bibr" rid="CR1">2014</xref>) opened up the whole Python ecosystem to the neuroimaging community. Code can be shared between researchers via online services such as GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com">https://github.com</ext-link>), and the whole neuroimaging software ecosystem can be run on any machine or server through the use of container software such as Docker (<ext-link ext-link-type="uri" xlink:href="https://www.docker.com">https://www.docker.com</ext-link>) or Singularity (<ext-link ext-link-type="uri" xlink:href="https://www.sylabs.io">https://www.sylabs.io</ext-link>). Combined with a continuous integration service such as CircleCI (<ext-link ext-link-type="uri" xlink:href="https://circleci.com">https://circleci.com</ext-link>) or TravisCI (<ext-link ext-link-type="uri" xlink:href="https://travis-ci.org">https://travis-ci.org</ext-link>), this allows the creation of easy-to-read, transparent, shareable and continuously tested open-source neuroimaging processing pipelines.</p>
    <p id="Par6"><italic>Second</italic>, the next major advancement in the neuroimaging field was the introduction of a common dataset standard, such as the NIfTI standard (<ext-link ext-link-type="uri" xlink:href="https://nifti.nimh.nih.gov/">https://nifti.nimh.nih.gov/</ext-link>). This was important for the formatting of neuroimaging data. The neuroimaging community gathered together in a consortium to define a standard format for the storage of neuroimaging datasets, the so-called brain imaging data structure (BIDS; Gorgolewski et al. <xref ref-type="bibr" rid="CR23">2016</xref>). A common data structure format facilitates the sharing of datasets and makes it possible to create universal neuroimaging toolboxes that work out-of-the-box on any BIDS conforming dataset. Additionally, through services like OpenNeuro (Gorgolewski et al. <xref ref-type="bibr" rid="CR22">2017</xref>), a free online platform for sharing neuroimaging data, one can test the robustness and flexibility of a new neuroimaging toolbox on hundreds of different datasets.</p>
    <p id="Par7">Software toolboxes like MRIQC (Esteban et al. <xref ref-type="bibr" rid="CR13">2017</xref>) and fMRIPrep (Esteban et al. <xref ref-type="bibr" rid="CR14">2019</xref>) have shown how fruitful this new neuroimaging ecosystem can be and have highlighted the importance and need of good quality control and high-quality preprocessing workflows with consistent results from diverse datasets. Given the recent developments in the field of data-driven analyses to decode brain states from fMRI time-series, there is an increased need for reliable and reproducible statistical analysis of fMRI data, the fundamental input of more advanced machine learning methods, such as multi-voxel pattern analysis (MVPA) and convolutional neuronal networks (CNNs). Here, we build from the fMRIPrep workflow to expand it to a fully automated pipeline for univariate and multivariate individual and group analyses.</p>
    <p id="Par8">fMRIflows provides flexible temporal and spatial filtering, to account for two recent findings in the data-driven model field. <italic>First</italic>, flexible spatial filtering can become of importance when performing multivariate analysis, as it has been shown that the correct spatial band-pass filtering can improve signal decoding accuracy (Sengupta et al. <xref ref-type="bibr" rid="CR43">2018</xref>). <italic>Second</italic>, correct temporal filtering during pre-processing is important and can lead to improved SNR, especially for fMRI datasets with a temporal sampling rate below one second (Viessmann et al. <xref ref-type="bibr" rid="CR50">2018</xref>), but only if the filter is applied orthogonally to the other filters during pre-processing to ensure that previously removed noise is not reintroduced into the data (Hallquist et al. <xref ref-type="bibr" rid="CR26">2006</xref>; Lindquist et al. <xref ref-type="bibr" rid="CR33">2019</xref>). Due to technical improvements in imaging recording through acceleration techniques such as GRAPPA (Griswold et al. <xref ref-type="bibr" rid="CR25">2002</xref>) and simultaneous multi-slice/multiband acquisitions (Feinberg et al. <xref ref-type="bibr" rid="CR15">2010</xref>; Moeller et al. <xref ref-type="bibr" rid="CR35">2010</xref>; Feinberg and Setsompop <xref ref-type="bibr" rid="CR16">2013</xref>), faster sampling rates became possible, to the point that respiratory and cardiac signals can be sufficiently sampled in the BOLD signal. This creates new challenges for the pre-processing of functional images, especially when the external recording of those physiological sources cannot be readily achieved.</p>
    <p id="Par9">fMRIflows presents a consortium of fully automatic neuroimaging pipelines for fMRI analysis, performing standardized pre-processing, as well as 1st- and 2nd-level statistical analyses for univariate and multivariate analysis, with the additional creation of informative quality-control figures. fMRIflows is predicated on the insights and code base of MRIQC (Esteban et al. <xref ref-type="bibr" rid="CR13">2017</xref>) and fMRIPrep (Esteban et al. <xref ref-type="bibr" rid="CR14">2019</xref>), extending their functionality with regard to the following aspects: (a) flexible temporal and spatial filtering of fMRI data, i.e. low- or band-pass filtering allowing for the exclusion of high-frequency oscillations introduced through physiological noise (Viessmann et al. <xref ref-type="bibr" rid="CR50">2018</xref>); (b) accessible and modifiable code base; (c) automatic computation of 1st-level contrasts for univariate and multivariate analysis; and (d) automatic computation of 2nd-level contrasts for univariate and multivariate analysis.</p>
    <p id="Par10">In this paper, we (1) describe the different pipelines included in fMRIflows and illustrate the different processing steps involved, (2) explain the software structure and setup, and (3) validate fMRIflows’ performance by comparing it to other widely used neuroimaging toolboxes, such as fMRIPrep (Esteban et al. <xref ref-type="bibr" rid="CR14">2019</xref>), FSL (Jenkinson et al. <xref ref-type="bibr" rid="CR29">2012</xref>) and SPM (Friston et al. <xref ref-type="bibr" rid="CR19">2006</xref>).</p>
  </sec>
  <sec id="Sec2">
    <title>Materials and Methods</title>
    <sec id="Sec3">
      <title>fMRIflows’ Processing Pipelines</title>
      <p id="Par11">The complete code base of fMRIflows is open access and stored conveniently in six different Jupyter Notebooks on <ext-link ext-link-type="uri" xlink:href="https://github.com/miykael/fmriflows">https://github.com/miykael/fmriflows</ext-link>. The first notebook does not contain any processing pipeline, but rather serves as a user input document that helps to create JSON files, which will contain the execution specific parameters for the five processing pipelines contained in fMRIflows: (1) anatomical preprocessing, (2) functional preprocessing, (3) 1st-level analysis, (4) 2nd-level univariate analysis and (5) 2nd-level multivariate analysis. Each of these five pipelines stores its results in a sub hierarchical folder, specified as an output folder by the user. In the following section, we explain the content of those six Jupyter Notebooks.</p>
      <sec id="Sec4">
        <title>Specification Preparation</title>
        <p id="Par12">Each fMRIflows processing pipeline needs specific input parameters to run. Those parameters range from subject ID and number of functional runs per subject, to requested voxel resolution after image normalization, etc. Each notebook will read the relevant specification parameters from a predefined JSON file that starts with the prefix “fmriflows_spec”. There is one specification file for the anatomical and functional preprocessing, one for the 1st and 2nd level univariate analysis, and one for the 2nd-level multivariate analysis. For an example of these three JSON files, see Supplementary Note 1. The first notebook contained in fMRIflows, called 01_spec_preparation.ipynb, can be used to create those JSON files, based on the provided dataset and some standard default parameters. It does so by using Nibabel v2.3.0 (Brett et al. <xref ref-type="bibr" rid="CR8">2018</xref>), PyBIDS v0.8 (Yarkoni et al. <xref ref-type="bibr" rid="CR51">2019</xref>) and other standard Python libraries. It is up to the user to change any potential processing parameter should they be different from the used default values.</p>
      </sec>
      <sec id="Sec5">
        <title>Anatomical Preprocessing</title>
        <p id="Par13">The anatomical preprocessing pipeline is contained within the notebook 02_preproc_anat.ipynb and uses the JSON file fmriflows_spec_preproc.json for parameter specification such as voxel resolution. If a specific value is not set, fMRIflows normalizes to an isometric voxel resolution of 1 mm<sup>3</sup> by default. However, the user can also choose an anisometric voxel resolution that varies in all three dimensions. Additionally, the user can decide to have a fast or precise normalization. The precise normalization can take up to eight times as long as the fast approach, but can provide a more precise alignment. Visual inspection on performed normalization is always desirable since both normalization algorithms may fail in case of a noisy dataset or undetected artifacts. For an example of the JSON file content, see Supplementary Note 1.</p>
        <p id="Par14">The anatomical preprocessing pipeline only depends on the subject specific T1-weighted (T1w) anatomical images as input files. The individual processing steps are visualized in Fig. <xref rid="Fig1" ref-type="fig">1</xref> and consist of: (1) image reorientation, (2) cropping of field of view (FOV), (3) correction of intensity non-uniformity (INU), (4) image segmentation, (5) brain extraction and (6) image normalization. For a more detailed description of the steps involved in this processing pipeline, see Supplementary Note 2.<fig id="Fig1"><label>Fig. 1</label><caption><p>Depiction of fMRIflows’ anatomical preprocessing pipeline. Arrows indicate dependency between the different processing steps and data flow. Name of each node describes functionality, with the corresponding software dependency mentioned in brackets</p></caption><graphic xlink:href="10548_2022_935_Fig1_HTML" id="MO1"/></fig></p>
      </sec>
      <sec id="Sec6">
        <title>Functional Preprocessing</title>
        <p id="Par15">The functional preprocessing pipeline is contained within the notebook 03_preproc_func.ipynb and uses the JSON file fmriflows_spec_preproc.json for parameter specification. As specification parameters, users can indicate if slice-time correction should be applied or not, and if so which reference timepoint should be used. The user can also indicate to which isometric or anisometric voxel resolution functional images should be sampled to, and if the sampling is into subject or template space. For the template space, the ICBM 2009c nonlinear asymmetric brain template is used (Fonov et al. <xref ref-type="bibr" rid="CR18">2011</xref>). Furthermore, users can specify possible values for low-, high- or band-pass filters in the temporal or spatial domain. Additionally, to investigate nuisance regressors, users can specify the number of CompCor (Behzadi et al. <xref ref-type="bibr" rid="CR4">2007</xref>) or independent component analysis (ICA) components they want to extract and which threshold values they want to use to detect outlier volumes. The implications of those parameters will be explained in more details in the following sections. For an example of the JSON file content, see Supplementary Note 1.</p>
        <p id="Par16">The functional preprocessing pipeline depends as inputs on the output files from the anatomical preprocessing pipeline, as well as the subject-specific functional images and accompanying descriptive JSON file that contains information about the temporal resolution (TR) and slice order of the functional image recording. This JSON file is part of the BIDS standard and therefore should be available in the BIDS conform dataset. The individual processing steps are schematized in Fig. <xref rid="Fig2" ref-type="fig">2</xref> and consist of: (1) image reorientation, (2) non-steady-state detection, (3) creation of functional brain mask, (4) slice time correction, (5) estimation of motion parameters, (6) two-step estimation of coregistration parameters between functional and anatomical image, (7) finalization of motion parameters, (8) single-shot spatial interpolation applying motion correction, coregistration and if specified normalizing images to the template image, (9) construction and application of brain masks, (10) temporal filtering and (11) spatial filtering. It is important to mention that the functional preprocessing is done for each functional run separately to prevent inter-run contaminations. For a more detailed description of the steps involved in this processing pipeline, see Supplementary Note 3.<fig id="Fig2"><label>Fig. 2</label><caption><p>Depiction of fMRIflows’ functional preprocessing pipeline. Arrows indicate dependency between the different processing steps and data flow. Name of each node describes functionality, with the corresponding software dependency mentioned in brackets. Steps that can be grouped into specific sections are contained within a red box to facilitate understanding of the pipeline. Color of arrows indicated if connection stays within a section (red) or not (blue). Nodes depicted as gray boxes indicate that they can be run multiple times with iterating input values, i.e. performing a spatial smoothing with an FWHM value of 4 and 8 mm (Color figure online)</p></caption><graphic xlink:href="10548_2022_935_Fig2_HTML" id="MO2"/></fig></p>
      </sec>
      <sec id="Sec7">
        <title>1st-Level Analysis</title>
        <p id="Par17">The first level analysis pipeline is contained within the notebook 04_analysis_1st-level.ipynb and uses the JSON file fmriflows_spec_analysis.json for parameter specification. As specification parameters, users can indicate which nuisance regressors to include in the GLM, if outliers should be considered, and if the data is already in template space or if this normalization should be done after the estimation of the contrasts. Users can also specify other GLM model parameters, such as the high-pass filter value and the type of basis function that should be used to model the hemodynamic response function (HRF). Additionally, the users will also specify a list of contrasts they want to be estimated, or if they want to create specific contrasts for each stimulus column in the design matrix, and/or for each session separately, which then later might also be used for multivariate analysis. For an example of the JSON file content, see Supplementary Note 1.</p>
        <p id="Par18">The 1st-level analysis pipeline depends on a number of outputs from the previous anatomical and functional preprocessing pipelines, i.e. the TSV (tab separated value) file containing motion parameters and confound regressors, a text file indicating the number of non-steady-state volumes removed from the functional image, and a text file containing a list of indexes identifying outlier volumes. Additionally, the 1st-level analysis pipeline also requires BIDS conform events files containing information on the applied experimental design, including types of conditions and their respective onsets and durations. The individual processing steps included in the 1st-level analysis consist of: (1) collecting preprocessed files and model relevant information, (2) model specification and estimation, (3) univariate contrast estimation, (4) optional preparation for multivariate analysis, (5) optional spatial normalization of contrasts (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). All of the relevant steps, that is model creation, estimation and contrast computation are performed with SPM version 12. For a more detailed description of the steps involved in this processing pipeline, see Supplementary Note 4.<fig id="Fig3"><label>Fig. 3</label><caption><p>Depiction of fMRIflows’ 1st-level analysis pipeline. Arrows indicate dependency between the different processing steps and data flow. Name of each node describes functionality, with the corresponding software dependency mentioned in brackets. Sections that can be grouped into specific sections are contained within a red box to facilitate understanding of the pipeline. Color of arrows indicated if connection stays within a section (red) or not (blue). Nodes depicted in green are optional and can be run if spatial normalization was not yet performed during preprocessing (Color figure online)</p></caption><graphic xlink:href="10548_2022_935_Fig3_HTML" id="MO3"/></fig></p>
      </sec>
      <sec id="Sec8">
        <title>2nd-Level Univariate Analysis</title>
        <p id="Par19">The second level univariate analysis pipeline is contained within the notebook 05_analysis_2nd-level.ipynb and uses the JSON file fmriflows_spec_analysis.json for parameter specification. Users can specify the probability value used as a cutoff for the threshold of the GM probability tissue map in template space that is later used during the model estimation. Additionally, users can specify voxel- and cluster-threshold topological thresholding of the statistical contrast, as well as relevant AtlasReader (Notter et al. <xref ref-type="bibr" rid="CR36">2019</xref>) parameters for the creation of the output tables and figures.</p>
        <p id="Par20">The 2nd-level univariate analysis pipeline depends only on the estimated contrasts from the 1st-level univariate analysis. No further contrast specification is required as fMRIflows currently only implements a simple one-sample t-test. The individual processing steps included in the 2nd-level univariate analysis consist of: (1) gathering of the 1st-level contrasts, (2) creation and estimation of 2nd-level model, (3) estimation of contrast estimation, (4) topological thresholding of contrasts, (5) results creation with AtlasReader. As for the 1st-level analysis, all of the relevant model creation, estimation and contrast computation are performed with SPM12. All results were corrected for false positive rate (FPR). For a more detailed description of the steps involved in this processing pipeline, see Supplementary Note 5.</p>
      </sec>
      <sec id="Sec9">
        <title>2nd-Level Multivariate Analysis</title>
        <p id="Par21">The second level multivariate analysis pipeline is contained within the notebook 06_analysis_multivariate.ipynb and uses the JSON file fmriflows_spec_multivariate.json for parameter specification. Users can define a list of classifiers to use for the multivariate analysis, the sphere radius and step size of the searchlight approach. To perform a 2nd-level analysis of searchlight results users can decide between a classical GLM approach testing against chance level and a more recommended permutation based method as described in Stelzer et al. (<xref ref-type="bibr" rid="CR47">2013</xref>) with the option of determining the number of permutations. Additionally, users can specify voxel- and cluster-threshold topological thresholding of the statistical contrast, as well as relevant AtlasReader parameters for the creation of the output tables and figures.</p>
        <p id="Par22">The 2nd-level multivariate analysis pipeline depends on the estimated contrasts from the 1st-level multivariate analysis, the associated CSV file containing a list of the corresponding contrast labels and a list of binary classification identifiers. In contrast to the other notebooks, this notebook uses Python 2.7 to accommodate the requirements of PyMVPA v2.6.5 (Hanke et al. <xref ref-type="bibr" rid="CR27">2009</xref>). The individual processing steps included in the 2nd-level multivariate analysis consist of: (1) data preparation for the analysis with PyMVPA, (2) searchlight classification, (3) computation of group analysis using a t-test, (4) computation of group analysis according to Stelzer et al. (<xref ref-type="bibr" rid="CR47">2013</xref>), and (5) results creation with AtlasReader. All results were corrected for FPR. For a more detailed description of the steps involved in this processing pipeline, see Supplementary Note 6.</p>
      </sec>
    </sec>
    <sec id="Sec10">
      <title>Infrastructure and Access to fMRIflows</title>
      <p id="Par23">The source code of fMRIflows is available at GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/miykael/fmriflows">https://github.com/miykael/fmriflows</ext-link>) and is licensed under the BSD 3-Clause “New” or “Revised” License. The code is written in Python v3.7.2 (<ext-link ext-link-type="uri" xlink:href="https://www.python.org">https://www.python.org</ext-link>), stored in Jupyter Notebooks v4.4.0 (Kluyver et al. <xref ref-type="bibr" rid="CR31">2016</xref>) and distributed via Docker v18.09.2 (<ext-link ext-link-type="uri" xlink:href="https://docker.com">https://docker.com</ext-link>) containers that are publicly available via Docker Hub (<ext-link ext-link-type="uri" xlink:href="https://hub.docker.com">https://hub.docker.com</ext-link>). The usage of Docker allows the user to run fMRIflows on any major operating system, with the following command:</p>
      <p id="Par24">docker run-it-p 9999:8888-v /home/user/ds001:/data miykael/fmriflows.</p>
      <p id="Par25">The first flag -it indicates that the Docker container should be run in interactive mode, while the second flag -p 9999:8888 defines the port (here 9999) that we want to use to access the Jupyter Notebooks via the web-browser. The third flag, -v /home/user/ds001:/data tells fMRIflows the location of the BIDS conform dataset that should be mounted in the Docker container, here located at /home/user/ds001. Once the docker container is launched, the interactive Jupyter Notebooks can be accessed through the web-browser.</p>
      <p id="Par26">fMRIflows uses many different software packages for the individual processing steps. The neuroimaging software that are used are: Nipype v1.1.9 (Gorgolewski et al. <xref ref-type="bibr" rid="CR21">2011</xref>), FSL v5.0.9 (Smith et al. <xref ref-type="bibr" rid="CR46">2004</xref>), ANTs v2.2.0 (Avants et al. <xref ref-type="bibr" rid="CR3">2011</xref>), SPM12 v7219 (Penny et al. <xref ref-type="bibr" rid="CR38">2011</xref>), AFNI v18.0.5 (Cox and Hyde <xref ref-type="bibr" rid="CR12">1997</xref>), Nilearn v0.5 (Abraham et al. <xref ref-type="bibr" rid="CR1">2014</xref>), Nibabel v2.3.0 (Brett et al. <xref ref-type="bibr" rid="CR8">2018</xref>), PyMVPA v2.6.5 (Hanke et al. <xref ref-type="bibr" rid="CR27">2009</xref>), Convert3D v1.1 (<ext-link ext-link-type="uri" xlink:href="https://sourceforge.net/p/c3d">https://sourceforge.net/p/c3d</ext-link>), AtlasReader v0.1 (Notter et al. <xref ref-type="bibr" rid="CR36">2019</xref>) and PyBIDS v0.8 (Yarkoni et al. <xref ref-type="bibr" rid="CR51">2019</xref>). In addition to some standard Python libraries, fMRIflows also uses Numpy (Oliphant <xref ref-type="bibr" rid="CR37">2007</xref>), Scipy (Jones et al. <xref ref-type="bibr" rid="CR30">2001</xref>), Matplotlib (Hunter <xref ref-type="bibr" rid="CR28">2007</xref>), Pandas (McKinney et al. <xref ref-type="bibr" rid="CR34">2010</xref>) and Seaborn (<ext-link ext-link-type="uri" xlink:href="http://seaborn.pydata.org">http://seaborn.pydata.org</ext-link>).</p>
      <p id="Par27">With every new pull request pushed to the GitHub Repository of fMRIflows, a test instance on CircleCI (<ext-link ext-link-type="uri" xlink:href="https://circleci.com">https://circleci.com</ext-link>) is deployed to test the complete code base for execution errors. This framework allows the continuous integration of new code to fMRIflows, and guarantees the general functionality of the software package. Outputs are not controlled for their correctness.</p>
    </sec>
    <sec id="Sec11">
      <title>Validation of fMRIflows</title>
      <p id="Par28">fMRIflows was validated in two phases. In <italic>Phase 1</italic>, we validated the proficiency of the toolbox by applying it on different kinds of fMRI datasets conforming to the BIDS standard (Gorgolewski et al. <xref ref-type="bibr" rid="CR23">2016</xref>) available via OpenNeuro.org (Gorgolewski et al. <xref ref-type="bibr" rid="CR22">2017</xref>). Insights during this phase allowed us to improve the code base and make fMRIflows robust to a diverse set of datasets. In <italic>Phase 2</italic>, we compared the performance of the toolbox to similar neuroimaging preprocessing pipelines such as fMRIPrep, FSL, and SPM. To better understand where fMRIflows overlaps or diverges from comparable processing pipelines, we investigated the preprocessing, subject-level and group-level outcomes for all four toolboxes, run on three different datasets.</p>
      <sec id="Sec12">
        <title>Phase 1: Proficiency Validation</title>
        <p id="Par29">To investigate the capabilities and flaws of the initial implementation of the toolbox, fMRIflows was run on different datasets, either available publicly via OpenNeuro.org or available privately to the authors. Such an approach allowed the exploration of datasets with different temporal and spatial resolutions, SNRs, FOVs, numbers of slices, scanner characteristics, and other sequence parameters, such as acceleration factors and flip angles.</p>
      </sec>
      <sec id="Sec13">
        <title>Phase 2: Performance Validation</title>
        <p id="Par30">To validate the performance of fMRIflows, we used three different task-based fMRI datasets and compared its preprocessing to the three neuroimaging processing pipelines fMRIPrep, FSL and SPM. Comparison was done on preprocessing, subject-level and group-level outputs. Because of differences in how FSL and SPM perform subject- and group-level analyses and due to the lack of such routines in fMRIPrep, all subject- and group-level analyses for the performance validation were performed using identical Nistats (Abraham et al. <xref ref-type="bibr" rid="CR1">2014</xref>) routines.</p>
        <p id="Par31">The three datasets (see Table <xref rid="Tab1" ref-type="table">1</xref>) were all acquired on scanners with a magnetic field strength of 3 T and differ in many sequence parameters, most notably in the temporal resolution with which they were recorded. This is especially important as we aim to highlight that the right handling of temporal filtering is crucial for datasets with a temporal resolution below 1000 ms.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Overview of the datasets used to validate fMRIflows</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">TR2000</th><th align="left">TR1000</th><th align="left">TR600</th></tr></thead><tbody><tr><td align="left">Temporal resolution (ms)</td><td align="left">2000</td><td align="left">1000</td><td align="left">600</td></tr><tr><td align="left">Spatial resolution</td><td align="left">3.5 × 3.5 × 3.3</td><td align="left">2.0 × 2.0 × 2.4</td><td align="left">3.0 × 3.0 × 3.0</td></tr><tr><td align="left">Number of slices</td><td align="left">36</td><td align="left">64</td><td align="left">24</td></tr><tr><td align="left">Slice Order</td><td align="left">Descending</td><td align="left">Unknown</td><td align="left">Interleaved</td></tr><tr><td align="left">Coverage</td><td align="left">Whole brain</td><td align="left">Whole brain</td><td align="left">Slab</td></tr><tr><td align="left">Volumes per run</td><td align="left">275</td><td align="left">453</td><td align="left">600</td></tr><tr><td align="left">Number of runs</td><td align="left">4</td><td align="left">4</td><td align="left">6</td></tr><tr><td align="left">Acceleration factor</td><td align="left">None</td><td align="left">4</td><td align="left">3</td></tr><tr><td align="left">Magnetic strength (T)</td><td align="left">3</td><td align="left">3</td><td align="left">3</td></tr><tr><td align="left">Number of subjects</td><td align="left">12</td><td align="left">20</td><td align="left">17</td></tr><tr><td align="left">Sequence type</td><td align="left">2D-EPI</td><td align="left">Multi-band</td><td align="left">SMS</td></tr><tr><td align="left">Task</td><td align="left">Audio–visual memory task</td><td align="left">Mixed gamble task</td><td align="left">Audio–visual observation task</td></tr><tr><td align="left">Data availability</td><td align="left">OpenNeuro.org (ds001345, v 1.0.1)</td><td align="left">OpenNeuro.org (ds001734, v.1.0.4)</td><td align="left">OpenNeuro.org (will be made available after publication of experimental work)</td></tr></tbody></table></table-wrap></p>
        <p id="Par32"><bold>Dataset TR2000</bold> has a comparably low temporal sampling and spatial resolution. It serves as a standard dataset, recorded with a standard EPI scan sequence. The dataset and paradigm are described in more details in Notter et al. (under review). In short, participants performed a continuous recognition task and indicated for each image whether it is old or new. When the image was presented for the first time (new) it was either presented with no sound (unisensory visual context) or together with a sound (multisensory context).</p>
        <p id="Par33"><bold>Dataset TR1000</bold> has a rather high temporal sampling and spatial resolution and serves as an advanced dataset, recorded with a scan sequence using a multiband acceleration technique. The dataset and paradigm are described in more detail in Botvinik-Nezer et al. (<xref ref-type="bibr" rid="CR6">2019</xref>). In short, participants performed a mixed gambling task in which they were asked to either accept or reject a possible monetary gain or loss.</p>
        <p id="Par34"><bold>Dataset TR600</bold> has a very high temporal sampling with a moderate spatial resolution and serves as an extreme dataset, recorded with scan sequences using a simultaneous multi-slice (SMS) acceleration technique (Feinberg et al. <xref ref-type="bibr" rid="CR15">2010</xref>). This dataset was collected for another project. In short, participants were shown auditory, visual or audiovisual stimuli containing either an animal (as an image or sound), pure noise or both together. Participants performed a discrimination task in which they had to indicate if they perceived a stimuli with an animal in it or not, independent of the stimuli modality. The stimuli were either presented in a unisensory or multisensory context.</p>
        <p id="Par35">All participants of the Datasets TR2000 and TR600 have been included in the performance validation, while only the first 20 out of the 120 total participants of the Dataset TR1000 was used in order to reduce computation time and make this dataset comparable to the other two. Datasets TR2000 and TR1000 are already publicly available through the OpenNeuro platform. Dataset TR600 is in preparation to be published on OpenNeuro as well. Until then, this dataset is available upon request.</p>
        <p id="Par36">The <bold>pre-processing pipelines</bold> with fMRIflows, fMRIPrep, FSL and SPM were based on the default parameters and only differed in the following points from their standard implementations: (1) Functional images were resampled to an isometric voxel resolution according to the dominant resolution dimension within a dataset; (2) Spatial smoothing of the functional images is applied after preprocessing of the images, using a Nilearn routine and a smoothing kernel with a full width at half maximum (FWHM) of 6 mm, in order to keep the approaches comparable, as spatial smoothing is not included in the fMRIPrep workflow; (3) Anatomical images in the FSL pipeline were first cropped to a standard FOV, followed by brain extraction using FSL’s BET before FSL’s FEAT was launched; (4) In the case of FSL, the normalization from structural to standard space was done using a non-linear warping approach with 12 degrees of freedom and a spline interpolation model; (5) In the case of SPM, the template brain for the normalization was its standard tissue probability brain TPM, while for fMRIflows, fMRIPrep and FSL, the ICBM 2009c nonlinear asymmetric brain template was used.</p>
        <p id="Par37">The statistical inference was not performed on any of the investigated toolboxes to prevent the introduction of a software specific bias. The 1st- and 2nd-level analysis was performed using Nistats, Nilearn and other Python toolboxes and only differed between the toolboxes in the following ways: (1) the estimated motion parameters added to the design matrix during the 1st-level analysis differed for each toolbox as they were based on the software-specific preprocessing routine; (2) the number of volumes per run used during the 1st-level analysis of fMRIflows might differ slightly from the other approaches, as the fMRIflows routine removes non-steady state volumes during the preprocessing; (3) SPM used its own tissue probability map to create a binary mask restricted to gray matter voxels during the group analysis, while the other three toolboxes used the ICBM 2009c gray matter probability map instead.</p>
        <p id="Par38">To compare the unthresholded group statistic maps between the toolboxes, we created for each pairwise combination of preprocessing approach a Bland–Altman 2D histogram plot, as described by (Bowring et al. <xref ref-type="bibr" rid="CR7">2018</xref>). These plots show the difference between the statistic value (y-axis), against the mean statistic value (x-axis) for all voxels within the intersection of the respective brain mask. In other words, it summarized in a 2D histogram plot, for each voxel how much higher the statistical value in toolbox B is (y-axis), in comparison to toolbox A’s statistical value (x-axis).</p>
        <p id="Par39">The complete lists of parameters, the scripts to perform preprocessing, 1st- and 2nd-level analysis and the scripts to create individual figures can be found on fMRIflows GitHub page (<ext-link ext-link-type="uri" xlink:href="https://github.com/miykael/fmriflows/tree/master/paper">https://github.com/miykael/fmriflows/tree/master/paper</ext-link>). Derivatives generated for the validation in phase 2 can be inspected and downloaded on NeuroVault (Gorgolewski et al. <xref ref-type="bibr" rid="CR24">2015</xref>) under the following links: (1) Standard deviation maps of temporal averages after preprocessing (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/neurovault.collection:5645">https://identifiers.org/neurovault.collection:5645</ext-link>), (2) temporal SNR maps after preprocessing (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/neurovault.collection:5713">https://identifiers.org/neurovault.collection:5713</ext-link>), (3) binarized 1st-level activation count maps (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/neurovault.collection:5647">https://identifiers.org/neurovault.collection:5647</ext-link>), (4) 2nd-level activation maps (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/neurovault.collection:5646">https://identifiers.org/neurovault.collection:5646</ext-link>).</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Results</title>
    <sec id="Sec15">
      <title>Summary of Outputs Obtained by fMRIflows’ Processing Pipelines</title>
      <sec id="Sec16">
        <title>Output Generated After Executing the Anatomical Preprocessing Pipeline</title>
        <p id="Par40">After the execution of the anatomical preprocessing pipeline, the following files are generated for each subject: (1) image of the inhomogeneity-corrected full head image, (2) image of the extracted brain, (3) binary mask used for the brain extraction, (4) individual tissue probability maps for gray matter (GM), white matter (WM), cerebrospinal fluid (CSF), skull and head, (5) normalized anatomical image in template space, (6) reverse-normalized template image in subject space, (7) plus the corresponding transformation matrices used for output 5 and 6. Each anatomical preprocessing output folder also contains (8) the ICBM 2009c brain template used for the normalization, sampled to the requested voxel resolution.</p>
        <p id="Par41">In addition to these files, the following three informative figures are generated: (1) tissue segmentation, (2) brain extraction and (3) spatial normalization of the anatomical image. A shortened version of those three figures, as well as their explanation are shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>.<fig id="Fig4"><label>Fig. 4</label><caption><p>Summary of output figures generated by fMRIflows after executing the anatomical preprocessing pipeline. (Top) coronal view of the image segmentation output, showing gray matter tissue in green, white matter tissue in beige, cerebrospinal fluid in blue. (Middle) sagittal view of the brain extraction output, showing the extracted brain image in red, and the original anatomical image in gray. (Bottom) axial view of the spatial normalization output, showing the normalized brain image highlighted in yellow, overlaid over the ICBM 2009c brain template in gray. Regions in red and blue show negative and positive deformation discrepancy between the normalized subject image and the template (Color figure online)</p></caption><graphic xlink:href="10548_2022_935_Fig4_HTML" id="MO4"/></fig></p>
      </sec>
      <sec id="Sec17">
        <title>Output Generated After Executing the Functional Preprocessing Pipeline</title>
        <p id="Par42">After the execution of the functional preprocessing pipeline, the following files are generated separately for each subject, each functional run and each temporal filtering: (1) text file indicating which volumes were detected as outliers, (2) tabular separated (TSV) file containing all extracted confound regressors, (3) text file containing the six motion parameter regressors according to FSL’s output scheme, (4) binary masks for the brain, (5) masks for anatomical and functional component based noise correction, (6) functional mean image, and (7) completely preprocessed functional images, separated by spatial smoothing approaches. Each subject folder also contains (8) one text file per functional run indicating the number of non-steady-state volumes at the beginning of run.</p>
        <p id="Par43">The following is a more detailed description of the multiple confounds fMRIflows estimates during functional preprocessing:</p>
        <p id="Par44"><bold>Confounds based on motion parameters:</bold> in addition to the head motion parameters created during preprocessing, fMRIflows also computes (1) 24-parameter Volterra expansion of the motion parameters (Friston et al. <xref ref-type="bibr" rid="CR20">1996</xref>) using custom scripts and (2) Framewise Displacement (FD) component (Power et al. <xref ref-type="bibr" rid="CR40">2012</xref>) using Nipype.</p>
        <p id="Par45"><bold>Confounds based on global signal:</bold> functional images before spatial smoothing were used to compute confound regressors, such as (1) DVARS, which represents the spatial standard deviation of the signal after temporal differencing, to identify motion-affected frames (Power et al. <xref ref-type="bibr" rid="CR40">2012</xref>), using Nipype and (2) four global signal curves representing the average signal in the total brain volume (TV), GM, WM and CSF, using Nilearn.</p>
        <p id="Par46"><bold>Detection of outlier volumes:</bold> the user can specify which of the six signal curves for FD, DVARS and average signal in TV, GM, WM and CSF to use to identify outlier volumes (see Fig. <xref rid="Fig5" ref-type="fig">5</xref>A). Those are volumes that have larger fluctuations in the signal values in a given volume, compared to the z-scored standard deviation throughout the time course. The exact threshold for each curve can be adapted by the user, but its default value is set to a z-value of 3.27, representing 99%, for the FD, DVARS and TV signal. The identification number of each outlier volume is stored in a text file that might be used in the 1st-level pipeline during the GLM model estimation to remove the effect of those volumes from the overall analysis, also known as censoring (Caballero-Gaudes and Reynolds <xref ref-type="bibr" rid="CR9">2016</xref>).<fig id="Fig5"><label>Fig. 5</label><caption><p>Example of general output figures generated by fMRIflows after executing the functional preprocessing pipeline. The dataset used to generate these figures was recorded with a TR of 600 ms and had a total of 600 volumes per run. Preprocessing included a low-pass filter at 0.2 Hz. Distribution plots on the right side of the figures in part A and B represent value frequency in y-direction. <bold>A</bold> Depiction of the nuisance confounds FD, DVARS and TV. Detected outlier volumes are highlighted with vertical black bars. <bold>B</bold> Estimation of translation head motion after application of low-pass filtering at 0.2 Hz in color, and before temporal filtering in light gray. <bold>C</bold> Depiction of brain masks used to compute DVARS (red), and temporal (green) and anatomical (blue) CompCor confounds, overlaid on the mean functional image (grey) (Color figure online)</p></caption><graphic xlink:href="10548_2022_935_Fig5_HTML" id="MO5"/></fig></p>
        <p id="Par47"><bold>Confounds based on signal components:</bold> using the temporal filtered functional images, two different kinds of approaches are performed to extract components that could be used for denoising or dimensionality reduction of the data. The first approach is called CompCor (Behzadi et al. <xref ref-type="bibr" rid="CR4">2007</xref>) and uses principal component analysis (PCA) to estimate the main sources of noise within specific confound regions. Regions are either defined by their temporal or anatomical characteristics. The temporal CompCor approach (tCompCor) considers the 2% most variable voxels within the confound brain mask as sources of confounds. The anatomical CompCor approach (aCompCor), considers voxels within twice eroded WM and CSF brain masks as sources of confounds. The user can specify how many aCompCor and tCompCor components should be computed, but the default value is set to five each. The second approach uses independent component analysis (ICA) to perform source separation in the signal (Fig. <xref rid="Fig6" ref-type="fig">6</xref>). Using Nilearn’s CanICA routine, fMRIflows computes by default the top ten independent components throughout the confound masks. The number of confounds to extract can be adjusted by the user. It is the user's responsibility to evaluate appropriately whether residual artifacts are present and need to be removed.<fig id="Fig6"><label>Fig. 6</label><caption><p>Example of ICA output figures generated by fMRIflows after executing the functional preprocessing pipeline. The dataset used to generate these figures was recorded with a TR of 600 ms and had a total of 600 volumes per run. <bold>A</bold> Correlation between the first three ICA components and the functional image over time (left) and the corresponding power density spectrum with frequency on the x-axis (right). First component most likely depicts respiration at 0.6 Hz, while third component is most likely visual activation induced by the visual stimulation task during data acquisition. <bold>B</bold> Correlation strength between a given ICA component and the location in the brain volume for the first three ICA components</p></caption><graphic xlink:href="10548_2022_935_Fig6_HTML" id="MO6"/></fig></p>
        <p id="Par48"><bold>Storage of confound information:</bold> all of the confound curves computed after functional preprocessing are stored in a TSV file to allow for easy access.</p>
        <p id="Par49"><bold>Diverse set of overview figures:</bold> to allow for visual inspection of the numerous outputs generated after the execution of the functional preprocessing pipeline, fMRIflows creates many informative overview figures. These overviews cover the motion parameters used for head motion correction, the anatomical and temporal CompCor components, FD, DVARS, average signal in TV, GM, WM and CSF, and the ICA components. fMRIflows also creates a brain overview figure showing the extent of the different masks applied during functional preprocessing, a spatial correlation map between the ICA components and the individual voxel signal, and a carpet plot according to Power (<xref ref-type="bibr" rid="CR39">2017</xref>) and Esteban et al. (<xref ref-type="bibr" rid="CR14">2019</xref>). To better visualize underlying structures in the carpet plot the time series traces are sorted by their correlation coefficients to the average signal within a given region, allowing for a positive or negative time lag of 2 volumes. A shortened version of all these figures, as well as their explanations are shown in Figs. <xref rid="Fig5" ref-type="fig">5</xref>, <xref rid="Fig6" ref-type="fig">6</xref> and <xref rid="Fig7" ref-type="fig">7</xref>.<fig id="Fig7"><label>Fig. 7</label><caption><p>Example of a carpet plot figure generated by fMRIflows after executing the functional preprocessing pipeline. The dataset used to generate this figure was recorded with a TR of 600 ms and had a total of 600 volumes per run. This panel shows the signal after preprocessing for every other voxel (y-axis), over time in volumes (x-axis). The panel shows voxels in the gray matter (top part), white matter (between blue and red line) and CSF (bottom section). The data was standardized to the average signal, and ordered within a given region according to the correlation coefficient between a voxel and to the average signal of this region</p></caption><graphic xlink:href="10548_2022_935_Fig7_HTML" id="MO7"/></fig></p>
      </sec>
      <sec id="Sec18">
        <title>Output Generated After Executing the 1st-Level Analysis Pipeline</title>
        <p id="Par50">After the execution of the 1st-level analysis, the following files are generated for the univariate analysis: (1) contrasts and statistical map of the specified contrasts, (2) SPM.mat file containing the information relevant for the model, (3) visualization of the design matrix used in the 1st-level model depicting the regressor for the stimuli, motion and confounds, and (4) glass brain plot for each estimated contrast thresholded at the top 2% of positive and negative values created with AtlasReader (Notter et al. <xref ref-type="bibr" rid="CR36">2019</xref>) to provide a general overview of the quality of contrasts. The multivariate analysis part of this notebook creates: (1) one contrast image per condition and session which later can be used as samples for the multivariate analysis, and (2) a label file identifying the condition of each contrast.</p>
      </sec>
      <sec id="Sec19">
        <title>Output Generated After Executing the 2nd-Level Analysis Pipeline</title>
        <p id="Par51">After the execution of the 2nd-level univariate analysis, the following files are generated, individually for each contrast and spatial and temporal filter that was applied: (1) contrasts and statistical map of one-sample <italic>t</italic>-test contrast, (2) SPM.mat file containing the information relevant for the model, (3) thresholded statistical maps with corresponding AtlasReader outputs (i.e. glass brain plot to provide a result overview, cross section plot showing each significant cluster individually, informative tables concerning the peak and cluster extent of each cluster).</p>
        <p id="Par52">After the execution of the 2nd-level multivariate analysis, the following files are generated, for each specified comparison individually: (1) subject-specific permutation files needed for correction according to Stelzer et al. (<xref ref-type="bibr" rid="CR47">2013</xref>), (2) group-average prediction accuracy maps as well as corresponding feature-wise maps representing chance level acquired via bootstrapping approach (Stelzer et al. <xref ref-type="bibr" rid="CR47">2013</xref>), (3) group-average prediction accuracy maps after correction for multiple comparisons and (4) thresholded statistical result maps with corresponding AtlasReader outputs (i.e. glass brain plot to provide a result overview, cross section plot showing each significant cluster individually, informative tables concerning the peak and cluster extent of each cluster).</p>
      </sec>
    </sec>
    <sec id="Sec20">
      <title>Results of Phase 1: Proficiency Validation</title>
      <p id="Par53">Due to differences in scanner hardware, scan protocols, research requirements and expertise of the person who records the images, fMRI datasets can come in many different shapes and forms. We ran fMRIflows on several datasets to make sure that it is capable of dealing with differences inherent to each of them. In this section, we summarize the main issues we encountered during this process and describe how we tackled each of them.</p>
      <sec id="Sec21">
        <title>Image Orientation</title>
        <p id="Par54">fMRIflows reorients all anatomical and functional images at the beginning of the preprocessing pipeline to the neurological convention RAS (right, anterior, superior) to prevent failures of coregistration between anatomical and functional images due to orientation mismatches within subjects.</p>
      </sec>
      <sec id="Sec22">
        <title>Image Extent</title>
        <p id="Par55">Some datasets have unusually large image coverage along the inferior–superior axis, which means that their anatomical images also often contain part of the participant’s neck. This can lead to unwanted outcomes in certain neuroimaging routines, as they were not tested for such additional tissue coverage. This is most pronounced in the case of FSL’s BET routine, which has difficulty finding the center and extent of the brain, or SPM’s segmentation routine that depends on the distribution of the voxel intensities within the whole volume. To prevent these and other unforeseen behaviors, fMRIflows uses FSL’s robustfov routine to restrict all anatomical images to the same spatial extent.</p>
      </sec>
      <sec id="Sec23">
        <title>Image Inhomogeneity</title>
        <p id="Par56">Depending on the scan sequence protocol or the scanner hardware itself, some datasets can contain strong image intensity inhomogeneities, caused by an inhomogeneous bias field during data acquisition. This can have a negative effect on many different neuroimaging routines, most pronounced in brain extraction and image segmentation. To tackle this issue, fMRIflows uses ANTs’ N4BiasFieldCorrection routine, which allows the analysis of datasets with even low image quality and strong image inhomogeneity. In the anatomical preprocessing pipeline, inhomogeneity correction is applied to improve the final output image. In the functional preprocessing pipeline, inhomogeneity correction is only applied to improve the estimation and extraction of different tissue types, but does not directly change the values in the final output image.</p>
      </sec>
      <sec id="Sec24">
        <title>Brain Extraction</title>
        <p id="Par57">Different brain extraction routines were explored to ensure: (1) that the extraction is sufficiently robust to handle different kinds of datasets, (2) that it is neither too conservative nor liberal with the removal of non-brain tissues, and (3) that it has an overall reasonably fast computation time. The best and most consistent results were achieved using SPM’s image segmentation routine, followed by a specific thresholding and merging of the GM, WM and CSF probability maps. FSL’s BET routine was not robust enough to lead to stable results on all tested datasets. While ANTs’ Atropos routine led to comparably good results, we went with SPM because of the much faster computation time.</p>
      </sec>
      <sec id="Sec25">
        <title>Image Interpolation</title>
        <p id="Par58">For the single-shot spatial interpolation during normalization, we used ANTs and explored NearestNeighbor, BSpline and LanczosWindowedSinc (Lanczos <xref ref-type="bibr" rid="CR32">1964</xref>) interpolation. NearestNeighbor interpolation led to unnatural looking voxel-to-voxel value transitions. BSpline led in general to good results, but had issues especially with datasets that did not have full brain coverage and introduced some rippling low value fluctuations at the borders of non-zero voxels. LanczosWindowedSinc interpolation led to the best outcome by minimizing the smoothing effects and preventing the introduction of additional confounds reaffirming the observations from fMRIPrep (Esteban et al. <xref ref-type="bibr" rid="CR14">2019</xref>).</p>
      </sec>
    </sec>
    <sec id="Sec26">
      <title>Results of Phase 2: Performance Validation</title>
      <p id="Par59">The performance validation of fMRIflows was conducted on three different task-based fMRI datasets, as described in Table <xref rid="Tab1" ref-type="table">1</xref>. The preprocessing of fMRIflows was compared to other neuroimaging processing pipelines such as fMRIPrep, FSL and SPM. We tested fMRIflows’ preprocessing pipeline with and without a temporal low-pass filter of 0.2 Hz to better understand performance differences between toolboxes and to stress the importance of adequate temporal filtering when processing fMRI datasets with high temporal resolution.</p>
      <sec id="Sec27">
        <title>Estimated Spatial Smoothness After Functional Preprocessing</title>
        <p id="Par60">Each preprocessing step that resamples a functional image, such as slice time correction, motion correction, spatial or temporal interpolation has the potential to increase the spatial smoothness in the data. The less smoothness is introduced during preprocessing, the closer the data are to their initial version. We used AFNI’s 3dFWHMx to estimate the average spatial smoothness (FWHM) of each functional image after preprocessing to compare the amount of data manipulation that was applied to the raw data (see Fig. <xref rid="Fig8" ref-type="fig">8</xref>). As this FWHM value depends on the voxel resolution of a given dataset, we normalized it by the volume of the voxel to achieve a common FWHM value per 1 mm<sup>3</sup>.<fig id="Fig8"><label>Fig. 8</label><caption><p>Investigation of estimated spatial smoothness after functional preprocessing of three different datasets, processed with varying approaches. The five different preprocessing approaches fMRIflows with (blue) and without (orange) a low-pass filter at 0.2 Hz, fMRIPrep (green), FSL (red) and SPM (violet) are plotted separately for the dataset TR2000 (left), TR1000 (middle) and TR600 (right). The violin plots indicate the overall distribution of the normalized smoothness estimates of each functional image (depicted in individual dots: TR2000 = 48 dots, TR1000 = 80 dots, TR600 = 102 dots). The red horizontal line represents the median value, while the horizontal black lines indicate the 25 and 75 percentiles of the value distribution respectively. Two-sided t-test were computed for each pair of approaches used and each dataset. Significant differences between groups are indicated with *<italic>p</italic> &lt; 0.05 and ***<italic>p</italic> &lt; 0.001. All results are corrected with the Tukey multiple comparison test (Color figure online)</p></caption><graphic xlink:href="10548_2022_935_Fig8_HTML" id="MO8"/></fig></p>
        <p id="Par61">Overall, the estimated spatial smoothness after preprocessing with fMRIflows (without low-pass filter) is comparable to the one with fMRIPrep, while SPM’s is in general significantly lower and FSL’s is slightly higher. All results in Fig. <xref rid="Fig8" ref-type="fig">8</xref> are corrected with the Tukey multiple comparison test. The differences with respect to SPM are probably due to the fewer numbers of resampling steps involved in SPM’s preprocessing pipeline. The differences with respect to FSL are probably due to the interpolation method used during image resampling. While the FSL preprocessing pipeline uses the spline interpolation, fMRIflows and fMRIPrep use the LanczosWindowedSinc interpolation, which is known to minimize the smoothing during interpolation. The application of a temporal low-pass filter at 0.2 Hz during fMRIflows’ preprocessing leads to a significantly higher spatial smoothness for the TR600 dataset when compared with the other approaches. This effect might also be present for the TR1000 dataset. However, there the difference between the fMRIflows preprocessing with and without low-pass filtering is not significant. This increased spatial smoothness for the approach that uses a low-pass filter makes sense, as the goal of the temporal low-pass filter itself is to smooth the time series values. This temporal smoothing forcibly also increases the spatial smoothness at each individual time point.</p>
      </sec>
      <sec id="Sec28">
        <title>Performance Check of Spatial Normalization</title>
        <p id="Par62">We computed the standard deviation map for each population, based on the temporal average map of each preprocessed functional image, to compare the performance of spatial normalization of the different preprocessing methods on the three different datasets (see Fig. <xref rid="Fig9" ref-type="fig">9</xref>).<fig id="Fig9"><label>Fig. 9</label><caption><p>Depiction of standard deviation maps of the temporal averages of three different datasets, after multiple functional preprocessing approaches. Preprocessing was done with fMRIflows (with a temporal low-pass filter at 0.2 Hz; without low-pass filter looks identical), fMRIPrep, FSL and SPM (from top to bottom) separated for the TR2000 (left), TR1000 (middle) and TR600 (right) dataset. Color value represents the standard deviation value over all subjects. Color scale is the same within a dataset and was set manually to highlight the border effects in gray matter regions. Regions with high inter-subject variability are shown in yellow, while regions with low inter-subject variability are shown in blue. Outline of the brain and subcortical white matter regions is delineated in red and is based on the ICBM 2009c brain template, except for the analysis with SPM where it is based on SPM’s tissue probability map template (Color figure online)</p></caption><graphic xlink:href="10548_2022_935_Fig9_HTML" id="MO9"/></fig></p>
        <p id="Par63">The averaged standard deviation maps after fMRIflows’ and fMRIPrep’s preprocessing are very similar, which is not surprising as fMRIflows uses the same ANTs normalization routine with very similar parameters. The main difference lies in the fact that fMRIflows applies a brain extraction on the functional images as well, which is not performed with fMRIPrep.</p>
      </sec>
      <sec id="Sec29">
        <title>Temporal Signal-to-Noise Ratio (tSNR) After Preprocessing</title>
        <p id="Par64">We computed the voxel-wise temporal SNR according to Smith et al. (<xref ref-type="bibr" rid="CR45">2013</xref>) to assess the amount of informative signal contained in the data after preprocessing. This measurement serves as a rough estimate to compare different preprocessing methods, but did not allow a direct comparison between datasets, as the tSNR value is a relative measurement that depends highly on the paradigm presented, the initial spatial and temporal resolution of the functional images, as well as the MRI scan sequence specific parameters such as acceleration factors (Smith et al. <xref ref-type="bibr" rid="CR45">2013</xref>). Using Nipype’s TSNR routine, we first removed 2nd-degree polynomial drifts in each functional image, and estimated tSNR maps by computing each voxel’s temporal mean, dividing it by its temporal standard deviation, and multiplying it by the square root of the number of time points recorded in a given run. By averaging the tSNR maps over the population, we get a general tSNR map per preprocessing method for each dataset (see Fig. <xref rid="Fig10" ref-type="fig">10</xref>).<fig id="Fig10"><label>Fig. 10</label><caption><p>Depiction of temporal signal-to-noise ratio maps of three different datasets, after multiple functional preprocessing approaches. Preprocessing was done with fMRIflows (with and without a temporal low-pass filter at 0.2 Hz), fMRIPrep, FSL and SPM (from top to bottom) separated for the TR2000 (left), TR1000 (middle) and TR600 (right) dataset. Color value represents the tSNR value as computed with the Nipype routine TSNR. Color scale was set manually and differs between datasets, but is held constant between different preprocessing methods (Color figure online)</p></caption><graphic xlink:href="10548_2022_935_Fig10_HTML" id="MO10"/></fig></p>
        <p id="Par65">In general, preprocessing with fMRIflows without temporal low-pass filter led to similar average tSNR maps as preprocessing with fMRIPrep. Overall, preprocessing with FSL led to slightly increased average tSNR values, while preprocessing with SPM led to slightly decreased average tSNR maps. The additional application of a low-pass filter at 0.2 Hz in all three datasets led to increased tSNR values after preprocessing with fMRIflows. This effect was more pronounced for higher temporal resolution (as in Dataset TR1000 and TR600). The scales in Fig. <xref rid="Fig10" ref-type="fig">10</xref> were set manually so that the fMRIflows (without low-pass filter) approach shows comparable intensities for the three datasets.</p>
      </sec>
      <sec id="Sec30">
        <title>Performance Check After 1st-Level Analysis</title>
        <p id="Par66">To investigate the effect of the different preprocessing methods on the 1st-level analysis, we carried out within-subject statistical analysis using Nistats. The activation maps were estimated using a general linear model (GLM). The GLM included a constant term, the stimuli regressors convolved with a double-gamma canonical hemodynamic response function, six motion parameters (three translation and three rotation), and a high pass filter at 100 Hz, represented by a set of cosine functions, and no temporal derivatives. The input data were smoothed using a kernel with a FWHM of 6 mm, using a Nilearn routine. The analysis pipelines between the preprocessing methods and datasets were kept as identical as possible, and differed only in the number of time points contained in the dataset and the estimated motion parameters. The statistical map for each participant was binarized at z = 3.09, which corresponds to a one-sided test value of <italic>p</italic> &lt; 0.001. The population average of these maps is shown in Fig. <xref rid="Fig11" ref-type="fig">11</xref>.<fig id="Fig11"><label>Fig. 11</label><caption><p>Depiction of binarized 1st-level activation count maps, thresholded at <italic>p</italic> &lt; 0.001, after multiple functional preprocessing approaches. Preprocessing was done with fMRIflows (with and without a temporal low-pass filter at 0.2 Hz), fMRIPrep, FSL and SPM (from top to bottom) separated for the TR2000 (left), TR1000 (middle) and TR600 (right) dataset. Activation count maps were normalized to the ICBM 2009c brain template. Color code represents the fraction of participants that show significant activation above a <italic>p</italic>-value threshold at 0.001 and corrected for false positive rate (FPR) (Color figure online)</p></caption><graphic xlink:href="10548_2022_935_Fig11_HTML" id="MO11"/></fig></p>
        <p id="Par67">The results show that the thresholded activation count maps between the fMRIflows approach without a low-pass filter, fMRIPrep, FSL and SPM do not differ too much between each other, for all three datasets. In contrast to the other preprocessing methods, however, the preprocessing with fMRIflows with a low-pass filter at 0.2 Hz drastically increased the size and fraction value of the thresholded activation count maps, for the datasets TR1000 and TR600. Thus, appropriate temporal filtering increased the statistics for datasets with higher temporal resolution remarkably. For a more detailed comparison between all the toolboxes, see Supplementary Note 7.</p>
      </sec>
      <sec id="Sec31">
        <title>Performance Check After 2nd-Level Analysis</title>
        <p id="Par68">To investigate the effect of the different preprocessing methods on the 2nd-level analysis, we carried out between-subject statistical analysis using Nistats and computed one-sample t-test for each preprocessing method and dataset. The unthresholded group-level T-statistic maps of each analysis were then compared to each other on a voxel-by-voxel level using Bland–Altman 2D histograms (Bowring et al. <xref ref-type="bibr" rid="CR7">2018</xref>), see Fig. <xref rid="Fig12" ref-type="fig">12</xref>.<fig id="Fig12"><label>Fig. 12</label><caption><p>Bland–Altman 2D histograms of three different datasets, comparing unthresholded group-level T-statistic maps between multiple processing approaches. Datasets TR2000 (top), TR1000 (middle) and TR600 (bottom) were used for the comparison. Density plots show the relationship between average T-statistic value (horizontal) and difference of T-statistic values (vertical) at corresponding voxels for different pairwise combinations of toolboxes. The difference of T-statistics was always computed in contrast to a preprocessing with fMRIflows using a low-pass filter at 0.2 Hz, while the average T-statistics in horizontal direction investigated the preprocessing with (from left to right) fMRIflows without a low-pass filter, fMRIPrep, FSL and SPM. Distribution plots next to x- and y-axis depict occurrence of a given value in this domain. Color code within the figure indicates the number of voxels at this given overlap, from a few (blue) to many (yellow). Yellow horizontal line at zero indicates no value differences between corresponding voxels. Red dashed line depicts horizontal density average (Color figure online)</p></caption><graphic xlink:href="10548_2022_935_Fig12_HTML" id="MO12"/></fig></p>
        <p id="Par69">The results shown in Fig. <xref rid="Fig12" ref-type="fig">12</xref> indicate no pronounced differences between the preprocessing with fMRIflows with a low-pass filter at 0.2 Hz and the other four approaches for the analysis of the TR2000 dataset. An increased variability in the y-direction indicated a decrease in voxel-to-voxel correspondence, which might be explained by different spatial normalization implementations. The fact that the average horizontal density value (dashed line) is close to the zero line (horizontal solid line) indicated that the different preprocessing methods led to comparable group-level results with the TR2000 dataset. The Bland–Altman plots for the TR1000 and TR600 datasets showed a clear increase of t-statistic when the preprocessing was done with fMRIflows with a low-pass filter at 0.2 Hz, compared to any other method. This effect was stronger for higher t-values. For a more detailed comparison between all the toolboxes, see Supplementary Note 8.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec32">
    <title>Discussion</title>
    <p id="Par70">fMRIflows is a fully automatic fMRI analysis pipeline, which can perform state-of-the-art preprocessing, including 1st-level and 2nd-level univariate analyses as well as multivariate analyses. The goal of such an autonomous approach is to improve objectiveness of the analyses, maximize transparency, facilitate ease-of-use, and provide accessible and updated analysis approaches to every researcher, including users outside the field of neuroimaging. While the predefined analysis pipelines help to reduce the number of error-prone manual interventions to a minimum, it also has the advantage of decreasing the number of analytical degrees of freedom available to a user to its minimum (Carp <xref ref-type="bibr" rid="CR11">2012</xref>). This constraint in flexibility is important as it helps to control the variability in data processing and analysis (Botvinik-Nezer et al. <xref ref-type="bibr" rid="CR5">2020</xref>). fMRIPrep showed a clear need for such analysis-agnostic approaches and was therefore chosen to provide much of the groundwork for fMRIflows. Our pipeline provides a reliable methodological framework for analyzing fMRI data and for obtaining statistical results that are comparable across different scanners/laboratories and experimental designs. fMRIflows achieves: (1) high SNR after preprocessing, (2) reproducible within-subject t-statistics, and (3) reproducible between-subject t-statistics. The flexibility for the user to perform both spatial and temporal filtering is particularly important in the context of datasets that had a temporal sampling equal to or below 1000 ms or if the statistical output will be used for more advanced analyses, such as MVPA. fMRIflows also improved the overall computation time needed to perform preprocessing and 1st and 2nd-level analyses. Indeed, Nipype provides a parallel execution feature of processing pipelines, which is not yet possible with FSL or SPM. fMRIPrep uses the same boost of parallelism but is overall much slower if the default execution of FreeSurfer’s recon-all routine is performed. However, fMRIflows does not yet support parallel computation via a job scheduler on a computation cluster, which is currently possible with fMRIPrep.</p>
    <p id="Par71">In comparison with other neuroimaging software/pipelines like fMRIPrep, FSL and SPM, fMRIflows achieved comparable or improved results in (1) SNR after preprocessing, (2) within-subject t-statistics, and (3) between-subject t-statistics. These results were more obvious in the context of datasets that had a temporal sampling equal to or below 1000 ms, and if a low-pass filter at 0.2 Hz was applied.</p>
    <p id="Par72">The inclusion of many informative visual reports allows a direct quality control and verification of the performed processing steps, as fMRIflows’ outputs provide a general quality assessment even though it is not as detailed and rigorous as MRIQC (Esteban et al. <xref ref-type="bibr" rid="CR13">2017</xref>). In contrast to other software packages, fMRIflows uses an adapted visualization of the carpet plot proposed by Power (<xref ref-type="bibr" rid="CR39">2017</xref>) to highlight underlying temporal structure and voxel-to-voxel correlations within different brain tissue regions and/or throughout the brain. Such approaches help to observe general signal trends and sudden abrupt signal changes throughout the brain, but the exact implications of these modified carpet plots need to be further investigated.</p>
    <p id="Par73">Being an open-source project, shared via GitHub, facilitates the transparency in the development of fMRIflows. Users can inspect the complete history of the changes and have access to all discussions connected to the software. Code adaptations and additional support to new usage will be proposed by the user community, which will make the adaptation to newest standards easy and straightforward. In addition to the version-controlled system used on GitHub, a continuous integration scheme with CircleCi will ensure continuous functionality.</p>
    <p id="Par74">Results of fMRIflows’ validation phase 1 suggests that the software is capable of analyzing different types of datasets, independently of the extent of head coverage, original image orientation, spatial or temporal resolution. By increasing the user base and testing fMRIflows on many more datasets, new adaptations might be required and hidden bugs could emerge. Users can observe any changes done to the software in the future directly on GitHub and are encouraged to state any questions or comments in connection with the software on the community driven neuroinformatics forum NeuroStars (<ext-link ext-link-type="uri" xlink:href="https://neurostars.org">https://neurostars.org</ext-link>).</p>
    <p id="Par75">Further development of the software will involve (1) moving away from an SPM dependency for the 1st and 2nd-level modeling, (2) using the more flexible FitLins toolbox (<ext-link ext-link-type="uri" xlink:href="https://github.com/poldracklab/fitlins">https://github.com/poldracklab/fitlins</ext-link>) conforming the results with the BIDS statistical models proposal (BEP002), and (3) implementing an fMRIflows BIDS-App to further improve the toolbox’s accessibility.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec33">
      <p>Below is the link to the electronic supplementary material.<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="10548_2022_935_MOESM1_ESM.docx"><caption><p>Supplementary file1 (DOCX 6606 kb)</p></caption></media></supplementary-material></p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Michael P. Notter and Peer Herholz have made equal contributions to this work.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>We thank the creator of fMRIPrep for providing an excellent starting point and inspiration in the development of fMRIflows. We also thank the developers of PyMVPA, Nilearn, Nibabel and Nistats for bringing the neuroimaging domain into the Python universe, and the developer of Nipype and BIDS for creating a clear framework to execute processing pipelines, as well as the whole neuroimaging open source and science community with its numerous contributors.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>MPN: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Writing, Visualization, Project administration; PH: Conceptualization, Methodology, Software, Validation, Writing; SDC: Methodology, Supervision, Writing—Review and Editing, Validation; OFG: Writing—Reviewing and Editing, Validation; AII: Writing—Review and Editing, Validation; AG: Writing—Reviewing and Editing, Validation; MMM: Supervision, Writing—Reviewing and Editing, Funding acquisition.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open access funding provided by University of Lausanne. This work was supported by the Swiss National Science Foundation (Grant Numbers 320030-169206 to M.M.M.) and research funds from the Radiology Service at the University Hospital in Lausanne (CHUV). P.H. was supported in parts by funding from the Canada First Research Excellence Fund, awarded to McGill University for the Healthy Brains for Healthy Lives Initiative, the National Institutes of Health (NIH) NIH-NIBIB P41 EB019936 (ReproNim), the National Institute Of Mental Health of the NIH under Award Number R01MH096906, as well as by a Research Scholar Award from Brain Canada, in partnership with Health Canada, for the Canadian Open Neuroscience Platform Initiative. A. G. was supported by the Marie Curie Fellowship Grant Funding, Grant Number DVL-894612.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1" notes-type="COI-statement">
      <title>Conflict of interest</title>
      <p id="Par76">The authors declare no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Abraham</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Eickenberg</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gervais</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Mueller</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kossaifi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Machine learning for neuroimaging with scikit-learn</article-title>
        <source>Front Neuroinform</source>
        <year>2014</year>
        <volume>8</volume>
        <fpage>14</fpage>
        <pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id>
        <?supplied-pmid 24600388?>
        <pub-id pub-id-type="pmid">24600388</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <mixed-citation publication-type="other">Ashburner J (2009) Preparing fMRI data for statistical analysis. In: fMRI techniques and protocols. Springer, New York, pp 151–178</mixed-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Avants</surname>
            <given-names>BB</given-names>
          </name>
          <name>
            <surname>Tustison</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Cook</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>A reproducible evaluation of ANTs similarity metric performance in brain image registration</article-title>
        <source>Neuroimage</source>
        <year>2011</year>
        <volume>54</volume>
        <fpage>2033</fpage>
        <lpage>2044</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.025</pub-id>
        <?supplied-pmid 20851191?>
        <pub-id pub-id-type="pmid">20851191</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Behzadi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Restom</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Liau</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>TT</given-names>
          </name>
        </person-group>
        <article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title>
        <source>Neuroimage</source>
        <year>2007</year>
        <volume>37</volume>
        <fpage>90</fpage>
        <lpage>101</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id>
        <?supplied-pmid 17560126?>
        <pub-id pub-id-type="pmid">17560126</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Botvinik-Nezer</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Iwanir</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Holzmeister</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Johannesson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kirchler</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Dreber</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Camerer</surname>
            <given-names>CF</given-names>
          </name>
          <name>
            <surname>Poldrack</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Schonberg</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>fMRI data of mixed gambles from the Neuroimaging Analysis Replication and Prediction Study</article-title>
        <source>Sci Data</source>
        <year>2019</year>
        <volume>6</volume>
        <fpage>106</fpage>
        <pub-id pub-id-type="doi">10.1038/s41597-019-0113-7</pub-id>
        <?supplied-pmid 31263104?>
        <pub-id pub-id-type="pmid">31263104</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Botvinik-Nezer</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Variability in the analysis of a single neuroimaging dataset by many teams</article-title>
        <source>Nature</source>
        <year>2020</year>
        <volume>582</volume>
        <fpage>84</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-020-2314-9</pub-id>
        <?supplied-pmid 32483374?>
        <pub-id pub-id-type="pmid">32483374</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <mixed-citation publication-type="other">Bowring A, Maumet C, Nichols T (2018) Exploring the impact of analysis software on task fMRI results. BioRxiv:285585</mixed-citation>
    </ref>
    <ref id="CR8">
      <mixed-citation publication-type="other">Brett M et al (2018) NiBabel: access a cacophony of neuro-imaging file formats, version 2.3.0</mixed-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caballero-Gaudes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Reynolds</surname>
            <given-names>RC</given-names>
          </name>
        </person-group>
        <article-title>Methods for cleaning the BOLD fMRI signal</article-title>
        <source>Neuroimage</source>
        <year>2016</year>
        <volume>154</volume>
        <fpage>128</fpage>
        <lpage>149</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.12.018</pub-id>
        <?supplied-pmid 27956209?>
        <pub-id pub-id-type="pmid">27956209</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caballero-Gaudes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Reynolds</surname>
            <given-names>RC</given-names>
          </name>
        </person-group>
        <article-title>Methods for cleaning the BOLD fMRI signal</article-title>
        <source>Neuroimage</source>
        <year>2017</year>
        <volume>154</volume>
        <fpage>128</fpage>
        <lpage>149</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.12.018</pub-id>
        <?supplied-pmid 27956209?>
        <pub-id pub-id-type="pmid">27956209</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carp</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>The secret lives of experiments: methods reporting in the fMRI literature</article-title>
        <source>Neuroimage</source>
        <year>2012</year>
        <volume>63</volume>
        <fpage>289</fpage>
        <lpage>300</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.07.004</pub-id>
        <?supplied-pmid 22796459?>
        <pub-id pub-id-type="pmid">22796459</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cox</surname>
            <given-names>RW</given-names>
          </name>
          <name>
            <surname>Hyde</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>Software tools for analysis and visualization of fMRI data</article-title>
        <source>NMR Biomed</source>
        <year>1997</year>
        <volume>10</volume>
        <fpage>171</fpage>
        <lpage>178</lpage>
        <pub-id pub-id-type="doi">10.1002/(SICI)1099-1492(199706/08)10:4/5&lt;171::AID-NBM453&gt;3.0.CO;2-L</pub-id>
        <?supplied-pmid 9430344?>
        <pub-id pub-id-type="pmid">9430344</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Esteban</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Birman</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Schaer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Koyejo</surname>
            <given-names>OO</given-names>
          </name>
          <name>
            <surname>Poldrack</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Gorgolewski</surname>
            <given-names>KJ</given-names>
          </name>
        </person-group>
        <article-title>MRIQC: advancing the automatic prediction of image quality in MRI from unseen sites</article-title>
        <source>PLoS ONE</source>
        <year>2017</year>
        <volume>12</volume>
        <fpage>e0184661</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0184661</pub-id>
        <?supplied-pmid 28945803?>
        <pub-id pub-id-type="pmid">28945803</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Esteban</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Markiewicz</surname>
            <given-names>CJ</given-names>
          </name>
          <name>
            <surname>Blair</surname>
            <given-names>RW</given-names>
          </name>
          <name>
            <surname>Moodie</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Isik</surname>
            <given-names>AI</given-names>
          </name>
          <name>
            <surname>Erramuzpe</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kent</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Goncalves</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>DuPre</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Oya</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Wright</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Durnez</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Poldrack</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Gorgolewski</surname>
            <given-names>KJ</given-names>
          </name>
        </person-group>
        <article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>111</fpage>
        <lpage>116</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id>
        <?supplied-pmid 30532080?>
        <pub-id pub-id-type="pmid">30532080</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Feinberg</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Setsompop</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Ultra-fast MRI of the human brain with simultaneous multi-slice imaging</article-title>
        <source>J Magn Reson</source>
        <year>2013</year>
        <volume>229</volume>
        <fpage>90</fpage>
        <lpage>100</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jmr.2013.02.002</pub-id>
        <?supplied-pmid 23473893?>
        <pub-id pub-id-type="pmid">23473893</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Feinberg</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Moeller</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Auerbach</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Ramanna</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gunther</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Glasser</surname>
            <given-names>MF</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>KL</given-names>
          </name>
          <name>
            <surname>Ugurbil</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Yacoub</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Multiplexed echo planar imaging for sub-second whole brain fMRI and fast diffusion imaging</article-title>
        <source>PLoS ONE</source>
        <year>2010</year>
        <volume>5</volume>
        <fpage>e15710</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0015710</pub-id>
        <?supplied-pmid 21187930?>
        <pub-id pub-id-type="pmid">21187930</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fischl</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>FreeSurfer</article-title>
        <source>Neuroimage</source>
        <year>2012</year>
        <volume>62</volume>
        <fpage>774</fpage>
        <lpage>781</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id>
        <?supplied-pmid 22248573?>
        <pub-id pub-id-type="pmid">22248573</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fonov</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Evans</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Botteron</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Almli</surname>
            <given-names>CR</given-names>
          </name>
          <name>
            <surname>McKinstry</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Collins</surname>
            <given-names>DL</given-names>
          </name>
        </person-group>
        <article-title>Unbiased average age-appropriate atlases for pediatric studies</article-title>
        <source>Neuroimage</source>
        <year>2011</year>
        <volume>54</volume>
        <fpage>313</fpage>
        <lpage>327</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.07.033</pub-id>
        <?supplied-pmid 20656036?>
        <pub-id pub-id-type="pmid">20656036</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friston</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Howard</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Frackowiak</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Turner</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Movement-related effects in fMRI time-series</article-title>
        <source>Magn Reson Med</source>
        <year>1996</year>
        <volume>35</volume>
        <fpage>346</fpage>
        <lpage>355</lpage>
        <pub-id pub-id-type="doi">10.1002/mrm.1910350312</pub-id>
        <?supplied-pmid 8699946?>
        <pub-id pub-id-type="pmid">8699946</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Friston</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Penny</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ashburner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kiebel</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <source>Statistical parametric mapping: the analysis of functional brain images</source>
        <year>2006</year>
        <publisher-loc>Amsterdam</publisher-loc>
        <publisher-name>Elsevier</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR21">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorgolewski</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Burns</surname>
            <given-names>CD</given-names>
          </name>
          <name>
            <surname>Madison</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Clark</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Halchenko</surname>
            <given-names>YO</given-names>
          </name>
          <name>
            <surname>Waskom</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>SS</given-names>
          </name>
        </person-group>
        <article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title>
        <source>Front Neuroinform</source>
        <year>2011</year>
        <volume>5</volume>
        <fpage>13</fpage>
        <pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id>
        <?supplied-pmid 21897815?>
        <pub-id pub-id-type="pmid">21897815</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorgolewski</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Rivera</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Schwarz</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Maumet</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sochat</surname>
            <given-names>VV</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>TE</given-names>
          </name>
          <name>
            <surname>Poldrack</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Poline</surname>
            <given-names>J-B</given-names>
          </name>
          <name>
            <surname>Yarkoni</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Margulies</surname>
            <given-names>DS</given-names>
          </name>
        </person-group>
        <article-title>NeuroVault.org: a web-based repository for collecting and sharing unthresholded statistical maps of the human brain</article-title>
        <source>Front Neuroinform</source>
        <year>2015</year>
        <volume>9</volume>
        <fpage>8</fpage>
        <pub-id pub-id-type="doi">10.3389/fninf.2015.00008</pub-id>
        <?supplied-pmid 25914639?>
        <pub-id pub-id-type="pmid">25914639</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorgolewski</surname>
            <given-names>KJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title>
        <source>Sci Data</source>
        <year>2016</year>
        <volume>3</volume>
        <fpage>160044</fpage>
        <pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id>
        <?supplied-pmid 27326542?>
        <pub-id pub-id-type="pmid">27326542</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Gorgolewski</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Esteban</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Schaefer</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Wandell</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Poldrack</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>OpenNeuro—a free online platform for sharing and analysis of neuroimaging data</source>
        <year>2017</year>
        <publisher-loc>Vancouver</publisher-loc>
        <publisher-name>Organization for Human Brain Mapping</publisher-name>
        <fpage>1677</fpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Griswold</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Jakob</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Heidemann</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Nittka</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Jellus</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kiefer</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Haase</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Generalized autocalibrating partially parallel acquisitions (GRAPPA)</article-title>
        <source>Magn Reson Med</source>
        <year>2002</year>
        <volume>47</volume>
        <fpage>1202</fpage>
        <lpage>1210</lpage>
        <pub-id pub-id-type="doi">10.1002/mrm.10171</pub-id>
        <?supplied-pmid 12111967?>
        <pub-id pub-id-type="pmid">12111967</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hallquist</surname>
            <given-names>MN</given-names>
          </name>
          <name>
            <surname>Hwang</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Luna</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>The nuisance of nuisance regression: spectral misspecification in a common approach to resting-state fMRI preprocessing reintroduces noise and obscures functional connectivity</article-title>
        <source>Neuroimage</source>
        <year>2006</year>
        <volume>82</volume>
        <fpage>208</fpage>
        <lpage>225</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.116</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hanke</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Halchenko</surname>
            <given-names>YO</given-names>
          </name>
          <name>
            <surname>Sederberg</surname>
            <given-names>PB</given-names>
          </name>
          <name>
            <surname>Hanson</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Haxby</surname>
            <given-names>JV</given-names>
          </name>
          <name>
            <surname>Pollmann</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>PyMVPA: A Python toolbox for multivariate pattern analysis of fMRI data</article-title>
        <source>Neuroinformatics</source>
        <year>2009</year>
        <volume>7</volume>
        <fpage>37</fpage>
        <lpage>53</lpage>
        <pub-id pub-id-type="doi">10.1007/s12021-008-9041-y</pub-id>
        <?supplied-pmid 19184561?>
        <pub-id pub-id-type="pmid">19184561</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hunter</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>Matplotlib: a 2D graphics environment</article-title>
        <source>Comput Sci Eng</source>
        <year>2007</year>
        <volume>9</volume>
        <fpage>90</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jenkinson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Beckmann</surname>
            <given-names>CF</given-names>
          </name>
          <name>
            <surname>Behrens</surname>
            <given-names>TEJ</given-names>
          </name>
          <name>
            <surname>Woolrich</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>SM</given-names>
          </name>
        </person-group>
        <source>FSL Neuroimage</source>
        <year>2012</year>
        <volume>62</volume>
        <fpage>782</fpage>
        <lpage>790</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id>
        <?supplied-pmid 21979382?>
        <pub-id pub-id-type="pmid">21979382</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <mixed-citation publication-type="other">Jones E, Oliphant T, Peterson P et al (2001) {SciPy}: open source scientific tools for {Python}</mixed-citation>
    </ref>
    <ref id="CR31">
      <mixed-citation publication-type="other">Kluyver T, Ragan-Kelley B, Pérez F, Granger B, Bussonnier M, Frederic J, Kelley K, Hamrick J, Grout J, Corlay S, Ivanov P, Avila D, Abdalla S, Willing C (2016) Jupyter Notebooks—a publishing format for reproducible computational workflows. In: Positioning and power in academic publishing: players, agents and agendas. IOS Press, Amsterdam, pp 87–90</mixed-citation>
    </ref>
    <ref id="CR32">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lanczos</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of noisy data</article-title>
        <source>J Soc Ind Appl Math B</source>
        <year>1964</year>
        <volume>1</volume>
        <fpage>76</fpage>
        <lpage>85</lpage>
        <pub-id pub-id-type="doi">10.1137/0701007</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lindquist</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Geuter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wager</surname>
            <given-names>TD</given-names>
          </name>
          <name>
            <surname>Caffo</surname>
            <given-names>BS</given-names>
          </name>
        </person-group>
        <article-title>Modular preprocessing pipelines can reintroduce artifacts into fMRI data</article-title>
        <source>Hum Brain Mapp</source>
        <year>2019</year>
        <volume>40</volume>
        <issue>8</issue>
        <fpage>2358</fpage>
        <lpage>2376</lpage>
        <pub-id pub-id-type="doi">10.1002/hbm.24528</pub-id>
        <?supplied-pmid 30666750?>
        <pub-id pub-id-type="pmid">30666750</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <mixed-citation publication-type="other">McKinney W et al (2010) Data structures for statistical computing in Python. In: Proceedings of the 9th Python in science conference, 2010, pp 51–56</mixed-citation>
    </ref>
    <ref id="CR35">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moeller</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Yacoub</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Olman</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Auerbach</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Strupp</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Harel</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Uğurbil</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Multiband multislice GE-EPI at 7 Tesla, with 16-fold acceleration using partial parallel imaging with application to high spatial and temporal whole-brain fMRI</article-title>
        <source>Magn Reson Med</source>
        <year>2010</year>
        <volume>63</volume>
        <fpage>1144</fpage>
        <lpage>1153</lpage>
        <pub-id pub-id-type="doi">10.1002/mrm.22361</pub-id>
        <?supplied-pmid 20432285?>
        <pub-id pub-id-type="pmid">20432285</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Notter</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Gale</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Herholz</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Markello</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Notter-Bielser</surname>
            <given-names>M-L</given-names>
          </name>
          <name>
            <surname>Whitaker</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>AtlasReader: a Python package to generate coordinate tables, region labels, and informative figures from statistical MRI images</article-title>
        <source>J Open Source Softw</source>
        <year>2019</year>
        <volume>4</volume>
        <fpage>1257</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.01257</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Oliphant</surname>
            <given-names>TE</given-names>
          </name>
        </person-group>
        <article-title>Python for scientific computing</article-title>
        <source>Comput Sci Eng</source>
        <year>2007</year>
        <volume>9</volume>
        <issue>3</issue>
        <fpage>10</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1109/MCSE.2007.58</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Penny</surname>
            <given-names>WD</given-names>
          </name>
          <name>
            <surname>Friston</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Ashburner</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Kiebel</surname>
            <given-names>SJ</given-names>
          </name>
          <name>
            <surname>Nichols</surname>
            <given-names>TE</given-names>
          </name>
        </person-group>
        <source>Statistical parametric mapping: the analysis of functional brain images</source>
        <year>2011</year>
        <publisher-loc>Amsterdam</publisher-loc>
        <publisher-name>Elsevier</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR39">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Power</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>A simple but useful way to assess fMRI scan qualities</article-title>
        <source>Neuroimage</source>
        <year>2017</year>
        <volume>154</volume>
        <fpage>150</fpage>
        <lpage>158</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.009</pub-id>
        <?supplied-pmid 27510328?>
        <pub-id pub-id-type="pmid">27510328</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Power</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Barnes</surname>
            <given-names>KA</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>AZ</given-names>
          </name>
          <name>
            <surname>Schlaggar</surname>
            <given-names>BL</given-names>
          </name>
          <name>
            <surname>Petersen</surname>
            <given-names>SE</given-names>
          </name>
        </person-group>
        <article-title>Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion</article-title>
        <source>Neuroimage</source>
        <year>2012</year>
        <volume>59</volume>
        <fpage>2142</fpage>
        <lpage>2154</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.018</pub-id>
        <?supplied-pmid 22019881?>
        <pub-id pub-id-type="pmid">22019881</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Power</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Plitt</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Laumann</surname>
            <given-names>TO</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Sources and implications of whole-brain fMRI signals in humans</article-title>
        <source>Neuroimage</source>
        <year>2017</year>
        <volume>146</volume>
        <fpage>609</fpage>
        <lpage>625</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.09.038</pub-id>
        <?supplied-pmid 27751941?>
        <pub-id pub-id-type="pmid">27751941</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sengupta</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pollmann</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Hanke</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Spatial band-pass filtering aids decoding musical genres from auditory cortex 7 T fMRI</article-title>
        <source>F1000Res</source>
        <year>2018</year>
        <volume>7</volume>
        <fpage>142</fpage>
        <pub-id pub-id-type="doi">10.12688/f1000research.13689.1</pub-id>
        <?supplied-pmid 29707198?>
        <pub-id pub-id-type="pmid">29707198</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Jenkinson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Woolrich</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Beckmann</surname>
            <given-names>CF</given-names>
          </name>
          <name>
            <surname>Behrens</surname>
            <given-names>TEJ</given-names>
          </name>
          <name>
            <surname>Johansen-Berg</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Bannister</surname>
            <given-names>PR</given-names>
          </name>
          <name>
            <surname>De Luca</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Drobnjak</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Flitney</surname>
            <given-names>DE</given-names>
          </name>
          <name>
            <surname>Niazy</surname>
            <given-names>RK</given-names>
          </name>
          <name>
            <surname>Saunders</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Vickers</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>De Stefano</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Brady</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Matthews</surname>
            <given-names>PM</given-names>
          </name>
        </person-group>
        <article-title>Advances in functional and structural MR image analysis and implementation as FSL</article-title>
        <source>Neuroimage</source>
        <year>2004</year>
        <volume>23</volume>
        <issue>Suppl 1</issue>
        <fpage>S208</fpage>
        <lpage>S219</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.07.051</pub-id>
        <?supplied-pmid 15501092?>
        <pub-id pub-id-type="pmid">15501092</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>SM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Resting-state fMRI in the Human Connectome Project</article-title>
        <source>Neuroimage</source>
        <year>2013</year>
        <volume>80</volume>
        <fpage>144</fpage>
        <lpage>168</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.039</pub-id>
        <?supplied-pmid 23702415?>
        <pub-id pub-id-type="pmid">23702415</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stelzer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Turner</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Statistical inference and multiple testing correction in classification-based multi-voxel pattern analysis (MVPA): random permutations and cluster size control</article-title>
        <source>Neuroimage</source>
        <year>2013</year>
        <volume>65</volume>
        <fpage>69</fpage>
        <lpage>82</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.09.063</pub-id>
        <?supplied-pmid 23041526?>
        <pub-id pub-id-type="pmid">23041526</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Strother</surname>
            <given-names>SC</given-names>
          </name>
        </person-group>
        <article-title>Evaluating fMRI preprocessing pipelines</article-title>
        <source>IEEE Eng Med Biol Mag</source>
        <year>2006</year>
        <volume>25</volume>
        <fpage>27</fpage>
        <lpage>41</lpage>
        <pub-id pub-id-type="doi">10.1109/MEMB.2006.1607667</pub-id>
        <?supplied-pmid 16568935?>
        <pub-id pub-id-type="pmid">16568935</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Viessmann</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Möller</surname>
            <given-names>HE</given-names>
          </name>
          <name>
            <surname>Jezzard</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Dual regression physiological modeling of resting-state EPI power spectra: effects of healthy aging</article-title>
        <source>Neuroimage</source>
        <year>2018</year>
        <volume>187</volume>
        <fpage>68</fpage>
        <lpage>76</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.01.011</pub-id>
        <?supplied-pmid 29398431?>
        <pub-id pub-id-type="pmid">29398431</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yarkoni</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>PyBIDS: Python tools for BIDS datasets</article-title>
        <source>J Open Source Softw</source>
        <year>2019</year>
        <volume>4</volume>
        <fpage>1294</fpage>
        <pub-id pub-id-type="doi">10.21105/joss.01294</pub-id>
        <?supplied-pmid 32775955?>
        <pub-id pub-id-type="pmid">32775955</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
