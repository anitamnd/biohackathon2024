<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">G3 (Bethesda)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Genetics</journal-id>
    <journal-id journal-id-type="publisher-id">g3journal</journal-id>
    <journal-title-group>
      <journal-title>G3: Genes|Genomes|Genetics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2160-1836</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9635651</article-id>
    <article-id pub-id-type="pmid">36124944</article-id>
    <article-id pub-id-type="doi">10.1093/g3journal/jkac226</article-id>
    <article-id pub-id-type="publisher-id">jkac226</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software and Data Resources</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01180</subject>
        <subject>AcademicSubjects/SCI01140</subject>
        <subject>AcademicSubjects/SCI00010</subject>
        <subject>AcademicSubjects/SCI00960</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>learnMET: an R package to apply machine learning methods for genomic prediction using multi-environment trial data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Westhues</surname>
          <given-names>Cathy C</given-names>
        </name>
        <aff><institution>Division of Plant Breeding Methodology, Department of Crop Sciences, University of Goettingen</institution>, 37075 Goettingen, <country country="DE">Germany</country></aff>
        <aff><institution>Center for Integrated Breeding Research, University of Goettingen</institution>, 37075 Goettingen, <country country="DE">Germany</country></aff>
        <xref rid="jkac226-cor1" ref-type="corresp"/>
        <!--cathy.jubin@uni-goettingen.de-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Simianer</surname>
          <given-names>Henner</given-names>
        </name>
        <aff><institution>Center for Integrated Breeding Research, University of Goettingen</institution>, 37075 Goettingen, <country country="DE">Germany</country></aff>
        <aff><institution>Animal Breeding and Genetics Group, Department of Animal Sciences, University of Gottingen</institution>, 37075 Gottingen, <country country="DE">Germany</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2882-4074</contrib-id>
        <name>
          <surname>Beissinger</surname>
          <given-names>Timothy M</given-names>
        </name>
        <aff><institution>Division of Plant Breeding Methodology, Department of Crop Sciences, University of Goettingen</institution>, 37075 Goettingen, <country country="DE">Germany</country></aff>
        <aff><institution>Center for Integrated Breeding Research, University of Goettingen</institution>, 37075 Goettingen, <country country="DE">Germany</country></aff>
        <xref rid="jkac226-cor1" ref-type="corresp"/>
        <!--beissinger@gwdg.de-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>de los Campos</surname>
          <given-names>G</given-names>
        </name>
        <role>Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="jkac226-cor1">Corresponding author: Division of Plant Breeding Methodology, Department of Crop Sciences, University of Goettingen, Carl-Sprengel-Weg 1, 37075, Goettingen, Germany. Email: <email>cathy.jubin@uni-goettingen.de</email>; *Corresponding author: Division of Plant Breeding Methodology, Department of Crop Sciences, University of Goettingen, Carl-Sprengel-Weg 1, 37075, Goettingen, Germany. Email: <email>beissinger@gwdg.de</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>11</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-09-19">
      <day>19</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>19</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <volume>12</volume>
    <issue>11</issue>
    <elocation-id>jkac226</elocation-id>
    <history>
      <date date-type="accepted">
        <day>29</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="received">
        <day>26</day>
        <month>4</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>23</day>
        <month>9</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press on behalf of Genetics Society of America.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="jkac226.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>We introduce the R-package <italic toggle="yes">learnMET</italic>, developed as a flexible framework to enable a collection of analyses on multi-environment trial breeding data with machine learning-based models. <italic toggle="yes">learnMET</italic> allows the combination of genomic information with environmental data such as climate and/or soil characteristics. Notably, the package offers the possibility of incorporating weather data from field weather stations, or to retrieve global meteorological datasets from a NASA database. Daily weather data can be aggregated over specific periods of time based on naive (for instance, nonoverlapping 10-day windows) or phenological approaches. Different machine learning methods for genomic prediction are implemented, including gradient-boosted decision trees, random forests, stacked ensemble models, and multilayer perceptrons. These prediction models can be evaluated via a collection of cross-validation schemes that mimic typical scenarios encountered by plant breeders working with multi-environment trial experimental data in a user-friendly way. The package is published under an MIT license and accessible on GitHub.</p>
    </abstract>
    <kwd-group>
      <kwd>multienvironment trials</kwd>
      <kwd>machine learning</kwd>
      <kwd>genotype ×</kwd>
      <kwd>environment interaction</kwd>
      <kwd>genomic prediction</kwd>
      <kwd>R software</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>University of Göttingen</institution>
            <institution-id institution-id-type="DOI">10.13039/501100003385</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Center for Integrated Breeding Research</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="13"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>Introduction</title>
    <p>Large amounts of data from various sources (phenotypic records from field trials, genomic or omics data, environmental information) are regularly gathered as part of multi-environment trials (MET). The efficient exploitation of these extensive datasets has become of utmost interest for breeders to address essentially two objectives: (1) accurately predicting genotype performance in future environments; (2) untangling complex relationships between genetic markers, environmental covariables (ECs), and phenotypes to better understand the pervasive phenomenon of genotype-by-environment (G × E) interaction. </p>
    <p>Many R packages have recently been developed that allow to implement genomic prediction models accounting for G × E effects using mixed models: BGLR (<xref rid="jkac226-B33" ref-type="bibr">Pérez and de Los Campos 2014</xref>), sommer (<xref rid="jkac226-B11" ref-type="bibr">Covarrubias-Pazaran 2016</xref>), Bayesian Genomic Genotype × Environment Interaction (BGGE) (<xref rid="jkac226-B20" ref-type="bibr">Granato <italic toggle="yes">et al.</italic> 2018</xref>), Bayesian Multi-Trait Multi-Environment for Genomic Selection (BMTME) (<xref rid="jkac226-B30" ref-type="bibr">Montesinos-López <italic toggle="yes">et al.</italic> 2019</xref>), bWGR (<xref rid="jkac226-B43" ref-type="bibr">Xavier <italic toggle="yes">et al.</italic> 2019</xref>), EnvRtype (<xref rid="jkac226-B10" ref-type="bibr">Costa-Neto, Galli, <italic toggle="yes">et al.</italic> 2021</xref>), and MegaLMM (<xref rid="jkac226-B37" ref-type="bibr">Runcie <italic toggle="yes">et al.</italic> 2021</xref>). BGGE presents a speed advantage compared to BGLR, that is explained by the use of an optimization procedure for sparse covariance matrices, while BMTME additionally exploits the genetic correlation among traits and environments to build linear G × E models. EnvRtype further widens the range of opportunities in Bayesian kernel models with the possibility to use nonlinear arc-cosine kernels aiming at reproducing a deep learning approach (<xref rid="jkac226-B14" ref-type="bibr">Cuevas <italic toggle="yes">et al.</italic> 2019</xref>; <xref rid="jkac226-B9" ref-type="bibr">Costa-Neto, Fritsche-Neto, <italic toggle="yes">et al.</italic> 2021</xref>), and to harness environmental data retrieved by the package.</p>
    <p>While Bayesian approaches have been successful at dramatically improving predictive ability in multi-environment breeding experiments (<xref rid="jkac226-B13" ref-type="bibr">Cuevas <italic toggle="yes">et al.</italic> 2017</xref>, <xref rid="jkac226-B14" ref-type="bibr">2019</xref>; <xref rid="jkac226-B10" ref-type="bibr">Costa-Neto, Fritsche-Neto, <italic toggle="yes">et al.</italic> 2021</xref>), data-driven machine learning algorithms represent alternative predictive modeling techniques with increased flexibility with respect to the form of the mapping function between input and output variables. In particular, nonlinear effects including gene × gene and genotype × environment (G × E) interactions can be captured with machine learning models (<xref rid="jkac226-B36" ref-type="bibr">Ritchie <italic toggle="yes">et al.</italic> 2003</xref>; <xref rid="jkac226-B28" ref-type="bibr">McKinney <italic toggle="yes">et al.</italic> 2006</xref>; <xref rid="jkac226-B12" ref-type="bibr">Crossa <italic toggle="yes">et al.</italic> 2019</xref>; <xref rid="jkac226-B41" ref-type="bibr">Westhues <italic toggle="yes">et al.</italic> 2021</xref>). G × E interactions are of utmost interest for plant breeders, especially when they present a crossover type, because the latter implies a change in the relative ranking of genotypes across different environments. Breeders generally cope with G × E by either (1) focusing their program on wide adaptation of cultivars over a target population of environments, from which follows that the developed varieties are not the best ones for a given environment, and positive G × E interactions are not exploited, or (2) identifying varieties that are the best adapted to specific environments (<xref rid="jkac226-B3" ref-type="bibr">Bernardo 2002</xref>). Enhancing the modeling of genotype-by-environment interactions, by the inclusion of environmental covariates related to critical developmental stages, also resulted in an increase of predictive ability in many studies using MET datasets (<xref rid="jkac226-B22" ref-type="bibr">Heslot <italic toggle="yes">et al.</italic> 2012</xref>; <xref rid="jkac226-B31" ref-type="bibr">Monteverde <italic toggle="yes">et al.</italic> 2019</xref>; <xref rid="jkac226-B35" ref-type="bibr">Rincent <italic toggle="yes">et al.</italic> 2019</xref>; <xref rid="jkac226-B9" ref-type="bibr">Costa-Neto, Fritsche-Neto, <italic toggle="yes">et al.</italic> 2021</xref>).</p>
    <p>In this article, we describe the R-package learnMET and its principal functionalities. learnMET provides a pipeline to (1) facilitate environmental characterization and (2) evaluate and compare different types of machine learning approaches to predict quantitative traits based on relevant cross-validation (CV) schemes for MET datasets. The package offers flexibility by allowing to specify the sets of predictors to be used in predictions, and different methods to process genomic information to model genetic effects.</p>
    <p>To validate the predictive performance of the models, different CV schemes are covered by the package, that aim at addressing concrete plant breeding prediction problems with multi-environment field experiments. We borrow the same terminology as in previous related studies (see <xref rid="jkac226-B6" ref-type="bibr">Burgueño <italic toggle="yes">et al.</italic> 2012</xref>; <xref rid="jkac226-B23" ref-type="bibr">Jarquín <italic toggle="yes">et al.</italic> 2014</xref>, <xref rid="jkac226-B24" ref-type="bibr">2017</xref>), as follows: (1) CV1: predicting the performance of newly developed genotypes (never tested in any of the environments included in the MET); (2) CV2: predicting the performance of genotypes that have been tested in some environments but not in others (also referred to as field sparse testing); (3) CV0: predicting the performance of genotypes in new environments, i.e. the environment has not been tested; and (4) CV00: predicting the performance of newly developed genotypes in new environments, i.e. both environment and genotypes have not been observed in the training set. For CV0 and CV00, four configurations are implemented: leave-one-environment-out, leave-one-site-out, leave-one-year-out, and forward prediction.</p>
  </sec>
  <sec sec-type="methods">
    <title>Methods</title>
    <sec>
      <title>Installation and dependencies</title>
      <p>Using the devtools package (<xref rid="jkac226-B42" ref-type="bibr">Wickham <italic toggle="yes">et al.</italic> 2021</xref>), learnMET can be easily installed from GitHub and loaded (<xref rid="jkac226-BOX1" ref-type="boxed-text">Box 1</xref>).
<boxed-text id="jkac226-BOX1" position="float"><label>Box 1.</label><caption><p>Install learnMET</p></caption><p>&gt; devtools::install_github(“cjubin/learnMET”)</p><p>&gt; library(learnMET)</p></boxed-text>Dependencies are automatically installed or updated when executing the command above.</p>
    </sec>
    <sec>
      <title>Real multi-environment trial datasets</title>
      <p>Three toy datasets are included with the learnMET package to illustrate how input data should be provided by the user and how the different functionalities of the package can be utilized.</p>
      <sec>
        <title>Rice datasets</title>
        <p>The datasets were obtained from the INIA’s Rice Breeding Program (Uruguay) and were used in previous studies (<xref rid="jkac226-B32" ref-type="bibr">Monteverde <italic toggle="yes">et al.</italic> 2018</xref>, <xref rid="jkac226-B31" ref-type="bibr">2019</xref>). We used phenotypic data for three traits from two breeding populations of rice (<italic toggle="yes">indica</italic>, composed of 327 elite breeding lines; and <italic toggle="yes">japonica</italic>, composed of 320 elite breeding lines). The two populations were evaluated at a single location (Treinta y Tres, Uruguay) across multiple years (2010–2012 for <italic toggle="yes">indica</italic> and 2009–2013 for <italic toggle="yes">japonica</italic>) and were genotyped using genotyping-by-sequencing (GBS) (<xref rid="jkac226-B31" ref-type="bibr">Monteverde <italic toggle="yes">et al.</italic> 2019</xref>). ECs, characterizing three developmental stages throughout the growing season, were directly available. More details about the dataset are given in <xref rid="jkac226-B32" ref-type="bibr">Monteverde <italic toggle="yes">et al.</italic> (2018)</xref>.</p>
      </sec>
      <sec>
        <title>Maize datasets</title>
        <p>A subset of phenotypic and genotypic datasets, collected and made available by the G2F initiative (<ext-link xlink:href="http://www.genomes2fields.org" ext-link-type="uri">www.genomes2fields.org</ext-link>), were integrated into learnMET. Hybrid genotypic data were computed in silico based on the GBS data from inbred parental lines. For more information about the original datasets, please refer to <xref rid="jkac226-B1" ref-type="bibr">AlKhalifah <italic toggle="yes">et al.</italic> (2018)</xref> and <xref rid="jkac226-B27" ref-type="bibr">McFarland <italic toggle="yes">et al.</italic> (2020)</xref>. In total, phenotypic data, collected from 22 environments covering 4 years (2014–2017) and 6 different locations in American states and Canadian provinces, are included in the package.</p>
      </sec>
    </sec>
    <sec>
      <title>Running learnMET</title>
      <p>learnMET can be implemented as a three-step pipeline. These are described next.</p>
      <sec>
        <title>Step 1: specifying input data and processing parameters</title>
        <p>The first function in the learnMET pipeline is <italic toggle="yes">create_METData()</italic> (<xref rid="jkac226-BOX2" ref-type="boxed-text">Box 2</xref>). The user must provide genotypic and phenotypic data, as well as basic information about the field experiments (e.g. longitude, latitude, planting, and harvest date). Missing genotypic data should be imputed beforehand. Climate covariables can be directly provided as day-interval-aggregated variables, using the argument <italic toggle="yes">climate_variables</italic>. Alternatively, in order to compute weather-based covariables, based on daily weather data, the user can set the <italic toggle="yes">compute_climatic_ECs</italic> argument to TRUE, and two possibilities are given. The first one is to provide raw daily weather data (with the <italic toggle="yes">raw_weather_data</italic> argument), which will undergo a quality control with the generation of an output file with flagged values. The second possibility, if the user does not have weather data available from measurements (e.g. from an in-field weather station), is the retrieval of daily weather records from the NASA’s Prediction of Worldwide Energy Resources (NASA POWER) database (<ext-link xlink:href="https://power.larc.nasa.gov/" ext-link-type="uri">https://power.larc.nasa.gov/</ext-link>), using the package nasapower (<xref rid="jkac226-B39" ref-type="bibr">Sparks 2018</xref>). Spatiotemporal information contained in the <italic toggle="yes">info_environments</italic> argument is required. Note that the function also checks which environments are characterized by in-field weather data in the <italic toggle="yes">raw_weather_data</italic> argument, in order to retrieve satellite-based weather data for the remaining environments without in-field weather stations. An overview of the pipeline is provided in <xref rid="jkac226-F1" ref-type="fig">Fig. 1</xref>.</p>
        <fig position="float" id="jkac226-F1">
          <label>Fig. 1.</label>
          <caption>
            <p>Overview of the pipeline regarding integration of weather data using the function <italic toggle="yes">create_METData()</italic> within the learnMET package. The blue circle signals the first step of the process, when the function is initially called. The blue boxes indicate how the arguments of the function should be given, according to the type of datasets available to the user. The green boxes indicate a task which is run in the pipeline via internal functions of the package. The red circle signals the final step, when the METData object is created and contains environmental covariates. Details on the quality control tests implemented on daily weather data are provided at <ext-link xlink:href="https://cjubin.github.io/learnMET/reference/qc_raw_weather_data.html" ext-link-type="uri">https://cjubin.github.io/learnMET/reference/qc_raw_weather_data.html</ext-link>, and on the methods to build ECs based on aggregation of daily data at <ext-link xlink:href="https://cjubin.github.io/learnMET/reference/get_ECs.html" ext-link-type="uri">https://cjubin.github.io/learnMET/reference/get_ECs.html</ext-link>.</p>
          </caption>
          <graphic xlink:href="jkac226f1" position="float"/>
        </fig>
        <p>Some covariates are additionally computed, based on the daily weather data, such as vapor pressure deficit or the reference evapotranspiration using the Penman-Monteith (FAO-56) equation. The aggregation of daily information into day-interval-based values is also carried out within this function. Four methods are available and should be specified with the argument <italic toggle="yes">method_ECs_intervals</italic>: (1) default: use of a definite number of intervals across all environments (i.e. the window length varies according to the duration of the growing season); (2) use of day-windows of fixed length (i.e. each window spans a given number of days, which remains identical across environments), that can be adjusted by the user; (3) use of specific day intervals according to each environment provided by the user, which should correspond to observed or assumed relevant phenological intervals; and (4) based on the estimated crop growth stage within each environment using accumulated growing degree-days in degrees Celsius.</p>
        <p>Besides weather-based information, soil characterization for each environment can also be provided given the <italic toggle="yes">soil_variables</italic> argument. The output of <italic toggle="yes">create_METData()</italic> is a list object of class <italic toggle="yes">METData</italic>, required as input for all other functionalities of the package.
<boxed-text id="jkac226-BOX2" position="float"><label>Box 2.</label><caption><p>Integration of input data in a METData list object</p></caption><p><bold><italic toggle="yes">Case 1</italic>: ECs directly provided by the user</bold></p><p>&gt; library(learnMET)</p><p>&gt; data(geno_indica)</p><p>&gt; data(map_indica)</p><p>&gt; data(pheno_indica)</p><p>&gt; data(info_environments_indica)</p><p>&gt; data(env_data_indica)</p><p>&gt; METdata_indica &lt;- create_METData(</p><p>geno = geno_indica,</p><p>map = map_indica,</p><p>pheno = pheno_indica,</p><p>climate_variables = climate_variables_indica,</p><p>info_environments = info_environments_indica,</p><p>compute_climatic_ECs = FALSE,</p><p>path_to_save =“/learnMET_analyses/indica”)</p><p><bold><italic toggle="yes">Case 2</italic>: daily climate data automatically retrieved and ECs calculated via the package</bold></p><p>&gt; data(geno_G2F)</p><p>&gt; data(pheno_G2F)</p><p>&gt; data(map_G2F)</p><p>&gt; data(info_environments_G2F)</p><p>&gt; data(soil_G2F)</p><p>&gt; METdata_g2f &lt;- create_METData(</p><p>geno = geno_G2F,</p><p>pheno = pheno_G2F,</p><p>map = map_G2F,</p><p>climate_variables = NULL,</p><p>raw_weather_data = NULL,</p><p>compute_climatic_ECs = TRUE,</p><p>info_environments = info_environments_G2F,</p><p>soil_variables = soil_G2F,</p><p>path_to_save =“/learnMET_analyses/G2F”)</p><p>Note: code example to use in-field daily weather data provided at <ext-link xlink:href="https://cjubin.github.io/learnMET/articles/vignette_getweatherdata.html" ext-link-type="uri">https://cjubin.github.io/learnMET/articles/vignette_getweatherdata.html</ext-link></p></boxed-text></p>
      </sec>
      <sec>
        <title>Machine learning-based models implemented</title>
        <p>Different machine learning-based regression methods are provided as S3 classes in an object-oriented programming style. These methods are called within the pipeline of the <italic toggle="yes">predict_trait_MET_cv()</italic> function, that is presented in the following section. In particular, the XGBoost gradient boosting library (<xref rid="jkac226-B7" ref-type="bibr">Chen and Guestrin 2016</xref>), the Random Forest algorithm (<xref rid="jkac226-B5" ref-type="bibr">Breiman 2001</xref>), stacked ensemble models with Lasso regularization as meta-learners (<xref rid="jkac226-B40" ref-type="bibr">Van der Laan <italic toggle="yes">et al.</italic> 2007</xref>), and multilayer perceptrons (MLP) using Keras (<xref rid="jkac226-B8" ref-type="bibr">Chollet <italic toggle="yes">et al.</italic> 2015</xref>) are implemented as prediction methods. In this section, we briefly present how these machine learning algorithms work.</p>
        <p>Gradient-boosted decision trees (GBDT) can be seen as an additive regression model, where the final model is an ensemble of weak learners (i.e. a regression tree in this case), in which each base learner is fitted in a forward sequential manner (<xref rid="jkac226-B18" ref-type="bibr">Friedman 2001</xref>). Considering a certain loss function (e.g. mean-squared error for regression), a new tree is fitted to the residuals of the prior model (i.e. an ensemble of trees) to minimize this loss function. Then, the previous model is subsequently updated with the current model. From this definition, it becomes clear that GBDT and Random Forest models strongly differ from each other, since for GBDT, trees are built conditional on past trees, and the trees contribute unequally to the final model (<xref rid="jkac226-B25" ref-type="bibr">Kuhn <italic toggle="yes">et al.</italic> 2013</xref>).</p>
        <p>In contrast, in Random Forest algorithms, trees are created independently from each other, and results from each tree are only combined at the end of the process. The concept of GBDT was originally developed by <xref rid="jkac226-B18" ref-type="bibr">Friedman (2001)</xref>. In <italic toggle="yes">learnMET</italic>, a set of prediction models, denoted <italic toggle="yes">xgb_reg</italic> and <italic toggle="yes">rf_reg</italic>, is proposed that use the XGBoost algorithm or the Random Forest algorithm, respectively, with different input variables.</p>
        <p>An MLP consists of one input layer, one or more hidden layers, and one output layer. Each layer, with the exception of the final output layer, includes a bias neuron (i.e. a constant value that acts like the intercept in a linear equation and is used to adjust the output) and is fully connected to the next layer. Here, the first hidden layer receives the marker genotypes and the ECs as input, computes a weighted linear summation of these inputs (i.e. <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>⊺</mml:mo></mml:msup><mml:mo>·</mml:mo><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">X</italic> represent the input features, <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mo>⊺</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> the vector of weights, and <italic toggle="yes">b</italic> the bias), and transforms the latter with a nonlinear activation function <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, yielding the output of the given neuron. In the next hidden layers, each neuron (also named node) in one layer connects with a given weight to each neuron in the consecutive layer. The last hidden layer is generally connected with a linear function to the output layer that consists of a single node. In MLP, learning is done via backpropagation: the network makes a prediction for each training instance, calculates the error associated with this prediction, estimates the error contribution from each connection at each hidden layer by iterating backward from the last layer (reverse pass), and finally changes the connection weights to decrease this error, usually using gradient descent step (<xref rid="jkac226-B19" ref-type="bibr">Géron 2019</xref>). For more details about deep learning methods in genomic prediction, we refer to the review written by <xref rid="jkac226-B34" ref-type="bibr">Pérez-Enciso and Zingaretti (2019)</xref>. In <italic toggle="yes">learnMET</italic>, a set of prediction models named <italic toggle="yes">DL_reg</italic>, are proposed that apply MLP models with different input variables.</p>
        <p>Stacked models can be understood as an ensemble method that exploits the capabilities of many well-working models (called base learners) on a classification or regression task. The theoretical background of this method was originally proposed by <xref rid="jkac226-B4" ref-type="bibr">Breiman (1996)</xref>, and further developed by <xref rid="jkac226-B40" ref-type="bibr">Van der Laan <italic toggle="yes">et al.</italic> (2007)</xref>. In the first step, different individual base learners are fitted to the same training set resamples (typically generated via CV), and potentially using different sets of predictor variables or different hyperparameter settings. Then, the predictions of the base learners are used as input to predict the output by fitting a regularization method, such as Lasso, on the cross-validated predictions. Hence, the final model has learned how to combine the first-level predictions of the base learners, and this stacked ensemble is expected to achieve similar or better results than any of the base learners (<xref rid="jkac226-B40" ref-type="bibr">Van der Laan <italic toggle="yes">et al.</italic> 2007</xref>). This implies also that some weak learners, trained in the first stage, are generally excluded by variable selection from the resulting ensemble model if their predictions are highly correlated with other models, or irrelevant for predicting the trait of interest. In <italic toggle="yes">learnMET</italic>, prediction models named <italic toggle="yes">stacking_reg</italic> apply stacked ensemble models with different base learners and input variables. For instance, <italic toggle="yes">stacking_reg_3</italic> combines a support vector machine regression model fitted to the ECs, an elastic net model fitted to the SNPs data, and a XGBoost model using as features the 40 genomic-based PCs and the ECs. The stacked model was designed to embrace individual learners as diverse as possible, in order to improve the likelihood that the predictions of the different models are different from each other, and that the meta learning algorithm really benefits from combining these first-level predictions. Regularized regression methods are widely used for genomic selection (<xref rid="jkac226-B44" ref-type="bibr">Zou and Hastie 2005</xref>; <xref rid="jkac226-B15" ref-type="bibr">de los Campos <italic toggle="yes">et al.</italic> 2013</xref>), thus our choice to incorporate Elastic Net as an individual learner to estimate the SNPs effects.</p>
      </sec>
      <sec>
        <title>Step 2: model evaluation through cross-validation</title>
        <p>The second function in a typical workflow is <italic toggle="yes">predict_trait_MET_cv()</italic> (<xref rid="jkac226-BOX3" ref-type="boxed-text">Box 3</xref>). The goal of this function is to assess a given prediction method with a specific CV scenario that mimic concrete plant breeding situations.</p>
        <p>When <italic toggle="yes">predict_trait_MET_cv()</italic> is executed, a list of training/test splits is constructed according to the CV scheme chosen by the user. Each training set in each sub-element of this list is processed (e.g. standardization and removal of predictors with null variance, feature extraction based on principal component analysis), and the corresponding test set is processed using the same transformations. Performance metrics are computed on the test set, such as the Pearson correlation between predicted and observed phenotypic values (always calculated within the same environment, regardless of how the test sets are defined according to the different CV schemes), and the root mean square error. Analyses are fully reproducible given that seed and tuned hyperparameters are stored with the output of <italic toggle="yes">predict_trait_MET_cv()</italic>. Note that, if one wants to compare models using the same CV partitions, specifying the seed and modifying the model would be sufficient.</p>
        <p>The function applies a nested CV to obtain an unbiased generalization performance estimate. After splitting the complete dataset using an outer CV partition (based on either CV1, CV2, CV0, or CV00 prediction problems), an inner CV scheme is applied to the outer training dataset for optimization of hyperparameters. Subsequently, the best hyperparameters are selected and used to train the model using all training data. Model performance is then evaluated based on the predictions of the unseen test data using this trained model. This procedure is repeated for each training-test partition of the outer CV assignments. <xref rid="jkac226-T1" ref-type="table">Table 1</xref> shows the different arguments that can be adjusted when executing the CV evaluation.</p>
        <table-wrap position="float" id="jkac226-T1">
          <label>Table 1.</label>
          <caption>
            <p>Description of the main arguments used with the function predict_trait_MET_cv().</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="left" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Function argument</th>
                <th align="center" rowspan="1" colspan="1">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">METData</td>
                <td rowspan="1" colspan="1">An object created by the initial function of the package create_METData().</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">trait</td>
                <td rowspan="1" colspan="1">Name of the trait to predict.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">prediction_method</td>
                <td rowspan="1" colspan="1">String to name the trait to predict.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">lat_lon_included</td>
                <td rowspan="1" colspan="1">Logical to use longitude and latitude as predictor variables. FALSE by default.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">yr_included</td>
                <td rowspan="1" colspan="1">Logical to use yr effect as dummy variable. FALSE by default.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">cv_type</td>
                <td rowspan="1" colspan="1">String indicating the CV scheme to use among “cv0” (prediction of genotypes in new environments), “cv00” (prediction of new genotypes in new environments), “cv1” (prediction of new genotypes), or “cv2” (prediction of incomplete field trials). Default is “cv0.”</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">cv0_type</td>
                <td rowspan="1" colspan="1">String indicating the type of cv0 scenario, among “leave-one-environment-out”, “leave-one-site-out”, “leave-one-yr-out”, and “forward-prediction.” Default is “leave-one-environment-out.”</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">nb_folds_cv1</td>
                <td rowspan="1" colspan="1">Integer for the number of folds to use in the cv1 scheme, if selected.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">repeats_cv1</td>
                <td rowspan="1" colspan="1">Integer for the number of repeats in the cv1 scheme, if selected.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">nb_folds_cv2</td>
                <td rowspan="1" colspan="1">Integer for the number of folds to use in the cv2 scheme, if selected.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">repeats_cv2</td>
                <td rowspan="1" colspan="1">Integer for the number of repeats in the cv2 scheme, if selected.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">include_env_predictors</td>
                <td rowspan="1" colspan="1">Logical to indicate if ECs should be used in predictions. TRUE by default.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">list_env_predictors</td>
                <td rowspan="1" colspan="1">Vector of character strings with the names of the environmental predictors which should be used in predictions. NULL by default, which means that all environmental predictor variables are used.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">seed</td>
                <td rowspan="1" colspan="1">Integer with the seed value. Default is NULL, which implies that a random seed is generated, used in the other stages of the pipeline, and given as output for reproducibility.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">save_processing</td>
                <td rowspan="1" colspan="1">Logical to save the processing steps used to build the model in a RDS file. Default is FALSE.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">path_folder</td>
                <td rowspan="1" colspan="1">String to indicate the full path where the RDS file with results and plots generated during the analysis should be saved.</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">num_pcs</td>
                <td rowspan="1" colspan="1">Optional argument. Integer to indicate the number of PCs to derive from the genotype matrix or from the genomic relationship matrix (encouraged to speed up CV with large datasets).</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">save_model</td>
                <td rowspan="1" colspan="1">Logical indicating whether the fitted model for each training-test partition should be saved. Default is FALSE.</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
        <p>Note that the classes we developed for preprocessing data and for fitting machine learning-based methods use functions from the tidymodels collection of R packages for machine learning (<xref rid="jkac226-B26" ref-type="bibr">Kuhn and Wickham 2020</xref>), such as Bayesian optimization to tune hyperparameters (function <italic toggle="yes">tune_bayes()</italic>) or the package <italic toggle="yes">stacks</italic>. For models based on XGBoost, the number of boosting iterations, the learning rate, and the depth of trees represent important hyperparameters that are automatically tuned. Ranges of hyperparameter values are predefined based on expert knowledge. Bayesian optimization techniques use a surrogate model of the objective function in order to select better hyperparameter combinations based on past results (<xref rid="jkac226-B38" ref-type="bibr">Shahriari <italic toggle="yes">et al.</italic> 2016</xref>). As more combinations are assessed, more data become available from which this surrogate model can learn to sample new combinations from the hyperparameter space that are more likely to yield an improvement. This technique allows a reduction of the number of model settings tested during the hyperparameter tuning.</p>
        <p>
          <boxed-text id="jkac226-BOX3" position="float">
            <label>Box 3.</label>
            <caption>
              <p>Evaluation of a prediction method using a CV scheme (i.e. METData object with phenotypic data)</p>
            </caption>
            <p>&gt; res_cv0_indica &lt;- predict_trait_MET_cv(</p>
            <p>METData = METdata_indica,</p>
            <p>trait =“GC”,</p>
            <p>prediction_method =“xgb_reg_1”,</p>
            <p>cv_type =“cv0”,</p>
            <p>cv0_type =“leave-one-year-out”,</p>
            <p>seed = 100,</p>
            <p>path_folder =“/project1/indica_cv_res/cv0”)</p>
          </boxed-text>
        </p>
      </sec>
      <sec>
        <title>Extracting evaluation metrics from the output</title>
        <p>Once a model has been evaluated with a CV scheme, various results can be extracted from the returned object, as shown in <xref rid="jkac226-BOX4" ref-type="boxed-text">Box 4</xref>, and plots for visualization of results are also saved in the <italic toggle="yes">path_folder</italic>.</p>
        <boxed-text id="jkac226-BOX4" position="float">
          <label>Box 4.</label>
          <caption>
            <p>Extraction of results from returned object of class <italic toggle="yes">met_cv</italic></p>
          </caption>
          <p># Extract predictions for each test set in the CV scheme:</p>
          <p>&gt; pred_2010 &lt;- res_cv0_indica$list_results_cv[[1]]$prediction_df</p>
          <p>&gt; pred_2011 &lt;- res_cv0_indica$list_results_cv[[2]]$prediction_df</p>
          <p>&gt; pred_2012 &lt;- res_cv0_indica$list_results_cv[[3]]$prediction_df</p>
          <p># The length of the list_results_cv sub-element is equal to the number of train/test sets partitions.</p>
          <p># Extract Pearson correlation between predicted and observed values for 2010:</p>
          <p>&gt; cor_2010 &lt;- res_cv0_indica$list_results_cv[[1]]$cor_pred_obs</p>
          <p># Extract root mean square error between predicted and observed values for 2011:</p>
          <p>&gt; rmse_2011 &lt;- res_cv0_indica$list_results_cv[[2]]$rmse_pred_obs</p>
          <p># Get the seed used:</p>
          <p>&gt; seed &lt;- res_cv0_indica$seed_used</p>
        </boxed-text>
      </sec>
      <sec>
        <title>Step 3: prediction of performance for a new test set</title>
        <p>The third module in the package aims at implementing predictions for unobserved configurations of genotypic and environmental predictors using the function <italic toggle="yes">predict_trait_MET()</italic> (<xref rid="jkac226-BOX5" ref-type="boxed-text">Box 5</xref>). The user needs to provide a table of genotype IDs (e.g. name of new varieties) with their growing environments (i.e. year and location) using the argument <italic toggle="yes">pheno</italic> in the function <italic toggle="yes">create_METData()</italic>. Genotypic data of the selection candidates to test within this test set should all be provided using the <italic toggle="yes">geno</italic> argument. Regarding characterization of new environments, the user can either provide a table of environments, with longitude, latitude, and growing season dates, or can directly provide a table of ECs that should be consistent with the ECs provided for the training set. Environmental variables for the unobserved test set should be provided or computed with the same aggregation method (i.e. same <italic toggle="yes">method_ECs_intervals</italic>) as for the training set. To build an appropriate model with learning parameters, able to generalize well on new data, a hyperparameter optimization with CV is conducted on the entire training dataset when using the function <italic toggle="yes">predict_trait_MET()</italic>.</p>
        <p>This function can potentially be applied to harness historical weather data and to obtain predictions across multiple years at a set of given locations (<xref rid="jkac226-B16" ref-type="bibr">de Los Campos <italic toggle="yes">et al.</italic> 2020</xref>), or to conjecture about the best selection candidates to assess in field trials at specific locations. However, we emphasize the importance of both environmental and genetic similarity between training and test sets. If the selection candidates within the test set are not strongly genetically related to the genotypes included in the training set, or if the climatic conditions experienced in the test set differ too much from the feature space covered within the training set, the prediction results might not be trustworthy for decision making.</p>
        <p>The function <italic toggle="yes">analysis_predictions_best_genotypes()</italic> takes directly the output of <italic toggle="yes">predict_trait_MET()</italic> and can be used to visualize the predicted yield of the best performing genotypes at each of the locations across years included in the test set.</p>
        <p>
          <boxed-text id="jkac226-BOX5" position="float">
            <label>Box 5.</label>
            <caption>
              <p>Prediction of new observations using a training set and a test set (i.e. phenotypic data not required)</p>
            </caption>
            <p># Create a training set composed of years 2014, 2015 and 2016:</p>
            <p>&gt; METdata_G2F_training &lt;-</p>
            <p>create_METData(</p>
            <p>geno = geno_G2F,</p>
            <p>pheno = pheno_G2F[pheno_G2F$year %in% c(2014,2015,2016),],</p>
            <p>map = map_G2F,</p>
            <p>climate_variables = NULL,</p>
            <p>compute_climatic_ECs = TRUE,</p>
            <p>et0 = T, # Possibility to calculate reference evapotranspiration with the package (if TRUE, elevation data should be preferably added as a column in info_environments)</p>
            <p>info_environments = info_environments_G2F[info_environments_G2F$year %in% c(2014,2015,2016),],</p>
            <p>soil_variables = soil_G2F[soil_G2F$year %in% c(2014,2015,2016),],</p>
            <p>path_to_save =“/project1/g2f_trainingset”) # path where daily weather data and plots are saved</p>
            <p># Create a prediction set (same default method to compute ECs as above):</p>
            <p>&gt; METdata_G2F_new &lt;-</p>
            <p>create_METData(</p>
            <p>geno = geno_G2F,</p>
            <p>pheno = as.data.frame(pheno_G2F[pheno_G2F$year %in% 2017 , ] % &gt;% dplyr::select(-pltht, -yld_bu_ac, -earht)),</p>
            <p>map = map_G2F,</p>
            <p>et0 = T,</p>
            <p>climate_variables = NULL,</p>
            <p>compute_climatic_ECs = TRUE,</p>
            <p>info_environments = info_environments_G2F[info_environments_G2F$year %in% 2017 , ],</p>
            <p>soil_variables = soil_G2F[soil_G2F$year %in% 2017 , ],</p>
            <p>path_to_save =“/project1/g2f_testset”,</p>
            <p>as_test_set = T) # in order to provide only predictor variables (no phenotypic data for the test set available) in <italic toggle="yes">pheno</italic> argument.</p>
            <p># Fitting the model to the training set and predicting the test set</p>
            <p>&gt; results_list &lt;- predict_trait_MET(</p>
            <p>METData_training = METdata_G2F_training,</p>
            <p>METData_new = METdata_G2F_new,</p>
            <p>trait =“yld_bu_ac”,</p>
            <p>prediction_method =“xgb_reg_1”,</p>
            <p>use_selected_markers = F,</p>
            <p>save_model = TRUE,</p>
            <p># save_model set to TRUE in order to retrieve subsequently variable importance</p>
            <p>lat_lon_included = F,</p>
            <p>year_included = F,</p>
            <p>num_pcs = 200,</p>
            <p>include_env_predictors = T,</p>
            <p>seed = 100,</p>
            <p>path_folder =“/project1/g2f_results_year_2017”</p>
            <p>)</p>
          </boxed-text>
        </p>
      </sec>
      <sec>
        <title>Interpreting ML models</title>
        <p>Compared to parametric models, ML techniques are often considered as black-boxes implementations that complicate the task of understanding the importance of different factors (genetic, environmental, management, or their respective interactions) driving the phenotypic response. Therefore, various methods have recently been proposed to aid the understanding and interpretation of the output of ML models. Among these techniques, some are model-specific techniques (<xref rid="jkac226-B29" ref-type="bibr">Molnar 2022</xref>), in the sense that they are only appropriate for certain types of algorithms. For instance, the Gini importance or the gain-based feature importance measures can only be applied for tree-based methods (e.g. decision trees, Random Forests, gradient-boosted trees), since it calculates how much a predictor variable can reduce the sum of squared errors in the child nodes, compared to the parent node, across all splits for which this given predictor was used. Feature importances are in this case scaled between 0 and 100.</p>
        <p>Other model-agnostic interpretation techniques have been developed, that provide the advantage of being independent from the original machine learning algorithm applied, thereby allowing straightforward comparisons across models (<xref rid="jkac226-B29" ref-type="bibr">Molnar 2022</xref>). After shuffling the values of a given predictor variable, the value of the loss function (e.g. root mean square error in regression problems), estimated using the predictions of the shuffled data and the observed values, can be used to obtain an estimate of the permutation-based variable importance. <xref rid="jkac226-B17" ref-type="bibr">Fisher <italic toggle="yes">et al.</italic> (2019)</xref> formally defined the permutation importance for a variable <italic toggle="yes">j</italic> as follows: <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mtext>diff</mml:mtext><mml:mi>j</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mover><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mtext>permuted</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mover><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mtext>original</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula>, where <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mi>L</mml:mi><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mover><mml:mi>f</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>X</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> is the loss function evaluating the performance of the model, <italic toggle="yes">X</italic><sub>original</sub> is the original matrix of predictor variables, and <italic toggle="yes">X</italic><sub>permuted</sub> is the matrix obtained after permuting the variable <italic toggle="yes">j</italic> in <italic toggle="yes">X</italic><sub>original</sub>. The reason behind this approach is that, if a predictor contributes strongly to a model’s predictions, shuffling its values will result in increased error estimates. On the other hand, if the variable is irrelevant for the fitted model, it should not affect the prediction error. It is recommended to repeat the permutation process to obtain a more reliable average estimate of the variable importance (<xref rid="jkac226-B17" ref-type="bibr">Fisher <italic toggle="yes">et al.</italic> 2019</xref>; <xref rid="jkac226-B29" ref-type="bibr">Molnar 2022</xref>). Another interesting aspect of permutation-based variable importance is the possibility to calculate it using either the training or the unused test set. Computing variable importance using unseen data is useful to evaluate whether the explanatory variables, identified as relevant for prediction during model training, are truly important to deliver accurate predictions, and whether the model does not overfit. However, in the latter case, one needs to ensure that the training and test set are sufficiently related. New data might behave very differently from the data used for training without implying that the trained model is fundamentally wrong. The function <italic toggle="yes">variable_importance_split()</italic> enables retrieving variable importance, either with a model-specific method (via the package vip proposed by <xref rid="jkac226-B21" ref-type="bibr">Greenwell <italic toggle="yes">et al.</italic> 2020</xref>), when available, or based on a permutation-based method (argument <italic toggle="yes">type</italic>, see <xref rid="jkac226-BOX6" ref-type="boxed-text">Box 6</xref>), and the calculation is made by default using the training set, but can be achieved for the test set by setting the argument <italic toggle="yes">unseen_data</italic> to TRUE.
<boxed-text id="jkac226-BOX6" position="float"><label>Box 6.</label><caption><p>Retrieving variable importance using the fitted model and the training data</p></caption><p>&gt; fitted_split &lt;- results_list$list_results[[1]]</p><p># Model-specific: variable importance based on the gain as importance metric from the XGBoost model (via vip package)</p><p>&gt; variable_importance &lt;- variable_importance_split(</p><p>object = fitted_split,</p><p>path_plot =“/project1/variable_imp_trset”,</p><p>type =“model_specific”)</p><p># Model-agnostic: variable importance based on 10 permutations</p><p>&gt; variable_importance &lt;- variable_importance_split(</p><p>object = fitted_split,</p><p>path_plot =“/project1/variable_imp_trset”,</p><p>type =“model_agnostic”,</p><p>permutations = 10)</p><p># Model-agnostic: accumulated local effects plot</p><p>&gt; ALE_plot_split(fitted_split,</p><p>path_plot =“/project1/ale_plots,”</p><p>variable =”freq_P_sup10_2”)</p></boxed-text>Accumulated local effects (ALE) plots, also model agnostic, allow to examine the influence of a given predictor variable on the model prediction, conditional on the predictor value (<xref rid="jkac226-B2" ref-type="bibr">Apley and Zhu 2020</xref>). Compared to partial dependence (PD) plots, they provide the advantage of addressing the bias that emerges when features are correlated. While predictions are computed over the marginal distribution of predictor variables in the case of PD plots (i.e. meaning that predictions of unrealistic instances are considered), ALE plots offer a solution to this issue by considering the conditional distribution, thus avoiding to use predictions of unrealistic training observations. To build an ALE plot, the range of the explanatory variable is first split into equally sized small windows, such as quantiles. For each window, the ALE method only considers observations that show for this feature a value falling within the interval. Then, it computes model predictions for the upper limit and for the lower limit of the interval for these data instances, and calculates the difference in predictions. The changes of predictions are averaged within each interval, which allows to block the impact of other features. These average effects are then accumulated across all intervals and centered at 0. The function <italic toggle="yes">ALE_plot_split()</italic> yields the ALE plot for a given predictor variable. An example is provided in <xref rid="jkac226-BOX6" ref-type="boxed-text">Box 6</xref>.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>Results and discussion</title>
    <p>To illustrate the use of learnMET with METs datasets, we provide here two example pipelines, both of which are available in the official package documentation. The first one demonstrates an implementation that requires no user-provided weather data, while the second pipeline shows prediction results obtained based on user-provided environmental data.</p>
    <sec>
      <title>Retrieving meteorological data from NASA POWER database for each environment</title>
      <p>When running the commands for step 1 (<xref rid="jkac226-BOX1" ref-type="boxed-text">Box 1</xref>, Case 2) on the maize dataset, a set of weather-based variables (see documentation of the package) is automatically calculated using weather data retrieved from the NASA POWER database. By default, the method used to compute ECs uses a fixed number of day-windows (10) that span the complete growing season within each environment. This optional argument can be modified via the argument <italic toggle="yes">method_ECs_intervals</italic> (detailed information about the different methods can be found at <ext-link xlink:href="https://cjubin.github.io/learnMET/reference/get_ECs.html" ext-link-type="uri">https://cjubin.github.io/learnMET/reference/get_ECs.html</ext-link>). The function <italic toggle="yes">summary()</italic> provides a quick overview of the elements stored and collected in this first step of the pipeline (<xref rid="jkac226-BOX7" ref-type="boxed-text">Box 7</xref>).
<boxed-text id="jkac226-BOX7" position="float"><label>Box 7.</label><caption><p>Summary method for class METData</p></caption><p>&gt; summary(METdata_g2f)</p></boxed-text>Clustering analyses, that can help to identify groups of environments with similar climatic conditions and to identify outliers, were generated based on (a) only climate data; (b) only soil data (if available); and (c) all environmental variables together, for a range of values for <italic toggle="yes">K</italic> = 2 to 10 clusters (<xref rid="jkac226-F2" ref-type="fig">Fig. 2</xref>).</p>
      <fig position="float" id="jkac226-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Output results from the <italic toggle="yes">create_METData()</italic> function. a) Cluster analysis using <italic toggle="yes">K</italic>-means algorithm (<italic toggle="yes">K</italic> = 4) to identify groups of similar environments based on environmental data. b) Total within-cluster sum of squares as a function of the number of clusters. c) Average Silhouette score as a function of the number of clusters. These methods can help users decide on the optimal number of clusters. Data used here are a subset of the Genomes to Fields maize dataset (<xref rid="jkac226-B1" ref-type="bibr">AlKhalifah <italic toggle="yes">et al.</italic> 2018</xref>; <xref rid="jkac226-B27" ref-type="bibr">McFarland <italic toggle="yes">et al.</italic> 2020</xref>). Weather data were retrieved from NASA POWER database via the package nasapower <xref rid="jkac226-B39" ref-type="bibr">Sparks (2018)</xref>. Plots are saved in the directory provided in the <italic toggle="yes">path_to_save</italic> argument.</p>
        </caption>
        <graphic xlink:href="jkac226f2" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Benchmarking two prediction methods from <italic toggle="yes">learnMET</italic> and a linear reaction norm model</title>
      <p>Phenotypic traits were predicted by the reaction norm model proposed by <xref rid="jkac226-B23" ref-type="bibr">Jarquín <italic toggle="yes">et al.</italic> (2014)</xref>, thereafter denoted as G-W-G × W, that account for the random linear effects of the molecular markers (G), of the environmental covariates (W), and of the interaction term (G × W), under the following assumptions:
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>μ</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo>ε</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
with <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">g</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">G</mml:mi><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">G</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mi mathvariant="bold">X</mml:mi><mml:mo>′</mml:mo><mml:mo>/</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> (with <italic toggle="yes">p</italic> being the number of SNPs and <italic toggle="yes">X</italic> the scaled and centered marker matrix), <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">W</mml:mi><mml:mo>′</mml:mo><mml:mo>/</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:math></inline-formula> (with <italic toggle="yes">q</italic> being the number of ECs and <italic toggle="yes">W</italic> the scaled and centered matrix that contains the ECs), <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mi mathvariant="bold">g</mml:mi><mml:mi mathvariant="bold">w</mml:mi><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mi mathvariant="bold">Z</mml:mi><mml:mi mathvariant="bold">g</mml:mi></mml:msub><mml:mi mathvariant="bold">G</mml:mi><mml:msubsup><mml:mi mathvariant="bold">Z</mml:mi><mml:mi mathvariant="bold">g</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>]</mml:mo><mml:mo>∘</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:msubsup><mml:mo>σ</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mi>w</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>) where <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mo>°</mml:mo></mml:math></inline-formula> denotes the Hadamard product (cell by cell product), <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ε</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mover><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>IID</mml:mtext></mml:mrow></mml:mover></mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mo>σ</mml:mo></mml:mrow><mml:mo>ε</mml:mo><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p>
      <p>For additional details about the benchmark model, we refer to the original publication of <xref rid="jkac226-B23" ref-type="bibr">Jarquín <italic toggle="yes">et al.</italic> (2014)</xref>. We implemented this model using BGLR (<xref rid="jkac226-B33" ref-type="bibr">Pérez and de Los Campos 2014</xref>), for which the MCMC algorithm was run for 20,000 iterations and the first 2,000 iterations were removed as burn-in using a thinning equal to 5.</p>
      <p>Two prediction models proposed in <italic toggle="yes">learnMET</italic> were tested: (1) <italic toggle="yes">xgb_reg_1</italic>, which is an XGBoost model that uses a certain number of principal components (PCs) derived from the marker matrix and ECs, as features and (2) <italic toggle="yes">stacking_reg_3</italic>. Although computationally more expensive than parametric methods, we paid attention to reasonable computational time (e.g. maximum of 13.3 hours to fit <italic toggle="yes">stacking_reg_3</italic> model to <italic toggle="yes">n</italic> = 4,587 training instances with 10 CPUs).</p>
      <p>We conducted a forward CV0 CV scheme, meaning that future years were predicted when using only past years as the training set. For the rice datasets, at least two years of data were used to introduce variation in the EC matrix characterizing the training set (only one location was tested each year). Year, location or year-location effects were not incorporated in any of the linear and machine learning models, because we focused our evaluation on how the different models could efficiently capture the effects of SNPs and ECs, and of SNP × EC interaction effects.</p>
      <p>Results from the benchmarking approach are presented in <xref rid="jkac226-F3" ref-type="fig">Figs. 3</xref> and <xref rid="jkac226-F4" ref-type="fig">4</xref>. We have observed that the machine learning models are competitive with the linear reaction norm approach and tend to outperform it, albeit not consistently, as the training set size increases. Applied to small training set sizes, sophisticated prediction models are likely not able to capture informative patterns related to SNP × EC interactions, and linear models perform better. Similarly, the root mean square error was generally reduced with the machine learning methods as the training set increased (<xref rid="jkac226-F4" ref-type="fig">Fig. 4</xref>). Machine learning also performed better with the G2F data that integrated multiple locations per year and was therefore larger and probably more relevant to learn G × E patterns than with the rice dataset. Therefore, we encourage users to first evaluate whether their datasets are sufficiently large to leverage the potential of the advanced techniques proposed in this package and whether the latter provide satisfying predictive abilities in CV settings.</p>
      <fig position="float" id="jkac226-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Correlations between predicted and observed values for a forward prediction scenario using two machine learning models and a linear reaction norm approach. a) Three traits predicted for two rice populations. Each year is predicted based on at least two past years of phenotypic data (one single location). b) Grain yield predicted for the G2F dataset. GC (rice data), percentage of chalky kernels; GY (rice data), grain yield (kg/ha); PHR (rice data), percentage of head rice recovery; GY (G2F), bushels per acre.</p>
        </caption>
        <graphic xlink:href="jkac226f3" position="float"/>
      </fig>
      <fig position="float" id="jkac226-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>Root mean square error between predicted and observed values for a forward prediction scenario using two machine learning models and a linear reaction norm approach. a) Three traits predicted for two rice populations. Each year is predicted based on at least two past years of phenotypic data (one single location). b) Grain yield predicted for the G2F dataset. GC (rice data), percentage of chalky kernels; GY (rice data), grain yield (kg/ha); PHR (rice data), percentage of head rice recovery; GY (G2F), bushels per acre.</p>
        </caption>
        <graphic xlink:href="jkac226f4" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>Model interpretation from a gradient-boosted model fitted to the maize dataset</title>
      <p><xref rid="jkac226-F5" ref-type="fig">Figure 5a</xref> illustrates the permutation-based approach on the maize dataset, and <xref rid="jkac226-F5" ref-type="fig">Fig. 5, b</xref> and <xref rid="jkac226-F5" ref-type="fig">c</xref> describe how two environmental variables (sum of photothermal time and frequency of rainfall) influence the average prediction of maize grain yield using ALE plots. We should stress that the size of the dataset employed here is likely too small to make real inferences about the relationship between the predictor variables and the outcome (sharp drops observed at some feature values). Our goal here is essentially to illustrate how these functions can be used to gain insights into a model’s predictions using the package.</p>
      <fig position="float" id="jkac226-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>Model interpretation methods applied on the model fitted to a subset of the G2F dataset from years 2014 to 2016 (17 environments included) with <italic toggle="yes">xgb_reg_1</italic> for the trait grain yield. a) Model-agnostic variable importance using 10 permutations. The top 40 most important predictor variables are displayed, and the table containing results across all permutations for all variables is returned. ALE plots for (b) sum of photothermal time during the 1st day-interval of the growing season, and (c) the frequency of days with an amount of precipitation above 10 mm during the 2nd day-interval of the growing season. Tick marks indicate the unique values observed for the given covariate in the training set.</p>
        </caption>
        <graphic xlink:href="jkac226f5" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>Concluding remarks and future developments</title>
    <p><italic toggle="yes">learnMET</italic> was developed to make the integration of complex datasets, originating from various data sources, user-friendly. The package provides flexibility at various levels: (1) regarding the use of weather data, with the possibility to provide on-site weather station data, or to retrieve external weather data, or a mix of both if on-site data are only partially available; (2) regarding how time intervals for aggregation of daily weather data are defined; (3) regarding the diversity of nonlinear machine learning models proposed; (4) regarding options to provide manually specified subsets of predictor variables (for instance, for environmental features via the argument <italic toggle="yes">list_env_predictors</italic> in <italic toggle="yes">predict_trait_MET_cv()</italic>).</p>
    <p>To allow analyses on larger datasets, future developments of the package should include parallel processing to improve the scalability of the package and to best harness high performance computing resources. Improvements and extensions of stacked models and deep learning models are also intended, as we did not investigate in-depth the network architecture (e.g. number of nodes per layer, type of activation function, type of optimizer), nor other types of deep learning models that might perform better (e.g. convolutional neural networks). Finally, the package could be extended to allow genotype-specific ECs, because the timing of developmental stages differs across genotypes (e.g. due to variability in earliness) and should ideally be taken into account.</p>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgments</title>
    <p>This work used the Scientific Compute Cluster at GWDG, the joint data center of Max Planck Society for the Advancement of Science (MPG) and University of Göttingen. We acknowledge support by the Open Access Publication Funds of the Göttingen University. The authors thank the G2F Consortium for collecting data and making these publicly available. The authors are grateful to Eliana Monteverde for her useful input regarding the rice dataset, and also thank the National Institute of Agricultural Research (INIA-Uruguay) and technical staff from the experimental station from Treinta y Tres (Uruguay) for collecting the data. In this work, data from the NASA POWER database were used. These data were obtained from the NASA Langley Research Center POWER Project funded through the NASA Earth Science Directorate Applied Science Program.</p>
    <sec>
      <title>Funding</title>
      <p>Financial support for CCW was provided by KWS SAAT SE by means of a PhD fellowship. Additional financial support was provided by the University of Göttingen and by the Center for Integrated Breeding Research.</p>
    </sec>
    <sec>
      <title>Conflicts of interest</title>
      <p>None declared.</p>
    </sec>
  </ack>
  <sec sec-type="data-availability">
    <title>Data Availability</title>
    <p>The software is available on GitHub at <ext-link xlink:href="https://github.com/cjubin/learnMET" ext-link-type="uri">https://github.com/cjubin/learnMET</ext-link>. Documentation and vignettes are provided at <ext-link xlink:href="https://cjubin.github.io/learnMET/" ext-link-type="uri">https://cjubin.github.io/learnMET/</ext-link>. All scripts used to obtain the results presented in this article can be found on GitHub at <ext-link xlink:href="https://github.com/cjubin/learnMET/tree/main/scripts\_publication" ext-link-type="uri">https://github.com/cjubin/learnMET/tree/main/scripts\_publication</ext-link>.</p>
  </sec>
  <ref-list id="ref1">
    <title>Literature cited</title>
    <ref id="jkac226-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>AlKhalifah</surname><given-names>N</given-names></string-name>, <string-name><surname>Campbell</surname><given-names>DA</given-names></string-name>, <string-name><surname>Falcon</surname><given-names>CM</given-names></string-name>, <string-name><surname>Gardiner</surname><given-names>JM</given-names></string-name>, <string-name><surname>Miller</surname><given-names>ND</given-names></string-name>, <string-name><surname>Romay</surname><given-names>MC</given-names></string-name>, <string-name><surname>Walls</surname><given-names>R</given-names></string-name>, <string-name><surname>Walton</surname><given-names>R</given-names></string-name>, <string-name><surname>Yeh</surname><given-names>C-T</given-names></string-name>, <string-name><surname>Bohn</surname><given-names>M</given-names></string-name></person-group>, <etal>et al</etal><article-title>Maize genomes to fields: 2014 and 2015 field season genotype, phenotype, environment, and inbred ear image datasets</article-title>. <source>BMC Res Notes</source>. <year>2018</year>;<volume>11</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">29291749</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Apley</surname><given-names>DW</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>J.</given-names></string-name></person-group><article-title>Visualizing the effects of predictor variables in black box supervised learning models</article-title>. <source>J R Stat Soc Series B Stat Methodol</source>. <year>2020</year>;<volume>82</volume>(<issue>4</issue>):<fpage>1059</fpage>–<lpage>1086</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B3">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bernardo</surname><given-names>R.</given-names></string-name></person-group><source>Breeding for Quantitative Traits in Plants</source>, Vol. <volume>1</volume>. <publisher-loc>Woodbury (MN</publisher-loc>): <publisher-name>Stemma Press</publisher-name>; <year>2002</year>.</mixed-citation>
    </ref>
    <ref id="jkac226-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breiman</surname><given-names>L.</given-names></string-name></person-group><article-title>Stacked regressions</article-title>. <source>Mach Learn</source>. <year>1996</year>;<volume>24</volume>(<issue>1</issue>):<fpage>49</fpage>–<lpage>64</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breiman</surname><given-names>L.</given-names></string-name></person-group><article-title>Random forests</article-title>. <source>Mach Learn</source>. <year>2001</year>;<volume>45</volume>(<issue>1</issue>):<fpage>5</fpage>–<lpage>32</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Burgueño</surname><given-names>J</given-names></string-name>, <string-name><surname>de los Campos</surname><given-names>G</given-names></string-name>, <string-name><surname>Weigel</surname><given-names>K</given-names></string-name>, <string-name><surname>Crossa</surname><given-names>J.</given-names></string-name></person-group><article-title>Genomic prediction of breeding values when modeling genotype × environment interaction using pedigree and dense molecular markers</article-title>. <source>Crop Sci</source>. <year>2012</year>;<volume>52</volume>(<issue>2</issue>):<fpage>707</fpage>–<lpage>719</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>T</given-names></string-name>, <string-name><surname>Guestrin</surname><given-names>C.</given-names></string-name></person-group> Xgboost: a scalable tree boosting system. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA. <year>2016</year>. p. <fpage>785</fpage>–<lpage>794</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B8">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Chollet</surname><given-names>F</given-names></string-name></person-group>, <etal>et al</etal><year>2015</year>. Keras. <ext-link xlink:href="https://keras.io" ext-link-type="uri">https://keras.io</ext-link>.</mixed-citation>
    </ref>
    <ref id="jkac226-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Costa-Neto</surname><given-names>G</given-names></string-name>, <string-name><surname>Fritsche-Neto</surname><given-names>R</given-names></string-name>, <string-name><surname>Crossa</surname><given-names>J.</given-names></string-name></person-group><article-title>Nonlinear kernels, dominance, and envirotyping data increase the accuracy of genome-based prediction in multi-environment trials</article-title>. <source>Heredity</source>. <year>2021</year>;<volume>126</volume>(<issue>1</issue>):<fpage>92</fpage>–<lpage>106</lpage>.<pub-id pub-id-type="pmid">32855544</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Costa-Neto</surname><given-names>G</given-names></string-name>, <string-name><surname>Galli</surname><given-names>G</given-names></string-name>, <string-name><surname>Carvalho</surname><given-names>HF</given-names></string-name>, <string-name><surname>Crossa</surname><given-names>J</given-names></string-name>, <string-name><surname>Fritsche-Neto</surname><given-names>R.</given-names></string-name></person-group><article-title>Envrtype: a software to interplay enviromics and quantitative genomics in agriculture</article-title>. <source>G3 (Bethesda)</source>. <year>2021</year>;<volume>11</volume>:<fpage>jkab040</fpage>.<pub-id pub-id-type="pmid">33835165</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Covarrubias-Pazaran</surname><given-names>G.</given-names></string-name></person-group><article-title>Genome-assisted prediction of quantitative traits using the R package sommer</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>(<issue>6</issue>):<fpage>e0156744</fpage>.<pub-id pub-id-type="pmid">27271781</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Crossa</surname><given-names>J</given-names></string-name>, <string-name><surname>Martini</surname><given-names>JWR</given-names></string-name>, <string-name><surname>Gianola</surname><given-names>D</given-names></string-name>, <string-name><surname>Pérez-Rodríguez</surname><given-names>P</given-names></string-name>, <string-name><surname>Jarquin</surname><given-names>D</given-names></string-name>, <string-name><surname>Juliana</surname><given-names>P</given-names></string-name>, <string-name><surname>Montesinos-López</surname><given-names>O</given-names></string-name>, <string-name><surname>Cuevas</surname><given-names>J.</given-names></string-name></person-group><article-title>Deep kernel and deep learning for genome-based prediction of single traits in multienvironment breeding trials</article-title>. <source>Front Genet</source>. <year>2019</year>;<volume>10</volume>:<fpage>1168</fpage>.<pub-id pub-id-type="pmid">31921277</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cuevas</surname><given-names>J</given-names></string-name>, <string-name><surname>Crossa</surname><given-names>J</given-names></string-name>, <string-name><surname>Montesinos-López</surname><given-names>OA</given-names></string-name>, <string-name><surname>Burgueño</surname><given-names>J</given-names></string-name>, <string-name><surname>Pérez-Rodríguez</surname><given-names>P</given-names></string-name>, <string-name><surname>de Los Campos</surname><given-names>G.</given-names></string-name></person-group><article-title>Bayesian genomic prediction with genotype × environment interaction kernel models</article-title>. <source>G3 (Bethesda)</source>. <year>2017</year>;<volume>7</volume>(<issue>1</issue>):<fpage>41</fpage>–<lpage>53</lpage>.<pub-id pub-id-type="pmid">27793970</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cuevas</surname><given-names>J</given-names></string-name>, <string-name><surname>Montesinos-López</surname><given-names>O</given-names></string-name>, <string-name><surname>Juliana</surname><given-names>P</given-names></string-name>, <string-name><surname>Guzmán</surname><given-names>C</given-names></string-name>, <string-name><surname>Pérez-Rodríguez</surname><given-names>P</given-names></string-name>, <string-name><surname>González-Bucio</surname><given-names>J</given-names></string-name>, <string-name><surname>Burgueño</surname><given-names>J</given-names></string-name>, <string-name><surname>Montesinos-López</surname><given-names>A</given-names></string-name>, <string-name><surname>Crossa</surname><given-names>J.</given-names></string-name></person-group><article-title>Deep kernel for genomic and near infrared predictions in multi-environment breeding trials</article-title>. <source>G3 (Bethesda)</source>. <year>2019</year>;<volume>9</volume>(<issue>9</issue>):<fpage>2913</fpage>–<lpage>2924</lpage>.<pub-id pub-id-type="pmid">31289023</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de los Campos</surname><given-names>G</given-names></string-name>, <string-name><surname>Hickey</surname><given-names>JM</given-names></string-name>, <string-name><surname>Pong-Wong</surname><given-names>R</given-names></string-name>, <string-name><surname>Daetwyler</surname><given-names>HD</given-names></string-name>, <string-name><surname>Calus</surname><given-names>MPL.</given-names></string-name></person-group><article-title>Whole-genome regression and prediction methods applied to plant and animal breeding</article-title>. <source>Genetics</source>. <year>2013</year>;<volume>193</volume>(<issue>2</issue>):<fpage>327</fpage>–<lpage>345</lpage>.<pub-id pub-id-type="pmid">22745228</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>de Los Campos</surname><given-names>G</given-names></string-name>, <string-name><surname>Pérez-Rodríguez</surname><given-names>P</given-names></string-name>, <string-name><surname>Bogard</surname><given-names>M</given-names></string-name>, <string-name><surname>Gouache</surname><given-names>D</given-names></string-name>, <string-name><surname>Crossa</surname><given-names>J.</given-names></string-name></person-group><article-title>A data-driven simulation platform to predict cultivars’ performances under uncertain weather conditions</article-title>. <source>Nat Commun</source>. <year>2020</year>;<volume>11</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">31911652</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fisher</surname><given-names>A</given-names></string-name>, <string-name><surname>Rudin</surname><given-names>C</given-names></string-name>, <string-name><surname>Dominici</surname><given-names>F.</given-names></string-name></person-group><article-title>All models are wrong, but many are useful: learning a variable’s importance by studying an entire class of prediction models simultaneously</article-title>. <source>J Mach Learn Res</source>. <year>2019</year>;<volume>20</volume>:<fpage>1</fpage>–<lpage>81</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Friedman</surname><given-names>JH.</given-names></string-name></person-group><article-title>Greedy function approximation: a gradient boosting machine</article-title>. <source>Ann Stat</source>. <year>2001</year>;29(5):<fpage>1189</fpage>–<lpage>1232</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Géron</surname><given-names>A.</given-names></string-name></person-group><year>2019</year>. <source>Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems</source>. <publisher-name>O’Reilly Media, UK Ltd</publisher-name>.</mixed-citation>
    </ref>
    <ref id="jkac226-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Granato</surname><given-names>I</given-names></string-name>, <string-name><surname>Cuevas</surname><given-names>J</given-names></string-name>, <string-name><surname>Luna-Vázquez</surname><given-names>F</given-names></string-name>, <string-name><surname>Crossa</surname><given-names>J</given-names></string-name>, <string-name><surname>Montesinos-López</surname><given-names>O</given-names></string-name>, <string-name><surname>Burgueño</surname><given-names>J</given-names></string-name>, <string-name><surname>Fritsche-Neto</surname><given-names>R.</given-names></string-name></person-group><article-title>BGGE: a new package for genomic-enabled prediction incorporating genotype × environment interaction models</article-title>. <source>G3 (Bethesda)</source>. <year>2018</year>;<volume>8</volume>(<issue>9</issue>):<fpage>3039</fpage>–<lpage>3047</lpage>.<pub-id pub-id-type="pmid">30049744</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenwell</surname><given-names>BM</given-names></string-name>, <string-name><surname>Boehmke</surname><given-names>BC</given-names></string-name>, <string-name><surname>Gray</surname><given-names>B.</given-names></string-name></person-group><article-title>Variable importance plots—an introduction to the vip package</article-title>. <source>R J</source>. <year>2020</year>;<volume>12</volume>(<issue>1</issue>):<fpage>343</fpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Heslot</surname><given-names>N</given-names></string-name>, <string-name><surname>Yang</surname><given-names>H-P</given-names></string-name>, <string-name><surname>Sorrells</surname><given-names>ME</given-names></string-name>, <string-name><surname>Jannink</surname><given-names>J-L.</given-names></string-name></person-group><article-title>Genomic selection in plant breeding: a comparison of models</article-title>. <source>Crop Sci</source>. <year>2012</year>;<volume>52</volume>(<issue>1</issue>):<fpage>146</fpage>–<lpage>160</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jarquín</surname><given-names>D</given-names></string-name>, <string-name><surname>Crossa</surname><given-names>J</given-names></string-name>, <string-name><surname>Lacaze</surname><given-names>X</given-names></string-name>, <string-name><surname>Du Cheyron</surname><given-names>P</given-names></string-name>, <string-name><surname>Daucourt</surname><given-names>J</given-names></string-name>, <string-name><surname>Lorgeou</surname><given-names>J</given-names></string-name>, <string-name><surname>Piraux</surname><given-names>F</given-names></string-name>, <string-name><surname>Guerreiro</surname><given-names>L</given-names></string-name>, <string-name><surname>Pérez</surname><given-names>P</given-names></string-name>, <string-name><surname>Calus</surname><given-names>M</given-names></string-name></person-group>, <etal>et al</etal><article-title>A reaction norm model for genomic selection using high-dimensional genomic and environmental data</article-title>. <source>Theor Appl Genet</source>. <year>2014</year>;<volume>127</volume>(<issue>3</issue>):<fpage>595</fpage>–<lpage>607</lpage>.<pub-id pub-id-type="pmid">24337101</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jarquín</surname><given-names>D</given-names></string-name>, <string-name><surname>da Silva</surname><given-names>CL</given-names></string-name>, <string-name><surname>Gaynor</surname><given-names>RC</given-names></string-name>, <string-name><surname>Poland</surname><given-names>J</given-names></string-name>, <string-name><surname>Fritz</surname><given-names>A</given-names></string-name></person-group>, Howard R, Battenfield S, Crossa J. Increasing genomic-enabled prediction accuracy by modeling genotype x environment interactions in Kansas wheat. Plant Genome. <year>2017</year>;<volume>10</volume>:<fpage>1</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B25">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kuhn</surname><given-names>M</given-names></string-name>, <string-name><surname>Johnson</surname><given-names>K.</given-names></string-name></person-group><source>Applied Predictive Modeling</source>, Vol. <volume>26</volume>. <publisher-name>Springer</publisher-name>; <year>2013</year>.</mixed-citation>
    </ref>
    <ref id="jkac226-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Kuhn</surname><given-names>M</given-names></string-name>, <string-name><surname>Wickham</surname><given-names>H.</given-names></string-name></person-group> Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. <year>2020</year>. Retrieved from <ext-link xlink:href="https://CRAN.R-project.org/package=tidymodels" ext-link-type="uri">https://CRAN.R-project.org/package=tidymodels</ext-link></mixed-citation>
    </ref>
    <ref id="jkac226-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McFarland</surname><given-names>BA</given-names></string-name>, <string-name><surname>AlKhalifah</surname><given-names>N</given-names></string-name>, <string-name><surname>Bohn</surname><given-names>M</given-names></string-name>, <string-name><surname>Bubert</surname><given-names>J</given-names></string-name>, <string-name><surname>Buckler</surname><given-names>ES</given-names></string-name>, <string-name><surname>Ciampitti</surname><given-names>I</given-names></string-name>, <string-name><surname>Edwards</surname><given-names>J</given-names></string-name>, <string-name><surname>Ertl</surname><given-names>D</given-names></string-name>, <string-name><surname>Gage</surname><given-names>JL</given-names></string-name>, <string-name><surname>Falcon</surname><given-names>CM</given-names></string-name></person-group>, <etal>et al</etal><article-title>Maize genomes to fields (G2F): 2014–2017 field seasons: genotype, phenotype, climatic, soil, and inbred ear image datasets</article-title>. <source>BMC Res Notes</source>. <year>2020</year>;<volume>13</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">31898526</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>McKinney</surname><given-names>BA</given-names></string-name>, <string-name><surname>Reif</surname><given-names>DM</given-names></string-name>, <string-name><surname>Ritchie</surname><given-names>MD</given-names></string-name>, <string-name><surname>Moore</surname><given-names>JH.</given-names></string-name></person-group><article-title>Machine learning for detecting gene-gene interactions</article-title>. <source>Appl Bioinformatics</source>. <year>2006</year>;<volume>5</volume>(<issue>2</issue>):<fpage>77</fpage>–<lpage>88</lpage>.<pub-id pub-id-type="pmid">16722772</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Molnar</surname><given-names>C.</given-names></string-name></person-group><source>Interpretable Machine Learning - A Guide for Making Black Box Models Explainable</source>, <edition>2nd ed</edition>.; <year>2022</year>. Accessed via <ext-link xlink:href="https://christophm.github.io/interpretable-ml-book" ext-link-type="uri">https://christophm.github.io/interpretable-ml-book</ext-link>.</mixed-citation>
    </ref>
    <ref id="jkac226-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Montesinos-López</surname><given-names>OA</given-names></string-name>, <string-name><surname>Montesinos-López</surname><given-names>A</given-names></string-name>, <string-name><surname>Luna-Vázquez</surname><given-names>FJ</given-names></string-name>, <string-name><surname>Toledo</surname><given-names>FH</given-names></string-name>, <string-name><surname>Pérez-Rodríguez</surname><given-names>P</given-names></string-name>, <string-name><surname>Lillemo</surname><given-names>M</given-names></string-name>, <string-name><surname>Crossa</surname><given-names>J.</given-names></string-name></person-group><article-title>An r package for Bayesian analysis of multi-environment and multi-trait multi-environment data for genome-based prediction</article-title>. <source>G3 (Bethesda)</source>. <year>2019</year>;<volume>9</volume>(<issue>5</issue>):<fpage>1355</fpage>–<lpage>1369</lpage>.<pub-id pub-id-type="pmid">30819822</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Monteverde</surname><given-names>E</given-names></string-name>, <string-name><surname>Gutierrez</surname><given-names>L</given-names></string-name>, <string-name><surname>Blanco</surname><given-names>P</given-names></string-name>, <string-name><surname>Pérez de Vida</surname><given-names>F</given-names></string-name>, <string-name><surname>Rosas</surname><given-names>JE</given-names></string-name>, <string-name><surname>Bonnecarrère</surname><given-names>V</given-names></string-name>, <string-name><surname>Quero</surname><given-names>G</given-names></string-name>, <string-name><surname>McCouch</surname><given-names>S.</given-names></string-name></person-group><article-title>Integrating molecular markers and environmental covariates to interpret genotype by environment interaction in rice (<italic toggle="yes">Oryza sativa</italic> L.) grown in subtropical areas</article-title>. <source>G3 (Bethesda)</source>. <year>2019</year>;<volume>9</volume>(<issue>5</issue>):<fpage>1519</fpage>–<lpage>1531</lpage>.<pub-id pub-id-type="pmid">30877079</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Monteverde</surname><given-names>E</given-names></string-name>, <string-name><surname>Rosas</surname><given-names>JE</given-names></string-name>, <string-name><surname>Blanco</surname><given-names>P</given-names></string-name>, <string-name><surname>Pérez de Vida</surname><given-names>F</given-names></string-name>, <string-name><surname>Bonnecarrère</surname><given-names>V</given-names></string-name>, <string-name><surname>Quero</surname><given-names>G</given-names></string-name>, <string-name><surname>Gutierrez</surname><given-names>L</given-names></string-name>, <string-name><surname>McCouch</surname><given-names>S.</given-names></string-name></person-group><article-title>Multienvironment models increase prediction accuracy of complex traits in advanced breeding lines of rice</article-title>. <source>Crop Sci</source>. <year>2018</year>;<volume>58</volume>(<issue>4</issue>):<fpage>1519</fpage>–<lpage>1530</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pérez</surname><given-names>P</given-names></string-name>, <string-name><surname>de Los Campos</surname><given-names>G.</given-names></string-name></person-group><article-title>Genome-wide regression and prediction with the BGLR statistical package</article-title>. <source>Genetics</source>. <year>2014</year>;<volume>198</volume>(<issue>2</issue>):<fpage>483</fpage>–<lpage>495</lpage>.<pub-id pub-id-type="pmid">25009151</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pérez-Enciso</surname><given-names>M</given-names></string-name>, <string-name><surname>Zingaretti</surname><given-names>LM.</given-names></string-name></person-group><article-title>A guide on deep learning for complex trait genomic prediction</article-title>. <source>Genes</source>. <year>2019</year>;<volume>10</volume>(<issue>7</issue>):<fpage>553</fpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rincent</surname><given-names>R</given-names></string-name>, <string-name><surname>Malosetti</surname><given-names>M</given-names></string-name>, <string-name><surname>Ababaei</surname><given-names>B</given-names></string-name>, <string-name><surname>Touzy</surname><given-names>G</given-names></string-name>, <string-name><surname>Mini</surname><given-names>A</given-names></string-name>, <string-name><surname>Bogard</surname><given-names>M</given-names></string-name>, <string-name><surname>Martre</surname><given-names>P</given-names></string-name>, <string-name><surname>Le Gouis</surname><given-names>J</given-names></string-name>, <string-name><surname>van Eeuwijk</surname><given-names>F.</given-names></string-name></person-group><article-title>Using crop growth model stress covariates and AMMI decomposition to better predict genotype-by-environment interactions</article-title>. <source>Theor Appl Genet</source>. <year>2019</year>;<volume>132</volume>(<issue>12</issue>):<fpage>3399</fpage>–<lpage>3411</lpage>.<pub-id pub-id-type="pmid">31562567</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ritchie</surname><given-names>MD</given-names></string-name>, <string-name><surname>White</surname><given-names>BC</given-names></string-name>, <string-name><surname>Parker</surname><given-names>JS</given-names></string-name>, <string-name><surname>Hahn</surname><given-names>LW</given-names></string-name>, <string-name><surname>Moore</surname><given-names>JH.</given-names></string-name></person-group><article-title>Optimization of neural network architecture using genetic programming improves detection and modeling of gene-gene interactions in studies of human diseases</article-title>. <source>BMC Bioinformatics</source>. <year>2003</year>;<volume>4</volume>(<issue>1</issue>):<fpage>28</fpage>.<pub-id pub-id-type="pmid">12846935</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Runcie</surname><given-names>DE</given-names></string-name>, <string-name><surname>Qu</surname><given-names>J</given-names></string-name>, <string-name><surname>Cheng</surname><given-names>H</given-names></string-name>, <string-name><surname>Crawford</surname><given-names>L.</given-names></string-name></person-group><article-title>Megalmm: mega-scale linear mixed models for genomic predictions with thousands of traits</article-title>. <source>Genome Biol</source>. <year>2021</year>;<volume>22</volume>(<issue>1</issue>):<fpage>1</fpage>–<lpage>25</lpage>.<pub-id pub-id-type="pmid">33397451</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shahriari</surname><given-names>B</given-names></string-name>, <string-name><surname>Swersky</surname><given-names>K</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Z</given-names></string-name>, <string-name><surname>Adams</surname><given-names>RP</given-names></string-name>, <string-name><surname>Freitas</surname><given-names>ND.</given-names></string-name></person-group><article-title>Taking the human out of the loop: a review of Bayesian optimization</article-title>. <source>Proc IEEE</source>. <year>2016</year>;<volume>104</volume>(<issue>1</issue>):<fpage>148</fpage>–<lpage>175</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B39">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Sparks</surname><given-names>AH.</given-names></string-name></person-group><year>2018</year>. nasapower: a NASA POWER global meteorology, surface solar energy and climatology data client for R.</mixed-citation>
    </ref>
    <ref id="jkac226-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Van der Laan</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Polley</surname><given-names>EC</given-names></string-name>, <string-name><surname>Hubbard</surname><given-names>AE.</given-names></string-name></person-group><article-title>Super learner</article-title>. <source>Stat Appl Genet Mol Biol</source>. <year>2007</year>;<volume>6</volume>(<issue>1</issue>).</mixed-citation>
    </ref>
    <ref id="jkac226-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Westhues</surname><given-names>CC</given-names></string-name>, <string-name><surname>Mahone</surname><given-names>GS</given-names></string-name>, <string-name><surname>da Silva</surname><given-names>S</given-names></string-name>, <string-name><surname>Thorwarth</surname><given-names>P</given-names></string-name>, <string-name><surname>Schmidt</surname><given-names>M</given-names></string-name>, <string-name><surname>Richter</surname><given-names>J-C</given-names></string-name>, <string-name><surname>Simianer</surname><given-names>H</given-names></string-name>, <string-name><surname>Beissinger</surname><given-names>TM.</given-names></string-name></person-group><article-title>Prediction of maize phenotypic traits with genomic and environmental predictors using gradient boosting frameworks</article-title>. <source>Front Plant Sci</source>. <year>2021</year>;<volume>12</volume>:<fpage>699589</fpage>.<pub-id pub-id-type="pmid">34880880</pub-id></mixed-citation>
    </ref>
    <ref id="jkac226-B42">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Wickham</surname><given-names>H</given-names></string-name>, <string-name><surname>Hester</surname><given-names>J</given-names></string-name>, <string-name><surname>Chang</surname><given-names>W</given-names></string-name>, <string-name><surname>Hester</surname><given-names>MJ.</given-names></string-name></person-group><year>2021</year>. Package ‘devtools’. Retrieved from <ext-link xlink:href="https://cran.r-project.org/web/packages/devtools/index.html" ext-link-type="uri">https://cran.r-project.org/web/packages/devtools/index.html</ext-link></mixed-citation>
    </ref>
    <ref id="jkac226-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xavier</surname><given-names>A</given-names></string-name>, <string-name><surname>Muir</surname><given-names>WM</given-names></string-name>, <string-name><surname>Rainey</surname><given-names>KM.</given-names></string-name></person-group><article-title>bWGR: Bayesian whole-genome regression</article-title>. <source>Bioinformatics</source>. <year>2019</year>;<volume>36</volume>:<fpage>1957</fpage>–<lpage>1959</lpage>.</mixed-citation>
    </ref>
    <ref id="jkac226-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zou</surname><given-names>H</given-names></string-name>, <string-name><surname>Hastie</surname><given-names>T.</given-names></string-name></person-group><article-title>Regularization and variable selection via the elastic net</article-title>. <source>J R Stat Soc Series B Stat Methodol</source>. <year>2005</year>;<volume>67</volume>(<issue>2</issue>):<fpage>301</fpage>–<lpage>320</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
