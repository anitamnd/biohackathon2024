<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8253769</article-id>
    <article-id pub-id-type="publisher-id">93149</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-021-93149-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>OpenHSV: an open platform for laryngeal high-speed videoendoscopy</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Kist</surname>
          <given-names>Andreas M.</given-names>
        </name>
        <address>
          <email>andreas.kist@fau.de</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dürr</surname>
          <given-names>Stephan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Schützenberger</surname>
          <given-names>Anne</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Döllinger</surname>
          <given-names>Michael</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.5330.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 3311</institution-id><institution>Division of Phoniatrics and Pediatric Audiology, Department of Otorhinolaryngology, Head and Neck Surgery, </institution><institution>University Hospital Erlangen, Friedrich-Alexander-University Erlangen-Nürnberg, </institution></institution-wrap>Waldstr. 1, 91054 Erlangen, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.5330.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 3311</institution-id><institution>Department of Artificial Intelligence in Biomedical Engineering, </institution><institution>Friedrich-Alexander-University Erlangen-Nürnberg, </institution></institution-wrap>Henkestr. 91, 91054 Erlangen, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>2</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>2</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>11</volume>
    <elocation-id>13760</elocation-id>
    <history>
      <date date-type="received">
        <day>4</day>
        <month>11</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>3</day>
        <month>6</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">High-speed videoendoscopy is an important tool to study laryngeal dynamics, to quantify vocal fold oscillations, to diagnose voice impairments at laryngeal level and to monitor treatment progress. However, there is a significant lack of an open source, expandable research tool that features latest hardware and data analysis. In this work, we propose an open research platform termed OpenHSV that is based on state-of-the-art, commercially available equipment and features a fully automatic data analysis pipeline. A publicly available, user-friendly graphical user interface implemented in Python is used to interface the hardware. Video and audio data are recorded in synchrony and are subsequently fully automatically analyzed. Video segmentation of the glottal area is performed using efficient deep neural networks to derive glottal area waveform and glottal midline. Established quantitative, clinically relevant video and audio parameters were implemented and computed. In a preliminary clinical study, we recorded video and audio data from 28 healthy subjects. Analyzing these data in terms of image quality and derived quantitative parameters, we show the applicability, performance and usefulness of OpenHSV. Therefore, OpenHSV provides a valid, standardized access to high-speed videoendoscopy data acquisition and analysis for voice scientists, highlighting its use as a valuable research tool in understanding voice physiology. We envision that OpenHSV serves as basis for the next generation of clinical HSV systems.</p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Medical research</kwd>
      <kwd>Oral anatomy</kwd>
      <kwd>Diagnosis</kwd>
      <kwd>Health services</kwd>
      <kwd>Medical imaging</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100006360</institution-id>
            <institution>Bundesministerium für Wirtschaft und Energie</institution>
          </institution-wrap>
        </funding-source>
        <award-id>ZF4010105/BA8</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100008662</institution-id>
            <institution>Joachim Herz Stiftung</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Friedrich-Alexander-Universität Erlangen-Nürnberg (1041)</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open Access funding enabled and organized by Projekt DEAL.</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Laryngeal high-speed videoendoscopy (HSV) has been an emerging tool since decades in investigating voice physiology and pathophysiology<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. The vocal folds, the main source of our voice and being located in the larynx (Fig. <xref rid="Fig1" ref-type="fig">1</xref>), are oscillating at very high frequencies. Typical fundamental frequencies for males and females are around 120 and 250 Hz, respectively<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. According to the Nyquist–Shannon sampling theorem, the sampling rate has to be at least twice as high as the fundamental frequency to estimate the frequency. However, to observe the opening-closing transition within each cycle in an accurate and detailed way, a recent study suggests that sampling roughly 20-times higher, i.e. around 4000 Hz, is sufficient, given the average fundamental frequencies for humans<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Standard cameras are not able to acquire images at these high rates at full resolution. The current clinical gold standard uses a technique called stroboscopy. In stroboscopy, the fundamental frequency is computed from a high-resolution audio signal and the camera only acquires a single frame every n-th oscillation cycle (similar to shown glottal areas above the glottal area waveform (GAW), Fig. <xref rid="Fig1" ref-type="fig">1</xref>). This works well for healthy subjects with regular phonation, however, fails on irregular oscillations as often observed in patients<sup><xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR7">7</xref></sup>. In contrast, HSV acquires typically at 4000 fps or more<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup> and is therefore capable to resolve every oscillation cycle for low to very high phonation frequencies. <fig id="Fig1"><label>Figure 1</label><caption><p>Laryngeal high-speed videoendoscopy is performed with a rigid endoscope yielding a top view of the larynx showing the vocal folds and the glottis. Glottis is segmented for each recorded video frame resulting in the glottal area waveform (GAW, blue). Stroboscopy is limited to single time points of individual cycles (dashed line). Using the GAW and the audio signal, quantitative parameters and the phonovibrogram are computed.</p></caption><graphic xlink:href="41598_2021_93149_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par3">Despite the proven usefulness of HSV<sup><xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR11">11</xref></sup>, there have been only two commercially available HSV systems from KayPentax and Richard Wolf, that were launched years ago. Hence, in most cases HSV examinations are performed with either of the two, or very likely with unique research setups with custom hardware and custom software that is not standardized and often hinders comparability of results<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>. The main drawbacks of HSV, and we believe the reasons why HSV is still rarely applied in the clinic, are the high purchasing costs and the technical limitations, such as temporal and spatial resolution and sensitivity of the camera<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, and, first and foremost, the needed complex analysis of the HSV footage<sup><xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR16">16</xref></sup>. In the analysis workflow, image processing, i.e. segmenting the glottis (Fig. <xref rid="Fig1" ref-type="fig">1</xref>), is a major bottleneck. Although fully automatic solutions for glottis segmentation have been proposed<sup><xref ref-type="bibr" rid="CR17">17</xref>–<xref ref-type="bibr" rid="CR20">20</xref></sup>, these methods have not seen further adaptation. With the advent of deep learning, however, this bottleneck has been successfully addressed<sup><xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR23">23</xref></sup> and fast yet reliable solutions have been suggested<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. Since several years, we have been developing a standalone analysis platform, Glottis Analysis Tools (GAT), that allows video and audio data analysis<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>. However, GAT is by design not interconnected with hardware and data acquisition. In summary, there is a lack of a unifying research platform that allows both, data acquisition and analysis, using state-of-the-art hardware and analysis tools.</p>
    <p id="Par4">In this study, we suggest a novel and open research tool that we term OpenHSV, that offers an examination-ready HSV hardware setup that acquires video and audio in synchrony and tested in a clinical environment. Additionally, we provide a user-friendly graphical user interface that implements a basic patient management system, an audio and video preview and acquisition feature, and a fully automatic data analysis platform based on state-of-the-art deep neural networks, providing a solid foundation for next generation clinical accredited, commercial systems<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Hardware</title>
      <p id="Par5">The OpenHSV system is designed in a modular way to adapt to new hardware developments in terms of optics and technical equipment. In our study, a rigid, oral endoscope with 70° optics (Olympus), attached to a zoom lens (neomed) and connected to a color high-speed camera running at 4000 fps with a maximum ISO of 10,000 (IDT CCM-1540) is used. To determine a useful range of focal lengths, we tested different lenses from various suppliers (12 mm and 23 mm Karl Storz, 35 mm Richard Wolf, 80 mm Lighthouse, 15–25 mm neomed, see “<xref rid="Sec13" ref-type="sec">Results</xref>”). Illumination is provided through a high power LED light source (Storz LED 300) connected via a light-fiber guide. Audio is recorded via a high-performance lavalier microphone (DPA 4060) connected to an audio interface (Focusrite Scarlet 2i2) using the XLR interface and is placed on a custom 3D printed microphone mount attached to the endoscope. The camera “Synch Out” signal is connected via a BNC to ¼ in TRS cable directly to one channel of the audio interface. The foot switch is connected to the “External Trigger In” port of the camera. An overview of the connection scheme of the individual parts is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. All components are connected to a standard commercial computer (Intel i5 processor, 16 GB RAM) equipped with an additional, current Gigabit ethernet card to connect the high-speed camera to the computer. We use deep neural networks that are optimized for CPU architectures and hence, no dedicated high-end graphics card is needed. However, when available, the graphics card is automatically utilized (see section “<xref rid="Sec6" ref-type="sec">Data analysis</xref>”).<fig id="Fig2"><label>Figure 2</label><caption><p>Connection scheme of the OpenHSV system. IN (blue) depicts entry of data, light or signal to a device and OUT (green) the exit of a data, light or signal from a device. The camera sends data to the computer and a reference signal to the audio interface to synchronize audio and video data. The audio signal is recorded via a high-quality microphone. A foot switch acts as an external trigger signal that stops the recording. The light source provides high power light via a light fiber to the endoscope. The endoscope is connected to a lens that relays the image to the high-speed camera.</p></caption><graphic xlink:href="41598_2021_93149_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par6">We further provide STL files online to 3D print custom holders for cables, the endoscope and the microphone. A droplet exposure protection shield, owing to the current COVID19 pandemic, to protect the camera is also available. These parts can be easily printed on a conventional stereolithography (SLA) or fused deposition modeling (FMD) 3D printers, where we found the latter faster and cheaper. We provide a tabular parts list in the supplement (Supplementary Table <xref rid="MOESM1" ref-type="media">1</xref>) and on the online documentation.</p>
    </sec>
    <sec id="Sec4">
      <title>Data acquisition</title>
      <p id="Par7">The examination, data acquisition and data analysis is performed using a dedicated graphical user interface (GUI) as described in a separate section. The high-speed camera is equipped with an on-board memory of 8 GB, allowing to record about 1.6 s at full spatial resolution and full speed (1440 × 1024 px and 4000 fps, respectively). During an examination, the video data is constantly written to a circular buffer on the on-board memory until an external trigger (e.g. a foot switch) is provided. By default, the trigger signal stops the recording, saving the last 1.6 s of footage. The camera provides a “Synch Out” signal that is an edge signal indicating the respective frame start. We refer to this signal as reference signal. We record the reference signal simultaneously with the audio signal to synchronize the video footage with the audio signal. Audio and reference signal are digitized at 80 kHz with 24 bit resolution.</p>
      <p id="Par8">After the external trigger, the acquisition of the video data stops immediately, the audio signal acquires another 1 s to ensure the correct alignment of video and audio. An acquired video can be previewed, the complete video or a fraction thereof selected, and downloaded from the camera to the computer. Video footage is saved in two ways, lossless and lossy for data analysis and portability, respectively. The data is stored as “.mp4” files using the h264 codec. Audio is saved as uncompressed “.wav” files. Patient, video, and audio metadata are saved as “.json” file. If data analysis was performed, the glottal area segmentations are saved as “.hdf5” files and quantitative parameters as “.csv” files.</p>
    </sec>
    <sec id="Sec5">
      <title>Audio and video signal alignment</title>
      <p id="Par9">The audio file contains the camera reference signal together with the subject audio signal. We use a multi-step analysis pipeline to align the audio signal to the camera frames (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a). First, we compute a rolling standard deviation (std) using a 2.5 ms window of the raw reference signal. Next, we z-score the std signal and find the most prominent peak defining the end trigger event (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b). Each frame is indicated by a peak in the reference signal (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c). We detect the total recorded frames on the camera as peaks relative to the end trigger (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c). The audio signal corresponding to the selected and transferred data is extracted and used for further analysis. We do not correct for the potential time delay between source generation and acoustic signal detection.<fig id="Fig3"><label>Figure 3</label><caption><p>Audio–Video alignment. (<bold>a</bold>) Analysis pipeline. (<bold>b</bold>) Detection of end trigger using a normalized, rolling standard deviation (std) on example data. (<bold>c</bold>) Detection of recorded frames and extraction of selected area on the same example data as shown in panel (<bold>b</bold>).</p></caption><graphic xlink:href="41598_2021_93149_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec6">
      <title>Data analysis</title>
      <p id="Par10">Data analysis is performed individually for video and audio data (Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>).</p>
      <sec id="Sec7">
        <title>Video</title>
        <p id="Par11">After acquisition and region of interest (ROI) selection, we implemented a fully automatic glottis segmentation based on established, efficient and CPU optimized deep neural networks<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> trained on the openly available BAGLS dataset<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. The BAGLS dataset contains 59,250 high-speed video frames with the corresponding glottis segmentation mask. The exact training process is described elsewhere<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. Briefly, an optimized encoder-decoder network based on the U-Net architecture<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> is trained to predict glottal area segmentations based on endoscopic images. After manually selecting an ROI around the glottis, the full data is downloaded from the camera and the ROI data is subsequently analyzed on a frame by frame basis by the deep neural network. The use of an ROI is recommended, as this accelerates significantly the network inference and removes variances of the distant image. We provide with OpenHSV a pre-trained network that is also individually accessible at our Github repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/anki-xyz/openhsv/cnn">https://github.com/anki-xyz/openhsv/cnn</ext-link>). The resulting glottal area waveform (GAW) is used as basis for further computations of quantitative parameters<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> and is a one-dimensional function of all identified, i.e. segmented, pixels within one frame over time. We subsequently detect individual cycles in the GAW using standard peak finding algorithms as implemented in scipy<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. For symmetry measures, we estimate the glottal midline at each maximum cycle using either image moments or principal component analysis in the segmentation mask similar to previous works<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, also incorporating temporal context by summing adjacent frames to improve midline detection. We next identify the intersection of each glottal midline estimate with the segmented glottal area to find the anterior and posterior glottis points. Finally, we compute the phonovibrogram (PVG) as previously reported<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> and the GAW for the left and right vocal fold as the area of left and right vocal fold to the estimated midline, respectively.</p>
        <p id="Par12">Video or image quality was assessed using the Natural Image Quality Evaluator (NIQE). The NIQE score is a blind, no-reference score that reports image quality based on the statistics of natural scenes<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> and was already successfully applied to investigate laryngeal endoscopy image quality<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. In general, the lower the NIQE score, the better the image quality. Briefly, the NIQE score is based on natural scene statistics extracted from undistorted images. These statistics were used to construct quality aware features that were themselves fitted to a multivariate Gaussian model serving as reference. The NIQE score then represents the distance between a multivariate Gaussian fit extracted from the test image and the aforementioned natural scene-derived multivariate Gaussian reference model. We computed the NIQE score using its implementation in scikit-video for the monochrome and the color images in the BAGLS dataset and for the OpenHSV-derived example images.</p>
      </sec>
      <sec id="Sec8">
        <title>Audio</title>
        <p id="Par14">We similarly process audio signals to the GAW (see Supplementary Fig. <xref rid="MOESM1" ref-type="media">1</xref>a). First, we select the corresponding subset of the audio data in relation to the video data using the video reference signal acquired simultaneously with the audio signal (see audio and video signal alignment, Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Next, we compute the fundamental frequency similar to the GAW (Table <xref rid="Tab1" ref-type="table">1</xref>) to ensure validity of both signals (see also Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Clinical parameters contained in OpenHSV.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Clinical parameter</th><th align="left">Source signal</th><th align="left">References</th></tr></thead><tbody><tr><td align="left">Mean-Jitter</td><td align="left">Audio, GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR34">34</xref></sup></td></tr><tr><td align="left">Jitter (%)</td><td align="left">Audio, GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR35">35</xref></sup></td></tr><tr><td align="left">Mean-Shimmer</td><td align="left">Audio, GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR34">34</xref></sup></td></tr><tr><td align="left">Shimmer (%)</td><td align="left">Audio, GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR35">35</xref></sup></td></tr><tr><td align="left">Harmonics to noise ratio (HNR)</td><td align="left">Audio</td><td align="left"><sup><xref ref-type="bibr" rid="CR36">36</xref></sup></td></tr><tr><td align="left">Cepstral peak prominence (CPP)</td><td align="left">Audio</td><td align="left"><sup><xref ref-type="bibr" rid="CR37">37</xref></sup></td></tr><tr><td align="left">Open quotient (OQ)</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td></tr><tr><td align="left">Closing quotient (CQ)</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR39">39</xref></sup></td></tr><tr><td align="left">Speed quotient (SQ)</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td></tr><tr><td align="left">Asymmetry quotient (AQ)</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR40">40</xref></sup></td></tr><tr><td align="left">Rate quotient (RQ)</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td></tr><tr><td align="left">Speed index (SI)</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td></tr><tr><td align="left">Fundamental frequency (F0)</td><td align="left">Audio, GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td></tr><tr><td align="left">Amplitude perturbation factor (APF)</td><td align="left">Audio, GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR41">41</xref></sup></td></tr><tr><td align="left">Amplitude perturbation quotient (APQ)</td><td align="left">Audio, GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR41">41</xref></sup></td></tr><tr><td align="left">Glottis gap index (GGI)</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup></td></tr><tr><td align="left">Amplitude quotient</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR44">44</xref></sup></td></tr><tr><td align="left">Stiffness</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR45">45</xref></sup></td></tr><tr><td align="left">Amplitude symmetry index (ASI)</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR46">46</xref></sup></td></tr><tr><td align="left">Phase asymmetry index (PAI)</td><td align="left">GAW</td><td align="left"><sup><xref ref-type="bibr" rid="CR47">47</xref></sup></td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
    <sec id="Sec9">
      <title>Quantitative parameter computation</title>
      <p id="Par15">Given the total GAW, the GAW for the left and the right vocal fold, and the audio signal, we compute quantitative parameters. In the initial release, we provide in total 18 clinically relevant parameters for the GAW and nine clinical parameters for the audio signal (Table <xref rid="Tab1" ref-type="table">1</xref>). All parameters have been previously reported (see references in Table <xref rid="Tab1" ref-type="table">1</xref>) and have been reported in detail for healthy subjects<sup><xref ref-type="bibr" rid="CR48">48</xref>–<xref ref-type="bibr" rid="CR50">50</xref></sup>. Individual detected cycles in video and audio data were used to compute jitter and shimmer measures, as well as all other GAW measures. The complete audio signal was used for harmonics-to-noise-ratio (HNR) and cepstral peak prominence (CPP). We used the partial GAW for left and right vocal fold to compute the amplitude symmetry index and the phase asymmetry index. A comprehensive overview of these parameters is given in Refs.<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR50">50</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup><bold>.</bold></p>
    </sec>
    <sec id="Sec10">
      <title>Graphical user interface (GUI)</title>
      <p id="Par16">The OpenHSV GUI (Supplementary Movie <xref rid="MOESM1" ref-type="media">2</xref>) is written in Python 3.6 and mainly based on the libraries PyQt5 and pyqtgraph. The high-speed camera is interfaced using the camera manufacturer’s software developmental kit (IDT SDK). Video data are processed as multi-dimensional numpy arrays<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. We interact with the audio interface via the sounddevice library. Patient data is recorded and saved to a local file system; the patient, video and audio recording metadata is further saved to a human-readable JSON file. The GUI provides a tabular overview of all recorded patients that further contains a search option to allow retrieving dynamically metadata from a given subset of patients. It gives also fast and easy access to previously recorded data, being for example important to visually compare multiple acquisitions at different time points of the same patient.</p>
    </sec>
    <sec id="Sec11">
      <title>Clinical study</title>
      <p id="Par17">We recruited 28 healthy individuals to perform a preliminary clinical study. All individuals were identified as normophonic, had no laryngoscopic organic or functional disorders and did not report any issues with their voice. All participants gave their written and informed consent. This study was approved by the local ethics committee at the University Hospital Erlangen (#290_15) and was conducted in accordance with respective guidelines and relevant regulations. All acquisitions were made with the same settings and equipment. We analyzed an at least 1000 frame long segment in each recording with at least 20 glottal cycles, as recommended previously<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>.</p>
    </sec>
  </sec>
  <sec id="Sec13">
    <title>Results</title>
    <sec id="Sec14">
      <title>Setup</title>
      <p id="Par19">The OpenHSV setup consists of a mobile, equipment storage tower and a mobile imaging unit (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). In particular, we use a mobile platform containing a typical consumer-grade computer to interact with equipment and to conduct examinations, an illumination unit for providing light and an audio interface to record audio and the camera synchronization signal (Fig. <xref rid="Fig4" ref-type="fig">4</xref>a). A consumer-grade, 23″ monitor together with keyboard and computer mouse that can be disinfected is used to interact with the software. The imaging unit as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>b uses a rigid endoscope. The endoscope is connected to a lens and to the high-speed camera. The light-guide transmits light from the illumination unit to the endoscope to illuminate the larynx.<fig id="Fig4"><label>Figure 4</label><caption><p>The mobile imaging unit. (<bold>a</bold>) The mobile equipment tower consisting of computer, illumination source, audio interface, monitor and human–device-interaction components, (<bold>b</bold>) the imaging unit consisting of rigid endoscope, microphone, lens, high-speed camera, and light-guide.</p></caption><graphic xlink:href="41598_2021_93149_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par20">We first evaluated the image formation process from endoscope exit pupil to camera chip via a lens (basically an optical telescope) as these optics are crucial for a good image quality (Fig. <xref rid="Fig5" ref-type="fig">5</xref>a). Further, the lens’ focal length determines the image size, i.e. the pixels covered on the camera chip, and the signal-to-noise ratio, as a fixed amount of light is distributed across a varying surface (Fig. <xref rid="Fig5" ref-type="fig">5</xref>b). We found that a high-quality endoscope with 10 mm exit pupil diameter delivers large images together with a very high light intensity. We investigated different lenses with different focal lengths (12–80 mm) to determine the best trade-off between image magnification and signal to noise ratio. In Fig. <xref rid="Fig5" ref-type="fig">5</xref>c, we show example images from the same scene and the same recording settings with varying lenses and found, the larger the focal length of the lens, the larger the projected image size on the camera chip (Fig. <xref rid="Fig5" ref-type="fig">5</xref>c,d). In Fig. <xref rid="Fig5" ref-type="fig">5</xref>e, we show that the dynamic range of the images is higher the less the focal length is. In Supplementary Fig. <xref rid="MOESM1" ref-type="media">3</xref> we show the dynamic range on example images and their respective intensity distribution histograms. Overall, low focal length lenses provide sharp images with satisfactory dynamic range. In case of the 80 mm focal length lens, there is no satisfactory image possible. However, we would like to point out that all measurements are due to the combination of endoscope, lens, camera and acquisition settings. In an examination scenario, we found that focal lengths up to 25 mm are a good trade-off between available dynamic range and image size.<fig id="Fig5"><label>Figure 5</label><caption><p>Image formation process. (<bold>a</bold>) Optical setup including the endoscope exit pupil, the lens simplified as telescope, and the camera chip. The real image is formed on the camera chip. (<bold>b</bold>) Image size on camera chip depending on the focal length. Focal length and image size vary proportionally; focal length and light intensity are inversely correlated. (<bold>c</bold>) Example images from the same scenery with lenses with varying focal lengths. (<bold>d</bold>) Chip coverage in percent vs. focal length. Black line indicates one-exponential fit. (<bold>e</bold>) Available dynamic range vs. focal length. Black line indicates one-exponential fit.</p></caption><graphic xlink:href="41598_2021_93149_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec15">
      <title>Clinical examination</title>
      <p id="Par21">We next tested the ability to record simultaneously video footage and audio data in a typical examination setting, and analyze the resulting data (Fig. <xref rid="Fig6" ref-type="fig">6</xref>). With the imaging unit shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>a, we performed examinations of healthy subjects. Using our custom GUI (Fig. <xref rid="Fig7" ref-type="fig">7</xref>), we are able to control the recording settings and receive a live feedback of the video and the audio data. As the footage can be pretty large (several gigabytes) and may contain not relevant information, such as sequences without phonation or swallowing artifacts, the examiner is able to select a subset from the whole recording. Selected video data will be transferred to the computer.<fig id="Fig6"><label>Figure 6</label><caption><p>Examination and data analysis workflow. Audio and video data is acquired. Next, a subsection is selected and transferred to the computer. Using the video data, the glottal area is segmented and converted to signals that are used for parameter computation. The audio signal is aligned to the video footage using the reference signal and is subsequently analyzed.</p></caption><graphic xlink:href="41598_2021_93149_Fig6_HTML" id="MO6"/></fig><fig id="Fig7"><label>Figure 7</label><caption><p>The OpenHSV graphical user interface. Camera image (left) and audio trace (right) are previewed online. The reference signal (pink) and the audio trace (yellow) are shown, together with the estimated fundamental frequency of the audio data. After an end-trigger (e.g. using a foot-switch), the user selects a footage range and is able to analyze and/or save the audio and video material and optionally analyzes the data directly.</p></caption><graphic xlink:href="41598_2021_93149_Fig7_HTML" id="MO7"/></fig></p>
      <p id="Par22">The video analysis is based on the segmentation of the glottal area. The segmentation is performed fully automatic using a deep neural network as described elsewhere<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>. The segmentation is further converted to the glottal area waveform (GAW). Next, we define the glottal symmetry axis fully automatically<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> and convert the segmentation map into a phonovibrogram that allows a two-dimensional representation of the laryngeal dynamics<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>.</p>
    </sec>
    <sec id="Sec16">
      <title>Clinical validation</title>
      <p id="Par23">Our aim is to compare the OpenHSV recordings to data generated by established hardware and to validate our novel equipment and analysis platform. Therefore, we conducted a small-scale clinical study and analyzed 28 examinations from healthy individuals recorded with the OpenHSV system.</p>
      <p id="Par24">The subject age range was from 17 to 46 with a median age of 20. In Supplementary Fig. <xref rid="MOESM1" ref-type="media">4</xref>, we show representative images from the recordings. In Supplementary Movie <xref rid="MOESM1" ref-type="media">1</xref>, we show an example recording of 1000 consecutive frames as used in our analysis procedure. Using the analysis procedure depicted in Fig. <xref rid="Fig6" ref-type="fig">6</xref> and described in the methods, we compute for each recording the raw endoscopy video, the corresponding segmentation maps, the glottal area waveform (GAW), and the corresponding audio and reference signal (Fig. <xref rid="Fig8" ref-type="fig">8</xref>a).<fig id="Fig8"><label>Figure 8</label><caption><p>OpenHSV provides clinical relevant information. (<bold>a</bold>) Exemplary video, segmentation and audio data. (<bold>b</bold>) Image quality of OpenHSV compared to BAGLS dataset using NIQE. (<bold>c</bold>) Correlation of fundamental frequency determined in audio and video signal. Line indicates straight line of origin and perfect agreement between audio and GAW. (<bold>d</bold>) Exemplary GAW-derived quantitative parameters, namely Open Quotient (OQ), Closing Quotient (CQ), Speed Index (SI) and Glottal Gap Index (GGI). (<bold>e</bold>) Exemplary audio signal-derived quantitative parameters, namely mean Jitter, mean Shimmer, Harmonics-to-Noise-Ratio (HNR), and Cepstral Peak Prominence (CPP). Better values are indicated with gray arrow heads. For CQ and SI, 0.5 and 0 are desired values.</p></caption><graphic xlink:href="41598_2021_93149_Fig8_HTML" id="MO8"/></fig></p>
      <sec id="Sec17">
        <title>Image quality</title>
        <p id="Par25">We first determined the image quality of the OpenHSV system using the Natural Image Quality Evaluation (NIQE) score, a blind image quality metric that needs no reference images. We found that the OpenHSV system outperforms other imaging modalities that are contained in the BAGLS benchmark dataset that consists of a blend of data from seven different institutions having different equipment and recording conditions<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. As shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>b, the mean NIQE for the OpenHSV System is 13.19 compared to the mean NIQE score of 28.79 and 22.42 for RGB and monochrome images in the BAGLS dataset, respectively. Even though that monochrome images pool color on each pixel and do not show interpolation artifacts due to the Bayer matrix, the image quality is still worse than the OpenHSV data (Fig. <xref rid="Fig8" ref-type="fig">8</xref>b).</p>
      </sec>
      <sec id="Sec18">
        <title>Video-audio signal alignment</title>
        <p id="Par26">As the oscillating vocal folds are the main source of the phonation, the vocal fold fundamental oscillation frequency should be identical to the fundamental frequency determined from the corresponding audio signal. As shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>c, the fundamental frequencies are almost identical given the accuracy of our measurement systems showing typically deviations of less than 2 Hz (median 1.76 Hz) and are therefore negligible. Example audio and GAW power spectra of the analyzed recordings are shown in Supplementary Fig. <xref rid="MOESM1" ref-type="media">2</xref>.</p>
      </sec>
      <sec id="Sec19">
        <title>Clinical quantitative parameters</title>
        <p id="Par27">We next computed clinically relevant parameters for healthy subjects that we implemented in OpenHSV. In general, the computed parameters (Tables <xref rid="Tab2" ref-type="table">2</xref>, <xref rid="Tab3" ref-type="table">3</xref>) have a similar magnitude as reported previously for healthy subjects<sup><xref ref-type="bibr" rid="CR48">48</xref>–<xref ref-type="bibr" rid="CR50">50</xref></sup>. We provide the distributions for a subset of GAW-derived and audio-derived parameters in Fig. <xref rid="Fig8" ref-type="fig">8</xref>d,e. In comparison to a recent study that focused on the analysis of HSV data of healthy individuals<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, we found similar value distributions for parameters derived from the GAW, such as a similar open quotient (ours 0.998 vs. 0.927–0.999 reported) and asymmetry quotient (ours 0.501 vs. 0.511–0.554 reported). Similarly, the mean Jitter and mean Shimmer for the GAW signals observed (0.176 ms and 0.140 dB) are comparable with the aforementioned study (0.126–0.166 ms for mean Jitter and 0.102–0.130 dB for mean Shimmer<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>). We additionally observe on the audio data high values for HNR and CPP (on average 15.21 dB and 18.60 dB, respectively) which is an indication for healthy phonation (HNR on average 11.9 dB for normals<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, CPP &gt; 10 dB<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>). A good indication that both, video and audio, signals are in high synchrony are the similarities in fundamental frequencies between video and audio data (compare Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref>, Fig. <xref rid="Fig8" ref-type="fig">8</xref>c). We therefore conclude that the whole system, consisting of experimental setup and analysis software, produces reliable and plausible results for the investigated healthy subjects.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Glottal area waveform (GAW) parameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Parameter</th><th align="left">Mean</th><th align="left">std</th><th align="left">Min</th><th align="left">Max</th><th align="left">Unit</th></tr></thead><tbody><tr><td align="left">Mean Jitter</td><td align="left">0.176</td><td align="left">0.070</td><td align="left">0.072</td><td align="left">0.375</td><td align="left">ms</td></tr><tr><td align="left">Jitter%</td><td align="left">5.286</td><td align="left">2.067</td><td align="left">2.041</td><td align="left">10.962</td><td align="left">au</td></tr><tr><td align="left">Mean Shimmer</td><td align="left">0.140</td><td align="left">0.127</td><td align="left">0.035</td><td align="left">0.440</td><td align="left">dB</td></tr><tr><td align="left">Shimmer%</td><td align="left">0.257</td><td align="left">0.249</td><td align="left">0.059</td><td align="left">0.889</td><td align="left">au</td></tr><tr><td align="left">Fundamental frequency (F0)</td><td align="left">302</td><td align="left">49</td><td align="left">235</td><td align="left">410</td><td align="left">Hz</td></tr><tr><td align="left">Open quotient</td><td align="left">0.998</td><td align="left">0.003</td><td align="left">0.989</td><td align="left">1.000</td><td align="left">au</td></tr><tr><td align="left">Closing quotient</td><td align="left">0.504</td><td align="left">0.044</td><td align="left">0.402</td><td align="left">0.567</td><td align="left">au</td></tr><tr><td align="left">Speed quotient</td><td align="left">1.020</td><td align="left">0.030</td><td align="left">0.971</td><td align="left">1.093</td><td align="left">au</td></tr><tr><td align="left">Asymmetry quotient</td><td align="left">0.501</td><td align="left">0.007</td><td align="left">0.482</td><td align="left">0.518</td><td align="left">au</td></tr><tr><td align="left">Rate quotient</td><td align="left">1.027</td><td align="left">0.033</td><td align="left">0.973</td><td align="left">1.129</td><td align="left">au</td></tr><tr><td align="left">Amplitude quotient</td><td align="left">3.735</td><td align="left">0.741</td><td align="left">2.137</td><td align="left">5.160</td><td align="left">au</td></tr><tr><td align="left">Speed index</td><td align="left">0.003</td><td align="left">0.014</td><td align="left">− 0.035</td><td align="left">0.035</td><td align="left">au</td></tr><tr><td align="left">Glottal gap index</td><td align="left">0.150</td><td align="left">0.126</td><td align="left">0.000</td><td align="left">0.367</td><td align="left">au</td></tr><tr><td align="left">Stiffness</td><td align="left">0.278</td><td align="left">0.065</td><td align="left">0.186</td><td align="left">0.431</td><td align="left">au</td></tr><tr><td align="left">Amplitude symmetry index</td><td align="left">0.974</td><td align="left">0.014</td><td align="left">0.932</td><td align="left">0.993</td><td align="left">au</td></tr><tr><td align="left">Phase asymmetry index</td><td align="left">0.070</td><td align="left">0.063</td><td align="left">0.006</td><td align="left">0.258</td><td align="left">au</td></tr></tbody></table></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Audio parameters. <sup>a</sup>Amplitude perturbation quotient with varying windows sizes.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Parameter</th><th align="left">Mean</th><th align="left">std</th><th align="left">Min</th><th align="left">Max</th><th align="left">Unit</th></tr></thead><tbody><tr><td align="left">Mean Jitter</td><td align="left">0.079</td><td align="left">0.072</td><td align="left">0.009</td><td align="left">0.286</td><td align="left">ms</td></tr><tr><td align="left">Jitter%</td><td align="left">2.408</td><td align="left">2.260</td><td align="left">0.312</td><td align="left">8.280</td><td align="left">au</td></tr><tr><td align="left">Mean Shimmer</td><td align="left">0.590</td><td align="left">0.603</td><td align="left">0.167</td><td align="left">3.095</td><td align="left">dB</td></tr><tr><td align="left">Shimmer%</td><td align="left">2.093</td><td align="left">1.964</td><td align="left">0.623</td><td align="left">10.283</td><td align="left">au</td></tr><tr><td align="left">Fundamental frequency (F0)</td><td align="left">302</td><td align="left">50</td><td align="left">235</td><td align="left">412</td><td align="left">Hz</td></tr><tr><td align="left">Harmonics-noise-ratio (HNR)</td><td align="left">15.126</td><td align="left">2.415</td><td align="left">11.224</td><td align="left">21.000</td><td align="left">dB</td></tr><tr><td align="left">Cepstral peak prominence (CPP)</td><td align="left">18.596</td><td align="left">1.629</td><td align="left">16.160</td><td align="left">22.771</td><td align="left">dB</td></tr><tr><td align="left">Amplitude perturbation factor (APF)</td><td align="left">6.877</td><td align="left">7.237</td><td align="left">1.931</td><td align="left">37.181</td><td align="left">au</td></tr><tr><td align="left">APQ3<sup>a</sup></td><td align="left">3.665</td><td align="left">4.494</td><td align="left">0.671</td><td align="left">22.623</td><td align="left">au</td></tr><tr><td align="left">APQ5<sup>a</sup></td><td align="left">3.812</td><td align="left">3.156</td><td align="left">1.196</td><td align="left">15.979</td><td align="left">au</td></tr><tr><td align="left">APQ11<sup>a</sup></td><td align="left">5.132</td><td align="left">3.506</td><td align="left">1.537</td><td align="left">16.584</td><td align="left">au</td></tr></tbody></table></table-wrap></p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec20">
    <title>Discussion</title>
    <p id="Par28">In this study, we suggest a new and open research hardware and software platform that we termed OpenHSV. OpenHSV’s software and analysis package is distributed open source and the hardware can be purchased commercially off-the-shelf. Using state-of-the-art components, we are able to acquire both, high quality audio signals and video footage. OpenHSV allows further the direct signal analysis and provides on time clinically relevant information. OpenHSV can be easily expanded by adding custom written Python code.</p>
    <p id="Par29">Medical equipment requires to be setup with low levels of expertise. Being a research tool, the setup of OpenHSV is non-trivial and needs attention. While we provide detailed instructions in our online documentation, personnel without basic knowledge in computer science (hardware and software installation) may have difficulties to setup OpenHSV. As we are happy to provide help, we highlight that OpenHSV is not a simple Plug&amp;Play system. However, parts of OpenHSV, especially parts of the data analysis functionalities, have been integrated in commercial and clinical accredited systems, combining both, openness and easiness for future researchers and clinical examiners<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>.</p>
    <p id="Par30">High-speed videoendoscopy strongly relies on high-speed cameras. These cameras are highly specialized and various setup configurations are used<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. Especially, cameras from the two existing commercial systems are very handy and have small camera chips. The size of the camera chip is indeed a limiting factor for image quality. The larger the individual pixel size, and the higher the desired resolution, the larger the camera chip (see also Fig. <xref rid="Fig5" ref-type="fig">5</xref>). As our endoscope exit pupil size and the amount of transmitted light is fixed, an image magnification worsens the signal-to-noise-ratio. Thus, cameras with a smaller sensor size are likely better suited. However, we were not able to find another camera that fulfills the inclusion criteria of acquiring at 4000 fps, state-of-the-art spatial resolution and low-weight body and small form factor, which are important features to be considered in camera selection.</p>
    <p id="Par31">A typical bottleneck of high-speed cameras is the data transfer from the camera to the computer. To allow high-resolution acquisitions, typically, high-speed cameras write the high-speed footage to an internal memory and transfer the data to the main computer on request. This has the major drawback that a full-frame, 1.5 s long recording with about 8 GBs of data needs roughly 10 min for data transfer. Therefore, it is impractical to record larger fractions of data of a single subject multiple times, e.g. different phonations, when time is a relevant factor. OpenHSV is potentially able to be extended to support various equipment, for example live streaming of high-speed footage as integrated into the next generation of clinical high-speed videoendoscopy systems<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. However, as OpenHSV is designed as research tool, OpenHSV has its strength in flexibility and customization.</p>
    <p id="Par32">We found that our preliminary clinical study shows that both, audio and video data can be recorded and successfully analyzed using OpenHSV, having a good agreement between audio and video data (Fig. <xref rid="Fig8" ref-type="fig">8</xref>c). As we analyzed 28 healthy individuals, we believe that our data represents general validity, as we show that computed quantitative parameters for audio and video data are of similar magnitude as expected for healthy individuals<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR50">50</xref>,<xref ref-type="bibr" rid="CR54">54</xref></sup>. However, it remains to be investigated how OpenHSV performs on subjects showing pathologies. As we show that OpenHSV provides a better image quality compared to previous systems (Fig. <xref rid="Fig8" ref-type="fig">8</xref>b), we are certain that also organic pathologies and inflammations are at least on par.</p>
    <p id="Par33">As we and others have shown the promise of HSV in analyzing voice pathologies<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>, we are confident that OpenHSV is another major step forward to disseminate HSV further into research and eventually towards broad clinical application.</p>
  </sec>
  <sec id="Sec21">
    <title>Conclusions</title>
    <p id="Par34">HSV is an important tool to study voice physiology. We contribute OpenHSV, an open system with video and audio acquisition accompanied with data analysis. These unique properties of OpenHSV will enable researchers to conduct HSV studies with latest equipment and image processing technique. Due to the modular nature of OpenHSV, we expect that researchers expand OpenHSV to their individual needs.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec22">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41598_2021_93149_MOESM1_ESM.xlsx">
            <caption>
              <p>Supplementary Information.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41598_2021_93149_MOESM2_ESM.mp4">
            <caption>
              <p>Supplementary Video 1.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM3">
          <media xlink:href="41598_2021_93149_MOESM3_ESM.mp4">
            <caption>
              <p>Supplementary Video 2.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM4">
          <media xlink:href="41598_2021_93149_MOESM4_ESM.pdf">
            <caption>
              <p>Supplementary Figures.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary Information</title>
    <p>The online version contains supplementary material available at 10.1038/s41598-021-93149-0.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported by BMWi ZIM-Kooperationsprojekte (ZF4010105BA8). AMK was also supported by a Joachim-Herz-foundation Add-On fellowship.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>A.M.K. setup and evaluated hard- and software, programed the graphical user interface and performed data analysis, interpreted the data and prepared figures. S.D. and A.S. acquired data, tested the system and interpreted the data. M.D. acquired funding, supervised the project and interpreted the data. A.M.K. wrote the manuscript with the help of all authors.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open Access funding enabled and organized by Projekt DEAL.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The OpenHSV software is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/anki-xyz/openhsv">https://github.com/anki-xyz/openhsv</ext-link>. All further information, including documentation is available on the Github repository. The datasets used and analyzed during the current study are available from the corresponding author upon request.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code availability</title>
    <p>The OpenHSV code to conduct high-speed videoendoscopy examinations, to analyze the acquired data and to store subject data is available open source at <ext-link ext-link-type="uri" xlink:href="https://github.com/anki-xyz/openhsv">https://github.com/anki-xyz/openhsv</ext-link>. We further provide an in-depth documentation of the code at <ext-link ext-link-type="uri" xlink:href="https://openhsv.readthedocs.io/en/latest/">https://openhsv.readthedocs.io/en/latest/</ext-link>. In the documentation, we explain how to setup the OpenHSV system and perform first recordings. Different cameras can be used in general, when the camera interface is adapted accordingly. To operate OpenHSV for debugging purposes, we added a dummy camera interface that loops through an example video. All algorithms, such as parameter computation, midline prediction and audio analysis can be tested using the example files provided with OpenHSV.</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par35">AMK and MD are part of a joint project with wevosys, funded by the German Bundesministerium für Wirtschaft und Energie (BMWi). AMK and MD have no financial relations to wevosys. SD and AS declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Deliyski</surname>
            <given-names>DD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Clinical implementation of laryngeal high-speed videoendoscopy: Challenges and evolution</article-title>
        <source>Folia Phoniatr. Logop.</source>
        <year>2008</year>
        <volume>60</volume>
        <fpage>33</fpage>
        <lpage>44</lpage>
        <pub-id pub-id-type="doi">10.1159/000111802</pub-id>
        <?supplied-pmid 18057909?>
        <pub-id pub-id-type="pmid">18057909</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Titze</surname>
            <given-names>IR</given-names>
          </name>
        </person-group>
        <article-title>Physiologic and acoustic differences between male and female voices</article-title>
        <source>J. Acoust. Soc. Am.</source>
        <year>1989</year>
        <volume>85</volume>
        <fpage>1699</fpage>
        <lpage>1707</lpage>
        <pub-id pub-id-type="doi">10.1121/1.397959</pub-id>
        <?supplied-pmid 2708686?>
        <pub-id pub-id-type="pmid">2708686</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schützenberger</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Laryngeal high-speed videoendoscopy: Sensitivity of objective parameters towards recording frame rate</article-title>
        <source>BioMed. Res. Int.</source>
        <year>2016</year>
        <volume>2016</volume>
        <fpage>4575437</fpage>
        <pub-id pub-id-type="doi">10.1155/2016/4575437</pub-id>
        <?supplied-pmid 27990428?>
        <pub-id pub-id-type="pmid">27990428</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kang</surname>
            <given-names>D-H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Real-time simultaneous DKG and 2D DKG using high-speed digital camera</article-title>
        <source>J. Voice</source>
        <year>2017</year>
        <volume>31</volume>
        <issue>247</issue>
        <fpage>e1</fpage>
        <lpage>247.e7</lpage>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kendall</surname>
            <given-names>KA</given-names>
          </name>
        </person-group>
        <article-title>High-speed laryngeal imaging compared with videostroboscopy in healthy subjects</article-title>
        <source>Arch. Otolaryngol. Neck Surg.</source>
        <year>2009</year>
        <volume>135</volume>
        <fpage>274</fpage>
        <lpage>281</lpage>
        <pub-id pub-id-type="doi">10.1001/archoto.2008.557</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Powell</surname>
            <given-names>ME</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comparison of videostroboscopy to stroboscopy derived from high-speed videoendoscopy for evaluating patients with vocal fold mass lesions</article-title>
        <source>Am. J. Speech Lang. Pathol.</source>
        <year>2016</year>
        <volume>25</volume>
        <fpage>576</fpage>
        <lpage>589</lpage>
        <pub-id pub-id-type="doi">10.1044/2016_AJSLP-15-0050</pub-id>
        <?supplied-pmid 27716854?>
        <pub-id pub-id-type="pmid">27716854</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Woo</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Objective measures of stroboscopy and high-speed video</article-title>
        <source>Adv. Neurolaryngol.</source>
        <year>2020</year>
        <volume>85</volume>
        <fpage>25</fpage>
        <lpage>44</lpage>
        <pub-id pub-id-type="doi">10.1159/000456681</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Echternach</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sundberg</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Traser</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Richter</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Vocal fold vibrations at high soprano fundamental frequencies</article-title>
        <source>J. Acoust. Soc. Am.</source>
        <year>2013</year>
        <volume>133</volume>
        <fpage>EL82</fpage>
        <lpage>EL87</lpage>
        <pub-id pub-id-type="doi">10.1121/1.4773200</pub-id>
        <?supplied-pmid 23363198?>
        <pub-id pub-id-type="pmid">23363198</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The next step in voice assessment: High-speed digital endoscopy and objective evaluation</article-title>
        <source>Curr. Bioinform.</source>
        <year>2009</year>
        <volume>4</volume>
        <fpage>101</fpage>
        <lpage>111</lpage>
        <pub-id pub-id-type="doi">10.2174/157489309788184774</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mehta</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Zañartu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Quatieri</surname>
            <given-names>TF</given-names>
          </name>
          <name>
            <surname>Deliyski</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Hillman</surname>
            <given-names>RE</given-names>
          </name>
        </person-group>
        <article-title>Investigating acoustic correlates of human vocal fold vibratory phase asymmetry through modeling and laryngeal high-speed videoendoscopy</article-title>
        <source>J. Acoust. Soc. Am.</source>
        <year>2011</year>
        <volume>130</volume>
        <fpage>3999</fpage>
        <lpage>4009</lpage>
        <pub-id pub-id-type="doi">10.1121/1.3658441</pub-id>
        <?supplied-pmid 22225054?>
        <pub-id pub-id-type="pmid">22225054</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zacharias</surname>
            <given-names>SRC</given-names>
          </name>
          <name>
            <surname>Deliyski</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Gerlach</surname>
            <given-names>TT</given-names>
          </name>
        </person-group>
        <article-title>Utility of laryngeal high-speed videoendoscopy in clinical voice assessment</article-title>
        <source>J. Voice</source>
        <year>2018</year>
        <volume>32</volume>
        <fpage>216</fpage>
        <lpage>220</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jvoice.2017.05.002</pub-id>
        <?supplied-pmid 28596101?>
        <pub-id pub-id-type="pmid">28596101</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hertegård</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Larsson</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>A portable high-speed camera system for vocal fold examinations</article-title>
        <source>J. Voice</source>
        <year>2014</year>
        <volume>28</volume>
        <fpage>681</fpage>
        <lpage>687</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jvoice.2014.04.002</pub-id>
        <?supplied-pmid 25008381?>
        <pub-id pub-id-type="pmid">25008381</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kaneko</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Sakaguchi</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Inoue</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Takahashi</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Low-cost high-speed imaging system for observing vocal fold vibration in voice disorders</article-title>
        <source>ORL</source>
        <year>2012</year>
        <volume>74</volume>
        <fpage>208</fpage>
        <lpage>210</lpage>
        <pub-id pub-id-type="doi">10.1159/000341095</pub-id>
        <?supplied-pmid 22868889?>
        <pub-id pub-id-type="pmid">22868889</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Andrade-Miranda</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Stylianou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Deliyski</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Godino-Llorente</surname>
            <given-names>JI</given-names>
          </name>
          <name>
            <surname>Henrich Bernardoni</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Laryngeal image processing of vocal folds motion</article-title>
        <source>Appl. Sci.</source>
        <year>2020</year>
        <volume>10</volume>
        <fpage>1556</fpage>
        <pub-id pub-id-type="doi">10.3390/app10051556</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maryn</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Intersegmenter variability in high-speed laryngoscopy-based glottal area waveform measures</article-title>
        <source>Laryngoscope</source>
        <year>2020</year>
        <volume>130</volume>
        <fpage>E654</fpage>
        <lpage>E661</lpage>
        <pub-id pub-id-type="doi">10.1002/lary.28475</pub-id>
        <?supplied-pmid 31840827?>
        <pub-id pub-id-type="pmid">31840827</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schlegel</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kniesburges</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dürr</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schützenberger</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Machine learning based identification of relevant parameters for functional voice disorders derived from endoscopic high-speed recordings</article-title>
        <source>Sci. Rep.</source>
        <year>2020</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-66405-y</pub-id>
        <pub-id pub-id-type="pmid">31913322</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Cerrolaza, J. J. <italic>et al.</italic> Fully-automatic glottis segmentation with active shape models. in <italic>7th International Workshop on Models and Analysis of Vocal Emissions for Biomedical Applications, MAVEBA 2011</italic> 35–38 (Florence, Italy, 2011).</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gloger</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Lehnert</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Schrade</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Völzke</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Fully automated glottis segmentation in endoscopic videos using local color and shape features of glottal regions</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <year>2015</year>
        <volume>62</volume>
        <fpage>795</fpage>
        <lpage>806</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2014.2364862</pub-id>
        <?supplied-pmid 25350912?>
        <pub-id pub-id-type="pmid">25350912</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karakozoglou</surname>
            <given-names>S-Z</given-names>
          </name>
          <name>
            <surname>Henrich</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>d’Alessandro</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Stylianou</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Automatic glottal segmentation using local-based active contours and application to glottovibrography</article-title>
        <source>Speech Commun.</source>
        <year>2012</year>
        <volume>54</volume>
        <fpage>641</fpage>
        <lpage>654</lpage>
        <pub-id pub-id-type="doi">10.1016/j.specom.2011.07.010</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Schenk, F. <italic>et al.</italic> Automatic glottis segmentation from laryngeal high-speed videos using 3D active contours. in <italic>18th International Conference on Medical Image Understanding and Analysis (MIUA)</italic> 111–116 (2014).</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fehling</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Grosch</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Schuster</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Schick</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Lohscheller</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Fully automatic segmentation of glottis and vocal folds in endoscopic laryngeal high-speed videos using a deep Convolutional LSTM Network</article-title>
        <source>PLoS One</source>
        <year>2020</year>
        <volume>15</volume>
        <fpage>e0227791</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0227791</pub-id>
        <?supplied-pmid 32040514?>
        <pub-id pub-id-type="pmid">32040514</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gómez</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>BAGLS, a multihospital benchmark for automatic glottis segmentation</article-title>
        <source>Sci. Data</source>
        <year>2020</year>
        <volume>7</volume>
        <fpage>186</fpage>
        <pub-id pub-id-type="doi">10.1038/s41597-020-0526-3</pub-id>
        <?supplied-pmid 32561845?>
        <pub-id pub-id-type="pmid">32561845</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Laves</surname>
            <given-names>M-H</given-names>
          </name>
          <name>
            <surname>Bicker</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kahrs</surname>
            <given-names>LA</given-names>
          </name>
          <name>
            <surname>Ortmaier</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>A dataset of laryngeal endoscopic images with comparative study on convolution neural network-based semantic segmentation</article-title>
        <source>Int. J. Comput. Assist. Radiol. Surg.</source>
        <year>2019</year>
        <pub-id pub-id-type="doi">10.1007/s11548-018-01910-0</pub-id>
        <?supplied-pmid 30649670?>
        <pub-id pub-id-type="pmid">30649670</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kist</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Efficient biomedical image segmentation on EdgeTPUs at point of care</article-title>
        <source>IEEE Access</source>
        <year>2020</year>
        <volume>8</volume>
        <fpage>139356</fpage>
        <lpage>139366</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3012722</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kist</surname>
            <given-names>AM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep learning enhanced novel software tool for laryngeal dynamics analysis</article-title>
        <source>J. Speech Lang. Hear. Res.</source>
        <year>2021</year>
        <volume>64</volume>
        <issue>6</issue>
        <fpage>1889</fpage>
        <lpage>1903</lpage>
        <pub-id pub-id-type="doi">10.1044/2021_JSLHR-20-00498</pub-id>
        <?supplied-pmid 34000199?>
        <pub-id pub-id-type="pmid">34000199</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">wevosys. lingWAVES 4 High Speed Videoendoscopy (HSV). <ext-link ext-link-type="uri" xlink:href="https://www.wevosys.com/products/lingwaves4/lingwaves4_high_speed_videoendoscopy.html">https://www.wevosys.com/products/lingwaves4/lingwaves4_high_speed_videoendoscopy.html</ext-link>. Accessed 3 May 2021 (2020).</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: Convolutional networks for biomedical image segmentation. in <italic>International Conference on Medical image computing and computer-assisted intervention</italic> 234–241 (Springer, Cham, 2015).</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schlegel</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dependencies and ill-designed parameters within high-speed videoendoscopy and acoustic signal analysis</article-title>
        <source>J. Voice</source>
        <year>2019</year>
        <volume>33</volume>
        <fpage>811</fpage>
        <lpage>e1</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jvoice.2018.04.011</pub-id>
        <?supplied-pmid 29861291?>
        <pub-id pub-id-type="pmid">29861291</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Virtanen</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>SciPy 1.0: Fundamental algorithms for scientific computing in Python</article-title>
        <source>Nat. Methods</source>
        <year>2020</year>
        <volume>17</volume>
        <fpage>261</fpage>
        <lpage>272</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
        <?supplied-pmid 32015543?>
        <pub-id pub-id-type="pmid">32015543</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kist</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Zilker</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gómez</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Schützenberger</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Rethinking glottal midline detection</article-title>
        <source>Sci. Rep.</source>
        <year>2020</year>
        <volume>10</volume>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-77216-6</pub-id>
        <pub-id pub-id-type="pmid">31913322</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lohscheller</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Eysholdt</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Toy</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Phonovibrography: Mapping high-speed movies of vocal fold vibrations into 2-D diagrams for visualizing and analyzing the underlying laryngeal dynamics</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2008</year>
        <volume>27</volume>
        <fpage>300</fpage>
        <lpage>309</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2007.903690</pub-id>
        <?supplied-pmid 18334426?>
        <pub-id pub-id-type="pmid">18334426</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mittal</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Soundararajan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bovik</surname>
            <given-names>AC</given-names>
          </name>
        </person-group>
        <article-title>Making a “completely blind” image quality analyzer</article-title>
        <source>IEEE Signal Process. Lett.</source>
        <year>2012</year>
        <volume>20</volume>
        <fpage>209</fpage>
        <lpage>212</lpage>
        <pub-id pub-id-type="doi">10.1109/LSP.2012.2227726</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gómez</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Semmler</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Schützenberger</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bohr</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Low-light image enhancement of high-speed endoscopic videos using a convolutional neural network</article-title>
        <source>Med. Biol. Eng. Comput.</source>
        <year>2019</year>
        <volume>57</volume>
        <fpage>1451</fpage>
        <lpage>1463</lpage>
        <pub-id pub-id-type="doi">10.1007/s11517-019-01965-4</pub-id>
        <?supplied-pmid 30900057?>
        <pub-id pub-id-type="pmid">30900057</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Horii</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Vocal shimmer in sustained phonation</article-title>
        <source>J. Speech Lang. Hear. Res.</source>
        <year>1980</year>
        <volume>23</volume>
        <fpage>202</fpage>
        <lpage>209</lpage>
        <pub-id pub-id-type="doi">10.1044/jshr.2301.202</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bielamowicz</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kreiman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gerratt</surname>
            <given-names>BR</given-names>
          </name>
          <name>
            <surname>Dauer</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Berke</surname>
            <given-names>GS</given-names>
          </name>
        </person-group>
        <article-title>Comparison of voice analysis systems for perturbation measurement</article-title>
        <source>J. Speech Hear. Res.</source>
        <year>1996</year>
        <volume>39</volume>
        <fpage>126</fpage>
        <lpage>134</lpage>
        <pub-id pub-id-type="doi">10.1044/jshr.3901.126</pub-id>
        <?supplied-pmid 8820704?>
        <pub-id pub-id-type="pmid">8820704</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yumoto</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Gould</surname>
            <given-names>WJ</given-names>
          </name>
          <name>
            <surname>Baer</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Harmonics-to-noise ratio as an index of the degree of hoarseness</article-title>
        <source>J. Acoust. Soc. Am.</source>
        <year>1982</year>
        <volume>71</volume>
        <fpage>1544</fpage>
        <lpage>1550</lpage>
        <pub-id pub-id-type="doi">10.1121/1.387808</pub-id>
        <?supplied-pmid 7108029?>
        <pub-id pub-id-type="pmid">7108029</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hillenbrand</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cleveland</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Erickson</surname>
            <given-names>RL</given-names>
          </name>
        </person-group>
        <article-title>Acoustic correlates of breathy vocal quality</article-title>
        <source>J. Speech Lang. Hear. Res.</source>
        <year>1994</year>
        <volume>37</volume>
        <fpage>769</fpage>
        <lpage>778</lpage>
        <pub-id pub-id-type="doi">10.1044/jshr.3704.769</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Baken, R. J. &amp; Orlikoff, R. F. <italic>Clinical Measurement of Speech and Voice</italic>. (Cengage Learning, 2000).</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Holmberg</surname>
            <given-names>EB</given-names>
          </name>
          <name>
            <surname>Hillman</surname>
            <given-names>RE</given-names>
          </name>
          <name>
            <surname>Perkell</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>Glottal airflow and transglottal air pressure measurements for male and female speakers in soft, normal, and loud voice</article-title>
        <source>J. Acoust. Soc. Am.</source>
        <year>1988</year>
        <volume>84</volume>
        <fpage>511</fpage>
        <lpage>529</lpage>
        <pub-id pub-id-type="doi">10.1121/1.396829</pub-id>
        <?supplied-pmid 3170944?>
        <pub-id pub-id-type="pmid">3170944</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Henrich</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Just noticeable differences of open quotient and asymmetry coefficient in singing voice</article-title>
        <source>J. Voice</source>
        <year>2003</year>
        <volume>17</volume>
        <fpage>481</fpage>
        <lpage>494</lpage>
        <pub-id pub-id-type="doi">10.1067/S0892-1997(03)00005-5</pub-id>
        <?supplied-pmid 14740930?>
        <pub-id pub-id-type="pmid">14740930</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Kasuya, H., Endo, Y. &amp; Saliu, S. Novel acoustic measurements of jitter and shimmer characteristics from pathological voice. In <italic>Third European Conference on Speech Communication and Technology</italic> (1993).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kunduk</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>McWhorter</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Lohscheller</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Assessment of the variability of vocal fold dynamics within and between recordings with high-speed imaging and by phonovibrogram</article-title>
        <source>Laryngoscope</source>
        <year>2010</year>
        <volume>120</volume>
        <fpage>981</fpage>
        <lpage>987</lpage>
        <pub-id pub-id-type="doi">10.1002/lary.20832</pub-id>
        <?supplied-pmid 20422695?>
        <pub-id pub-id-type="pmid">20422695</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Patel</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dubrovskiy</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Characterizing vibratory kinematics in children and adults with high-speed digital imaging</article-title>
        <source>J. Speech Lang. Hear. Res.</source>
        <year>2014</year>
        <volume>57</volume>
        <fpage>S674</fpage>
        <lpage>S686</lpage>
        <pub-id pub-id-type="doi">10.1044/2014_JSLHR-S-12-0278</pub-id>
        <?supplied-pmid 24686982?>
        <pub-id pub-id-type="pmid">24686982</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schlegel</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Dependencies and ill-designed parameters within high-speed videoendoscopy and acoustic signal analysis</article-title>
        <source>J. Voice Off. J. Voice Found.</source>
        <year>2018</year>
        <pub-id pub-id-type="doi">10.1016/j.jvoice.2018.04.011</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Munhall</surname>
            <given-names>KG</given-names>
          </name>
          <name>
            <surname>Ostry</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Parush</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Characteristics of velocity profiles of speech movements</article-title>
        <source>J. Exp. Psychol. Hum. Percept. Perform.</source>
        <year>1985</year>
        <volume>11</volume>
        <fpage>457</fpage>
        <pub-id pub-id-type="doi">10.1037/0096-1523.11.4.457</pub-id>
        <?supplied-pmid 3161986?>
        <pub-id pub-id-type="pmid">3161986</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Qiu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Schutte</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>An automatic method to quantify the vibration properties of human vocal folds via videokymography</article-title>
        <source>Folia Phoniatr. Logop.</source>
        <year>2003</year>
        <volume>55</volume>
        <fpage>128</fpage>
        <lpage>136</lpage>
        <pub-id pub-id-type="doi">10.1159/000070724</pub-id>
        <?supplied-pmid 12771464?>
        <pub-id pub-id-type="pmid">12771464</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mehta</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Deliyski</surname>
            <given-names>DD</given-names>
          </name>
          <name>
            <surname>Quatieri</surname>
            <given-names>TF</given-names>
          </name>
          <name>
            <surname>Hillman</surname>
            <given-names>RE</given-names>
          </name>
        </person-group>
        <article-title>Automated measurement of vocal fold vibratory asymmetry from high-speed videoendoscopy recordings</article-title>
        <source>J. Speech Lang. Hear. Res.</source>
        <year>2011</year>
        <volume>54</volume>
        <fpage>47</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1044/1092-4388(2010/10-0026)</pub-id>
        <?supplied-pmid 20699347?>
        <pub-id pub-id-type="pmid">20699347</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Analysis of vocal fold function from acoustic data simultaneously recorded with high-speed endoscopy</article-title>
        <source>J. Voice</source>
        <year>2012</year>
        <volume>26</volume>
        <fpage>726</fpage>
        <lpage>733</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jvoice.2012.02.001</pub-id>
        <?supplied-pmid 22632795?>
        <pub-id pub-id-type="pmid">22632795</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Inwald</surname>
            <given-names>EC</given-names>
          </name>
          <name>
            <surname>Döllinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Schuster</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Eysholdt</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Bohr</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Multiparametric analysis of vocal fold vibrations in healthy and disordered voices in high-speed imaging</article-title>
        <source>J. Voice</source>
        <year>2011</year>
        <volume>25</volume>
        <fpage>576</fpage>
        <lpage>590</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jvoice.2010.04.004</pub-id>
        <?supplied-pmid 20728308?>
        <pub-id pub-id-type="pmid">20728308</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schlegel</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Influence of spatial camera resolution in high-speed videoendoscopy on laryngeal parameters</article-title>
        <source>PLoS One</source>
        <year>2019</year>
        <volume>14</volume>
        <fpage>e0215168</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0215168</pub-id>
        <?supplied-pmid 31009488?>
        <pub-id pub-id-type="pmid">31009488</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Schlegel</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <source>Assessment of Clinical Voice Parameters and Parameter Reduction Using Supervised Learning Approaches</source>
        <year>2020</year>
        <publisher-name>Friedrich-Alexander-University Erlangen-Nürnberg</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Harris</surname>
            <given-names>CR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Array programming with NumPy</article-title>
        <source>Nature</source>
        <year>2020</year>
        <volume>585</volume>
        <fpage>357</fpage>
        <lpage>362</lpage>
        <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
        <?supplied-pmid 32939066?>
        <pub-id pub-id-type="pmid">32939066</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schlegel</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Influence of analyzed sequence length on parameters in laryngeal high-speed videoendoscopy</article-title>
        <source>Appl. Sci.</source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>2666</fpage>
        <pub-id pub-id-type="doi">10.3390/app8122666</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heman-Ackah</surname>
            <given-names>YD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cepstral peak prominence: A more reliable measure of dysphonia</article-title>
        <source>Ann. Otol. Rhinol. Laryngol.</source>
        <year>2003</year>
        <volume>112</volume>
        <fpage>324</fpage>
        <lpage>333</lpage>
        <pub-id pub-id-type="doi">10.1177/000348940311200406</pub-id>
        <?supplied-pmid 12731627?>
        <pub-id pub-id-type="pmid">12731627</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
