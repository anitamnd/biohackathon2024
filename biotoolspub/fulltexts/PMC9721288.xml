<?DTDIdentifier.IdentifierValue http://null/schema/dtds/document/fulltext/xcr/xocs-article.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName xocs-article.xsd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName ftrr2jats.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Biomed Signal Process Control</journal-id>
    <journal-id journal-id-type="iso-abbrev">Biomed Signal Process Control</journal-id>
    <journal-title-group>
      <journal-title>Biomedical Signal Processing and Control</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1746-8094</issn>
    <issn pub-type="epub">1746-8094</issn>
    <publisher>
      <publisher-name>Elsevier Ltd.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9721288</article-id>
    <article-id pub-id-type="pii">S1746-8094(22)00940-5</article-id>
    <article-id pub-id-type="doi">10.1016/j.bspc.2022.104486</article-id>
    <article-id pub-id-type="publisher-id">104486</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>COVID-19 CT ground-glass opacity segmentation based on attention mechanism threshold</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au000001">
        <name>
          <surname>Rao</surname>
          <given-names>Yunbo</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au000002">
        <name>
          <surname>Lv</surname>
          <given-names>Qingsong</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au000003">
        <name>
          <surname>Zeng</surname>
          <given-names>Shaoning</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">b</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au000004">
        <name>
          <surname>Yi</surname>
          <given-names>Yuling</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au000005">
        <name>
          <surname>Huang</surname>
          <given-names>Cheng</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author" id="au000006">
        <name>
          <surname>Gao</surname>
          <given-names>Yun</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">d</xref>
      </contrib>
      <contrib contrib-type="author" id="au000007">
        <name>
          <surname>Cheng</surname>
          <given-names>Zhanglin</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">e</xref>
      </contrib>
      <contrib contrib-type="author" id="au000008">
        <name>
          <surname>Sun</surname>
          <given-names>Jihong</given-names>
        </name>
        <xref rid="aff6" ref-type="aff">f</xref>
      </contrib>
      <aff id="aff1"><label>a</label>School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, 611731, China</aff>
      <aff id="aff2"><label>b</label>Yangtze Delta Region Institute (Huzhou), University of Electronic Science and Technology of China, Huzhou, 313000, China</aff>
      <aff id="aff3"><label>c</label>Fifth Clinical College of Chongqing Medical University, Chongqing, 402177, China</aff>
      <aff id="aff4"><label>d</label>Chongqing University of Posts and Telecommunications, Chongqing, 400065, China</aff>
      <aff id="aff5"><label>e</label>Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China</aff>
      <aff id="aff6"><label>f</label>Sir Run Run Shaw Hospital, Zhejiang University School of Medicine, Hangzhou, 310014, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author.</corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>5</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>5</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <volume>81</volume>
    <fpage>104486</fpage>
    <lpage>104486</lpage>
    <history>
      <date date-type="received">
        <day>15</day>
        <month>8</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>23</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>1</day>
        <month>12</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Elsevier Ltd. All rights reserved.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Elsevier Ltd</copyright-holder>
      <license>
        <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
      </license>
    </permissions>
    <abstract id="d1e2348">
      <p>The ground glass opacity (GGO) of the lung is one of the essential features of COVID-19. The GGO in computed tomography (CT) images has various features and low-intensity contrast between the GGO and edge structures. These problems pose significant challenges for segmenting the GGO. To tackle these problems, we propose a new threshold method for accurate segmentation of GGO. Specifically, we offer a framework for adjusting the threshold parameters according to the image contrast. Three functions include Attention mechanism threshold, Contour equalization, and Lung segmentation (ACL). The lung is divided into three areas using the attention mechanism threshold. Further, the segmentation parameters of the attention mechanism thresholds of the three parts are adaptively adjusted according to the image contrast. Only the segmentation regions restricted by the lung segmentation results are retained. Extensive experiments on four COVID datasets show that ACL can segment GGO images at low contrast well. Compared with the state-of-the-art methods, the similarity Dice of the ACL segmentation results is improved by 8.9%, the average symmetry surface distance ASD is reduced by 23%, and the required computational power <inline-formula><mml:math id="d1e2355" altimg="si1.svg" display="inline"><mml:mrow><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mi>O</mml:mi><mml:mi>P</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> are only 0.09% of those of deep learning models. For GGO segmentation, ACL is more lightweight, and the accuracy is higher. Code will be released at <ext-link ext-link-type="uri" xlink:href="https://github.com/Lqs-github/ACL" id="interref2">https://github.com/Lqs-github/ACL</ext-link>.</p>
    </abstract>
    <kwd-group id="d1e2398">
      <title>Keywords</title>
      <kwd>COVID-19</kwd>
      <kwd>GGO segmentation</kwd>
      <kwd>Attention mechanism</kwd>
      <kwd>Adaptive threshold</kwd>
      <kwd>Pneumonia</kwd>
      <kwd>CT image</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p id="d1e2434">The outbreak of COVID-19 has a significant impact on the whole world <xref rid="b1" ref-type="bibr">[1]</xref>. The World Health Organization Global Case Report (updated on 11 October 2022) showed that there were more than 619.16 million confirmed cases of COVID-19, including 6.54 million deaths in 200 countries and regions around the world <xref rid="b2" ref-type="bibr">[2]</xref>. Reverse transcriptase chain reaction (RT-PCR) is the medical gold standard for diagnosing COVID-19 <xref rid="b3" ref-type="bibr">[3]</xref>. However, the shortage of medical staff and testing equipment to handle a large number of suspected cases limits the speed and accuracy of medical diagnosis. In addition, with the spread of COVID-19 virus variants reported, the false-negative rate of RT-PCR detection is also high <xref rid="b4" ref-type="bibr">[4]</xref>. CT and other imaging techniques are a good supplement, and clinical trials have also proved the effectiveness of CT imaging in diagnosing COVID-19 <xref rid="b5" ref-type="bibr">[5]</xref>. Therefore, it is crucial to have an efficient segmentation solution of CT GGO images for automatic COVID-19 diagnosis <xref rid="b6" ref-type="bibr">[6]</xref>. As shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>, we can observe the three-layer structure of COVID-19 lung CT images with low contrast, including lung background (black area), normal tissue, and GGO. However, manually mapping the GGO area is a tedious task. Marked areas are also easily influenced by subjective factors and limited by personal experience and energy.</p>
    <p id="d1e2465">Deep learning has been widely used to segment GGO areas in COVID-19. For example, Inf-net uses the constraints between the interior and edges of GGO to segment GGO <xref rid="b7" ref-type="bibr">[7]</xref>. In addition, there are image pyramids to segment targets by improving image quality <xref rid="b8" ref-type="bibr">[8]</xref> and segmentation methods to improve network models such as U-net <xref rid="b9" ref-type="bibr">[9]</xref>, <xref rid="b10" ref-type="bibr">[10]</xref>. A large number of network segmentation methods have been proposed to determine the GGO area <xref rid="b11" ref-type="bibr">[11]</xref>. Image analysis of pixels can implement a much faster segmentation which is rarely discussed.<fig id="fig1"><label>Fig. 1</label><caption><p>The left is the CT image of COVID-19’s lung with low contrast, and the right is the segmentation result by ACL (ours) on the CT image. The red part is the GGO area.</p></caption><graphic xlink:href="gr1_lrg"/></fig></p>
    <p id="d1e2486">In machine learning, threshold is commonly preferred for pixel-level segmentation tasks on images. The image pixel values are divided according to the threshold to get the segmentation target. Adaptive threshold tends to compute thresholds from global or local GGO CT image features or use multiple sets of thresholds to jointly determine the class of targets. The GGO CT image is more finely divided into multiple sub-image tasks for multi-threshold segmentation, facilitating a more accurate acquisition of segmentation targets <xref rid="b12" ref-type="bibr">[12]</xref>. However, it also faces the problem of coordinating the relationship between numerous block thresholds and uneven local contrast degree of the image, which resulted in segmented block image disconnection <xref rid="b13" ref-type="bibr">[13]</xref>. Further, local thresholds are constructed based on the class’s uncertainty and the region’s homogeneity with sub-regions of images at different scales co-associated. The details of the overall target structure are preserved, and the segmentation effect of the local threshold is obtained.</p>
    <p id="d1e2496">Segmentation of GGO can be summarized into four main problems. (1) The shape and texture of GGO regions are extremely variable, and it is not easy to find fixed features for segmentation. (2) The contrast between GGO regions and edge tissues is low, making it difficult to segment edges accurately. (3) Segmenting GGO blurred edges with a single fixed threshold is challenging. (4) The thresholds of values and their selection are problems that need to be solved. In addition, there are only a few methods to segment COVID-19 GGO based on the threshold. It is difficult to fix the threshold selection and parameter adjustment, and the segmentation effect needs to be improved <xref rid="b14" ref-type="bibr">[14]</xref>.</p>
    <p id="d1e2502">We propose a threshold-based attention mechanism to segment GGO regions in COVID-19 images. Three functions are designed: Attention mechanism threshold to segment regions, contour equalization, and lung segmentation (ACL). Our motivation is that clinicians use prior information first to determine the approximate location of lung infection. Furthermore, we compare inner lung tissues and the empirical relationship between internal lung tissues and normal lung contour <xref rid="b15" ref-type="bibr">[15]</xref>. The attention mechanism Threshold roughly separates the lung background, GGO area, and normal tissue. Then, the GGO area is determined according to the empirical relationship between image contrast quality and normal lung contour. Finally, balance the refined contour of the GGO area and normal tissue. In addition, to further verify the features of this method in CT image segmentation, the segmentation effect of various common pneumonia is also confirmed. Specifically, our adaptive threshold segmentation method has a fast and accurate segmentation ability in the specific case of lung GGO segmentation in CT images, which is better than some of the cutting-edge models and performs better.</p>
    <p id="d1e2508">To sum up, our contributions are as follows:</p>
    <p id="d1e2510">
      <list list-type="simple" id="d1e2512">
        <list-item id="lst1">
          <label>(1)</label>
          <p id="d1e2516">We present a new efficient attention mechanism threshold method, including Attention mechanism threshold, Contour equalization, and Lung segmentation (ACL). It can accurately segment GGO without complicated training processes.</p>
        </list-item>
        <list-item id="lst2">
          <label>(2)</label>
          <p id="d1e2525">The contour equalization in ACL can solve the situation of extreme contrast. Classifying CT images according to their quality can refine different contours and improve segmentation accuracy.</p>
        </list-item>
        <list-item id="lst3">
          <label>(3)</label>
          <p id="d1e2530">ACL can also accurately segment ordinary pneumonia. The experimental results on four pneumonia segmentation data sets show that the segmentation effect is better than the frontier models.</p>
        </list-item>
      </list>
    </p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Related work</title>
    <p id="d1e2537">This section discusses three types of work most relevant to our work. (1) GGO segmentation based on deep learning networks. (2) GGO segmentation without deep neural networks. (3) Artificial intelligence progresses for COVID-19 diagnosis.</p>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Deep learning in segmentation of lung diseases</title>
      <p id="d1e2544">Since the outbreak of COVID-19, more and more deep neural networks have been proposed for the segmentation of lung GGO. Deep learning solutions can be classified into three categories: First is deep neural networks with an attention mechanism trained by a primary information constraint <xref rid="b16" ref-type="bibr">[16]</xref>. For example, Inf-net obtained the low-level semantic features of contours in advance and then performed a network training to segment the GGO areas of chest CT <xref rid="b7" ref-type="bibr">[7]</xref>, <xref rid="b17" ref-type="bibr">[17]</xref>. It was also influential in connecting an attention-rejecting network with an interactive attention-thinning network to segment the infected regions of GGO <xref rid="b18" ref-type="bibr">[18]</xref>. Besides these, there are many other promising solutions, including extended convolution residual attention block, enhancement of receptive field, and improvement of feature map segmentation <xref rid="b19" ref-type="bibr">[19]</xref>. All these methods paid attention to the fact that the known information can be considered a constraint to limit the network training further <xref rid="b20" ref-type="bibr">[20]</xref>. However, the blurred edges make it difficult to segment the contour due to the problem of low contrast between the GGO area and the lung background.</p>
      <p id="d1e2567">The next group of deep segmentation methods was designed to improve the quality of CT images. For example, multi-scale feature image fusion and enhancement networks were applied to segment GGO <xref rid="b21" ref-type="bibr">[21]</xref>. A novel data augmentation based on Gabor filter and convolutional deep learning for improving COVID-19 images <xref rid="b22" ref-type="bibr">[22]</xref>, and Dense GAN and multi-layer attention to segment GGO <xref rid="b23" ref-type="bibr">[23]</xref>.<fig id="fig2"><label>Fig. 2</label><caption><p>Flow chart of GGO segmentation and lung segmentation. The input is the CT lung slice of the patient, and the output is the map indicating the GGO area. The three functions of attention mechanism threshold (A), contour equalization (C), and lung threshold segmentation (L) in the graph are the method ACL.</p></caption><graphic xlink:href="gr2_lrg"/></fig><fig id="fig3"><label>Fig. 3</label><caption><p>Attention mechanism threshold. <inline-formula><mml:math id="d1e47" altimg="si2.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="d1e57" altimg="si3.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e67" altimg="si4.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> divide the image pixels into three areas <inline-formula><mml:math id="d1e78" altimg="si5.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="d1e88" altimg="si6.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e98" altimg="si7.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>.</p></caption><graphic xlink:href="gr3_lrg"/></fig></p>
      <p id="d1e2586">The final design is combing the attention mechanism and image enhancement. For example, CNN based on Sugeno fuzzy integral was devoted to detecting GGO in COVID-19 <xref rid="b24" ref-type="bibr">[24]</xref>. Anam-net improved by a lightweight CNN was applied to segment GGO <xref rid="b25" ref-type="bibr">[25]</xref>, <xref rid="b26" ref-type="bibr">[26]</xref>. In particular, detecting GGO can also be utilized feature selection based on particle swarm optimization algorithm, and ant colony algorithm <xref rid="b27" ref-type="bibr">[27]</xref>. However, deep neural networks are also trained on the premise of limiting adequate prior information and improving the image quality <xref rid="b28" ref-type="bibr">[28]</xref>. Therefore, when the prior information is enough <xref rid="b29" ref-type="bibr">[29]</xref>, it is not necessary to train these intense networks using a great deal of computation.</p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>Non-network in segmentation of lung diseases</title>
      <p id="d1e2614">The good news is that non-network methods showed their promising performance in the segmentation of lung GGO. Unlike deep neural networks, two separate tasks are usually involved in non-network models. One is the segmentation of the lung contour. Another is the final segmentation of the GGO. Segmentation of lung based on the morphological operation <xref rid="b30" ref-type="bibr">[30]</xref>. Combine Gabor filter for threshold segmentation <xref rid="b31" ref-type="bibr">[31]</xref>. Automatic multi-organ segmentation of abdominal CT volume based on locally linear embedding graph segmentation <xref rid="b32" ref-type="bibr">[32]</xref>. These methods all show fast and accurate results in lung contour segmentation. The other is to segment the GGO area: (1) The global task is split into local segmentation tasks, and the lung nodule regions are obtained and merged <xref rid="b33" ref-type="bibr">[33]</xref>. (2) Image features are enhanced by increasing the image resolution to segment lung nodules <xref rid="b34" ref-type="bibr">[34]</xref>. (3) Features are combined with support vector machines for lung cancer segmentation <xref rid="b35" ref-type="bibr">[35]</xref>. Moreover, distinguishing from deep learning methods, non-network methods usually apply two distinct algorithms for the two segmentation tasks, i.e., lung contour segmentation and GGO segmentation <xref rid="b36" ref-type="bibr">[36]</xref>.</p>
      <p id="d1e2645">However, a general problem in these methods is that it is complicated to choose their optimal parameters <xref rid="b37" ref-type="bibr">[37]</xref>. For example, selecting the size of the operator in image preprocessing is challenging <xref rid="b33" ref-type="bibr">[33]</xref>. It is not easy to choose the appropriate segmentation points for the image when threshold segmentation also <xref rid="b38" ref-type="bibr">[38]</xref>. We observed that non-network methods have some advantages over deep learning. Most methods are based on fast pixel segmentation and do not require extensive training <xref rid="b39" ref-type="bibr">[39]</xref>, threshold-based segmentation used in this work considers the change of the relationship between shape and position. It leads to a better performance of the non-network method than deep learning in lung contour segmentation.</p>
      <p id="d1e2664">CT image GGO is still not accurate enough by threshold segmentation. It is mainly due to the difficulty of parameter selection. It motivates us to explore a new schema. Let the segmentation automatically adjust its parameters subject to the quality of the images. Much preliminary information is available when segmenting the GGO area of lung CT images. For example, one-dimensional CT rapid localization of lung cavity <xref rid="b5" ref-type="bibr">[5]</xref>. The characteristic information of pneumonia <xref rid="b40" ref-type="bibr">[40]</xref> and so on are all useful prior information. We consider the corresponding parameters of various situations as preliminary information to generate a mapping relationship between the input images and the parameters.</p>
    </sec>
    <sec id="sec2.3">
      <label>2.3</label>
      <title>AI progresses for COVID-19</title>
      <p id="d1e2679">AI progresses for COVID-19, especially the techniques represented by the aforementioned automatic segmentation methods, and has been widely used in COVID-19 diagnosis. The successful applications include diagnosis of suspected patient <xref rid="b7" ref-type="bibr">[7]</xref>, <xref rid="b41" ref-type="bibr">[41]</xref>, development of patient’s condition, as well as their follow-up cure <xref rid="b42" ref-type="bibr">[42]</xref>. Segmentation of GGO areas in CT images is beneficial for understanding a patient’s specific conditions. Deep neural evolution algorithms can directly analyse whether the patient is sick or not in diagnosing suspected patients. For example, there are three types of pneumonia diagnosis based on the architecture, including confirmed COVID-19, confirmed virus, and bacterial cases. Diagnosis of suspected patients can be implemented by Convolutional Neural Networks (CNN) and dynamic feature selection. The prediction effect of these methods on COVID-19 is similar to that of radiologists. In addition, quantitative results by segmentation can be used to quantify the infection status <xref rid="b43" ref-type="bibr">[43]</xref>, <xref rid="b44" ref-type="bibr">[44]</xref>. Furthermore, for example, an intelligent analysis system based on local binary patterns and patient information cloud system doctors for a better diagnosis and follow-up treatment. In a word, supported by an efficient segmentation technique, AI can significantly facilitate the diagnosis of COVID-19 <xref rid="b45" ref-type="bibr">[45]</xref>.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Methodology</title>
    <p id="d1e2703">In this section, we detail the implementation of the attention mechanism threshold segmentation of the lung cavity segmentation algorithm. As shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>, the process is divided into GGO segmentation and lung segmentation. (1) The GGO segmentation consists of two functions: attention mechanism threshold and contour equalization. (2) The lung lumen segmentation is obtained from the designed lung threshold segmentation function. The complete process threshold segmentation of GGO is realized.</p>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Overview of ACL</title>
      <p id="d1e2714">Our segmentation method is shown in <xref rid="fig2" ref-type="fig">Fig. 2</xref>. The CT lung image of the patient first divides the lung region of the image into four equal parts. The CT image is divided into three areas by three threshold segmentation functions. Calculate the contrast of the image by using the designed contrast Eqs. <xref rid="fd1" ref-type="disp-formula">(1)</xref>, <xref rid="fd2" ref-type="disp-formula">(2)</xref>. According to different contrast values, the contour of the GGO area is refined. Finally, the segmentation result of the GGO area is combined with the segmentation result of the lung cavity, which is called the final labelling result of the GGO area. Only the segmentation region of the GGO in the lung cavity is retained. Notably, in this framework, we do not adjust the specific parameters. According to the quality of the image itself, we adopted a uniform threshold and segmentation standard. We use the GGO matrix and the standard tissue matrix in the intermediate process instead of operating on the original image. We only make the final colouring mark on the original image to display the predicted lung infection area. Next, we will introduce the key components of the GGO segmentation, attention mechanism threshold, contour equalization, lung segmentation and parameters of the algorithms.</p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>GGO segmentation</title>
      <sec id="sec3.2.1">
        <label>3.2.1</label>
        <title>Attention mechanism threshold</title>
        <p id="d1e2735">Most of the COVID-19 CT image histograms are regularly distributed through statistical analysis of segmented data sets. Therefore, threshold segmentation can roughly distinguish the specific tissue region and background of the GGO region. As shown in <xref rid="fig3" ref-type="fig">Fig. 3</xref>, we first use the OSTU threshold method to determine the threshold of the image and divide the image pixels into two parts. Then, the second threshold is divided for the lower grey value. We separate the parts more considerably than the second threshold in the whole image. Finally, the third threshold segmentation divides the lung into three parts. Both the GGO region (<inline-formula><mml:math id="d1e2743" altimg="si6.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="d1e2753" altimg="si7.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) and specific tissue region (<inline-formula><mml:math id="d1e2765" altimg="si6.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="d1e2775" altimg="si7.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) exist in the second threshold segmentation, and all pixels are more diminutive than the second threshold segmentation are the background regions <inline-formula><mml:math id="d1e2785" altimg="si5.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>.</p>
        <p id="d1e2795">In the process of obtaining the threshold, we used <inline-formula><mml:math id="d1e2798" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e2814" altimg="si14.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e2833" altimg="si15.svg" display="inline"><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula>, but the actual effect was very poor. We realize that the dividing line of the histogram between normal tissue and the GGO area in the local area is constantly changing. Therefore, in theory, <inline-formula><mml:math id="d1e2848" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e2864" altimg="si4.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> cannot separate the GGO area from normal tissue. Then, as shown in the Alg. 1, it is a reasonable segmentation method to separate the GGO area from the normal tissue by the separation threshold <inline-formula><mml:math id="d1e2874" altimg="si34.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> of each block of the image. At the same time, we find that the change of the second threshold between blocks is usually small, and the second threshold is used to distinguish the background from other features. Therefore, <inline-formula><mml:math id="d1e2886" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e2903" altimg="si2.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e2913" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e2929" altimg="si3.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are used to separate the background, and then combine the GGO areas of each block separated by <inline-formula><mml:math id="d1e2939" altimg="si34.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. Finally, a GGO map of the whole lung is formed.</p>
      </sec>
      <sec id="sec3.2.2">
        <label>3.2.2</label>
        <title>Contour equalization</title>
        <p id="d1e2955">After obtaining the regions of <inline-formula><mml:math id="d1e2958" altimg="si24.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, the contour needs to be further refined. Threshold segmentation is essentially the classification of pixels <xref rid="b46" ref-type="bibr">[46]</xref>, and the grey level of outline tends to decrease continuously as shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>, resulting in that even the GGO area will be segmented into normal tissues <xref rid="b47" ref-type="bibr">[47]</xref>. Therefore, we expand and erode the affected and the average regions to a certain extent. However, this operation will lead to pixel loss in the area, we use filtering to compensate. According to the image quality, the parameters such as swelling, corrosion, and filter compensation are set to balance the contour of the GGO area and normal tissue and obtain more accurate segmentation results. The above method can solve most cases well, but segmenting the whole image with low or high brightness is challenging. As shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>, this is because under highly high and external brightness conditions, the grey value of the GGO area is relatively consistent with that of normal tissue, and it is difficult to distinguish it through the threshold relationship directly. It is necessary to revise the shape of the results of threshold processing. <fig id="dfig1"><graphic xlink:href="fx1001_lrg"/></fig>
</p>
        <p id="d1e3012">In this extreme case, our inspiration mainly comes from how clinicians judge the GGO area. Usually, we first locate the lung cavity, then we see a large size of highly dark grey or a large area of bright light in the lung. Then, it is judged that these two types of sites are pathological areas. Therefore, we classify the images into three categories according to the contrast quality of the images and discuss the parameters of swelling, corrosion and filtering compensation to balance the contours of the GGO area and normal tissue and obtain more accurate segmentation results. The contrast value is calculated as follows. <fig id="dfig2"><graphic xlink:href="fx1002_lrg"/></fig>
</p>
        <p id="d1e3025">Image size is <inline-formula><mml:math id="d1e3028" altimg="si25.svg" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="d1e3038" altimg="si26.svg" display="inline"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents the pixel value at the image <inline-formula><mml:math id="d1e3055" altimg="si27.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. The total variance (<inline-formula><mml:math id="d1e3071" altimg="si28.svg" display="inline"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:math></inline-formula>) of the image is: <disp-formula-group id="fd1"><label>(1)</label><disp-formula id="d1e3086"><mml:math id="d1e3087" altimg="si30.svg" class="aligned" display="block"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msup id="mmlalignd1e3130"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo linebreak="goodbreak">+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo id="mmlalignd1e3230" indentalign="id" indenttarget="mmlalignd1e3130" linebreak="newline">+</mml:mo><mml:mspace width="0.16667em"/><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo linebreak="goodbreak">+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></disp-formula-group>and image contrast value is: <disp-formula-group id="fd2"><label>(2)</label><disp-formula id="d1e3337"><mml:math id="d1e3338" altimg="si33.svg" class="aligned" display="block"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>3</mml:mn><mml:mo linebreak="badbreak">∗</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>m</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>4</mml:mn><mml:mo linebreak="badbreak">∗</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></disp-formula-group>
</p>
      </sec>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Lung segmentation</title>
      <p id="d1e3445">Because of the similarity between the GGO area and tissue, as shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref>, it is difficult to separate the lung cavity area from the whole image. Hence, we design a new lung segmentation algorithm with the adaptive threshold to divide the entire lung.</p>
      <p id="d1e3451">The lung tissues will be separated together. It is worth noting that in Alg. 1, <inline-formula><mml:math id="d1e3460" altimg="si34.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> can be used to distinguish the GGO area from normal tissue. We can use the attributes of <inline-formula><mml:math id="d1e3473" altimg="si34.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to binarize the original image. At the same time, we can roughly distinguish the lung contour from the lung tissue. As shown in <xref rid="fig6" ref-type="fig">Fig. 6</xref>, <inline-formula><mml:math id="d1e3489" altimg="si5.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the background part, <inline-formula><mml:math id="d1e3499" altimg="si6.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the lung part, and <inline-formula><mml:math id="d1e3510" altimg="si7.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the intrapulmonary part. Filtering the whole lung can separate the lung cavity well.<fig id="fig4"><label>Fig. 4</label><caption><p>The grey value of GGO contour in different contrast CT images gradually decreases.</p></caption><graphic xlink:href="gr4_lrg"/></fig><fig id="fig5"><label>Fig. 5</label><caption><p>GGO in extremely high brightness and extremely low brightness CT images. The left shows a CT GGO image with extremely high brightness and shallow contrast. The right CT GGO image has extremely low brightness and contrast.</p></caption><graphic xlink:href="gr5_lrg"/></fig><fig id="fig6"><label>Fig. 6</label><caption><p>Lung segmentation process. (a) is the result of lung cavity filling, (b) denotes the binarization result using <inline-formula><mml:math id="d1e133" altimg="si34.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, (c) represents the result of the closed operation, and (d) expresses the result of lung segmentation.</p></caption><graphic xlink:href="gr6_lrg"/></fig></p>
      <p id="d1e3519">Although there are burrs and depressions at the edges of the lung contour, as shown in <xref rid="fig6" ref-type="fig">Fig. 6</xref>(d), Alg. 1 and Alg. 3 only separate the pixels with the same grey value of the GGO, burrs will not be marked as the GGO area. In addition, since the lung cavity divided by the threshold of global <inline-formula><mml:math id="d1e3526" altimg="si34.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is adopted, it has been proved that the concave part is normal tissue, and the concave part will not be marked as the GGO area. <fig id="dfig3"><graphic xlink:href="fx1003_lrg"/></fig>
</p>
    </sec>
    <sec id="sec3.4">
      <label>3.4</label>
      <title>Parameters of the ACL</title>
      <p id="d1e3551">ACL algorithm parameters are divided into two types: fixed and adaptively computed.</p>
      <p id="d1e3553">Attention Mechanism Threshold: All parameters are obtained from the adaptive calculation of Alg. 1.<fig id="fig7"><label>Fig. 7</label><caption><p>ACL actual effect comparison chart. The first column shows the original CT lung image from COV1, COV2, COV3 and COV4, which is classified into <inline-formula><mml:math id="d1e153" altimg="si41.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mfenced open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e181" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e201" altimg="si43.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="d1e219" altimg="si44.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e240" altimg="si45.svg" display="inline"><mml:mfenced><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e259" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e279" altimg="si47.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="d1e297" altimg="si44.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e318" altimg="si49.svg" display="inline"><mml:mfenced close=")"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. The second to fifth columns are the GGO segmentation results of Gate-UNet, Inf, JCS, and ACL (ours). The last column is Ground True. The top right corner of each resulting image shows <inline-formula><mml:math id="d1e339" altimg="si50.svg" display="inline"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> (%) and <inline-formula><mml:math id="d1e351" altimg="si51.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>.</p></caption><graphic xlink:href="gr7_lrg"/></fig><fig id="fig8"><label>Fig. 8</label><caption><p>Dice of ACL and three methods in different contrast CT images. The ordinate is the <inline-formula><mml:math id="d1e369" altimg="si50.svg" display="inline"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> index, and the abscissa is image contrast. The Dice of four methods of contrast image at every five units is tested. Orange shows Gate-UNet, purple represents Inf-Net, blue expresses JCS, and green is ACL.</p></caption><graphic xlink:href="gr8_lrg"/></fig></p>
      <p id="d1e3559">Contour Equalization: <inline-formula><mml:math id="d1e3562" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e3582" altimg="si54.svg" display="inline"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> 1 is set as 150. <inline-formula><mml:math id="d1e3596" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e3617" altimg="si54.svg" display="inline"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> 2 is taken as 300, and the size of the empty matrix is 3 × 3 for disconnected domain and expansion operation. The mean filter template is 3 × 3. These parameters are the default preferred parameters built into the respective algorithms.</p>
      <p id="d1e3630">Lung Segmentation: The size of the closed operation is the empty matrix of 3 × 3. The binarization parameters are calculated by the algorithm adaptively.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Experiments</title>
    <sec id="sec4.1">
      <label>4.1</label>
      <title>Segmentation dataset</title>
      <p id="d1e3642">The COVID CT segmentation datasets consists of four parts. Two sets of COVID-19 data sets are segmented by a radiologist using three labels, including ground glass, nodules, and pleural effusion. The COVID CT segmentation datasets were published on April 2, 2020, and April 13, 2020, respectively <xref rid="b48" ref-type="bibr">[48]</xref>. In the experiments, we only used CT images with GGO labels. Another group of COVID-19 is the COVID CT segmentation data set collected by Zhao on April 8, 2020 <xref rid="b49" ref-type="bibr">[49]</xref>. Zhao collected the non-COVID CT segmentation dataset from June 2011 to the present and applied it to other pneumonia segmentation experiments. Finally, we also use the Kaggle dataset <xref rid="b12" ref-type="bibr">[12]</xref> for GGO segmentation testing as shown in <xref rid="tbl1" ref-type="table">Table 1</xref>.</p>
      <p id="d1e3661">Since mixing COVID-19 virus datasets lead to performance bias <xref rid="b51" ref-type="bibr">[51]</xref>, we independently conduct segmentation experiments on the four COVID datasets. The rest of the non-COVID-19 images were combined for extended testing. In total, the test results of the state-of-the-art COVID segmentation method and the state-of-the-art pneumonia segmentation method are compared in four COVID datasets.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Public CT image segmentation dataset of COVID-19 and the non-COVID CT image segmentation dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">#COV #Non-COV</th><th align="left">Cases</th><th align="left">Slices</th><th align="left">Download</th></tr></thead><tbody><tr><td align="left">COVID-19 <break/>(2020.4.2)</td><td align="left">COV1</td><td align="left">110</td><td align="left" rowspan="2">3700</td><td align="left"><xref rid="b48" ref-type="bibr">[48]</xref></td></tr><tr><td align="left">COVID-19 <break/>(2020.4.13)</td><td align="left">COV2</td><td align="left">350</td><td align="left"><xref rid="b48" ref-type="bibr">[48]</xref></td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left">COVID-CT <break/>(2020.4.8)</td><td align="left">COV3</td><td align="left">216</td><td align="left">349</td><td align="left"><xref rid="b50" ref-type="bibr">[50]</xref></td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left">COVID-CT <break/>(2011.6.15)</td><td align="left">Non-COV</td><td align="left">133</td><td align="left">397</td><td align="left"><xref rid="b50" ref-type="bibr">[50]</xref></td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left">COVID-19 <break/>(Kaggle)</td><td align="left">COV4</td><td align="left">–</td><td align="left">100</td><td align="left"><xref rid="b12" ref-type="bibr">[12]</xref></td></tr></tbody></table></table-wrap></p>
      <p id="d1e3669">The experimental environment of ACL is as follows. With Inter(R) Core(TM) i7-10700 CPU &amp; GeForce RTX 3070 hardware and Matlab2020 software, ACL segmented 4149 CT GGO images in a total time of 387.655 s.</p>
    </sec>
    <sec id="sec4.2">
      <label>4.2</label>
      <title>Evaluation indicators</title>
      <p id="d1e3676">We use three widely used evaluation indicators to subdivide specificity (<inline-formula><mml:math id="d1e3680" altimg="si57.svg" display="inline"><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>), sensitivity (<inline-formula><mml:math id="d1e3689" altimg="si58.svg" display="inline"><mml:mrow><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula>), and Dice similarity coefficient. We also introduce two golden indexes in medical image segmentation: relative volume difference (<inline-formula><mml:math id="d1e3699" altimg="si59.svg" display="inline"><mml:mrow><mml:mi>R</mml:mi><mml:mi>V</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>) and average symmetrical surface distance (<inline-formula><mml:math id="d1e3710" altimg="si51.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>). We can know the specific difference between the predicted and actual contour. The formula is as follows:</p>
      <p id="d1e3719">Relative Volume Difference (<inline-formula><mml:math id="d1e3723" altimg="si59.svg" display="inline"><mml:mrow><mml:mi>R</mml:mi><mml:mi>V</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula>) indicates the volume difference between the predicted and actual labels, <disp-formula id="fd3"><label>(3)</label><mml:math id="d1e3739" altimg="si62.svg" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>V</mml:mi><mml:mi>D</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">∗</mml:mo><mml:mn>100</mml:mn><mml:mtext>%</mml:mtext><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="d1e3791" altimg="si63.svg" display="inline"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the outline of actual segmentation, and <inline-formula><mml:math id="d1e3806" altimg="si64.svg" display="inline"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> expresses the outline of the actual label. Therefore, we know the overall gap between the predicted contour and the actual contour of the GGO area. <inline-formula><mml:math id="d1e3818" altimg="si51.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:math></inline-formula> indicates the specific average distance between the predicted contour and the label contour. <disp-formula-group id="fd4"><label>(4)</label><disp-formula id="d1e3833"><mml:math id="d1e3834" altimg="si67.svg" class="aligned" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfenced id="mmlalignd1e3857" open="{"><mml:mrow><mml:mo>∀</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula><disp-formula id="d1e3880"><mml:math id="d1e3881" altimg="si68.svg" class="aligned" display="block"><mml:mrow><mml:mfenced close="}" id="mmlalignd1e3887"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:mo>∃</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></disp-formula-group>
<disp-formula id="fd5"><label>(5)</label><mml:math id="d1e3975" altimg="si69.svg" display="block"><mml:mrow><mml:mi>A</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="d1e4028" altimg="si70.svg" display="inline"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the pixels of the boundary in the predicted <inline-formula><mml:math id="d1e4044" altimg="si71.svg" display="inline"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4060" altimg="si72.svg" display="inline"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is ground true. <inline-formula><mml:math id="d1e4073" altimg="si73.svg" display="inline"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the nearest pixel to the prediction boundary of <inline-formula><mml:math id="d1e4089" altimg="si70.svg" display="inline"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="d1e4105" altimg="si75.svg" display="inline"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> refers to the set of pixels closest to the real contour.</p>
      <p id="d1e4116">
        <table-wrap position="anchor" id="tbl2">
          <label>Table 2</label>
          <caption>
            <p>Comparison of indicators of various advanced methods.</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left">Method</th>
                <th align="left">FLOPs</th>
                <th colspan="5" align="left">COV1<hr/></th>
                <th colspan="5" align="left">COV2<hr/></th>
              </tr>
              <tr>
                <th align="left"/>
                <th align="left"/>
                <th align="left">Dice</th>
                <th align="left">SP</th>
                <th align="left">SE</th>
                <th align="left">RVD</th>
                <th align="left">ASD</th>
                <th align="left">Dice</th>
                <th align="left">SP</th>
                <th align="left">SE</th>
                <th align="left">RVD</th>
                <th align="left">ASD</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left">U-Net <xref rid="b52" ref-type="bibr">[52]</xref></td>
                <td align="left">38.1G</td>
                <td align="left">0.439</td>
                <td align="left">0.858</td>
                <td align="left">0.534</td>
                <td align="left">4.713</td>
                <td align="left">1.521</td>
                <td align="left">0.432</td>
                <td align="left">0.812</td>
                <td align="left">0.513</td>
                <td align="left">5.148</td>
                <td align="left">1.530</td>
              </tr>
              <tr>
                <td align="left">U-Net++ <xref rid="b53" ref-type="bibr">[53]</xref></td>
                <td align="left">65.9G</td>
                <td align="left">0.581</td>
                <td align="left">0.902</td>
                <td align="left">0.672</td>
                <td align="left">4.152</td>
                <td align="left">1.208</td>
                <td align="left">0.548</td>
                <td align="left">0.874</td>
                <td align="left">0.607</td>
                <td align="left">4.424</td>
                <td align="left">1.225</td>
              </tr>
              <tr>
                <td align="left">At-UNet++ <xref rid="b25" ref-type="bibr">[25]</xref></td>
                <td align="left">31.7G</td>
                <td align="left">0.583</td>
                <td align="left">0.921</td>
                <td align="left">0.637</td>
                <td align="left">3.831</td>
                <td align="left">1.204</td>
                <td align="left">0.548</td>
                <td align="left">0.885</td>
                <td align="left">0.593</td>
                <td align="left">4.105</td>
                <td align="left">1.293</td>
              </tr>
              <tr>
                <td align="left">Dense-UNet <xref rid="b54" ref-type="bibr">[54]</xref></td>
                <td align="left">43.7G</td>
                <td align="left">0.515</td>
                <td align="left">0.840</td>
                <td align="left">0.594</td>
                <td align="left">4.376</td>
                <td align="left">1.386</td>
                <td align="left">0.474</td>
                <td align="left">0.788</td>
                <td align="left">0.586</td>
                <td align="left">4.628</td>
                <td align="left">1.430</td>
              </tr>
              <tr>
                <td align="left">Gate-UNet <xref rid="b55" ref-type="bibr">[55]</xref></td>
                <td align="left">714.4G</td>
                <td align="left">0.623</td>
                <td align="left">0.826</td>
                <td align="left">0.658</td>
                <td align="left">3.192</td>
                <td align="left">0.913</td>
                <td align="left">0.615</td>
                <td align="left">0.816</td>
                <td align="left">0.625</td>
                <td align="left">3.317</td>
                <td align="left">0.945</td>
              </tr>
              <tr>
                <td align="left">Inf-Net <xref rid="b7" ref-type="bibr">[7]</xref></td>
                <td align="left">13.9G</td>
                <td align="left">0.682</td>
                <td align="left">0.943</td>
                <td align="left">0.792</td>
                <td align="left">2.513</td>
                <td align="left">0.769</td>
                <td align="left">0.631</td>
                <td align="left">0.914</td>
                <td align="left">0.723</td>
                <td align="left">2.608</td>
                <td align="left">0.811</td>
              </tr>
              <tr>
                <td align="left">2D-UNet <xref rid="b19" ref-type="bibr">[19]</xref></td>
                <td align="left">42.8G</td>
                <td align="left">0.768</td>
                <td align="left">0.935</td>
                <td align="left">0.842</td>
                <td align="left">1.834</td>
                <td align="left">0.624</td>
                <td align="left">0.699</td>
                <td align="left">0.849</td>
                <td align="left">0.781</td>
                <td align="left">1.929</td>
                <td align="left">0.680</td>
              </tr>
              <tr>
                <td align="left">JCS <xref rid="b41" ref-type="bibr">[41]</xref></td>
                <td align="left">51.4G</td>
                <td align="left">
                  <bold>0.785</bold>
                </td>
                <td align="left">0.937</td>
                <td align="left">0.814</td>
                <td align="left">1.741</td>
                <td align="left">0.620</td>
                <td align="left">0.746</td>
                <td align="left">0.872</td>
                <td align="left">0.778</td>
                <td align="left">1.789</td>
                <td align="left">0.654</td>
              </tr>
              <tr>
                <td align="left">LCOV-Net <xref rid="b56" ref-type="bibr">[56]</xref></td>
                <td align="left">35.1G</td>
                <td align="left">0.776</td>
                <td align="left">0.929</td>
                <td align="left">0.821</td>
                <td align="left">1.761</td>
                <td align="left">0.617</td>
                <td align="left">0.740</td>
                <td align="left">0.921</td>
                <td align="left">
                  <bold>0.812</bold>
                </td>
                <td align="left">1.890</td>
                <td align="left">0.647</td>
              </tr>
              <tr>
                <td align="left">LwMLA-NET <xref rid="b12" ref-type="bibr">[12]</xref></td>
                <td align="left">2.1G</td>
                <td align="left">0.767</td>
                <td align="left">0.917</td>
                <td align="left">
                  <bold>0.845</bold>
                </td>
                <td align="left">
                  <bold>1.759</bold>
                </td>
                <td align="left">0.631</td>
                <td align="left">
                  <bold>0.753</bold>
                </td>
                <td align="left">0.922</td>
                <td align="left">0.810</td>
                <td align="left">1.826</td>
                <td align="left">0.651</td>
              </tr>
              <tr>
                <td colspan="12">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td align="left">ACL (Ours)</td>
                <td align="left">
                  <bold>13.2M</bold>
                </td>
                <td align="left">0.771</td>
                <td align="left">
                  <bold>0.931</bold>
                </td>
                <td align="left">0.817</td>
                <td align="left">1.766</td>
                <td align="left">
                  <bold>0.592</bold>
                </td>
                <td align="left">0.736</td>
                <td align="left">
                  <bold>0.925</bold>
                </td>
                <td align="left">0.807</td>
                <td align="left">
                  <bold>1.781</bold>
                </td>
                <td align="left">
                  <bold>0.634</bold>
                </td>
              </tr>
              <tr>
                <td colspan="12">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td align="left">Method</td>
                <td align="left">FLOPs</td>
                <td colspan="5" align="left">COV3<hr/></td>
                <td colspan="5" align="left">COV4<hr/></td>
              </tr>
              <tr>
                <td align="left"/>
                <td align="left"/>
                <td align="left">Dice</td>
                <td align="left">SP</td>
                <td align="left">SE</td>
                <td align="left">RVD</td>
                <td align="left">ASD</td>
                <td align="left">Dice</td>
                <td align="left">SP</td>
                <td align="left">SE</td>
                <td align="left">RVD</td>
                <td align="left">ASD</td>
              </tr>
              <tr>
                <td colspan="12">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td align="left">U-Net <xref rid="b52" ref-type="bibr">[52]</xref></td>
                <td align="left">–</td>
                <td align="left">0.362</td>
                <td align="left">0.762</td>
                <td align="left">0.490</td>
                <td align="left">5.482</td>
                <td align="left">1.593</td>
                <td align="left">0.367</td>
                <td align="left">0.768</td>
                <td align="left">0.478</td>
                <td align="left">5.610</td>
                <td align="left">1.799</td>
              </tr>
              <tr>
                <td align="left">U-Net++ <xref rid="b53" ref-type="bibr">[53]</xref></td>
                <td align="left">–</td>
                <td align="left">0.471</td>
                <td align="left">0.865</td>
                <td align="left">0.591</td>
                <td align="left">4.532</td>
                <td align="left">1.366</td>
                <td align="left">0.483</td>
                <td align="left">0.749</td>
                <td align="left">0.556</td>
                <td align="left">4.586</td>
                <td align="left">1.822</td>
              </tr>
              <tr>
                <td align="left">At-UNet++ <xref rid="b25" ref-type="bibr">[25]</xref></td>
                <td align="left">–</td>
                <td align="left">0.494</td>
                <td align="left">0.843</td>
                <td align="left">0.592</td>
                <td align="left">4.141</td>
                <td align="left">1.316</td>
                <td align="left">0.495</td>
                <td align="left">0.717</td>
                <td align="left">0.583</td>
                <td align="left">4.525</td>
                <td align="left">1.420</td>
              </tr>
              <tr>
                <td align="left">Dense-UNet <xref rid="b54" ref-type="bibr">[54]</xref></td>
                <td align="left">–</td>
                <td align="left">0.506</td>
                <td align="left">0.795</td>
                <td align="left">0.593</td>
                <td align="left">4.617</td>
                <td align="left">1.421</td>
                <td align="left">0.520</td>
                <td align="left">0.766</td>
                <td align="left">0.579</td>
                <td align="left">4.649</td>
                <td align="left">1.780</td>
              </tr>
              <tr>
                <td align="left">Gate-UNet <xref rid="b55" ref-type="bibr">[55]</xref></td>
                <td align="left">–</td>
                <td align="left">0.629</td>
                <td align="left">0.873</td>
                <td align="left">0.671</td>
                <td align="left">3.017</td>
                <td align="left">0.889</td>
                <td align="left">0.645</td>
                <td align="left">0.802</td>
                <td align="left">0.660</td>
                <td align="left">3.265</td>
                <td align="left">1.104</td>
              </tr>
              <tr>
                <td align="left">Inf-Net <xref rid="b7" ref-type="bibr">[7]</xref></td>
                <td align="left">–</td>
                <td align="left">0.657</td>
                <td align="left">
                  <bold>0.916</bold>
                </td>
                <td align="left">0.741</td>
                <td align="left">2.585</td>
                <td align="left">0.793</td>
                <td align="left">0.673</td>
                <td align="left">0.729</td>
                <td align="left">0.703</td>
                <td align="left">2.784</td>
                <td align="left">1.272</td>
              </tr>
              <tr>
                <td align="left">2D-UNet <xref rid="b19" ref-type="bibr">[19]</xref></td>
                <td align="left">–</td>
                <td align="left">0.653</td>
                <td align="left">0.821</td>
                <td align="left">0.763</td>
                <td align="left">1.935</td>
                <td align="left">0.708</td>
                <td align="left">0.666</td>
                <td align="left">
                  <bold>0.805</bold>
                </td>
                <td align="left">0.734</td>
                <td align="left">2.371</td>
                <td align="left">1.033</td>
              </tr>
              <tr>
                <td align="left">JCS <xref rid="b41" ref-type="bibr">[41]</xref></td>
                <td align="left">–</td>
                <td align="left">0.641</td>
                <td align="left">0.786</td>
                <td align="left">0.761</td>
                <td align="left">1.947</td>
                <td align="left">0.674</td>
                <td align="left">0.651</td>
                <td align="left">0.794</td>
                <td align="left">0.739</td>
                <td align="left">2.446</td>
                <td align="left">0.949</td>
              </tr>
              <tr>
                <td align="left">LCOV-Net <xref rid="b56" ref-type="bibr">[56]</xref></td>
                <td align="left">–</td>
                <td align="left">0.731</td>
                <td align="left">0.825</td>
                <td align="left">
                  <bold>0.792</bold>
                </td>
                <td align="left">1.921</td>
                <td align="left">0.650</td>
                <td align="left">0.739</td>
                <td align="left">0.803</td>
                <td align="left">
                  <bold>0.786</bold>
                </td>
                <td align="left">2.104</td>
                <td align="left">0.926</td>
              </tr>
              <tr>
                <td align="left">LwMLA-NET <xref rid="b12" ref-type="bibr">[12]</xref></td>
                <td align="left">–</td>
                <td align="left">0.728</td>
                <td align="left">0.871</td>
                <td align="left">0.779</td>
                <td align="left">1.918</td>
                <td align="left">0.669</td>
                <td align="left">
                  <bold>0.757</bold>
                </td>
                <td align="left">0.739</td>
                <td align="left">0.742</td>
                <td align="left">2.215</td>
                <td align="left">0.871</td>
              </tr>
              <tr>
                <td colspan="12">
                  <hr/>
                </td>
              </tr>
              <tr>
                <td align="left">ACL (Ours)</td>
                <td align="left">–</td>
                <td align="left">
                  <bold>0.734</bold>
                </td>
                <td align="left">0.812</td>
                <td align="left">0.784</td>
                <td align="left">
                  <bold>1.819</bold>
                </td>
                <td align="left">
                  <bold>0.639</bold>
                </td>
                <td align="left">0.711</td>
                <td align="left">0.704</td>
                <td align="left">0.722</td>
                <td align="left">
                  <bold>2.071</bold>
                </td>
                <td align="left">
                  <bold>0.747</bold>
                </td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </p>
    </sec>
    <sec id="sec4.3">
      <label>4.3</label>
      <title>COVID-19 GGO segmentation results</title>
      <p id="d1e4124">To compare the GGO segmentation performance of the ACL framework and the network method position of the equivalent effect of this framework, we systematically compared 9 advanced models, including U-Net <xref rid="b52" ref-type="bibr">[52]</xref>, U-Net++ <xref rid="b53" ref-type="bibr">[53]</xref>, Attention-UNet <xref rid="b25" ref-type="bibr">[25]</xref>, Dense-UNet <xref rid="b54" ref-type="bibr">[54]</xref>, Gate-UNet <xref rid="b55" ref-type="bibr">[55]</xref>, Inf-Net <xref rid="b7" ref-type="bibr">[7]</xref>, 2D-UNet <xref rid="b19" ref-type="bibr">[19]</xref>, JCS <xref rid="b41" ref-type="bibr">[41]</xref> and LCOV-Net <xref rid="b56" ref-type="bibr">[56]</xref>.</p>
      <p id="d1e4164">(1) Quantitative results. Each method index is shown in the <xref rid="tbl2" ref-type="table">Table 2</xref>. The part in bold indicates that the method corresponding to this indicator is the best. The amount of floating-point operations required by ACL is small. In the framework, ACL only needs computational power to calculate image contrast and pixel segmentation.</p>
      <p id="d1e4170">At the same time, it can be seen that other indicators of the ACL framework are equivalent to the <xref rid="b12" ref-type="bibr">[12]</xref>, <xref rid="b19" ref-type="bibr">[19]</xref>, <xref rid="b41" ref-type="bibr">[41]</xref>, <xref rid="b56" ref-type="bibr">[56]</xref> method. <inline-formula><mml:math id="d1e4177" altimg="si76.svg" display="inline"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mo>±</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>04</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4201" altimg="si77.svg" display="inline"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mo>±</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4222" altimg="si78.svg" display="inline"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mo>±</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>03</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4242" altimg="si79.svg" display="inline"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>R</mml:mi><mml:mi>V</mml:mi><mml:mi>D</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mo>±</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>02</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4264" altimg="si80.svg" display="inline"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>A</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mo>±</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>02</mml:mn></mml:mrow></mml:math></inline-formula>, The error of each index is only within 0.1. However, the ACL framework does not have an important division of network training. Furthermore, ASD shows that the GGO segmentation results of ACL frames have the smallest average distance from labels, more accurately limiting the GGO range.</p>
      <p id="d1e4285">(2) Qualitative results. The results of GGO segmentation are shown in <xref rid="fig7" ref-type="fig">Fig. 7</xref>. It can be seen that under extremely low brightness, the edge of the GGO segmented by other methods is not accurate enough. In contrast, ACL can still accurately segment the GGO area due to the balance between local threshold and contour segmentation by <inline-formula><mml:math id="d1e4294" altimg="si34.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> according to image quality.<fig id="fig9"><label>Fig. 9</label><caption><p>ACL without pulmonary cavity segmentation. (a) expresses the original CT image, (b) represents the directly binarized image with unlimited lung cavity range, (c) is <inline-formula><mml:math id="d1e389" altimg="si6.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> more prominent than the segmentation in <inline-formula><mml:math id="d1e399" altimg="si2.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, and (d) shows the remaining part <inline-formula><mml:math id="d1e409" altimg="si7.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>.</p></caption><graphic xlink:href="gr9_lrg"/></fig></p>
      <p id="d1e4305">Specifically, the subdivision results produced by ACL are closer to ground truth, and there are fewer wrong subdivision pixels. It can be seen that gate-UNet missed the GGO area on the edge of normal tissue, and Inf-net and joint classification segmentation (JCS) improved the results, but only a few GGO areas could be segmented. The main reason is that the two characteristics are similar in the GGO area next to normal tissue, and the boundary is not apparent. ACL can divide this area. All pixels will be classified and calculated in Alg. 1. Threshold segmentation is based on pixel classification <xref rid="b57" ref-type="bibr">[57]</xref>, and even pixels with gradually reduced edges will be classified into different categories. After the contour of Alg. 2 is balanced, different segmentation results are obtained. Includes two cases, one is that <inline-formula><mml:math id="d1e4312" altimg="si6.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the GGO area and <inline-formula><mml:math id="d1e4322" altimg="si7.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the normal tissue. The other is that both <inline-formula><mml:math id="d1e4333" altimg="si6.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e4343" altimg="si7.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are pathological tissues. The specific meaning of <inline-formula><mml:math id="d1e4353" altimg="si89.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is different due to different image contrast.</p>
      <p id="d1e4362">(3) Dice of ACL and three methods in different contrast CT images. Because the three methods of Gate-UNet, Inf-Net, and JCS perform better in some indicators, we chose these three methods to discuss the performance with ACL about contrast. As shown in <xref rid="fig8" ref-type="fig">Fig. 8</xref>, when each method is in <inline-formula><mml:math id="d1e4371" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e4391" altimg="si100.svg" display="inline"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≈</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4410" altimg="si50.svg" display="inline"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> begins to decline. When <inline-formula><mml:math id="d1e4422" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e4442" altimg="si103.svg" display="inline"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≈</mml:mo><mml:mn>410</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4460" altimg="si50.svg" display="inline"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> back to normal. It is roughly equivalent to Dice of ACL and JCS in contrast <inline-formula><mml:math id="d1e4473" altimg="si105.svg" display="inline"><mml:mfenced close="]" open="["><mml:mrow><mml:mn>120</mml:mn><mml:mo>,</mml:mo><mml:mn>410</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>. However, when it in contrast, <inline-formula><mml:math id="d1e4484" altimg="si106.svg" display="inline"><mml:mfenced close="]" open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4495" altimg="si107.svg" display="inline"><mml:mfenced close="]" open="("><mml:mrow><mml:mn>410</mml:mn><mml:mo>,</mml:mo><mml:mn>600</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, ACL is superior to other methods, and the extreme cases are also in these two contrast values. The overall GGO segmentation effect of ACL is equivalent to that of the JCS method. However, the error distance is stable under different contrast, indicating that the segmented GGO area of ACL is closer to the Ground Truth.<fig id="fig10"><label>Fig. 10</label><caption><p>The effect of lung segmentation. The first row shows the original CT image, which contains the <inline-formula><mml:math id="d1e427" altimg="si41.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mfenced open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e455" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e475" altimg="si43.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="d1e493" altimg="si44.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e514" altimg="si45.svg" display="inline"><mml:mfenced><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e533" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e553" altimg="si47.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="d1e571" altimg="si44.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e592" altimg="si49.svg" display="inline"><mml:mfenced close=")"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. The second row shows the Alg. 3 lung segmentation results. The third row is the GGO mask.</p></caption><graphic xlink:href="gr10_lrg"/></fig></p>
    </sec>
    <sec id="sec4.4">
      <label>4.4</label>
      <title>Ablation study</title>
      <p id="d1e4510">In this subsection, we conduct three experiments to verify the performance of each module of ACL.</p>
      <p id="d1e4512">(1) Attention mechanism threshold (A). In the ACL framework, we adopt <inline-formula><mml:math id="d1e4515" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e4531" altimg="si2.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4541" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e4558" altimg="si3.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e4568" altimg="si34.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>. As shown in <xref rid="tbl3" ref-type="table">Table 3</xref>, eight combinations are proposed to search for the best solution. ✓is <inline-formula><mml:math id="d1e4584" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e4601" altimg="si114.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>
<inline-formula><mml:math id="d1e4611" altimg="si115.svg" display="inline"><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="d1e4628" altimg="si116.svg" display="inline"><mml:mi>X</mml:mi></mml:math></inline-formula> is <inline-formula><mml:math id="d1e4633" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e4650" altimg="si118.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>B</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4686" altimg="si115.svg" display="inline"><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4703" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e4719" altimg="si2.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4730" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="d1e4746" altimg="si3.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e4756" altimg="si34.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the best combinations.</p>
      <p id="d1e4767">(2) Contour equalization (C). We divide images into three categories <inline-formula><mml:math id="d1e4772" altimg="si142.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e4781" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e4801" altimg="si43.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4819" altimg="si44.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e4840" altimg="si45.svg" display="inline"><mml:mfenced><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e4859" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e4879" altimg="si47.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4897" altimg="si44.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e4918" altimg="si49.svg" display="inline"><mml:mfenced close=")"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, and carry out corresponding contour balancing operations respectively. The joint operation of three kinds of images is verified to verify that the scheme is the optimal contour balance. As shown in <xref rid="tbl4" ref-type="table">Table 4</xref>, classify the corresponding images into the corresponding categories, and the segmentation effect is the best. Better results can be obtained only by corresponding processing, <inline-formula><mml:math id="d1e4943" altimg="si140.svg" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≥</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula>.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Discussion schemes of the three thresholds.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><inline-formula><mml:math id="d1e1666" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e1682" altimg="si2.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left"><inline-formula><mml:math id="d1e1692" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e1708" altimg="si3.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left"><inline-formula><mml:math id="d1e1718" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e1734" altimg="si4.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></th><th align="left">Dice</th></tr></thead><tbody><tr><td align="left">✕</td><td align="left">✓</td><td align="left">✕</td><td align="left">0.718</td></tr><tr><td align="left">✕</td><td align="left">✓</td><td align="left">✓</td><td align="left">0.427</td></tr><tr><td align="left">✕</td><td align="left">✕</td><td align="left">✓</td><td align="left">0.173</td></tr><tr><td align="left">✕</td><td align="left">✕</td><td align="left">✕</td><td align="left">0.141</td></tr><tr><td align="left">✓</td><td align="left">✕</td><td align="left">✕</td><td align="left">0.727</td></tr><tr><td align="left">✓</td><td align="left">✕</td><td align="left">✓</td><td align="left">0.612</td></tr><tr><td align="left">✓</td><td align="left">✓</td><td align="left">✓</td><td align="left">0.694</td></tr><tr><td align="left">✓</td><td align="left">✓</td><td align="left">✕</td><td align="left"><bold>0.771(ACL)</bold></td></tr></tbody></table></table-wrap></p>
      <p id="d1e4968">(3) Lung segmentation (L). Before segmenting the GGO area, we need to get the lung cavity area to narrow the scope of GGO segmentation. When the whole graph is directly divided, the lack of restrictions leads to that <inline-formula><mml:math id="d1e4973" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e4989" altimg="si3.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e4999" altimg="si34.svg" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> can no longer directly divide <inline-formula><mml:math id="d1e5012" altimg="si6.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e5022" altimg="si7.svg" display="inline"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> as shown in <xref rid="fig9" ref-type="fig">Fig. 9</xref>(c) and (d). The threshold of the attention mechanism will consider all the pixels of the CT image. However, many black backgrounds and lung tissue pixels will cause the threshold to deviate, and the pixels in the lung cannot be accurately classified. Segmentation area <inline-formula><mml:math id="d1e5036" altimg="si50.svg" display="inline"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> is only 0.13, hence cavity segmentation effectively improves the segmentation performance of GGO in ACL.<table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>Dice of each type of contrast image in various operations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><inline-formula><mml:math id="d1e1834" altimg="si50.svg" display="inline"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula></th><th align="left"><inline-formula><mml:math id="d1e1846" altimg="si142.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e1855" altimg="si43.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula></th><th align="left"><inline-formula><mml:math id="d1e1872" altimg="si144.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e1891" altimg="si47.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula></th><th align="left"><inline-formula><mml:math id="d1e1908" altimg="si146.svg" display="inline"><mml:mfenced close=")" open="("><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left"><inline-formula><mml:math id="d1e1931" altimg="si142.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e1940" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e1960" altimg="si43.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula></td><td align="left">0.841</td><td align="left">0.725</td><td align="left">0.159</td></tr><tr><td align="left"><inline-formula><mml:math id="d1e1984" altimg="si44.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e2005" altimg="si45.svg" display="inline"><mml:mfenced><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e2024" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e2044" altimg="si47.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula></td><td align="left">0.784</td><td align="left">0.817</td><td align="left">0.175</td></tr><tr><td align="left"><inline-formula><mml:math id="d1e2068" altimg="si44.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e2089" altimg="si49.svg" display="inline"><mml:mfenced close=")"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula></td><td align="left">0.121</td><td align="left">0.159</td><td align="left">0.872</td></tr></tbody></table></table-wrap></p>
      <p id="d1e5047">(4) Lung segmentation result. Lung segmentation is significant as the limiting condition of GGO segmentation. On the one hand, we need them to combine and restrain the effects of GGO segmentation. On the other hand, we need the results of lung segmentation to help reduce the scope of GGO segmentation. The results of adaptive threshold lung segmentation. Adaptive lung segmentation can still completely segment the lung cavity and provide a limited focusing range.</p>
      <p id="d1e5049">However, there are burrs in the segmented lung cavity. It can be seen from <xref rid="fig10" ref-type="fig">Fig. 10</xref>, redundant burrs will not affect the segmentation range of the GGO. The content of Lung segmentation includes all pixel positions in the Ground Truth. When Alg. 1 and Alg. 2 are finished, only by keeping the GGO area of Alg. 3. Get the final segmentation result. It can be seen that ACL can segment the normal tissue well when encountering holes, and the edge segmentation is closer to the real label. In extreme contrast, ACL can still retain detailed information about the GGO.</p>
    </sec>
    <sec id="sec4.5">
      <label>4.5</label>
      <title>Other pneumonia segmentation results</title>
      <p id="d1e5060">In COVID-19 segmentation, we found that ACL’s attention mechanism threshold segmentation effectively uses the image’s information. In addition, ACL can also be used to segment common pneumonia. The characteristics of common pneumonia are similar to those of COVID-19 <xref rid="b58" ref-type="bibr">[58]</xref>, such as a small ground glass shadow area and a small amount of pleural effusion <xref rid="b59" ref-type="bibr">[59]</xref>.</p>
      <p id="d1e5070">In this way, we also segmented the other pneumonia <xref rid="b49" ref-type="bibr">[49]</xref>, and the segmentation of three kinds of contrast images as shown in <xref rid="fig11" ref-type="fig">Fig. 11</xref>. Notably, the second column of CT images contained a piece of normal tissue within the lung. ACL can still accurately segment GGO. In the strict sense, our lung segmentation algorithm does not segment the boundaries of the lung. Instead, areas within the lung cavity that may contain GGOs are to be isolated. When finally merging the threshold results of the attention mechanism, the lung segmentation algorithm has already separated all the pixels of the normal tissue in this part of the lung. Similarly, there are two holes in the third column of images, and the lung segmentation algorithm does not separate the pixels within the holes. If the result of the attention mechanism threshold algorithm has GGO in the hole, then the final segmentation result will also be presented in the hole.</p>
      <p id="d1e5080">It can be seen that the ACL is also suitable for segmenting CT images of common pneumonia, with an average <inline-formula><mml:math id="d1e5085" altimg="si171.svg" display="inline"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>77</mml:mn><mml:mo>.</mml:mo><mml:mn>2</mml:mn><mml:mtext>%</mml:mtext></mml:mrow></mml:math></inline-formula>. Mainly because in the specific lung region of CT images, the block threshold segmentation combined with prior information <xref rid="b60" ref-type="bibr">[60]</xref> can be used to segment three area, including GGO region, normal tissue, and background. These areas combine the designed GGO contour balance to fine-tune the segmentation boundary further. ACL can show an efficient segmentation effect in lung image GGO segmentation. Therefore, the ACL framework can be applied to the segmentation of COVID-19 GGO and common pneumonia.<fig id="fig11"><label>Fig. 11</label><caption><p>ACL’s segmentation effect on other pneumonia data sets. The first row shows CT image of common pneumonia, which contains the <inline-formula><mml:math id="d1e621" altimg="si41.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:mfenced open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e649" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e669" altimg="si43.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="d1e687" altimg="si44.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e708" altimg="si45.svg" display="inline"><mml:mfenced><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e727" altimg="si42.svg" display="inline"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="d1e747" altimg="si47.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, <inline-formula><mml:math id="d1e765" altimg="si44.svg" display="inline"><mml:mfenced open="("><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula><inline-formula><mml:math id="d1e786" altimg="si170.svg" display="inline"><mml:mfenced close="]"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>. The second row shows the Alg. 3 lung segmentation results. The third row is the Ground Truth.</p></caption><graphic xlink:href="gr11_lrg"/></fig></p>
      <p id="d1e5110">ACL can be used well in CT images to group pixel points into many classes by a group of thresholds. The segmentation target is obtained after the pixels are grouped. Unfortunately, when the grey values of target pixels are no longer relatively concentrated as in CT images, threshold segmentation can hardly solve the problem of mixing grey-level pixel features.</p>
    </sec>
  </sec>
  <sec id="sec5">
    <label>5</label>
    <title>Conclusion</title>
    <p id="d1e5117">We propose a new attention mechanism of threshold combination, which no longer classifies a single pixel into a specific category. With continuous segmentation and threshold selection, the individual pixels are classified most logically. Minor pixel adjustments on the edges further adjust the contour equalization. In addition, a threshold algorithm is designed to segment the lung cavity, thus proposing a complete adaptive threshold process. It can assist physicians in making fast and accurate diagnoses. ACL does not require a lot of tedious training processes compared to deep learning algorithms. GGO can perform the segmentation of CT images directly with pre-set parameters. The hardware overhead is small, the program execution is fast, and the GGO segmentation is accurate.</p>
    <p id="d1e5119">ACL benefits from the roughly fixed grey-level distribution of CT images of human tissues. ACL is also only applicable to segmenting regions of interest within CT images. For the natural environment, ACL is also challenging to segment the target by the threshold, which is extremely difficult to have generalizability. Therefore, targeted solutions can be found first when segmenting specific categories such as CT images. In addition, due to its small overhead and adaptive tuning parameters, ACL can be used as a pre-input for deep learning networks to achieve more advanced semantic tasks, e.g., ACL can guide consolidation segmentation or separate GGO from other tissue margins.</p>
  </sec>
  <sec id="d1e5121">
    <title>CRediT authorship contribution statement</title>
    <p id="d1e5124"><bold>Yunbo Rao:</bold> Writing – review &amp; editing, Supervision, Project administration, Funding acquisition. <bold>Qingsong Lv:</bold> Conceptualization, Methodology, Software, Validation, Writing – original draft, Visualization. <bold>Shaoning Zeng:</bold> Writing – review &amp; editing, Project administration, Data curation. <bold>Yuling Yi:</bold> Formal analysis, Resources. <bold>Cheng Huang:</bold> Formal analysis, Data curation. <bold>Yun Gao:</bold> Conceptualization, Methodology, Software, Resources, Visualization. <bold>Zhanglin Cheng:</bold> Methodology, Writing – review &amp; editing, Supervision. <bold>Jihong Sun:</bold> Data curation, Supervision.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="d1e5153">The authors declare the following financial interests/personal relationships which may be considered as potential competing interests: Yunbo Rao reports financial support was provided by University of Electronic Science and Technology of China. Yunbo Rao reports a relationship with University of Electronic Science and Technology of China that includes: funding grants.</p>
  </sec>
</body>
<back>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="b1">
      <label>1</label>
      <element-citation publication-type="journal" id="sb1">
        <person-group person-group-type="author">
          <name>
            <surname>Khan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mehran</surname>
            <given-names>M.T.</given-names>
          </name>
          <name>
            <surname>Haq</surname>
            <given-names>Z.U.</given-names>
          </name>
          <name>
            <surname>Ullah</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Naqvi</surname>
            <given-names>S.R.</given-names>
          </name>
          <name>
            <surname>Ihsan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Abbass</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Applications of artificial intelligence in COVID-19 pandemic: A comprehensive review</article-title>
        <source>Expert Syst. Appl.</source>
        <volume>185</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">115695</object-id>
      </element-citation>
    </ref>
    <ref id="b2">
      <label>2</label>
      <element-citation publication-type="book" id="sb2">
        <person-group person-group-type="author">
          <name>
            <surname>organization</surname>
            <given-names>W.H.</given-names>
          </name>
        </person-group>
        <part-title>Weekly operational update on COVID-19 - 23 November 2021</part-title>
        <year>2021</year>
        <comment>
          <ext-link ext-link-type="uri" xlink:href="https://covid19.who.int/" id="interref4">https://covid19.who.int/</ext-link>
        </comment>
      </element-citation>
    </ref>
    <ref id="b3">
      <label>3</label>
      <element-citation publication-type="journal" id="sb3">
        <person-group person-group-type="author">
          <name>
            <surname>Nagi</surname>
            <given-names>A.T.</given-names>
          </name>
          <name>
            <surname>Awan</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Mohammed</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Mahmoud</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Majumdar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Thinnukool</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>Performance Analysis for COVID-19 Diagnosis using custom and state-of-the-art deep learning models</article-title>
        <source>Appl. Sci.</source>
        <volume>12</volume>
        <issue>13</issue>
        <year>2022</year>
        <fpage>6364</fpage>
      </element-citation>
    </ref>
    <ref id="b4">
      <label>4</label>
      <element-citation publication-type="journal" id="sb4">
        <person-group person-group-type="author">
          <name>
            <surname>Ghajarzadeh</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Fard</surname>
            <given-names>M.M.</given-names>
          </name>
          <name>
            <surname>Alebouyeh</surname>
            <given-names>M.R.</given-names>
          </name>
          <name>
            <surname>Otaghvar</surname>
            <given-names>H.A.</given-names>
          </name>
          <name>
            <surname>Dabbagh</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mohseni</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kashani</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Fard</surname>
            <given-names>A.M.M.</given-names>
          </name>
          <name>
            <surname>Faiz</surname>
            <given-names>S.H.R.</given-names>
          </name>
        </person-group>
        <article-title>The prominent chest CT findings in Covid-19 patients: A systematic review and meta-analysis</article-title>
        <source>Ann. Rom. Soc. Cell Biol.</source>
        <year>2021</year>
        <fpage>2466</fpage>
        <lpage>2484</lpage>
      </element-citation>
    </ref>
    <ref id="b5">
      <label>5</label>
      <element-citation publication-type="book" id="sb5">
        <person-group person-group-type="author">
          <name>
            <surname>Proskurov</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Kurmukov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Pisov</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Belyaev</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>Fast lung localization in computed tomography by a 1D detection network</part-title>
        <source>2021 Ural Symposium on Biomedical Engineering, Radioelectronics and Information Technology</source>
        <conf-name>USBEREIT</conf-name>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>0173</fpage>
        <lpage>0176</lpage>
      </element-citation>
    </ref>
    <ref id="b6">
      <label>6</label>
      <element-citation publication-type="journal" id="sb6">
        <person-group person-group-type="author">
          <name>
            <surname>Ng</surname>
            <given-names>M.-Y.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>E.Y.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Lui</surname>
            <given-names>M.M.-s.</given-names>
          </name>
          <name>
            <surname>Lo</surname>
            <given-names>C.S.-Y.</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Khong</surname>
            <given-names>P.-L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Imaging profile of the COVID-19 infection: radiologic findings and literature review</article-title>
        <source>Radiol.: Cardiothorac. Imaging</source>
        <volume>2</volume>
        <issue>1</issue>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">e200034</object-id>
      </element-citation>
    </ref>
    <ref id="b7">
      <label>7</label>
      <element-citation publication-type="journal" id="sb7">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>D.-P.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>G.-P.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Shao</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Inf-net: Automatic covid-19 lung infection segmentation from ct images</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>39</volume>
        <issue>8</issue>
        <year>2020</year>
        <fpage>2626</fpage>
        <lpage>2637</lpage>
        <pub-id pub-id-type="pmid">32730213</pub-id>
      </element-citation>
    </ref>
    <ref id="b8">
      <label>8</label>
      <element-citation publication-type="journal" id="sb8">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>C.P.</given-names>
          </name>
        </person-group>
        <article-title>Hybrid transfer learning and broad learning system for wearing mask detection in the COVID-19 era</article-title>
        <source>IEEE Trans. Instrum. Meas.</source>
        <volume>70</volume>
        <year>2021</year>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="pmid">33776080</pub-id>
      </element-citation>
    </ref>
    <ref id="b9">
      <label>9</label>
      <element-citation publication-type="book" id="sb9">
        <person-group person-group-type="author">
          <name>
            <surname>Faruk</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <part-title>RGU-net: Residual guided U-net architecture for automated segmentation of COVID-19 anomalies using CT images</part-title>
        <source>2021 International Conference on Automation, Control and Mechatronics for Industry 4.0</source>
        <conf-name>ACMI</conf-name>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="b10">
      <label>10</label>
      <element-citation publication-type="journal" id="sb10">
        <person-group person-group-type="author">
          <name>
            <surname>Shamim</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Awan</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Mohd Zain</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Naseem</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Mohammed</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Garcia-Zapirain</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Automatic COVID-19 lung infection segmentation through modified unet model</article-title>
        <source>J. Healthc. Eng.</source>
        <year>2022</year>
        <pub-id pub-id-type="doi">10.1155/2022/6566982</pub-id>
      </element-citation>
    </ref>
    <ref id="b11">
      <label>11</label>
      <element-citation publication-type="journal" id="sb11">
        <person-group person-group-type="author">
          <name>
            <surname>Paluru</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Dayal</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Jenssen</surname>
            <given-names>H.B.</given-names>
          </name>
          <name>
            <surname>Sakinis</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Cenkeramaddi</surname>
            <given-names>L.R.</given-names>
          </name>
          <name>
            <surname>Prakash</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Yalavarthy</surname>
            <given-names>P.K.</given-names>
          </name>
        </person-group>
        <article-title>Anam-net: Anamorphic depth embedding-based lightweight CNN for segmentation of anomalies in COVID-19 chest CT images</article-title>
        <source>IEEE Trans. Neural Netw. Learn. Syst.</source>
        <volume>32</volume>
        <issue>3</issue>
        <year>2021</year>
        <fpage>932</fpage>
        <lpage>946</lpage>
        <pub-id pub-id-type="pmid">33544680</pub-id>
      </element-citation>
    </ref>
    <ref id="b12">
      <label>12</label>
      <element-citation publication-type="journal" id="sb12">
        <person-group person-group-type="author">
          <name>
            <surname>Roy</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Banik</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bhattacharjee</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Krejcar</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Kollmann</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>LwMLA-NET: A lightweight multi-level attention-based network for segmentation of COVID-19 lungs abnormalities from CT images</article-title>
        <source>IEEE Trans. Instrum. Meas.</source>
        <volume>71</volume>
        <year>2022</year>
        <fpage>1</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="b13">
      <label>13</label>
      <element-citation publication-type="journal" id="sb13">
        <person-group person-group-type="author">
          <name>
            <surname>Ibrahim</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Zebari</surname>
            <given-names>D.A.</given-names>
          </name>
          <name>
            <surname>Mohammed</surname>
            <given-names>H.J.</given-names>
          </name>
          <name>
            <surname>Mohammed</surname>
            <given-names>M.A.</given-names>
          </name>
        </person-group>
        <article-title>Effective hybrid deep learning model for COVID-19 patterns identification using CT images</article-title>
        <source>Expert Syst.</source>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">e13010</object-id>
      </element-citation>
    </ref>
    <ref id="b14">
      <label>14</label>
      <element-citation publication-type="book" id="sb14">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>K.W.</given-names>
          </name>
          <name>
            <surname>Chin</surname>
            <given-names>R.K.Y.</given-names>
          </name>
        </person-group>
        <part-title>An adaptive data processing framework for cost-effective COVID-19 and pneumonia detection</part-title>
        <source>2021 IEEE International Conference on Signal and Image Processing Applications</source>
        <conf-name>ICSIPA</conf-name>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>150</fpage>
        <lpage>155</lpage>
      </element-citation>
    </ref>
    <ref id="b15">
      <label>15</label>
      <element-citation publication-type="book" id="sb15">
        <person-group person-group-type="author">
          <name>
            <surname>Saha</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Gourisaria</surname>
            <given-names>M.K.</given-names>
          </name>
          <name>
            <surname>Harshvardhan</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>Distinguishing pneumonia and COVID-19: Utilizing computer vision to mimic clinician efficacy</part-title>
        <source>2021 International Conference on Artificial Intelligence and Smart Systems</source>
        <conf-name>ICAIS</conf-name>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>834</fpage>
        <lpage>841</lpage>
      </element-citation>
    </ref>
    <ref id="b16">
      <label>16</label>
      <element-citation publication-type="journal" id="sb16">
        <person-group person-group-type="author">
          <name>
            <surname>Allioui</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Mohammed</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Benameur</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Al-Khateeb</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Abdulkareem</surname>
            <given-names>K.H.</given-names>
          </name>
          <name>
            <surname>Garcia-Zapirain</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Damaševičius</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Maskeliūnas</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>A multi-agent deep reinforcement learning approach for enhancement of COVID-19 CT image segmentation</article-title>
        <source>J. Pers. Med.</source>
        <volume>12</volume>
        <issue>2</issue>
        <year>2022</year>
        <fpage>309</fpage>
        <pub-id pub-id-type="pmid">35207796</pub-id>
      </element-citation>
    </ref>
    <ref id="b17">
      <label>17</label>
      <element-citation publication-type="book" id="sb17">
        <person-group person-group-type="author">
          <name>
            <surname>Cendejas-Zaragoza</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Rodriguez-Obregon</surname>
            <given-names>D.E.</given-names>
          </name>
          <name>
            <surname>Mejia-Rodriguez</surname>
            <given-names>A.R.</given-names>
          </name>
          <name>
            <surname>Arce-Santana</surname>
            <given-names>E.R.</given-names>
          </name>
          <name>
            <surname>Santos-Diaz</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>COVID-19 volumetric pulmonary lesion estimation on CT images using a U-NET and probabilistic active contour segmentation</part-title>
        <source>2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society</source>
        <conf-name>EMBC</conf-name>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>3850</fpage>
        <lpage>3853</lpage>
      </element-citation>
    </ref>
    <ref id="b18">
      <label>18</label>
      <element-citation publication-type="journal" id="sb18">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Nian</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>PAM-DenseNet: A deep convolutional neural network for computer-aided COVID-19 diagnosis</article-title>
        <source>IEEE Trans. Cybern.</source>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b19">
      <label>19</label>
      <element-citation publication-type="journal" id="sb19">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Xing</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Exploiting shared knowledge from non-covid lesions for annotation-efficient covid-19 ct lung infection segmentation</article-title>
        <source>IEEE J. Biomed. Health Inf.</source>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b20">
      <label>20</label>
      <element-citation publication-type="book" id="sb20">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>D.-P.</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>M.-M.</given-names>
          </name>
          <name>
            <surname>Borji</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Enhanced-alignment measure for binary foreground map evaluation</part-title>
        <year>2018</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1805.10421" id="interref5">arXiv:1805.10421</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b21">
      <label>21</label>
      <element-citation publication-type="journal" id="sb21">
        <person-group person-group-type="author">
          <name>
            <surname>Yan</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ai</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>COVID-19 chest CT image segmentation network by multi-scale fusion and enhancement operations</article-title>
        <source>IEEE Trans. Big Data</source>
        <volume>7</volume>
        <issue>1</issue>
        <year>2021</year>
        <fpage>13</fpage>
        <lpage>24</lpage>
      </element-citation>
    </ref>
    <ref id="b22">
      <label>22</label>
      <element-citation publication-type="journal" id="sb22">
        <person-group person-group-type="author">
          <name>
            <surname>Barshooi</surname>
            <given-names>A.H.</given-names>
          </name>
          <name>
            <surname>Amirkhani</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>A novel data augmentation based on Gabor filter and convolutional deep learning for improving the classification of COVID-19 chest X-Ray images</article-title>
        <source>Biomed. Signal Process. Control</source>
        <volume>72</volume>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">103326</object-id>
      </element-citation>
    </ref>
    <ref id="b23">
      <label>23</label>
      <element-citation publication-type="journal" id="sb23">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Dense GAN and multi-layer attention based lesion segmentation method for COVID-19 CT images</article-title>
        <source>Biomed. Signal Process. Control</source>
        <volume>69</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">102901</object-id>
      </element-citation>
    </ref>
    <ref id="b24">
      <label>24</label>
      <element-citation publication-type="journal" id="sb24">
        <person-group person-group-type="author">
          <name>
            <surname>Kundu</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>P.K.</given-names>
          </name>
          <name>
            <surname>Mirjalili</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sarkar</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 detection from lung CT-Scans using a fuzzy integral-based CNN ensemble</article-title>
        <source>Comput. Biol. Med.</source>
        <volume>138</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">104895</object-id>
      </element-citation>
    </ref>
    <ref id="b25">
      <label>25</label>
      <element-citation publication-type="book" id="sb25">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <part-title>MA-UNet++: A multi-attention guided U-Net++ for COVID-19 CT segmentation</part-title>
        <source>2022 13th Asian Control Conference</source>
        <conf-name>ASCC</conf-name>
        <year>2022</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>682</fpage>
        <lpage>687</lpage>
      </element-citation>
    </ref>
    <ref id="b26">
      <label>26</label>
      <element-citation publication-type="journal" id="sb26">
        <person-group person-group-type="author">
          <name>
            <surname>Mohammed</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Al-Khateeb</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Yousif</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mostafa</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Kadry</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Abdulkareem</surname>
            <given-names>K.H.</given-names>
          </name>
          <name>
            <surname>Garcia-Zapirain</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>Novel crow swarm optimization algorithm and selection approach for optimal deep learning COVID-19 diagnostic model</article-title>
        <source>Comput. Intell. Neurosci.</source>
        <volume>2022</volume>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="b27">
      <label>27</label>
      <element-citation publication-type="journal" id="sb27">
        <person-group person-group-type="author">
          <name>
            <surname>Narin</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Accurate detection of COVID-19 using deep features based on X-Ray images and feature selection methods</article-title>
        <source>Comput. Biol. Med.</source>
        <volume>137</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">104771</object-id>
      </element-citation>
    </ref>
    <ref id="b28">
      <label>28</label>
      <element-citation publication-type="journal" id="sb28">
        <person-group person-group-type="author">
          <name>
            <surname>Abdulkareem</surname>
            <given-names>K.H.</given-names>
          </name>
          <name>
            <surname>Mostafa</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Al-Qudsy</surname>
            <given-names>Z.N.</given-names>
          </name>
          <name>
            <surname>Mohammed</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Al-Waisy</surname>
            <given-names>A.S.</given-names>
          </name>
          <name>
            <surname>Kadry</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Nam</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Automated system for identifying COVID-19 infections in computed tomography images using deep learning models</article-title>
        <source>J. Healthc. Eng.</source>
        <volume>2022</volume>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="b29">
      <label>29</label>
      <element-citation publication-type="book" id="sb29">
        <person-group person-group-type="author">
          <name>
            <surname>Somkuwar</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Rama</surname>
            <given-names>S.V.</given-names>
          </name>
        </person-group>
        <part-title>COVID-19 and pneumonia prognosis system using DWT based feature extraction, machine learning and ANN</part-title>
        <source>2021 International Conference on Intelligent Technologies</source>
        <conf-name>CONIT</conf-name>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>7</lpage>
      </element-citation>
    </ref>
    <ref id="b30">
      <label>30</label>
      <element-citation publication-type="journal" id="sb30">
        <person-group person-group-type="author">
          <name>
            <surname>Jin</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Meng</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Domain adaptation based self-correction model for COVID-19 infection segmentation in CT images</article-title>
        <source>Expert Syst. Appl.</source>
        <volume>176</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">114848</object-id>
      </element-citation>
    </ref>
    <ref id="b31">
      <label>31</label>
      <element-citation publication-type="book" id="sb31">
        <person-group person-group-type="author">
          <name>
            <surname>Chanda</surname>
            <given-names>P.B.</given-names>
          </name>
          <name>
            <surname>Sarkar</surname>
            <given-names>S.K.</given-names>
          </name>
        </person-group>
        <part-title>Effective and reliable lung segmentation of chest images with medical image processing and machine learning approaches</part-title>
        <source>2020 IEEE International Conference on Advent Trends in Multidisciplinary Research and Innovation</source>
        <conf-name>ICATMRI</conf-name>
        <year>2020</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="b32">
      <label>32</label>
      <element-citation publication-type="journal" id="sb32">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Y.-q.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Automatic multi-organ segmentation from abdominal CT volumes with LLE-based graph partitioning and 3D Chan-Vese model</article-title>
        <source>Comput. Biol. Med.</source>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">105030</object-id>
      </element-citation>
    </ref>
    <ref id="b33">
      <label>33</label>
      <element-citation publication-type="book" id="sb33">
        <person-group person-group-type="author">
          <name>
            <surname>Lavanya</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Arivalagan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Princye</surname>
            <given-names>P.H.</given-names>
          </name>
          <name>
            <surname>Sivasubramanian</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Madhu</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>A review on lung nodule segmentation techniques for nodule detection</part-title>
        <source>2020 4th International Conference on Electronics, Communication and Aerospace Technology</source>
        <conf-name>ICECA</conf-name>
        <year>2020</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1423</fpage>
        <lpage>1428</lpage>
      </element-citation>
    </ref>
    <ref id="b34">
      <label>34</label>
      <element-citation publication-type="book" id="sb34">
        <person-group person-group-type="author">
          <name>
            <surname>Halder</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chatterjee</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Dey</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Superpixel and Density Based Region segmentation algorithm for lung nodule detection</part-title>
        <source>2020 IEEE Calcutta Conference</source>
        <conf-name>CALCON</conf-name>
        <year>2020</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>511</fpage>
        <lpage>515</lpage>
      </element-citation>
    </ref>
    <ref id="b35">
      <label>35</label>
      <element-citation publication-type="book" id="sb35">
        <person-group person-group-type="author">
          <name>
            <surname>Firdaus</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Sigit</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Harsono</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Anwar</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <part-title>Lung cancer detection based on CT-scan images with detection features using gray level co-occurrence matrix (GLCM) and support vector machine (SVM) methods</part-title>
        <source>2020 International Electronics Symposium</source>
        <conf-name>IES</conf-name>
        <year>2020</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>643</fpage>
        <lpage>648</lpage>
      </element-citation>
    </ref>
    <ref id="b36">
      <label>36</label>
      <element-citation publication-type="journal" id="sb36">
        <person-group person-group-type="author">
          <name>
            <surname>Mahmoudi</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Benameur</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Mabrouk</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Mohammed</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Garcia-Zapirain</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Bedoui</surname>
            <given-names>M.H.</given-names>
          </name>
        </person-group>
        <article-title>A deep learning-based diagnosis system for COVID-19 detection and pneumonia screening using CT imaging</article-title>
        <source>Appl. Sci.</source>
        <volume>12</volume>
        <issue>10</issue>
        <year>2022</year>
        <fpage>4825</fpage>
      </element-citation>
    </ref>
    <ref id="b37">
      <label>37</label>
      <element-citation publication-type="journal" id="sb37">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Cekderi</surname>
            <given-names>A.B.</given-names>
          </name>
        </person-group>
        <article-title>Two-dimensional reciprocal cross entropy multi-threshold combined with improved firefly algorithm for lung parenchyma segmentation of COVID-19 CT image</article-title>
        <source>Biomed. Signal Process. Control</source>
        <volume>78</volume>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">103933</object-id>
      </element-citation>
    </ref>
    <ref id="b38">
      <label>38</label>
      <element-citation publication-type="book" id="sb38">
        <person-group person-group-type="author">
          <name>
            <surname>Chanda</surname>
            <given-names>P.B.</given-names>
          </name>
          <name>
            <surname>Sarkar</surname>
            <given-names>S.K.</given-names>
          </name>
        </person-group>
        <part-title>Effective and reliable lung segmentation of chest images with medical image processing and machine learning approaches</part-title>
        <source>2020 IEEE International Conference on Advent Trends in Multidisciplinary Research and Innovation</source>
        <conf-name>ICATMRI</conf-name>
        <year>2020</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="b39">
      <label>39</label>
      <element-citation publication-type="journal" id="sb39">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>W.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 CT image denoising algorithm based on adaptive threshold and optimized weighted median filter</article-title>
        <source>Biomed. Signal Process. Control</source>
        <volume>75</volume>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">103552</object-id>
      </element-citation>
    </ref>
    <ref id="b40">
      <label>40</label>
      <element-citation publication-type="journal" id="sb40">
        <person-group person-group-type="author">
          <name>
            <surname>Khan</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Zaki</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ali</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Intelligent pneumonia identification from chest x-rays: A systematic literature review</article-title>
        <source>IEEE Access</source>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b41">
      <label>41</label>
      <element-citation publication-type="journal" id="sb41">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Y.-H.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>S.-H.</given-names>
          </name>
          <name>
            <surname>Mei</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Fan</surname>
            <given-names>D.-P.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>R.-G.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>M.-M.</given-names>
          </name>
        </person-group>
        <article-title>Jcs: An explainable covid-19 diagnosis system by joint classification and segmentation</article-title>
        <source>IEEE Trans. Image Process.</source>
        <volume>30</volume>
        <year>2021</year>
        <fpage>3113</fpage>
        <lpage>3126</lpage>
        <pub-id pub-id-type="pmid">33600316</pub-id>
      </element-citation>
    </ref>
    <ref id="b42">
      <label>42</label>
      <element-citation publication-type="journal" id="sb42">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Weng</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hubbard</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>An intelligent early warning system of analyzing Twitter data using machine learning on COVID-19 surveillance in the US</article-title>
        <source>Expert Syst. Appl.</source>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">116882</object-id>
      </element-citation>
    </ref>
    <ref id="b43">
      <label>43</label>
      <element-citation publication-type="journal" id="sb43">
        <person-group person-group-type="author">
          <name>
            <surname>Banerjee</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sarkar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Roy</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>P.K.</given-names>
          </name>
          <name>
            <surname>Sarkar</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>COVID-19 chest X-ray detection through blending ensemble of CNN snapshots</article-title>
        <source>Biomed. Signal Process. Control</source>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">104000</object-id>
      </element-citation>
    </ref>
    <ref id="b44">
      <label>44</label>
      <element-citation publication-type="journal" id="sb44">
        <person-group person-group-type="author">
          <name>
            <surname>Sahin</surname>
            <given-names>M.E.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning-based approach for detecting COVID-19 in chest X-rays</article-title>
        <source>Biomed. Signal Process. Control</source>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">103977</object-id>
      </element-citation>
    </ref>
    <ref id="b45">
      <label>45</label>
      <element-citation publication-type="journal" id="sb45">
        <person-group person-group-type="author">
          <name>
            <surname>Al-Areqi</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Konyar</surname>
            <given-names>M.Z.</given-names>
          </name>
        </person-group>
        <article-title>Effectiveness evaluation of different feature extraction methods for classification of covid-19 from computed tomography images: A high accuracy classification study</article-title>
        <source>Biomed. Signal Process. Control</source>
        <volume>76</volume>
        <year>2022</year>
        <object-id pub-id-type="publisher-id">103662</object-id>
      </element-citation>
    </ref>
    <ref id="b46">
      <label>46</label>
      <element-citation publication-type="book" id="sb46">
        <person-group person-group-type="author">
          <name>
            <surname>Ahammad</surname>
            <given-names>S.H.</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>M.Z.U.</given-names>
          </name>
          <name>
            <surname>Lay-Ekuakille</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Giannoccaro</surname>
            <given-names>N.I.</given-names>
          </name>
        </person-group>
        <part-title>An efficient optimal threshold-based segmentation and classification model for multi-level spinal cord injury detection</part-title>
        <source>2020 IEEE International Symposium on Medical Measurements and Applications (MeMeA)</source>
        <year>2020</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="b47">
      <label>47</label>
      <element-citation publication-type="journal" id="sb47">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hef</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Duan</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lia</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Lib</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Quantitative analysis and automated lung ultrasound scoring for evaluating COVID-19 pneumonia with neural networks</article-title>
        <source>IEEE Trans. Ultrason. Ferroelectr. Freq. Control</source>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b48">
      <label>48</label>
      <element-citation publication-type="book" id="sb48">
        <person-group person-group-type="author">
          <name>
            <surname>Hofmanninger</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>COVID-19 CT segmentation dataset</part-title>
        <year>2020</year>
        <comment>
          <ext-link ext-link-type="uri" xlink:href="https://medicalsegmentation.com/covid19/" id="interref6">https://medicalsegmentation.com/covid19/</ext-link>
        </comment>
      </element-citation>
    </ref>
    <ref id="b49">
      <label>49</label>
      <element-citation publication-type="book" id="sb49">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <part-title>COVID-CT-dataset: a CT scan dataset about COVID-19</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2003.13865" id="interref7">arXiv:2003.13865</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b50">
      <label>50</label>
      <element-citation publication-type="book" id="sb50">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <part-title>Covid-ct-dataset: a ct scan dataset about covid-19</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2003.13865" id="interref8">arXiv:2003.13865</ext-link>. 490</comment>
      </element-citation>
    </ref>
    <ref id="b51">
      <label>51</label>
      <element-citation publication-type="journal" id="sb51">
        <person-group person-group-type="author">
          <name>
            <surname>Teixeira</surname>
            <given-names>L.O.</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>R.M.</given-names>
          </name>
          <name>
            <surname>Bertolini</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Oliveira</surname>
            <given-names>L.S.</given-names>
          </name>
          <name>
            <surname>Nanni</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Cavalcanti</surname>
            <given-names>G.D.</given-names>
          </name>
          <name>
            <surname>Costa</surname>
            <given-names>Y.M.</given-names>
          </name>
        </person-group>
        <article-title>Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images</article-title>
        <source>Sensors</source>
        <volume>21</volume>
        <issue>21</issue>
        <year>2021</year>
        <fpage>7116</fpage>
        <pub-id pub-id-type="pmid">34770423</pub-id>
      </element-citation>
    </ref>
    <ref id="b52">
      <label>52</label>
      <element-citation publication-type="book" id="sb52">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <part-title>U-net: Convolutional networks for biomedical image segmentation</part-title>
        <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>
        <year>2015</year>
        <publisher-name>Springer</publisher-name>
        <fpage>234</fpage>
        <lpage>241</lpage>
      </element-citation>
    </ref>
    <ref id="b53">
      <label>53</label>
      <element-citation publication-type="book" id="sb53">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Rahman Siddiquee</surname>
            <given-names>M.M.</given-names>
          </name>
          <name>
            <surname>Tajbakhsh</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Unet++: A nested u-net architecture for medical image segmentation</part-title>
        <source>Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</source>
        <year>2018</year>
        <publisher-name>Springer</publisher-name>
        <fpage>3</fpage>
        <lpage>11</lpage>
      </element-citation>
    </ref>
    <ref id="b54">
      <label>54</label>
      <element-citation publication-type="journal" id="sb54">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>C.-W.</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>P.-A.</given-names>
          </name>
        </person-group>
        <article-title>H-DenseUNet: hybrid densely connected UNet for liver and tumor segmentation from CT volumes</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <volume>37</volume>
        <issue>12</issue>
        <year>2018</year>
        <fpage>2663</fpage>
        <lpage>2674</lpage>
        <pub-id pub-id-type="pmid">29994201</pub-id>
      </element-citation>
    </ref>
    <ref id="b55">
      <label>55</label>
      <element-citation publication-type="journal" id="sb55">
        <person-group person-group-type="author">
          <name>
            <surname>Schlemper</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Oktay</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Schaap</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Heinrich</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kainz</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Glocker</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Attention gated networks: Learning to leverage salient regions in medical images</article-title>
        <source>Med. Image Anal.</source>
        <volume>53</volume>
        <year>2019</year>
        <fpage>197</fpage>
        <lpage>207</lpage>
        <pub-id pub-id-type="pmid">30802813</pub-id>
      </element-citation>
    </ref>
    <ref id="b56">
      <label>56</label>
      <element-citation publication-type="book" id="sb56">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <part-title>LCOV-NET: A lightweight neural network for COVID-19 pneumonia lesion segmentation from 3D CT images</part-title>
        <source>2021 IEEE 18th International Symposium on Biomedical Imaging</source>
        <conf-name>ISBI</conf-name>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>42</fpage>
        <lpage>45</lpage>
      </element-citation>
    </ref>
    <ref id="b57">
      <label>57</label>
      <element-citation publication-type="journal" id="sb57">
        <person-group person-group-type="author">
          <name>
            <surname>Chakraborty</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Mali</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>SuFMoFPA: A superpixel and meta-heuristic based fuzzy image segmentation approach to explicate COVID-19 radiological images</article-title>
        <source>Expert Syst. Appl.</source>
        <volume>167</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">114142</object-id>
      </element-citation>
    </ref>
    <ref id="b58">
      <label>58</label>
      <element-citation publication-type="journal" id="sb58">
        <person-group person-group-type="author">
          <name>
            <surname>Chakraborty</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Mali</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>SUFMACS: A machine learning-based robust image segmentation framework for COVID-19 radiological image interpretation</article-title>
        <source>Expert Syst. Appl.</source>
        <volume>178</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">115069</object-id>
      </element-citation>
    </ref>
    <ref id="b59">
      <label>59</label>
      <element-citation publication-type="book" id="sb59">
        <person-group person-group-type="author">
          <name>
            <surname>Nath</surname>
            <given-names>M.K.</given-names>
          </name>
          <name>
            <surname>Kanhe</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Mishra</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>A novel deep learning approach for classification of COVID-19 images</part-title>
        <source>2020 IEEE 5th International Conference on Computing Communication and Automation</source>
        <conf-name>ICCCA</conf-name>
        <year>2020</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>752</fpage>
        <lpage>757</lpage>
      </element-citation>
    </ref>
    <ref id="b60">
      <label>60</label>
      <element-citation publication-type="book" id="sb60">
        <person-group person-group-type="author">
          <name>
            <surname>Kumar</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Ranjan</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <part-title>A review on deep learning based pneumonia detection systems</part-title>
        <source>2021 International Conference on Artificial Intelligence and Smart Systems</source>
        <conf-name>ICAIS</conf-name>
        <year>2021</year>
        <publisher-name>IEEE</publisher-name>
        <fpage>289</fpage>
        <lpage>296</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec sec-type="data-availability" id="da1">
    <title>Data availability</title>
    <p id="d1e2422">Code will be released at <ext-link ext-link-type="uri" xlink:href="https://github.com/Lqs-github/ACL" id="interref3">https://github.com/Lqs-github/ACL</ext-link>.</p>
  </sec>
  <ack id="d1e5155">
    <title>Acknowledgements</title>
    <p id="d1e5158">This research was supported by the <funding-source id="GS1">Science and Technology Project of Sichuan, China</funding-source> (Grant NOs. 2022ZHCG0033, 2021YFG0314, 2020YFG0459), and the <funding-source id="GS2"><institution-wrap><institution-id institution-id-type="doi">10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source> (Grant NO. U19A2078).</p>
  </ack>
</back>
