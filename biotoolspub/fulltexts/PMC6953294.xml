<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6953294</article-id>
    <article-id pub-id-type="publisher-id">3190</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3190-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepECA: an end-to-end learning framework for protein contact prediction from a multiple sequence alignment</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Fukuda</surname>
          <given-names>Hiroyuki</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4567-4768</contrib-id>
        <name>
          <surname>Tomii</surname>
          <given-names>Kentaro</given-names>
        </name>
        <address>
          <email>k-tomii@aist.go.jp</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2151 536X</institution-id><institution-id institution-id-type="GRID">grid.26999.3d</institution-id><institution>Department of Computational Biology and Medical Sciences, Graduate School of Frontier Sciences, </institution><institution>The University of Tokyo, </institution></institution-wrap>5-1-5 Kashiwanoha, Kashiwa-shi, Chiba-ken, 277-8562 Japan </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2230 7538</institution-id><institution-id institution-id-type="GRID">grid.208504.b</institution-id><institution>Artificial Intelligence Research Center (AIRC), </institution><institution>Biotechnology Research Institute for Drug Discovery, Real World Big-Data Computation Open Innovation Laboratory (RWBC-OIL), National Institute of Advanced Industrial Science and Technology (AIST), </institution></institution-wrap>2-4-7 Aomi, Koto-ku, Tokyo, 135-0064 Japan </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>9</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>9</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>10</elocation-id>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>4</day>
        <month>11</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s). 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Recently developed methods of protein contact prediction, a crucially important step for protein structure prediction, depend heavily on deep neural networks (DNNs) and multiple sequence alignments (MSAs) of target proteins. Protein sequences are accumulating to an increasing degree such that abundant sequences to construct an MSA of a target protein are readily obtainable. Nevertheless, many cases present different ends of the number of sequences that can be included in an MSA used for contact prediction. The abundant sequences might degrade prediction results, but opportunities remain for a limited number of sequences to construct an MSA. To resolve these persistent issues, we strove to develop a novel framework using DNNs in an end-to-end manner for contact prediction.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We developed neural network models to improve precision of both deep and shallow MSAs. Results show that higher prediction accuracy was achieved by assigning weights to sequences in a deep MSA. Moreover, for shallow MSAs, adding a few sequential features was useful to increase the prediction accuracy of long-range contacts in our model. Based on these models, we expanded our model to a multi-task model to achieve higher accuracy by incorporating predictions of secondary structures and solvent-accessible surface areas. Moreover, we demonstrated that ensemble averaging of our models can raise accuracy. Using past CASP target protein domains, we tested our models and demonstrated that our final model is superior to or equivalent to existing meta-predictors.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">The end-to-end learning framework we built can use information derived from either deep or shallow MSAs for contact prediction. Recently, an increasing number of protein sequences have become accessible, including metagenomic sequences, which might degrade contact prediction results. Under such circumstances, our model can provide a means to reduce noise automatically. According to results of tertiary structure prediction based on contacts and secondary structures predicted by our model, more accurate three-dimensional models of a target protein are obtainable than those from existing ECA methods, starting from its MSA. DeepECA is available from <ext-link ext-link-type="uri" xlink:href="https://github.com/tomiilab/DeepECA">https://github.com/tomiilab/DeepECA</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Contact prediction</kwd>
      <kwd>Convolutional neural network</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Multiple sequence alignment</kwd>
      <kwd>Protein</kwd>
      <kwd>Secondary structure prediction</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009619</institution-id>
            <institution>Japan Agency for Medical Research and Development</institution>
          </institution-wrap>
        </funding-source>
        <award-id>JP19am0101110</award-id>
        <principal-award-recipient>
          <name>
            <surname>Tomii</surname>
            <given-names>Kentaro</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par11">Many methods have been developed for protein contact prediction, a crucially important step for protein structure prediction [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR19">19</xref>]. In the earlier stages of contact prediction history, most successful prediction methods were based on evolutionary coupling analysis (ECA) of large multiple sequence alignments (MSAs) of homologous sequences. In evolutionary processes, pairs of residues that are mutually proximate in the tertiary structure tend to co-evolve to maintain their structure. For instance, when one becomes larger, the other becomes smaller. Alternatively, when one becomes a positively charged residue, the other becomes a negatively charged residue.</p>
    <p id="Par12">Usually, evolutionary information includes noise because of indirect correlation between residues (A and B) when residues (A and C) and residues (B and C) are directly correlated. True correlation must be distinguished from such noise. Many challenges have been undertaken to do so. The methods used to address them can be categorized into two groups: Graphical Lasso and pseudo-likelihood maximization. Friedman et al. developed Graphical Lasso, a graph structure estimation method, in 2008 [<xref ref-type="bibr" rid="CR20">20</xref>]. It can estimate the graph structure from a covariance matrix using likelihood estimation of a precision matrix with L1 regularization. A well-known program that applies Graphical Lasso to contact prediction problems is PSICOV [<xref ref-type="bibr" rid="CR4">4</xref>]. A pseudo-likelihood method is used for an approximation method for probabilistic models, such as a Potts model, to estimate interaction strength between residues. It is usually difficult to calculate the marginal probability exactly. For that reason, such an approximation method is often used. Major programs using this method are EVFold [<xref ref-type="bibr" rid="CR5">5</xref>], plmDCA [<xref ref-type="bibr" rid="CR11">11</xref>], GREMLIN [<xref ref-type="bibr" rid="CR7">7</xref>], and CCMpred [<xref ref-type="bibr" rid="CR13">13</xref>].</p>
    <p id="Par13">After these extensive studies of ECA, meta-predictors emerged. The methods achieve protein contact prediction using the ECA method results as input features. MetaPSICOV [<xref ref-type="bibr" rid="CR14">14</xref>], a well-known supervised method, uses outputs of PSICOV, CCMpred, and FreeContact [<xref ref-type="bibr" rid="CR12">12</xref>] as input features and uses many other features such as secondary structure probability, solvent accessibility, and Shannon entropy. Using 672 features in this way, MetaPSICOV improved prediction accuracy much more than a single ECA method can. Subsequently, Wang et al. [<xref ref-type="bibr" rid="CR19">19</xref>] proposed a method based on an ultra-deep residual neural network and achieved much higher accuracy than had ever been attained previously. The recently reported DeepCov [<xref ref-type="bibr" rid="CR21">21</xref>], which is a conceptually similar method to ours uses a covariance matrix calculated from MSA for input features for DNN. For the 13th Community Wide Experiment on the Critical Assessment of Techniques for Protein Structure Prediction (CASP13), several groups used a deep neural network (DNN) for contact prediction. Among them, ResPRE [<xref ref-type="bibr" rid="CR22">22</xref>] used a precision matrix instead of a covariance matrix and DeepMetaPSICOV [<xref ref-type="bibr" rid="CR23">23</xref>] which combined the covariance-based method, DeepCov and features from MetaPSICOV.</p>
    <p id="Par14">Nevertheless, despite recent success achieved using these methods, most of them do not predict contacts from MSA directly. None has any means of optimizing the input MSAs. Some room for improvement remains for contact prediction pipeline optimization. As presented herein, we describe a novel approach to contact prediction that can extract correlation information, and which can predict contacts directly from MSA using a DNN in an end-to-end manner. Using DNN, one can outperform existing ECA methods, MetaPSICOV, DeepCov, ResPRE and DeepMetaPSICOV, and obtain comparable accuracy to that of RaptorX-Contact [<xref ref-type="bibr" rid="CR19">19</xref>] using no other additional input feature such as secondary structures. Furthermore, our DNN-based method can provide a means of optimizing the input MSAs in a supervised manner. The weight of each sequence in MSA is parameterized (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). It can be optimized through DNN to eliminate noise sequences in MSA automatically. In this model, we expect that more important sequences have greater weights and that less-important sequences have less weight after optimization. Today, a growing number of protein sequences are obtainable so that not all sequences in MSA necessarily have the same contacts. These sequences can introduce noise that affects contact prediction. In addition, Fox et al. [<xref ref-type="bibr" rid="CR24">24</xref>] reported that the contact prediction accuracy depends on the MSA accuracy. Motivated by those findings, we attempt to weight the sequences of MSA correctly. We also report that adding features and ensemble averaging can raise accuracy considerably and that high accuracy of secondary structures prediction can be achieved with our contact model using multi-task learning. Our experiments demonstrate that addition of a few features and the use of ensemble averaging are effective means of raising accuracy. High accuracy of secondary structures and accessible surface area prediction can be achieved using our contact model with multi-task learning. This result of multi-task learning suggests that contact information includes secondary structure and accessible surface area information. It can help to raise the accuracy of these predictions. Finally, we build a tertiary structure solely from predicted contacts and predicted secondary structures and retrieve a TMscore [<xref ref-type="bibr" rid="CR25">25</xref>] greater than 0.5 for 50 out of 105 (48%) CASP11 domains and 18 out of 55 (33%) CASP12 domains.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Schematic representation of weighted MSA: The left panel shows a part of the MSA. The right panel shows weight values for each sequence in the MSA</p></caption><graphic xlink:href="12859_2019_3190_Fig1_HTML" id="MO1"/></fig></p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <sec id="Sec3">
      <title>Effects of weighting sequences in an MSA</title>
      <p id="Par15">Here, we demonstrate that weighting of sequences in an MSA can boost prediction accuracy. Our network can learn correctly how to weight the MSA sequence. Figure <xref rid="Fig2" ref-type="fig">2</xref>a presents the distribution of the weight values of one protein. Results show that some values were nearly zero, which indicates that some noise sequences were present in the original MSA.
<fig id="Fig2"><label>Fig. 2</label><caption><p><bold>a</bold> One example of weight distribution in the sequences of one MSA for T0843 on the CASP11 dataset. <bold>b</bold> Accuracy improvement depends on the number of sequences in an MSA. We divided 160 protein domains into five bins according to their lengths. The numbers of proteins in the bins are equal (i.e., 32 protein domains in each bin). <bold>c</bold> Baseline Model top <italic>L</italic> accuracy shown against the Weighted MSA Model when we have over 200 homologous sequences and <bold>d</bold> with fewer than 200 homologous sequences</p></caption><graphic xlink:href="12859_2019_3190_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par16">To investigate the result further, we calculate the prediction accuracy dependence on the number of sequences in MSA using 160 protein domains of the CASP11 and CASP12 datasets. For these assessments, we select the results of Long top <italic>L</italic> prediction as a measure of accuracy because this area has the greatest number of predictions and because the standard deviation is smallest. Figure <xref rid="Fig2" ref-type="fig">2</xref>b shows that we can improve the prediction accuracy of more than 70% of targets when we have more than 200 sequences, but we cannot improve it when we have only a few sequences. The percentage of improvement is the number of improved proteins divided by the total number of proteins in a bin. This result demonstrates that the network can remove noise sequences when MSA has numerous homologous sequences. Figures <xref rid="Fig2" ref-type="fig">2</xref>c and d show an accuracy comparison between our Baseline Model and Weighted MSA Model (about our models, see <xref rid="Sec12" ref-type="sec">Method</xref>), which also supports our result.</p>
      <p id="Par17">Another approach to test our models is to increase the noise sequences in MSA and testing of the prediction accuracy robustness. We use HHblits and set <italic>E</italic>-values 1 and 3 and eliminate the “-cov” option to produce noisy MSAs and to predict contacts using these noisy MSAs as input. Table <xref rid="Tab1" ref-type="table">1</xref> presents the results. Because of the increasing noise, the prediction accuracy of Baseline Model is decreasing but that of Weighted MSA Model largely retains its accuracy. This result also indicates that our Weighted MSA Model can eliminate noise sequences.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Top <italic>L</italic> Contact Prediction Accuracy on the CASP11 dataset against HHblits e-values</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab1_HTML" id="MO3"/></table-wrap></p>
      <p id="Par18">In the experiments conducted on the CASP11 and CASP12 datasets, but not in all prediction categories, we can improve accuracy using the Weighted MSA Model. To assess the effects of weighting sequences further, we compare the accuracies of the Baseline Model and the Weighted MSA Model on one of our five validation datasets. The best epochs of each model are determined by the average loss of the validation set. Using these epochs, the accuracies of the models are calculated. Table <xref rid="Tab2" ref-type="table">2</xref> shows that the accuracies of the Weighted MSA Model are higher than those of the Baseline Model at every distance and prediction count. These differences were inferred as significant from Student’s <italic>t</italic>-test results.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Accuracy comparison between the Baseline Model and the Weighted MSA Model tested on the validation dataset and the <italic>p</italic>-value of Student’s <italic>t</italic>-test</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab2_HTML" id="MO4"/></table-wrap></p>
      <p id="Par19">To investigate the extent to which each feature (gap ratio, sequence identity and sequence identity with a consensus sequence) contributes to improvement of accuracy, we train the Weighted MSA Model without each feature and their average values. Furthermore, we compare the prediction accuracies for the validation dataset. The results are shown as “Drop Consensus”, “Drop Identity”, and “Drop Gap Ratio” models in Table <xref rid="Tab3" ref-type="table">3</xref>a. Prediction accuracies of these feature-dropped models are between those of the Baseline Model and the Weighted MSA Model. The accuracy becomes lowest when we drop sequence identity with a consensus sequence and its average value, which means that the contribution of this feature to the accuracy is the highest among three features. The contribution of the gap ratio is the smallest, but a slight contribution is observed in Medium <italic>L</italic>/5 and Long <italic>L</italic>/5 categories.</p>
      <p id="Par20">In the paper describing PSICOV, another method to weight sequences in MSA was introduced before ours. It weights sequences in an MSA using several redundant sequences in the MSA to eliminate redundancy. However, it is not optimized in an end-to-end manner. To compare the accuracy of these two weighting methods, we calculate the weight values of PSICOV separately and apply them to our Baseline Model. The result is presented as the “Baseline+PSICOV” model in Table <xref rid="Tab3" ref-type="table">3</xref> (B). In this experiment using our weighting method, the Weighted MSA Model is equivalent to or better than “Baseline+PSICOV” model at every distance and prediction count.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Accuracy comparisons of (<bold>a</bold>) the dropped feature models and (<bold>b</bold>) the weighing method of PSICOV against the Weighted MSA Model tested on the validation dataset. Bold typeface characters show the highest accuracy in the columns</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab3_HTML" id="MO5"/></table-wrap></p>
      <p id="Par21">Finally, we present distributions of sequence weights calculated using the Weighted MSA Model for a protein chain from the validation dataset. The calculated weights are shown respectively against the gap ratio, sequence identity, and sequence identity with a consensus sequence (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). As shown in Figs. <xref rid="Fig3" ref-type="fig">3</xref> and S1, dependencies of sequence weights against their gap ratio and sequence identity can be observed to some extent in some cases. However, such dependencies are not always evident. As described above, sequence identity with a consensus sequence and its average value have the highest contribution to our model. The relations between weights and this feature are complicated. At least, these are not linear dependencies (perhaps because we use DNN to weight the sequences). Other examples of relations between weights and features are shown in Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S1. These plots show that these relations vary depending on proteins and their MSAs.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Distributions of weight values of (<bold>a</bold>) the gap ratio, (<bold>b</bold>) sequence identity and (<bold>c</bold>) identity with a consensus sequence. Each dot represents a sequence in the MSA of 1EEJ</p></caption><graphic xlink:href="12859_2019_3190_Fig3_HTML" id="MO6"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Effects of adding features</title>
      <p id="Par23">In our experiments, adding a few sequential features was useful for increasing the prediction accuracy in cases with shallow MSAs. Results showed that the Feature Added Model can produce considerable accuracy gains of prediction at long range for the CASP11 and CASP12 datasets (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). Although DNN can find useful features automatically, handmade feature engineering is still effective in our experiments. For this experiment, we added five features, as described in <xref rid="Sec12" ref-type="sec">Method</xref>.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Accuracy improvement depends on the number of sequences in an MSA. The mean differences of prediction accuracy, between the Feature Added model and the Weighted MSA Model, against the number of sequences in an MSA, are shown for (<bold>a</bold>) top <italic>L</italic>/5, (<bold>b</bold>) top <italic>L</italic>/2, and (<bold>c</bold>) top <italic>L</italic> contacts of prediction at long range. The number of proteins in each bin is equal (i.e., 32 protein domains in each bin)</p></caption><graphic xlink:href="12859_2019_3190_Fig4_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>Effects of multi-task learning</title>
      <p id="Par24">Presumably, a predicted contact map includes secondary structure information. Based on this assumption, we tried to use multi-task learning to predict contacts and secondary structures simultaneously. We examined three state secondary structure prediction. Table <xref rid="Tab4" ref-type="table">4</xref> presents the results. Our method outperformed existing methods such as RaptorX-Property [<xref ref-type="bibr" rid="CR26">26</xref>] and SCRATCH-1D [<xref ref-type="bibr" rid="CR27">27</xref>] in terms of prediction accuracy. This result demonstrates that our 2D feature maps are a good representation of secondary structure prediction. It also demonstrates that we can extract useful information from these feature maps through multi-task learning. In our experiments, convergence of the secondary structure prediction differed from that of contact prediction. We use the best epoch of each. SCRATCH-1D uses structural data from PDB to predict secondary structures. The time stamp of the structural data is June 2015, which is after the CASP11 experiment. This might explain why SCRATCH-1D obtains better results with the CASP11 dataset than the results obtained using the CASP12 dataset.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Secondary structure prediction accuracy on the (<bold>a</bold>) CASP11 and (<bold>b</bold>) CASP12 datasets. Bold typeface characters show the highest accuracy in the column</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab4_HTML" id="MO8"/></table-wrap></p>
      <p id="Par25">To investigate these results further, the recall and precision of each predicted secondary structure class on the CASP11 and CASP12 datasets are calculated and are presented in Table <xref rid="Tab5" ref-type="table">5</xref>. The model shows especially good results for precision of sheet prediction on both the CASP11 and CASP12 datasets. Although SCRATCH-1D shows better results for the recall of helix and sheet prediction and precision of coil prediction on the CASP11 dataset because of the structural data used in SCRATCH-1D, our model outperforms the other two methods in almost all classes on the CASP12 dataset.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Recall and precision of secondary structure components on the (<bold>a</bold>) CASP11 and (<bold>b</bold>) CASP12 datasets. Bold typeface characters show the highest accuracy in the column</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab5_HTML" id="MO9"/></table-wrap></p>
      <p id="Par26">We also compared the prediction results of accessible surface area with those obtained using two other methods. Our model, which is a regression model, outputs the predicted accessible surface area as a real number. However, RaptorX-Property is a classification model that outputs the relative solvent accessibility in three states: B, Buried; M, Medium; and E, Exposed. (10 and 40% are the thresholds). Furthermore, SCRATCH-1D outputs relative solvent accessibility in 20 classes (0–95% in 5% increments). To compare these three results, the results of our models and SCRATCH-1D are converted to three state prediction, similarly to RaptorX-Property. As in secondary structure prediction, our model can obtain the highest accuracies among these three methods (Table <xref rid="Tab6" ref-type="table">6</xref>).
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Accessible surface area prediction accuracy on the (<bold>a</bold>) CASP11 and (<bold>b</bold>) CASP12 datasets. Bold typeface characters show the highest accuracy in the columns</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab6_HTML" id="MO10"/></table-wrap></p>
      <p id="Par27">Finally, we analyze what types of contacts (e.g. helix–helix, helix–sheet and sheet–sheet) are better predicted with the Feature Added Model and the Multi-task Model. Table <xref rid="Tab7" ref-type="table">7</xref> shows the results. On both the CASP11 and CASP12 dataset, recalls of the Multi-task Model are equivalent to or higher than those of the Feature Added Model for contacts of all three types rather than a particular type of contact. Regarding precision, the sheet–sheet contact of the Feature Added Model is better than that of the Multi-task Model. The secondary structure types contribute somewhat to the contact prediction accuracy.
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Recall and Precision of three types of contact: helix–helix (HH), helix–sheet (HS), and sheet–sheet (SS) on the (<bold>a</bold>) CASP11 and (<bold>b</bold>) CASP12 datasets</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab7_HTML" id="MO11"/></table-wrap></p>
    </sec>
    <sec id="Sec6">
      <title>Effects of ensemble averaging</title>
      <p id="Par28">Regarding the model ensemble, according to the machine learning theory, ensemble methods of some types exist such as bagging, boosting, and stacking. Our ensemble averaging is similar to bagging. It uses bootstrapping samples as training data. However, in our case, we use datasets from cross validation. Generally, ensemble models use weak classifiers such as a decision tree as a base model. We use DNN, which is not regarded as a weak classifier. However, in our experiments, the ensemble model is still effective. Tables <xref rid="Tab8" ref-type="table">8</xref> and <xref rid="Tab9" ref-type="table">9</xref> show that ensemble-learning can raise the accuracy considerably for almost all prediction categories, except Medium top <italic>L</italic>/10 prediction on the CASP12 dataset.
<table-wrap id="Tab8"><label>Table 8</label><caption><p>Contact prediction accuracy comparison between single learning and ensemble averaging on the CASP11 dataset. Bold typeface characters show that ensemble averaging can raise the accuracy of this field</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab8_HTML" id="MO12"/></table-wrap>
<table-wrap id="Tab9"><label>Table 9</label><caption><p>Contact prediction accuracy comparison between single learning and ensemble averaging on the CASP12 dataset. Bold typeface characters signify that ensemble averaging can raise the accuracy of this field</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab9_HTML" id="MO13"/></table-wrap></p>
      <p id="Par29">We also investigate how contact prediction accuracy depends on the training datasets in our ensemble averaging. We test 3-, 5-, 7-, and 10-fold and compare the respective degrees of accuracy using a Baseline Model. Generally, it is expected that as the number of folds increases, prediction accuracy is also increasing, but it eventually reaches a plateau because the overlap of data is large and because the model diversity becomes small. Table <xref rid="Tab10" ref-type="table">10</xref> shows that the 10-fold result yields the highest accuracy at almost all prediction categories. However, the difference is not so large. We use 5-fold to save computational time for all experiments.
<table-wrap id="Tab10"><label>Table 10</label><caption><p>Dependencies of prediction accuracy on the number of folds on the CASP11 dataset. Bold typeface characters show the highest accuracy in the column</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab10_HTML" id="MO14"/></table-wrap></p>
    </sec>
    <sec id="Sec7">
      <title>Accuracy comparison for the CASP11 and CASP12 targets</title>
      <p id="Par30">Tables <xref rid="Tab11" ref-type="table">11</xref> and <xref rid="Tab12" ref-type="table">12</xref> respectively present the predictive accuracies of five existing methods and our methods. We evaluated our method using the CASP11 and CASP12 datasets. Both the CASP11 and CASP12 datasets yielded similar results. Even our baseline method outperformed existing ECA methods at every distance and prediction count. Additionally, our baseline model outperformed DeepCov, which also takes the covariance matrices as input and which uses DNN. Comparison against other existing models revealed that the Multi-task Model can outperform metaPSICOV, ResPRE, and DeepMetaPSICOV, and that it can obtain comparable results to those of RaptorX-Contact.
<table-wrap id="Tab11"><label>Table 11</label><caption><p>Contact prediction accuracy on the CASP11 dataset. Bold typeface characters show the highest accuracy in the column</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab11_HTML" id="MO15"/></table-wrap>
<table-wrap id="Tab12"><label>Table 12</label><caption><p>Contact prediction accuracy on the CASP12 dataset. Bold typeface characters show the highest accuracy in the column.</p></caption><graphic position="anchor" xlink:href="12859_2019_3190_Tab12_HTML" id="MO16"/></table-wrap></p>
      <p id="Par31">Among our models, results show that Weighted MSA, Feature Added, and Multi-task Models can gradually raise the total accuracy compared with our baseline model, except for Weighted MSA Model in CASP12. The Weighted MSA Model is ineffective in such situations because most CASP12 targets have an insufficient number of homologous sequences in MSA.</p>
    </sec>
    <sec id="Sec8">
      <title>Tertiary structure prediction</title>
      <p id="Par32">From the predicted contacts and secondary structures obtained using our Multi-task Model, we attempt to construct tertiary structures using the CONFOLD script [<xref ref-type="bibr" rid="CR28">28</xref>]. We measure the quality of predicted structures in terms of the TMscore. The average TMscores are 0.472 (CASP11) and 0.402 (CASP12). We can obtain a TMscore over 0.5 only by MSA information against 50 in 105 (48%) of CASP11 domains and 18 in 55 (33%) of CASP12 domains. Especially when we have more than 0.8 top <italic>L</italic> predicted contact accuracy, the numbers improve to 17 in 22 (77%) of CASP11 domains and 5 in 7 (71%) of CASP 12 domains. Here, we present an example of the best predicted structure T0811-D1 (TMscore 0.818) in CASP11 and T0920-D1 (TMscore 0.848) in CASP12 (Fig. <xref rid="Fig5" ref-type="fig">5</xref>). In these domains, the accuracies of top <italic>L</italic> contact predictions are 85.3% (T0811-D1) and 86.3% (T0920-D1).
<fig id="Fig5"><label>Fig. 5</label><caption><p>(<bold>a</bold>) Our best predicted model T0811-D1 in CASP11 and (<bold>b</bold>) T0920-D1 in CASP12. Cyan shows the native structure. Green represents our model</p></caption><graphic xlink:href="12859_2019_3190_Fig5_HTML" id="MO17"/></fig></p>
    </sec>
    <sec id="Sec9">
      <title>Calculation time</title>
      <p id="Par33">In terms of calculation time, our method also exhibits good performance. We compare the calculation time of our method with that of CCMpred, which is the fastest method among existing ECA methods. Table <xref rid="Tab13" ref-type="table">13</xref> shows that our method takes much less time than the CCMpred with or without GPU, when we used 150 proteins in the PSICOV dataset. Although Graphical Lasso and pseudo-likelihood methods have iterative calculations, neural network methods can calculate the result directly. Results are obtainable in a short time once one has completed network training. Our method is practically useful when huge numbers of contact predictions are necessary.
<table-wrap id="Tab13"><label>Table 13</label><caption><p>Calculation time of CCMpred and our method</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>CCMpred (CPU)</th><th>CCMpred (GPU)</th><th>Our DNN (CPU)</th><th>Our DNN (GPU)</th></tr></thead><tbody><tr><td>Time (min)</td><td char="." align="char">585</td><td char="." align="char">47</td><td char="." align="char">10</td><td char="." align="char">2</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Discussion</title>
    <p id="Par34">This report presented a novel approach of end-to-end learning for protein contact prediction. On the CASP11 and CASP12 test proteins, for all precisions (short, medium, and long), we confirmed that our models performed better than any other ECA method. Moreover, we were able to obtain comparable results to those obtained using RaptorX-Contact, a successful prediction method that uses outputs of an ECA method (CCMpred) and additional features as inputs, although we use much simpler features derived from an MSA as inputs. Using our prediction results including secondary structures as inputs of other meta-predictors might engender higher precision.</p>
    <p id="Par35">When extracting correlation information for one residue pair, 21 × 21 correlation scores from 21 × 21 amino acid pairs are obtained. However, these scores are merely averaged in PSICOV. By contrast, our method uses 441 covariance matrices as input features and feeds them to the CNN architecture. This method does not engender loss of information, which is an important benefit of our method compared to PSICOV. Moreover, the CNN architecture can extract useful features from covariance matrices automatically through convolutional operation.</p>
    <p id="Par36">Comparison with existing meta-predictors such as metaPSICOV, DeepMetaPSICOV, and RaptorX-Contact revealed that, although we use only correlation information based on an MSA and use no other feature such a secondary structure as input, all our methods outperformed metaPSICOV. Moreover, the Multi-task Model outperformed DeepMetaPSICOV and yielded comparable results to those obtained using RaptorX-Contact. Our methods show better results for short range prediction than results obtained with RaptorX-Contact.</p>
    <p id="Par37">Using DNN, we can not only raise the accuracy of contact prediction: we also have an opportunity to weight sequences in an MSA in an end-to-end manner. Recently, we have become able to access an increasing number of protein sequences including metagenomic sequences, which can include many noise sequences for contact prediction. In such situations, our method provides a means to eliminate noise sequences automatically and to find relevant ones.</p>
    <p id="Par38">Results of our study demonstrate that adding features and using ensemble averaging can raise accuracy. Furthermore, we demonstrate that we can obtain high prediction accuracy of contact, secondary structure and accessible surface area prediction in one network merely using MSA information. This result illustrates that contact information strongly regulates the secondary structure but that the secondary structure information does not include contact information. Recently, Hanson et al. [<xref ref-type="bibr" rid="CR29">29</xref>] described that the predicted contact maps improve the accuracy of secondary structure prediction. Our result is consistent with those described in that report.</p>
    <p id="Par39">When the available homologous sequences are few, existing methods, including our methods, are incapable of predicting contacts accurately, although our method is effective to some degree for cases of shallow MSAs. As the next step, we would like to improve the MSA construction process and to collect sufficient evolutional information from wider sequence spaces through extensive research.</p>
    <p id="Par40">As for tertiary structure prediction, some proteins exist for which we cannot obtain good models, even though our contact prediction results are fairly good. One example of these results is T0845-D1. For this protein, the predicted contact accuracy is 86.6% (for top <italic>L</italic> prediction), but the resultant TMscore is 0.276. Figure <xref rid="Fig6" ref-type="fig">6</xref> portrays the structure of this sample. The general shape of this predicted model is similar to the native structure, but all strands go in opposite directions against the native structure. Actually, T0845 is a 97-residue protein with 127 long-range contacts (1.32 L). In this case, 86.6% top <italic>L</italic> prediction is insufficient. More precise contact information would be necessary to solve such a mirror image-like problem. Furthermore, more sophisticated tertiary structure construction methods are necessary.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Badly predicted model obtained in spite of good predicted contacts: (<bold>a</bold>) predicted model and (<bold>b</bold>) native structure</p></caption><graphic xlink:href="12859_2019_3190_Fig6_HTML" id="MO18"/></fig></p>
  </sec>
  <sec id="Sec11">
    <title>Conclusions</title>
    <p id="Par41">As described in this paper, we propose an end-to-end learning framework of protein contact prediction that can effectively use information derived from either deep or shallow MSAs. For deep MSAs, our model can perform weighting of the sequences in MSA to eliminate noise sequences and to gain accuracy. However, for shallow MSAs, it is useful to add some features derived from the sequence itself and MSA to improve the accuracy. Results demonstrate that our model can obtain good results compared with existing ECA methods such as PSICOV, CCMpred, DeepCOV, and ResPRE when tested on the CASP11 and CASP12 datasets. Moreover, our Multi-task Model is good at predicting secondary structures. Using these predicted contact and secondary structures, we can obtain more accurate three-dimensional models of a target protein than those obtained using existing ECA methods, starting from its MSA.</p>
  </sec>
  <sec id="Sec12">
    <title>Method</title>
    <sec id="Sec13">
      <title>Datasets</title>
      <p id="Par42">An original dataset was prepared for this study using the following steps. 1) A set of non-redundant amino acid sequences was obtained from PISCES, a PDB sequence culling server (30% sequence identity cutoff, 2.5 Å resolution cutoff, 1.0 R-factor cutoff, 15,209 total number of chains as of April 5, 2018) [<xref ref-type="bibr" rid="CR30">30</xref>]. 2) PDB files were retrieved. Then true contact pairs were calculated from the protein coordinates. For this study, we defined a contact if the distance of C<sub>β</sub> atoms of the residue pair was less than 8 Å. For glycine residues, C<sub>α</sub> atoms were used instead of C<sub>β</sub> atoms. The PDB coordinates include many missing values (in our dataset, more than 5000 proteins have at least one missing value for C<sub>β</sub> atoms). Therefore, we marked a residue pair that had a missing C<sub>β</sub> coordinate as NaN and excluded it when we calculated the loss. 3) Removal of redundancy was performed with the test set (see below). We excluded from our dataset those proteins sharing &gt; 25% sequence identity or having a BLAST <italic>E</italic>-value &lt; 0.1 with any test protein by blastp [<xref ref-type="bibr" rid="CR31">31</xref>]. 4) Proteins with length greater than 700 residues or with fewer than 25 residues were also eliminated. At this stage, our dataset comprised 13,262 protein chains. In ensemble averaging (see below), we split them into five (up to ten) sets and used one of them as a validation set. We used the remaining sets as training sets for the respective models. For our Multi-task Model described below, secondary structures and solvent-accessible surface areas of proteins were calculated using DSSP [<xref ref-type="bibr" rid="CR32">32</xref>]. We used only those proteins for which the secondary structure states could be assigned for 80% or more of their residues. We noticed that one protein, 12AS had been removed by error. Consequently, 1938 protein chains were excluded from the 13,262 protein chains. For fair comparison between our models, the remaining 11,324 protein chains were used in all experiments. We used one of our five training/validation datasets to evaluate effects of weighting sequences in an MSA (results shown in Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref> and Fig. <xref rid="Fig3" ref-type="fig">3</xref>). This dataset includes 9058 protein chains for training and 2266 protein chains for validation. As the test sets for benchmarking our methods, we used the CASP11 (105 domains) and CASP12 (55 domains) dataset [<xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>] obtained from the CASP download area (<ext-link ext-link-type="uri" xlink:href="http://www.predictioncenter.org/download_area/">http://www.predictioncenter.org/download_area/</ext-link>). We prepared MSAs for proteins in both our original and test datasets using HHblits [<xref ref-type="bibr" rid="CR35">35</xref>] with three iterations. The threshold <italic>E</italic>-value was set to 0.001 on the UniProt20_2016 library. Sequence coverage was set to 60% using the “-cov” option. These settings were the same as those used in PSICOV.</p>
    </sec>
    <sec id="Sec14">
      <title>Neural network models</title>
      <p id="Par43">We developed our neural network models to achieve improvement in the respective precisions of both shallow and deep MSAs. Moreover, we expanded our model to a multi-task model to increase the prediction accuracy by incorporation with predictions of secondary structures and solvent-accessible surface areas. Methods using convolutional neural networks (CNNs), which are widely applied to image classification tasks, have been used successfully for protein contact prediction [<xref ref-type="bibr" rid="CR36">36</xref>]. Therefore, we also used CNNs in our models.</p>
      <p id="Par44">As in Graphical Lasso methods, our models take covariance matrices calculated from MSAs as their inputs to calculate the probability of contact for each residue pair in a protein. To calculate covariance matrices, we used a formula used for a study of PSICOV, as shown below.
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ S{a}_i{b}_j=f\left({a}_i{b}_j\right)-f\left({a}_i\right)f\left({b}_j\right) $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mi>S</mml:mi><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:math><graphic xlink:href="12859_2019_3190_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par45">Therein, <italic>a</italic> and <italic>b</italic> respectively represent amino acid types at positions <italic>i</italic> and <italic>j</italic>. Also, <italic>f</italic> (<italic>a</italic><sub><italic>i</italic></sub>) (and <italic>f</italic> (<italic>b</italic><sub><italic>j</italic></sub>)), respectively denote frequencies of amino acid <italic>a</italic> (and <italic>b</italic>) at position <italic>i</italic> (and <italic>j</italic>); <italic>f</italic> (<italic>a</italic><sub><italic>i</italic></sub><italic>b</italic><sub><italic>j</italic></sub>) stands for the frequency of amino acid pairs <italic>a</italic> and <italic>b</italic> at positions <italic>i</italic> and <italic>j</italic>. If no correlation is found between <italic>i</italic> and <italic>j</italic> with respect to amino acid pairs <italic>a</italic> and <italic>b</italic>, then <italic>Sa</italic><sub><italic>i</italic></sub><italic>b</italic><sub><italic>j</italic></sub> is equal to zero. Using this formula with pairs of 21 amino acid type (including a gap), one can obtain 441 <italic>L</italic> × <italic>L</italic> covariance matrices, where <italic>L</italic> signifies the sequence length of a target protein. Our input covariance matrices are <italic>L</italic> × <italic>L</italic> pixel images with 441 channels: typical color images have three channels. Therefore, we can apply a CNN. For this study, we adopt a residual network [<xref ref-type="bibr" rid="CR37">37</xref>] to deepen the model and to achieve higher accuracy. We tested the four model variants described below. Their architectures are presented in Fig. <xref rid="Fig7" ref-type="fig">7</xref>.
<fig id="Fig7"><label>Fig. 7</label><caption><p>Architectures of the proposed networks: (<bold>a</bold>) Baseline Model, (<bold>b</bold>) Weighted MSA Model, (<bold>c</bold>) Feature Added Model, and (<bold>d</bold>) Multi-task Model</p></caption><graphic xlink:href="12859_2019_3190_Fig7_HTML" id="MO19"/></fig></p>
      <p id="Par46">A) Baseline Model: First, in this model, 441 channels of <italic>L</italic> × <italic>L</italic> covariance matrices calculated from MSAs are fed into a 1 × 1 CNN to reduce the dimensionality of channels to 128. Then the matrices are fed into the 30-block residual network. Each residual block has two CNN layers. The total number of layers in our residual network is 60. We used 60 layers because of GPU memory limitations. Each output of the residual network is 128 channels of <italic>L</italic> × <italic>L</italic> matrices. We transform them and feed them into a fully connected layer and sigmoid function to obtain contact probabilities.</p>
      <p id="Par47">B) Weighted MSA Model: To reduce noise of MSA, we weight each sequence of an MSA in this model. This weighting is also assigned using a neural network. First, we use a multilayer perceptron (MLP) network to calculate the weight for each sequence in an MSA using features of seven types: the number of sequences in an MSA, sequence identity with a target sequence, sequence identity with a consensus sequence of an MSA, the gap ratio for each sequence, and average values of the last three features (i.e., sequence identities and a gap ratio). The MLP, which has two hidden layers and for which each hidden layer has seven nodes, are used for this task. The output of this network is then used to weight each sequence in an MSA. Subsequently, based on the weighted MSA, 441 <italic>L</italic> × <italic>L</italic> covariance matrices are calculated and are fed into a 1 × 1 CNN. Because all these calculations can be written as matrix operations and because they can be represented by one connected network, gradients of loss function with respect to each variable in MLP and CNN are calculable through backpropagation. Consequently, the network can be optimized completely in an end-to-end manner.</p>
      <p id="Par48">C) Feature Added Model: To this model, we add five features: a query sequence, a Position Specific Score Matrix (PSSM), entropy of each column of weighted MSA, mutual information of each column pair of weighted MSA, and sequence separations calculated from query sequences. The first three features are 1D features of length <italic>L</italic>. These 1D features are stacked <italic>L</italic> times vertically to shape <italic>L</italic> × <italic>L</italic> matrices. We also used a transposed version of these matrices because information of both <italic>i</italic> and <italic>j</italic> at position (<italic>i</italic>, <italic>j</italic>) must be obtained. We treat query sequences and PSSMs as categorical variables and apply one-hot encoding to these features. The final dimensions of these features are (<italic>L</italic>, <italic>L</italic>, 20 × 2) for query sequences, (<italic>L</italic>, <italic>L</italic>, 21 × 2) for PSSMs, and (<italic>L</italic>, <italic>L</italic>, 1 × 2) for entropy. The final dimensions of both mutual information and sequence separations are (<italic>L</italic>, <italic>L</italic>, 1). Finally, after concatenating these features to covariance matrices and reducing their dimensionality to 128, we feed them into residual networks.</p>
      <p id="Par49">D) Multi-task Model: Secondary structures are also key elements to predict tertiary structures. Multi-task learning, a common technique of DNN [<xref ref-type="bibr" rid="CR38">38</xref>, <xref ref-type="bibr" rid="CR39">39</xref>] is also used in protein research [<xref ref-type="bibr" rid="CR40">40</xref>]. In our case, we try to predict contacts, secondary structures, and accessible surface areas simultaneously using multi-task learning. Although the network is based on the Feature Added model, after 20 blocks of residual network, we separate the residual blocks for each task: we share the parameters of 20 residual blocks within these three tasks and do not share the last 10 residual blocks. Finally, the outputs of these residual blocks are fed respectively into a fully connected layer to predict contacts, secondary structures, and accessible surface areas. For the secondary structures and accessible surface areas, we use an <italic>i</italic>-th row and an <italic>i</italic>-th column of the <italic>L</italic> × <italic>L</italic> matrices and concatenate them as features of <italic>i</italic>-th residues.</p>
      <p id="Par50">We calculate the losses separately and add them for joint training.</p>
      <p id="Par51">Total Loss = Loss Contact + Loss Secondary Structure + Loss Accessible Surface Area (2).</p>
      <p id="Par52">We define each term, in eq. (2), as</p>
      <p id="Par53">
        <disp-formula id="Equ2">
          <label>3</label>
          <alternatives>
            <tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{Contact}\kern0.28em \mathrm{Loss}=-{\sum}_{ij}\left({y}_{Contact\kern0.28em ij}\log {p}_{Contact\kern0.28em ij}+\left(1-{y}_{Contact\kern0.28em ij}\right)\log \left(1-{P}_{Contact\kern0.28em ij}\right)\right) $$\end{document}</tex-math>
            <mml:math id="M4" display="block">
              <mml:mrow>
                <mml:mi mathvariant="normal">C</mml:mi>
                <mml:mi mathvariant="normal">o</mml:mi>
                <mml:mi mathvariant="normal">n</mml:mi>
                <mml:mi mathvariant="normal">t</mml:mi>
                <mml:mi mathvariant="normal">a</mml:mi>
                <mml:mi mathvariant="normal">c</mml:mi>
                <mml:mi mathvariant="normal">t</mml:mi>
              </mml:mrow>
              <mml:mspace width="thickmathspace"/>
              <mml:mrow>
                <mml:mi mathvariant="normal">L</mml:mi>
                <mml:mi mathvariant="normal">o</mml:mi>
                <mml:mi mathvariant="normal">s</mml:mi>
                <mml:mi mathvariant="normal">s</mml:mi>
              </mml:mrow>
              <mml:mo>=</mml:mo>
              <mml:mo>−</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mo>∑</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mi>j</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>y</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>C</mml:mi>
                    <mml:mi>o</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>c</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mspace width="thickmathspace"/>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mi>log</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>p</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>C</mml:mi>
                    <mml:mi>o</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>c</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mspace width="thickmathspace"/>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>+</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo>−</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>y</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>C</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>c</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mspace width="thickmathspace"/>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>)</mml:mo>
                </mml:mrow>
                <mml:mi>log</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo>−</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>P</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>C</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>c</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mspace width="thickmathspace"/>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>)</mml:mo>
                </mml:mrow>
                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:math>
            <graphic xlink:href="12859_2019_3190_Article_Equ2.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p id="Par54">where <italic>y</italic><sub><italic>contact ij</italic></sub> is the true label (1 for contact, otherwise 0) for the residue pair of (<italic>i</italic>, <italic>j</italic>) positions and <italic>p</italic><sub><italic>contact ij</italic></sub> is the predicted contact probability. The summation is calculated over all residue pairs of (<italic>i</italic>, <italic>j</italic>), except when the true label is not missing values.
<disp-formula id="Equ3"><label>4</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{Secondary}\kern0.28em \mathrm{Structure}\kern0.28em \mathrm{Loss}=-{\sum}_k\left({y}_{Helix\kern0.28em k}\log {p}_{Helix\kern0.28em k}+{y}_{Sheet\kern0.28em k}\log {p}_{Sheet\kern0.28em k}+{y}_{Coil\kern0.28em k}\log {p}_{Coil\kern0.28em k}\right) $$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">y</mml:mi></mml:mrow><mml:mspace width="thickmathspace"/><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mspace width="thickmathspace"/><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mspace width="thickmathspace"/><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mspace width="thickmathspace"/><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thickmathspace"/><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mspace width="thickmathspace"/><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mspace width="thickmathspace"/><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>l</mml:mi><mml:mspace width="thickmathspace"/><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math><graphic xlink:href="12859_2019_3190_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par55">Therein, <italic>y</italic><sub><italic>Helix k</italic></sub>, <italic>y</italic><sub><italic>Sheet k</italic></sub>, and <italic>y</italic><sub><italic>Coil k</italic></sub> respectively represent the one-hot encoded true label for the <italic>k</italic><sub><italic>th</italic></sub> residue of helix, sheet, and coil. In addition, <italic>p</italic><sub><italic>Helix k</italic></sub>, <italic>p</italic><sub><italic>Sheet k</italic></sub>, and <italic>p</italic><sub><italic>Coil k</italic></sub> respectively denote their predicted probabilities. The summation is calculated over all residues, except when the true label is missing.
<disp-formula id="Equ4"><label>5</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{Accessible}\ \mathrm{Surface}\ \mathrm{Area}\ \mathrm{Loss}=\sqrt{\frac{\sum_k{\left( AS{A}_{true\kern0.24em k}- AS{A}_{pred\;k}\right)}^2}{N}} $$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mtext>Accessible Surface Area Loss</mml:mtext><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="italic">AS</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext mathvariant="italic">true</mml:mtext><mml:mspace width="0.24em"/><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="italic">AS</mml:mi><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mtext mathvariant="italic">pred</mml:mtext><mml:mspace width="0.12em"/><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:msqrt></mml:math><graphic xlink:href="12859_2019_3190_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par56">In that equation, <italic>ASA</italic><sub><italic>true k</italic></sub> and <italic>ASA</italic><sub><italic>pred k</italic></sub> respectively stand for the accessible surface area of the true value and the predicted value of the <italic>k</italic><sub><italic>th</italic></sub> residue. In addition, <italic>N</italic> signifies the total number of residues calculated from the accessible surface area. The summation is over the same residues as those used in the case of secondary structures.</p>
      <p id="Par57">For our experiments, all filter sizes of convolutional operations in the residual network are 3 × 3. The ReLU activation function is used. We trained all these networks using the ADAM optimizer with the learning rate of 0.0005. Batch normalization is used to obtain higher accuracy and faster convergence. One batch includes the data of one domain. Proteins have their different lengths. Therefore, input matrices can have different sizes. However, because the number of our network parameters is independent of protein length, we can deal comprehensively with proteins of different lengths. Furthermore, by calculating the gradient and updating the network parameters by one batch size, we obviate the use of zero padding. All hyperparameters and network architectures such as the number of layers and variation of connections are selected according to the results achieved for validation sets. All experiments were conducted using an ordinary desktop computer with a GPU (GeForce TITAN X; Nvidia Corp.) using the TensorFlow library. Training required several days to calculate 20–30 epochs.</p>
    </sec>
    <sec id="Sec15">
      <title>Ensemble averaging</title>
      <p id="Par58">To raise accuracy, we used ensemble averaging. We split our dataset into five sets. Consequently, we were able to obtain five (or up to ten) different models trained with five (or up to ten; see Table <xref rid="Tab10" ref-type="table">10</xref>) different sets. Our final prediction result for each residue pair was obtained simply by averaging these predicted probabilities.</p>
    </sec>
    <sec id="Sec16">
      <title>Cropping and sampling</title>
      <p id="Par59">To overcome the GPU memory size limitation and to deepen the network, we crop a part of the protein sequences and sample the sequences in MSAs. More concretely, when the sequence length is greater than 200 residues, we crop 200 residues from all protein sequences. When the number of sequences in MSAs is greater than 30,000, we sample 30,000 sequences from them. That number is adequate because our residual network has 3 × 3 filters and 60 layers and because it covers only 121 × 121 of the covariance matrices. We observed decreased prediction accuracy for sampling numbers less than 10,000. These cropping and sampling are only done during training. Entire sequences and MSAs are used during prediction.</p>
    </sec>
    <sec id="Sec17">
      <title>Evaluation of prediction results</title>
      <p id="Par60">To assess contact prediction accuracies, we compared our results with those obtained using existing prediction methods. According to sequence separations of residue pairs, we defined the contact types as “short” 6 &lt; =|<italic>i</italic> - <italic>j</italic>| &lt; =11, “medium” 12 &lt; =|<italic>i</italic> - <italic>j</italic>| &lt; =23, and “long” 24 &lt; =|<italic>i</italic> - <italic>j</italic>|, and compared the top <italic>L</italic>/<italic>k</italic> (<italic>k</italic> = 10,5,2,1) prediction results as described by Wang et al. [<xref ref-type="bibr" rid="CR19">19</xref>]. The prediction accuracy (precision) was calculated using the following eq.</p>
      <p id="Par61">TP / (TP + FP) (6).</p>
      <p id="Par62">In that equation, TP represents the number of true contacts among the predicted ones: TP + FP is the number of all predicted contacts. We selected PSICOV, CCMpred, DeepCov and ResPRE as representatives of ECA methods and selected MetaPSICOV, DeepMetaPSICOV and RaptorX-Contact as representatives of meta-predictors to be compared. We performed calculations with our own local prediction directed by instructions for using each method. The same MSAs used in our models are also used for these models except for MetaPSICOV and RaptorX-Contact. For MetaPSICOV “–id 99” option was used in its default setting. For the RaptorX-Contact, no local execution file was available. Predictions were calculated on their server. However, for 3 out of 105 CASP11 domains and for 1 out of 55 CASP12 domains, the results were not retrieved because of server error. The MSAs were prepared by their server originally. They differed from ours. Using the CASP11 and CASP12 datasets, we calculated the accuracy for each separate domain, not an entire protein.</p>
      <p id="Par63">For evaluation of secondary structure and for accessible surface area prediction, we used RaptorX-Property and SCRATCH-1D as state-of-the-art methods. We calculated the results obtained using local prediction. To evaluate prediction results of secondary structure, we also measured recall: TP/(TP + FN).</p>
    </sec>
    <sec id="Sec18">
      <title>Tertiary structure prediction</title>
      <p id="Par64">To predict tertiary structures from obtained contacts and secondary structure predictions, we used a script in the CONFOLD package. We mixed up all three (short, medium, and long) ranges of predicted contacts, ordered them by their probability of contact; then we used (up to) the top 2 <italic>L</italic> contacts among them as inputs for the script.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec19">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2019_3190_MOESM1_ESM.png">
            <caption>
              <p><bold>Additional file 1: Figure S1.</bold> Distributions of weight values of gap ratio, sequence identity and sequence identity with a consensus sequence. Each dot represents a sequence in each MSA. These protein domains (1JF3A, 2R6UA and 2RDEA) are randomly selected from on validation dataset.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CASP</term>
        <def>
          <p id="Par4">Critical assessment of protein structure prediction</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p id="Par5">Convolutional neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>DNN</term>
        <def>
          <p id="Par6">Deep neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>ECA</term>
        <def>
          <p id="Par7">Evolutionary coupling analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>MLPs</term>
        <def>
          <p id="Par8">Multilayer perceptrons</p>
        </def>
      </def-item>
      <def-item>
        <term>MSA</term>
        <def>
          <p id="Par9">Multiple sequence alignment</p>
        </def>
      </def-item>
      <def-item>
        <term>PSSM</term>
        <def>
          <p id="Par10">Position specific score matrix</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s12859-019-3190-x.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>Computations were performed partially on the NIG supercomputer at National Institute of Genetics (NIG), ROIS. We deeply appreciate the National Institute of Genetics for supporting our research.</p>
    <sec id="FPar1">
      <title>Ethical approval and consent to participate</title>
      <p id="Par65">Not applicable.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>HF and KT designed the research, analyzed the data, and wrote the manuscript. HF implemented the method. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This research was partially supported by the Initiative on Promotion of Supercomputing for Young or Women Researchers, Supercomputing Division, Information Technology Center, The University of Tokyo, and by the Platform Project for Supporting Drug Discovery and Life Science Research (Basis for Supporting Innovative Drug Discovery and Life Science Research (BINDS)) from AMED under Grant Number JP19am0101110. The funding bodies did not play any roles in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>
      <ext-link ext-link-type="uri" xlink:href="https://github.com/tomiilab/DeepECA">https://github.com/tomiilab/DeepECA</ext-link>
    </p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p id="Par66">Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par67">The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dunn</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Wahl</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Gloor</surname>
            <given-names>GB</given-names>
          </name>
        </person-group>
        <article-title>Mutual information without the influence of phylogeny or entropy dramatically improves residue contact prediction</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <issue>3</issue>
        <fpage>333</fpage>
        <lpage>340</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btm604</pub-id>
        <pub-id pub-id-type="pmid">18057019</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Björkholm</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Daniluk</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kryshtafovych</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fidelis</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Andersson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hvidsten</surname>
            <given-names>TR</given-names>
          </name>
        </person-group>
        <article-title>Using multi-data hidden Markov models trained on local neighborhoods of protein structure to predict residue-residue contacts</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>10</issue>
        <fpage>1264</fpage>
        <lpage>1270</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp149</pub-id>
        <pub-id pub-id-type="pmid">19289446</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Balakrishnan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kamisetty</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Carbonell</surname>
            <given-names>JG</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>SI</given-names>
          </name>
          <name>
            <surname>Langmead</surname>
            <given-names>CJ</given-names>
          </name>
        </person-group>
        <article-title>Learning generative models for protein fold families</article-title>
        <source>Proteins</source>
        <year>2011</year>
        <volume>79</volume>
        <issue>4</issue>
        <fpage>1061</fpage>
        <lpage>1078</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.22934</pub-id>
        <pub-id pub-id-type="pmid">21268112</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
          <name>
            <surname>Buchan</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Cozzetto</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Pontil</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>2</issue>
        <fpage>184</fpage>
        <lpage>190</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btr638</pub-id>
        <pub-id pub-id-type="pmid">22101153</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marks</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Colwell</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Sheridan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hopf</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Pagnani</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zecchina</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Protein 3D structure computed from evolutionary sequence variation</article-title>
        <source>PLoS One</source>
        <year>2011</year>
        <volume>6</volume>
        <issue>12</issue>
        <fpage>e28766</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0028766</pub-id>
        <pub-id pub-id-type="pmid">22163331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Di Lena</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Nagata</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Deep architectures for protein contact map prediction</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>19</issue>
        <fpage>2449</fpage>
        <lpage>2457</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts475</pub-id>
        <pub-id pub-id-type="pmid">22847931</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kamisetty</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ovchinnikov</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Baker</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Assessing the utility of coevolution-based residue-residue contact predictions in a sequence- and structure-rich era</article-title>
        <source>Proc Natl Acad Sci U S A</source>
        <year>2013</year>
        <volume>110</volume>
        <issue>39</issue>
        <fpage>15674</fpage>
        <lpage>15679</lpage>
        <pub-id pub-id-type="doi">10.1073/pnas.1314045110</pub-id>
        <pub-id pub-id-type="pmid">24009338</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eickholt</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A study and benchmark of DNcon: a method for protein residue-residue contact prediction using deep networks</article-title>
        <source>BMC Bioinformatics</source>
        <year>2013</year>
        <volume>14</volume>
        <issue>Suppl 14</issue>
        <fpage>S12</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-14-S14-S12</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Predicting protein contact map using evolutionary and physical constraints by integer programming</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>13</issue>
        <fpage>i266</fpage>
        <lpage>i273</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt211</pub-id>
        <pub-id pub-id-type="pmid">23812992</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ekeberg</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lovkvist</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Lan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Weigt</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Aurell</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Improved contact prediction in proteins: using pseudolikelihoods to infer Potts models</article-title>
        <source>Phys Rev E Stat Nonlinear Soft Matter Phys</source>
        <year>2013</year>
        <volume>87</volume>
        <issue>1</issue>
        <fpage>012707</fpage>
        <pub-id pub-id-type="doi">10.1103/PhysRevE.87.012707</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ekeberg</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hartonen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Aurell</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Fast pseudolikelihood maximization for direct-coupling analysis of protein structure from many homologous amino-acid sequences</article-title>
        <source>J Comput Phys</source>
        <year>2014</year>
        <volume>276</volume>
        <fpage>341</fpage>
        <lpage>356</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jcp.2014.07.024</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kaján</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Hopf</surname>
            <given-names>TA</given-names>
          </name>
          <name>
            <surname>Kalaš</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Marks</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Rost</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>FreeContact: fast and free software for protein contact prediction from residue co-evolution</article-title>
        <source>BMC Bioinformatics</source>
        <year>2014</year>
        <volume>15</volume>
        <fpage>85</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-15-85</pub-id>
        <pub-id pub-id-type="pmid">24669753</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Seemayer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gruber</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>CCMpred – fast and precise prediction of protein residue-residue contacts from correlated mutations</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>21</issue>
        <fpage>3128</fpage>
        <lpage>3130</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu500</pub-id>
        <pub-id pub-id-type="pmid">25064567</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kosciolek</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tetchner</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>MetaPSICOV: combining coevolution methods for accurate prediction of contacts and long range hydrogen bonding in proteins</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <issue>7</issue>
        <fpage>999</fpage>
        <lpage>1006</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu791</pub-id>
        <pub-id pub-id-type="pmid">25431331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Andreani</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>bbcontacts: prediction of b-strand pairing from direct coupling patterns</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <issue>11</issue>
        <fpage>1729</fpage>
        <lpage>1737</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv041</pub-id>
        <pub-id pub-id-type="pmid">25618863</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Protein contact prediction by integrating joint evolutionary coupling analysis and supervised learning</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <issue>21</issue>
        <fpage>3506</fpage>
        <lpage>3513</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv472</pub-id>
        <pub-id pub-id-type="pmid">26275894</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Dahl</surname>
            <given-names>DB</given-names>
          </name>
          <name>
            <surname>Vannucci</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Joo</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>JW</given-names>
          </name>
        </person-group>
        <article-title>KScons: a Bayesian approach for protein residue contact prediction using the knob-socket model of protein tertiary structure</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>24</issue>
        <fpage>3774</fpage>
        <lpage>3781</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw553</pub-id>
        <pub-id pub-id-type="pmid">27559156</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Golkov V, Skwark MJ, Golkov A, Dosovitskiy A, Brox T, Meiler J, Cremers D. Protein contact prediction from amino acid co-evolution using convolutional networks for graph-valued images. NIPS Proceedings. 2016.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Accurate De novo prediction of protein contact map by ultra-deep learning model</article-title>
        <source>PLoS Comput Biol</source>
        <year>2017</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>e1005324</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1005324</pub-id>
        <pub-id pub-id-type="pmid">28056090</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Sparse inverse covariance estimation with the graphical lasso</article-title>
        <source>Biostatistics</source>
        <year>2008</year>
        <volume>9</volume>
        <issue>3</issue>
        <fpage>432</fpage>
        <lpage>441</lpage>
        <pub-id pub-id-type="doi">10.1093/biostatistics/kxm045</pub-id>
        <pub-id pub-id-type="pmid">18079126</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>DT</given-names>
          </name>
          <name>
            <surname>Kandathil</surname>
            <given-names>SM</given-names>
          </name>
        </person-group>
        <article-title>High precision in protein contact prediction using fully convolutional neural networks and minimal sequence features</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>34</volume>
        <issue>19</issue>
        <fpage>3308</fpage>
        <lpage>3315</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty341</pub-id>
        <pub-id pub-id-type="pmid">29718112</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Yang</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Jun</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Chengxin</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Dong-Jun</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Yang</given-names>
          </name>
        </person-group>
        <article-title>ResPRE: high-accuracy protein contact prediction by coupling precision matrix with deep residual neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2019</year>
        <volume>35</volume>
        <issue>22</issue>
        <fpage>4647</fpage>
        <lpage>4655</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btz291</pub-id>
        <pub-id pub-id-type="pmid">31070716</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kandathil</surname>
            <given-names>Shaun M.</given-names>
          </name>
          <name>
            <surname>Greener</surname>
            <given-names>Joe G.</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>David T.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of interresidue contacts with DeepMetaPSICOV in CASP13</article-title>
        <source>Proteins: Structure, Function, and Bioinformatics</source>
        <year>2019</year>
        <volume>87</volume>
        <issue>12</issue>
        <fpage>1092</fpage>
        <lpage>1099</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25779</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fox</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sievers</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Higgins</surname>
            <given-names>DG</given-names>
          </name>
        </person-group>
        <article-title>Using de novo protein structure predictions to measure the quality of very large multiple sequence alignments</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>6</issue>
        <fpage>814</fpage>
        <lpage>820</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btv592</pub-id>
        <pub-id pub-id-type="pmid">26568625</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Skolnick</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Scoring function for automated assessment of protein structure template quality</article-title>
        <source>Proteins</source>
        <year>2004</year>
        <volume>57</volume>
        <issue>4</issue>
        <fpage>s702</fpage>
        <lpage>s710</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.20264</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>RaptorX-property: a web server for protein structure property prediction</article-title>
        <source>Nucleic Acids Res</source>
        <year>2016</year>
        <volume>44</volume>
        <issue>W1</issue>
        <fpage>W430</fpage>
        <lpage>W435</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkw306</pub-id>
        <pub-id pub-id-type="pmid">27112573</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Magnan</surname>
            <given-names>CN</given-names>
          </name>
          <name>
            <surname>Baldi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>SSpro/ACCpro: almost perfect prediction of protein secondary structure and relative solvent accessibility using profiles, machine learning and structural similarity</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>18</issue>
        <fpage>2592</fpage>
        <lpage>2597</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu352</pub-id>
        <pub-id pub-id-type="pmid">24860169</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Adhikari</surname>
            <given-names>Badri</given-names>
          </name>
          <name>
            <surname>Bhattacharya</surname>
            <given-names>Debswapna</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>Renzhi</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>Jianlin</given-names>
          </name>
        </person-group>
        <article-title>CONFOLD: Residue-residue contact-guidedab initioprotein folding</article-title>
        <source>Proteins: Structure, Function, and Bioinformatics</source>
        <year>2015</year>
        <volume>83</volume>
        <issue>8</issue>
        <fpage>1436</fpage>
        <lpage>1449</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.24829</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hanson</surname>
            <given-names>Jack</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>Kuldip</given-names>
          </name>
          <name>
            <surname>Litfin</surname>
            <given-names>Thomas</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Yuedong</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Yaoqi</given-names>
          </name>
        </person-group>
        <article-title>Improving prediction of protein secondary structure, backbone angles, solvent accessibility and contact numbers by using predicted contact maps and an ensemble of recurrent and residual convolutional neural networks</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>35</volume>
        <issue>14</issue>
        <fpage>2403</fpage>
        <lpage>2410</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bty1006</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Dunbrack</surname>
            <given-names>R. L.</given-names>
          </name>
        </person-group>
        <article-title>PISCES: a protein sequence culling server</article-title>
        <source>Bioinformatics</source>
        <year>2003</year>
        <volume>19</volume>
        <issue>12</issue>
        <fpage>1589</fpage>
        <lpage>1591</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btg224</pub-id>
        <pub-id pub-id-type="pmid">12912846</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Camacho</surname>
            <given-names>Christiam</given-names>
          </name>
          <name>
            <surname>Coulouris</surname>
            <given-names>George</given-names>
          </name>
          <name>
            <surname>Avagyan</surname>
            <given-names>Vahram</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Ning</given-names>
          </name>
          <name>
            <surname>Papadopoulos</surname>
            <given-names>Jason</given-names>
          </name>
          <name>
            <surname>Bealer</surname>
            <given-names>Kevin</given-names>
          </name>
          <name>
            <surname>Madden</surname>
            <given-names>Thomas L</given-names>
          </name>
        </person-group>
        <article-title>BLAST+: architecture and applications</article-title>
        <source>BMC Bioinformatics</source>
        <year>2009</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>421</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-10-421</pub-id>
        <pub-id pub-id-type="pmid">20003500</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kabsch</surname>
            <given-names>Wolfgang</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>Christian</given-names>
          </name>
        </person-group>
        <article-title>Dictionary of protein secondary structure: Pattern recognition of hydrogen-bonded and geometrical features</article-title>
        <source>Biopolymers</source>
        <year>1983</year>
        <volume>22</volume>
        <issue>12</issue>
        <fpage>2577</fpage>
        <lpage>2637</lpage>
        <pub-id pub-id-type="doi">10.1002/bip.360221211</pub-id>
        <pub-id pub-id-type="pmid">6667333</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Monastyrskyy</surname>
            <given-names>Bohdan</given-names>
          </name>
          <name>
            <surname>D'Andrea</surname>
            <given-names>Daniel</given-names>
          </name>
          <name>
            <surname>Fidelis</surname>
            <given-names>Krzysztof</given-names>
          </name>
          <name>
            <surname>Tramontano</surname>
            <given-names>Anna</given-names>
          </name>
          <name>
            <surname>Kryshtafovych</surname>
            <given-names>Andriy</given-names>
          </name>
        </person-group>
        <article-title>New encouraging developments in contact prediction: Assessment of the CASP11 results</article-title>
        <source>Proteins: Structure, Function, and Bioinformatics</source>
        <year>2015</year>
        <volume>84</volume>
        <fpage>131</fpage>
        <lpage>144</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.24943</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schaarschmidt</surname>
            <given-names>Joerg</given-names>
          </name>
          <name>
            <surname>Monastyrskyy</surname>
            <given-names>Bohdan</given-names>
          </name>
          <name>
            <surname>Kryshtafovych</surname>
            <given-names>Andriy</given-names>
          </name>
          <name>
            <surname>Bonvin</surname>
            <given-names>Alexandre M.J.J.</given-names>
          </name>
        </person-group>
        <article-title>Assessment of contact predictions in CASP12: Co-evolution and deep learning coming of age</article-title>
        <source>Proteins: Structure, Function, and Bioinformatics</source>
        <year>2017</year>
        <volume>86</volume>
        <fpage>51</fpage>
        <lpage>66</lpage>
        <pub-id pub-id-type="doi">10.1002/prot.25407</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Remmert</surname>
            <given-names>Michael</given-names>
          </name>
          <name>
            <surname>Biegert</surname>
            <given-names>Andreas</given-names>
          </name>
          <name>
            <surname>Hauser</surname>
            <given-names>Andreas</given-names>
          </name>
          <name>
            <surname>Söding</surname>
            <given-names>Johannes</given-names>
          </name>
        </person-group>
        <article-title>HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment</article-title>
        <source>Nature Methods</source>
        <year>2011</year>
        <volume>9</volume>
        <issue>2</issue>
        <fpage>173</fpage>
        <lpage>175</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.1818</pub-id>
        <pub-id pub-id-type="pmid">22198341</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Adhikari B. DEEPCON: Protein Contact Prediction using Dilated Convolutional Neural Networks with Dropout. bioRxiv. 2019:590455.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), IEEE. 2016;77:770–8.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caruana</surname>
            <given-names>Rich</given-names>
          </name>
        </person-group>
        <source>Machine Learning</source>
        <year>1997</year>
        <volume>28</volume>
        <issue>1</issue>
        <fpage>41</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1007379606734</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Yu Z, Qiang Y. A Survey on Multi-Task Learning. arXiv: 1707.08114 2018.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Heffernan R, Paliwal K, Lyons J, Dehzangi A, Sharma A, Wang J, Sattar A, Yang Y, Zhou Y. Improving prediction of secondary structure, local backbone angles, and solvent accessible surface area of proteins by iterative deep learning. Nat Sci Rep. 2015;5:11476.</mixed-citation>
    </ref>
  </ref-list>
</back>
