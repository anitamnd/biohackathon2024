<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Plant Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Plant Methods</journal-id>
    <journal-title-group>
      <journal-title>Plant Methods</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1746-4811</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5341458</article-id>
    <article-id pub-id-type="publisher-id">161</article-id>
    <article-id pub-id-type="doi">10.1186/s13007-017-0161-y</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>AutoRoot: open-source software employing a novel image analysis approach to support fully-automated plant phenotyping</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Pound</surname>
          <given-names>Michael P.</given-names>
        </name>
        <address>
          <email>michael.pound@nottingham.ac.uk</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fozard</surname>
          <given-names>Susan</given-names>
        </name>
        <address>
          <email>s.fozard4@lancaster.ac.uk</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Torres Torres</surname>
          <given-names>Mercedes</given-names>
        </name>
        <address>
          <email>mercedes.torrestorres@nottingham.ac.uk</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Forde</surname>
          <given-names>Brian G.</given-names>
        </name>
        <address>
          <email>b.g.forde@lancaster.ac.uk</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>French</surname>
          <given-names>Andrew P.</given-names>
        </name>
        <address>
          <email>andrew.p.french@nottingham.ac.uk</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 8868</institution-id><institution-id institution-id-type="GRID">grid.4563.4</institution-id><institution>School of Computer Science, </institution><institution>University of Nottingham, </institution></institution-wrap>Nottingham, NG8 1BB UK </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI"> 0000 0000 8190 6402</institution-id><institution-id institution-id-type="GRID">grid.9835.7</institution-id><institution>Lancaster Environment Centre, </institution><institution>Lancaster University, </institution></institution-wrap>Lancaster, LA1 4YQ UK </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1936 8868</institution-id><institution-id institution-id-type="GRID">grid.4563.4</institution-id><institution>School of Biosciences, </institution><institution>University of Nottingham, </institution></institution-wrap>Sutton Bonington, LE12 5RD UK </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>8</day>
      <month>3</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>8</day>
      <month>3</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2017</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>12</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>8</month>
        <year>2016</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>2</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Computer-based phenotyping of plants has risen in importance in recent years. Whilst much software has been written to aid phenotyping using image analysis, to date the vast majority has been only semi-automatic. However, such interaction is not desirable in high throughput approaches. Here, we present a system designed to analyse plant images in a completely automated manner, allowing genuine high throughput measurement of root traits. To do this we introduce a new set of <italic>proxy</italic> traits.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>We test the system on a new, automated image capture system, the Microphenotron, which is able to image many 1000s of roots/h. A simple experiment is presented, treating the plants with differing chemical conditions to produce different phenotypes. The automated imaging setup and the new software tool was used to measure proxy traits in each well. A correlation matrix was calculated across automated and manual measures, as a validation. Some particular proxy measures are very highly correlated with the manual measures (e.g. proxy length to manual length, r<sup>2</sup> &gt; 0.9). This suggests that while the automated measures are not directly equivalent to classic manual measures, they can be used to indicate phenotypic differences (hence the term, <italic>proxy</italic>). In addition, the raw discriminative power of the new proxy traits was examined. Principal component analysis was calculated across all proxy measures over two phenotypically-different groups of plants. Many of the proxy traits can be used to separate the data in the two conditions.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p>The new proxy traits proposed tend to correlate well with equivalent manual measures, where these exist. Additionally, the new measures display strong discriminative power. It is suggested that for particular phenotypic differences, different traits will be relevant, and not all will have meaningful manual equivalent measures. However, approaches such as PCA can be used to interrogate the resulting data to identify differences between datasets. Select images can then be carefully manually inspected if the nature of the precise differences is required. We suggest such flexible measurement approaches are necessary for fully automated, high throughput systems such as the Microphenotron.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (doi:10.1186/s13007-017-0161-y) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Image analysis</kwd>
      <kwd>Phenotyping</kwd>
      <kwd>Traits</kwd>
      <kwd>Software</kwd>
      <kwd>Automated analysis</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id>
            <institution>Biotechnology and Biological Sciences Research Council</institution>
          </institution-wrap>
        </funding-source>
        <award-id>BB/M004260/1</award-id>
        <principal-award-recipient>
          <name>
            <surname>Forde</surname>
            <given-names>Brian G.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2017</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Phenotyping is the process of measuring features or traits of a plant’s appearance. This appearance is affected by the plant’s growth characteristics (as determined by its genome) and the effect of the environment (such as stress factors or nutrient availability); quantifying the phenome is therefore our gateway into understanding these hidden factors. In recent years, the field of software-assisted phenotyping for plants has advanced tremendously [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. The need to measure more traits from plant images, using larger and more varied image datasets has driven the need to develop more resilient computer vision algorithms to assist with this process [<xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR6">6</xref>]. Tasks such as measuring architectural traits from images are extremely labour intensive, yet computationally challenging to completely automate. Approaches to date largely <italic>semi</italic>-automate the process (e.g. [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>]). This leverages the expert-biologist’s knowledge of the domain to guide a computer algorithm, which can take care of the low-level processing, making the biologist’s job easier—but not removing user involvement entirely.</p>
    <p>The problems of shoot and root phenotyping present distinct challenges, as such many tools are developed with a focus on roots specifically. For truly automated phenotyping, bottom-up approaches are the most common. GiaRoots [<xref ref-type="bibr" rid="CR9">9</xref>], EzRhizo [<xref ref-type="bibr" rid="CR10">10</xref>], WinRhizo and RootReader2D are notable examples. These often apply a low level binary classification operation such as local or global thresholding, to determine which pixels correspond to root material, and which to background. The benefit of assigning labels to pixels in this way is that broad quantitative measures of root systems can be calculated quickly. Measurements such as inferred rootmass, width, tortuosity etc., are easily computed on binary images. However, it is often challenging to perform this thresholding in the presence of noisy images, and large amounts of processing can be required to clean up the signal, for example by removing anomalously-detected foreground pixels. In those cases where the removal process is not perfect, the resultant trait values become flawed. It is commonly held that this low-level error can be overcome by increasing the size of the dataset, something that can be done trivially in automated systems. This approach undoubtedly has merit, but the extent to which this is true in practice will often be a function of the input images, and of the traits being measured; care must be taken.</p>
    <p>More recently, methods that adopt a machine learning approach to quantifying the images (e.g. [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR13">13</xref>]), have grown in popularity. Learning algorithms are able to effectively adapt parameters based on a training image set. However, they require an initial training phase on a new image set before they can be used. To generate the training data, an expert must label the data by hand to provide suitable ground truth data for learning.</p>
    <p>Processing power is increasingly in demand for these approaches, as the image analysis methods become more and more complex. This is especially true with recent deep learning approaches [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>], which can require substantial GPU-based hardware systems to train the complex networks in a feasible time. Other, more traditional image analysis software can still make heavy demands on processing power. Online frameworks are available to help take some of the processing power requirements of these algorithms away from the user, and simplify the processing pipeline (e.g. the iPlant Collaborative Project [<xref ref-type="bibr" rid="CR16">16</xref>]). Still, despite helping with raw processing power and providing a consistent user interface—both important requirements for successful real-world use of software—the underlying algorithms still require research and development to become fully automatic.</p>
    <p>To adopt a fully automated phenotyping approach, any software must fulfil particular criteria. Once running it should not make any demands on the user; all images in the set must be processed in one go, as a batch. Previous software has taken this approach (e.g. [<xref ref-type="bibr" rid="CR17">17</xref>]), requiring the user to provide some details to the software initially, but after that period, batch processing proceeds through an entire set of images. However, it is still beneficial to perform manual visual checking of the final results, to confirm whether the images have been successfully processed. At some point, though, this approach becomes infeasible. With the introduction of more capable robotics enabling very high throughput image capture, it becomes challenging to verify that all the results are satisfactory. We need to ask the software to either place a confidence in the measured results, or provide results which are inherently probabilistic. To say a root is 32.6 mm long requires much certainty on behalf of the software and developers, and the degree of this certainty is often not addressed in automated software. To say root A is longer than root B may be just as valuable, yet requires a looser set of processing requirements.</p>
    <p>The software itself should run sufficiently fast so as to keep up with throughput of the image capture, or at least be able to batch process results offline and in time for the arrival of the next batch. With typical phenotyping studies requiring 1000s of images (e.g. [<xref ref-type="bibr" rid="CR18">18</xref>]) the issue of processing speed is becoming increasingly important. This is the motivation behind the approach presented here. We propose a system designed explicitly to work with images generated in a high throughput manner using a robotic capture system. The software does not require pre-training per image set, as image capture settings are able to be kept consistent as part of the imaging setup. Processing requirements are sufficiently small that an image can be processed in a few seconds on a standard PC (i.e. faster than the rate of image capture). User interaction is not required during processing, and results are automatically generated.</p>
    <p>The nature of the results is such that the necessity to manually validate the results is minimized; we propose measures in the next section which implicitly handle the inherent uncertainty in most image analysis approaches. To achieve automated phenotyping with measurable reliability, a new set of <italic>proxy</italic> measurement traits are proposed, described here. Traditional manual phenotyping involves measuring visible features of a plant using tools such as a ruler or protractor. Despite appearances, even these measurements are not <italic>certain</italic>; and at some degree of accuracy are always incorrect. Implicit in such measurements is an error: a user can only read a measurement to a certain level of accuracy, for example, or may miscount the number of leaves or lateral roots etc. Despite the error, we often consider such measures as a ‘ground truth’ or gold standard; that is, they represent a direct measure of the trait which we often consider to be error-free. Statistical methods applied to measurement data will reveal variations in measurement accuracy, but are considered post-measurement, rather than during the measurement process itself. The same is true of automated image analysis tools—thresholding applies a hard <italic>root</italic> or <italic>no root</italic> label to each pixel, which is sensitive to noise and lighting [<xref ref-type="bibr" rid="CR19">19</xref>], and there is often little indication of the relative error on a particular image.</p>
    <p>Our approach, built here into the new AutoRoot software, differs from traditional root analysis tools by first making no firm judgement as to the location of root pixels—i.e. a thresholding approach is not used. A shortest path-based approach instead measures the confidence that a given pixel is part of the root system, and to which plant it belongs. It is these confidences that drive our phenotypic measurements. In essence, each trait is calculated based on where the root material is likely to be. We will demonstrate that the new traits we propose to measure correlate well with typical manual measures, and can be used as further input in analysis approaches (such as clustering or more advanced machine learning based techniques) without any further interpretation necessary. We also show the measures are able to separate two experimental growth conditions in a new high throughput imaging setup, in a completely automated manner. We believe that these proposed probabilistic <italic>proxy</italic> traits, have application in automated phenotyping where measurement of direct traditional traits is challenging, or not possible.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <p>Images of roots and shoots of Arabidopsis seedlings were acquired using an automated image capture system called the Microphenotron [<xref ref-type="bibr" rid="CR20">20</xref>]. Briefly, the seedlings were grown in agar-filled ‘phytostrips’, which are transparent plastic growth devices that are comprised of a strip of eight flat-sided growth vessels. Growth conditions and nutrient media were as described by Forde et al. [<xref ref-type="bibr" rid="CR21">21</xref>]. The internal distance from the front to the back of each growth vessel is 2 mm, obliging the root system to develop in an essentially 2-D conformation to facilitate imaging and image analysis. Images of the phytostrips are captured robotically, such that each image contains eight wells of seedlings. The precision of the robotic manipulation, combined with the physical boundaries of the growth vessel, places definite limits on where the plant material can be found in the image. In our system we use this consistency to limit our search for root material to these wells, however the methods we propose here would work equally well on larger images. Hereafter, where we discuss image analysis approaches, we are referring to the contents of a single well, and this process is repeated for the number of wells in the input image.</p>
    <p>Images are prepared by first converting to greyscale, before contrast is increased by performing a contrast stretching operation. This maps a range of intensity values representing the background and foreground pixels, into the full 8-bit grayscale range of 0–255 intensity levels. In practice, this has the effect of increasing the grey-level distance between the foreground and background, improving the confidence measures that are calculated next. The Microphenotron has very consistent lighting environment, and as such these values are easily determined once before being applied to all images. It is not necessary to perform an adaptive method, such as histogram normalisation. We tested a histogram normalisation approach which redistributes grey levels in the image such that a small percentage of pixels are saturated at either the minimum zero value, or the maximum greyscale value. We found, however, that for dense root systems, histogram normalisation would produce insufficient contrast, spreading the intensity levels of root pixels over the full range, rather than significantly brightening them.</p>
    <p>Rather than performing a binary threshold, which assigns a 100% confidence value to each pixel’s classification, we aim to calculate a ‘likelihood’ that any given pixel belongs to the root system. We begin by locating numerous candidate root locations within a horizontal strip at the top of each image well. The height of this strip is 5% of the well height. Seeds are planted above this position in all images we have encountered. This is done by finding local intensity maxima (pixels brighter than their neighbours) by scanning across each pixel row. These positions act as points at which we are confident there is root material, and thus points well connected to these will have a high likelihood of also being root.</p>
    <p>These candidate points are used to initiate a Dijkstra’s shortest path search [<xref ref-type="bibr" rid="CR22">22</xref>] through all pixels in the well. Dijkstra’s algorithm will find the shortest path between nodes in a graph. We consider the pixels inside the well regions as a fully-connected set of nodes. When run to completion, it is also possible to calculate the length of a path from any source nodes, to any point on the image. Our Dijkstra approach is similar to that found in our existing RootNav tool [<xref ref-type="bibr" rid="CR7">7</xref>]; a graph is produced with a node at each pixel, and edges between neighbouring pixels. Unlike RootNav, where edges are weighted based on the output of an expectation–maximisation classification step, the weights in this system are based solely on the grayscale intensity of the pixels. In short, bright pixels will generally have low, favourable weights, and darker pixels will have higher, less favourable weights. Weights are calculated as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w(p,q) = 1 - \alpha \cdot I(p) \cdot I(q),$$\end{document}</tex-math><mml:math id="M2" display="block"><mml:mrow><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi mathvariant="italic">α</mml:mi><mml:mo>·</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>·</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13007_2017_161_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>p</italic> and <italic>q</italic> are the two pixels between which the weights are calculated. <italic>I</italic>(<italic>x</italic>), is the normalised intensity of the image at pixel <italic>x</italic>, where 0 ≤ <italic>I</italic>(<italic>x</italic>) ≤ 1. The additional weight <italic>α</italic> is 1 if the pixels are in the same row or column, and <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sqrt 2$$\end{document}</tex-math><mml:math id="M4"><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:math><inline-graphic xlink:href="13007_2017_161_Article_IEq1.gif"/></alternatives></inline-formula> if they are on a diagonal, to account for the increased distances between the pixel centres.</p>
    <p>This results in Dijkstra’s algorithm finding shorter paths through bright regions of the image, and longer ones through dark regions; but note we require no arbitrary intensity boundary between foreground (root) and background (well). Simply, the algorithm penalises travel over non-root material.</p>
    <p>Since our approach obtains a number of candidate source points representing the top of the root system, we adapted the Dijkstra implementation to consider termination at any one point in a set of points, rather than at a single point. The output of this step is a value, for each pixel, representing the distance required to travel from the source points at the top of the well, to each pixel’s position, taking account the weights as described previously. It is these distances that we treat as likelihoods in our probabilistic traits, under the assumption that shorter paths travel over brighter pixels (more likely root material). This approach has the added benefit of ignoring noisy bright pixels that are unconnected to the root structure—to reach these pixels the shortest path must travel over background, and thus the lengths of paths to noise are often significantly longer. Where noise appears close to the root system, this may not always hold; however we have observed this infrequently during our experiments.</p>
    <p>In order to quantify phenotypic traits within each image or well, we define a likelihood function <italic>L</italic>, that for any pixel produces a likelihood that it belongs to the root system:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L(x,y) = 1 - dijkstra(x,y) /max(dijkstra(u,v),\quad \forall u,v)$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">/</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:mo>∀</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><graphic xlink:href="13007_2017_161_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where dijkstra(<italic>x</italic>, <italic>y</italic>) is the shortest distance to pixel <italic>x</italic>, <italic>y</italic>, from any start position (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The value <italic>L</italic> is normalised using the maximum theoretical value of distance for an image of that size. In practice this can be calculated as the maximum distance returned by Dijkstra, averaged over a number of wells. Optionally, the function <italic>L</italic> can also be raised to a power, i.e. <italic>L</italic>
<sup><italic>n</italic></sup> to decrease the distance at which the likelihood drops off from bright pixels. We have found this has minimal impact on results, but can be useful if plants are more established, and thus the maximum distance from root material is never large.<fig id="Fig1"><label>Fig. 1</label><caption><p><bold>a</bold> Top-down image from the Microphenotron. <bold>b</bold> Side-facing image of the same plants in <bold>a</bold>. <bold>c</bold>
<italic>L</italic>(<italic>x</italic>, <italic>y</italic>) visualised as a heat map for the set of eight wells in <bold>b</bold>. <italic>Brighter areas</italic> indicate regions the software considers more likely to be root material</p></caption><graphic xlink:href="13007_2017_161_Fig1_HTML" id="MO4"/></fig>
</p>
    <p>The function <italic>L</italic> indicates a likelihood of there being a root at a given location. Note, we are not thresholding the image at this point, rather we wish to assign a confidence level to represent how certain we are that this pixel belongs to a root. Any subsequent trait derived from this data can maintain the idea of this confidence. Therefore, we avoid the problematic situation of having to determine a priori the exact existence or non-existence or root at each location in the image, and can instead make an educated guess.</p>
    <p>Once image pixels are assigned a confidence level indicating root material presence, and a distance to the determined anchor point, a number of interesting, but non-traditional traits can be measured from the image. They are measured in the image space (i.e. in pixels) but as we will see this is not important and conversion to real-world units is not necessary. The measures currently implemented in AutoRoot are presented in Table <xref rid="Tab1" ref-type="table">1</xref>. <table-wrap id="Tab1"><label>Table 1</label><caption><p>Proxy traits proposed here</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Proposed trait</th><th align="left">Description</th><th align="left">Nearest traditional equivalent</th></tr></thead><tbody><tr><td align="left">Centroid</td><td align="left">The weighted centre of mass of the root system</td><td align="left">Centre of mass of all root pixels</td></tr><tr><td align="left">Mass</td><td align="left">A normalised sum of all likelihoods generated by <italic>L</italic> in a given image or well</td><td align="left">Sum of all root pixels</td></tr><tr><td align="left">Width/depth (M)</td><td align="left">Bounding box width and depth of the root system. Calculated as maximum point of mass on the <italic>extremities</italic> of the root system. We define this as: <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\arg \max_{x} (L(x,y) \cdot x )$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mo>arg</mml:mo><mml:msub><mml:mo movablelimits="true">max</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>·</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="13007_2017_161_Article_IEq3.gif"/></alternatives></inline-formula> and similarly for y and the other sides of the bounding box</td><td align="left">Maximum width and depth reached by all root pixels</td></tr><tr><td align="left">Width/depth (p95)</td><td align="left">Alternative bounding box width and depth of the root system. Calculated to enclose 95% of the calculated root likelihood</td><td align="left">Maximum width and depth reached by all root pixels, discounting a small number of root outliers</td></tr><tr><td align="left">Depth (p99)</td><td align="left">Alternative depth measurement, calculated as 99% of the root likelihood</td><td align="left">Maximum depth reached by all root pixels, discounting less outliers than p95</td></tr><tr><td align="left">Quadrant mass</td><td align="left">The mass trait split horizontally into four regions for each well, giving a measure of root material within each quadrant (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>a)</td><td align="left">Root pixel count in four regions (at varying depths)</td></tr><tr><td align="left">Orientation</td><td align="left">Ten brackets of orientation representing the direction of the root system at each pixel. These range from 0°, horizontal, to 90°, vertical. (see Additional file <xref rid="MOESM1" ref-type="media">1</xref>: Figure S1)</td><td align="left">Histogram of all root angles</td></tr><tr><td align="left">Quadrant orientation</td><td align="left">The orientation trait split horizontally into four regions for each well (as previously). Orientations are now grouped into four brackets per quadrant, rather than 10, giving 16 values for each well in total</td><td align="left">Histogram of root angles at different depths</td></tr><tr><td align="left">Leaf hue</td><td align="left">The average hue for each leaf pixel in the top image, for each well</td><td align="left">Average pixel hue i.e. leaf colour</td></tr><tr><td align="left">Leaf area</td><td align="left">Total pixel count for all non-white pixels in the top image, per well. Non-white is defined as having a saturation value above a low threshold of 20%</td><td align="left">Pixel count of leaf area</td></tr></tbody></table><table-wrap-foot><p>Where appropriate, a nearest traditional measure is listed. <italic>L</italic> refers to the root likelihood function (Eq. <xref rid="Equ2" ref-type="">2</xref>). Note the final two traits are measured from a top-down camera view</p></table-wrap-foot></table-wrap>
</p>
    <p>These traits are calculated as a weighted function (e.g. a sum or average), where the contribution of each pixel to the final measurement is given by its likelihood. So rather than, for example, measuring the mass of the root system by summing over all thresholded foreground pixels, we sum the likelihoods of all pixels in a well. This means that all pixels are considered within each measured trait, but that those with high likelihood of being root material will make a significantly higher contribution. The orientation <inline-formula id="IEq2"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta$$\end{document}</tex-math><mml:math id="M10"><mml:mi mathvariant="italic">θ</mml:mi></mml:math><inline-graphic xlink:href="13007_2017_161_Article_IEq2.gif"/></alternatives></inline-formula> at each pixel is calculated using a Sobel convolution:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\theta = \tan^{ - 1} (Gy/Gx),$$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mrow><mml:mi mathvariant="italic">θ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mo>tan</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mi>y</mml:mi><mml:mo stretchy="false">/</mml:mo><mml:mi>G</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic xlink:href="13007_2017_161_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where Gx and Gy are the gradients in the <italic>x</italic> and <italic>y</italic> directions, calculated using convolution with a Sobel operator.</p>
  </sec>
  <sec id="Sec3">
    <title>Implementation and performance</title>
    <p>AutoRoot runs in real time, typically taking a few seconds to process each well, and approximately 15 s to process each image on a standard, modern machine. The size of the images we used in this experiment were 5 MP, with each well approximately 290 px wide and 710 px tall. For images of this resolution, the software comfortably runs using &lt;2 GB of RAM. The AutoRoot software has been developed in C#, and as such runs on any Windows installation. The full code is available under an open-source license.</p>
  </sec>
  <sec id="Sec4" sec-type="results">
    <title>Results</title>
    <p>In the following experiments we examine how useful the new metrics are as proxy measures for classically measured traits via a simple phenotyping experiment. We also consider the usefulness of using the new proxy measures directly as ways of discriminating different phenotypes. We have purposefully chosen clearly visible and distinct phenotypes in order to demonstrate both the process and descriptive power of the proxy traits. AutoRoot has been used successfully to recover more subtle phenotypic differences in an Arabidopsis root and shoot chemical screening experiment [<xref ref-type="bibr" rid="CR20">20</xref>].</p>
    <p><italic>Arabidopsis thaliana</italic> seedlings were germinated and grown in phytostrips filled with nutrient agar (a 20-fold dilution of Gamborg’s B5 medium pH 5.7, 0.5 mM KNO<sub>3</sub>, 0.5% sucrose and solidified with 0.7% Phytagel). Treatments were applied using the approach previously described [<xref ref-type="bibr" rid="CR23">23</xref>], where the phytostrips are placed in the wells of a microtitre plate containing 150 ul nutrient solution, with or without 40% (w/v) polyethylene glycol 8000 (PEG). Inclusion of the PEG in the microtitre plates is designed to impose a water-deficit stress on the growing seedlings, a treatment that is known to inhibit Arabidopsis root growth [<xref ref-type="bibr" rid="CR23">23</xref>].</p>
    <p>For the imaging, plants were automatically imaged in clear wells of growth media, using the Microphenotron system [<xref ref-type="bibr" rid="CR20">20</xref>] (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>).</p>
    <sec id="Sec5">
      <title>Automated measures as proxies for manual metrics</title>
      <p>The measures that we propose can act as proxy measures for the more classical manual measures of the phenotype. To demonstrate this, plants in 23 images, each featuring eight wells, and grown in multiple conditions as described above (so expressing a variety of phenotypes) were both measured manually, and analysed with the new software. Manual measurements were carried out using the Fiji software (<ext-link ext-link-type="uri" xlink:href="http://fiji.sc/">http://fiji.sc/</ext-link>) [<xref ref-type="bibr" rid="CR24">24</xref>]. As wells contained multiple plants, a simple and fast approximation method was used to manually quantify root length, lateral count and estimate mass per well (inspired by the fast ‘shovelomics’ approach developed for in-field phenotyping [<xref ref-type="bibr" rid="CR25">25</xref>]). To measure length, a representative root length was estimated for the well, using the straight line measuring rule in Fiji (see red bars in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a for examples). Root density (sometimes termed mass, or abundance of root material) was estimated by categorising the well depending on the amount of root material present: 0 representing no growth at all, and 10 representing an abundance of root growth. The number of laterals in the well was categorised into bins (0–5, 6–10, 11–15, 16+). This is a coarse measure, but identifying individual laterals, and judging whether a particular section of root is lateral or not can be subjective, so no finer granularity was sought. Table <xref rid="Tab2" ref-type="table">2</xref> shows example measures (automatic and manual) from the wells in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a.<fig id="Fig2"><label>Fig. 2</label><caption><p><bold>a</bold> Manual measures derived from three varying wells—<italic>red bar</italic> indicates manual root length estimations. <italic>Green ticks</italic> indicate approximate boundaries for quartile divisions—Q0 at the <italic>top</italic>, Q3 at the <italic>bottom</italic>. Comparison of manual and automatically derived measures for these wells can be seen in Table <xref rid="Tab1" ref-type="table">1</xref> and panel. Note the subjective judgment required for root length in the left hand well. <bold>b</bold> Manual length plotted against <italic>proxy</italic>-depth for a mixed population of plants in 184 wells. To note in the graph, some of the plants had reached the <italic>bottom</italic> of the wells, represented at the cluster at (1400, 700)</p></caption><graphic xlink:href="13007_2017_161_Fig2_HTML" id="MO5"/></fig>
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Well label corresponds to well from Fig. <xref rid="Fig2" ref-type="fig">2</xref>a</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Well</th><th align="left">Manual length</th><th align="left">Manual density</th><th align="left">Manual laterals</th><th align="left">AutoRoot depth (M) (<italic>proxy</italic>-<italic>length</italic>)</th><th align="left">AutoRoot mass (<italic>proxy-mass</italic>)</th><th align="left">AutoRoot orientation 1 (<italic>proxy-laterals</italic>)</th></tr></thead><tbody><tr><td align="left">Left</td><td char="." align="char">644</td><td char="." align="char">1</td><td char="." align="char">0</td><td char="." align="char">441</td><td char="." align="char">54,313</td><td char="." align="char">76</td></tr><tr><td align="left">Middle</td><td char="." align="char">852</td><td char="." align="char">5</td><td char="." align="char">10</td><td char="." align="char">442</td><td char="." align="char">75,919</td><td char="." align="char">297</td></tr><tr><td align="left">Right</td><td char="." align="char">1400</td><td char="." align="char">9</td><td char="." align="char">15</td><td char="." align="char">709</td><td char="." align="char">153,493</td><td char="." align="char">1632</td></tr></tbody></table><table-wrap-foot><p>Manual measures are presented first, followed by automatically-derived proxy equivalents. Note proxy traits have arbitrary units</p></table-wrap-foot></table-wrap>
</p>
      <p>In order to examine the relationship between proxy parameters and manual measures, a correlation matrix was calculated between all proxy measures and manual measures for this dataset (see Table <xref rid="Tab3" ref-type="table">3</xref>). <table-wrap id="Tab3"><label>Table 3</label><caption><p>Correlation matrix between a subset of the metrics, showing the highest-correlated proxy measures (in italics) with each manual measure</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Length</th><th align="left">Density</th><th align="left">Laterals</th></tr></thead><tbody><tr><td align="left">Centroid Y</td><td char="." align="char"><italic>0</italic>.<italic>95</italic></td><td char="." align="char">0.71</td><td char="." align="char">0.79</td></tr><tr><td align="left">Q3 mass</td><td char="." align="char"><italic>0</italic>.<italic>94</italic></td><td char="." align="char">0.71</td><td char="." align="char">0.76</td></tr><tr><td align="left">Depth (M)</td><td char="." align="char"><italic>0</italic>.<italic>94</italic></td><td char="." align="char">0.70</td><td char="." align="char">0.79</td></tr><tr><td align="left">Depth (p95)</td><td char="." align="char"><italic>0</italic>.<italic>94</italic></td><td char="." align="char">0.70</td><td char="." align="char">0.79</td></tr><tr><td align="left">Depth (p99)</td><td char="." align="char"><italic>0</italic>.<italic>92</italic></td><td char="." align="char">0.69</td><td char="." align="char">0.78</td></tr><tr><td align="left">Mass</td><td char="." align="char">0.91</td><td char="." align="char"><italic>0</italic>.<italic>79</italic></td><td char="." align="char"><italic>0</italic>.<italic>83</italic></td></tr><tr><td align="left">Orientation: 2</td><td char="." align="char">0.86</td><td char="." align="char"><italic>0</italic>.<italic>79</italic></td><td char="." align="char"><italic>0</italic>.<italic>83</italic></td></tr><tr><td align="left">Orientation: 3</td><td char="." align="char">0.83</td><td char="." align="char"><italic>0</italic>.<italic>81</italic></td><td char="." align="char"><italic>0</italic>.<italic>84</italic></td></tr><tr><td align="left">Orientation: 4</td><td char="." align="char">0.80</td><td char="." align="char"><italic>0</italic>.<italic>83</italic></td><td char="." align="char"><italic>0</italic>.<italic>86</italic></td></tr><tr><td align="left">Orientation: 5</td><td char="." align="char">0.72</td><td char="." align="char"><italic>0</italic>.<italic>85</italic></td><td char="." align="char"><italic>0</italic>.<italic>84</italic></td></tr></tbody></table></table-wrap>
</p>
      <p>For the manual measurement of length, the most correlated proxy measures are proxy-depth, centroid-Y and the vertical extent of the bounding boxes containing 95 and 99% of root material (Table <xref rid="Tab3" ref-type="table">3</xref>). Q3 mass also provides a good correlation—this represents the amount of root material in the third horizontal quarter down the well. Intuitively, the amount of root material in the third and fourth quadrants will increase with the overall length of the root system, dependent on the particular length of the root system. Figure <xref rid="Fig2" ref-type="fig">2</xref>b shows proxy depth plotted against manually estimated length, showing a good correlation throughout the range of data.</p>
      <p>The manual density measure correlates highly with both overall mass, and orientation measures which capture lateral root emergence, as these represent much of the root mass in a dense image. The manual lateral count again correlates highly with root mass at orientations 3–5 (which mainly capture near-horizontal to 45° lateral root material), and correlates well with mass (as much of the mass of the root system is given over to lateral roots).</p>
      <p>Correlation is important as we are not measuring pure, classic traits here. Therefore if we can identify a proxy measure (or combination of measures) which correlate well with a classical trait, then these measures can be used to identify the phenotypes of interest. A strong positive correlation will show that as proxy measure A increases, this also indicates an increase in classical trait A, so phenotypes can be compared relatively. Hence the need to calibrate measures into real world units is not required.</p>
      <p>Finally, note that the specific subset of proxy traits which correlate well with classical measures of interest will vary dependant on the exact nature of the phenotype, and so you may experience different correlations to those listed here. This is the motivation behind directly using the proxy measures as phenotypic discriminators, as investigated in the following section.</p>
    </sec>
    <sec id="Sec6">
      <title>Proxy measures as discriminators</title>
      <p>As well as being correlated with traditional traits, the new proxy measures are useful discriminative measures in their own right. To demonstrate this, we analysed the raw data produced by the software from the same growth experiment described above. All well plates in all images were analysed, fully automatically without user interaction. Scatterplots of pairs of proxy measures (commonly referred to as a pairs plot) were produced (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). This figure gives a helpful indication of how useful the measures are in identifying phenotypic differences in our example dataset. As can be seen in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, the two growth conditions are strongly separable in many of the newly proposed measures. Based on this, traditional unsupervised clustering approaches such as k-means would work effectively in separating phenotypic classes.<fig id="Fig3"><label>Fig. 3</label><caption><p><italic>Scatter plots</italic> for pairs of proxy traits, for PEG (<italic>red</italic>) and control (<italic>black</italic>) datasets. <italic>Inset</italic> biplot of the PCA across components 1 and 2. Note how many of the proxy traits clearly separate the two experimental conditions. Therefore, they can be used to detect phenotypically different datasets directly (see Additional file <xref rid="MOESM2" ref-type="media">2</xref>: Figure S2 for example images)</p></caption><graphic xlink:href="13007_2017_161_Fig3_HTML" id="MO6"/></fig>
</p>
      <p>Often, if individual traits do not provide enough separability, or some measures (such as variants on depth) are strongly correlated, or the dimensionality of the dataset is large, principal component analysis (PCA) can be used to perform dimensionality reduction. PCA provides a new set of variables that are created by linearly combining the original variables and maximising the variance of the dataset. The inset in Fig. <xref rid="Fig3" ref-type="fig">3</xref> shows a biplot of the results of PCA on this dataset, for principal components 1 and 2 (the components with the largest proportion of the variance of the data). Together, these components represent over 78% of the variance in this dataset, and can be shown to separate easily the control and treated plants.</p>
    </sec>
  </sec>
  <sec id="Sec7" sec-type="conclusion">
    <title>Conclusion</title>
    <p>In this paper we present an alternative to traditional manual image measures, which we term <italic>proxy traits</italic>. They can be calculated over complex images where segmenting all parts of a root system is not possible, or not reliable. These proxy traits are well suited to fully automated analysis settings, especially as part of automated robotic-based systems. Computational performance exceeds image capture, so does not produce an additional bottleneck.</p>
    <p>As shown in the results, the proposed proxy measures are able to correlate well with existing metrics commonly derived from image data. This suggests these fast-to-calculate and automatically-derived measures are suitable for replacing some of these traditional metrics, and can be used to identify relative differences in phenotypic conditions.</p>
    <p>In addition, analysis of the proxy measures via statistical techniques such as PCA and visualisation using scatter plots reveals that it is possible to directly discriminate between different plants exhibiting different phenotypic traits. As can be seen from Fig. <xref rid="Fig3" ref-type="fig">3</xref>, many of the proxy traits separate across the two datasets, showing that they can be used to identify phenotypic differences between the plants. Of course, for more subtle phenotypic differences, the separation may not be as dramatic, and only some traits (or combination of traits) may separate conditions. If this is the case, usual statistical treatments can be applied to the proxy measures to identify subtle phenotypic differences, just as with traditional measures.</p>
    <p>The new measures are not a suitable alternative to manual or semi-automatic approaches, where fine-grained phenotypic analysis is required. However, we have shown that they can be used as an alternative to many traditional high-throughput measures, and that they are able to separate phenotypically-different datasets. In many cases (e.g. [<xref ref-type="bibr" rid="CR7">7</xref>]) fine-grained capture of a root system architecture is used as an intermediate step to producing phenotypic traits, in which case this intermediate step could be skipped, significantly speeding up image analysis. Finally, there is no reason to believe similar traits could not be used in phenotyping other systems, such as plant shoots, where similar proxies could be used.</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec8">
        <title>Additional files</title>
        <p>
          <media position="anchor" xlink:href="13007_2017_161_MOESM1_ESM.docx" id="MOESM1">
            <caption>
              <p><bold>Additional file 1: Figure S1.</bold> Histogram of the relative frequencies of orientations in control and treated.</p>
            </caption>
          </media>
          <media position="anchor" xlink:href="13007_2017_161_MOESM2_ESM.docx" id="MOESM2">
            <caption>
              <p><bold>Additional file 2: Figure S2.</bold> Example control and treatment images.</p>
            </caption>
          </media>
        </p>
      </sec>
    </app>
  </app-group>
  <ack>
    <title>Authors’ contributions</title>
    <p>APF and MPP designed and wrote the AutoRoot algorithms and software. SF and BGF conducted Microphenotron data capture and analysis. APF and MPP conducted AutoRoot validations and wrote the manuscript, with assistance from SF and BGF. MTT assisted with statistical analysis. All authors read and approved the final manuscript. </p>
    <sec id="FPar1">
      <title>Acknowledgements</title>
      <p>None.</p>
    </sec>
    <sec id="FPar2">
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="FPar3">
      <title>Availability of data and materials</title>
      <p>The AutoRoot software is open source and available from <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.5281/zenodo.60433">http://dx.doi.org/10.5281/zenodo.60433</ext-link>.</p>
    </sec>
    <sec id="FPar4">
      <title>Funding</title>
      <p>This work was partly supported by funding from the UK Biotechnology and Biological Sciences Research Council (Grant No. BB/M004260/1).</p>
    </sec>
    <sec id="FPar5">
      <title>Plant material</title>
      <p>Plant material (<italic>Arabidopsis thaliana</italic> and <italic>Eragrostis teff</italic>) was obtained from commercial or authorised sources and no special permission or licences were required for their use. Transgenic Arabidopsis lines were handled and disposed of according to The Genetically Modified Organisms (Contained Use) Regulations 2014 issued by the UK Health and Safety Executive.</p>
    </sec>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Walter</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Liebisch</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hund</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Plant phenotyping: from bean weighing to image analysis</article-title>
        <source>Plant Methods</source>
        <year>2015</year>
        <volume>11</volume>
        <fpage>1</fpage>
        <lpage>11</lpage>
        <pub-id pub-id-type="doi">10.1186/s13007-015-0056-8</pub-id>
        <pub-id pub-id-type="pmid">25649124</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dhondt</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wuyts</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Inzé</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Cell to whole-plant phenotyping: the best is yet to come</article-title>
        <source>Trends Plant Sci</source>
        <year>2013</year>
        <volume>18</volume>
        <fpage>428</fpage>
        <lpage>439</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tplants.2013.04.008</pub-id>
        <?supplied-pmid 23706697?>
        <pub-id pub-id-type="pmid">23706697</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fahlgren</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Gehan</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Baxter</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Lights, camera, action: high-throughput plant phenotyping is ready for a close-up</article-title>
        <source>Curr Opin Plant Biol</source>
        <year>2015</year>
        <volume>24</volume>
        <fpage>93</fpage>
        <lpage>99</lpage>
        <pub-id pub-id-type="doi">10.1016/j.pbi.2015.02.006</pub-id>
        <?supplied-pmid 25733069?>
        <pub-id pub-id-type="pmid">25733069</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Scharr</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Minervini</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>French</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Klukas</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kramer</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Luengo</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Pape</surname>
            <given-names>J-M</given-names>
          </name>
          <name>
            <surname>Polder</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Vukadinovic</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Tsaftaris</surname>
            <given-names>SA</given-names>
          </name>
        </person-group>
        <article-title>Leaf segmentation in plant phenotyping: a collation study</article-title>
        <source>Mach Vis Appl</source>
        <year>2015</year>
        <volume>27</volume>
        <fpage>585</fpage>
        <lpage>606</lpage>
        <pub-id pub-id-type="doi">10.1007/s00138-015-0737-3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Walter</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Liebisch</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hund</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Plant phenotyping: from bean weighing to image analysis</article-title>
        <source>Plant Methods</source>
        <year>2015</year>
        <volume>11</volume>
        <fpage>4</fpage>
        <pub-id pub-id-type="doi">10.1186/s13007-015-0056-8</pub-id>
        <pub-id pub-id-type="pmid">25657813</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cai</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zeng</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Connor</surname>
            <given-names>JN</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>CY</given-names>
          </name>
          <name>
            <surname>Melino</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Kumar</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Miklavic</surname>
            <given-names>SJ</given-names>
          </name>
        </person-group>
        <article-title>RootGraph: a graphic optimization tool for automated image analysis of plant roots</article-title>
        <source>J Exp Bot</source>
        <year>2015</year>
        <volume>66</volume>
        <issue>21</issue>
        <fpage>6551</fpage>
        <lpage>6562</lpage>
        <pub-id pub-id-type="doi">10.1093/jxb/erv359</pub-id>
        <?supplied-pmid 26224880?>
        <pub-id pub-id-type="pmid">26224880</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Pound MP, French AP, Atkinson J, Wells DM, Bennett MJ, Pridmore TP. RootNav: navigating images of complex root architectures. Plant Physiol. 2013;162;4:1802–14.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Lobet G, Pagès L, Draye X. A novel image analysis toolbox enabling quantitative analysis of root system architecture. Plant Physiol. 2011;157;1:29–39</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Galkovskyi</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Mileyko</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bucksch</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Moore</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Symonova</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Price</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Topp</surname>
            <given-names>CN</given-names>
          </name>
          <name>
            <surname>Iyer-Pascuzzi</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Zurek</surname>
            <given-names>PR</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Harer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Benfey</surname>
            <given-names>PN</given-names>
          </name>
          <name>
            <surname>Weitz</surname>
            <given-names>JS</given-names>
          </name>
        </person-group>
        <article-title>GiA roots: software for the high throughput analysis of plant root system architecture</article-title>
        <source>BMC Plant Biol</source>
        <year>2012</year>
        <volume>12</volume>
        <fpage>116</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2229-12-116</pub-id>
        <?supplied-pmid 22834569?>
        <pub-id pub-id-type="pmid">22834569</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Armengaud</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>EZ-Rhizo software</article-title>
        <source>Plant Signal Behav</source>
        <year>2009</year>
        <volume>4</volume>
        <fpage>139</fpage>
        <lpage>141</lpage>
        <pub-id pub-id-type="doi">10.4161/psb.4.2.7763</pub-id>
        <?supplied-pmid 19649192?>
        <pub-id pub-id-type="pmid">19649192</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ganapathysubramanian</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Sarkar</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Machine learning for high-throughput stress phenotyping in plants</article-title>
        <source>Trends Plant Sci</source>
        <year>2016</year>
        <volume>21</volume>
        <fpage>110</fpage>
        <lpage>124</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tplants.2015.10.015</pub-id>
        <?supplied-pmid 26651918?>
        <pub-id pub-id-type="pmid">26651918</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tsaftaris</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Minervini</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Scharr</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Machine learning for plant phenotyping needs image processing</article-title>
        <source>Trends Plant Sci</source>
        <year>2016</year>
        <volume>21</volume>
        <fpage>989</fpage>
        <lpage>991</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tplants.2016.10.002</pub-id>
        <?supplied-pmid 27810146?>
        <pub-id pub-id-type="pmid">27810146</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Navarro</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Pérez</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Egea-Cortines</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Machine learning and computer vision system for phenotype data acquisition and analysis in plants</article-title>
        <source>Sensors</source>
        <year>2016</year>
        <volume>16</volume>
        <issue>5</issue>
        <fpage>641</fpage>
        <pub-id pub-id-type="doi">10.3390/s16050641</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <?supplied-pmid 26017442?>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Pound MP, Burgess AJ, Wilson MH, Atkinson JA, Griffiths M, Jackson AS, Bulat A, Tzimiropoulos G, Wells DM, Murchie EH, Pridmore TP, French AP. Deep machine learning provides state-of-the-art performance in image-based plant phenotyping. bioRxiv. 2016. 53033. doi:10.1101/053033.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goff</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Vaughn</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>McKay</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lyons</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Stapleton</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Gessler</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Matasci</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Hanlon</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lenards</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Muir</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Merchant</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Lowry</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Mock</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Helmke</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kubach</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Narro</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hopkins</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Micklos</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hilgert</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Gonzales</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Jordan</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Skidmore</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Dooley</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Cazes</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>McLay</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Pasternak</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Koesterke</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Piel</surname>
            <given-names>WH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The iPlant collaborative: cyberinfrastructure for plant biology</article-title>
        <source>Front Plant Sci</source>
        <year>2011</year>
        <volume>2</volume>
        <fpage>34</fpage>
        <pub-id pub-id-type="doi">10.3389/fpls.2011.00034</pub-id>
        <?supplied-pmid 22645531?>
        <pub-id pub-id-type="pmid">22645531</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>French</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ubeda-Tomás</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Holman</surname>
            <given-names>TJ</given-names>
          </name>
          <name>
            <surname>Bennett</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Pridmore</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>High-throughput quantification of root growth using a novel image-analysis tool</article-title>
        <source>Plant Physiol</source>
        <year>2009</year>
        <volume>150</volume>
        <fpage>1784</fpage>
        <lpage>1795</lpage>
        <pub-id pub-id-type="doi">10.1104/pp.109.140558</pub-id>
        <?supplied-pmid 19515787?>
        <pub-id pub-id-type="pmid">19515787</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Atkinson JA, Wingen LU, Griffiths M, Pound MP, Gaju O, Foulkes MJ, Gouis JL, Griffiths S, Bennett MJ, King J, Wells DM. Phenotyping pipeline reveals major seedling root growth QTL in hexaploid wheat. J Exp Bot. 2015;66;8:2283–92. doi:10.1093/jxb/erv006.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pridmore</surname>
            <given-names>TP</given-names>
          </name>
          <name>
            <surname>French</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Pound</surname>
            <given-names>MP</given-names>
          </name>
        </person-group>
        <article-title>What lies beneath: underlying assumptions in bioimage analysis</article-title>
        <source>Trends Plant Sci</source>
        <year>2012</year>
        <volume>17</volume>
        <fpage>688</fpage>
        <lpage>692</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tplants.2012.07.003</pub-id>
        <?supplied-pmid 22902890?>
        <pub-id pub-id-type="pmid">22902890</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Burrell T, Fozard S, Holroyd GH, French AP, Pound MP, Bigley CJ, et al. The Microphenotron: a robotic miniaturized plant phenotyping platform with diverse applications in chemical biology. Plant Methods. 2017;13:10. doi:10.1186/s13007-017-0158-6.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Forde</surname>
            <given-names>BG</given-names>
          </name>
          <name>
            <surname>Cutler</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Zaman</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Krysan</surname>
            <given-names>PJ</given-names>
          </name>
        </person-group>
        <article-title>Glutamate signalling via a MEKK1 kinase-dependent pathway induces changes in Arabidopsis root architecture</article-title>
        <source>Plant J</source>
        <year>2013</year>
        <volume>75</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1111/tpj.12201</pub-id>
        <?supplied-pmid 23574009?>
        <pub-id pub-id-type="pmid">23574009</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dijkstra</surname>
            <given-names>EW</given-names>
          </name>
        </person-group>
        <article-title>A note on two problems in connexion with graphs</article-title>
        <source>Numer Math</source>
        <year>1959</year>
        <volume>1</volume>
        <fpage>269</fpage>
        <lpage>271</lpage>
        <pub-id pub-id-type="doi">10.1007/BF01386390</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van der Weele</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Spollen</surname>
            <given-names>WG</given-names>
          </name>
          <name>
            <surname>Sharp</surname>
            <given-names>RE</given-names>
          </name>
          <name>
            <surname>Baskin</surname>
            <given-names>TI</given-names>
          </name>
        </person-group>
        <article-title>Growth of <italic>Arabidopsis thaliana</italic> seedlings under water deficit studied by control of water potential in nutrient-agar media</article-title>
        <source>J Exp Bot</source>
        <year>2000</year>
        <volume>51</volume>
        <fpage>1555</fpage>
        <lpage>1562</lpage>
        <pub-id pub-id-type="doi">10.1093/jexbot/51.350.1555</pub-id>
        <?supplied-pmid 11006306?>
        <pub-id pub-id-type="pmid">11006306</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Schindelin J. Fiji is just ImageJ (batteries included). In: ImageJ user and developer conference, Luxembourg. 2008.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Trachsel</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kaeppler</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Brown</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Lynch</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Shovelomics: high throughput phenotyping of maize (<italic>Zea mays</italic> L.) root architecture in the field</article-title>
        <source>Plant Soil</source>
        <year>2011</year>
        <volume>341</volume>
        <fpage>75</fpage>
        <lpage>87</lpage>
        <pub-id pub-id-type="doi">10.1007/s11104-010-0623-8</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
