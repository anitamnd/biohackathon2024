<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v3.0 20080202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing3.dtd?>
<?SourceDTD.Version 3.0?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Biomed Opt Express</journal-id>
    <journal-id journal-id-type="iso-abbrev">Biomed Opt Express</journal-id>
    <journal-id journal-id-type="publisher-id">BOE</journal-id>
    <journal-title-group>
      <journal-title>Biomedical Optics Express</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2156-7085</issn>
    <publisher>
      <publisher-name>Optical Society of America</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8176814</article-id>
    <article-id pub-id-type="publisher-id">414680</article-id>
    <article-id pub-id-type="doi">10.1364/BOE.414680</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ML-SIM: universal reconstruction of structured illumination microscopy images using transfer learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Christensen</surname>
          <given-names>Charles N.</given-names>
        </name>
        <contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5355-1063</contrib-id>
        <xref ref-type="aff" rid="aff1">1</xref>
        <xref ref-type="aff" rid="aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ward</surname>
          <given-names>Edward N.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lu</surname>
          <given-names>Meng</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lio</surname>
          <given-names>Pietro</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kaminski</surname>
          <given-names>Clemens F.</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
        <xref ref-type="corresp" rid="cor1">*</xref>
      </contrib>
      <aff id="aff1"><label>1</label><institution>University of Cambridge</institution>, Department of Chemical Engineering and Biotechnology, Laser Analytics Group, Philippa Fawcett Dr, Cambridge, <country country="GB">UK</country></aff>
      <aff id="aff2"><label>2</label><institution>University of Cambridge</institution>, Department of Computer Science and Technology, Artificial Intelligence Group, JJ Thomson Ave, Cambridge, <country country="GB">UK</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>Corresponding author: <email>cfk23@cam.ac.uk</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>4</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <day>01</day>
      <month>5</month>
      <year>2021</year>
    </pub-date>
    <volume>12</volume>
    <issue>5</issue>
    <fpage>2720</fpage>
    <lpage>2733</lpage>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>11</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>29</day>
        <month>1</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>1</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Published by The Optical Society under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 License</ext-link>. Further distribution of this work must maintain attribution to the author(s) and the published article’s title, journal citation, and DOI.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>
          <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>
        </license-p>
      </license>
    </permissions>
    <abstract>
      <p>Structured illumination microscopy (SIM) has become an important technique for optical super-resolution imaging because it allows a doubling of image resolution at speeds compatible with live-cell imaging. However, the reconstruction of SIM images is often slow, prone to artefacts, and requires multiple parameter adjustments to reflect different hardware or experimental conditions. Here, we introduce a versatile reconstruction method, ML-SIM, which makes use of transfer learning to obtain a parameter-free model that generalises beyond the task of reconstructing data recorded by a specific imaging system for a specific sample type. We demonstrate the generality of the model and the high quality of the obtained reconstructions by application of ML-SIM on raw data obtained for multiple sample types acquired on distinct SIM microscopes. ML-SIM is an end-to-end deep residual neural network that is trained on an auxiliary domain consisting of simulated images, but is transferable to the target task of reconstructing experimental SIM images. By generating the training data to reflect challenging imaging conditions encountered in real systems, ML-SIM becomes robust to noise and irregularities in the illumination patterns of the raw SIM input frames. Since ML-SIM does not require the acquisition of experimental training data, the method can be efficiently adapted to any specific experimental SIM implementation. We compare the reconstruction quality enabled by ML-SIM with current state-of-the-art SIM reconstruction methods and demonstrate advantages in terms of generality and robustness to noise for both simulated and experimental inputs, thus making ML-SIM a useful alternative to traditional methods for challenging imaging conditions. Additionally, reconstruction of a SIM stack is accomplished in less than 200 ms on a modern graphics processing unit, enabling future applications for real-time imaging. Source code and ready-to-use software for the method are available at <ext-link ext-link-type="uri" xlink:href="http://ML-SIM.github.io">http://ML-SIM.github.io</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group id="sp1">
        <funding-source>Engineering and Physical Sciences Research Council<named-content content-type="doi">10.13039/501100000266</named-content></funding-source>
        <award-id>L015889</award-id>
      </award-group>
      <award-group id="sp2">
        <funding-source>Wellcome Trust<named-content content-type="doi">10.13039/100010269</named-content></funding-source>
        <award-id>089703</award-id>
      </award-group>
      <award-group id="sp3">
        <funding-source>Medical Research Council<named-content content-type="doi">10.13039/501100000265</named-content></funding-source>
        <award-id>K015850</award-id>
      </award-group>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec sec-type="introduction" id="sec1">
    <label>1.</label>
    <title>Introduction</title>
    <p>Structured illumination microscopy (SIM) is an optical super-resolution imaging technique that was proposed more than a decade ago [<xref rid="r1" ref-type="bibr">1</xref>–<xref rid="r5" ref-type="bibr">5</xref>], and continues to stand as a powerful alternative to techniques such as Single Molecule Localization Microscopy (SMLM) [<xref rid="r6" ref-type="bibr">6</xref>,<xref rid="r7" ref-type="bibr">7</xref>] and Stimulated Emission Depletion (STED) microscopy [<xref rid="r8" ref-type="bibr">8</xref>]. The principle of SIM is that by illuminating a fluorescent sample with a patterned illumination, interference patterns are generated that contain information about the fine details of the sample structure that are unobservable in diffraction-limited imaging. In the simplest case of a sinusoidal illumination pattern with a spatial frequency of <inline-formula><mml:math id="m1"><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, the images acquired are a superposition of three copies of the sample’s frequency spectrum, shifted by +<inline-formula><mml:math id="m2"><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, 0, and -<inline-formula><mml:math id="m3"><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. The super-resolution image is reconstructed by isolating the three superimposed spectra and shifting them into their correct location in frequency space. The resulting spectrum is then transformed back into real space, leading to an image that is doubled in resolution. Isolating the three frequency spectra is mathematically analogous to solving three simultaneous equations. This requires the acquisition of three raw images, with the phase of the SIM patterns shifted with respect to one another along the direction of <inline-formula><mml:math id="m4"><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>. Ideally, these phase shifts are in increments of <inline-formula><mml:math id="m5"><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:math></inline-formula> to ensure that the averaged illumination, i.e. the sum of all patterns, yields a homogeneous illumination field. Finally, to obtain isotropic resolution enhancement in all directions, this process is repeated twice, rotating the patterns by <inline-formula><mml:math id="m6"><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:math></inline-formula> each time, to yield a total of 9 images (i.e. 3 phase shifts for each of the 3 pattern orientations).</p>
    <p>While SIM can be extended to resolve features down to the 50-60 nm range [<xref rid="r9" ref-type="bibr">9</xref>,<xref rid="r10" ref-type="bibr">10</xref>], it does not offer the highest resolution of the available super-resolution methods. However, the inherent speed of SIM makes it uniquely suited for live-cell imaging [<xref rid="r11" ref-type="bibr">11</xref>,<xref rid="r12" ref-type="bibr">12</xref>]. SIM also requires relatively low illumination intensities, and therefore reduces photo-toxicity and photo-bleaching compared to other methods. Many of the drawbacks of SIM relate to the reconstruction process, which can be time-consuming and prone to artefacts. In all but optimal imaging conditions, deviations from the expected imaging model or the incorrect estimation of experimental parameters (pixel size, wavelength, optical transfer function, image filters, phase step size etc.) introduce artefacts, degrading the final image quality [<xref rid="r13" ref-type="bibr">13</xref>]. This becomes especially prominent for images with low signal-to-noise ratios, where traditional methods will mistakenly reconstruct noise as signal, leading to artefacts that can be hard to distinguish from real features in the sample. At worst, the reconstruction process fails completely. These issues can introduce an element of subjectivity into the reconstruction process, leading to a temptation to adjust reconstruction parameters until the ’expected’ result is obtained. In addition, traditional reconstruction methods are computationally demanding. The processing time for a single reconstruction in popular implementations such as FairSIM [<xref rid="r14" ref-type="bibr">14</xref>], a plugin for ImageJ/Fiji, and OpenSIM running in MATLAB [<xref rid="r15" ref-type="bibr">15</xref>], can reach tens of seconds even on high-end machines, making real-time processing during SIM image acquisition infeasible. Finally, traditional methods cannot easily reconstruct images from SIM data that is underdetermined, e.g. inputs with fewer than 9 frames and / or recordings with uneven phase steps between frames. These drawbacks limit the applicability of SIM when imaging highly dynamic processes [<xref rid="r16" ref-type="bibr">16</xref>]. Examples include the peristaltic movement of the endoplasmic reticulum [<xref rid="r17" ref-type="bibr">17</xref>] or the process of cell division [<xref rid="r18" ref-type="bibr">18</xref>], which require low light level imaging at high speed to reduce the effects of photo-toxicity and photo-bleaching.</p>
    <p>In this work, we propose a versatile reconstruction method, ML-SIM, that addresses these issues with transfer learning. Transfer learning is a branch of machine learning that aims to exploit the knowledge obtained in an auxiliary domain to facilitate solving a specific task in the target domain [<xref rid="r19" ref-type="bibr">19</xref>]. While there are several methods to achieve this, a modern approach is to train a deep neural network to solve a similar task on a large dataset in the auxiliary domain, after which the network can be fine-tuned by slight changes to network architecture and retrained on a much smaller set of examples from the target domain [<xref rid="r20" ref-type="bibr">20</xref>]. ML-SIM uses an end-to-end deep residual neural network that is trained in an auxiliary domain consisting of simulated images using a high degree of randomisation. The training in the auxiliary domain is, in our case, sufficient for the network to generalise to a wide range of practically encountered conditions. This means that further fine-tuning of the model by training on real-world datasets, e.g. obtained from actual SIM experiments, is mostly not necessary. However, further fine-tuning and retraining is possible and supported by ML-SIM, thus offering maximal flexibility of the code to work for any experimental SIM implementation. Importantly, no output images from traditional reconstruction methods are required for training, thereby avoiding having the network undesirably learn to reproduce the reconstruction artefacts affecting the traditional methods. In a recent study [<xref rid="r21" ref-type="bibr">21</xref>], the problem of performing SIM reconstruction with a neural network, using U-Net [<xref rid="r22" ref-type="bibr">22</xref>], was attempted in exactly this manner of using traditional reconstructed outputs as targets for training, thus simply approximating the current methods and prohibiting the network from becoming superior. Furthermore, the proposed deep residual network of ML-SIM is found to be significantly more capable of SIM reconstruction than the simpler U-Net – see Section <xref ref-type="sec" rid="sec3-3">3.3</xref>. The training data in the auxiliary domain is generated by a simulation of the SIM imaging process. It is due to these training pairs of synthetic inputs and ideal, high-resolution targets (ground truths) for supervised learning that ML-SIM avoids exposing the model to the traditional reconstruction artefacts. Although the training data used is simulated and unrelated to real microscopic samples, we find that the method indeed generalises beyond the auxiliary domain, and we demonstrate successful application to experimental data obtained from two distinct SIM microscopes. This greatly empowers the method in the context of generalised reconstruction for super-resolution SIM imaging, since models can be customised to SIM setups of any configuration by changing simulation parameters used in the generation of the training data.</p>
  </sec>
  <sec sec-type="methods" id="sec2">
    <label>2.</label>
    <title>Methods</title>
    <sec id="sec2-1">
      <label>2.1</label>
      <title>Convolution neural networks</title>
      <p>Artificial neural networks consist of a sequence of layers that each performs a simple operation, typically a weighted sum followed by a nonlinear activation function, where every weight corresponds to a neuron in the layer. The weights are trainable, meaning that they are updated after every evaluation of an input performed during training. The updating scheme can be as simple as gradient descent with gradients determined via backpropagation of a loss calculated as the deviation between the network’s output and a known target. A convolutional layer is no different, but utilises spatial information by only applying filters to patches of neighbouring pixels. The number of learned filters in one layer is a parameter, but is typically a power of 2, such as 32, 64 or 128. The network links past layers to present layers by skip connections to avoid the vanishing gradient problem. This type of architecture is known as a residual neural network [<xref rid="r23" ref-type="bibr">23</xref>].</p>
      <p>Motivated by the results summarised in <xref rid="g007" ref-type="fig">Fig. 7</xref>, and with the certainty that the entire input stack is utilised for the output reconstruction, the RCAN architecture was chosen for ML-SIM. The depth of the network was chosen to be around 100 convolutional layers (10 residual groups with 3 residual blocks). The network was then trained for 200 epochs with a learning rate of <inline-formula><mml:math id="m7"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>, which was halved after every 20 epochs, using the Adam optimiser [<xref rid="r24" ref-type="bibr">24</xref>]. The models were implemented with Pytorch and trained using an Nvidia Tesla K80 GPU for approximately a day per model. Models have been trained on the full DIV2K image dataset except for a small collection of randomly selected images used for validation during training.</p>
    </sec>
    <sec id="sec2-2">
      <label>2.2</label>
      <title>Generating simulated data</title>
      <p>The ideal optical transfer function is generated based on a given objective numerical aperture (NA), pixel size and fluorescence emission wavelength. The illumination stripe patterns were then calculated from their spatial frequency <inline-formula><mml:math id="m8"><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and a phase <inline-formula><mml:math id="m9"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, <disp-formula id="e1"><label>(1)</label><mml:math id="m10"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula> where <inline-formula><mml:math id="m11"><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula> = <inline-formula><mml:math id="m12"><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula> for a pattern orientation <inline-formula><mml:math id="m13"><mml:mi>θ</mml:mi></mml:math></inline-formula> relative to the horizontal axis. <inline-formula><mml:math id="m14"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> defines the phase of the pattern (i.e. the lateral shift in the direction of <inline-formula><mml:math id="m15"><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>), and <inline-formula><mml:math id="m16"><mml:mi>m</mml:mi></mml:math></inline-formula> is the modulation depth which defines the relative strength of the super-resolution information contained in the raw images. In total, 9 images were generated for each target image, corresponding to three phase shifts for each of three pattern orientations – see Figure S3 for a depiction. The use of ML-SIM with different configurations of phase shifts and pattern orientations is covered in <inline-supplementary-material xlink:href="boe-12-5-2720-s001.pdf" xlink:type="simple" id="d24e441" content-type="local-data">Supplement 1</inline-supplementary-material>. The fluorescent response of the sample can then be modelled by the multiplication of the sample structure, <inline-formula><mml:math id="m17"><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> (input image), and the illumination pattern intensity <inline-formula><mml:math id="m18"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. The final image, <inline-formula><mml:math id="m19"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, is then blurred by the PSF, <inline-formula><mml:math id="m20"><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, and noised with the addition of white Gaussian noise, <inline-formula><mml:math id="m21"><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, <disp-formula id="e2"><label>(2)</label><mml:math id="m22"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:mrow><mml:mo>⊗</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula> where <inline-formula><mml:math id="m23"><mml:mo>⊗</mml:mo></mml:math></inline-formula> is the convolution operation. The option of using Poisson noise rather than Gaussian noise is explored in <inline-supplementary-material xlink:href="boe-12-5-2720-s001.pdf" xlink:type="simple" id="d24e586" content-type="local-data">Supplement 1</inline-supplementary-material>. In addition to the Gaussian noise, <inline-formula><mml:math id="m24"><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, added pixel-by-pixel, a random error is added to the parameters for the stripe patterns, <inline-formula><mml:math id="m25"><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="m26"><mml:mi>θ</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="m27"><mml:mi>ϕ</mml:mi></mml:math></inline-formula>, to approximate the inherent uncertainty in an experimental setup for illumination pattern generation. The importance of including these types of errors is described in <inline-supplementary-material xlink:href="boe-12-5-2720-s001.pdf" xlink:type="simple" id="d24e622" content-type="local-data">Supplement 1</inline-supplementary-material>.</p>
      <p>The images generated from <xref rid="e2" ref-type="disp-formula">2</xref> are used as inputs in a supervised learning approach. The targets used to calculate loss for optimising the neural network are the clean grayscale source images used as <inline-formula><mml:math id="m28"><mml:mi>S</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. These targets could as well be blurred with a PSF corresponding to the best theoretically achievable resolution of standard SIM, but as explored in <inline-supplementary-material xlink:href="boe-12-5-2720-s001.pdf" xlink:type="simple" id="d24e643" content-type="local-data">Supplement 1</inline-supplementary-material> this is not found to be beneficial and instead the unmodified source images are used as targets and referred to as the ground truths. The values for <inline-formula><mml:math id="m29"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="m30"><mml:msub><mml:mi>k</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> that are used for data generation are given in <inline-supplementary-material xlink:href="boe-12-5-2720-s001.pdf" xlink:type="simple" id="d24e661" content-type="local-data">Supplement 1</inline-supplementary-material>.</p>
    </sec>
    <sec id="sec2-3">
      <label>2.3</label>
      <title>Microscopy</title>
      <p>For the experimental data described in Section <xref ref-type="sec" rid="sec3">3</xref>, two custom-built SIM microscopes were used. For the imaging of the endoplasmic reticulum (ER), a SIM instrument based on a phase-only spatial light modulator was used. The microscope objective used was a 60X 1.2 NA water immersion lens and fluorescence was imaged with a sCMOS camera. Cells were labelled with VAPA-GFP and excited by 488 nm laser light. For the imaging of the cell membrane, a novel SIM setup based on interferometry for the pattern generation was used [<xref rid="r25" ref-type="bibr">25</xref>]. In this system, the angle and phase shifts are achieved by rotation of a scanning mirror, the repeatability of which introduces uncertainty into the phase shifting. The microscope objective used was a 60X 1.2 NA water immersion lens, and fluorescence was imaged with an sCMOS camera. The cell membrane was stained with a CAAX-Venus label and excited with 491 nm laser light. On both systems, 200 nm beads labelled with Rhodamine B were excited by 561 nm laser light. For both images, the traditional reconstruction methods that have been tested managed to reconstruct the raw SIM stacks, although with varying success for the interferometric SIM setup due to the irregularity of the phase stepping.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec3">
    <label>3.</label>
    <title>Results</title>
    <sec id="sec3-1">
      <label>3.1</label>
      <title>Using machine learning to train a reconstruction model</title>
      <p>ML-SIM is built on an artificial neural network. Its purpose is to process a sequence of raw SIM frames (i.e. a stack of nine images representing the number of raw images acquired in a SIM experiment), into a single super-resolved image. To achieve this, a supervised learning approach is used to train the network, where pairs of inputs and desired outputs (ground truths) are presented during training to learn a mapping function. These training data could be acquired on a real SIM system using a diverse collection of samples and experimental conditions. However, the targets corresponding to the required inputs are more difficult to obtain. At least two approaches seem possible: (A) using outputs from traditional reconstruction methods as targets [<xref rid="r21" ref-type="bibr">21</xref>]; and (B) using images from other super-resolution microscopy techniques that are able to achieve higher resolution than SIM (e.g SMLM or STED). Option (A) would prohibit ML-SIM from producing reconstructions that surpass the quality of traditional methods and would be prone to reproducing the artefacts mentioned in Section <xref ref-type="sec" rid="sec1">1</xref>. Option (B) requires a capability to perform correlative imaging of the same sample, which may be difficult to achieve since training requires hundreds or even thousands of distinct data pairs [<xref rid="r29" ref-type="bibr">29</xref>]. In addition, both approaches require the preparation of many unique samples to build a training set diverse enough for the model to generalise well. Hence, these options were not pursued in this work and we approached the problem instead by starting with ground truth images, and simulating inputs by mimicking the SIM process in silico, allowing for very diverse training sets to be built. We used the image set DIV2K [<xref rid="r30" ref-type="bibr">30</xref>], which consists of 1000 high-resolution images of a large variety of objects, patterns and environments. To generate the SIM data, images from the image set were first resized to a standard resolution of 512x512 pixels and transformed to greyscale. Raw SIM images were then calculated using a SIM model adapted from the OpenSIM package [<xref rid="r15" ref-type="bibr">15</xref>]. The model and underlying parameters are described in Section <xref ref-type="sec" rid="sec2">2</xref>. The simulated raw SIM stacks were used as input to the neural network and the output compared to the known ground truth in order to calculate a loss to update the network weights. <xref rid="g001" ref-type="fig">Figure 1</xref> shows an overview of the training process with an example of a simulated SIM input. The architecture of the neural network is further described in <inline-supplementary-material xlink:href="boe-12-5-2720-s001.pdf" xlink:type="simple" id="d24e705" content-type="local-data">Supplement 1</inline-supplementary-material>.</p>
      <fig id="g001" fig-type="figure" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>Data processing pipeline for ML-SIM. Training data for the model is generated by taking images from the commonly used image dataset DIV2K and simulating the imaging process of SIM using a model adapted from the open source library OpenSIM. The simulation can be further optimised to reflect the properties of the experimental system for which the reconstruction method is desired, for example to match the pixel size of the detector or numerical aperture of the detection optics. The outputs of the simulation are image stacks of the same size as those acquired by the microscope (here 9 frames).</p>
        </caption>
        <graphic xlink:href="boe-12-5-2720-g001"/>
      </fig>
    </sec>
    <sec id="sec3-2">
      <label>3.2</label>
      <title>Application of the trained model</title>
      <p>To begin with, we tested that the network had learned to reconstruct simulated SIM stacks. Prior to training, a separate partition of DIV2K was selected for testing. A sample from this test partition is shown in <xref rid="g002" ref-type="fig">Fig. 2</xref>. The stripe pattern for two of the nine frames of the input SIM stack are shown in the leftmost panel. The stripe patterns cancel out when all 9 frames are summed together (second column), and this corresponds to the case of even illumination in a wide-field microscope. Compared to the wide-field image, the reconstruction from ML-SIM is seen to have a much improved resolution with a peak signal-to-noise ratio (PSNR) value more than 7 dB higher as well as a significantly higher structural similarity index (SSIM). Beyond these metrics, several features of the image can be seen to be resolved after reconstruction that were not visible beforehand, such as the vertical posts seen to the right side of the cropped region.</p>
      <fig id="g002" fig-type="figure" orientation="portrait" position="float">
        <label>Fig. 2.</label>
        <caption>
          <p>Generation of training datasets for ML-SIM. Column 1: Sample from test partition of DIV2K (ground truth) transformed to a raw data stack of 9 frames via simulation of the SIM imaging process. Two different orientations are shown for the excitation patterns. Column 2: Wide-field image, obtained as the mean of the 9 raw frames. Column 3: Super-resolved image obtained through reconstruction with ML-SIM. Column 4: Ground truth. The image quality metrics shown in brackets are the peak signal-to-noise ratio and the structural similarity index [<xref rid="r26" ref-type="bibr">26</xref>], respectively.</p>
        </caption>
        <graphic xlink:href="boe-12-5-2720-g002"/>
      </fig>
      <p>It should also be noted that the reconstruction has not amplified the noise by introducing any evident artefacts, even though the input image featured a significant amount of Gaussian noise in addition to randomisation of the stripe frequency and phase – see Section <xref ref-type="sec" rid="sec2">2</xref> for definitions of those parameters. As further described in Section <xref ref-type="sec" rid="sec2">2</xref>, the neural network underlying ML-SIM is different to those of generative networks, which means that the model is more strongly penalised during training for introducing image content that is not in the real image. We argue that, even though this results in slightly more blurred output images than would be achievable with a generative network [<xref rid="r31" ref-type="bibr">31</xref>,<xref rid="r32" ref-type="bibr">32</xref>], the absence of artificial features is preferable in scientific imaging applications. This trade-off is referred to as minimising the mathematical reconstruction error (e.g. root-mean-square deviation) rather than optimising the perceptual quality [<xref rid="r33" ref-type="bibr">33</xref>,<xref rid="r34" ref-type="bibr">34</xref>].</p>
      <p>While ML-SIM is able to reconstruct simulated SIM stack inputs, it is of course only valuable if it also works on real SIM data, acquired experimentally. The ML-SIM model was trained on input data from simulations, using data bearing little resemblance to real-world biological SIM data. Any success for real-world SIM reconstructions therefore requires the model to have generalised the SIM process in such a way that it becomes independent of image content and sample type. This requires a realistic simulation of the SIM imaging process to generate training data that is sufficiently diverse on the one hand, and reflects measurement imperfections as encountered in practical SIM imaging. The former was avoided through use of a diverse training dataset, and the latter through use of the well-known imaging response function (Section <xref ref-type="sec" rid="sec2">2</xref>, Eq. (<xref rid="e2" ref-type="disp-formula">2</xref>)), and introduction of uncertainty in the stripe patterns. To test ML-SIM on experimental data, SIM images of different samples were acquired with two different SIM setups [<xref rid="r35" ref-type="bibr">35</xref>]. The resulting reconstructed outputs are shown in <xref rid="g003" ref-type="fig">Fig. 3</xref>, where they are compared to outputs of traditional reconstruction methods: OpenSIM [<xref rid="r15" ref-type="bibr">15</xref>], a cross-correlation (CC-SIM) phase retrieval approach [<xref rid="r28" ref-type="bibr">28</xref>,<xref rid="r36" ref-type="bibr">36</xref>], and FairSIM [<xref rid="r14" ref-type="bibr">14</xref>]. The images are grayscale images of signal intensity mapped to the Viridis colour table. ML-SIM is seen to obtain resolution on par with the other methods but producing less noisy background and fewer artefacts. The bottom two rows of images of beads and cell membranes were acquired with phase steps deviating from the ideal <inline-formula><mml:math id="m31"><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>3</mml:mn></mml:math></inline-formula>. This reflects a difficulty with the interferometric SIM setup (see Section <xref ref-type="sec" rid="sec2">2</xref>) to achieve equidistant, and precisely defined, phase steps for each illumination pattern angle. This means that the reconstruction algorithm must handle inconsistent phase changes, a factor only the cross-correlation method was capable of handling. However, although CC-SIM has improved resolution, artefacts are apparent, seen as vertical lines and ringing in the images. ML-SIM, on the other hand, reconstructed with fewer artefacts and strongly improved background rejection.</p>
      <fig id="g003" fig-type="figure" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Reconstruction of SIM images from four different samples imaged on two different experimental SIM set-ups. Microscope 1 uses a spatial light modulator for stripe pattern generation [<xref rid="r27" ref-type="bibr">27</xref>], while microscope 2 uses interferometric pattern generation. Both instruments were used to image a sample consisting of fluorescent beads as well as biological samples featuring the endoplasmic reticulum (ER) and a cell membrane, respectively. (Top) Full field-of-view images where each upper left half shows the reconstruction output from ML-SIM and each lower right half shows the wide-field version taken as the mean of the raw SIM stack. (Bottom) Cropped regions of reconstruction outputs from OpenSIM [<xref rid="r15" ref-type="bibr">15</xref>], CC-SIM [<xref rid="r28" ref-type="bibr">28</xref>], FairSIM [<xref rid="r14" ref-type="bibr">14</xref>] and ML-SIM. Panels in rows 2 to 5 correspond to regions indicated by coloured boxes in the full frame images.</p>
        </caption>
        <graphic xlink:href="boe-12-5-2720-g003"/>
      </fig>
      <p>To further demonstrate the super-resolution performance of ML-SIM, a sample of 30 nm microtubules labelled with Alexa-647 was imaged on microscope 1. The reconstruction outputs and line profiles across neighbouring microtubules for both ML-SIM and FairSIM are shown on <xref rid="g004" ref-type="fig">Fig. 4</xref>. The displayed cropped region contains two parallel microtubules which are separated by a gap of size below the diffraction limit and thus not resolved in the wide-field image. In the outputs from ML-SIM and FairSIM the gap is clearly visible. The distance between the peaks in the line profile for ML-SIM and FairSIM is <inline-formula><mml:math id="m32"><mml:mo>≃</mml:mo></mml:math></inline-formula> 150 nm, which is close to the theoretically achievable resolution with standard SIM [<xref rid="r3" ref-type="bibr">3</xref>]. Analysis of the resulting OTFs after reconstruction is also provided in <inline-supplementary-material xlink:href="boe-12-5-2720-s001.pdf" xlink:type="simple" id="d24e823" content-type="local-data">Supplement 1</inline-supplementary-material>.</p>
      <fig id="g004" fig-type="figure" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>Reconstruction of a SIM image of tubulin structures. The reconstruction output of ML-SIM is compared with a wide-field projected image and FairSIM. (Top) Full field-of-view of reconstructed image and line profiles across two parallel microtubules at the position indicated by the red line. While the microtubules are not resolved in widefield mode, both ML-SIM and FairSIM enable them to be clearly distinguished. (Bottom) Cropped regions of the reconstruction outputs corresponding to the area enclosed by the yellow rectangle.</p>
        </caption>
        <graphic xlink:href="boe-12-5-2720-g004"/>
      </fig>
      <p>The application of ML-SIM to TIRF-SIM image data using a sample image from the official FairSIM test image repository is described in <inline-supplementary-material xlink:href="boe-12-5-2720-s001.pdf" xlink:type="simple" id="d24e833" content-type="local-data">Supplement 1</inline-supplementary-material>.</p>
    </sec>
    <sec id="sec3-3">
      <label>3.3</label>
      <title>Performance assessment</title>
      <p>We performed a quantitative comparison of ML-SIM with traditional reconstruction methods on reconstructions of simulated raw SIM stacks generated from two image datasets; a subset of 10 DIV2K images, unseen during training, and 24 images from a dataset referred to as Kodak 24, commonly used for image restoration benchmarking [<xref rid="r30" ref-type="bibr">30</xref>,<xref rid="r37" ref-type="bibr">37</xref>]. Parameters for OpenSIM, CC-SIM and FairSIM were all systematically adjusted to produce the highest achievable output quality. Consequently, each method required completely different parameter configurations than those used for reconstructions of the experimental data shown in <xref rid="g003" ref-type="fig">Fig. 3</xref>. For ML-SIM however, there were no tunable parameters. The optical transfer function (OTF) is estimated within each method even though the function is known for the simulated images – this is the same premise as for the reconstruction of the experimental samples in <xref rid="g003" ref-type="fig">Fig. 3</xref>, for which the OTFs were unknown. Each method applies an identical Wiener filter to the final reconstruction output, whereas the output of ML-SIM is untouched. The performance scores for all methods measured in PSNR and SSIM averaged over the entire image sets are listed in Table S1 with scores for wide-field as a reference in terms of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM). For both metrics, ML-SIM has the highest scores with a PSNR that is 2 dB higher than for OpenSIM. CC-SIM and FairSIM lag behind but both methods still succeed in improving the input beyond the baseline wide-field reference. The performance gap between OpenSIM and the other traditional methods is likely due to a better estimation of the OTF, because OpenSIM assumes an OTF that is similar to the one used when simulating the SIM data.</p>
      <p>A more challenging test image than those based on DIV2K and Kodak 24 images is shown in <xref rid="g005" ref-type="fig">Fig. 5</xref>. This simulated test image is reconstructed with the same three traditional methods. OpenSIM is found to achieve the best reconstruction quality of the three with a PSNR score of 13.84 dB versus 12.56 dB for CC-SIM and 12.88 dB for FairSIM. The same image reconstructed with ML-SIM results in a PSNR score of 16.32 dB – again about 2 dB higher than OpenSIM. Two cropped regions comparing OpenSIM and ML-SIM are shown in <xref rid="g005" ref-type="fig">Fig. 5</xref>. The area in the upper right corner of the test image is particularly challenging to recover due to the single-pixel point patterns and the densely spaced vertical lines. While the points vanish in the wide-field image, these are recovered both by OpenSIM and ML-SIM. The resolution of the point sources are slightly superior in the ML-SIM reconstruction, and ML-SIM manages to recover the high-frequency information in the top line pattern very well. Overall it is also seen that the reconstruction from ML-SIM contains much less noise, which is especially evident in the zoomed region of the face. This suggests that ML-SIM is less prone to amplify noise present in the input image. We tested this further by gradually adding more Gaussian image noise to the input image, and again comparing the reconstructions from the various methods. The results of this test are shown in <xref rid="g006" ref-type="fig">Fig. 6</xref>, where it is seen clearly that ML-SIM performs best at high noise levels. As more noise is added the gap in performance is seen to increase between ML-SIM and the other models indicating that the neural network has learned to perform denoising as part of the reconstruction process. This is supported by the cropped regions on the right side of the figure, which show cleaner detail in the image when compared to the input, wide-field and OpenSIM images. OpenSIM was found to perform consistently well in this noise test, whereas FairSIM and CC-SIM struggled to reconstruct at all for higher noise levels. This is not surprising, since added noise may cause the parameter estimation to converge to incorrect optima, which can heavily corrupt the reconstruction outputs. As a result, the reconstruction outputs from FairSIM and CC-SIM were of worse quality than the wide-field reference at higher noise levels.</p>
      <fig id="g005" fig-type="figure" orientation="portrait" position="float">
        <label>Fig. 5.</label>
        <caption>
          <p>Reconstructions of a test target with OpenSIM and ML-SIM and comparison to the ground truth. OpenSIM was found to be the best performing traditional method on this test sample, both in terms of PSNR and SSIM with the other methods achieving PSNR scores of 12.56 dB (CC-SIM) and 12.88 dB (FairSIM).</p>
        </caption>
        <graphic xlink:href="boe-12-5-2720-g005"/>
      </fig>
      <fig id="g006" fig-type="figure" orientation="portrait" position="float">
        <label>Fig. 6.</label>
        <caption>
          <p>(Left) Reconstruction quality as measured by the structural similarity index, SSIM, as a function of the amount of noise added to an input image. Gaussian noise is added to every frame of the raw SIM stack. Noise is normally distributed with a standard deviation <inline-formula><mml:math id="m33"><mml:mi>η</mml:mi><mml:mo>⋅</mml:mo><mml:mi>σ</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="m34"><mml:mi>σ</mml:mi></mml:math></inline-formula> is the standard deviation of the input image. (Right) Images at low (<inline-formula><mml:math id="m35"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>) and high noise levels (<inline-formula><mml:math id="m36"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>9</mml:mn></mml:math></inline-formula>) reconstructed with OpenSIM and ML-SIM, respectively. PSNR and SSIM scores using the ground truth as reference are shown in the lower right hand corner of every image.</p>
        </caption>
        <graphic xlink:href="boe-12-5-2720-g006"/>
      </fig>
      <p>Several architectures were tested as part of this research to select the one most suitable for ML-SIM. U-Net [<xref rid="r22" ref-type="bibr">22</xref>] is a popular, versatile and easily trained network, but its performance was found to fall short of state-of-the-art single image super-resolution networks such as EDSR [<xref rid="r38" ref-type="bibr">38</xref>] and RCAN [<xref rid="r39" ref-type="bibr">39</xref>]. These super-resolution networks have been customised to be able to handle input stacks of up to 9 frames and output a single frame with no upsampling, i.e. the upsampling modules of those networks have been omitted – see Figure S2 for a depiction. In addition to testing different network architectures the number of frames of the input raw SIM stack, up to a total of 9, was also varied. In the left-hand side of <xref rid="g007" ref-type="fig">Fig. 7</xref> the convergence of test scores on a validation set during training are shown for the various architectures and input configurations considered. It is found that SIM reconstruction with subsets containing only 3 or 6 frames still performed significantly better than if the network learns to perform a more simple deconvolution operation by just training on a wide-field input. This confirms that the network learns to extract information from all 9 frames in the full stack versus a subset of it or the mean of its frames. Only using a subset of 3 frames does however cause a substantial reconstruction quality loss compared to using 6 frames, which is not surprising since the corresponding analytical reconstruction problem becomes underdetermined for fewer than 4 frames [<xref rid="r16" ref-type="bibr">16</xref>]. The RCAN model performs better than EDSR with a consistently higher PSNR score when trained on all 9 frames, while performing similarly to EDSR when trained with 3 fewer frames. Based on these results RCAN is chosen as the default architecture for ML-SIM. The fact that reconstruction with fewer than 9 frames is possible could be exploited for compressed, faster SIM imaging as done in [<xref rid="r21" ref-type="bibr">21</xref>] using a U-Net model, although this inevitably comes at a loss of quality.</p>
      <fig id="g007" fig-type="figure" orientation="portrait" position="float">
        <label>Fig. 7.</label>
        <caption>
          <p>(Left) Validation test set scores during training for different network architectures and input dimensions. The two state-of-the-art single image super-resolution architectures, RCAN and EDSR, have been modified to perform SIM reconstruction. The number of frames of the raw SIM stack, up to a total of 9, is also varied to confirm that the network learns to extract information from all 9 frames in the full stack. (Right) Computation time for reconstruction of a single raw SIM stack of 9 frames. The shown times are averages of 24 consequtive reconstructions with sample standard deviations of 0.0034, 0.13, 0.51 and 3.7 seconds for ML-SIM, FairSIM, CC-SIM and OpenSIM, respectively.</p>
        </caption>
        <graphic xlink:href="boe-12-5-2720-g007"/>
      </fig>
      <p>Regarding the computation time for each reconstruction method, we measured the average time for reconstructing the raw SIM stacks based on the Kodak 24 image dataset one by one. The timing for each method is then the mean of 24 time samples with an associated standard deviation. The timings are shown on the right-hand side of <xref rid="g007" ref-type="fig">Fig. 7</xref>. Computations were performed on a computer running Windows 10 with an Intel i5 6500 CPU, 16 GB DDR4 RAM and a Nvidia GTX 1080 Ti GPU. ML-SIM finishes a reconstruction in less than 200 ms, which is more than an order of magnitude faster than the other methods. Substantial speedups are to be expected when using neural networks due to the computations being greatly parallelisable, thus making it easy to use GPU acceleration – this was similarly found in [<xref rid="r40" ref-type="bibr">40</xref>], where a neural network was used for reconstruction of stochastic optical reconstruction microscopy images. The traditional methods for SIM reconstruction are more difficult to parallelise, partly because the numerical optimisation algorithms needed for parameter estimation tend to be iterative and sequential. This therefore provides a computational advantage of ML-SIM. At 170 ms per reconstructed image from a SIM stack of 9 frames, the reconstructed image rate is about 6 per second, corresponding to an imaging system that captures 54 frames per second, which could provide fluent, real-time, super-resolution feedback to the user during image acquisition.</p>
    </sec>
    <sec id="sec3-4">
      <label>3.4</label>
      <title>Web app, desktop app and source code</title>
      <p>The source code for training ML-SIM and applying the model for reconstruction is available in a public repository on GitHub, <ext-link ext-link-type="uri" xlink:href="https://github.com/charlesnchr/ML-SIM">https://github.com/charlesnchr/ML-SIM</ext-link>, and figshare (<ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/software/ML-SIM/13702195">Code 1</ext-link>) [<xref rid="r41" ref-type="bibr">41</xref>]. This repository includes source code for generating the training data by simulating the SIM imaging process with parameters that can be easily adapted to reflect specific SIM setups (e.g. by changing stripe orientations, number of frames, etc.). The repository also holds source code for a desktop program with pre-built installers for Windows, macOS and Linux. The program makes it easy to use ML-SIM and perform batch processing via a graphical user interface. During installation required dependencies such as Python, Pytorch and pre-trained ML-SIM models are automatically fetched. If the pre-trained models perform suboptimally, it is easy to train a new model that is more specific to a given SIM setup and set the program to use this custom model for reconstruction. The program includes a plugin for <inline-formula><mml:math id="m37"><mml:mi>μ</mml:mi></mml:math></inline-formula> Manager [<xref rid="r42" ref-type="bibr">42</xref>] that enables a real-time live-view of ML-SIM reconstructed output during acquisition in many imaging systems thanks to the wide support of camera drivers in <inline-formula><mml:math id="m38"><mml:mi>μ</mml:mi></mml:math></inline-formula> Manager. See <inline-supplementary-material xlink:href="boe-12-5-2720-s001.pdf" xlink:type="simple" id="d24e969" content-type="local-data">Supplement 1</inline-supplementary-material> for more details. Furthermore, we have created a web app accessible via <ext-link ext-link-type="uri" xlink:href="http://ML-SIM.github.io">http://ML-SIM.github.io</ext-link> with a browser-based online implementation of ML-SIM that is ready for quick testing using a pre-trained model and does not require installation of any software.</p>
    </sec>
  </sec>
  <sec sec-type="discussions" id="sec4">
    <label>4.</label>
    <title>Discussion</title>
    <p>We demonstrate and validate a SIM reconstruction method, ML-SIM, which takes advantage of transfer learning by training a model in an auxiliary domain consisting of simulated images and generalises to the target task of reconstructing experimental SIM images with no fine-tuning or retraining necessary. The training data was generated by simulating raw SIM image data from images obtained from common image repositories, serving as ground truths. ML-SIM successfully reconstructed artificial test targets that were of a completely different nature than the diverse images used to generate the training datasets. More importantly, it successfully reconstructed real data obtained by two distinct experimental SIM implementations. We compared the performance of ML-SIM to widely used reconstruction methods, OpenSIM [<xref rid="r15" ref-type="bibr">15</xref>], FairSIM [<xref rid="r14" ref-type="bibr">14</xref>], and CC-SIM [<xref rid="r28" ref-type="bibr">28</xref>]. In all cases, reconstruction outputs from ML-SIM contained less noise and fewer artefacts, while achieving similar resolution improvements. Through a randomisation of phase shifts in the simulated training data, it was also possible to successfully reconstruct images that could not be processed successfully with two of the traditional reconstruction methods. ML-SIM shows robustness to unpredictable variations in the SIM imaging parameters and deviations from equidistant phase shifts. Similarly, ML-SIM reconstructed images that were strongly degraded by noise even beyond the point where the other methods failed.</p>
    <p>A central advantage of the transfer learning approach of the ML-SIM method is that the simulated data that constitute the auxiliary domain can be made arbitrarily diverse by randomisation of optical parameters, enabling the model to become highly generalised for the target task. Furthermore, the simulation can also be optimised to a specific system by changing relevant optical parameters. In principle, this makes the method applicable to any SIM setup regardless of its configuration. For instance, a SIM setup with another illumination pattern configuration, e.g. 5 orientations and 5 phase shifts (5x5 stacks), is trivial to support with ML-SIM by changing just two parameters in the pipeline. General, pretrained models for SIM microscopes with configurations for 3x3, 3x5 and 5x5 stacks are provided at <ext-link ext-link-type="uri" xlink:href="http://ML-SIM.github.io">http://ML-SIM.github.io</ext-link> along with source code and software to use them.</p>
    <p>A future direction could be to fine-tune the training data by incorporating a more sophisticated image formation model. This image formation model might also take certain optical aberrations into account. Currently, out-of-focus light from above and below the focal plane is not simulated in the training data. As with the other reconstruction methods, this can result in artefacts in regions of the sample with dense out-of-focus structures. Given that the spatial frequency information required to remove this background is available in SIM, it is possible that an updated ML-SIM network could be constructed that incorporates an efficient means for background rejection [<xref rid="r43" ref-type="bibr">43</xref>,<xref rid="r44" ref-type="bibr">44</xref>].</p>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>The authors thank Jerome Boulanger from the Laboratory of Molecular Biology in Cambridge, UK, for general guidance on reconstruction for SIM. The authors also thank group member Lisa Hecker from the Laser Analytics Group of the University of Cambridge for providing experimental data from the SLM and interferometric SIM microscopes described in this study.</p>
  </ack>
  <sec id="sec5" sec-type="funding">
    <title>Funding</title>
    <p><funding-source rid="sp1">Engineering and Physical Sciences Research Council<named-content content-type="doi">10.13039/501100000266</named-content></funding-source> (<award-id rid="sp1">L015889</award-id>); <funding-source rid="sp2">Wellcome Trust<named-content content-type="doi">10.13039/100010269</named-content></funding-source> (<award-id rid="sp2">089703</award-id>); <funding-source rid="sp3">Medical Research Council<named-content content-type="doi">10.13039/501100000265</named-content></funding-source> (<award-id rid="sp3">K015850</award-id>).</p>
  </sec>
  <sec id="sec6" sec-type="COI-statement">
    <title>Disclosures</title>
    <p>The authors declare no conflicts of interest.</p>
  </sec>
  <sec id="sec7" sec-type="supplementary-material">
    <title>Supplemental document</title>
    <p>See <inline-supplementary-material xlink:href="boe-12-5-2720-s001.pdf" xlink:type="simple" id="d24e1040" content-type="local-data">Supplement 1</inline-supplementary-material> for supporting content.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="r1">
      <label>1</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheppard</surname><given-names>C. J. R.</given-names></name></person-group>, “<article-title>Super-resolution in confocal imaging</article-title>,” <source>Optik</source>
<volume>80</volume>, <fpage>53</fpage>–<lpage>54</lpage> (<year>1988</year>).</mixed-citation>
    </ref>
    <ref id="r2">
      <label>2</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Heintzmann</surname><given-names>R.</given-names></name><name><surname>Cremer</surname><given-names>C. G.</given-names></name></person-group>, “<article-title>Laterally modulated excitation microscopy: improvement of resolution by using a diffraction grating</article-title>,” in <source><italic>Optical Biopsies and Microscopic Techniques III</italic></source>, vol. <volume>3568</volume> (<publisher-name>SPIE</publisher-name>, <year>1999</year>), pp. <fpage>185</fpage>–<lpage>196</lpage>.</mixed-citation>
    </ref>
    <ref id="r3">
      <label>3</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gustafsson</surname><given-names>M. G.</given-names></name></person-group>, “<article-title>Surpassing the lateral resolution limit by a factor of two using structured illumination microscopy</article-title>,” <source>J. Microsc.</source>
<volume>198</volume>(<issue>2</issue>), <fpage>82</fpage>–<lpage>87</lpage> (<year>2000</year>).<pub-id pub-id-type="doi">10.1046/j.1365-2818.2000.00710.x</pub-id><pub-id pub-id-type="pmid">10810003</pub-id></mixed-citation>
    </ref>
    <ref id="r4">
      <label>4</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gustafsson</surname><given-names>M. G. L.</given-names></name><name><surname>Shao</surname><given-names>L.</given-names></name><name><surname>Carlton</surname><given-names>P. M.</given-names></name><name><surname>Wang</surname><given-names>C. J. R.</given-names></name><name><surname>Golubovskaya</surname><given-names>I. N.</given-names></name><name><surname>Cande</surname><given-names>W. Z.</given-names></name><name><surname>Agard</surname><given-names>D. A.</given-names></name><name><surname>Sedat</surname><given-names>J. W.</given-names></name></person-group>, “<article-title>Three-dimensional resolution doubling in wide-field fluorescence microscopy by structured illumination</article-title>,” <source>Biophys. J.</source>
<volume>94</volume>(<issue>12</issue>), <fpage>4957</fpage>–<lpage>4970</lpage> (<year>2008</year>).<pub-id pub-id-type="doi">10.1529/biophysj.107.120345</pub-id><pub-id pub-id-type="pmid">18326650</pub-id></mixed-citation>
    </ref>
    <ref id="r5">
      <label>5</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schermelleh</surname><given-names>L.</given-names></name><name><surname>Heintzmann</surname><given-names>R.</given-names></name><name><surname>Leonhardt</surname><given-names>H.</given-names></name></person-group>, “<article-title>A guide to super-resolution fluorescence microscopy</article-title>,” <source>J. Cell Biol.</source>
<volume>190</volume>(<issue>2</issue>), <fpage>165</fpage>–<lpage>175</lpage> (<year>2010</year>).<pub-id pub-id-type="doi">10.1083/jcb.201002018</pub-id><pub-id pub-id-type="pmid">20643879</pub-id></mixed-citation>
    </ref>
    <ref id="r6">
      <label>6</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moerner</surname><given-names>W. E.</given-names></name><name><surname>Kador</surname><given-names>L.</given-names></name></person-group>, “<article-title>Optical detection and spectroscopy of single molecules in a solid</article-title>,” <source>Phys. Rev. Lett.</source>
<volume>62</volume>(<issue>21</issue>), <fpage>2535</fpage>–<lpage>2538</lpage> (<year>1989</year>).<pub-id pub-id-type="doi">10.1103/PhysRevLett.62.2535</pub-id><pub-id pub-id-type="pmid">10040013</pub-id></mixed-citation>
    </ref>
    <ref id="r7">
      <label>7</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Betzig</surname><given-names>E.</given-names></name></person-group>, “<article-title>Proposed method for molecular optical imaging</article-title>,” <source>Opt. Lett.</source>
<volume>20</volume>(<issue>3</issue>), <fpage>237</fpage> (<year>1995</year>).<pub-id pub-id-type="doi">10.1364/OL.20.000237</pub-id><pub-id pub-id-type="pmid">19859146</pub-id></mixed-citation>
    </ref>
    <ref id="r8">
      <label>8</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hell</surname><given-names>S. W.</given-names></name><name><surname>Wichmann</surname><given-names>J.</given-names></name></person-group>, “<article-title>Breaking the diffraction resolution limit by stimulated emission: stimulated-emission-depletion fluorescence microscopy</article-title>,” <source>Opt. Lett.</source>
<volume>19</volume>(<issue>11</issue>), <fpage>780</fpage> (<year>1994</year>).<pub-id pub-id-type="doi">10.1364/OL.19.000780</pub-id><pub-id pub-id-type="pmid">19844443</pub-id></mixed-citation>
    </ref>
    <ref id="r9">
      <label>9</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>D.</given-names></name><name><surname>Shao</surname><given-names>L.</given-names></name><name><surname>Chen</surname><given-names>B.-C.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>M.</given-names></name><name><surname>Moses</surname><given-names>B.</given-names></name><name><surname>Milkie</surname><given-names>D. E.</given-names></name><name><surname>Beach</surname><given-names>J. R.</given-names></name><name><surname>Hammer</surname><given-names>J. A.</given-names></name><name><surname>Pasham</surname><given-names>M.</given-names></name></person-group>, “<article-title>Extended-resolution structured illumination imaging of endocytic and cytoskeletal dynamics</article-title>,” <source>Science</source>
<volume>349</volume>(<issue>6251</issue>), <fpage>aab3500</fpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1126/science.aab3500</pub-id><pub-id pub-id-type="pmid">26315442</pub-id></mixed-citation>
    </ref>
    <ref id="r10">
      <label>10</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rego</surname><given-names>E. H.</given-names></name><name><surname>Shao</surname><given-names>L.</given-names></name><name><surname>Macklin</surname><given-names>J. J.</given-names></name><name><surname>Winoto</surname><given-names>L.</given-names></name><name><surname>Johansson</surname><given-names>G. A.</given-names></name><name><surname>Kamps-Hughes</surname><given-names>N.</given-names></name><name><surname>Davidson</surname><given-names>M. W.</given-names></name><name><surname>Gustafsson</surname><given-names>M. G.</given-names></name></person-group>, “<article-title>Nonlinear structured-illumination microscopy with a photoswitchable protein reveals cellular structures at 50-nm resolution</article-title>,” <source>Proc. Natl. Acad. Sci.</source>
<volume>109</volume>(<issue>3</issue>), <fpage>E135</fpage>–<lpage>E143</lpage> (<year>2012</year>).<pub-id pub-id-type="doi">10.1073/pnas.1107547108</pub-id><pub-id pub-id-type="pmid">22160683</pub-id></mixed-citation>
    </ref>
    <ref id="r11">
      <label>11</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ströhl</surname><given-names>F.</given-names></name><name><surname>Kaminski</surname><given-names>C. F.</given-names></name></person-group>, “<article-title>Frontiers in structured illumination microscopy</article-title>,” <source>Optica</source>
<volume>3</volume>(<issue>6</issue>), <fpage>667</fpage> (<year>2016</year>).<pub-id pub-id-type="doi">10.1364/OPTICA.3.000667</pub-id></mixed-citation>
    </ref>
    <ref id="r12">
      <label>12</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winter</surname><given-names>P. W.</given-names></name><name><surname>Shroff</surname><given-names>H.</given-names></name></person-group>, “<article-title>Faster fluorescence microscopy: advances in high speed biological imaging</article-title>,” <source>Curr. Opin. Chem. Biol.</source>
<volume>20</volume>, <fpage>46</fpage>–<lpage>53</lpage> (<year>2014</year>).<pub-id pub-id-type="doi">10.1016/j.cbpa.2014.04.008</pub-id><pub-id pub-id-type="pmid">24815857</pub-id></mixed-citation>
    </ref>
    <ref id="r13">
      <label>13</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ball</surname><given-names>G.</given-names></name><name><surname>Demmerle</surname><given-names>J.</given-names></name><name><surname>Kaufmann</surname><given-names>R.</given-names></name><name><surname>Davis</surname><given-names>I.</given-names></name><name><surname>Dobbie</surname><given-names>I. M.</given-names></name><name><surname>Schermelleh</surname><given-names>L.</given-names></name></person-group>, “<article-title>SIMcheck: a toolbox for successful super-resolution structured illumination microscopy</article-title>,” <source>Sci. Rep.</source>
<volume>5</volume>(<issue>1</issue>), <fpage>15915</fpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1038/srep15915</pub-id><pub-id pub-id-type="pmid">26525406</pub-id></mixed-citation>
    </ref>
    <ref id="r14">
      <label>14</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müller</surname><given-names>M.</given-names></name><name><surname>Mönkemöller</surname><given-names>V.</given-names></name><name><surname>Hennig</surname><given-names>S.</given-names></name><name><surname>Hübner</surname><given-names>W.</given-names></name><name><surname>Huser</surname><given-names>T.</given-names></name></person-group>, “<article-title>Open-source image reconstruction of super-resolution structured illumination microscopy data in ImageJ</article-title>,” <source>Nat. Commun.</source>
<volume>7</volume>(<issue>1</issue>), <fpage>10980</fpage> (<year>2016</year>).<pub-id pub-id-type="doi">10.1038/ncomms10980</pub-id><pub-id pub-id-type="pmid">26996201</pub-id></mixed-citation>
    </ref>
    <ref id="r15">
      <label>15</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lal</surname><given-names>A.</given-names></name><name><surname>Shan</surname><given-names>C.</given-names></name><name><surname>Xi</surname><given-names>P.</given-names></name></person-group>, “<article-title>Structured illumination microscopy image reconstruction algorithm</article-title>,” <source>IEEE J. Select. Topics Quantum Electron.</source>
<volume>22</volume>(<issue>4</issue>), <fpage>50</fpage>–<lpage>63</lpage> (<year>2016</year>).<pub-id pub-id-type="doi">10.1109/JSTQE.2016.2521542</pub-id></mixed-citation>
    </ref>
    <ref id="r16">
      <label>16</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ströhl</surname><given-names>F.</given-names></name><name><surname>Kaminski</surname><given-names>C. F.</given-names></name></person-group>, “<article-title>Speed limits of structured illumination microscopy</article-title>,” <source>Opt. Lett.</source>
<volume>42</volume>(<issue>13</issue>), <fpage>2511</fpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1364/OL.42.002511</pub-id><pub-id pub-id-type="pmid">28957272</pub-id></mixed-citation>
    </ref>
    <ref id="r17">
      <label>17</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holcman</surname><given-names>D.</given-names></name><name><surname>Parutto</surname><given-names>P.</given-names></name><name><surname>Chambers</surname><given-names>J. E.</given-names></name><name><surname>Fantham</surname><given-names>M.</given-names></name><name><surname>Young</surname><given-names>L. J.</given-names></name><name><surname>Marciniak</surname><given-names>S. J.</given-names></name><name><surname>Kaminski</surname><given-names>C. F.</given-names></name><name><surname>Ron</surname><given-names>D.</given-names></name><name><surname>Avezov</surname><given-names>E.</given-names></name></person-group>, “<article-title>Single particle trajectories reveal active endoplasmic reticulum luminal flow</article-title>,” <source>Nat. Cell Biol.</source>
<volume>20</volume>(<issue>10</issue>), <fpage>1118</fpage>–<lpage>1125</lpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1038/s41556-018-0192-2</pub-id><pub-id pub-id-type="pmid">30224760</pub-id></mixed-citation>
    </ref>
    <ref id="r18">
      <label>18</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Planchon</surname><given-names>T. A.</given-names></name><name><surname>Gao</surname><given-names>L.</given-names></name><name><surname>Milkie</surname><given-names>D. E.</given-names></name><name><surname>Davidson</surname><given-names>M. W.</given-names></name><name><surname>Galbraith</surname><given-names>J. A.</given-names></name><name><surname>Galbraith</surname><given-names>C. G.</given-names></name><name><surname>Betzig</surname><given-names>E.</given-names></name></person-group>, “<article-title>Rapid three-dimensional isotropic imaging of living cells using Bessel beam plane illumination</article-title>,” <source>Nat. Methods</source>
<volume>8</volume>(<issue>5</issue>), <fpage>417</fpage>–<lpage>423</lpage> (<year>2011</year>).<pub-id pub-id-type="doi">10.1038/nmeth.1586</pub-id><pub-id pub-id-type="pmid">21378978</pub-id></mixed-citation>
    </ref>
    <ref id="r19">
      <label>19</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>S. J.</given-names></name><name><surname>Yang</surname><given-names>Q.</given-names></name></person-group>, “<article-title>A survey on transfer learning</article-title>,” <source>IEEE Trans. Knowl. Data Eng.</source>
<volume>22</volume>(<issue>10</issue>), <fpage>1345</fpage>–<lpage>1359</lpage> (<year>2010</year>).<pub-id pub-id-type="doi">10.1109/TKDE.2009.191</pub-id></mixed-citation>
    </ref>
    <ref id="r20">
      <label>20</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>J.</given-names></name><name><surname>Behbood</surname><given-names>V.</given-names></name><name><surname>Hao</surname><given-names>P.</given-names></name><name><surname>Zuo</surname><given-names>H.</given-names></name><name><surname>Xue</surname><given-names>S.</given-names></name><name><surname>Zhang</surname><given-names>G.</given-names></name></person-group>, “<article-title>Transfer learning using computational intelligence: A survey</article-title>,” <source>Knowledge-Based Systems</source>
<volume>80</volume>, <fpage>14</fpage>–<lpage>23</lpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1016/j.knosys.2015.01.010</pub-id></mixed-citation>
    </ref>
    <ref id="r21">
      <label>21</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>L.</given-names></name><name><surname>Liu</surname><given-names>B.</given-names></name><name><surname>Zhao</surname><given-names>F.</given-names></name><name><surname>Hahn</surname><given-names>S.</given-names></name><name><surname>Dong</surname><given-names>B.</given-names></name><name><surname>Song</surname><given-names>R.</given-names></name><name><surname>Elston</surname><given-names>T. C.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><name><surname>Hahn</surname><given-names>K. M.</given-names></name></person-group>, “<article-title>Deep learning enables structured illumination microscopy with low light levels and enhanced speed</article-title>,” <source>Nat. Commun.</source>
<volume>11</volume>(<issue>1</issue>), <fpage>1934</fpage> (<year>2020</year>).<pub-id pub-id-type="doi">10.1038/s41467-020-15784-x</pub-id><pub-id pub-id-type="pmid">32321916</pub-id></mixed-citation>
    </ref>
    <ref id="r22">
      <label>22</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O.</given-names></name><name><surname>Fischer</surname><given-names>P.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name></person-group>, “<article-title>U-net: Convolutional networks for biomedical image segmentation</article-title>,” in <conf-name>International Conference on Medical image computing and computer-assisted intervention</conf-name> (<publisher-name>Springer</publisher-name>, <year>2015</year>), pp. <fpage>234</fpage>–<lpage>241</lpage>.</mixed-citation>
    </ref>
    <ref id="r23">
      <label>23</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group>, “<article-title>Deep residual learning for image recognition</article-title>,” <conf-name>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</conf-name>
<bold>2016-Decem</bold>, <fpage>770</fpage>–<lpage>778</lpage> (<year>2016</year>).</mixed-citation>
    </ref>
    <ref id="r24">
      <label>24</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Ba</surname><given-names>J. L.</given-names></name></person-group>, “<article-title>Adam: A method for stochastic optimization</article-title>,” Tech. rep. (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="r25">
      <label>25</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Liu</surname><given-names>Q.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Han</surname><given-names>Y.</given-names></name><name><surname>Kuang</surname><given-names>C.</given-names></name><name><surname>Xu</surname><given-names>L.</given-names></name><name><surname>Yang</surname><given-names>H.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name></person-group>, “<article-title>Three-dimensional super-resolution imaging of live whole cells using galvanometer-based structured illumination microscopy</article-title>,” <source>Opt. Express</source>
<volume>27</volume>(<issue>5</issue>), <fpage>7237</fpage>–<lpage>7248</lpage> (<year>2019</year>).<pub-id pub-id-type="doi">10.1364/OE.27.007237</pub-id><pub-id pub-id-type="pmid">30876291</pub-id></mixed-citation>
    </ref>
    <ref id="r26">
      <label>26</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Bovik</surname><given-names>A. C.</given-names></name><name><surname>Sheikh</surname><given-names>H. R.</given-names></name><name><surname>Simoncelli</surname><given-names>E. P.</given-names></name></person-group>, “<article-title>Image quality assessment: from error visibility to structural similarity</article-title>,” <source>IEEE Trans. on Image Process.</source>
<volume>13</volume>(<issue>4</issue>), <fpage>600</fpage>–<lpage>612</lpage> (<year>2004</year>).<pub-id pub-id-type="doi">10.1109/TIP.2003.819861</pub-id></mixed-citation>
    </ref>
    <ref id="r27">
      <label>27</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiolka</surname><given-names>R.</given-names></name><name><surname>Beck</surname><given-names>M.</given-names></name><name><surname>Stemmer</surname><given-names>A.</given-names></name></person-group>, “<article-title>Structured illumination in total internal reflection fluorescence microscopy using a spatial light modulator</article-title>,” <source>Opt. Lett.</source>
<volume>33</volume>(<issue>14</issue>), <fpage>1629</fpage>–<lpage>1631</lpage> (<year>2008</year>).<pub-id pub-id-type="doi">10.1364/OL.33.001629</pub-id><pub-id pub-id-type="pmid">18628820</pub-id></mixed-citation>
    </ref>
    <ref id="r28">
      <label>28</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wicker</surname><given-names>K.</given-names></name><name><surname>Mandula</surname><given-names>O.</given-names></name><name><surname>Best</surname><given-names>G.</given-names></name><name><surname>Fiolka</surname><given-names>R.</given-names></name><name><surname>Heintzmann</surname><given-names>R.</given-names></name></person-group>, “<article-title>Phase optimisation for structured illumination microscopy</article-title>,” <source>Opt. Express</source>
<volume>21</volume>(<issue>2</issue>), <fpage>2032</fpage> (<year>2013</year>).<pub-id pub-id-type="doi">10.1364/OE.21.002032</pub-id><pub-id pub-id-type="pmid">23389185</pub-id></mixed-citation>
    </ref>
    <ref id="r29">
      <label>29</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauser</surname><given-names>M.</given-names></name><name><surname>Wojcik</surname><given-names>M.</given-names></name><name><surname>Kim</surname><given-names>D.</given-names></name><name><surname>Mahmoudi</surname><given-names>M.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name><name><surname>Xu</surname><given-names>K.</given-names></name></person-group>, “<article-title>Correlative super-resolution microscopy: new dimensions and new opportunities</article-title>,” <source>Chem. Rev.</source>
<volume>117</volume>(<issue>11</issue>), <fpage>7428</fpage>–<lpage>7456</lpage> (<year>2017</year>).<pub-id pub-id-type="doi">10.1021/acs.chemrev.6b00604</pub-id><pub-id pub-id-type="pmid">28045508</pub-id></mixed-citation>
    </ref>
    <ref id="r30">
      <label>30</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Agustsson</surname><given-names>E.</given-names></name><name><surname>Timofte</surname><given-names>R.</given-names></name></person-group>, “<article-title>NTIRE 2017 challenge on single image super-resolution: dataset and study</article-title>,” Tech. rep. (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="r31">
      <label>31</label>
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ledig</surname><given-names>C.</given-names></name><name><surname>Theis</surname><given-names>L.</given-names></name><name><surname>Huszár</surname><given-names>F.</given-names></name><name><surname>Caballero</surname><given-names>J.</given-names></name><name><surname>Cunningham</surname><given-names>A.</given-names></name><name><surname>Acosta</surname><given-names>A.</given-names></name><name><surname>Aitken</surname><given-names>A.</given-names></name><name><surname>Tejani</surname><given-names>A.</given-names></name><name><surname>Totz</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Shi</surname><given-names>W.</given-names></name></person-group>, “<article-title>Photo-realistic single image super-resolution using a generative adversarial network</article-title>,” <conf-name>Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, CVPR 2017, <fpage>105</fpage>–<lpage>114</lpage> (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="r32">
      <label>32</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Yu</surname><given-names>K.</given-names></name><name><surname>Wu</surname><given-names>S.</given-names></name><name><surname>Gu</surname><given-names>J.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Dong</surname><given-names>C.</given-names></name><name><surname>Loy</surname><given-names>C. C.</given-names></name><name><surname>Qiao</surname><given-names>Y.</given-names></name><name><surname>Tang</surname><given-names>X.</given-names></name></person-group>, “<article-title>ESRGAN: enhanced super-resolution generative adversarial networks</article-title>,” Tech. rep. (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="r33">
      <label>33</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Blau</surname><given-names>Y.</given-names></name><name><surname>Mechrez</surname><given-names>R.</given-names></name><name><surname>Timofte</surname><given-names>R.</given-names></name><name><surname>Michaeli</surname><given-names>T.</given-names></name><name><surname>Zelnik-Manor</surname><given-names>L.</given-names></name></person-group>, “<article-title>2018 PIRM challenge on perceptual image super-resolution</article-title>,” Tech. rep. (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="r34">
      <label>34</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Vasu</surname><given-names>S.</given-names></name><name><surname>Madam</surname><given-names>N. T.</given-names></name><name><surname>Rajagopalan</surname><given-names>A. N.</given-names></name></person-group>, “<article-title>Analyzing perception-distortion tradeoff using enhanced perceptual super-resolution network</article-title>,” Tech. rep. (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="r35">
      <label>35</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>L. J.</given-names></name><name><surname>Ströhl</surname><given-names>F.</given-names></name><name><surname>Kaminski</surname><given-names>C. F.</given-names></name></person-group>, “<article-title>A guide to structured illumination TIRF microscopy at high speed with multiple colors</article-title>,” <source>J. Vis. Exp.</source>
<volume>111</volume>, <fpage>53988</fpage> (<year>2016</year>)<pub-id pub-id-type="doi">10.3791/53988</pub-id>.</mixed-citation>
    </ref>
    <ref id="r36">
      <label>36</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>R.</given-names></name><name><surname>Chen</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Zhu</surname><given-names>D.</given-names></name><name><surname>Kuang</surname><given-names>C.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><name><surname>Liu</surname><given-names>X.</given-names></name></person-group>, “<article-title>Inverse matrix based phase estimation algorithm for structured illumination microscopy</article-title>,” <source>Biomed. Opt. Express</source>
<volume>9</volume>(<issue>10</issue>), <fpage>5037</fpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1364/BOE.9.005037</pub-id><pub-id pub-id-type="pmid">30319920</pub-id></mixed-citation>
    </ref>
    <ref id="r37">
      <label>37</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Lehtinen</surname><given-names>J.</given-names></name><name><surname>Munkberg</surname><given-names>J.</given-names></name><name><surname>Hasselgren</surname><given-names>J.</given-names></name><name><surname>Laine</surname><given-names>S.</given-names></name><name><surname>Karras</surname><given-names>T.</given-names></name><name><surname>Aittala</surname><given-names>M.</given-names></name><name><surname>Aila</surname><given-names>T.</given-names></name></person-group>, “<article-title>Noise2Noise: Learning image restoration without clean data</article-title>,” arXiv:1803.04189 (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="r38">
      <label>38</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>B.</given-names></name><name><surname>Son</surname><given-names>S.</given-names></name><name><surname>Kim</surname><given-names>H.</given-names></name><name><surname>Nah</surname><given-names>S.</given-names></name><name><surname>Lee</surname><given-names>K. M.</given-names></name></person-group>, “<article-title>Enhanced deep residual networks for single image super-resolution</article-title>,” Tech. rep. (<year>2017</year>).</mixed-citation>
    </ref>
    <ref id="r39">
      <label>39</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Zhong</surname><given-names>B.</given-names></name><name><surname>Fu</surname><given-names>Y.</given-names></name></person-group>, “<article-title>Image super-resolution using very deep residual channel attention networks</article-title>,” Tech. rep. (<year>2018</year>).</mixed-citation>
    </ref>
    <ref id="r40">
      <label>40</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nehme</surname><given-names>E.</given-names></name><name><surname>Weiss</surname><given-names>L. E.</given-names></name><name><surname>Michaeli</surname><given-names>T.</given-names></name><name><surname>Shechtman</surname><given-names>Y.</given-names></name></person-group>, “<article-title>Deep-STORM: super-resolution single-molecule microscopy by deep learning</article-title>,” <source>Optica</source>
<volume>5</volume>(<issue>4</issue>), <fpage>458</fpage> (<year>2018</year>).<pub-id pub-id-type="doi">10.1364/OPTICA.5.000458</pub-id></mixed-citation>
    </ref>
    <ref id="r41">
      <label>41</label>
      <mixed-citation publication-type="code"><person-group person-group-type="author"><name><surname>Christensen</surname><given-names>C. N.</given-names></name></person-group>, “<article-title>ML-SIM</article-title>,” <source>figshare</source> (<year>2021</year>). <ext-link ext-link-type="uri" xlink:href="https://figshare.com/articles/software/ML-SIM/13702195">https://figshare.com/articles/software/ML-SIM/13702195</ext-link>.</mixed-citation>
    </ref>
    <ref id="r42">
      <label>42</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelstein</surname><given-names>A.</given-names></name><name><surname>Amodaj</surname><given-names>N.</given-names></name><name><surname>Hoover</surname><given-names>K.</given-names></name><name><surname>Vale</surname><given-names>R.</given-names></name><name><surname>Stuurman</surname><given-names>N.</given-names></name></person-group>, “<article-title>Computer control of microscopes using <italic>μ</italic>manager</article-title>,” <source>Curr. Protoc. Mol. Biol.</source>
<volume>92</volume>(<issue>1</issue>), <fpage>14</fpage>–<lpage>20</lpage> (<year>2010</year>).<pub-id pub-id-type="doi">10.1002/0471142727.mb1420s92</pub-id></mixed-citation>
    </ref>
    <ref id="r43">
      <label>43</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qian</surname><given-names>J.</given-names></name><name><surname>Lei</surname><given-names>M.</given-names></name><name><surname>Dan</surname><given-names>D.</given-names></name><name><surname>Yao</surname><given-names>B.</given-names></name><name><surname>Zhou</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>Y.</given-names></name><name><surname>Yan</surname><given-names>S.</given-names></name><name><surname>Min</surname><given-names>J.</given-names></name><name><surname>Yu</surname><given-names>X.</given-names></name></person-group>, “<article-title>Full-color structured illumination optical sectioning microscopy</article-title>,” <source>Sci. Rep.</source>
<volume>5</volume>(<issue>1</issue>), <fpage>14513</fpage> (<year>2015</year>).<pub-id pub-id-type="doi">10.1038/srep14513</pub-id><pub-id pub-id-type="pmid">26415516</pub-id></mixed-citation>
    </ref>
    <ref id="r44">
      <label>44</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neil</surname><given-names>M.</given-names></name><name><surname>Juškaitis</surname><given-names>R.</given-names></name><name><surname>Wilson</surname><given-names>T.</given-names></name></person-group>, “<article-title>Real time 3D fluorescence microscopy by two beam interference illumination</article-title>,” <source>Opt. Commun.</source>
<volume>153</volume>(<issue>1-3</issue>), <fpage>1</fpage>–<lpage>4</lpage> (<year>1998</year>).<pub-id pub-id-type="doi">10.1016/S0030-4018(98)00210-7</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
