<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_CSBJ1570 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEga1 jpg ?>
<?FILEmmc1 docx ?>
<?FILEsi1 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Comput Struct Biotechnol J</journal-id>
    <journal-id journal-id-type="iso-abbrev">Comput Struct Biotechnol J</journal-id>
    <journal-title-group>
      <journal-title>Computational and Structural Biotechnology Journal</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2001-0370</issn>
    <publisher>
      <publisher-name>Research Network of Computational and Structural Biotechnology</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9207291</article-id>
    <article-id pub-id-type="pii">S2001-0370(22)00216-1</article-id>
    <article-id pub-id-type="doi">10.1016/j.csbj.2022.06.002</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Identification of plant vacuole proteins by exploiting deep representation learning features</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au005">
        <name>
          <surname>Jiao</surname>
          <given-names>Shihu</given-names>
        </name>
        <xref rid="af005" ref-type="aff">a</xref>
      </contrib>
      <contrib contrib-type="author" id="au010">
        <name>
          <surname>Zou</surname>
          <given-names>Quan</given-names>
        </name>
        <email>zouquan@nclab.net</email>
        <xref rid="af005" ref-type="aff">a</xref>
        <xref rid="af010" ref-type="aff">b</xref>
        <xref rid="af015" ref-type="aff">c</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <aff id="af005"><label>a</label>Yangtze Delta Region Institute (Quzhou), University of Electronic Science and Technology of China, Quzhou, China</aff>
      <aff id="af010"><label>b</label>State Key Laboratory of Tree Genetics and Breeding, Northeast Forestry University, Harbin, China</aff>
      <aff id="af015"><label>c</label>Institute of Fundamental and Frontier Sciences, University of Electronic Science and Technology of China, Chengdu, China</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author at: Yangtze Delta Region Institute (Quzhou), University of Electronic Science and Technology of China, Quzhou, China. <email>zouquan@nclab.net</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>08</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>08</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <volume>20</volume>
    <fpage>2921</fpage>
    <lpage>2927</lpage>
    <history>
      <date date-type="received">
        <day>18</day>
        <month>2</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>30</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>1</day>
        <month>6</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 The Authors</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract abstract-type="graphical" id="ab005">
      <title>Graphical abstract</title>
      <fig id="f0020" position="anchor">
        <graphic xlink:href="ga1"/>
      </fig>
    </abstract>
    <abstract id="ab010">
      <p>Plant vacuoles are the most important organelles for plant growth, development, and defense, and they play an important role in many types of stress responses. An important function of vacuole proteins is the transport of various classes of amino acids, ions, sugars, and other molecules. Accurate identification of vacuole proteins is crucial for revealing their biological functions. Several automatic and rapid computational tools have been proposed for the subcellular localization of proteins. Regrettably, they are not specific for the identification of plant vacuole proteins. To the best of our knowledge, there is only one computational software specifically trained for plant vacuolar proteins. Although its accuracy is acceptable, the prediction performance and stability of this method in practical applications can still be improved. Hence, in this study, a new predictor named iPVP-DRLF was developed to identify plant vacuole proteins specifically and effectively. This prediction software is designed using the light gradient boosting machine (LGBM) algorithm and hybrid features composed of classic sequence features and deep representation learning features. iPVP-DRLF achieved fivefold cross-validation and independent test accuracy values of 88.25 % and 87.16 %, respectively, both outperforming previous state-of-the-art predictors. Moreover, the blind dataset test results also showed that the performance of iPVP-DRLF was significantly better than the existing tools. The results of comparative experiments confirmed that deep representation learning features have an advantage over other classic sequence features in the identification of plant vacuole proteins. We believe that iPVP-DRLF would serve as an effective computational technique for plant vacuole protein prediction and facilitate related future research. The online server is freely accessible at <ext-link ext-link-type="uri" xlink:href="https://lab.malab.cn/%7eacy/iPVP-DRLF" id="ir005">https://lab.malab.cn/~acy/iPVP-DRLF</ext-link>. In addition, the source code and datasets are also accessible at <ext-link ext-link-type="uri" xlink:href="https://github.com/jiaoshihu/iPVP-DRLF" id="ir010">https://github.com/jiaoshihu/iPVP-DRLF</ext-link>.</p>
    </abstract>
    <kwd-group id="kg005">
      <title>Keywords</title>
      <kwd>Vacuole proteins</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Deep representation learning</kwd>
      <kwd>Feature selection</kwd>
      <kwd>Light gradient boosting machine</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="s0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0005">Vacuoles are the largest membrane-bound organelles (up to 90 % of plant cells) and play essential roles in plant growth and development. Vacuoles have important cellular functions, such as storage of inorganic ions and metabolites, protein degradation, detoxification, and regulation of cytoplasmic ion homeostasis [<xref rid="b0005" ref-type="bibr">[1]</xref>, <xref rid="b0010" ref-type="bibr">[2]</xref>, <xref rid="b0015" ref-type="bibr">[3]</xref>]. During seed development, a large number of protein storage vacuoles in the tissue are nutrient reservoirs for embryo development and seed germination. There are many peripheral proteins and transmembrane proteins that are related to vacuole activity, such as channel proteins, proton pumps, transport proteins, and various solution carrier proteins [<xref rid="b0020" ref-type="bibr">4</xref>]. Changes in the abundance and activity of these proteins determine the specific functions of vacuoles. Another important function of vacuoles and lysosomes is the hydrolysis of intracellular proteins and membrane proteins and turnover of organelles (e.g., plastids, mitochondria, peroxides, and partial nuclei). This function helps to remove excess or damaged organelles, which are key factors in cell homeostasis and survival [<xref rid="b0025" ref-type="bibr">[5]</xref>, <xref rid="b0030" ref-type="bibr">[6]</xref>].</p>
    <p id="p0010">Research on the biochemical properties and physiological functions of plant vacuole proteins (PVPs) is the basis for our understanding of the mechanisms underlying vacuole biogenesis and maintenance [<xref rid="b0035" ref-type="bibr">[7]</xref>, <xref rid="b0040" ref-type="bibr">[8]</xref>, <xref rid="b0045" ref-type="bibr">[9]</xref>]. Experimental subcellular localization methods are the most reliable means for characterizing of the biological activities of vacuolar proteins; however, they are usually costly and time-consuming. Therefore, it is important to develop computational methods for the identification of PVPs. Recently, computational prediction algorithms for protein subcellular localization have emerged [<xref rid="b0050" ref-type="bibr">[10]</xref>, <xref rid="b0055" ref-type="bibr">[11]</xref>, <xref rid="b0060" ref-type="bibr">[12]</xref>]. However, most of them have not been specifically developed for PVPs [<xref rid="b0065" ref-type="bibr">13</xref>], and therefore, they perform poorly in identifying PVPs. To the best of our knowledge, there is only one machine learning tool—a support vector machine (SVM)-based model named VacPred—designed for PVP identification. VacPred has provided two of the best prediction models based on the SVM algorithm using two commonly used classic feature extraction methods: dipeptide composition (DPC) and the position-specific scoring matrix (PSSM)-based feature descriptor K-PSSM. The PSSM-based model showed slightly better performance than the DPC-based model on the blind dataset with an accuracy of approximately 63 %. Therefore, although these predictors have markedly promoted research on PVP prediction, there is still a need to develop high-performance PVP predictive tools.</p>
    <p id="p0015">Recently, various sequence-based deep representation learning features for proteins have been proposed and have obtained satisfactory results in many protein-sequence analysis applications [<xref rid="b0070" ref-type="bibr">[14]</xref>, <xref rid="b0075" ref-type="bibr">[15]</xref>, <xref rid="b0080" ref-type="bibr">[16]</xref>, <xref rid="b0085" ref-type="bibr">[17]</xref>, <xref rid="b0090" ref-type="bibr">[18]</xref>, <xref rid="b0095" ref-type="bibr">[19]</xref>]. These methods are also called deep learning embeddings, which are obtained by the multi-dimensional transformation of protein sequences. Deep learning embedding models are always based on unsupervised or semi-supervised learning and trained on large protein sequence databases. These techniques can extract sequence statistics as completely as possible; however, they require considerable time and computing resources to obtain the embedded models. Nonetheless, we can take advantage of these embeddings for PVP prediction by using the idea of transfer learning.</p>
    <p id="p0020">To further establish a new advanced computational PVP predictor with improved accuracy, we used a computational strategy combining deep representation learning and classic sequence features. A two-step feature selection strategy, that is, a light gradient boosting machine (LGBM) combined with sequential forward search (SFS), was subsequently applied to identify the optimal feature subset from each high-dimensional feature. After optimization, we constructed an efficient LGBM-based PVP predictor named iPVP-DRLF. The construction workflow of iPVP-DRLF is shown in <xref rid="f0005" ref-type="fig">Fig. 1</xref>. Fivefold cross-validation and independent testing demonstrate that iPVP-DRLF achieves satisfactory overall performance for PVP identification. Furthermore, despite employing fewer features, it outperforms two existing state-of-the-art (SOTA) predictive models on the blind dataset, improving prediction accuracy by approximately 6.6 % and 3.5 %, respectively. Through feature visualization analysis using the uniform manifold approximation and projection (UMAP) algorithm [<xref rid="b0100" ref-type="bibr">20</xref>], we found that deep representation learning features could represent proteins better than other classic sequence features to distinguish PVPs from non-PVPs. Thus, it helps to improve the performance of iPVP-DRLF and leads to the development of powerful predictive tools.<fig id="f0005"><label>Fig. 1</label><caption><p>The workflow of the development and evaluation process for iPVP-DRLF.</p></caption><graphic xlink:href="gr1"/></fig></p>
  </sec>
  <sec id="s0010">
    <label>2</label>
    <title>Methods and materials</title>
    <sec id="s0015">
      <label>2.1</label>
      <title>Datasets</title>
      <p id="p0025">In this study, the datasets collected by Yadav <italic>et al</italic>. were used for model training and testing [<xref rid="b0065" ref-type="bibr">13</xref>]. Both PVPs and non-PVPs were derived from the UniProtKB/SwissProt database. The CD-HIT software [<xref rid="b0105" ref-type="bibr">21</xref>] was then used to remove redundant samples by setting the sequence identity threshold to 60 % [<xref rid="b0110" ref-type="bibr">22</xref>]. A total of 274 PVPs were obtained as the initial positive samples. Subsequently, 200 PVPs at 40 % identity cut-off were used as positive samples in the training dataset. The remaining 74 sequences were used to form a test dataset to verify the generalization ability of the predictive models. On the other hand, an equal number of negative samples with 40 % identity were collected to construct the balanced training and independent test datasets, respectively. In addition, Yadav <italic>et al</italic>. also created a PVP-blind dataset from cropPAL [<xref rid="b0115" ref-type="bibr">23</xref>] to further evaluate the performance of developed models. The blind dataset contains 227 vacuole proteins with sequence lengths greater than 50. The above-mentioned datasets are available at <ext-link ext-link-type="uri" xlink:href="https://lab.malab.cn/%7eacy/iPVP-DRLF" id="ir015">https://lab.malab.cn/~acy/iPVP-DRLF</ext-link>.</p>
    </sec>
    <sec id="s0020">
      <label>2.2</label>
      <title>Feature extraction</title>
      <p id="p0030">An effective sequence representation approach is crucial for developing satisfactorily performing predictive models. Here, various types of feature descriptors were studied. These sequence representation methods can be roughly divided into two groups: classic protein sequence encoding methods and deep representation learning features.</p>
      <sec id="s0025">
        <label>2.2.1</label>
        <title>Classic sequence encoding methods</title>
        <p id="p0035">In this study, seven types of sequence-based classic feature encoding schemes were investigated to convert protein sequences into feature vectors, including pseudo-amino acid composition (PAAC); amino acid composition (AAC); dipeptide composition (DPC); adaptive skip dipeptide composition (ASDC); quasi-sequence order (QSO); composition, transition, and distribution model of physicochemical properties (CTD) and dipeptide deviation from expected mean (DDE). These feature encoding algorithms can be implemented conveniently using various published state-of-the-art platforms, such as BioSeq-Analysis [<xref rid="b0120" ref-type="bibr">24</xref>], iFeature [<xref rid="b0125" ref-type="bibr">25</xref>], and iLearn [<xref rid="b0130" ref-type="bibr">26</xref>].</p>
      </sec>
      <sec id="s0030">
        <label>2.2.2</label>
        <title>Deep representation learning features</title>
        <p id="p0040">Deep learning has achieved marked success in biological sequence processing due to its powerful sequence representation ability and automatic feature extraction capabilities. It is a special type of machine learning method that can capture parameters in neural networks to automatically learn feature representation [<xref rid="b0135" ref-type="bibr">27</xref>]. Based on the principle of transfer learning, we can utilize pre-trained deep learning models for feature extraction of new data or migrate applications to other similar tasks. Typical examples of deep learning protein sequence embedding methods include UniRep [<xref rid="b0140" ref-type="bibr">28</xref>], bidirectional long short-term memory embedding model (BiLSTM) [<xref rid="b0145" ref-type="bibr">29</xref>], TAPE [<xref rid="b0150" ref-type="bibr">30</xref>], SSA [<xref rid="b0070" ref-type="bibr">14</xref>] and language embedding model (LM) [<xref rid="b0155" ref-type="bibr">31</xref>]. These techniques have been demonstrated to be powerful tools for many protein engineering task applications. In particular, unified representation (UniRep), proposed by Alley <italic>et al.</italic>, was based on a multiplicative long short-term memory architecture. It was trained on UniRef50 (a dataset with ∼ 24 million protein sequences) and has the ability to extract the biological, chemical, and evolutionary information within the protein sequences. Protein sequences can be represented as 1900-dimensional feature vectors (average final hidden state output) using UniRep. More detailed information can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/churchlab/UniRep" id="ir020">https://github.com/churchlab/UniRep</ext-link>. The BiLSTM embedding model was trained on the full set of protein domain sequences in the Pfam database, approximately 22 million protein sequences. BiLSTM embedding feature can be effectively combined with the global structural similarity between proteins and pairwise residue contact maps for individual proteins, allowing the vector matrix mapped from the protein sequences to be fully characterized [<xref rid="b0145" ref-type="bibr">29</xref>].</p>
      </sec>
    </sec>
    <sec id="s0035">
      <label>2.3</label>
      <title>Classifier</title>
      <p id="p0045">LightGBM is a distributed and efficient gradient-boosting framework developed by Microsoft Research [<xref rid="b0120" ref-type="bibr">24</xref>]. This algorithm is based on decision tree and can be used in various machine learning tasks, including classification, sorting, and regression. The traditional gradient boosting decision tree needs to scan all data samples when estimating the information gain of all possible split points. This process is very time-consuming. LightGBM uses two engineering optimization novelties to overcome this problem. The first is gradient-based one-side sampling (GOSS). The GOSS algorithm excludes most of the data samples with small gradients and only uses the remaining ones to estimate the information gain. This leads to a more accurate gain estimation and significantly reduces the number of data instances without losing much training accuracy. The second is the exclusive feature bundling algorithm, which reduces the feature number by bundling mutually exclusive features. Therefore, it provides satisfactory efficiency and scalability when large datasets or high-dimensional features are used. More details can be found in Ref. [<xref rid="b0160" ref-type="bibr">32</xref>]. We also compared seven other commonly used classifiers to find the best machine learning algorithm for PVP identification, including adaboost (AB), bagging, extremely randomized trees (ERT), gradient boosting machine (GBM), support vector machine (SVM), random forest (RF) and extreme gradient boosting (XGBT). We utilized scikit-learn to implement these efficient algorithms and tuned the hyperparameters via grid search (<ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/" id="ir025">https://scikit-learn.org/</ext-link>). The search range for each classifier is presented in <xref rid="s0100" ref-type="sec">Supplementary Table S1</xref>.</p>
    </sec>
    <sec id="s0040">
      <label>2.4</label>
      <title>Feature selection</title>
      <p id="p0050">To overcome overfitting and acquire the most significant feature space for modeling improvement, feature selection is commonly utilized. In recent years, scientists have proposed many methods to evaluate feature importance, such as analysis of variance (ANOVA), Chi2, and maximum-relevance-maximum-distance (MRMD) [<xref rid="b0165" ref-type="bibr">[33]</xref>, <xref rid="b0170" ref-type="bibr">[34]</xref>, <xref rid="b0175" ref-type="bibr">[35]</xref>, <xref rid="b0180" ref-type="bibr">[36]</xref>]. In this study, LGBM was used to rank features, and SFS was subsequently applied to search for the best feature subset [<xref rid="b0185" ref-type="bibr">37</xref>]. A brief introduction is provided here. The training data with true labels was first fed into and fitted to the LGBM model. Next, we could obtain the importance value of each feature according to the built-in function of the LGBM model. A feature ranking list was then generated based on the feature importance values. A higher-ranked feature in the list indicates that it is more informative. The second step was using SFS to search for the optimal feature set from the sorted feature list. Features were added one-by-one from a low index to a high index to form feature subsets with different dimensions. The feature subsets were then fed into the classifier to construct predictive models and evaluated by fivefold cross validation. Finally, the subset with which the prediction model achieves the best performance was considered optimal.</p>
    </sec>
    <sec id="s0045">
      <label>2.5</label>
      <title>Performance measurement</title>
      <p id="p0055">Fivefold cross-validation and independent testing were employed to evaluate the performance of the proposed machine learning models comprehensively and quantitatively [<xref rid="b0190" ref-type="bibr">[38]</xref>, <xref rid="b0195" ref-type="bibr">[39]</xref>, <xref rid="b0200" ref-type="bibr">[40]</xref>, <xref rid="b0205" ref-type="bibr">[41]</xref>, <xref rid="b0210" ref-type="bibr">[42]</xref>]. Fivefold cross-validation randomly splits the original training data into five subsets. Each time, four of them were used for training, and the remaining one was used as the validation dataset. The performance metrics on the five subsets were averaged to obtain the overall fivefold cross-validation results. In addition, an independent test was used to demonstrate the generalization ability of the proposed models. Four standard confusion matrix-based metrics in binary classification tasks were used to measure the recognition ability. These included accuracy (Acc), specificity (SP), sensitivity (SE), and Matthew’s correlation coefficient (MCC). They were calculated using Equations (1) – (4). Moreover, receiver operating characteristic (ROC) curves and the area under the ROC curve (AUC) were also used to make an intuitive performance comparison of constructed models.<disp-formula id="e0005"><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mfenced open="{"><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="italic">SE</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mo>=</mml:mo><mml:mspace width="0.166667em"/><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mn>100</mml:mn><mml:mo>%</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>-</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula></p>
      <p id="p0060">TP: true positive; FP: false positive; TN: true negative; FN: false negative.</p>
    </sec>
  </sec>
  <sec id="s0050">
    <label>3</label>
    <title>Experimental results</title>
    <sec id="s0055">
      <label>3.1</label>
      <title>Performance of classic sequence encoding methods</title>
      <p id="p0065">To determine the best PVP sequence feature representation type, we first developed predictive models using classic protein sequence descriptors based on the LGBM classifier. Notably, several of them are high-dimensional features, which are always redundant, noisy, and computationally expensive. Thus, a feature selection procedure based on the LGBM classifier was performed to remove redundant and irrelevant features, where only the most discriminative features were retained to construct predictive models with upgraded efficiency. The feature selection SFS curves of each descriptor are shown in <xref rid="s0100" ref-type="sec">Figure S1</xref>. The fivefold cross-validation performance results of models based on optimal feature subsets for different descriptors are summarized in <xref rid="t0005" ref-type="table">Table 1</xref>.<table-wrap position="float" id="t0005"><label>Table 1</label><caption><p>5-fold cross-validation results of different classic sequence descriptors.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Feature</th><th>Acc (%)</th><th>AUC</th><th>SE (%)</th><th>SP (%)</th><th>MCC</th></tr></thead><tbody><tr><td>DPC (34D)</td><td align="char">79.00</td><td align="char">0.840</td><td align="char"><bold>81.00</bold></td><td align="char">77.00</td><td align="char">0.580</td></tr><tr><td>CTD (51D)</td><td align="char">78.50</td><td align="char">0.845</td><td align="char">78.00</td><td align="char">79.00</td><td align="char">0.570</td></tr><tr><td>ASDC (57D)</td><td align="char">78.25</td><td align="char">0.829</td><td align="char">77.50</td><td align="char">79.00</td><td align="char">0.565</td></tr><tr><td>DDE (38D)</td><td align="char"><bold>79.25</bold></td><td align="char"><bold>0.867</bold></td><td align="char">79.00</td><td align="char"><bold>79.50</bold></td><td align="char"><bold>0.585</bold></td></tr><tr><td>PAAC (22D)</td><td align="char">72.00</td><td align="char">0.775</td><td align="char">67.50</td><td align="char">76.50</td><td align="char">0.442</td></tr><tr><td>AAC (20D)</td><td align="char">68.25</td><td align="char">0.742</td><td align="char">67.00</td><td align="char">69.50</td><td align="char">0.365</td></tr><tr><td>QSO (44D)</td><td align="char">71.00</td><td align="char">0.774</td><td align="char">70.00</td><td align="char">72.00</td><td align="char">0.420</td></tr></tbody></table><table-wrap-foot><fn><p>Note: The best performance value of each column is highlighted in bold for clarification. Numbers in parentheses represent feature dimensions after feature selection.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0070">As can be observed from <xref rid="t0005" ref-type="table">Table 1</xref>, four types of sequence descriptors, namely DPC, CTD, ASDC and DDE, achieve very similar performance in terms of five performance measures. And DDE seems to be the most powerful classic feature encoding method for PVP prediction. The associated model achieves the highest Acc, AUC, SP, and MCC values of 79.25 %, 0.867, 79.50 %, and 0.585, respectively. Following the DDE, the DPC feature outperforms the other remaining features in terms of Acc and MCC, and most indicators are only slightly lower than the DDE. Furthermore, DPC has an SE score of 81.00 %, the highest among all features. AAC performs the worst among all the descriptors, with Acc and MCC values of 68.25 % and 0.365, respectively. The QSO and PAAC encodings also achieve a comparatively lower performance with an accuracy value of about 70 %, possibly because they can only capture limited informative patterns from PVP sequences. These observations are consistent with previous studies suggesting that the DPC features are critical and essential for prediction of PVPs. Based on these facts, we then integrated the two top-performing features, DDE and DPC, into our hybrid feature to explore more comprehensive and discriminative feature encoding strategies for PVP prediction.</p>
    </sec>
    <sec id="s0060">
      <label>3.2</label>
      <title>Performance of deep representation learning features</title>
      <p id="p0075">Based on the pre-trained deep learning models, we investigated five types of effective deep representation learning features. Notably, most deep representation learning features have higher dimensionality than classical feature descriptors. Therefore, we applied the same two-step feature selection method to determine the optimal feature space for each descriptor. The feature selection results are shown in <xref rid="s0100" ref-type="sec">Figure S2</xref>. <xref rid="t0010" ref-type="table">Table 2</xref> summarizes the performance of the optimal feature subsets for each descriptor on the LGBM classifier.<table-wrap position="float" id="t0010"><label>Table 2</label><caption><p>Fivefold cross-validation results of different deep representation learning features.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Features</th><th>Acc (%)</th><th>AUC</th><th>SE (%)</th><th>SP (%)</th><th>MCC</th></tr></thead><tbody><tr><td>BiLSTM (44D)</td><td align="char"><bold>85.25</bold></td><td align="char">0.908</td><td align="char"><bold>84.00</bold></td><td align="char">86.50</td><td align="char"><bold>0.705</bold></td></tr><tr><td>LM (40D)</td><td align="char">82.00</td><td align="char">0.888</td><td align="char">81.00</td><td align="char">83.00</td><td align="char">0.640</td></tr><tr><td>SSA (28D)</td><td align="char">75.25</td><td align="char">0.815</td><td align="char">74.50</td><td align="char">76.00</td><td align="char">0.505</td></tr><tr><td>TAPE (55D)</td><td align="char">83.00</td><td align="char">0.893</td><td align="char"><bold>84.00</bold></td><td align="char">82.00</td><td align="char">0.660</td></tr><tr><td>UniRep (60D)</td><td align="char">85.00</td><td align="char"><bold>0.915</bold></td><td align="char">82.50</td><td align="char"><bold>87.50</bold></td><td align="char">0.701</td></tr></tbody></table><table-wrap-foot><fn><p>Note: The best performance value of each column is highlighted in bold for clarification. Numbers in parentheses represent feature dimensions after feature selection.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0080">Deep representation learning features achieve a better and more stable performance compared with the classic sequence features. Except for SSA, the other deep representation learning features show significantly improved performance. Among all the deep representation learning features, BiLSTM achieves the overall best performance in terms of the three metrics: Acc (85.25 %), SE (84.00 %) and MCC (0.705). UniRep outperforms all other descriptors on the remaining two metrics, AUC and SP, with values of 0.915 and 87.50 %, respectively. Furthermore, the Acc and MCC scores of UniRep are only slightly worse than BiLSTM. The accuracy of BiLSTM and UniRep achieves a marked improvement of approximately 6 % compared with the best performing classic sequence features. Even TAPE and LM achieve better performance than DDE on five metrics. These results indicate that deep representation learning embedding methods can capture more informative patterns for identifying vacuole proteins from sequences. Considering the overall performance on the training sets, UniRep and BiLSTM were selected for the next feature analysis experiment.</p>
    </sec>
    <sec id="s0065">
      <label>3.3</label>
      <title>Performance of hybrid features</title>
      <p id="p0085">To further explore the best model for PVP identification, we investigated the performance of hybrid features that integrated sequence information from multiple aspects. Therefore, we directly combined the aforementioned four best individual descriptors, namely DDE, DPC, UniRep and BiLSTM, to obtain a 176-dimensional hybrid feature vector (named F176). Preliminary experiments showed that this hybrid feature still contains redundant information. Hence, we performed the same feature selection as described in section 2.4. Here, we systematically evaluated the SFS results based on eight widely used machine learning classifiers. The SFS curves of each classifier are shown in <xref rid="s0100" ref-type="sec">Figure S3</xref>. The fivefold cross-validation performance results for different classifiers are summarized in <xref rid="t0015" ref-type="table">Table 3</xref>. In this step, the testing dataset was employed to further estimate the comprehensive performance of these models, and the prediction results are also presented in <xref rid="t0015" ref-type="table">Table 3</xref>.<table-wrap position="float" id="t0015"><label>Table 3</label><caption><p>Performance comparison of eight machine learning models based on the corresponding optimal feature subset of F176.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Classifier</th><th colspan="5">Fivefold cross-validation<hr/></th><th colspan="5">Independent testing<hr/></th></tr><tr><th>Acc (%)</th><th>AUC</th><th>SE (%)</th><th>SP (%)</th><th>MCC</th><th>Acc (%)</th><th>AUC</th><th>SE (%)</th><th>SP (%)</th><th>MCC</th></tr></thead><tbody><tr><td>AB</td><td align="char"><bold>89.50</bold></td><td align="char"><bold>0.943</bold></td><td align="char"><bold>89.00</bold></td><td align="char"><bold>90.00</bold></td><td align="char"><bold>0.790</bold></td><td align="char">82.43</td><td align="char">0.885</td><td align="char">85.14</td><td align="char">79.73</td><td align="char">0.650</td></tr><tr><td>Bagging</td><td align="char">84.25</td><td align="char">0.895</td><td align="char">82.50</td><td align="char">86.00</td><td align="char">0.685</td><td align="char">83.11</td><td align="char">0.898</td><td align="char">85.14</td><td align="char">81.08</td><td align="char">0.663</td></tr><tr><td>ERT</td><td align="char">83.50</td><td align="char">0.888</td><td align="char">80.00</td><td align="char">87.00</td><td align="char">0.672</td><td align="char">85.81</td><td align="char"><bold>0.936</bold></td><td align="char"><bold>90.54</bold></td><td align="char">81.08</td><td align="char">0.719</td></tr><tr><td>GBM</td><td align="char">86.25</td><td align="char">0.925</td><td align="char">87.50</td><td align="char">85.00</td><td align="char">0.725</td><td align="char">83.11</td><td align="char">0.899</td><td align="char">85.14</td><td align="char">81.08</td><td align="char">0.663</td></tr><tr><td>LGBM</td><td align="char">88.25</td><td align="char">0.933</td><td align="char"><bold>89.00</bold></td><td align="char">87.50</td><td align="char">0.765</td><td align="char"><bold>87.16</bold></td><td align="char">0.916</td><td align="char">89.19</td><td align="char"><bold>85.14</bold></td><td align="char"><bold>0.744</bold></td></tr><tr><td>RF</td><td align="char">84.25</td><td align="char">0.899</td><td align="char">83.00</td><td align="char">85.50</td><td align="char">0.685</td><td align="char">84.46</td><td align="char">0.926</td><td align="char">87.84</td><td align="char">81.08</td><td align="char">0.691</td></tr><tr><td>SVM</td><td align="char">86.75</td><td align="char">0.922</td><td align="char">85.50</td><td align="char">88.00</td><td align="char">0.735</td><td align="char">80.41</td><td align="char">0.871</td><td align="char"><bold>90.54</bold></td><td align="char">70.27</td><td align="char">0.621</td></tr><tr><td>XGBT</td><td align="char">88.25</td><td align="char">0.926</td><td align="char">88.00</td><td align="char">88.50</td><td align="char">0.765</td><td align="char">83.78</td><td align="char">0.900</td><td align="char">85.14</td><td align="char">82.43</td><td align="char">0.676</td></tr></tbody></table><table-wrap-foot><fn><p>Note: The best performance value of each column is highlighted in bold for clarification.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0090">We found that the performance of different classifiers varied greatly. Moreover, it can be observed from <xref rid="t0015" ref-type="table">Table 3</xref> that there is no distinct regularity between the validation dataset and the training dataset. The AB classifier achieves the overall best performance in terms of five metrics, and the accuracy score is up to 89.50 %. But its performance on the test dataset is not satisfactory, with an accuracy of 82.43 %. We observed that the LGBM classifier significantly outperforms the other classifiers on three metrics (Acc, SP, and MCC) on the independent test set. Furthermore, the Acc, AUC, SP and MCC of the LGBM classifier on the training dataset are 88.25 %, 0.933, 87.50 % and 0.765, respectively, which is only slightly worse than the AB classifier. Comprehensively considering the comparison results among the different classifiers, the consistently competitive performance on both datasets demonstrates that the LGBM is the most suitable algorithm for developing predictive models for vacuole protein identification. The new 63-dimensional fused feature vector (named F63) selected using LGBM and the SFS method is considered as the optimal subset. Therefore, the LGBM model trained on the F63 was determined to be the final model for use in the iPVP-DRLF implementation.</p>
      <p id="p0095">To understand the effectiveness of the deep representation learning features, we used the most popular feature analysis strategy, UMAP, for dimension reduction to analyze the distribution characteristics of the training samples. The distribution is shown in <xref rid="f0010" ref-type="fig">Fig. 2</xref>, and the positive and negative samples are distributed totally differentially in the five compared feature spaces (A-E). Notably, many PVPs and non-PVPs overlap in the feature space of the DPC and DDE. In contrast, although there are still some samples with overlapping distributions in the feature space of UniRep, BiLSTM, and the F63, marked boundaries appear to exist to separate the most positive and negative samples. Especially for the optimal feature subset F63, the gap between positive and negative clusters is more obvious, and only a very small number of positive and negative samples overlap. This suggests that the information extracted by the deep representation learning embedding methods is more effective in capturing the difference between PVPs and non-PVPs. Thus, the performance of the iPVP-DRLF was enhanced.<fig id="f0010"><label>Fig. 2</label><caption><p>UMAP distribution of PVPs and non-PVPs using the 63-dimensional vector F63 and four compared individual descriptors. The orange dots represent PVPs and the blue dots represent non-PVPs. (A-E) are the distributions of DDE, DPC, BiLSTM, UniRep and F63, respectively. F presents the ROC curves for iPVP-DRLF on the training and independent test datasets. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><graphic xlink:href="gr2"/></fig></p>
    </sec>
    <sec id="s0070">
      <label>3.4</label>
      <title>Comparison with existing predictors</title>
      <p id="p0100">In this section, we compared the prediction performance of iPVP-DRLF with two SOTA predictive models: VacPred-DPC and VacPred-PSSM. All of them were trained and validated using the same training and testing datasets. As shown in <xref rid="t0020" ref-type="table">Table 4</xref>, VacPred-PSSM outperforms VacPred-DPC in almost all result metrics, so we mainly compared iPVP-DRLF with VacPred-PSSM. <xref rid="f0015" ref-type="fig">Fig. 3</xref>A also visually shows the evaluation metrics comparison. It is clear that iPVP-DRLF achieved better performance in terms of most metrics compared to VacPred-PSSM on both datasets. Especially on the training dataset, the improvements made by our predictive model are statistically significant with an accuracy boost of about 6.5 %. Furthermore, the gap between SE and SP of our model is smaller, which indicates that iPVP-DRLF has a more balanced ability to identify positive and negative samples. It is also worth noting that both VacPred-DPC and VacPred-PSSM use 400-dimensional feature vectors, while ours is 63-dimensional, which is approximately-one-sixth of the former. This significantly reduces the computational cost.<table-wrap position="float" id="t0020"><label>Table 4</label><caption><p>Performance comparison of proposed iPVP-DRLF and the SOTA predictors.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Classifier</th><th colspan="5">Training<hr/></th><th colspan="5">Testing<hr/></th><th>Blind</th></tr><tr><th>Acc (%)</th><th>AUC</th><th>SE (%)</th><th>SP (%)</th><th>MCC</th><th>Acc (%)</th><th>AUC</th><th>SE (%)</th><th>SP (%)</th><th>MCC</th><th>Acc (%)</th></tr></thead><tbody><tr><td><bold>iPVP-DRLF</bold></td><td align="char"><bold>88.25</bold></td><td align="char"><bold>0.933</bold></td><td align="char"><bold>89.00</bold></td><td align="char"><bold>87.50</bold></td><td align="char"><bold>0.765</bold></td><td align="char"><bold>87.16</bold></td><td align="char">0.916</td><td align="char">89.19</td><td align="char"><bold>85.14</bold></td><td align="char"><bold>0.744</bold></td><td align="char"><bold>66.52</bold></td></tr><tr><td>VacPred-DPC</td><td align="char">75.50</td><td align="char">0.800</td><td align="char">70.00</td><td align="char">81.00</td><td align="char">0.510</td><td align="char">80.41</td><td align="char">0.840</td><td align="char">82.43</td><td align="char">78.38</td><td align="char">0.610</td><td align="char">59.91</td></tr><tr><td>VacPred-PSSM</td><td align="char">81.75</td><td align="char">0.860</td><td align="char">76.50</td><td align="char">87.00</td><td align="char">0.640</td><td align="char">86.49</td><td align="char"><bold>0.930</bold></td><td align="char"><bold>90.54</bold></td><td align="char">82.43</td><td align="char">0.730</td><td align="char">62.99</td></tr></tbody></table><table-wrap-foot><fn><p>Note: The best performance value of each column is highlighted in bold for clarification.</p></fn></table-wrap-foot></table-wrap><fig id="f0015"><label>Fig. 3</label><caption><p>Performance comparison of different PVPs prediction software. A presents the comparison of iPVP-DRLF with the SOTA predictor VacPred-PSSM on training, test and blind datasets. B shows the benchmark results of iPVP-DRLF and different published software using blind dataset.</p></caption><graphic xlink:href="gr3"/></fig></p>
      <p id="p0105">To verify the robustness of iPVP-DRLF, we further tested it using a blind dataset. In this section, we compared its performance not only with VacPred, but also with seven SOTA protein subcellular localization prediction tools, including PProwler 1.2, TargetP 1.1, Plant-mSubP-DPC, BaCelLo, pLoc-mPlant, Predotar v1.03 and Plant-mPLoc. For a fair comparison, the blind dataset collected by Yadav et al. was used to test all the compared predictors. The accuracy scores for the seven subcellular localization prediction tools on the blind datasets came from reference [<xref rid="b0065" ref-type="bibr">13</xref>]. As shown in <xref rid="f0015" ref-type="fig">Fig. 3</xref>B, iPVP-DRLF outperformed all the compared models with an accuracy of 66.52 %, which is an approximately 6.6 % and 3.5 % improvement compared with VacPred-DPC and VacPred-PSSM, respectively. Among the protein subcellular localization predictors, PProwler 1.2 achieves the best performance with an Acc score of 41.85 %. Overall, these predictors performed poorly, probably because they were not designed specifically for PVPs. The above analysis clearly suggests the practical applicability of iPVP-DRLF over other methods. ROC curves were drawn (<xref rid="f0010" ref-type="fig">Fig. 2</xref>F) to visually depict the predictive efficiency of iPVP-DRLF. The AUC values obtained by fivefold cross validation and independent testing are 0.933 and 0.916, respectively. Although the developed predictor shows favorable performance, we do observe that our model also does not perform as well on the blind data as it does on the training and independent test data. Thus, there is still some room to improve the generalization ability.</p>
    </sec>
    <sec id="s0075">
      <label>3.5</label>
      <title>Webserver implementation</title>
      <p id="p0110">The iPVP-DRLF is freely available at <ext-link ext-link-type="uri" xlink:href="https://lab.malab.cn/%7eacy/iPVP-DRLF" id="ir030">https://lab.malab.cn/~acy/iPVP-DRLF</ext-link>. The online server helps researchers determine whether their query protein sequences are plant vacuolar proteins. Here, we provide a simple introduction to server usage. The user first needs to paste or type protein sequences into the text box on the left and then click the “Submit” button for prediction. Notably, only FASTA-formatted sequences are supported as inputs for prediction. We have also provided an example of FASTA-formatted sequences in the input box. After the calculation is complete, the prediction results will be shown in a tabular format on the right. To reset the model and start new tasks, the “Resubmit” button can be clicked and the above-mentioned steps can be repeated to obtain new prediction results. Detailed step-by-step instructions on how to use the iPVP-DRLF server are available on the interface of the webserver. Furthermore, the datasets employed in this study can be downloaded from the web server to validate our findings or perform other research.</p>
    </sec>
  </sec>
  <sec id="s0080">
    <label>4</label>
    <title>Conclusion</title>
    <p id="p0115">This research covered a rarely explored area of protein sequence analysis in bioinformatics, that is, the computational identification of PVPs. Based on existing methods, we tackled this problem by combining classic sequence features and deep representation features to encode plant vacuole protein sequences. We found that deep representation learning features are more informative and help plant vacuole protein identification than the commonly used classic sequence features. Moreover, we have proposed a more informative feature representation scheme by integrating and learning from both deep learning embedding features (BiLSTM and UniRep) and classic sequence descriptors (DDE and DPC). Subsequently, these multi-view features were fused and fed into the LGBM classifier to construct the final predictive model. Application of the model shows favorable cross-validation, independent test and blind test accuracies of 88.25 %, 87.16 % and 66.52 %, respectively, all outperforming existing PVP predictors. Furthermore, the UMAP feature visualization demonstrates that the deep representation learning feature plays a more important role than the classic features in the model predictions. To facilitate the relevant research community, a user-friendly online webserver was implemented for iPVP-DRLF and made available for public use. Because of the current lack of highly accurate models specifically dedicated to PVP prediction, our study provides a complete methodology and benchmark and lays a foundation for further research in the future. We anticipate that iPVP-DRLF could serve as a powerful technique that could be used as a supplement to hands-on wet experiments for PVP identification and that its use could facilitate the elucidation of associated biological function mechanisms.</p>
  </sec>
  <sec id="s0085">
    <label>5</label>
    <title>Author's contribution</title>
    <p id="p0120">Quan Zou conceived and designed the experiment. Shihu Jiao performed the experiment and analyzed the results. Quan Zou and Shihu Jiao wrote and revised the manuscript.</p>
  </sec>
  <sec id="s0090">
    <title>Funding</title>
    <p id="p0125">The work was supported by the <funding-source id="gp020"><institution-wrap><institution-id institution-id-type="doi">10.13039/100016163</institution-id><institution>National Natural Science Foundation</institution></institution-wrap></funding-source> of China (No. 61922020 and No. 62131004), the Sichuan Provincial Science Fund for Distinguished Young Scholars (2021JDJQ0025), and the <funding-source id="gp015"><institution-wrap><institution-id institution-id-type="doi">10.13039/100016163</institution-id><institution>Special Science Foundation</institution></institution-wrap></funding-source> of Quzhou (2021D004).</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0130">The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p>
  </sec>
</body>
<back>
  <ref-list id="bi005">
    <title>References</title>
    <ref id="b0005">
      <label>1</label>
      <element-citation publication-type="journal" id="h0005">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Hicks</surname>
            <given-names>G.R.</given-names>
          </name>
          <name>
            <surname>Raikhel</surname>
            <given-names>N.V.</given-names>
          </name>
        </person-group>
        <article-title>Molecular composition of plant vacuoles: important but less understood regulations and roles of tonoplast lipids</article-title>
        <source>Plants (Basel, Switzerland)</source>
        <volume>4</volume>
        <issue>2</issue>
        <year>2015</year>
        <fpage>320</fpage>
        <lpage>333</lpage>
      </element-citation>
    </ref>
    <ref id="b0010">
      <label>2</label>
      <element-citation publication-type="journal" id="h0010">
        <person-group person-group-type="author">
          <name>
            <surname>Kolb</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>FYVE1 is essential for vacuole biogenesis and intracellular trafficking in arabidopsis</article-title>
        <source>Plant Physiol</source>
        <volume>167</volume>
        <issue>4</issue>
        <year>2015</year>
        <fpage>1361</fpage>
        <lpage>U414</lpage>
        <pub-id pub-id-type="pmid">25699591</pub-id>
      </element-citation>
    </ref>
    <ref id="b0015">
      <label>3</label>
      <element-citation publication-type="journal" id="h0015">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Vacuole biogenesis in plants: how many vacuoles, how many models?</article-title>
        <source>Trends Plant Sci</source>
        <volume>25</volume>
        <issue>6</issue>
        <year>2020</year>
        <fpage>538</fpage>
        <lpage>548</lpage>
        <pub-id pub-id-type="pmid">32407694</pub-id>
      </element-citation>
    </ref>
    <ref id="b0020">
      <label>4</label>
      <element-citation publication-type="journal" id="h0020">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Hicks</surname>
            <given-names>G.R.</given-names>
          </name>
          <name>
            <surname>Raikhel</surname>
            <given-names>N.V.</given-names>
          </name>
        </person-group>
        <article-title>Plant vacuole morphology and vacuolar trafficking. Frontiers</article-title>
        <source>Plant Sci</source>
        <volume>5</volume>
        <year>2014</year>
      </element-citation>
    </ref>
    <ref id="b0025">
      <label>5</label>
      <element-citation publication-type="journal" id="h0025">
        <person-group person-group-type="author">
          <name>
            <surname>Neuhaus</surname>
            <given-names>H.E.</given-names>
          </name>
          <name>
            <surname>Trentmann</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>Regulation of transport processes across the tonoplast. Frontiers</article-title>
        <source>Plant Sci</source>
        <volume>5</volume>
        <year>2014</year>
      </element-citation>
    </ref>
    <ref id="b0030">
      <label>6</label>
      <element-citation publication-type="journal" id="h0030">
        <person-group person-group-type="author">
          <name>
            <surname>Wiederhold</surname>
            <given-names>E.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The yeast vacuolar membrane proteome</article-title>
        <source>Mol Cell Proteomics</source>
        <volume>8</volume>
        <issue>2</issue>
        <year>2009</year>
        <fpage>380</fpage>
        <lpage>392</lpage>
        <pub-id pub-id-type="pmid">19001347</pub-id>
      </element-citation>
    </ref>
    <ref id="b0035">
      <label>7</label>
      <element-citation publication-type="journal" id="h0035">
        <person-group person-group-type="author">
          <name>
            <surname>Kataoka</surname>
            <given-names>T.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Vacuolar sulfate transporters are essential determinants controlling internal distribution of sulfate in Arabidopsis</article-title>
        <source>Plant Cell</source>
        <volume>16</volume>
        <issue>10</issue>
        <year>2004</year>
        <fpage>2693</fpage>
        <lpage>2704</lpage>
        <pub-id pub-id-type="pmid">15367713</pub-id>
      </element-citation>
    </ref>
    <ref id="b0040">
      <label>8</label>
      <mixed-citation publication-type="other" id="h0040">Martinoia, E., et al., Vacuolar Transporters in Their Physiological Context, in Annual Review of Plant Biology, Vol 63, S.S. Merchant, Editor. 2012. p. 183-213.</mixed-citation>
    </ref>
    <ref id="b0045">
      <label>9</label>
      <element-citation publication-type="journal" id="h0045">
        <person-group person-group-type="author">
          <name>
            <surname>Martinoia</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Maeshima</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Neuhaus</surname>
            <given-names>H.E.</given-names>
          </name>
        </person-group>
        <article-title>Vacuolar transporters and their essential role in plant metabolism</article-title>
        <source>J Exp Bot</source>
        <volume>58</volume>
        <issue>1</issue>
        <year>2007</year>
        <fpage>83</fpage>
        <lpage>102</lpage>
        <pub-id pub-id-type="pmid">17110589</pub-id>
      </element-citation>
    </ref>
    <ref id="b0050">
      <label>10</label>
      <element-citation publication-type="journal" id="h0050">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K.-C.</given-names>
          </name>
        </person-group>
        <article-title>pLoc-mPlant: predict subcellular localization of multi-location plant proteins by incorporating the optimal GO information into general PseAAC</article-title>
        <source>Mol BioSyst</source>
        <volume>13</volume>
        <issue>9</issue>
        <year>2017</year>
        <fpage>1722</fpage>
        <lpage>1727</lpage>
        <pub-id pub-id-type="pmid">28702580</pub-id>
      </element-citation>
    </ref>
    <ref id="b0055">
      <label>11</label>
      <element-citation publication-type="journal" id="h0055">
        <person-group person-group-type="author">
          <name>
            <surname>Sahu</surname>
            <given-names>S.S.</given-names>
          </name>
          <name>
            <surname>Loaiza</surname>
            <given-names>C.D.</given-names>
          </name>
          <name>
            <surname>Kaundal</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Plant-mSubP: a computational framework for the prediction of single- and multi-target protein subcellular localization using integrated machine-learning approaches</article-title>
        <source>Aob Plants</source>
        <volume>12</volume>
        <issue>3</issue>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="b0060">
      <label>12</label>
      <element-citation publication-type="journal" id="h0060">
        <person-group person-group-type="author">
          <name>
            <surname>Tahir</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Idris</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>MD-LBP: An efficient computational model for protein subcellular localization from HeLa cell lines using SVM</article-title>
        <source>Curr Bioinform</source>
        <volume>15</volume>
        <issue>3</issue>
        <year>2020</year>
        <fpage>204</fpage>
        <lpage>211</lpage>
      </element-citation>
    </ref>
    <ref id="b0065">
      <label>13</label>
      <element-citation publication-type="journal" id="h0065">
        <person-group person-group-type="author">
          <name>
            <surname>Yadav</surname>
            <given-names>A.K.</given-names>
          </name>
          <name>
            <surname>Singla</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>VacPred: Sequence-based prediction of plant vacuole proteins using machine-learning techniques</article-title>
        <source>J Biosci</source>
        <volume>45</volume>
        <issue>1</issue>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b0070">
      <label>14</label>
      <element-citation publication-type="journal" id="h0070">
        <person-group person-group-type="author">
          <name>
            <surname>Lv</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Anticancer peptides prediction with deep representation learning features</article-title>
        <source>Briefings Bioinf</source>
        <volume>22</volume>
        <issue>5</issue>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b0075">
      <label>15</label>
      <element-citation publication-type="journal" id="h0075">
        <person-group person-group-type="author">
          <name>
            <surname>Lv</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Identification of sub-Golgi protein localization by use of deep representation learning features</article-title>
        <source>Bioinformatics</source>
        <volume>36</volume>
        <issue>24</issue>
        <year>2020</year>
        <fpage>5600</fpage>
        <lpage>5609</lpage>
      </element-citation>
    </ref>
    <ref id="b0080">
      <label>16</label>
      <element-citation publication-type="journal" id="h0080">
        <person-group person-group-type="author">
          <name>
            <surname>Anteghini</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>dos Santos</surname>
            <given-names>V.M.</given-names>
          </name>
          <name>
            <surname>Saccenti</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>In-Pero</surname>
          </name>
        </person-group>
        <article-title>Exploiting Deep Learning Embeddings of Protein Sequences to Predict the Localisation of Peroxisomal Proteins</article-title>
        <source>Int J Mol Sci</source>
        <volume>22</volume>
        <issue>12</issue>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b0085">
      <label>17</label>
      <element-citation publication-type="journal" id="h0085">
        <person-group person-group-type="author">
          <name>
            <surname>Cui</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <article-title>Sequence representation approaches for sequence-based protein prediction tasks that use deep learning</article-title>
        <source>Brief Funct Genom</source>
        <volume>20</volume>
        <issue>1</issue>
        <year>2021</year>
        <fpage>61</fpage>
        <lpage>73</lpage>
      </element-citation>
    </ref>
    <ref id="b0090">
      <label>18</label>
      <element-citation publication-type="journal" id="h0090">
        <person-group person-group-type="author">
          <name>
            <surname>Long</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting protein phosphorylation sites based on deep learning</article-title>
        <source>Curr Bioinform</source>
        <volume>15</volume>
        <issue>4</issue>
        <year>2020</year>
        <fpage>300</fpage>
        <lpage>308</lpage>
      </element-citation>
    </ref>
    <ref id="b0095">
      <label>19</label>
      <element-citation publication-type="journal" id="h0095">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Review of the applications of deep learning in bioinformatics</article-title>
        <source>Curr Bioinform</source>
        <volume>15</volume>
        <issue>8</issue>
        <year>2020</year>
        <fpage>898</fpage>
        <lpage>911</lpage>
      </element-citation>
    </ref>
    <ref id="b0100">
      <label>20</label>
      <element-citation publication-type="journal" id="h0100">
        <person-group person-group-type="author">
          <name>
            <surname>Mcinnes</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Healy</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>UMAP: uniform manifold approximation and projection for dimension reduction</article-title>
        <source>J Open Source Software</source>
        <volume>3</volume>
        <issue>29</issue>
        <year>2018</year>
        <fpage>861</fpage>
      </element-citation>
    </ref>
    <ref id="b0105">
      <label>21</label>
      <element-citation publication-type="journal" id="h0105">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>CD-HIT Suite: a web server for clustering and comparing biological sequences</article-title>
        <source>Bioinformatics</source>
        <volume>26</volume>
        <issue>5</issue>
        <year>2010</year>
        <fpage>680</fpage>
        <lpage>682</lpage>
        <pub-id pub-id-type="pmid">20053844</pub-id>
      </element-citation>
    </ref>
    <ref id="b0110">
      <label>22</label>
      <element-citation publication-type="journal" id="h0110">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sequence clustering in bioinformatics: an empirical study</article-title>
        <source>Briefings Bioinf</source>
        <volume>21</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="b0115">
      <label>23</label>
      <element-citation publication-type="journal" id="h0115">
        <person-group person-group-type="author">
          <name>
            <surname>Hooper</surname>
            <given-names>C.M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Finding the Subcellular Location of Barley, Wheat, Rice and Maize Proteins: The Compendium of Crop Proteins with Annotated Locations (cropPAL)</article-title>
        <source>Plant Cell Physiol</source>
        <volume>57</volume>
        <issue>1</issue>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="b0120">
      <label>24</label>
      <element-citation publication-type="journal" id="h0120">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>BioSeq-Analysis2.0: an updated platform for analyzing DNA, RNA and protein sequences at sequence level and residue level based on machine learning approaches</article-title>
        <source>Nucleic Acids Res</source>
        <volume>47</volume>
        <issue>20</issue>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="b0125">
      <label>25</label>
      <element-citation publication-type="journal" id="h0125">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>iFeature: a Python package and web server for features extraction and selection from protein and peptide sequences</article-title>
        <source>Bioinformatics</source>
        <volume>34</volume>
        <issue>14</issue>
        <year>2018</year>
        <fpage>2499</fpage>
        <lpage>2502</lpage>
        <pub-id pub-id-type="pmid">29528364</pub-id>
      </element-citation>
    </ref>
    <ref id="b0130">
      <label>26</label>
      <element-citation publication-type="journal" id="h0130">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Z.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>iLearn: an integrated platform and meta-learner for feature engineering, machine-learning analysis and modeling of DNA, RNA and protein sequence data</article-title>
        <source>Briefings Bioinf</source>
        <volume>21</volume>
        <issue>3</issue>
        <year>2020</year>
        <fpage>1047</fpage>
        <lpage>1057</lpage>
      </element-citation>
    </ref>
    <ref id="b0135">
      <label>27</label>
      <element-citation publication-type="journal" id="h0135">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Computational identification of eukaryotic promoters based on cascaded deep capsule neural networks</article-title>
        <source>Brief Bioinform</source>
        <volume>22</volume>
        <issue>4</issue>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b0140">
      <label>28</label>
      <element-citation publication-type="journal" id="h0140">
        <person-group person-group-type="author">
          <name>
            <surname>Alley</surname>
            <given-names>E.C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>
        <source>Nat Methods</source>
        <volume>16</volume>
        <issue>12</issue>
        <year>2019</year>
        <fpage>p. 1315-+</fpage>
      </element-citation>
    </ref>
    <ref id="b0145">
      <label>29</label>
      <mixed-citation publication-type="other" id="h0145">Bepler, T. and B. Berger, Learning protein sequence embeddings using information from structure. 2019.</mixed-citation>
    </ref>
    <ref id="b0150">
      <label>30</label>
      <mixed-citation publication-type="other" id="h0150">Rao, R., et al., Evaluating Protein Transfer Learning with TAPE. 2019.</mixed-citation>
    </ref>
    <ref id="b0155">
      <label>31</label>
      <element-citation publication-type="book" id="h0155">
        <person-group person-group-type="author">
          <name>
            <surname>Nambiar</surname>
            <given-names>A.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Transforming the language of life: transformer neural networks for protein prediction tasks</part-title>
        <source>BCB '20: 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</source>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b0160">
      <label>32</label>
      <mixed-citation publication-type="other" id="h0160">Ke, G., et al. LightGBM: A Highly Efficient Gradient Boosting Decision Tree. in 31st Annual Conference on Neural Information Processing Systems (NIPS). 2017. Long Beach, CA.</mixed-citation>
    </ref>
    <ref id="b0165">
      <label>33</label>
      <element-citation publication-type="journal" id="h0165">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>Q.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A novel features ranking metric with application to scalable visual and bioinformatics data classification</article-title>
        <source>Neurocomputing</source>
        <volume>173</volume>
        <year>2016</year>
        <fpage>346</fpage>
        <lpage>354</lpage>
      </element-citation>
    </ref>
    <ref id="b0170">
      <label>34</label>
      <element-citation publication-type="journal" id="h0170">
        <person-group person-group-type="author">
          <name>
            <surname>Govern</surname>
            <given-names>A.D.M.</given-names>
          </name>
        </person-group>
        <article-title>A new and simpler approximation for ANOVA under variance heterogeneity</article-title>
        <source>J Educat Behav Stat</source>
        <volume>19</volume>
        <issue>2</issue>
        <year>1994</year>
        <fpage>91</fpage>
        <lpage>101</lpage>
      </element-citation>
    </ref>
    <ref id="b0175">
      <label>35</label>
      <element-citation publication-type="journal" id="h0175">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Scikit-learn: machine learning in python</article-title>
        <source>J Machine Learn Res</source>
        <volume>12</volume>
        <year>2011</year>
        <fpage>2825</fpage>
        <lpage>2830</lpage>
      </element-citation>
    </ref>
    <ref id="b0180">
      <label>36</label>
      <element-citation publication-type="journal" id="h0180">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MRMD2.0: A python tool for machine learning with feature ranking and reduction</article-title>
        <source>Curr Bioinform</source>
        <volume>15</volume>
        <issue>10</issue>
        <year>2020</year>
        <fpage>1213</fpage>
        <lpage>1221</lpage>
      </element-citation>
    </ref>
    <ref id="b0185">
      <label>37</label>
      <element-citation publication-type="journal" id="h0185">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>R.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Feature selection with multi-view data: A survey</article-title>
        <source>Inform Fusion</source>
        <volume>50</volume>
        <year>2019</year>
        <fpage>158</fpage>
        <lpage>167</lpage>
      </element-citation>
    </ref>
    <ref id="b0190">
      <label>38</label>
      <element-citation publication-type="journal" id="h0190">
        <person-group person-group-type="author">
          <name>
            <surname>Manayalan</surname>
            <given-names>B.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>mAHTPred: a sequence-based meta-predictor for improving the prediction of anti-hypertensive peptides using effective feature representation</article-title>
        <source>Bioinformatics</source>
        <volume>35</volume>
        <issue>16</issue>
        <year>2019</year>
        <fpage>2757</fpage>
        <lpage>2765</lpage>
        <pub-id pub-id-type="pmid">30590410</pub-id>
      </element-citation>
    </ref>
    <ref id="b0195">
      <label>39</label>
      <element-citation publication-type="journal" id="h0195">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Improved and Promising Identification of Human MicroRNAs by Incorporating a High-Quality Negative Set</article-title>
        <source>IEEE/ACM Trans Comput Biol Bioinf</source>
        <volume>11</volume>
        <issue>1</issue>
        <year>2014</year>
        <fpage>192</fpage>
        <lpage>201</lpage>
      </element-citation>
    </ref>
    <ref id="b0200">
      <label>40</label>
      <mixed-citation publication-type="other" id="h0200">Ke, G., et al., LightGBM: A Highly Efficient Gradient Boosting Decision Tree, in Advances in Neural Information Processing Systems 30, I. Guyon, et al., Editors. 2017.</mixed-citation>
    </ref>
    <ref id="b0205">
      <label>41</label>
      <mixed-citation publication-type="other" id="h0205">Li, J.P., Yuqian; Tang, Jijun; Zou, Quan; Guo, Fei, DeepATT: a hybrid category attention neural network for identifying functional effects of DNA sequences. Briefings in Bioinformatics, 2020: p. 1-1.</mixed-citation>
    </ref>
    <ref id="b0210">
      <label>42</label>
      <element-citation publication-type="journal" id="h0210">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepAVP: a dual-channel deep neural network for identifying variable-length antiviral peptides</article-title>
        <source>IEEE J Biomed Health Inf</source>
        <volume>24</volume>
        <issue>10</issue>
        <year>2020</year>
        <fpage>3012</fpage>
        <lpage>3019</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="s0100" sec-type="supplementary-material">
    <label>Appendix A</label>
    <title>Supplementary data</title>
    <p id="p0140">The following are the Supplementary data to this article:<supplementary-material content-type="local-data" id="m0005"><caption><title>Supplementary data 1</title></caption><media xlink:href="mmc1.docx"/></supplementary-material></p>
  </sec>
  <fn-group>
    <fn id="s0095" fn-type="supplementary-material">
      <label>Appendix A</label>
      <p id="p0135">Supplementary data to this article can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.csbj.2022.06.002" id="ir035">https://doi.org/10.1016/j.csbj.2022.06.002</ext-link>.</p>
    </fn>
  </fn-group>
</back>
