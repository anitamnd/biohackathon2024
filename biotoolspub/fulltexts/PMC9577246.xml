<?DTDIdentifier.IdentifierValue http://null/schema/dtds/document/fulltext/xcr/xocs-article.xsd?>
<?DTDIdentifier.IdentifierType schema?>
<?SourceDTD.DTDName xocs-article.xsd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName ftrr2jats.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Smart Health (Amst)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Smart Health (Amst)</journal-id>
    <journal-title-group>
      <journal-title>Smart Health (Amsterdam, Netherlands)</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">2352-6483</issn>
    <issn pub-type="epub">2352-6491</issn>
    <publisher>
      <publisher-name>Elsevier Inc.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9577246</article-id>
    <article-id pub-id-type="pii">S2352-6483(22)00082-4</article-id>
    <article-id pub-id-type="doi">10.1016/j.smhl.2022.100348</article-id>
    <article-id pub-id-type="publisher-id">100348</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Spatial–Temporal Synchronous Graph Transformer network (STSGT) for COVID-19 forecasting</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au000001">
        <name>
          <surname>Banerjee</surname>
          <given-names>Soumyanil</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author" id="au000002">
        <name>
          <surname>Dong</surname>
          <given-names>Ming</given-names>
        </name>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au000003">
        <name>
          <surname>Shi</surname>
          <given-names>Weisong</given-names>
        </name>
      </contrib>
      <aff id="aff1">Department of Computer Science, Wayne State University, 5057 Woodward Ave, Detroit, MI 48202, USA</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author.</corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="ppub">
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>13</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <volume>26</volume>
    <fpage>100348</fpage>
    <lpage>100348</lpage>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>9</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 Elsevier Inc. All rights reserved.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Elsevier Inc.</copyright-holder>
      <license>
        <license-p>Since January 2020 Elsevier has created a COVID-19 resource centre with free information in English and Mandarin on the novel coronavirus COVID-19. The COVID-19 resource centre is hosted on Elsevier Connect, the company's public news and information website. Elsevier hereby grants permission to make all its COVID-19-related research that is available on the COVID-19 resource centre - including this research content - immediately available in PubMed Central and other publicly funded repositories, such as the WHO COVID database with rights for unrestricted research re-use and analyses in any form or by any means with acknowledgement of the original source. These permissions are granted for free by Elsevier for as long as the COVID-19 resource centre remains active.</license-p>
      </license>
    </permissions>
    <abstract id="d1e1566">
      <p>COVID-19 has become a matter of serious concern over the last few years. It has adversely affected numerous people around the globe and has led to the loss of billions of dollars of business capital. In this paper, we propose a novel Spatial–Temporal Synchronous Graph Transformer network (STSGT) to capture the complex spatial and temporal dependency of the COVID-19 time series data and forecast the future status of an evolving pandemic. The layers of STSGT combine the graph convolution network (GCN) with the self-attention mechanism of transformers on a synchronous spatial–temporal graph to capture the dynamically changing pattern of the COVID time series. The spatial–temporal synchronous graph simultaneously captures the spatial and temporal dependencies between the vertices of the graph at a given and subsequent time-steps, which helps capture the heterogeneity in the time series and improve the forecasting accuracy. Our extensive experiments on two publicly available real-world COVID-19 time series datasets demonstrate that STSGT significantly outperforms state-of-the-art algorithms that were designed for spatial–temporal forecasting tasks. Specifically, on average over a 12-day horizon, we observe a potential improvement of 12.19% and 3.42% in Mean Absolute Error (MAE) over the next best algorithm while forecasting the daily infected and death cases respectively for the 50 states of US and Washington, D.C. Additionally, STSGT also outperformed others when forecasting the daily infected cases at the state level, e.g., for all the counties in the State of Michigan. The code and models are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/soumbane/STSGT" id="interref1">https://github.com/soumbane/STSGT</ext-link>.</p>
    </abstract>
    <abstract abstract-type="graphical" id="d1e1575">
      <title>Graphical abstract</title>
      <fig id="dfig1" position="anchor">
        <graphic xlink:href="ga1_lrg"/>
      </fig>
    </abstract>
    <kwd-group id="d1e1616">
      <title>Keywords</title>
      <kwd>Time-series forecasting</kwd>
      <kwd>Spatial–temporal graphs</kwd>
      <kwd>COVID-19 forecasting</kwd>
      <kwd>Transformers</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p id="d1e1643">The Coronavirus disease 2019 (COVID-19) is caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The first known case was identified in Wuhan, China in December 2019, and it has since rapidly spread around the globe. The coronavirus causes infection to the upper respiratory tract which could prove fatal if the immune system does not respond (<xref rid="b14" ref-type="bibr">Harapan et al., 2020</xref>). The World Health Organization (WHO) declared COVID-19 as a global pandemic in March, 2020 (<xref rid="b7" ref-type="bibr">Cucinotta &amp; Vanelli, 2020</xref>). Since then, COVID has disrupted many businesses and households worldwide. Millions of people around the globe got infected with the virus and numerous families lost their loved ones (<xref rid="b19" ref-type="bibr">Koh, Geller, &amp; VanderWeele, 2021</xref>) due to complications that aroused from the virus. Small businesses have been the ones that were the most adversely affected due to COVID (<xref rid="b3" ref-type="bibr">Bartik et al., 2020</xref>). The global economy and financial markets were also severely affected due to COVID (<xref rid="b26" ref-type="bibr">Pak et al., 2020</xref>). This in turn affected a lot of people worldwide who lost their jobs due to the economic downturn.</p>
    <p id="d1e1671">Deep learning has revolutionized the areas of computer vision, natural language processing, robotics, medical image processing and many others. Deep learning removes the feature engineering process of traditional machine learning and learns in an end-to-end manner by automatically extracting meaningful and important patterns from the input data. For example, many researchers have used deep learning models for detecting COVID-19 from medical images such as X-rays (<xref rid="b1" ref-type="bibr">Awan et al., 2021</xref>, <xref rid="b25" ref-type="bibr">Minaee et al., 2020</xref>). However, such methods could only be applied for persons already affected by COVID and these are not ideal for prevention of COVID-19.</p>
    <p id="d1e1678">In such situations, the prevention of COVID-19 becomes a matter of serious concern as a lot of lives could be saved if we could prevent the disease. The most important question that arises regarding the prevention of COVID is whether we can forecast the future status of the disease. In other words, if we could forecast the total number of infected and death cases in advance, then a lot of precautions could be taken to stop the disease from spreading. This leads us to the problem of time series analysis of COVID cases, where we make use of the past historical COVID data to forecast the future COVID cases. It is also equally important to forecast the COVID cases over the long-term since this would provide some time for the health care workers to prepare for difficult situations and help them serve the infected patients in a more efficient way. Additionally, a long-term forecasting would enable the common people to take safety precautions and prevent the disease.</p>
    <p id="d1e1680">Graph Neural Networks (GNNs) are a special type of machine learning algorithms that considers a graph as input and learns to predict the attributes of the vertices and edges of the graph. Recently, GNNs have been used for numerous applications where the data could be modeled as a graph. GNNs could also be combined with time series analysis algorithms to construct the spatial–temporal graph neural network. One of the classic cases where a spatial–temporal graph neural network has been used is traffic forecasting. In traffic forecasting, many researchers have proposed complex spatial–temporal graph neural networks to accurately capture the complex pattern in the spatial–temporal time series data. However, there are very few research works that deal with time series forecasting with COVID data. Some researchers have used spatial–temporal graph neural networks to forecast the COVID cases (<xref rid="b17" ref-type="bibr">Kapoor et al., 2020</xref>) but such methods were applied during the start of the pandemic (summer 2020) and as such the size of the dataset was too small to draw an inference. Hence, it becomes critically important that we develop a deep learning model which uses the COVID data from the start of the pandemic to recent times, so that this model could be used for prevention of the disease more effectively.</p>
    <p id="d1e1687">In this paper, we develop a novel Spatial–Temporal Synchronous Graph Transformer network (STSGT) which uses the historical COVID data to forecast the future infected and death cases for all the US states. In STSGT, first we denote the US states as vertices of a graph and the daily infected or death cases as the vertex features for a given time-step. The physical distances between the states are used as an adjacency matrix. Then, a large spatial–temporal synchronous graph is generated from the historical time-steps, where each day denote a time-step. This novel process ensures that we can capture the complex non-linear spatial–temporal dependencies between adjacent time-steps in a synchronous fashion. We apply the multi-head self-attention mechanism of transformers (<xref rid="b33" ref-type="bibr">Vaswani et al., 2017</xref>) on the spatial–temporal synchronous graph to infer the dependency of a vertex of the graph with another vertex. After each vertex finds its importance to all other vertices based on attention scores, we use the graph convolution network (GCN) (<xref rid="b18" ref-type="bibr">Kipf &amp; Welling, 2017</xref>) to aggregate the features (infected or death cases) from neighboring vertices and update the current vertex of the graph. Finally, we forecast the future COVID cases for both short-term, i.e., next day forecasting, and long-term, i.e., forecasting over the next 12 days.</p>
    <p id="d1e1699">The main contributions of our work are summarized as follows:</p>
    <p id="d1e1701">
      <list list-type="simple" id="d1e1703">
        <list-item id="lst1">
          <label>•</label>
          <p id="d1e1707">We propose a novel Spatial–Temporal Synchronous Graph Transformer network (STSGT) which can simultaneously capture the complex spatial and temporal dependencies of the COVID time series data and accurately forecast the future COVID cases by utilizing the multi-head self-attention mechanism of transformers combined with a graph convolution network (GCN).</p>
        </list-item>
        <list-item id="lst2">
          <label>•</label>
          <p id="d1e1712">To the best of our knowledge, this is the first work that performs such a thorough and detailed analysis with real world COVID-19 time series datasets over a long period of time from the start of the pandemic in March 2020 to Nov 2021 (right before the start of the Omicron wave).</p>
        </list-item>
        <list-item id="lst3">
          <label>•</label>
          <p id="d1e1717">Extensive experiments on two real-world public datasets demonstrate that our model STSGT could accurately forecast the daily infected and death cases at both the national (for all the US states) and state (e.g., for all the counties of the state of Michigan) levels. Our model significantly outperforms other state-of-the-art models designed for spatial–temporal forecasting tasks.</p>
        </list-item>
      </list>
    </p>
    <p id="d1e1719">The rest of the paper is organized as follows: Section <xref rid="sec2" ref-type="sec">2</xref> reviews the relevant research works and their applications. Section <xref rid="sec3" ref-type="sec">3</xref> describes the details of our proposed model STSGT along with its components. Section <xref rid="sec4" ref-type="sec">4</xref> describes the experimental setup and the results of our experiments. Section <xref rid="sec5" ref-type="sec">5</xref> presents discussion and future work. Lastly, Section <xref rid="sec6" ref-type="sec">6</xref> presents our conclusion.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Related works</title>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Graph Neural Networks</title>
      <p id="d1e1757">Graph Neural Networks (GNNs) have been significantly useful when the input data does not have a fixed structure as in an image or text and could be represented as a graph with a set of vertices, edges and an adjacency matrix. GNNs and its variants have been employed for numerous applications such as recommendation systems (<xref rid="b36" ref-type="bibr">Wu, Sun, Zhang, &amp; Cui, 2020</xref>), protein interface prediction (<xref rid="b11" ref-type="bibr">Fout, Byrd, Shariat, &amp; Ben-Hur, 2017</xref>) and semantic segmentation (<xref rid="b28" ref-type="bibr">Qi, Liao, Jia, Fidler, &amp; Urtasun, 2017</xref>). For example, a combination of Convolution Neural Networks (CNNs) and graph relation networks have been used for brain connectivity network analysis (<xref rid="b2" ref-type="bibr">Banerjee et al., 2020</xref>).</p>
      <p id="d1e1780">Graph Convolution Network (GCN) (<xref rid="b18" ref-type="bibr">Kipf &amp; Welling, 2017</xref>) was first developed in a transductive setting to apply the convolutional operation on graphs in the spectral domain. The attention mechanism was introduced in the spatial domain into the feature aggregation process of GCNs by Graph Attention Network (GAT) (<xref rid="b34" ref-type="bibr">Veličković et al., 2018</xref>), which learns the weights to aggregate features from the neighboring vertices. The first application of graph representation learning to large-scale graphs in an inductive setting was introduced by GraphSAGE (<xref rid="b13" ref-type="bibr">Hamilton, Ying, &amp; Leskovec, 2017</xref>). Cluster-GCN (<xref rid="b5" ref-type="bibr">Chiang et al., 2019</xref>) removed the redundant computation in the neighborhood sampling of GraphSAGE by clustering the graph and then train a GCN. A major limitation of increasing the number of layers of GCN is that it leads to over-smoothing, where vertex representations become indistinguishable as the number of layers of the graph network increases (<xref rid="b21" ref-type="bibr">Li, Han, and Wu, 2018</xref>, <xref rid="b22" ref-type="bibr">Li et al., 2019</xref>, <xref rid="b29" ref-type="bibr">Rong et al., 2019</xref>, <xref rid="b38" ref-type="bibr">Xu et al., 2018</xref>, <xref rid="b42" ref-type="bibr">Zhao and Akoglu, 2019</xref>).</p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>Spatial–Temporal forecasting</title>
      <p id="d1e1813">Spatial–Temporal forecasting is a fundamentally important problem and has lot of useful applications such as weather forecasting (<xref rid="b31" ref-type="bibr">Tekin, Karaahmetoglu, Ilhan, Balaban, &amp; Kozat, 2021</xref>) and action recognition (<xref rid="b39" ref-type="bibr">Yan, Xiong, &amp; Lin, 2018</xref>). In smart transportation, traffic forecasting is one of the most challenging problems where the goal is to forecast the future traffic status based on historical observations. Spatial–temporal graph neural networks have been extensively used for traffic forecasting. Spatio-temporal graph convolution networks (STGCN) (<xref rid="b41" ref-type="bibr">Yu, Yin, &amp; Zhu, 2018</xref>) used a GCN and a gated 1-D CNN to capture spatial and temporal dependencies respectively. Diffusion convolutional recurrent neural networks (DCRNN) (<xref rid="b23" ref-type="bibr">Li, Yu, Shahabi, &amp; Liu, 2018</xref>) used the diffusion convolution and the diffusion convolution gated recurrent unit to model the spatial and temporal dependencies respectively. An attention-based approach has been combined with GCN and used with the weekly, daily and hourly traffic data in ASTGCN (<xref rid="b12" ref-type="bibr">Guo, Lin, Feng, Song, &amp; Wan, 2019</xref>). Graph WaveNet (<xref rid="b35" ref-type="bibr">Wu, Pan, Long, Jiang, &amp; Zhang, 2019</xref>) uses a gated temporal convolutional network (TCN) (<xref rid="b20" ref-type="bibr">Lea, Vidal, Reiter, &amp; Hager, 2016</xref>) with a dilation factor and a GCN for capturing the long-range temporal and spatial dependencies respectively for traffic forecasting. Spatial–temporal synchronous graph convolutional networks (STSGCN) (<xref rid="b30" ref-type="bibr">Song, Lin, Guo, &amp; Wan, 2020</xref>) applied the GCN on a synchronous spatial–temporal graph to capture the complex spatial–temporal dependencies and heterogeneity in traffic data. Spatial–temporal fusion graph neural networks (STFGNN) (<xref rid="b24" ref-type="bibr">Li &amp; Zhu, 2021</xref>) fused a temporal graph with the spatial graph to forecast the future traffic conditions. Spatial–temporal graph neural networks has also recently been used to forecast the COVID-19 cases (<xref rid="b17" ref-type="bibr">Kapoor et al., 2020</xref>).</p>
    </sec>
    <sec id="sec2.3">
      <label>2.3</label>
      <title>Transformers</title>
      <p id="d1e1873">One of the most prominent architectures that have revolutionized the Natural Language Processing (NLP) field are transformers (<xref rid="b33" ref-type="bibr">Vaswani et al., 2017</xref>). Transformers introduced the multi-head self-attention mechanism which could capture the relationship between every pair of words in a sentence in a parallel manner, thereby improving over the performance of serial architectures such as long short-term memory (LSTM) (<xref rid="b16" ref-type="bibr">Hochreiter &amp; Schmidhuber, 1997</xref>) and gated recurrent unit (GRU) (<xref rid="b6" ref-type="bibr">Chung, Gulcehre, Cho, &amp; Bengio, 2014</xref>), which have the vanishing gradient problem (<xref rid="b15" ref-type="bibr">Hochreiter, 1998</xref>). Transformers and its recent variants such as BERT (<xref rid="b8" ref-type="bibr">Devlin, Chang, Lee, &amp; Toutanova, 2018</xref>) and large language models such as GPT-3 (<xref rid="b4" ref-type="bibr">Brown et al., 2020</xref>) have been used for numerous NLP applications such as machine translation, sentiment classification, speech recognition and many others.</p>
      <p id="d1e1907">The self-attention mechanism of transformers have also been used in computer vision by vision transformers (<xref rid="b10" ref-type="bibr">Dosovitskiy et al., 2020</xref>) for image recognition at scale. Recently, the self-attention mechanism of transformers has been adopted to the spatial domain of GNNs with graphormer (<xref rid="b40" ref-type="bibr">Ying et al., 2021</xref>) and to the spatial–temporal forecasting domain with the spatial–temporal transformer network (STTN) (<xref rid="b37" ref-type="bibr">Xu et al., 2020</xref>).</p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3</label>
    <title>Our approach</title>
    <p id="d1e1930">Our proposed Spatial–Temporal Synchronous Graph Transformer network (STSGT) could be summarized with three main points: (1) Generate the samples from the original time series data with a sliding window approach; (2) For each sample, connect each vertex with itself at the previous and the next time steps to construct a spatial–temporal synchronous graph; (3) Use STSGT layers to capture the complex spatial–temporal correlations and then forecast the future status of the spatial–temporal graph network.</p>
    <p id="d1e1932">In STSGT, we generate a large synchronous spatial–temporal graph with several days historical data and then calculate the multi-head self-attention between every pair of vertices of this large graph. This novel process not only captures the relationship between the COVID infected or death cases between neighboring states or counties in a given day but also simultaneously captures the relationship between those states or counties in the previous and following days. Note that STSGT is fundamentally different from existing approaches for spatial–temporal forecasting. First, in most other graph neural network-based spatial–temporal models, a GCN is used to capture the spatial dependency for a given time-step, and a separate 1D convolution unit or attention unit is used to capture the temporal dependency between the time-steps, while STSGT used the multi-head self-attentions to capture both spatial and temporal dependency synchronously. In the literature, the most similar model to our approach is STTN (<xref rid="b37" ref-type="bibr">Xu et al., 2020</xref>), which has separate spatial and temporal transformer units and hence cannot simultaneously capture the complex spatial–temporal dependencies between adjacent time steps. The synchronous mechanism in our model helps us to achieve better performance in forecasting.</p>
    <p id="d1e1939">In the following sections, we provide a detailed discussion on every component of STSGT.</p>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Generate samples using a Sliding Window</title>
      <p id="d1e1946">We use a sliding window approach to generate the samples from the COVID-19 time series datasets as shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>.</p>
      <p id="d1e1952">Each of the time step comprises of a spatial graph <inline-formula><mml:math id="d1e1955" altimg="si1.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e1978" altimg="si2.svg" display="inline"><mml:mi>V</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="d1e1983" altimg="si3.svg" display="inline"><mml:mi>E</mml:mi></mml:math></inline-formula> denote the set of vertices and edges of <inline-formula><mml:math id="d1e1989" altimg="si4.svg" display="inline"><mml:mi>G</mml:mi></mml:math></inline-formula> respectively. The number of vertices of the graph is denoted as <inline-formula><mml:math id="d1e1994" altimg="si5.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e1999" altimg="si6.svg" display="inline"><mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mi>V</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. “<inline-formula><mml:math id="d1e2014" altimg="si7.svg" display="inline"><mml:mi>A</mml:mi></mml:math></inline-formula>” denotes the adjacency matrix of <inline-formula><mml:math id="d1e2020" altimg="si4.svg" display="inline"><mml:mi>G</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e2025" altimg="si9.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. For each vertex <inline-formula><mml:math id="d1e2044" altimg="si5.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> of the graph <inline-formula><mml:math id="d1e2049" altimg="si4.svg" display="inline"><mml:mi>G</mml:mi></mml:math></inline-formula>, there are certain features <inline-formula><mml:math id="d1e2055" altimg="si12.svg" display="inline"><mml:mi>F</mml:mi></mml:math></inline-formula> that are associated with it. Hence, we denote graph signal matrix for each time-step <inline-formula><mml:math id="d1e2060" altimg="si13.svg" display="inline"><mml:mi>t</mml:mi></mml:math></inline-formula> of the time series as <inline-formula><mml:math id="d1e2065" altimg="si14.svg" display="inline"><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e2080" altimg="si15.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      <p id="d1e2108">Then, we can formulate the problem of COVID-19 time series forecasting as learning the function <inline-formula><mml:math id="d1e2113" altimg="si16.svg" display="inline"><mml:mi>f</mml:mi></mml:math></inline-formula> which uses the historical <inline-formula><mml:math id="d1e2118" altimg="si29.svg" display="inline"><mml:mi>M</mml:mi></mml:math></inline-formula> observations to forecast the future <inline-formula><mml:math id="d1e2123" altimg="si18.svg" display="inline"><mml:mi>H</mml:mi></mml:math></inline-formula> observations: <disp-formula id="fd1"><label>(1)</label><mml:math id="d1e2135" altimg="si19.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>M</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="d1e2272" altimg="si20.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> and <inline-formula><mml:math id="d1e2282" altimg="si21.svg" display="inline"><mml:mi>X</mml:mi></mml:math></inline-formula> denote the <inline-formula><mml:math id="d1e2287" altimg="si18.svg" display="inline"><mml:mi>H</mml:mi></mml:math></inline-formula> forecasted and <inline-formula><mml:math id="d1e2293" altimg="si29.svg" display="inline"><mml:mi>M</mml:mi></mml:math></inline-formula> ground truth observations respectively.<fig id="fig1"><label>Fig. 1</label><caption><p>Generating the samples from the training data using a sliding window approach.</p></caption><graphic xlink:href="gr1_lrg"/></fig></p>
    </sec>
    <sec id="sec3.2">
      <label>3.2</label>
      <title>Construction of the spatial–temporal synchronous graph</title>
      <p id="d1e2302">The goal behind the construction of the spatial–temporal synchronous graph is to capture the complex spatial–temporal dependencies that are present between vertices of previous and subsequent time-steps. To achieve this goal, we connected each vertex <inline-formula><mml:math id="d1e2305" altimg="si5.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> of the graph <inline-formula><mml:math id="d1e2310" altimg="si4.svg" display="inline"><mml:mi>G</mml:mi></mml:math></inline-formula> to itself in the previous and next time-step for the entire time period <inline-formula><mml:math id="d1e2315" altimg="si29.svg" display="inline"><mml:mi>M</mml:mi></mml:math></inline-formula> as illustrated in <xref rid="fig2" ref-type="fig">Fig. 2</xref>. This results in a larger spatial–temporal graph <inline-formula><mml:math id="d1e2325" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> where a given vertex at a particular time-step can aggregate information from another neighborhood vertex of previous and subsequent time-steps. The larger spatial–temporal graph also helps in capturing the long-range dependencies between the vertices of different time-steps.</p>
      <p id="d1e2334">The adjacency matrix of the spatial graph <inline-formula><mml:math id="d1e2341" altimg="si4.svg" display="inline"><mml:mi>G</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="d1e2346" altimg="si5.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> vertices at a given time-step <inline-formula><mml:math id="d1e2352" altimg="si13.svg" display="inline"><mml:mi>t</mml:mi></mml:math></inline-formula> is given as <inline-formula><mml:math id="d1e2357" altimg="si9.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Hence, the spatial–temporal synchronous graph <inline-formula><mml:math id="d1e2376" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> that we construct will have its adjacency matrix <inline-formula><mml:math id="d1e2386" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e2397" altimg="si38.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. In this large graph <inline-formula><mml:math id="d1e2425" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>, a given vertex’s index <inline-formula><mml:math id="d1e2435" altimg="si40.svg" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>p</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the spatial graph <inline-formula><mml:math id="d1e2456" altimg="si4.svg" display="inline"><mml:mi>G</mml:mi></mml:math></inline-formula> will be mapped to a new index in the large spatial–temporal graph <inline-formula><mml:math id="d1e2462" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>, and the new index will be: <inline-formula><mml:math id="d1e2472" altimg="si43.svg" display="inline"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e2493" altimg="si44.svg" display="inline"><mml:mrow><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>t</mml:mi><mml:mo>≤</mml:mo><mml:mi>M</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the time-step number in the larger spatial–temporal synchronous graph <inline-formula><mml:math id="d1e2514" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>.<fig id="fig2"><label>Fig. 2</label><caption><p>Spatial–Temporal Synchronous Graph <inline-formula><mml:math id="d1e36" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> generated from the historical <inline-formula><mml:math id="d1e46" altimg="si29.svg" display="inline"><mml:mi>M</mml:mi></mml:math></inline-formula> time steps.</p></caption><graphic xlink:href="gr2_lrg"/></fig><fig id="fig3"><label>Fig. 3</label><caption><p>Adjacency matrix <inline-formula><mml:math id="d1e59" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> of the Spatial–Temporal Synchronous Graph <inline-formula><mml:math id="d1e69" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>.</p></caption><graphic xlink:href="gr3_lrg"/></fig></p>
      <p id="d1e2523">In the large spatial–temporal synchronous graph <inline-formula><mml:math id="d1e2526" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>, when a given vertex at time-step <inline-formula><mml:math id="d1e2536" altimg="si47.svg" display="inline"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is connected to itself at subsequent time-step <inline-formula><mml:math id="d1e2546" altimg="si48.svg" display="inline"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the corresponding value in the large adjacency matrix <inline-formula><mml:math id="d1e2557" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> with <inline-formula><mml:math id="d1e2567" altimg="si50.svg" display="inline"><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> vertices is set to be 1. Hence, the adjacency matrix of <inline-formula><mml:math id="d1e2575" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> is formulated as: <disp-formula id="fd2"><label>(2)</label><mml:math id="d1e2591" altimg="si52.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable align="axis" class="array" columnlines="none" equalcolumns="false" equalrows="false"><mml:mtr><mml:mtd class="array" columnalign="left"><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd><mml:mtd class="array" columnalign="left"><mml:mtext>if </mml:mtext><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext> connects to </mml:mtext><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="array" columnalign="left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd><mml:mtd class="array" columnalign="left"><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="d1e2653" altimg="si55.svg" display="inline"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e2663" altimg="si56.svg" display="inline"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the same vertex <inline-formula><mml:math id="d1e2673" altimg="si57.svg" display="inline"><mml:mi>i</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="d1e2678" altimg="si58.svg" display="inline"><mml:mi>j</mml:mi></mml:math></inline-formula> respectively in subsequent time-steps of the spatial–temporal synchronous graph <inline-formula><mml:math id="d1e2684" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>.</p>
      <p id="d1e2693">We have shown the adjacency matrix <inline-formula><mml:math id="d1e2696" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> of the spatial–temporal synchronous graph <inline-formula><mml:math id="d1e2706" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> in <xref rid="fig3" ref-type="fig">Fig. 3</xref>. The diagonal of <inline-formula><mml:math id="d1e2721" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> are the adjacency matrices <inline-formula><mml:math id="d1e2731" altimg="si63.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the corresponding spatial graphs <inline-formula><mml:math id="d1e2749" altimg="si64.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for time-steps <inline-formula><mml:math id="d1e2767" altimg="si65.svg" display="inline"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> where <inline-formula><mml:math id="d1e2778" altimg="si66.svg" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>. Hence, we can say that <inline-formula><mml:math id="d1e2792" altimg="si67.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e2828" altimg="si68.svg" display="inline"><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="d1e2849" altimg="si69.svg" display="inline"><mml:mrow><mml:mi>k</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="d1e2858" altimg="si66.svg" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>.</p>
      <p id="d1e2871">The off-diagonal matrices of <inline-formula><mml:math id="d1e2874" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> indicate the connection between each vertex to itself in the previous or subsequent time-steps. Hence, we can say that the upper-triangular side of <inline-formula><mml:math id="d1e2884" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>, i.e. <inline-formula><mml:math id="d1e2894" altimg="si73.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="d1e2926" altimg="si74.svg" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is formed as: <inline-formula><mml:math id="d1e2949" altimg="si75.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="d1e2973" altimg="si76.svg" display="inline"><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e2998" altimg="si77.svg" display="inline"><mml:mrow><mml:mi>j</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>k</mml:mi><mml:mi>N</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="d1e3015" altimg="si78.svg" display="inline"><mml:mrow><mml:mi>i</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula>, for <inline-formula><mml:math id="d1e3025" altimg="si74.svg" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="d1e3048" altimg="si80.svg" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>p</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. In a similar manner, the lower triangular side of <inline-formula><mml:math id="d1e3062" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>, i.e. <inline-formula><mml:math id="d1e3073" altimg="si82.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="d1e3104" altimg="si74.svg" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is symmetric to the upper triangular side <inline-formula><mml:math id="d1e3127" altimg="si73.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="d1e3158" altimg="si74.svg" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. All the other elements are zero as indicated in <xref rid="fig3" ref-type="fig">Fig. 3</xref>.</p>
      <p id="d1e3185">In our approach, the spatial adjacency matrix <inline-formula><mml:math id="d1e3188" altimg="si7.svg" display="inline"><mml:mi>A</mml:mi></mml:math></inline-formula> for graph <inline-formula><mml:math id="d1e3193" altimg="si64.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at time-step <inline-formula><mml:math id="d1e3211" altimg="si65.svg" display="inline"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a weighted adjacency matrix formed by the physical distances between the vertices of <inline-formula><mml:math id="d1e3222" altimg="si64.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which we denote as the states of US or the counties in a state. Hence, for this adjacency matrix <inline-formula><mml:math id="d1e3240" altimg="si7.svg" display="inline"><mml:mi>A</mml:mi></mml:math></inline-formula>, the feature aggregation process of GCN in our model’s STSGT layer will not be affected as each vertex of <inline-formula><mml:math id="d1e3245" altimg="si64.svg" display="inline"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has a different influence on its neighboring vertices. But for the adjacency matrix <inline-formula><mml:math id="d1e3263" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> of the large spatial–temporal synchronous graph <inline-formula><mml:math id="d1e3274" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>, the off-diagonal matrices <inline-formula><mml:math id="d1e3284" altimg="si73.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="d1e3315" altimg="si82.svg" display="inline"><mml:mrow><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="d1e3347" altimg="si74.svg" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mi>k</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">≤</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) have entries that are <inline-formula><mml:math id="d1e3371" altimg="si97.svg" display="inline"><mml:mn>1</mml:mn></mml:math></inline-formula>. Our goal is to learn the temporal dependency between same vertices in adjacent time-steps (<inline-formula><mml:math id="d1e3377" altimg="si55.svg" display="inline"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e3387" altimg="si56.svg" display="inline"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) in an end-to-end fashion. This would make our feature aggregation process more general and flexible since the features between vertices of adjacent time-steps would be aggregated based on the learned temporal dependency. Hence, we define a mask matrix <inline-formula><mml:math id="d1e3397" altimg="si100.svg" display="inline"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> with a similar shape as <inline-formula><mml:math id="d1e3419" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>, i.e. <inline-formula><mml:math id="d1e3429" altimg="si102.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We calculate the Hadamard-product between the mask matrix and the adjacency matrix <inline-formula><mml:math id="d1e3468" altimg="si30.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> to form an updated matrix as follows: <disp-formula id="fd3"><label>(3)</label><mml:math id="d1e3484" altimg="si104.svg" display="block"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∘</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow></mml:math></disp-formula>where the mask matrix <inline-formula><mml:math id="d1e3542" altimg="si100.svg" display="inline"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> contains the learnable weights between the vertices of adjacent time-steps <inline-formula><mml:math id="d1e3563" altimg="si55.svg" display="inline"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="d1e3573" altimg="si56.svg" display="inline"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> during the training process, <inline-formula><mml:math id="d1e3583" altimg="si108.svg" display="inline"><mml:mo>∘</mml:mo></mml:math></inline-formula> denotes the Hadamard-product and <inline-formula><mml:math id="d1e3589" altimg="si109.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> denotes the updated adjacency matrix. Subsequently, <inline-formula><mml:math id="d1e3634" altimg="si112.svg" display="inline"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is used for all GCN operations on the spatial–temporal synchronous graph <inline-formula><mml:math id="d1e3661" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>.</p>
    </sec>
    <sec id="sec3.3">
      <label>3.3</label>
      <title>Spatial–temporal synchronous graph transformer network</title>
      <p id="d1e3675">The architecture of our proposed Spatial–Temporal Synchronous Transformer network (STSGT) is shown in <xref rid="fig4" ref-type="fig">Fig. 4</xref>.</p>
      <p id="d1e3681">The input of STSGT is generated from the graph signal matrix <inline-formula><mml:math id="d1e3688" altimg="si15.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> by considering <inline-formula><mml:math id="d1e3717" altimg="si29.svg" display="inline"><mml:mi>M</mml:mi></mml:math></inline-formula> continuous time-steps. Hence, we generate the spatial–temporal synchronous graph signal matrix <inline-formula><mml:math id="d1e3723" altimg="si115.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> which is basically the graph signal matrix of the large spatial–temporal synchronous graph <inline-formula><mml:math id="d1e3751" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>.<fig id="fig4"><label>Fig. 4</label><caption><p>The architecture of our proposed Spatial–Temporal Synchronous Graph Transformer network (STSGT).</p></caption><graphic xlink:href="gr4_lrg"/></fig><fig id="fig5"><label>Fig. 5</label><caption><p>The architecture of Spatial–Temporal Synchronous Graph Transformer layer. The adjacency matrix of the spatial–temporal synchronous graph <inline-formula><mml:math id="d1e95" altimg="si112.svg" display="inline"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is used by the GCN.</p></caption><graphic xlink:href="gr5_lrg"/></fig></p>
      <p id="d1e3760">The input graph signal matrix is passed through a single 1-D convolutional layer to map the input to a higher dimensional space. Hence, the output of the 1-D convolutional layer is the spatial–temporal synchronous graph signal matrix <inline-formula><mml:math id="d1e3763" altimg="si117.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="d1e3798" altimg="si118.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">&gt;</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:math></inline-formula>, and then <inline-formula><mml:math id="d1e3815" altimg="si119.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> is passed on to the STSGT layers.</p>
      <sec id="sec3.3.1">
        <label>3.3.1</label>
        <title>Spatial–Temporal Synchronous Graph Transformer layer</title>
        <p id="d1e3829">The architecture of the STSGT Layer is illustrated in <xref rid="fig5" ref-type="fig">Fig. 5</xref>.</p>
        <p id="d1e3835">Each layer comprises of the Spatial–Temporal Synchronous Transformer (STST) for calculating the self-attention between the vertices of the spatial–temporal synchronous graph <inline-formula><mml:math id="d1e3838" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>. A conventional GCN follows the STST to aggregate the information obtained through self-attention mechanism, from neighboring vertices of all vertices of <inline-formula><mml:math id="d1e3848" altimg="si119.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>. A GCN layer on the output of STST is defined as: <disp-formula id="fd4"><label>(4)</label><mml:math id="d1e3864" altimg="si122.svg" display="block"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mi>W</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="d1e3931" altimg="si123.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the output of STST, <inline-formula><mml:math id="d1e3969" altimg="si109.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the updated spatial–temporal synchronous adjacency matrix, <inline-formula><mml:math id="d1e4014" altimg="si125.svg" display="inline"><mml:mrow><mml:mi>W</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the learnable weight matrix, <inline-formula><mml:math id="d1e4045" altimg="si126.svg" display="inline"><mml:mrow><mml:mi>b</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the learnable bias, <inline-formula><mml:math id="d1e4068" altimg="si127.svg" display="inline"><mml:mi>σ</mml:mi></mml:math></inline-formula> is the non-linearity such as ReLU applied after the graph convolution, and <inline-formula><mml:math id="d1e4073" altimg="si128.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the output of the GCN layer.</p>
        <p id="d1e4114">Additionally, residual connections are added as shown in <xref rid="fig5" ref-type="fig">Fig. 5</xref> to allow the gradients to back-propagate to the initial layers of the model during the training process. Finally, the output of a STSGT layer is an updated spatial–temporal graph signal matrix <inline-formula><mml:math id="d1e4121" altimg="si128.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> which is passed on to the next STSGT layer.</p>
      </sec>
      <sec id="sec3.3.2">
        <label>3.3.2</label>
        <title>Temporal and spatial encodings</title>
        <p id="d1e4167">Temporal and spatial encodings are added to the input spatial–temporal graph signal matrix <inline-formula><mml:math id="d1e4170" altimg="si117.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, before passing it on to the first STSGT layer. This helps the model to distinguish between the different time-steps (temporal encodings) and the different vertices of a given time-step (spatial encodings). These temporal and spatial encodings are added as follows: <disp-formula id="fd5"><label>(5)</label><mml:math id="d1e4211" altimg="si131.svg" display="block"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak">=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>where the graph signal matrix after temporal and spatial encoding is <inline-formula><mml:math id="d1e4257" altimg="si117.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The temporal encoding added is <inline-formula><mml:math id="d1e4293" altimg="si133.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> which basically assigns same values for all vertices <inline-formula><mml:math id="d1e4332" altimg="si5.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> of a given time-step <inline-formula><mml:math id="d1e4337" altimg="si13.svg" display="inline"><mml:mi>t</mml:mi></mml:math></inline-formula> and different values for vertices in different time-steps. The spatial encoding added is <inline-formula><mml:math id="d1e4342" altimg="si136.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> which basically assigns same values for a given vertex <inline-formula><mml:math id="d1e4382" altimg="si5.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> across all time-steps and different values for vertices in the same time-step <inline-formula><mml:math id="d1e4387" altimg="si13.svg" display="inline"><mml:mi>t</mml:mi></mml:math></inline-formula>.</p>
      </sec>
      <sec id="sec3.3.3">
        <label>3.3.3</label>
        <title>Spatial–temporal synchronous self-attention</title>
        <p id="d1e4396">The architecture of the Spatial–Temporal Synchronous Transformer (STST) within each STSGT layer is illustrated in <xref rid="fig6" ref-type="fig">Fig. 6</xref> (left). First, the layer normalization is applied to the input graph signal matrix <inline-formula><mml:math id="d1e4403" altimg="si117.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Subsequently, the output of the layer norm is passed to a multi-head spatial–temporal synchronous self-attention mechanism as illustrated in <xref rid="fig6" ref-type="fig">Fig. 6</xref> (right). Layer normalization is applied again to the output of the multi-head self-attention. Finally, a three layer Multi-Layer Perceptron (MLP) is used to output the encoded graph signal matrix <inline-formula><mml:math id="d1e4443" altimg="si123.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
        <p id="d1e4480">The architecture of each head of the spatial–temporal synchronous self-attention mechanism is illustrated in <xref rid="fig6" ref-type="fig">Fig. 6</xref> (right). The input to each head is the spatial–temporal graph signal matrix <inline-formula><mml:math id="d1e4487" altimg="si117.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. This matrix is reshaped to <inline-formula><mml:math id="d1e4522" altimg="si142.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Then <inline-formula><mml:math id="d1e4556" altimg="si119.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> is projected to three higher dimensional latent subspaces by generating the Query(Q), Key(K) and Value(V) matrices. The Q, K and V matrices are generated as follows: <disp-formula id="fd6"><label>(6)</label><mml:math id="d1e4572" altimg="si144.svg" display="block"><mml:mrow><mml:mi>Q</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mspace width="2em"/><mml:mi>K</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mspace width="2em"/><mml:mi>V</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="d1e4633" altimg="si145.svg" display="inline"><mml:mrow><mml:mi>Q</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="d1e4659" altimg="si146.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4696" altimg="si147.svg" display="inline"><mml:mrow><mml:mi>K</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="d1e4722" altimg="si148.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="d1e4758" altimg="si149.svg" display="inline"><mml:mrow><mml:mi>V</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="d1e4784" altimg="si150.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="d1e4821" altimg="si151.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> and the projection matrices <inline-formula><mml:math id="d1e4847" altimg="si152.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are feed forward neural networks.</p>
        <p id="d1e4875">In a similar manner as the encoder of the transformer (<xref rid="b33" ref-type="bibr">Vaswani et al., 2017</xref>), we calculate the similarity or attention score <inline-formula><mml:math id="d1e4885" altimg="si153.svg" display="inline"><mml:mi>S</mml:mi></mml:math></inline-formula> between the query and key matrices in the higher dimensional latent subspace. The attention score <inline-formula><mml:math id="d1e4891" altimg="si153.svg" display="inline"><mml:mi>S</mml:mi></mml:math></inline-formula> is calculated as: <disp-formula id="fd7"><label>(7)</label><mml:math id="d1e4902" altimg="si155.svg" display="block"><mml:mrow><mml:mi>S</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="d1e4947" altimg="si156.svg" display="inline"><mml:mrow><mml:mi>S</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the attention score obtained from self-attention between all the vertices of the spatial–temporal synchronous graph <inline-formula><mml:math id="d1e4970" altimg="si28.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> with graph signal matrix <inline-formula><mml:math id="d1e4981" altimg="si119.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>. <inline-formula><mml:math id="d1e4991" altimg="si159.svg" display="inline"><mml:mi>Q</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="d1e4996" altimg="si160.svg" display="inline"><mml:mi>K</mml:mi></mml:math></inline-formula> are the query and key matrices respectively, and <inline-formula><mml:math id="d1e5001" altimg="si161.svg" display="inline"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the dimension of the key matrix <inline-formula><mml:math id="d1e5012" altimg="si160.svg" display="inline"><mml:mi>K</mml:mi></mml:math></inline-formula>. The softmax function normalizes the scores and the scaling factor <inline-formula><mml:math id="d1e5017" altimg="si163.svg" display="inline"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:math></inline-formula> prevents the saturation due to the softmax function.<fig id="fig6"><label>Fig. 6</label><caption><p>(Left) The architecture of STST and (right) The architecture of one head of the Spatial–Temporal Synchronous Self-Attention within each STST.</p></caption><graphic xlink:href="gr6_lrg"/></fig></p>
        <p id="d1e5033">Finally, the self-attention scores are multiplied by the value matrix <inline-formula><mml:math id="d1e5036" altimg="si2.svg" display="inline"><mml:mi>V</mml:mi></mml:math></inline-formula> to update the features of each vertex of <inline-formula><mml:math id="d1e5041" altimg="si119.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula>. The update is done as follows: <disp-formula id="fd8"><label>(8)</label><mml:math id="d1e5057" altimg="si166.svg" display="block"><mml:mrow><mml:mi>L</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>S</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="d1e5070" altimg="si153.svg" display="inline"><mml:mi>S</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="d1e5075" altimg="si2.svg" display="inline"><mml:mi>V</mml:mi></mml:math></inline-formula> are the attention score and value matrix respectively. The matrix <inline-formula><mml:math id="d1e5080" altimg="si169.svg" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> obtained with Eq. <xref rid="fd8" ref-type="disp-formula">(8)</xref> is passed through a fully connected layer to update the output spatial–temporal synchronous graph signal matrix <inline-formula><mml:math id="d1e5112" altimg="si170.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
      </sec>
      <sec id="sec3.3.4">
        <label>3.3.4</label>
        <title>Forecasting the future COVID status</title>
        <p id="d1e5151">We forecast the future values of the spatial–temporal COVID time series with an output layer (see <xref rid="fig4" ref-type="fig">Fig. 4</xref>) which consists of two 1D convolution layers to map the output of the last STSGT layer to the original dimension of the time series data. Let us denote the output of the last STSGT layer in <xref rid="fig4" ref-type="fig">Fig. 4</xref> as <inline-formula><mml:math id="d1e5162" altimg="si171.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, which is then transposed and reshaped to <inline-formula><mml:math id="d1e5196" altimg="si172.svg" display="inline"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We then use two 1D convolution layers on <inline-formula><mml:math id="d1e5232" altimg="si173.svg" display="inline"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> as follows: <disp-formula id="fd9"><label>(9)</label><mml:math id="d1e5253" altimg="si174.svg" display="block"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>~</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="d1e5316" altimg="si175.svg" display="inline"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is output of our STSGT model, <inline-formula><mml:math id="d1e5341" altimg="si127.svg" display="inline"><mml:mi>σ</mml:mi></mml:math></inline-formula> is the ReLU non-linearity, <inline-formula><mml:math id="d1e5346" altimg="si177.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="d1e5384" altimg="si178.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are the learnable weights and bias of the first 1D convolution layer and <inline-formula><mml:math id="d1e5411" altimg="si179.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="d1e5443" altimg="si180.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are the learnable weights and bias of the second 1D convolution layer respectively. Here, <inline-formula><mml:math id="d1e5463" altimg="si181.svg" display="inline"><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> is the hidden layer dimension of the output layers and <inline-formula><mml:math id="d1e5475" altimg="si18.svg" display="inline"><mml:mi>H</mml:mi></mml:math></inline-formula> is the length of the forecasting horizon. Finally, the output <inline-formula><mml:math id="d1e5480" altimg="si20.svg" display="inline"><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:math></inline-formula> is transposed to <inline-formula><mml:math id="d1e5491" altimg="si184.svg" display="inline"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mo linebreak="goodbreak" linebreakstyle="after">∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
        <p id="d1e5519">It should be noted that we forecast all the <inline-formula><mml:math id="d1e5522" altimg="si18.svg" display="inline"><mml:mi>H</mml:mi></mml:math></inline-formula> horizon time-steps <inline-formula><mml:math id="d1e5527" altimg="si186.svg" display="inline"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo>ˆ</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>H</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> at the same time instead of recursively forecasting one of the horizon time-step at a given time as done in previous works (<xref rid="b23" ref-type="bibr">Li, Yu, et al., 2018</xref>, <xref rid="b30" ref-type="bibr">Song et al., 2020</xref>, <xref rid="b41" ref-type="bibr">Yu et al., 2018</xref>).</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4</label>
    <title>Experiments</title>
    <p id="d1e5611">We evaluated our STSGT framework with three main tasks: (1) forecasting the COVID-19 daily infected cases for the 50 states of US and Washington, D.C.; (2) forecasting the COVID-19 daily infected cases at the state level with 83 counties for the state of Michigan; and (3) forecasting the COVID-19 daily death cases for the 50 states of US and Washington, D.C. We compared the performance of our proposed approach with baselines and state-of-the-art spatial–temporal forecasting algorithms. All our experiments were performed using the PyTorch 1.8.0 framework (<xref rid="b27" ref-type="bibr">Paszke et al., 2019</xref>).</p>
    <sec id="sec4.1">
      <label>4.1</label>
      <title>Datasets</title>
      <sec id="sec4.1.1">
        <label>4.1.1</label>
        <title>John Hopkins University (JHU) COVID-19 dataset</title>
        <p id="d1e5628">We used the publicly available COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University (<ext-link ext-link-type="uri" xlink:href="https://github.com/CSSEGISandData/COVID-19" id="interref2">https://github.com/CSSEGISandData/COVID-19</ext-link>) (<xref rid="b9" ref-type="bibr">Dong, Du, &amp; Gardner, 2020</xref>). Specifically, we used the time series dataset which is updated regularly with the number of infected and death cases across all states of the US. We used the time series data from March 15, 2020 to Nov 30, 2021 spanning 626 days.</p>
      </sec>
      <sec id="sec4.1.2">
        <label>4.1.2</label>
        <title>New York Times (NYT) COVID-19 dataset</title>
        <p id="d1e5643">We used the publicly available data from The New York Times, based on reports from state and local health agencies (<xref rid="b32" ref-type="bibr"><italic>The New York Times (2021). Coronavirus (COVID-19) data in the united states</italic>, 2021</xref>). Similar to the JHU dataset, we used the time series data for the daily infected and death cases across all US states. We used the time series data from March 18, 2020 to Nov 30, 2021 spanning 623 days.</p>
      </sec>
    </sec>
    <sec id="sec4.2">
      <label>4.2</label>
      <title>Comparison methods</title>
      <sec id="sec4.2.1">
        <label>4.2.1</label>
        <title>Baseline method</title>
        <p id="d1e5662">We used the following baseline method for spatial–temporal forecasting, to compare with our proposed approach.</p>
        <p id="d1e5664">
          <list list-type="simple" id="d1e5666">
            <list-item id="lst4">
              <label>•</label>
              <p id="d1e5670"><bold>Auto Regressive Integrated Moving Average (ARIMA)</bold> (<xref rid="b23" ref-type="bibr">Li, Yu, et al., 2018</xref>) - ARIMA consists of an auto-regressive (AR) and a moving average (MA) component and is used to forecast future values of a stationary time series. For our experiments, we used an order of (5, 1, 0) for ARIMA. That is, we used 5 AR and 0 MA components with first order difference (d=1) on the input time series. We used ARIMA as a baseline comparison method since it is one of the most common algorithms for time-series analysis, and it has been extensively used as a comparison method by several state-of-the-art spatial–temporal forecasting algorithms.</p>
            </list-item>
          </list>
        </p>
      </sec>
      <sec id="sec4.2.2">
        <label>4.2.2</label>
        <title>State-of-the-art Spatial–temporal Forecasting Algorithms</title>
        <p id="d1e5684">We compared our proposed approach with the following state-of-the-art methods that were designed for spatial–temporal forecasting. For each of the models below, the number of features or channels for each vertex of the graph is 1, which is the daily infected or death cases. The number of vertices for the graph per time-step is 51 (50 US states and Washington, D.C.) and 83 (for Michigan). The history (M) and horizon (H) for the sliding window is selected as 12 for all the spatial–temporal forecasting algorithms below. The default values are used for the hyper-parameters in each model, which we detailed in their descriptions below.</p>
        <p id="d1e5686">
          <list list-type="simple" id="d1e5688">
            <list-item id="lst5">
              <label>•</label>
              <p id="d1e5692"><bold>Spatial–Temporal Graph Convolution Network (STGCN)</bold> (<xref rid="b41" ref-type="bibr">Yu et al., 2018</xref>) - STGCN uses the spatial–temporal convolution blocks to forecast the horizon. Each of its spatial–temporal blocks consists of spectral graph convolution to capture the spatial dependency between vertices of same time-step and an independent temporal gated convolution block to capture the temporal dependency between the time-steps. We used the STGCN implementation (<ext-link ext-link-type="uri" xlink:href="https://github.com/FelixOpolka/STGCN-PyTorch" id="interref3">https://github.com/FelixOpolka/STGCN-PyTorch</ext-link>) with 16 spatial and output channels for each STGCN block.</p>
            </list-item>
            <list-item id="lst6">
              <label>•</label>
              <p id="d1e5707"><bold>Attention-based Spatial–Temporal Graph Convolution Network (ASTGCN(r))</bold> (<xref rid="b12" ref-type="bibr">Guo et al., 2019</xref>) - ASTGCN concatenates the output of several spatial–temporal blocks used independently on the hourly, daily and weekly spatial–temporal data to forecast the next hour status. Each of the spatial–temporal blocks consists of spectral graph convolution to capture the spatial dependency and a combination of spatial and temporal attention mechanism to capture the dynamic spatial and temporal correlations of the spatial–temporal data. We used only the recent component of ASTGCN (i.e., ASTGCN(r)), equivalent to only using the hourly data, to have a fair comparison with other methods. We used the ASTGCN implementation (<ext-link ext-link-type="uri" xlink:href="https://github.com/guoshnBJTU/ASTGCN-r-pytorch" id="interref4">https://github.com/guoshnBJTU/ASTGCN-r-pytorch</ext-link>) with 2 ASTGCN blocks, 16 time filters with stride 1 and 16 Chebyshev filters of order 2.</p>
            </list-item>
            <list-item id="lst7">
              <label>•</label>
              <p id="d1e5722"><bold>Graph WaveNet</bold> (<xref rid="b35" ref-type="bibr">Wu et al., 2019</xref>) - Graph WaveNet comprises of multiple layers with skip connections where each layer consists of two gated temporal convolutional networks (TCN) (<xref rid="b20" ref-type="bibr">Lea et al., 2016</xref>). A dilation factor is used for each TCN to consider the long-range temporal dependencies of the time series. The gated TCN is followed by an independent graph convolutional layer to capture the spatial dependencies. We used the Graph WaveNet implementation (<ext-link ext-link-type="uri" xlink:href="https://github.com/nnzhan/Graph-WaveNet" id="interref5">https://github.com/nnzhan/Graph-WaveNet</ext-link>) with the “double transition” adjacency matrix. We used 2 blocks with each block having 2 layers. We used 16 residual and dilation channels along with 128 skip channels and 256 end channels. A <inline-formula><mml:math id="d1e5741" altimg="si187.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> kernel size was used for the filter and gate convolutions.</p>
            </list-item>
            <list-item id="lst8">
              <label>•</label>
              <p id="d1e5757"><bold>Spatial–Temporal Transformer Network (STTN)</bold> (<xref rid="b37" ref-type="bibr">Xu et al., 2020</xref>) - STTN has several spatial–temporal blocks, and each of these blocks consists of an independent spatial and temporal transformer. The spatial transformer combines self-attention with a GCN using a gated mechanism to capture the spatial dependency. The output of the spatial transformer is passed to the temporal transformer which uses the self-attention mechanism to capture the temporal dependencies. We used the STTN implementation (<ext-link ext-link-type="uri" xlink:href="https://github.com/wubin5/STTN" id="interref6">https://github.com/wubin5/STTN</ext-link>) with 2 layers of spatial and temporal transformer, embedding size of 16 for each vertex of the graph, a 400-dimensional temporal encoding for the temporal transformer, a Chebyshev polynomial of order 3 for the spatial GCN and 2 heads for the multi-head self-attention for both spatial and temporal transformer.</p>
            </list-item>
          </list>
        </p>
      </sec>
    </sec>
    <sec id="sec4.3">
      <label>4.3</label>
      <title>Evaluation metrics</title>
      <p id="d1e5774">In this paper, we use the following three metrics to evaluate the performance of all the models. The Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Root Mean Square Log Error (RMSLE) is defined as: <disp-formula id="fd10"><label>(10)</label><mml:math id="d1e5783" altimg="si188.svg" display="block"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="("><mml:mrow><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>∣</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd11"><label>(11)</label><mml:math id="d1e5852" altimg="si189.svg" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula>
<disp-formula id="fd12"><label>(12)</label><mml:math id="d1e5927" altimg="si190.svg" display="block"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo linebreak="badbreak">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mo class="qopname">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mo class="qopname">log</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>i</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula>where, <inline-formula><mml:math id="d1e6018" altimg="si191.svg" display="inline"><mml:mo class="qopname">log</mml:mo></mml:math></inline-formula> denotes the natural logarithm.</p>
      <p id="d1e6022">In all of the above equations, <inline-formula><mml:math id="d1e6025" altimg="si5.svg" display="inline"><mml:mi>N</mml:mi></mml:math></inline-formula> is the total number of samples in the dataset, <inline-formula><mml:math id="d1e6030" altimg="si193.svg" display="inline"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the number of ground truth cases and <inline-formula><mml:math id="d1e6040" altimg="si194.svg" display="inline"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the forecasted number of cases. A lower MAE, RMSLE and RMSE indicate that the forecasted number of cases is closer to the ground truth number of cases.</p>
    </sec>
    <sec id="sec4.4">
      <label>4.4</label>
      <title>Implementation details</title>
      <p id="d1e6054">For all our experiments, we used the past 12 days history (M=12) to forecast the next 12 days horizon (H=12) using the sliding window approach shown in <xref rid="fig1" ref-type="fig">Fig. 1</xref>. The rationale behind the choice of 12 days history and horizon is that most of the recent state-of-the-art spatial–temporal forecasting algorithms (<xref rid="b12" ref-type="bibr">Guo et al., 2019</xref>, <xref rid="b35" ref-type="bibr">Wu et al., 2019</xref>, <xref rid="b41" ref-type="bibr">Yu et al., 2018</xref>), use the previous 12 time-steps to forecast the next 3, 6, 9 or 12 time-steps. We generate the adjacency matrix <inline-formula><mml:math id="d1e6066" altimg="si7.svg" display="inline"><mml:mi>A</mml:mi></mml:math></inline-formula> for the spatial graph <inline-formula><mml:math id="d1e6072" altimg="si4.svg" display="inline"><mml:mi>G</mml:mi></mml:math></inline-formula> at a given time-step by the euclidean distances between the states of the US. This distance is calculated by considering the coordinates of each state which in turn is the average of the latitude and longitude of all the counties for a given state. After generating the adjacency matrix, we normalized it with the maximum value and used a maximum threshold value of 0.3 to generate the final adjacency matrix for all US states and the counties at the state level. The same adjacency matrix was used for both JHU and NYT datasets to forecast the daily infected and death cases for all 50 US states and Washington, D.C. A separate adjacency matrix was generated at the state level, e.g. the state of Michigan, to forecast the daily infected cases for all the counties.</p>
      <p id="d1e6076">We divided the entire dataset in chronological order with 80% training, 10% validation and 10% testing. In other words, we used the data from March 15, 2020 to July 28, 2021 as the training set, July 29 to Sept 28, 2021 as the validation set and Sept 29 to Nov 30, 2021 as the testing set. For the NYT dataset, March 18, 2020 was used as the start date as the cases for one of the states were not reported till this date. It should be noted that cross-validation cannot be performed since the validation dataset has to follow the chronological order. Z-score normalization was applied to the inputs, i.e., the train, validation and test datasets were separately normalized by subtracting the mean and dividing by the standard deviation. We used the Mean Absolute Error (MAE) as the loss function and trained all the models for <inline-formula><mml:math id="d1e6079" altimg="si197.svg" display="inline"><mml:mrow><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> epochs with early stopping by monitoring the MAE on the validation set. We clipped the gradients to <inline-formula><mml:math id="d1e6085" altimg="si198.svg" display="inline"><mml:mrow><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> and used a batch size of 16 for training all models with a learning rate of 0.001 without using any learning rate decay.</p>
      <p id="d1e6096">The performance of our proposed STSGT framework is dependent on several hyper-parameters. These hyper-parameters include the number of STSGT layers, the number of attention heads for multi-head attention in each STST layer, the dimension of the Query(Q), Key(K) and Value(V) matrices for calculating the self-attention within each attention head, and the length of the history time-steps (M) for 12 horizon time-steps (H=12) in the sliding window (see <xref rid="fig1" ref-type="fig">Fig. 1</xref>). We empirically decided these hyper-parameters such that they are consistent with other spatial–temporal forecasting algorithms and thus we can have a fair comparison. Specifically, we used 2 STSGT layers in our model. Within each STSGT layer, we used 2 STST+GCN layer combinations. For the self-attention, we selected the Query(Q), Key(K) and Value(V) dimensions to be 16 with 2 attention heads in each of the STST layers. Additionally, for the sliding window with 12 horizon time-steps (H=12), we choose 12 historical time-steps (M=12).</p>
    </sec>
    <sec id="sec4.5">
      <label>4.5</label>
      <title>Experimental results</title>
      <sec id="sec4.5.1">
        <label>4.5.1</label>
        <title>Forecasting JHU COVID-19 daily infected cases for US states</title>
        <p id="d1e6112">The results of using our proposed STSGT model and other models to forecast the daily infected cases for the 50 US states and Washington, D.C. using the JHU dataset are summarized in <xref rid="tbl1" ref-type="table">Table 1</xref>. We summed the daily infected cases for all counties of a given state to generate the ground truth number of infected cases for that state. Clearly, our proposed STSGT significantly outperformed all the other models by achieving the lowest MAE, RMSLE and RMSE for both short-term horizon (next day, <inline-formula><mml:math id="d1e6119" altimg="si199.svg" display="inline"><mml:mrow><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) and long-term horizon indicated by the mean over the next 12 days (<inline-formula><mml:math id="d1e6130" altimg="si200.svg" display="inline"><mml:mrow><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula>). When comparing with the second best model ASTGCN(r), we observe that the mean MAE over the long-term, i.e. 12 days horizon, reduces by 131.22 while the RMSLE and RMSE reduces by 0.081 and 397.81 respectively. An interesting observation is that ASTGCN(r) and our proposed STSGT can better forecast the longer time horizons for all the states of US, compared to other models as seen by a lower 12 days mean MAE when compared to day 1 MAE.</p>
        <p id="d1e6139">A comparison of the average infected cases over the next 12 days horizon <inline-formula><mml:math id="d1e6144" altimg="si210.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>12</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> for four of the most affected US states due to COVID, is shown in <xref rid="fig7" ref-type="fig">Fig. 7</xref>. We compared the average ground truth and forecasted average infected cases for the states of New York, Florida, California and Texas for a period of 12 days ranging from Nov 6–17, 2021, which is a time frame from our testing set. We use the past 12 days historical data <inline-formula><mml:math id="d1e6162" altimg="si208.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>12</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> ranging from Oct 25–Nov 5, 2021, to forecast the next 12 days horizon <inline-formula><mml:math id="d1e6177" altimg="si210.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>12</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> infected cases (Nov 6–17, 2021). In <xref rid="fig7" ref-type="fig">Fig. 7</xref>, we observe that the average number of forecasted infected cases with our proposed STSGT is much closer to the ground truth number of average infected cases for most of the states, when compared to state-of-the-art spatial–temporal forecasting methods.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Results demonstrating the MAE, RMSLE and RMSE of the daily infected cases of 50 US states and Washington, D.C. with JHU data. The lowest MAE, RMSLE and RMSE are marked in <bold>bold</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Algorithm</th><th align="left">Forecasting <break/>horizon</th><th align="left">MAE</th><th align="left">RMSLE</th><th align="left">RMSE</th></tr></thead><tbody><tr><td align="left" rowspan="2">ARIMA (<xref rid="b23" ref-type="bibr">Li, Yu, et al., 2018</xref>)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e202" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left">2204.42</td><td align="left">5.996</td><td align="left">3724.19</td></tr><tr><td align="left">12 Days Mean</td><td align="left">2206.14</td><td align="left">6.069</td><td align="left">3895.20</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left" rowspan="2">STGCN (<xref rid="b41" ref-type="bibr">Yu et al., 2018</xref>)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e241" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left">1243.48</td><td align="left">0.869</td><td align="left">2518.78</td></tr><tr><td align="left">12 Days Mean</td><td align="left">1296.86</td><td align="left">0.905</td><td align="left">2369.93</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left" rowspan="2">ASTGCN(r) (<xref rid="b12" ref-type="bibr">Guo et al., 2019</xref>)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e280" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left">1078.06</td><td align="left">0.822</td><td align="left">2369.56</td></tr><tr><td align="left">12 Days Mean</td><td align="left">1076.29</td><td align="left">0.903</td><td align="left">2102.68</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left" rowspan="2">Graph <break/>WaveNet (<xref rid="b35" ref-type="bibr">Wu et al., 2019</xref>)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e321" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left">1140.02</td><td align="left">0.917</td><td align="left">2203.16</td></tr><tr><td align="left">12 Days Mean</td><td align="left">1315.56</td><td align="left">0.969</td><td align="left">2263.27</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left" rowspan="2">STTN (<xref rid="b37" ref-type="bibr">Xu et al., 2020</xref>)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e360" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left">1172.34</td><td align="left">0.948</td><td align="left">2475.33</td></tr><tr><td align="left">12 Days Mean</td><td align="left">1198.16</td><td align="left">0.984</td><td align="left">2191.91</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left" rowspan="2">STSGT <break/>(ours)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e396" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left"><bold>978.390</bold></td><td align="left"><bold>0.802</bold></td><td align="left"><bold>2129.74</bold></td></tr><tr><td align="left">12 Days Mean</td><td align="left"><bold>945.070</bold></td><td align="left"><bold>0.822</bold></td><td align="left"><bold>1704.87</bold></td></tr></tbody></table></table-wrap></p>
        <p id="d1e6194">
          <fig id="fig7">
            <label>Fig. 7</label>
            <caption>
              <p>A comparison of the average infected cases for the next 12 days horizon <inline-formula><mml:math id="d1e139" altimg="si210.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>12</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> for four US states.</p>
            </caption>
            <graphic xlink:href="gr7_lrg"/>
          </fig>
        </p>
      </sec>
      <sec id="sec4.5.2">
        <label>4.5.2</label>
        <title>Forecasting JHU COVID-19 daily death cases for US states</title>
        <p id="d1e6202">The results of using our proposed STSGT model and other models to forecast the daily death cases for the 50 US states and Washington, D.C. using the JHU dataset are summarized in <xref rid="tbl2" ref-type="table">Table 2</xref>. Similar to <xref rid="tbl1" ref-type="table">Table 1</xref>, we summed the daily death cases for all counties of a given state to generate the ground truth number of death cases for that state. Our proposed STSGT outperformed all the other models by achieving the lowest MAE for the short-term horizon (H=1) and the lowest MAE, RMSLE and RMSE for the long-term horizon indicated by the mean over the next 12 days (H=12). When comparing with the second best model ASTGCN(r), we observe that the mean MAE over the long-term, i.e. 12 days horizon, reduces by 0.73 while the RMSLE and RMSE reduces by 0.005 and 0.11 respectively. We observe that even though ASTGCN(r) achieves lower RMSLE and RMSE for forecasting the next day (day 1) death cases, our proposed model STSGT could better forecast the death cases over the long-term (12 days) as observed by a lower MAE, RMSLE and RMSE.</p>
        <p id="d1e6212">
          <fig id="fig8">
            <label>Fig. 8</label>
            <caption>
              <p>Daily Infected cases for Michigan’s Wayne County (left) and Oakland County (right), from October 4 to November 5, 2021.</p>
            </caption>
            <graphic xlink:href="gr8_lrg"/>
          </fig>
          <table-wrap position="float" id="tbl2">
            <label>Table 2</label>
            <caption>
              <p>Results demonstrating the MAE, RMSLE and RMSE of the daily death cases of 50 US states and Washington, D.C. with JHU data. The lowest MAE, RMSLE and RMSE are marked in <bold>bold</bold>.</p>
            </caption>
            <table frame="hsides" rules="groups">
              <thead>
                <tr>
                  <th align="left">Algorithm</th>
                  <th align="left">Forecasting <break/>horizon</th>
                  <th align="left">MAE</th>
                  <th align="left">RMSLE</th>
                  <th align="left">RMSE</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="2">ARIMA (<xref rid="b23" ref-type="bibr">Li, Yu, et al., 2018</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e472" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">42.62</td>
                  <td align="left">2.753</td>
                  <td align="left">74.89</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">43.66</td>
                  <td align="left">2.698</td>
                  <td align="left">110.60</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">STGCN (<xref rid="b41" ref-type="bibr">Yu et al., 2018</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e511" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">26.25</td>
                  <td align="left">0.966</td>
                  <td align="left">55.58</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">25.23</td>
                  <td align="left">0.952</td>
                  <td align="left">54.15</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">ASTGCN(r) (<xref rid="b12" ref-type="bibr">Guo et al., 2019</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e550" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">21.20</td>
                  <td align="left">
                    <bold>0.849</bold>
                  </td>
                  <td align="left">
                    <bold>47.73</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">21.36</td>
                  <td align="left">0.873</td>
                  <td align="left">47.25</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">Graph <break/>WaveNet (<xref rid="b35" ref-type="bibr">Wu et al., 2019</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e593" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">23.02</td>
                  <td align="left">0.889</td>
                  <td align="left">48.99</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">25.03</td>
                  <td align="left">0.922</td>
                  <td align="left">51.16</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">STTN (<xref rid="b37" ref-type="bibr">Xu et al., 2020</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e632" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">21.31</td>
                  <td align="left">0.863</td>
                  <td align="left">48.52</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">22.05</td>
                  <td align="left">0.892</td>
                  <td align="left">48.23</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">STSGT <break/>(ours)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e668" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">
                    <bold>20.73</bold>
                  </td>
                  <td align="left">0.855</td>
                  <td align="left">48.30</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">
                    <bold>20.63</bold>
                  </td>
                  <td align="left">
                    <bold>0.868</bold>
                  </td>
                  <td align="left">
                    <bold>47.14</bold>
                  </td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
        </p>
      </sec>
      <sec id="sec4.5.3">
        <label>4.5.3</label>
        <title>Forecasting JHU COVID-19 daily infected cases for Michigan</title>
        <p id="d1e6222">The results of using our proposed STSGT model and other models to forecast the daily infected cases at the state level, e.g., the state of Michigan, using the JHU dataset, are summarized in <xref rid="tbl3" ref-type="table">Table 3</xref>. In this case, the daily infected cases of the individual counties of Michigan served as the ground truth. We excluded the correctional facilities, counties denoted as “Out of Michigan” and unassigned counties, thus resulting in 83 counties in total. Hence, we used 83 vertices for the graph per time-step (i.e. for each day). As shown in <xref rid="tbl3" ref-type="table">Table 3</xref>, our proposed STSGT significantly outperformed all the other models by achieving the lowest MAE, RMSLE and RMSE for both short-term horizon (H=1) and long-term horizon indicated by the mean over the next 12 days (H=12). When comparing with the second best model STTN, we observe that the mean MAE over the long-term horizon (12 days) reduces by 9.86 while the RMSLE and RMSE reduces by 0.267 and 17.49 respectively. This experiment clearly demonstrates that our model could perform very well at the state level and hence our model could be used for forecasting infected and death cases for any US state.</p>
        <p id="d1e6232">The two most populated counties in Michigan are Wayne county with a population over 1.7 million and Oakland county with a population over 1.2 million. We plotted the ground truth and forecasted number of infected cases for Wayne and Oakland counties for a time frame in the testing set ranging from Oct 4–Nov 5, 2021 as shown in <xref rid="fig8" ref-type="fig">Fig. 8</xref>. We use the 12 days historical data <inline-formula><mml:math id="d1e6241" altimg="si208.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>M</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>12</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> to forecast the next day infected cases <inline-formula><mml:math id="d1e6255" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> in <xref rid="fig8" ref-type="fig">Fig. 8</xref>. Specifically, we use the data from Sept 22–Oct 3 to forecast the infected cases for Oct 4, then the data from Sept 24–Oct 5 to forecast the infected cases for Oct 6 and so on. We removed the days when the ground truth infected cases were zero as these cases were not reported in the JHU dataset for Michigan. In <xref rid="fig8" ref-type="fig">Fig. 8</xref>, we compare STSGT with other spatial–temporal forecasting algorithms and clearly observe that the forecasted number of infected cases with STSGT is much closer to the ground truth number of infected cases.<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>Results demonstrating the MAE, RMSLE and RMSE of the daily infected cases for 83 counties of the state of Michigan with JHU data. The lowest MAE, RMSLE and RMSE are marked in <bold>bold</bold>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Algorithm</th><th align="left">Forecasting <break/>horizon</th><th align="left">MAE</th><th align="left">RMSLE</th><th align="left">RMSE</th></tr></thead><tbody><tr><td align="left" rowspan="2">ARIMA (<xref rid="b23" ref-type="bibr">Li, Yu, et al., 2018</xref>)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e742" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left">124.52</td><td align="left">3.851</td><td align="left">270.78</td></tr><tr><td align="left">12 Days Mean</td><td align="left">142.46</td><td align="left">4.006</td><td align="left">322.06</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left" rowspan="2">STGCN (<xref rid="b41" ref-type="bibr">Yu et al., 2018</xref>)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e781" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left">48.30</td><td align="left">0.739</td><td align="left">95.99</td></tr><tr><td align="left">12 Days Mean</td><td align="left">57.31</td><td align="left">0.753</td><td align="left">126.73</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left" rowspan="2">ASTGCN(r) (<xref rid="b12" ref-type="bibr">Guo et al., 2019</xref>)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e820" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left">47.50</td><td align="left">0.772</td><td align="left">97.14</td></tr><tr><td align="left">12 Days Mean</td><td align="left">55.20</td><td align="left">0.785</td><td align="left">122.10</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left" rowspan="2">Graph <break/>WaveNet (<xref rid="b35" ref-type="bibr">Wu et al., 2019</xref>)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e861" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left">41.34</td><td align="left">0.658</td><td align="left">87.97</td></tr><tr><td align="left">12 Days Mean</td><td align="left">64.05</td><td align="left">0.879</td><td align="left">143.61</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left" rowspan="2">STTN (<xref rid="b37" ref-type="bibr">Xu et al., 2020</xref>)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e900" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left">42.50</td><td align="left">0.812</td><td align="left">81.37</td></tr><tr><td align="left">12 Days Mean</td><td align="left">49.33</td><td align="left">0.810</td><td align="left">108.31</td></tr><tr><td colspan="5"><hr/></td></tr><tr><td align="left" rowspan="2">STSGT <break/>(ours)</td><td align="left">Day 1 <inline-formula><mml:math id="d1e936" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left"><bold>22.04</bold></td><td align="left"><bold>0.430</bold></td><td align="left"><bold>40.78</bold></td></tr><tr><td align="left">12 Days Mean</td><td align="left"><bold>39.47</bold></td><td align="left"><bold>0.543</bold></td><td align="left"><bold>90.82</bold></td></tr></tbody></table></table-wrap></p>
      </sec>
      <sec id="sec4.5.4">
        <label>4.5.4</label>
        <title>Forecasting NYT COVID-19 daily infected cases for US states</title>
        <p id="d1e6282">The results of using our proposed STSGT model and other models to forecast the daily infected cases for the 50 US states and Washington, D.C. using the NYT dataset are summarized in <xref rid="tbl4" ref-type="table">Table 4</xref>. For the NYT dataset, the daily infected cases were reported for each state thereby eliminating the need for the county-wise summation process as done for the JHU dataset. Clearly, our proposed STSGT significantly outperformed all the other models by achieving the lowest MAE and RMSE for the short-term horizon (H=1) and the lowest MAE, RMSLE and RMSE for the long-term horizon indicated by the mean over the next 12 days (H=12). When comparing with the second best model ASTGCN(r), we observe that the mean MAE over the long-term horizon (12 days) reduces by 52.77 while the RMSLE and RMSE reduces by 0.010 and 136.08 respectively. This clearly shows the advantage of our model for forecasting over the long-term horizon.</p>
        <p id="d1e6288">
          <table-wrap position="anchor" id="tbl4">
            <label>Table 4</label>
            <caption>
              <p>Results demonstrating the MAE, RMSLE and RMSE of the daily infected cases of 50 US states and Washington, D.C. with NYT data. The lowest MAE, RMSLE and RMSE are marked in <bold>bold</bold>.</p>
            </caption>
            <table frame="hsides" rules="groups">
              <thead>
                <tr>
                  <th align="left">Algorithm</th>
                  <th align="left">Forecasting <break/>horizon</th>
                  <th align="left">MAE</th>
                  <th align="left">RMSLE</th>
                  <th align="left">RMSE</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="2">ARIMA (<xref rid="b23" ref-type="bibr">Li, Yu, et al., 2018</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1012" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">2152.48</td>
                  <td align="left">5.935</td>
                  <td align="left">3311.26</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">2185.04</td>
                  <td align="left">5.968</td>
                  <td align="left">3676.43</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">STGCN (<xref rid="b41" ref-type="bibr">Yu et al., 2018</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1051" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">1340.13</td>
                  <td align="left">0.975</td>
                  <td align="left">2303.10</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">1384.35</td>
                  <td align="left">0.997</td>
                  <td align="left">2357.91</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">ASTGCN(r) (<xref rid="b12" ref-type="bibr">Guo et al., 2019</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1090" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">939.840</td>
                  <td align="left">
                    <bold>0.852</bold>
                  </td>
                  <td align="left">1582.46</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">976.420</td>
                  <td align="left">0.879</td>
                  <td align="left">1626.42</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">Graph <break/>WaveNet (<xref rid="b35" ref-type="bibr">Wu et al., 2019</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1132" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">960.840</td>
                  <td align="left">0.854</td>
                  <td align="left">1506.59</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">1173.00</td>
                  <td align="left">0.919</td>
                  <td align="left">1905.55</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">STTN (<xref rid="b37" ref-type="bibr">Xu et al., 2020</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1171" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">1125.88</td>
                  <td align="left">0.978</td>
                  <td align="left">1794.09</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">1177.85</td>
                  <td align="left">1.029</td>
                  <td align="left">1854.19</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">STSGT <break/>(ours)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1207" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">
                    <bold>903.590</bold>
                  </td>
                  <td align="left">0.864</td>
                  <td align="left">
                    <bold>1465.55</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">
                    <bold>923.650</bold>
                  </td>
                  <td align="left">
                    <bold>0.869</bold>
                  </td>
                  <td align="left">
                    <bold>1490.34</bold>
                  </td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
        </p>
      </sec>
      <sec id="sec4.5.5">
        <label>4.5.5</label>
        <title>Forecasting NYT COVID-19 daily death cases for US states</title>
        <p id="d1e6296">The results of using our proposed STSGT model and other models to forecast the daily death cases for the 50 US states and Washington, D.C. using the NYT dataset are summarized in <xref rid="tbl5" ref-type="table">Table 5</xref>. Similar to <xref rid="tbl4" ref-type="table">Table 4</xref>, the daily death cases were reported for each state thereby eliminating the need for the county-wise summation process as done for the JHU dataset. Clearly, our proposed STSGT significantly outperformed all the other models by achieving the lowest MAE and RMSLE for the short-term horizon (<inline-formula><mml:math id="d1e6308" altimg="si199.svg" display="inline"><mml:mrow><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>) and the lowest MAE, RMSLE and RMSE for the long-term horizon indicated by the mean over the next 12 days (<inline-formula><mml:math id="d1e6320" altimg="si200.svg" display="inline"><mml:mrow><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula>). When comparing with the second best model ASTGCN(r) for the long-term (12 days mean), we observe that the mean MAE over 12 days horizon reduces by 2.02 while the RMSLE and RMSE reduces by 0.025 and 0.02 respectively.</p>
        <p id="d1e6329">
          <table-wrap position="anchor" id="tbl5">
            <label>Table 5</label>
            <caption>
              <p>Results demonstrating the MAE, RMSLE and RMSE of the daily death cases of 50 US states and Washington, D.C. with NYT data. The lowest MAE, RMSLE and RMSE are marked in <bold>bold</bold>.</p>
            </caption>
            <table frame="hsides" rules="groups">
              <thead>
                <tr>
                  <th align="left">Algorithm</th>
                  <th align="left">Forecasting <break/>horizon</th>
                  <th align="left">MAE</th>
                  <th align="left">RMSLE</th>
                  <th align="left">RMSE</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="left" rowspan="2">ARIMA (<xref rid="b23" ref-type="bibr">Li, Yu, et al., 2018</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1282" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">49.25</td>
                  <td align="left">2.693</td>
                  <td align="left">107.41</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">49.36</td>
                  <td align="left">2.661</td>
                  <td align="left">138.96</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">STGCN (<xref rid="b41" ref-type="bibr">Yu et al., 2018</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1321" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">28.86</td>
                  <td align="left">0.949</td>
                  <td align="left">67.85</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">28.43</td>
                  <td align="left">0.959</td>
                  <td align="left">63.87</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">ASTGCN(r) (<xref rid="b12" ref-type="bibr">Guo et al., 2019</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1360" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">25.90</td>
                  <td align="left">0.944</td>
                  <td align="left">62.07</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">24.40</td>
                  <td align="left">0.932</td>
                  <td align="left">57.47</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">Graph <break/>WaveNet (<xref rid="b35" ref-type="bibr">Wu et al., 2019</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1401" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">24.73</td>
                  <td align="left">0.921</td>
                  <td align="left">
                    <bold>60.07</bold>
                  </td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">26.18</td>
                  <td align="left">0.965</td>
                  <td align="left">60.57</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">STTN (<xref rid="b37" ref-type="bibr">Xu et al., 2020</xref>)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1441" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">29.85</td>
                  <td align="left">1.091</td>
                  <td align="left">69.55</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">28.09</td>
                  <td align="left">1.079</td>
                  <td align="left">62.97</td>
                </tr>
                <tr>
                  <td colspan="5">
                    <hr/>
                  </td>
                </tr>
                <tr>
                  <td align="left" rowspan="2">STSGT <break/>(ours)</td>
                  <td align="left">Day 1 <inline-formula><mml:math id="d1e1477" altimg="si201.svg" display="inline"><mml:mrow><mml:mo>(</mml:mo><mml:mi>H</mml:mi><mml:mo linebreak="goodbreak" linebreakstyle="after">=</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula></td>
                  <td align="left">
                    <bold>22.88</bold>
                  </td>
                  <td align="left">
                    <bold>0.912</bold>
                  </td>
                  <td align="left">61.39</td>
                </tr>
                <tr>
                  <td align="left">12 Days Mean</td>
                  <td align="left">
                    <bold>22.38</bold>
                  </td>
                  <td align="left">
                    <bold>0.907</bold>
                  </td>
                  <td align="left">
                    <bold>57.45</bold>
                  </td>
                </tr>
              </tbody>
            </table>
          </table-wrap>
        </p>
      </sec>
    </sec>
  </sec>
  <sec id="sec5">
    <label>5</label>
    <title>Discussion and future work</title>
    <p id="d1e6337">The present study investigated the impact and utility of spatial–temporal forecasting for COVID-19 time series analysis. Our proposed novel STSGT could accurately forecast the daily infected and death cases across the 50 US states and Washington, D.C. STSGT also performed extremely well at the state level by accurately forecasting the daily infected cases for all the counties of Michigan. Hence, our model could help the common people and the government by taking necessary precautions so that COVID could be prevented.</p>
    <p id="d1e6339">It should be noted that we have selected mid-March (March 15 or 18, 2020) as the start date for our analysis as the pandemic started in almost all of the US states around that date, and there were significant number of infected and death cases after mid-March. We chose Nov 30, 2021 as the end date of our analysis as the Omicron variant of COVID started dominating after this date, and the number of daily cases spiked to a new high. Hence, the daily cases from around December 1, 2021 until the end of Omicron wave (around Feb 15, 2022) formed a different pattern than what we observed from the start of the pandemic. We believe that our model needs to be re-trained again with the Omicron wave’s new data pattern to achieve accurate forecasting, and we leave this for future work. In future, we also plan to incorporate an efficient self-attention mechanism to further accelerate the training process of STSGT while maintaining similar performance as our current approach.</p>
  </sec>
  <sec id="sec6">
    <label>6</label>
    <title>Conclusion</title>
    <p id="d1e6346">In this paper, we proposed a novel model which can synchronously capture the complex spatial–temporal dependencies in the COVID data using the multi-head self-attention. We show through extensive experiments on two real-world public datasets that our model can outperform the existing models specifically designed for spatial–temporal forecasting. Our proposed model could have a significant impact on society by making the common people more aware and help the government eradicate this pandemic in the near future.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="d1e6351">The authors declare the following financial interests/personal relationships which may be considered as potential competing interests: Ming Dong and Weisong Shi reports financial support was provided by National Science Foundation.</p>
  </sec>
</body>
<back>
  <ref-list id="bib1">
    <title>References</title>
    <ref id="b1">
      <element-citation publication-type="journal" id="sb1">
        <person-group person-group-type="author">
          <name>
            <surname>Awan</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Bilal</surname>
            <given-names>M.H.</given-names>
          </name>
          <name>
            <surname>Yasin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Nobanee</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>N.S.</given-names>
          </name>
          <name>
            <surname>Zain</surname>
            <given-names>A.M.</given-names>
          </name>
        </person-group>
        <article-title>Detection of COVID-19 in chest X-ray images: A big data enabled deep learning approach</article-title>
        <source>International Journal of Environmental Research and Public Health</source>
        <volume>18</volume>
        <issue>19</issue>
        <year>2021</year>
        <fpage>10147</fpage>
        <pub-id pub-id-type="pmid">34639450</pub-id>
      </element-citation>
    </ref>
    <ref id="b2">
      <element-citation publication-type="journal" id="sb2">
        <person-group person-group-type="author">
          <name>
            <surname>Banerjee</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Dong</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>M.-H.</given-names>
          </name>
          <name>
            <surname>O’Hara</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Juhasz</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Asano</surname>
            <given-names>E.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep relational reasoning for the prediction of language impairment and postoperative seizure outcome using preoperative DWI connectome data of children with focal epilepsy</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <volume>40</volume>
        <issue>3</issue>
        <year>2020</year>
        <fpage>793</fpage>
        <lpage>804</lpage>
      </element-citation>
    </ref>
    <ref id="b3">
      <element-citation publication-type="journal" id="sb3">
        <person-group person-group-type="author">
          <name>
            <surname>Bartik</surname>
            <given-names>A.W.</given-names>
          </name>
          <name>
            <surname>Bertrand</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Cullen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Glaeser</surname>
            <given-names>E.L.</given-names>
          </name>
          <name>
            <surname>Luca</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Stanton</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>The impact of COVID-19 on small business outcomes and expectations</article-title>
        <source>Proceedings of the National Academy of Sciences</source>
        <volume>117</volume>
        <issue>30</issue>
        <year>2020</year>
        <fpage>17656</fpage>
        <lpage>17666</lpage>
      </element-citation>
    </ref>
    <ref id="b4">
      <element-citation publication-type="journal" id="sb4">
        <person-group person-group-type="author">
          <name>
            <surname>Brown</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Mann</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Ryder</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Subbiah</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kaplan</surname>
            <given-names>J.D.</given-names>
          </name>
          <name>
            <surname>Dhariwal</surname>
            <given-names>P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Language models are few-shot learners</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <volume>33</volume>
        <year>2020</year>
        <fpage>1877</fpage>
        <lpage>1901</lpage>
      </element-citation>
    </ref>
    <ref id="b5">
      <mixed-citation publication-type="other" id="sb5">Chiang, W.-L., Liu, X., Si, S., Li, Y., Bengio, S., &amp; Hsieh, C.-J. (2019). Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks. In <italic>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</italic> (pp. 257–266).</mixed-citation>
    </ref>
    <ref id="b6">
      <element-citation publication-type="book" id="sb6">
        <person-group person-group-type="author">
          <name>
            <surname>Chung</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Gulcehre</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cho</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <part-title>Empirical evaluation of gated recurrent neural networks on sequence modeling</part-title>
        <year>2014</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1412.3555" id="interref7">arXiv:1412.3555</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b7">
      <element-citation publication-type="journal" id="sb7">
        <person-group person-group-type="author">
          <name>
            <surname>Cucinotta</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Vanelli</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>WHO declares COVID-19 a pandemic</article-title>
        <source>Acta Biomedicine</source>
        <volume>91</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>157</fpage>
        <lpage>160</lpage>
      </element-citation>
    </ref>
    <ref id="b8">
      <element-citation publication-type="book" id="sb8">
        <person-group person-group-type="author">
          <name>
            <surname>Devlin</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>M.-W.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Toutanova</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <part-title>Bert: Pre-training of deep bidirectional transformers for language understanding</part-title>
        <year>2018</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1810.04805" id="interref8">arXiv:1810.04805</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b9">
      <element-citation publication-type="journal" id="sb9">
        <person-group person-group-type="author">
          <name>
            <surname>Dong</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Du</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Gardner</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>An interactive web-based dashboard to track COVID-19 in real time</article-title>
        <source>The Lancet Infectious Diseases</source>
        <volume>20</volume>
        <issue>5</issue>
        <year>2020</year>
        <fpage>533</fpage>
        <lpage>534</lpage>
        <pub-id pub-id-type="pmid">32087114</pub-id>
      </element-citation>
    </ref>
    <ref id="b10">
      <element-citation publication-type="book" id="sb10">
        <person-group person-group-type="author">
          <name>
            <surname>Dosovitskiy</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Beyer</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Kolesnikov</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Weissenborn</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Zhai</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Unterthiner</surname>
            <given-names>T.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>An image is worth 16x16 words: Transformers for image recognition at scale</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2010.11929" id="interref9">arXiv:2010.11929</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b11">
      <element-citation publication-type="journal" id="sb11">
        <person-group person-group-type="author">
          <name>
            <surname>Fout</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Byrd</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Shariat</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Ben-Hur</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Protein interface prediction using graph convolutional networks</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <volume>30</volume>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="b12">
      <element-citation publication-type="book" id="sb12">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <part-title>Attention based spatial-temporal graph convolutional networks for traffic flow forecasting</part-title>
        <series>Proceedings of the AAAI conference on artificial intelligence</series>
        <volume>vol. 33</volume>
        <year>2019</year>
        <fpage>922</fpage>
        <lpage>929</lpage>
      </element-citation>
    </ref>
    <ref id="b13">
      <mixed-citation publication-type="other" id="sb13">Hamilton, W. L., Ying, R., &amp; Leskovec, J. (2017). Inductive Representation Learning on Large Graphs. In <italic>Proceedings of the 31st international conference on neural information processing systems</italic> (pp. 1025–1035).</mixed-citation>
    </ref>
    <ref id="b14">
      <element-citation publication-type="journal" id="sb14">
        <person-group person-group-type="author">
          <name>
            <surname>Harapan</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Itoh</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Yufika</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Winardi</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Keam</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Te</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Coronavirus disease 2019 (COVID-19): A literature review</article-title>
        <source>Journal of Infection and Public Health</source>
        <volume>13</volume>
        <issue>5</issue>
        <year>2020</year>
        <fpage>667</fpage>
        <lpage>673</lpage>
        <pub-id pub-id-type="pmid">32340833</pub-id>
      </element-citation>
    </ref>
    <ref id="b15">
      <element-citation publication-type="journal" id="sb15">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>The vanishing gradient problem during learning recurrent neural nets and problem solutions</article-title>
        <source>International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</source>
        <volume>6</volume>
        <issue>02</issue>
        <year>1998</year>
        <fpage>107</fpage>
        <lpage>116</lpage>
      </element-citation>
    </ref>
    <ref id="b16">
      <element-citation publication-type="journal" id="sb16">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Computation</source>
        <volume>9</volume>
        <issue>8</issue>
        <year>1997</year>
        <fpage>1735</fpage>
        <lpage>1780</lpage>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="b17">
      <element-citation publication-type="book" id="sb17">
        <person-group person-group-type="author">
          <name>
            <surname>Kapoor</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Ben</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Perozzi</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Barnes</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Blais</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Examining covid-19 forecasting using spatio-temporal graph neural networks</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2007.03113" id="interref10">arXiv:2007.03113</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b18">
      <mixed-citation publication-type="other" id="sb18">Kipf, T. N., &amp; Welling, M. (2017). Semi-supervised classification with graph convolutional networks. In <italic>International conference on learning representations</italic>.</mixed-citation>
    </ref>
    <ref id="b19">
      <element-citation publication-type="journal" id="sb19">
        <person-group person-group-type="author">
          <name>
            <surname>Koh</surname>
            <given-names>H.K.</given-names>
          </name>
          <name>
            <surname>Geller</surname>
            <given-names>A.C.</given-names>
          </name>
          <name>
            <surname>VanderWeele</surname>
            <given-names>T.J.</given-names>
          </name>
        </person-group>
        <article-title>Deaths from COVID-19</article-title>
        <source>JAMA</source>
        <volume>325</volume>
        <issue>2</issue>
        <year>2021</year>
        <fpage>133</fpage>
        <lpage>134</lpage>
        <pub-id pub-id-type="pmid">33331884</pub-id>
      </element-citation>
    </ref>
    <ref id="b20">
      <element-citation publication-type="book" id="sb20">
        <person-group person-group-type="author">
          <name>
            <surname>Lea</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Vidal</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Reiter</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Hager</surname>
            <given-names>G.D.</given-names>
          </name>
        </person-group>
        <part-title>Temporal convolutional networks: A unified approach to action segmentation</part-title>
        <source>European conference on computer vision</source>
        <year>2016</year>
        <publisher-name>Springer</publisher-name>
        <fpage>47</fpage>
        <lpage>54</lpage>
      </element-citation>
    </ref>
    <ref id="b21">
      <element-citation publication-type="book" id="sb21">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>X.-M.</given-names>
          </name>
        </person-group>
        <part-title>Deeper insights into graph convolutional networks for semi-supervised learning</part-title>
        <year>2018</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1801.07606" id="interref11">arXiv:1801.07606</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b22">
      <mixed-citation publication-type="other" id="sb22">Li, G., Müller, M., Thabet, A., &amp; Ghanem, B. (2019). DeepGCNs: Can GCNs Go As Deep As CNNs?. In <italic>2019 IEEE/CVF international conference on computer vision</italic> (pp. 9266–9275).</mixed-citation>
    </ref>
    <ref id="b23">
      <mixed-citation publication-type="other" id="sb23">Li, Y., Yu, R., Shahabi, C., &amp; Liu, Y. (2018). Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting. In <italic>International conference on learning representations</italic>.</mixed-citation>
    </ref>
    <ref id="b24">
      <element-citation publication-type="book" id="sb24">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <part-title>Spatial-temporal fusion graph neural networks for traffic flow forecasting</part-title>
        <series>Proceedings of the AAAI conference on artificial intelligence</series>
        <volume>vol. 35</volume>
        <year>2021</year>
        <fpage>4189</fpage>
        <lpage>4196</lpage>
      </element-citation>
    </ref>
    <ref id="b25">
      <element-citation publication-type="journal" id="sb25">
        <person-group person-group-type="author">
          <name>
            <surname>Minaee</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kafieh</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Sonka</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yazdani</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Jamalipour Soufi</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Deep-COVID: Predicting COVID-19 from chest X-ray images using deep transfer learning</article-title>
        <source>Medical Image Analysis</source>
        <volume>65</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">101794</object-id>
      </element-citation>
    </ref>
    <ref id="b26">
      <element-citation publication-type="journal" id="sb26">
        <person-group person-group-type="author">
          <name>
            <surname>Pak</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Adegboye</surname>
            <given-names>O.A.</given-names>
          </name>
          <name>
            <surname>Adekunle</surname>
            <given-names>A.I.</given-names>
          </name>
          <name>
            <surname>Rahman</surname>
            <given-names>K.M.</given-names>
          </name>
          <name>
            <surname>McBryde</surname>
            <given-names>E.S.</given-names>
          </name>
          <name>
            <surname>Eisen</surname>
            <given-names>D.P.</given-names>
          </name>
        </person-group>
        <article-title>Economic consequences of the COVID-19 outbreak: The need for epidemic preparedness</article-title>
        <source>Frontiers in Public Health</source>
        <volume>8</volume>
        <year>2020</year>
      </element-citation>
    </ref>
    <ref id="b27">
      <element-citation publication-type="book" id="sb27">
        <person-group person-group-type="author">
          <name>
            <surname>Paszke</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gross</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Massa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Lerer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bradbury</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chanan</surname>
            <given-names>G.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Pytorch: An imperative style, high-performance deep learning library</part-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Wallach</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Larochelle</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Beygelzimer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>d’Alché Buc</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Fox</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Garnett</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <source>Advances in neural information processing systems 32</source>
        <year>2019</year>
        <publisher-name>Curran Associates, Inc.</publisher-name>
        <fpage>8024</fpage>
        <lpage>8035</lpage>
      </element-citation>
    </ref>
    <ref id="b28">
      <mixed-citation publication-type="other" id="sb28">Qi, X., Liao, R., Jia, J., Fidler, S., &amp; Urtasun, R. (2017). 3d graph neural networks for rgbd semantic segmentation. In <italic>Proceedings of the IEEE international conference on computer vision</italic> (pp. 5199–5208).</mixed-citation>
    </ref>
    <ref id="b29">
      <mixed-citation publication-type="other" id="sb29">Rong, Y., Huang, W., Xu, T., &amp; Huang, J. (2019). Dropedge: Towards deep graph convolutional networks on node classification. In <italic>International conference on learning representations</italic>.</mixed-citation>
    </ref>
    <ref id="b30">
      <element-citation publication-type="book" id="sb30">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <part-title>Spatial-temporal synchronous graph convolutional networks: A new framework for spatial-temporal network data forecasting</part-title>
        <series>Proceedings of the AAAI conference on artificial intelligence</series>
        <volume>vol. 34</volume>
        <year>2020</year>
        <fpage>914</fpage>
        <lpage>921</lpage>
      </element-citation>
    </ref>
    <ref id="b31">
      <element-citation publication-type="book" id="sb31">
        <person-group person-group-type="author">
          <name>
            <surname>Tekin</surname>
            <given-names>S.F.</given-names>
          </name>
          <name>
            <surname>Karaahmetoglu</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Ilhan</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Balaban</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Kozat</surname>
            <given-names>S.S.</given-names>
          </name>
        </person-group>
        <part-title>Spatio-temporal weather forecasting and attention mechanism on convolutional lstms</part-title>
        <year>2021</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2102.00696" id="interref12">arXiv:2102.00696</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b32">
      <element-citation publication-type="book" id="sb32">
        <part-title>The New York Times (2021). Coronavirus (COVID-19) data in the United States</part-title>
        <year>2021</year>
        <comment>Retrieved <ext-link ext-link-type="uri" xlink:href="https://github.com/nytimes/covid-19-data" id="interref13">https://github.com/nytimes/covid-19-data</ext-link>. [03 April 2022]</comment>
      </element-citation>
    </ref>
    <ref id="b33">
      <element-citation publication-type="journal" id="sb33">
        <person-group person-group-type="author">
          <name>
            <surname>Vaswani</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Shazeer</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Parmar</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Uszkoreit</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Jones</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Gomez</surname>
            <given-names>A.N.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Attention is all you need</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <volume>30</volume>
        <year>2017</year>
      </element-citation>
    </ref>
    <ref id="b34">
      <mixed-citation publication-type="other" id="sb34">Veličković, P., et al. (2018). Graph Attention Networks. In <italic>International conference on learning representations</italic>.</mixed-citation>
    </ref>
    <ref id="b35">
      <mixed-citation publication-type="other" id="sb35">Wu, Z., Pan, S., Long, G., Jiang, J., &amp; Zhang, C. (2019). Graph WaveNet for Deep Spatial-Temporal Graph Modeling. In <italic>Proceedings of the twenty-eighth international joint conference on artificial intelligence</italic> (pp. 1907–1913).</mixed-citation>
    </ref>
    <ref id="b36">
      <element-citation publication-type="book" id="sb36">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <part-title>Graph neural networks in recommender systems: a survey</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2011.02260" id="interref14">arXiv:2011.02260</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b37">
      <element-citation publication-type="book" id="sb37">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Dai</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Gao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>G.-J.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Spatial-temporal transformer networks for traffic flow forecasting</part-title>
        <year>2020</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:2001.02908" id="interref15">arXiv:2001.02908</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b38">
      <element-citation publication-type="book" id="sb38">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Tian</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Sonobe</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Kawarabayashi</surname>
            <given-names>K.-i.</given-names>
          </name>
          <name>
            <surname>Jegelka</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <part-title>Representation learning on graphs with jumping knowledge networks</part-title>
        <year>2018</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1806.03536" id="interref16">arXiv:1806.03536</ext-link></comment>
      </element-citation>
    </ref>
    <ref id="b39">
      <mixed-citation publication-type="other" id="sb39">Yan, S., Xiong, Y., &amp; Lin, D. (2018). Spatial temporal graph convolutional networks for skeleton-based action recognition. In <italic>Thirty-second AAAI conference on artificial intelligence</italic>.</mixed-citation>
    </ref>
    <ref id="b40">
      <element-citation publication-type="journal" id="sb40">
        <person-group person-group-type="author">
          <name>
            <surname>Ying</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cai</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zheng</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ke</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>D.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Do transformers really perform badly for graph representation?</article-title>
        <source>Advances in Neural Information Processing Systems</source>
        <volume>34</volume>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="b41">
      <mixed-citation publication-type="other" id="sb41">Yu, B., Yin, H., &amp; Zhu, Z. (2018). Spatio-temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting. In <italic>Proceedings of the 27th international joint conference on artificial intelligence</italic>.</mixed-citation>
    </ref>
    <ref id="b42">
      <element-citation publication-type="book" id="sb42">
        <person-group person-group-type="author">
          <name>
            <surname>Zhao</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Akoglu</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <part-title>Pairnorm: Tackling oversmoothing in gnns</part-title>
        <year>2019</year>
        <comment>arXiv preprint <ext-link ext-link-type="uri" xlink:href="arxiv:1909.12223" id="interref17">arXiv:1909.12223</ext-link></comment>
      </element-citation>
    </ref>
  </ref-list>
  <sec sec-type="data-availability" id="da1">
    <title>Data availability</title>
    <p id="d1e1634">I have shared the link to my data/code at the Attach File step. The links to the datasets and code is included in the manuscript file.</p>
  </sec>
  <ack id="d1e6353">
    <title>Acknowledgment</title>
    <p id="d1e6356">This work was partially supported by <funding-source id="GS1"><institution-wrap><institution-id institution-id-type="doi">10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source> (Grant No. 2027251).</p>
  </ack>
</back>
