<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Nucleic Acids Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Nucleic Acids Res</journal-id>
    <journal-id journal-id-type="publisher-id">nar</journal-id>
    <journal-title-group>
      <journal-title>Nucleic Acids Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0305-1048</issn>
    <issn pub-type="epub">1362-4962</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9371931</article-id>
    <article-id pub-id-type="pmid">35536244</article-id>
    <article-id pub-id-type="doi">10.1093/nar/gkac326</article-id>
    <article-id pub-id-type="publisher-id">gkac326</article-id>
    <article-categories>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00010</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>Narese/7</subject>
        <subject>Narese/24</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Methods Online</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Integrating convolution and self-attention improves language model of human genome for interpreting non-coding regions at base-resolution</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-5630-6357</contrib-id>
        <name>
          <surname>Yang</surname>
          <given-names>Meng</given-names>
        </name>
        <!--yangmeng1@genomics.cn-->
        <aff><institution>MGI, BGI-Shenzhen</institution>, <addr-line>Shenzhen 518083</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Department of Biology, University of Copenhagen</institution>, Copenhagen DK-2200, <country country="DK">Denmark</country></aff>
        <xref rid="COR1" ref-type="corresp"/>
        <xref rid="FN1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Huang</surname>
          <given-names>Lichao</given-names>
        </name>
        <aff><institution>MGI, BGI-Shenzhen</institution>, <addr-line>Shenzhen 518083</addr-line>, <country country="CN">China</country></aff>
        <xref rid="FN1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Huang</surname>
          <given-names>Haiping</given-names>
        </name>
        <aff><institution>MGI, BGI-Shenzhen</institution>, <addr-line>Shenzhen 518083</addr-line>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tang</surname>
          <given-names>Hui</given-names>
        </name>
        <aff><institution>MGI, BGI-Shenzhen</institution>, <addr-line>Shenzhen 518083</addr-line>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Nan</given-names>
        </name>
        <aff><institution>MGI, BGI-Shenzhen</institution>, <addr-line>Shenzhen 518083</addr-line>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yang</surname>
          <given-names>Huanming</given-names>
        </name>
        <aff><institution>BGI-Shenzhen</institution>, <addr-line>Shenzhen 518083</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Guangdong Provincial Academician Workstation of BGI Synthetic Genomics, BGI-Shenzhen</institution>, <addr-line>Shenzhen</addr-line>, <addr-line>518120</addr-line>, <country country="CN">China</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Wu</surname>
          <given-names>Jihong</given-names>
        </name>
        <!--jihongwu@fdu.edu.cn-->
        <aff><institution>Department of Ophthalmology, Eye &amp; ENT Hospital, Shanghai Medical College, Fudan University</institution>, <addr-line>Shanghai</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Shanghai Key Laboratory of Visual Impairment and Restoration, Science and Technology Commission of Shanghai Municipality</institution>, <addr-line>Shanghai</addr-line>, <country country="CN">China</country></aff>
        <aff><institution>Key Laboratory of Myopia (Fudan University), Chinese Academy of Medical Sciences, National Health Commission</institution>, <addr-line>Shanghai</addr-line>, <country country="CN">China</country></aff>
        <xref rid="COR2" ref-type="corresp"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Mu</surname>
          <given-names>Feng</given-names>
        </name>
        <!--mufeng@mgi-tech.com-->
        <aff><institution>MGI, BGI-Shenzhen</institution>, <addr-line>Shenzhen 518083</addr-line>, <country country="CN">China</country></aff>
        <xref rid="COR3" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="COR1">To whom correspondence should be addressed. Tel: +86 0755 36352505; Email: <email>yangmeng1@genomics.cn</email></corresp>
      <corresp id="COR2">Correspondence may also be addressed to Jihong Wu. Tel: +86 021 64377134; Email: <email>jihongwu@fdu.edu.cn</email></corresp>
      <corresp id="COR3">Correspondence may also be addressed to Feng Mu. Tel: +86 0755 36352505; Email: <email>mufeng@mgi-tech.com</email></corresp>
      <fn id="FN1">
        <p>The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <day>12</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-05-10">
      <day>10</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>10</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <volume>50</volume>
    <issue>14</issue>
    <fpage>e81</fpage>
    <lpage>e81</lpage>
    <history>
      <date date-type="accepted">
        <day>09</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>22</day>
        <month>2</month>
        <year>2022</year>
      </date>
      <date date-type="received">
        <day>22</day>
        <month>9</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press on behalf of Nucleic Acids Research.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution-NonCommercial License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact <email>journals.permissions@oup.com</email></license-p>
      </license>
    </permissions>
    <self-uri xlink:href="gkac326.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>Interpretation of non-coding genome remains an unsolved challenge in human genetics due to impracticality of exhaustively annotating biochemically active elements in all conditions. Deep learning based computational approaches emerge recently to help interpret non-coding regions. Here, we present LOGO (<underline>L</underline>anguage <underline>o</underline>f <underline>G</underline>en<underline>o</underline>me), a self-attention based contextualized pre-trained language model containing only two self-attention layers with 1 million parameters as a substantially light architecture that applies self-supervision techniques to learn bidirectional representations of the unlabelled human reference genome. LOGO is then fine-tuned for sequence labelling task, and further extended to variant prioritization task via a special input encoding scheme of alternative alleles followed by adding a convolutional module. Experiments show that LOGO achieves 15% absolute improvement for promoter identification and up to 4.5% absolute improvement for enhancer-promoter interaction prediction. LOGO exhibits state-of-the-art multi-task predictive power on thousands of chromatin features with only 3% parameterization benchmarking against the fully supervised model, DeepSEA and 1% parameterization against a recent BERT-based DNA language model. For allelic-effect prediction, locality introduced by one dimensional convolution shows improved sensitivity and specificity for prioritizing non-coding variants associated with human diseases. In addition, we apply LOGO to interpret type 2 diabetes (T2D) GWAS signals and infer underlying regulatory mechanisms. We make a conceptual analogy between natural language and human genome and demonstrate LOGO is an accurate, fast, scalable, and robust framework to interpret non-coding regions for global sequence labeling as well as for variant prioritization at base-resolution.</p>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Guangdong Provincial Academician Workstation of BGI Synthetic Genomics</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2017B090904014</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Program of Shanghai Academic Research Leader</institution>
            <institution-id institution-id-type="DOI">10.13039/501100012247</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>20XD1401100</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Program for Outstanding Medical Academic Leader</institution>
          </institution-wrap>
        </funding-source>
        <award-id>2019LJ01</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="19"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="SEC1">
    <title>INTRODUCTION</title>
    <p>In 2003, the Human Genome Project (HGP) successfully digitalized the ‘book of life’. It is convinced that biological structure and function are intrinsically encoded in the primary genome sequence. The non-coding regions, accounting for over 98% of the whole genome, implement significant yet largely unknown regulatory functions. Recent large consortia projects, including the ENCyclopedia of DNA Elements (ENCODE) (<xref rid="B1" ref-type="bibr">1</xref>,<xref rid="B2" ref-type="bibr">2</xref>), Roadmap Epigenomics (<xref rid="B3" ref-type="bibr">3</xref>), and the Genomics of Gene Regulation (GGR), have produced large amount of experimental mapping readouts to help annotate non-coding genome in specific tissues or cell-lines. On the other hand, Genome-wide association studies (GWAS) have discovered that the vast majority (&gt;90%) of associated genome loci for complex disease and traits fall in non-coding regions (<xref rid="B4" ref-type="bibr">4</xref>). Hence, it is of exceptional utility to explore these datasets and derive novel hypothesis to interpret non-coding regions. Unlike the protein coding region where there is a clear genetic code, incorporating broader sequence context is critical to understand functional effects of regulatory variants, which requires more powerful and semantic-rich representational model to capture higher-order complexity in the region. Deep learning has transformed ranges of tasks in computer vision and natural language processing (NLP). In bioinformatics field, deep learning based computational methods have also been proposed in various applications, such as predicting molecular phenotypes based on raw DNA sequence as input and achieving better performance than traditional machine learning approaches, as referred to an excellent review paper (<xref rid="B5" ref-type="bibr">5</xref>). One classical model is DeepSEA (<xref rid="B6" ref-type="bibr">6</xref>), pioneering to apply deep convolutional neural network (CNN) architecture to extract features of genome sequence given 1000-bp context and train on chromatin profiles in a supervised multitask learning manner. DeepSEA can able to predict the binary presence or absence of 919 chromatin marks.</p>
    <p>The inherent sequential nature of the genome is analogous to documents composed of words, characters and phrases. The exciting advance in the NLP field has shed light on using similar strategy to extract general and transferable information from biological sequence. Neural network model was introduced into NLP since 2013. Word2vec (<xref rid="B7" ref-type="bibr">7</xref>) was proposed to learn distributional vector embeddings of each word to capture their similarities given the sentence context. Word2vec uses multilayer perceptron (MLP) (<xref rid="B8" ref-type="bibr">8</xref>) to predict neighboring words given center word (called skip-gram) or predict center word given neighboring words (called ‘Continuous Bag of Words’, CBOW). The learned word vectors can then be directly queried for downstream text classification tasks. Word2vec essentially relies on modelling co-occurrence probabilities without considering word position information and static embeddings cannot handle words with multiple meanings, so-called polysemous words. Traditional CNN-based feature extractors rely on local parameter sharing and the pooling operation may lead to loss of global information. Recurrent Neural Network (RNN) (<xref rid="B9" ref-type="bibr">9</xref>,<xref rid="B10" ref-type="bibr">10</xref>) is an alternative architecture to process sequential data. RNN can capture position dependency information via passing the memory state from previous elements. RNN’s fundamental constraint of sequential operation leads to difficulty of parallelization and faces the risk of vanishing gradient when processing longer sequence. In 2017, Transformer (<xref rid="B11" ref-type="bibr">11</xref>) has emerged as a powerful architecture that relies completely on attention mechanism to draw global dependencies in Seq2Seq modelling task. In the encoder part, self-attention mechanism relates different positions across a single sequence to compute a contextualized representation with better parallelization. Transformer can tackle long-range dependency without position bias, outperforming CNNs or RNNs in many global sequence classification tasks. On the other hand, CNN is better at capturing locality.</p>
    <p>Since 2018, a new wave of pre-trained language models using self-supervision techniques have emerged as a core trend in NLP, including RNN-based ELMo (<xref rid="B12" ref-type="bibr">12</xref>), ULMFiT (<xref rid="B13" ref-type="bibr">13</xref>), Transformer-based OpenAI-GPT (<xref rid="B14" ref-type="bibr">14</xref>) and Google-BERT (<xref rid="B15" ref-type="bibr">15</xref>). Instead of conventional left-to-right unidirectional modeling, BERT, which stands for Bidirectional Encoder Representations from Transformers, leveraged a multilayer bidirectional Transformer architecture to pre-train on large unlabeled corpora by jointly incorporating both left and right contexts. The pre-trained model learns contextualized token embeddings through two proxy training objectives: MLM (Masked Language Model), predicting randomly masked tokens and NSP (Next Sentence Prediction), predicting whether two sentences follow each other. The pre-trained BERT can then be easily fine-tuned to various downstream NLP tasks and obtain new state-of-the-art results competing with human performance. Thereafter, a series of pre-trained models spring up to further improve performance, such as XLNet (<xref rid="B16" ref-type="bibr">16</xref>), UniLM (<xref rid="B17" ref-type="bibr">17</xref>), MASS (<xref rid="B18" ref-type="bibr">18</xref>), MT-DNN (<xref rid="B19" ref-type="bibr">19</xref>), XLM (<xref rid="B20" ref-type="bibr">20</xref>), ALBERT (<xref rid="B21" ref-type="bibr">21</xref>), RoBERTa (<xref rid="B22" ref-type="bibr">22</xref>) and ELECTRA (<xref rid="B23" ref-type="bibr">23</xref>). A comprehensive review can be found in an integrative reference (<xref rid="B24" ref-type="bibr">24</xref>). One representative model, ALBERT, a lite version of BERT, establishes better results with significantly reduced model size through factorized embeddings and cross-layer parameter sharing techniques. Unlike models trained on general domain corpora, SciBERT (<xref rid="B25" ref-type="bibr">25</xref>) and BioBERT (<xref rid="B26" ref-type="bibr">26</xref>) are proposed based on BERT backbone and trained on a large amount of multidisciplinary scientific literature and biomedical text corpus, respectively. The domain-specific BioBERT achieves dramatic improvement in biomedical text-mining tasks, such as name entity recognition, relation extraction and question answering. BioBERT is comprised of 12 layers with hidden size of 768, 12 heads and 12 attention heads in each layer. Recently, Transformer was reported by Facebook to learn protein structure and function (<xref rid="B27" ref-type="bibr">27</xref>). DNABERT (<xref rid="B28" ref-type="bibr">28</xref>) is recently proposed to learn the human genome and is composed of 12 Transformer layers with 768 hidden units and 12 attention heads in each layer, which is configured as a heavy version of BERT. DNABERT is fine-tuned on several functional sequence recognition tasks and splicing site identification. However, features learned by Transformer are too general and not specific or sensitive to a single or few changes in the sequence, unable to satisfy the needs of interpreting human genome at base-resolution. Recently, Facebook and Google both propose introducing the concept of convolution into Transformer architecture to bring soft inductive bias with better locality, namely ConViT (<xref rid="B29" ref-type="bibr">29</xref>) and CoAtNets (<xref rid="B30" ref-type="bibr">30</xref>).</p>
    <p>Motivated by these observations, in this study we develop LOGO, a pre-trained language model with much lighter architecture than the pioneering DNABERT, which is composed of only two layers with 256 hidden units and 8 heads (embedding size is set to 128), to learn contextualized representations of reference genome hg19. The main intuition to choose these hyperparameters is to keep LOGO as lightweight as possible to save GPU memories without compromising performance. We implement ablation studies covering both pre-training and downstream fine-tuning tasks and demonstrate how to determine the optimal choice of hyperparameters. Details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S2–S4</xref>. LOGO with 3-mer tokenization contains around 1 million parameters while DNABERT contains 100 million parameters. DNABERT leverages span masking strategy and consumes 25 days on 8 RTX2080TI GPUs to implement pre-training. LOGO has much faster pre-training speed, both due to lighter ALBERT-like structure as well as a random masking strategy following dividing the tokenized sequence into <italic toggle="yes">k</italic>-mers <italic toggle="yes">k</italic>-stride groups. LOGO accepts fixed-length DNA sequence input (1000 and 2000 bp) while DNABERT uses variable length input ranging from 5 bp to 500 bp. LOGO shows substantially more effective parameter efficiency than DNABERT. To demonstrate the versatility of LOGO, we implement fine-tuning for multiple downstream tasks and obtain excellent performance from aspects of accuracy, speed, scalability, and robustness. Sequence-level classification tasks include promoter prediction, promoter-enhancer interaction prediction and chromatin features prediction. Another key innovation of LOGO is that we introduce a novel encoding scheme for alternative alleles and leverage a hybrid architecture by mixing convolution and self-attention to alleviate the locality-insensitivity issue of Transformer and facilitate functional prioritization of noncoding variants. DNABERT only reports high-attention variants extracted from Transformer encoder while LOGO leverages convolution operation and forces the model to see the nucleotide change with allelic-effects. We also propose a framework to embed prior knowledge into LOGO and explore better performance over original settings. LOGO provides a unified framework not only for sequence labelling or motif identification task, but also for SNP or indel prioritization to interpret non-coding regions at base-resolution.</p>
  </sec>
  <sec sec-type="materials|methods" id="SEC2">
    <title>MATERIALS AND METHODS</title>
    <sec id="SEC2-1">
      <title>Architecture of the pre-training model</title>
      <p>The pre-training model leverages the encoder part of Transformer architecture and learns representations of input sequences via multi-head self-attention mechanism. We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as <italic toggle="yes">L</italic>, and the hidden size as <italic toggle="yes">H</italic>. Each training instance is started with a 100-bp bin as described above and extended forward along the reference genome until reaching 1000-bp length. The model processes the genome into sequential segments of <italic toggle="yes">k</italic>-mer tokens, and each token is labelled by a unique vocabulary ID as input. The size of token embedding has length <italic toggle="yes">E</italic> = 128. Token embeddings are summed with position embeddings and fed into Transformer encoder network. We leverage the ALBERT strategy to untie the input token embedding size <italic toggle="yes">E</italic> from the hidden layer size <italic toggle="yes">H</italic> in the Transformer encoder. The encoder is composed of a stack of <italic toggle="yes">L</italic> = 2 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention layer, and the second is a position-wise fully connected feed-forward layer. A residual connection is added to each sub-layer, followed by layer normalization, leading to output of each sub-layer being LayerNorm (<italic toggle="yes">x</italic> + Sublayer(<italic toggle="yes">x</italic>)), where Sublayer(<italic toggle="yes">x</italic>) is the function implemented by the sub-layer itself. Each hidden sub-layer produces vector outputs with dimension of <italic toggle="yes">H</italic> = 256 = dmodel. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the queries (<inline-formula><tex-math id="M0001" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$Q$\end{document}</tex-math></inline-formula>), keys (<inline-formula><tex-math id="M0001a" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K$\end{document}</tex-math></inline-formula>), values (<inline-formula><tex-math id="M0002" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$V$\end{document}</tex-math></inline-formula>), and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. For each self-attention layer, the input consists of queries and keys of dimension <inline-formula><tex-math id="M0003" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${d_k}$\end{document}</tex-math></inline-formula>, and values of dimension <inline-formula><tex-math id="M0004" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${d_v}$\end{document}</tex-math></inline-formula>. The dot products of the query with all keys, divided by <inline-formula><tex-math id="M0005" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\sqrt {{d_k}}$\end{document}</tex-math></inline-formula>, are fed into a <inline-formula><tex-math id="M0006" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$softmax$\end{document}</tex-math></inline-formula> layer to obtain the weights on the values. The attention functions on a set of queries are computed simultaneously, packed together into a matrix <inline-formula><tex-math id="M0007" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$Q$\end{document}</tex-math></inline-formula>. The keys and values are also packed together into matrices <inline-formula><tex-math id="M0008" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$K$\end{document}</tex-math></inline-formula> and matrices<inline-formula><tex-math id="M0009" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$\ V$\end{document}</tex-math></inline-formula>. The matrix of outputs is computed as:<disp-formula><tex-math id="M00010" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}Attention\ \left( {Q,K,V} \right) = {\rm{\ }}softmax\left( {\frac{{Q{K^T}}}{{\sqrt {{d_k}} }}} \right)V\end{equation*}$$\end{document}</tex-math></disp-formula></p>
      <p>To increase the capacity of the model, the input of each hidden layer is processed by multiple attention heads, which means on each projected version of queries, keys and values, the attention function is performed<inline-formula><tex-math id="M00011" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$A$\end{document}</tex-math></inline-formula>(number of heads) times in parallel. The outputs of each head are concatenated and projected, resulting in the final values, as depicted:<disp-formula><tex-math id="M00012" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}MultiHead ( {Q,K,V} ) &amp;=&amp; Concat( {hea{d_1}, \cdots , hea{d_h}} ){W^O},\\ &amp;&amp;\quad\times where\ \ hea{d_i} \\ &amp;=&amp; Attention( {QW_i^Q, KW_i^K, VW_i^V} )\end{eqnarray*}$$\end{document}</tex-math></disp-formula><disp-formula><tex-math id="M00013" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{eqnarray*}W_i^Q \in {\mathbb{R}^{{d_k} \times {d_{model}}}}, &amp;&amp; W_i^K \in {\mathbb{R}^{{d_k} \times {d_{model}}}},\ W_i^V \in {\mathbb{R}^{{d_v} \times {d_{model}}}},\\ &amp;&amp;{W^O} \in {\mathbb{R}^{h{d_v} \times {d_{model}}}}\end{eqnarray*}$$\end{document}</tex-math></disp-formula></p>
      <p>We employ <inline-formula><tex-math id="M00014" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$A\ = \ 8$\end{document}</tex-math></inline-formula> heads. For each of these, we use <inline-formula><tex-math id="M00015" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${d_k} = {d_v}\ = \ \frac{{{d_{model}}}}{A} = \ 32$\end{document}</tex-math></inline-formula>. Due to the reduced dimension of each head, the total computational cost is close to that of single-head attention with full dimensionality. Following each attention layer, the fully connected feed-forward network is applied to each position separately and identically, which consists of two linear transformations with a <inline-formula><tex-math id="M00016" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ReLU$\end{document}</tex-math></inline-formula> activation in between.<disp-formula><tex-math id="M00017" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}FFN\ \left( x \right) = \ {\rm{max}}\left( {0,\ x{W_1} + {b_1}} \right){W_2} + {b_2}\end{equation*}$$\end{document}</tex-math></disp-formula></p>
      <p>After a forward pass through <italic toggle="yes">L</italic> = 2 layers, a final classification layer is used to project the hidden state (<inline-formula><tex-math id="M00018" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}${d_{model}}$\end{document}</tex-math></inline-formula>) to output classes of dimension equal to <italic toggle="yes">k</italic>-mer vocabulary size.</p>
      <p>Motivated by ALBERT architecture, we use a factorization of token embedding parameters. By using this decomposition, the embedding parameters are reduced from <inline-formula><tex-math id="M00019" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$O( {V \times H} )$\end{document}</tex-math></inline-formula> to <inline-formula><tex-math id="M00020" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$O( {V \times E + E \times H} )$\end{document}</tex-math></inline-formula>. We also enforce sharing all parameters across two layers motivated by improved parameter efficiency of ALBERT. In original BERT model, for a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. Position embeddings are used to capture relative positions of each input token within a sequence. Since we discard NSP task, we do not use segment embeddings in pre-training stage. One contribution of this work is that we demonstrate a method to incorporate prior knowledge into the language model. Knowledge layer is introduced and encoded in one-hot format. For example, if we have <inline-formula><tex-math id="M00021" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$M$\end{document}</tex-math></inline-formula> knowledge items to label the input sequence, then a <italic toggle="yes">M</italic>-dimension one-hot knowledge vector is introduced and concatenated with input sequence vectors. For example, if a sequence is labelled by an annotation knowledge, all <italic toggle="yes">k</italic>-mers spanning from annotation start position to end position will be recorded as ‘1’ for this type of knowledge, and k-mers of other positions will be recorded as ‘0’. Knowledge embeddings are learned by the model and the dimensions are set as the same as token embeddings size. In this study, knowledge embeddings are only used in the fine-tuning stage in promoter prediction task.</p>
    </sec>
    <sec id="SEC2-2">
      <title>Pre-training</title>
      <p>We define similar self-supervised loss for Masked Language Model [MLM] pre-training task as in BERT and discard Next Sentence Prediction (NSP) task. In the pre-training stage, to balance the computation burden and representation utility, we generate and evaluate four types of <italic toggle="yes">k</italic>-mer (<italic toggle="yes">k</italic> = 3, 4, 5, 6) with 1-stride settings to tokenize the genome (Data generation and tokenization in <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S1</xref>). For each <italic toggle="yes">k</italic>-mer setting of 1000-bp sequence, <italic toggle="yes">k</italic> sets of input will be all used as training instances. For each set, we use similar masking strategy as in BERT. The masked token will be represented as [MASK]. We randomly masked 15% of <italic toggle="yes">k</italic>-mer tokens for prediction, 80% of which are replaced with [MASK], 10% are replaced by a random token from the vocabulary and another 10% remain unchanged. The original token at masked position will be predicted with cross-entropy loss. The pre-training loss is the sum of the mean masked LM likelihood. We follow the BERT notation conventions and denote the vocabulary embedding size as <italic toggle="yes">E</italic>, the number of encoder layers as <italic toggle="yes">L</italic>, and the hidden size as <italic toggle="yes">H</italic>. In LOGO model, each <italic toggle="yes">k</italic>-mer of input sequence will be represented as 128-dimension vocabulary token embedding vectors. The embedding size of hidden layers is set to be larger than input token embedding size as in ALBERT, since hidden layers are meant to learn context-dependent representations. All embeddings and model weights are expected to be learned by the model from MLM task. We use four Nvidia Tesla V100 SXM3 32G GPU to train the model. Because the number of training records exceeds 180 million (3-mer: 60 million<inline-formula><tex-math id="M00022" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>3, 4-mer: 60 million<inline-formula><tex-math id="M00023" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>4, 5-mer: 60 million<inline-formula><tex-math id="M00024" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>5, 6-mer: 60 million<inline-formula><tex-math id="M00025" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>6), to speed up training, we convert all data to Tensorflow tfrecord and adopt Tensorflow's <italic toggle="yes">‘Multi Worker Mirrored Strategy’</italic> strategy to support multi-machine and multi-GPU training. Parallel training technique is used on four GPUs to support large batch size. Hyperparameters are summarized as below: layers (<italic toggle="yes">L</italic>) = 2; token embedding size (<italic toggle="yes">E</italic>) = 128; hidden dimension size (<italic toggle="yes">H</italic>) = 256; attention heads (<italic toggle="yes">A</italic>) = 8; batch size (BSZ) = 512 for each GPU, 512<inline-formula><tex-math id="M00026" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>4 = 2048 for 4 GPUs; steps-per-epoch = 4000; maximum epochs = 100; sequence length = 333, 250, 200, 166 tokens for 3-mer, 4-mer, 5-mer, 6-mer setting, respectively to encode 1000-bp input sequence. We use an Adam optimizer with learning rate = 0.00001. Other hyperparameters are set as default in ALBERT.</p>
    </sec>
    <sec id="SEC2-3">
      <title>Analysis of T2D-related GWAS variants</title>
      <p>We download all T2D-associated SNPs from GWAS Catalog (2020–05-14 version) and obtain corresponding LD SNPs from LDlink, resulting in 156 175 SNPs (<italic toggle="yes">P</italic>-value ranging from 9<inline-formula><tex-math id="M00027" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–6</sup> to 6<inline-formula><tex-math id="M00028" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–447</sup>). To make fair comparison with DeepSEA, we use the same approach to compute chromatin effects of variants. For each SNP, we extract the 1000-bp or 2000-bp context sequence centered on that variant based on hg19 reference genome (SNPs locates at the 500th position). A pair of 1000-bp sequences centered on both reference allele and alternative allele at the variant position are used to calculate the probabilities for each chromatin feature. Absolute differences between probability values and relative log fold changes of odds are calculated following DeepSEA pipelines. Both forward and complementary sequences are computed and averaged to obtain the predicted chromatin effects. The magnitude of the predicted chromatin effect on a chromatin feature for an SNP is computed as the product of the absolute difference between probability values and the relative log fold change of odds. We use the same protocol as in DeepSEA to obtain negative non-functional SNPs which contains 1 000 000 negative SNPs randomly chosen from 1000 Genomes Project. We calculate chromatin effects for these negative SNPs to generate the empirical background distribution and use the same E-value definition to evaluate significance of variant effects. For each chromatin feature, E-value is computed as the proportion of negative SNPs with higher predicted chromatin effect magnitude on the same chromatin feature. We use fine-tuned LOGO-919, LOGO-2002 (Pretrained LOGO with 2000-bp context fine-tuned on 2002 chromatin features from ExPecto, <italic toggle="yes">n</italic> = 690 TF, 334 DHSs and 978 HM, respectively) and LOGO-3357 (Pretrained LOGO with 2000-bp context fine-tuned on 3357 chromatin features after integrating EpiMap features with ExPecto features, <italic toggle="yes">n</italic> = 826 TF, 668 DHSs and 1863 HM, respectively) to calculate E-values for 919, 2002 and 3357 chromatin features, respectively (data details in <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S4</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S5</xref>). A variant is considered as putative functional significant if at least one chromatin feature's <italic toggle="yes">E</italic>-value is equal or less than a certain threshold, i.e. 1<inline-formula><tex-math id="M00029" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–5</sup>, which might be used to infer underlying regulator disruption mechanism. Profile of Thurner islet chromatin states is downloaded from <ext-link xlink:href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5828664/bin/elife-31977-fig3-data5.zip" ext-link-type="uri">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5828664/bin/elife-31977-fig3-data5.zip</ext-link>. Profile of Varshney islet chromatin states is downloaded from <ext-link xlink:href="https://theparkerlab.med.umich.edu/data/papers/doi/10.1073/pnas.1621192114/" ext-link-type="uri">https://theparkerlab.med.umich.edu/data/papers/doi/10.1073/pnas.1621192114/</ext-link> after consultation with Dr. Narisu from Francis Collins Lab via email.</p>
    </sec>
    <sec id="SEC2-4">
      <title>Model architecture for LOGO-E2E</title>
      <p>To fine-tune on variant prioritization task in an end-to-end manner, we modify LOGO architecture to accommodate signed allelic information. We stack three layers to encode input sequence. The first layer is called ‘Ref layer’. We tokenize each 1000-bp context sequence extracted from hg19 reference genome using 6-mer-1-stride and feed it into ‘Ref layer’ via concatenating six sets of 6-mer [Ref] tokens in an interlaced manner. This novel operation is introduced to encode input sequence at base-resolution without compromising representation capacity of <italic toggle="yes">k</italic>-mer strategy. The second layer is called ‘Alt’ layer, we use this layer to encode allelic information at certain position. Only changed position compared to ‘Ref layer’ will have input value with corresponding 6-mer [Alt] token, other positions corresponding to the context sequence are set to [zero]. In this way, we explicitly encode the alternative allele to enforce the model to see directional alteration. The third layer is called ‘Type’ layer to encode variant type. In this paper, we do not evaluate SNVs and indels simultaneously, so we set [Type] token at corresponding position equal to 1 and other positions are set to [zero]. Each variant with surrounding context of certain length will be encoded as a matrix input containing ‘Ref’, ‘Alt’ and ‘Type’ information, which is formatted as a ‘npz’ compressed file. One-dimension convolutional layer is added after token embeddings and then fed into Transformer architecture. Three kernels with different sizes (<xref rid="B2" ref-type="bibr">2</xref>,<xref rid="B3" ref-type="bibr">3</xref>,<xref rid="B5" ref-type="bibr">5</xref>) are introduced to capture multi-scale features. Through experiments, it is found that this method reduces the weight updating frequency and makes the fluctuation range more stable during the fine-tuning process of the model. The final hidden state corresponding to learned [CLS] token embeddings is followed by a classification layer with sigmoid output of deleteriousness effect of target variant. Binary cross entropy loss is used to calculate loss function. We download LOGO-919 model weights and perform LOGO-E2E fine-tuning on HGMD training sets using 1 Nvidia TITAN Xp Pascal GPU. We use batch size of 64 and <italic toggle="yes">L</italic> = 2, <italic toggle="yes">E</italic> = 128, <italic toggle="yes">H</italic> = 256, <italic toggle="yes">A</italic> = 8. We use Adam optimizer with initial learning rate of 0.00001, and other parameters are set as default, and use early stopping strategy and stop training when validation loss no longer decreases for three consecutive epochs.</p>
    </sec>
    <sec id="SEC2-5">
      <title>Model architecture for LOGO-C2P</title>
      <p>For LOGO-C2P, one-dimension convolutional layer is added after token embeddings and then fed into Transformer architecture. Three kernels with different sizes (<xref rid="B2" ref-type="bibr">2</xref>,<xref rid="B3" ref-type="bibr">3</xref>,<xref rid="B5" ref-type="bibr">5</xref>) are introduced to capture multi-scale features. We follow similar pipelines as in DeepSEA for functional SNP prioritization architecture and firstly use previously trained LOGO-919/LOGO-2002 to generate chromatin effect features for both reference and alternative alleles. We then conduct the same absolute difference and relative log fold change transformation as DeepSEA and feed these features into boosted model to train the classifier at the second stage. We assess different model performances of whether or not preserving four base-level evolutionary feature used by DeepSEA, including PhastCons scores for primates (excluding humans), PhyloP scores for primates (excluding humans), and GERP++ neutral evolution and rejected substitution scores. We use well-trained LOGO-919, LOGO-2002 and DeepSEA to generate chromatin features for each target variant, convert these features into DMatrix format, and train a regularized logistic regression model, using the XGBoost v0.9 implementation (<ext-link xlink:href="https://github.com/tqchen/xgboost" ext-link-type="uri">https://github.com/tqchen/xgboost</ext-link>). It is worth mentioning that we discard z-score transformation as used in DeepSEA classifier. We argue that the tree-based approach does not require normalization as stated by original XGBoost author. The model is trained with L1 regularization parameter (alpha) = 20 and L2 regularization parameter (lambda) = 2000 for iterations = 1000. Other hyperparameters are set as: Step-size shrinkage parameter(eta):0.1booster:‘gbtree’, objective:‘binary:logistic’, loss:‘error’. We set early stopping rules when validation error no longer decreases for 200 epochs and preserve the best model weight. 1 Nvidia TITAN Xp Pascal GPU is used.</p>
    </sec>
    <sec id="SEC2-6">
      <title>Benchmarking of classifier performance on HGMD, ClinVar and GWAS variants</title>
      <p>For HGMD regulatory variants, the performance of each model is estimated by 10-fold cross-validation. For filtered 3498 regulatory variants, we construct negative controls from 1000 Genomes Project SNPs using two schemes: random sampling (unrestricted, 3690 negatives), and matched to positive ones within 1 kb (restricted, 3034 negatives). We fine-tune LOGO-E2E and LOGO-C2P on the HGMD dataset. We also retrain DeepSEA based classifier on the HGMD dataset with or without incorporating four evolutionary features. CADD-precomputed scores are downloaded from <ext-link xlink:href="http://krishna.gs.washington.edu/download/CADD/v1.3/whole_genome_SNVs.tsv.gz" ext-link-type="uri">http://krishna.gs.washington.edu/download/CADD/v1.3/whole_genome_SNVs.tsv.gz</ext-link>, FunSeq2 precomputed scores are downloaded from <ext-link xlink:href="http://org.gersteinlab.funseq.s3-website-us-east-1.amazonaws.com/funseq2.1.2/hg19_NCscore_funseq216.tsv.bgz" ext-link-type="uri">http://org.gersteinlab.funseq.s3-website-us-east-1.amazonaws.com/funseq2.1.2/hg19_NCscore_funseq216.tsv.bgz</ext-link>, GERP precomputed scores are downloaded from <ext-link xlink:href="http://mendel.stanford.edu/SidowLab/downloads/gerp/hg19.GERP_scores.tar.gz" ext-link-type="uri">http://mendel.stanford.edu/SidowLab/downloads/gerp/hg19.GERP_scores.tar.gz</ext-link>, LINSIGHT precomputed scores are downloaded from <ext-link xlink:href="http://genome-mirror.cshl.edu/" ext-link-type="uri">http://genome-mirror.cshl.edu/</ext-link>, CDTS metrics are downloaded from <ext-link xlink:href="http://www.hli-opendata.com/noncoding" ext-link-type="uri">http://www.hli-opendata.com/noncoding</ext-link>. DeepSEA functional scores are computed locally based on 919 chromatin effect predictions and four evolutionary information–derived scores. DeepSEA functional significance score for a variant is defined as the product of the geometric mean E-value for predicted chromatin effects and the geometric mean <italic toggle="yes">E</italic>-value for evolutionary conservation features. We also assess DeepSEA functional score without 4 evolutionary features. The direction of different scores for all metrics is modified to ensure lower rank represents higher probability of pathogenicity. For held-out ClinVar test set, negative controls are subsampled by bootstrapping 10 times. For held-out GWAS variants, positive samples are subsampled by bootstrapping 10 times. To compare all methods, we compute false-positive versus true-positive rates for the complete range of score thresholds. Area under the receiver operating characteristic (AUROC) is used for benchmarking. Data processing can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S6</xref>.</p>
    </sec>
    <sec id="SEC2-7">
      <title>Benchmarking of classifier performance on CADD indels</title>
      <p>We download the dataset from CADD Developmental release: v1.4, resulting in 3 675 207 indels. This dataset is less biased and contains much larger examples than manually curated ClinVar or HGMD. CADD is partially trained on this dataset, containing 1 837 708 <italic toggle="yes">proxy-neutral</italic> variants and 1 837 499 simulated <italic toggle="yes">de novo proxy-deleterious</italic> variants. The former sets emerge since the last human-ape ancestor and are fixed in human populations, which are considered neutral (or, at most, weakly deleterious). The latter are considered free of selective pressure including both neutral and deleterious indels. We fine-tune LOGO-E2E on these CADD Indels and use the expert-curated dataset as held-out test set. We download known indels from NCBI/NIH ClinVar database (2020-10-03 release), which leads to 5556 pathogenic (defined as pathogenic and likely pathogenic in ClinVar) and 313 benign indels (defined as benign and likely benign in ClinVar), respectively. Due to class imbalance, we subsample positive indels five times to construct balanced test sets and benchmark against CADD, LINSIGHT and DeepSEA. Area under the receiver operating characteristic (AUROC) is used to benchmark different methods.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="SEC3">
    <title>RESULTS</title>
    <sec id="SEC3-1">
      <title>LOGO learns contextualized representation of k-mers of human reference genome and achieves state-of-the-art performance in promoter prediction task</title>
      <p>The backbone of LOGO processing flow is motivated by recently emerged Transformer-based bidirectional encoder model (<xref rid="B15" ref-type="bibr">15</xref>,<xref rid="B21" ref-type="bibr">21</xref>) (Figure <xref rid="F1" ref-type="fig">1A</xref>). We conduct pre-training on human reference genome hg19 comprised of totally 3 billion base pairs. We segment both forward and complementary chain of whole genome sequence into 100-bp bins and get 60 million segments. For each bin, we extend forward to 1000-bp along the genome to create training instances, which are analogous to input sentences in the field of natural language.</p>
      <fig position="float" id="F1">
        <label>Figure 1.</label>
        <caption>
          <p>Overview of LOGO. LOGO is firstly pre-trained on human reference genome hg19 and then fine-tuned on several downstream tasks. (<bold>A</bold>) LOGO uses self-attention based Transformer architecture, a light version language model (ALBERT) with only 2 self-attention layers with 256 hidden unites and 8 heads. The input genome sequence is segmented by sequential k-mer tokens (<italic toggle="yes">k</italic> = 3, 4, 5, 6). Token embeddings under 1000-bp or 2000-bp context are learned via masked language model (LM) task. The pre-training objective is to predict the randomly masked tokens by a Softmax layer over the vocabulary. Token embeddings and position embeddings are summed up and fed into ALBERT network. For sequence labelling tasks at global sequence level, including promoter identification, enhancer-promoter interaction (EPI) and chromatin features prediction, [CLS] token is used as global features extracted by LOGO, standing for aggregated representations of each input sequence for sequence classification task. [SEP] token stands for the end of each input sequence (Methods). (<bold>B</bold>) For variant prioritization task, LOGO utilizes a multi-stream scheme to encode reference allele, alternative allele, and corresponding altered position as input. A convolutional layer is added to introduce locality to capture allelic-effect at base-resolution. Pre-trained LOGO weights are used as model initializations for variant prioritization task (Materials and Methods).</p>
        </caption>
        <graphic xlink:href="gkac326fig1" position="float"/>
      </fig>
      <p>Conventional one-hot encoded representation for each nucleotide has limited vocabulary size of five characters (i.e. A, G, C, T and Unknown/Undetected), which is considered as a semantically poor representation. k-mer encodes sequence into a certain length of successive nucleotides. For instance, a trinucleotide is a k-mer for which <italic toggle="yes">k</italic> = 3. To increase the information content, we tokenize each sequence in the way of <italic toggle="yes">k</italic>-mer representation. The intuition is that each nucleotide is not independent such as codon rules in coding region and regulatory motifs in non-coding region. Recent phrase-level or entity-level masking strategies used by the NLP community proved to be better. BERT or ALBERT generally allows maximum sequence length of 512 tokens, thus k-mer setting can dramatically reduce the number of tokens required to incorporate a 1000-bp context. Token vocabulary size equals to 5<sup><italic toggle="yes">k</italic></sup> when using k-mer strategy. 7-mer or longer settings result in unbearable computation burden and memory overflow due to explosive vocabulary size. To balance the computation consumption and representation capacity, we evaluate four types of <italic toggle="yes">k</italic>-mer (<italic toggle="yes">k</italic> = 3, 4, 5, 6) to tokenize the genome. For any given sequence, different sets of k-mer representations can be generated when choosing different split positions. To avoid biased segmentation and further augment training data, we slide 1-bp (<italic toggle="yes">k</italic>-mer-1-stride) for each 1000-bp sequence to generate multiple sets (<italic toggle="yes">n</italic> = <italic toggle="yes">k</italic>) of <italic toggle="yes">k</italic>-mer tokens as input training instances.</p>
      <p>Before being fed into the Transformer network, each token representation is created by summing its corresponding token and position embeddings. Token embeddings are learned through projecting the k-mer vectors into a distributional embedding space. To utilize the order of each sequence, we inject absolute position information of each input sequence and make the model learn position embeddings of the same size as token embeddings. During pre-training stage, we only adopt ‘masked language model’(MLM) task to train a bidirectional representation of the human genome. 15% of tokens are randomly masked in each input sequence and the pre-training objective is to predict the masked token by a Softmax layer over the vocabulary. We choose 15% masking ratio according to the empirical choice as in the original BERT model. However, we apply masks at random positions across the genome instead of at fixed positions, which is expected to inject sampling diversities into the model. We denote the <italic toggle="yes">k</italic>-mer tokens embedding size as E, the number of encoder layers as L, and the hidden layer embedding size as H, the number of self-attention heads as A. Visualization of model architecture can be seen in Figure <xref rid="F1" ref-type="fig">1B</xref>. Hyperparameters are set as follows, <italic toggle="yes">E</italic> = 128; <italic toggle="yes">L</italic> = 2; <italic toggle="yes">H</italic> = 256 and <italic toggle="yes">A</italic> = 8. We pre-train LOGO with <italic toggle="yes">k</italic>-mer tokenization (<italic toggle="yes">k</italic> = 3, 4, 5, 6) on hg19 for a maximum of 50 epochs on four Nvidia Tesla V100 32G GPU. Model hyperparameters are determined by choosing a model size as minimal as possible without compromising the performance. Detailed hyperparameters and pre-training configuration can be seen in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref> and corresponding ablation studies on how to choose these hyperparameters can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S2–S4</xref>.</p>
      <p>Bigger k leads to larger vocabulary size, therefore requiring increased model parameters, more memory usage and longer convergence time. We assess the pre-training performance based on the accuracy (ACC) of masked tokens prediction. 3-mer tokenization achieves the highest MLM accuracy with a minimum training time spent per epoch (Figure <xref rid="F2" ref-type="fig">2A</xref>, <xref rid="F2" ref-type="fig">B</xref>). For all k-mer settings, LOGO can achieve inflection point of pre-training accuracy after five epochs, though already surpasses 0.875 when training less than one epoch, revealing recurring sequence patterns of human genome is effectively learned. One epoch training time for 3-mer tokenization setting is around 11.4 h, and we reach accuracy plateau (ACC = 0.893) after 15 epochs. One epoch training time for 6-mer tokenization setting is around 70.8 h, and we reach accuracy plateau (ACC = 0.887) after 25 epochs. However, accuracy at the pre-training stage is not directly correlated with utility of specific fine-tuning tasks. We assess all four <italic toggle="yes">k</italic>-mer settings for different downstream tasks and only report the best one. Other details of the pre-training assessment can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S5</xref>.</p>
      <fig position="float" id="F2">
        <label>Figure 2.</label>
        <caption>
          <p>LOGO learns contextualized representation of k-mers of the human reference genome and achieves state-of-the-art performance for promoter prediction and enhancer-promoter interaction prediction. (<bold>A</bold>) Pre-training accuracy (ACC) plateaus after five epochs for all k-mer settings. 3-mer tokenization achieves the highest ACC. (<bold>B</bold>) LOGO pre-training time of one epoch for all <italic toggle="yes">k</italic>-mer settings is plotted. (All settings are trained on four Nvidia Tesla V100 32G GPU). Larger <italic toggle="yes">k</italic> leads to longer training time due to larger vocabulary size. (<bold>C</bold>) Pre-trained LOGO is fine-tuned on enhancer-promoter interaction prediction task (LOGO-EPI) and evaluated against DeepTACT on promoter capture Hi-C (PCHi-C) datasets in six different cell types, including fetal thymus (FoeT, <italic toggle="yes">n</italic> = 6676), monocytes (Mon, <italic toggle="yes">n</italic> = 8062), naïve CD4+ T cell (nCD4, <italic toggle="yes">n</italic> = 8712), total B cells (tB, <italic toggle="yes">n</italic> = 9036), total CD4+ T cell (tCD4, <italic toggle="yes">n</italic> = 8282) and total CD8+ T cell (tCD8, <italic toggle="yes">n</italic> = 8140). Mean area under precision-recall curve (AUPRC) are evaluated using 10-fold cross-validation. (<bold>D</bold>) Pre-trained LOGO using 5-mer tokenization (LOGO-5-mer) is fine-tuned on promoter prediction task and evaluated against DeeReCT-PromID on promoter sequences from EPDnew Database, including ones with TATA-box (TATA+, <italic toggle="yes">n</italic> = 2067), without TATA-box (TATA–, <italic toggle="yes">n</italic> = 14 388) and jointly (both, <italic toggle="yes">n</italic> = 16 455). Knowledge embedded LOGO (LOGO-K-5-mer) further boost performance. 11 annotations terms from GenBank, i.e. 'CDS', ‘exon’, ‘enhancer’, ‘insulator’, ‘conserved_region’, ‘protein_binding_site’, ‘pseudogene’, ‘DNAseI_hypersensitive_site’, ‘nucleotide_cleavage_site’, ‘silencer’ and ‘gene’ are introduced in one-hot encoded format as knowledge input. Metrics of mean Recall, mean Precision and mean F1-score are evaluated using 10-fold cross-validation.</p>
        </caption>
        <graphic xlink:href="gkac326fig2" position="float"/>
      </fig>
      <p>We first evaluate the utility of pre-trained LOGO on human promoter prediction tasks via fine-tuning. Data processing details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S2</xref>. Computational identification of promoters is analogous to sequence labeling or sentence classification task in NLP. Umarov <italic toggle="yes">et al.</italic> have developed CNN-based deep learning models, DeeReCT-PromID (<xref rid="B31" ref-type="bibr">31</xref>), to predict human RNA pol II promoters, outperforming other previous prediction tools. For benchmark purposes, we generate datasets in the same way with DeeReCT-PromID and define a positive promoter region from −200 bp to +400 bp window around all potential Transcription Starting Site (TSS) from EPDnew Database (<xref rid="B32" ref-type="bibr">32</xref>). Promoters with TATA-box (TATA+) and without TATA-box (TATA-) are assessed separately and afterwards jointly (Both), which leads to 2067, 14 388 and 16 455 positive sequences, respectively. Negative ones are constructed by randomly sampling outside the promoter region without containing a known TSS. Leveraging previously pre-trained LOGO model weights as initialization, we simply plug in these 600-bp sequence inputs, and tokenize them via different <italic toggle="yes">k</italic>-mer-1-stride settings (<italic toggle="yes">k</italic> = 3, 4, 5, 6) and feed them into LOGO. When conducting sequence classification tasks, model input starts with a token [CLS] as in BERT and ALBERT. We use the final hidden vector of the [CLS] token as the aggregated representation for classification tasks and fine-tune model parameters in an end-to-end manner, which only introduces a few extra parameters in the final classification layer with sigmoid outputs. We use batch size of 128, set early-stop rules and fine-tune the model at most 20 epochs. The average training time per epoch is only around 45 s, which demonstrates excellent efficiency of ‘pre-training and fine-tuning’ paradigm. The best hyper-parameters are chosen based on the validation sets (Materials and Methods). We evaluate different <italic toggle="yes">k</italic>-mer settings of LOGO on promoter prediction tasks. LOGO significantly outperforms DeeReCT-PromID in all-settings as evaluated by Precision, Recall and F1-score metrics, as shown in Figure <xref rid="F2" ref-type="fig">2D</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S6</xref>. LOGO with 5-mer setting (LOGO-5-mer) achieves 15.0% point absolute improvement of mean F1-score than CNN-based DeeReCT-PromID (10-fold cross-validation) in case ‘Both’. LOGO has demonstrated its powerful representation utility, which suggests bidirectional attention-based architecture confers an advantage to capture complex semantics of promoter structure over CNN-based model. We also show that pre-training generally benefits downstream promoter identification and chromatin feature identification compared with end-to-end supervised learning with random initializations (<xref rid="sup1" ref-type="supplementary-material">Supplementary Tables S7,S8</xref>).</p>
      <p>We further explore a framework to integrate prior knowledge into LOGO on promoter prediction task. GenBank (<xref rid="B33" ref-type="bibr">33</xref>) contains rich functional annotations of the human genome sequence, including CDS, exon, gene, promoter, enhancer, silencer, pseudogene, insulator, conserved region, protein binding site, DNAse I hypersensitive site, nucleotide cleavage site and so on. These annotations can be regarded as prior knowledge of sequence inputs. We download 11 annotations terms from GenBank, i.e. ‘CDS’, ‘exon’, ‘enhancer’, ‘insulator’, ‘conserved_region’, ‘protein_binding_site’, ‘pseudogene’, ‘DNAseI_hypersensitive_site’, ‘nucleotide_cleavage_site’, ‘silencer’ and ‘gene’. Annotations of ‘promoter’ are abandoned to avoid direct label leakage. We generate annotation labels for each input sequence in a start-to-end spanning mode based on the hg19 coordinate. We propose a knowledge-enabled LOGO by adding input layers of one-hot encoded annotations and concatenating them with <italic toggle="yes">k</italic>-mer inputs (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref>). Knowledge embeddings, position embeddings and token embeddings are summed up and then fed into Transformer network for fine-tuning tasks in an end-to-end manner (Figure <xref rid="F1" ref-type="fig">1B</xref>). Knowledge embedded LOGO with 5-mer setting (LOGO-K-5-mer) achieves F1-score of 0.933, yielding extra 3.2% absolute improvement than LOGO-5-mer (Figure <xref rid="F2" ref-type="fig">2D</xref>). We demonstrate the configurability and utility of knowledge-embedded framework for genome sequence labelling. We caution that this attempt is preliminary and might introduce position bias or indirect label leakage into the model. We envision that rationally incorporating experimentally validated human knowledge can assist in developing better sequence representation model for scientific discovery.</p>
    </sec>
    <sec id="SEC3-2">
      <title>LOGO can be used to predict regulatory interactions between enhancer-promoter sequence pairs</title>
      <p>Predicting 3D chromatin contacts between promoters and enhancers is critical to understand transcriptional regulation in specific cell-lines or tissues. Computational approach is needed to improve the resolution of Hi-C data and detect genome-wide physical interactions at corresponding regulatory elements. This task is analogous to general inter-sentence modelling in NLP, such as sentence pairs in paraphrasing, hypothesis-premise pairs in entailment, and question-passage pairs in the question-answering task. We draw lessons from the NLP field and consider 3D chromatin contacts prediction as a sequence pairing problem.</p>
      <p>Li <italic toggle="yes">et al.</italic> proposed DeepTACT (<xref rid="B34" ref-type="bibr">34</xref>), a CNN and RNN mixed deep learning model with one attention layer to predict enhancer-promoter interactions (EPI). DeepTACT leverage both raw sequence and chromatin accessibility information, but we only benchmark LOGO against DeepTACT version without accessibility information input due to unavailability of processed chromatin features. Data for promoter-enhancer interaction and fine-tuning can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S3</xref>. We retrain DeepTACT and fine-tune LOGO (LOGO-EPI) on the same bootstrapped dataset provided by the author of DeepTACT, which contains three parts: 2000-bp window enhancer sequences (<xref rid="B35" ref-type="bibr">35</xref>), 1000-bp window promoter sequences (<xref rid="B36" ref-type="bibr">36</xref>) and paired enhancer–promoter interaction (EPI) labels from promoter capture Hi-C (PCHi-C) experiments in six different cell types (<xref rid="B37" ref-type="bibr">37</xref>), i.e. fetal thymus (FoeT), monocytes (Mon), naïve CD4+ T cell (nCD4), total B cells (tB), total CD4+ T cell (tCD4) and total CD8+ T cell (tCD8). Details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S9</xref>. Similar data augmentation technique is applied to generate larger positive training examples. The performance of each model is evaluated by 10-fold cross-validation. LOGO-EPI uses 6-mer setting to tokenize input sequences. We add one 1D convolution operation for each input promoter or enhancer sequence before being fed into the Transformer network. The underlying intuition is to avoid large fluctuations of token embeddings during the fine-tuning stage and ensure certain disparities among tokens (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S2</xref>). The learned representations of paired promoter and enhancer sequences are concatenated and fed into the final binary classification layer (model architecture seen in Figure S1). LOGO-EPI achieves 0.23–4.47% absolute improvement than DeepTACT on AUPRC for six cell lines (Figure <xref rid="F2" ref-type="fig">2C</xref>). LOGO-EPI outperforms DeepTACT most significantly for tCD4 and LOGO-EPI yields more consistent performance while DeepTACT fluctuates across different cell lines (AUPRC details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S10</xref>).</p>
    </sec>
    <sec id="SEC3-3">
      <title>LOGO achieves superior performance on chromatin features prediction with significantly reduced model size and much less training time than previous models</title>
      <p>Next, we move on to compare LOGO against CNN-based DeepSEA (<xref rid="B6" ref-type="bibr">6</xref>) to predict chromatin features from DNA sequences. Unlike fully supervised training manner as DeepSEA, we fine-tune the pre-trained LOGO with chromatin features prediction task and demonstrate higher accuracy with significantly improved scalability. To make a proper comparison as well as demonstrate model scalability, we use three sets of chromatin profiles with some overlaps; the first one is the same as the original DeepSEA paper with 919 chromatin features, the second one is 2002 chromatin features expanded by DeepSEA developer group reported in ExPecto (<xref rid="B38" ref-type="bibr">38</xref>), and we construct the third one of 3357 chromatin features by integrating ExPecto's 690 transcriptional factors (TF) binding features with recently released 2850 EpiMap (<xref rid="B39" ref-type="bibr">39</xref>) (for epigenome integration across multiple annotation projects) features after deduplication. Data details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S4</xref>.</p>
      <p>In the first task, we use the same training, validation, and test sets as in DeepSEA. LOGO-919 obtains 0.70%, 0.70% and 0.80% absolute improvement of median AUROC than downloaded DeepSEA for predicting 690 TF binding, 125 DNase hypersensitive sites (DHSs) and 104 histone modification marks (HM), respectively (Figure <xref rid="F3" ref-type="fig">3A</xref>). The maximum increase is for transcription repressor ZNF274 binding in HepG2 cell line (AUROC = 0.703 by LOGO-919 versus AUROC = 0.582 by DeepSEA). LOGO’s model architecture and training strategy confer huge advantage on computation efficiency and memory consumption over traditional deep CNN-based architecture completely trained in a multitask supervised manner. LOGO has a much smaller parameter size compared to DeepSEA. LOGO-919 contains around 1.52 million parameters, which is 34x fewer than DeepSEA’s 52.8 million parameters (Figure <xref rid="F3" ref-type="fig">3C</xref>). LOGO-919 obtains better performance than downloaded DeepSEA after 33 hours of pre-training and fine-tuning on 4 Nvidia Tesla V100 GPU (around 110 h on 1 Nvidia TITAN Xp Pascal GPU). We also retrain DeepSEA from scratch on 1 Nvidia Titan Pascal GPU and stop after 1 month (720 hours) and reproduce slightly poorer performance than the downloaded version, which indicates LOGO-919 takes at least 6× shorter training time than DeepSEA. (Figure <xref rid="F3" ref-type="fig">3D</xref>). The improvement in parameter efficiency is the most critical advantage of LOGO framework, which gives LOGO superior advantage to extend to ever-growing chromatin maps. Interestingly, more complex models sometimes lead to inferior performance for chromatin features prediction. We train another 8-layer LOGO model with around 20M parameters, the same order of magnitude with DeepSEA. However, the results are even worse, which again justifies our choice of model hyperparameters (details can be seen in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2, Table S3, Table S4, Table S15</xref>). The learned semantic-rich representation for k-mer tokens in a self-supervised manner alleviates the excessive needs of cumbersome model fitting for different supervised tasks from scratch. To demonstrate this concept, we conduct the second experiment using 2002 chromatin features, including 690 TF binding, 334 DHSs and 978 HM features as reported in ExPecto model (Dataset details of the number of chromatin features can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S16</xref>). ExPecto also used CNN-based architecture and extend DeepSEA by doubling the number of convolution layers to increase model depth to satisfy doubled learning objectives, ending up with around 150 million parameters, nearly 3-fold of DeepSEA. We retrain the chromatin marks prediction part of ExPecto and stop after 1000 h. Compared with DeepSEA, the number of learning tasks for Expecto is doubled, while model parameters tripled, which shows severe lack of scalability. For fair comparison against ExPecto, we incorporate a 2000-bp context window while remaining other settings unchanged and fine-tune LOGO-2002 within 66 hours. We choose the time upper bound according to the intuition of doubled training time (66 h versus 33 h) for doubled tasks (2002 features versus 919 features) (Figure <xref rid="F3" ref-type="fig">3D</xref>). The model size only marginally increases (1.87 million parameters) due to the longer input context and additional parameters of the final classification layer (Figure <xref rid="F3" ref-type="fig">3C</xref>). The model backbone remains unchanged, and the results show LOGO-2002 can achieve comparable median AUROC with retrained ExPecto on held-out chromosome within 66 hours. The median AUROC for TF, DHSs and HM is 0.954, 0.913, 0.883 respectively (Figure <xref rid="F3" ref-type="fig">3B</xref>). We demonstrate that LOGO can scale easily via pre-training and fine-tuning paradigm with benefits of computational speed and reduced parameterization. It is noted that DNABERT contains &gt;100 million parameters while LOGO only contains 1 million parameter, which domonstrates LOGO’s superior efficiency of parameter sharing among attention layers. In addition, LOGO predicts chromatin features in a jointly multi-task manner while DNABERT only supports TF-binding site prediction one TF by one TF, which is considered as a much simpler task and cannot effectively transfer the knowledge among different chromatin annotations. In light of LOGO’s superior performance for sequence feature identification over DeepSEA, we further check whether LOGO can recapitulate those 4 representative variants reported by the original DeepSEA paper, i.e. chr1:109817590 G &gt; T; chr16:209709 T &gt; C; chr10. 23508363 A &gt; G and chr16:52599188 C &gt; T. We collect 1000 bp of DNA sequences centered around each variant and implement ‘in silico’ saturated mutagenesis by LOGO to scan all potential single-nucleotide substitutions and evaluate these mutation effects for binding events. LOGO is able to identify canonical motifs such as TTGCTCAA for CEBPB (HepG2), TGATAA for GATA1 (K562), GTAAATA for FOXA1 (HepG2) and GTACATA for FOXA2 (HepG2). Detailed results can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S10</xref>.</p>
      <fig position="float" id="F3">
        <label>Figure 3.</label>
        <caption>
          <p>LOGO fine-tuned on chromatin profiles outperforms DeepSEA with significantly reduced parameter size and consumes much less training time. (<bold>A</bold>) Receiver operating characteristic (ROC) curve is plotted to compare predictive power between DeepSEA (top) and LOGO-919 (down) for 690 Transcription factor binding (TF), 125 DNase hypersensitive sites (DHSs) and 104 histone modification marks (HM) on held-out chromosome using 1,000-bp context window. Metrics of median area under receiver operating curve (AUROC) for all TF, DHSs and HM are displayed above each curve. (<bold>B</bold>) Boxplot shows AUROC for three types of features predicted by LOGO-2002 and LOGO-3357 (Methods). Box plots show median, upper, and lower quartiles, and highest and lowest values excluding outliers. (<bold>C</bold>) Plot shows parameter size of DeepSEA, ExPecto, LOGO-919, LOGO-2002 and LOGO-3357. (<bold>D</bold>) Plot shows comparison of training time among DeepSEA, LOGO-919, LOGO-2002 and LOGO-3357. Training time for DeepSEA is recorded as duration of reproducing DeepSEA from scratch using 1 TITAN Xp Pascal GPU, with slightly lower model performance than downloaded version. Training time of LOGO-919, LOGO-2002 and LOGO-3357 include both pre-training (about 2 epochs) and fine-tuning. Three sets of GPU configurations used to fine-tune LOGO are indicated by different colors.</p>
        </caption>
        <graphic xlink:href="gkac326fig3" position="float"/>
      </fig>
      <p>Incorporating more comprehensive chromatin profiles and using task-specific features have both been reported useful for functional analysis of noncoding variants (<xref rid="B40" ref-type="bibr">40</xref>,<xref rid="B41" ref-type="bibr">41</xref>). Abundant experimental mappings of human epigenomes are continuously accumulating chromatin profiles for more cell types and tissues. Further expanded chromatin features require larger CNN-based models with explosive parameters, while LOGO can be easily extended to more chromatin features with marginally increased parameters. LOGO demonstrates its powerful scalability and easy deployment, which is of critical importance to tackle even larger-scale functional maps. To further prove this concept, in the third experiment, we utilize the most comprehensive chromatin profiles from EpiMap and integrate them with all TF features from ExPecto. We construct datasets of a 2000-bp context window paired with a label vector for 3357 chromatin features using Selene (<xref rid="B42" ref-type="bibr">42</xref>). We fine-tune LOGO-3357 using the same model architecture as LOGO-2002, achieving median AUROC of 0.926, 0.928, 0.883 for 826 TF, 668 DHSs and 1863 HM features respectively (Figure <xref rid="F3" ref-type="fig">3B</xref>). The slightly lower performance than LOGO-919 and LOGO-2002 is mainly due to less stringent dataset construction and less precise position calibration for EpiMap related features. LOGO-3357 has nearly 2.22 million parameters, which is still significantly fewer than DeepSEA and ExPecto (Figure <xref rid="F3" ref-type="fig">3C</xref>), again demonstrating its scalability to triple chromatin features without the need of increasing model size substantially or extending disproportionate training time. All training details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S11</xref>.</p>
    </sec>
    <sec id="SEC3-4">
      <title>LOGO can be used to predict functional effects of noncoding variants at base-resolution and provides mechanistic insights for investigating complex diseases</title>
      <p>The associated loci identified by GWAS provide abundant information regarding the genetic basis of human complex diseases and traits. Nevertheless, owing to Linkage Disequilibrium (LD), it remains challenging to identify high-resolution causal variants in an interpretable manner (<xref rid="B43" ref-type="bibr">43</xref>). Variants from GWAS catalog predominantly consist of marginally associated variants that have not been fine-mapped. We attempt to extend LOGO to prioritize noncoding functional variants for complex diseases based on the predicted signals of the above three sets of chromatin features. We anticipate that, if a complex disease-related variant exerts its effect via disruption of TF binding motif or via alteration of DNA accessibility or histone modification, this SNP can be identified <italic toggle="yes">de novo</italic> from sequence by LOGO. We choose type-2 diabetes (T2D) as an example to test this hypothesis and construct evaluation datasets from the latest published literatures and GWAS resources. (Data Details see in Methods). We employ the same DeepSEA E-value metric to estimate the regulatory potential of a SNP by comparing the allele-specific probabilities per SNP to one million random SNPs from the 1000 Genome Project (Phase 3). In order to provide the community with a comprehensive catalog of LOGO annotated regulatory genome, we have implemented LOGO for all dbSNP reported non-coding SNPs and derive functional scores for each variant (Details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S8</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S17</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S13</xref>), which can be accessed at <ext-link xlink:href="https://github.com/melobio/LOGO" ext-link-type="uri">https://github.com/melobio/LOGO</ext-link>.</p>
      <p>First, we demonstrate LOGO can be used to prioritize putative causal regulatory variants from GWAS reported T2D-associated loci. We hypothesize that if LOGO is fine-tuned on more comprehensive chromatin profiles, it can identify more functional variants within LD blocks. We download all T2D-associated SNPs from GWAS Catalog (<xref rid="B44" ref-type="bibr">44</xref>) (2020-05-14 version, <italic toggle="yes">P</italic>-value ranging from 9<inline-formula><tex-math id="M00030" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–6</sup> to 6<inline-formula><tex-math id="M00031" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–447</sup>) and corresponding LD SNPs (<italic toggle="yes">r</italic><sup>2</sup>&gt; 0.2) from LDlink (<xref rid="B45" ref-type="bibr">45</xref>), resulting in 156,175 SNPs after deduplication. A variant is considered as functional significant if at least one chromatin feature's <italic toggle="yes">E</italic>-value is equal or less than 1<inline-formula><tex-math id="M00032" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–5</sup> (<xref rid="B6" ref-type="bibr">6</xref>,<xref rid="B46" ref-type="bibr">46</xref>). LOGO-3357 can identify more functional SNPs (<italic toggle="yes">n</italic> = 14 764) than LOGO-2002 (<italic toggle="yes">n</italic> = 729) and LOGO-919 (<italic toggle="yes">n</italic> = 374), details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S12</xref>. Within the 71 GWAS Catalog lead SNPs identified by LOGO-3357, 30 of them reach genome-wide significance (<italic toggle="yes">P</italic>-value &lt; 5<inline-formula><tex-math id="M00033" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–8</sup>) in at least one GWAS. We divide all functional significant SNPs identified by LOGO-2002 and LOGO-3357 into two groups (<italic toggle="yes">r</italic><sup>2</sup> ≥ 0.5 and <italic toggle="yes">r</italic><sup>2</sup>&lt; 0.5), we compare the mean activated chromatin features (<italic toggle="yes">E</italic>-value &lt; 1<inline-formula><tex-math id="M00034" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–5</sup>) of each group and discover that SNPs with higher LD activate more chromatin marks (<italic toggle="yes">P</italic>-value = 0.00093 for LOGO 3357, <italic toggle="yes">P</italic>-value = 0.02 for LOGO 2002, by Mann–Whitney <italic toggle="yes">U</italic>-test, details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S3</xref>). Inspired by Basenji (<xref rid="B47" ref-type="bibr">47</xref>) and Enformer (<xref rid="B48" ref-type="bibr">48</xref>), we also implement saturation mutagenesis to interpret several T2D-related SNPs in a visually interpretable manner. We refer to a European T2D fine-mapping study conducted by Mahajan (<xref rid="B49" ref-type="bibr">49</xref>), who characterized 51 variants with posterior probability of association (PPA) &gt;80% by incorporating islet-specific epigenome information. For example, PPA of rs963740 boosts from 50.3% to 87.9% by fGWAS (<xref rid="B50" ref-type="bibr">50</xref>) (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S9</xref>), a statistical package for integrating regulatory annotations into GWAS. LOGO correctly predicted strong alteration of related chromatin feature caused by minor allele relative to the major allele, the most activated feature also indicates rs11257655 position overlapping with strong islets enhancer region and modulating the known motif of the transcription factor FoxA2. LOGO suggests that perturbed TF binding within islets as a potential etiological mechanism for T2D (Figure <xref rid="F4" ref-type="fig">4E</xref>). Another example variant consistent with Mahajan's finding (<xref rid="B49" ref-type="bibr">49</xref>) is SNP rs963740 (located in the DLEU1 locus), which can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S9</xref>.</p>
      <fig position="float" id="F4">
        <label>Figure 4.</label>
        <caption>
          <p>LOGO can be used to infer underlying regulatory mechanisms of T2D GWAS signals and prioritize functional variants both inherited diseases and complex traits or diseases. (<bold>A</bold>) Tissue enrichment results for significant T2D-related variants identified by LOGO-2002 located in promoter/enhancer state regions, sorted by -log(<italic toggle="yes">P</italic>-value) (Hypergeometric test). The size of circle/diamond represents the number of activated chromatin marks in corresponding tissue or cell types, also displayed on the right side of the plot. Red symbols indicate four well-known tissue types related to T2D. Tissue or cell types enriched by EpiMap for T2D GWAS signals are represented by diamond shape. (<bold>B</bold>) Prediction power of various models for prioritizing HGMD regulatory mutations (<italic toggle="yes">n</italic> = 3266) against negative controls (<italic toggle="yes">n</italic> = 3266) in restricted scenario that negative samples matched to positive ones within 1 kb. (<bold>C</bold>) Prediction power of various models for prioritizing HGMD regulatory variants (<italic toggle="yes">n</italic> = 3498) against negative controls (<italic toggle="yes">n</italic> = 3690) in unrestricted scenario of random sampling. Mean AUROC of 10-fold cross-validation for B and C are reported, Error bars represent standard deviations. (<bold>D</bold>) Comparison of model performance by metric of mean AUROC on the held-out test set from inherited disease domain (top: <italic toggle="yes">n</italic> = 177 stringent ClinVar regulatory variants, negative controls are bootstrapped 10 times) and complex trait or disease domain (bottom: <italic toggle="yes">n</italic> = 2731 genome-wide significant non-coding variants from GWAS Catalog, positive variants are bootstrapped 10 times). (<bold>E</bold>) LOGO prediction for rs11257655 captures its influence on CDC123-CAMK1D locus. rs11257655 is associated with T2D disease and overlapped with islets epigenome map. In silico mutagenesis of the region surrounding rs11257655 reveals an affected transcription factor motif.</p>
        </caption>
        <graphic xlink:href="gkac326fig4" position="float"/>
      </fig>
      <p>We demonstrate that LOGO has the potential of fine-mapping causal variants within LD block in an explainable manner and the model fine-tuned on more chromatin features provides more functional attributions. We further conduct tissue-enrichment analysis for all putative functional variants identified by LOGO-2002. Hypergeometric test is used to evaluate whether activated chromatin features are enriched in certain categories. We find that these SNPs are functionally enriched in 18 categories out of total 27 with activation signals, including smooth muscle (<italic toggle="yes">n</italic> = 51), lymphoblastoid (<italic toggle="yes">n</italic> = 45), adipose (<italic toggle="yes">n</italic> = 20), muscle (<italic toggle="yes">n</italic> = 63), spleen (<italic toggle="yes">n</italic> = 10), and liver (<italic toggle="yes">n</italic> = 48), (–log(<italic toggle="yes">P</italic>-value) = 11.1, 5.3, 5.0, 3.2, 3.1 and 1.3 respectively), which is consistent with years of pathogenesis research that insulin mainly acts on liver, muscle and adipose as T2D-relevant tissues (Figure <xref rid="F4" ref-type="fig">4A</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S8</xref>). Figure <xref rid="F4" ref-type="fig">4A</xref> has some abbreviations as follows: T2D, type 2 diabetes; EEM, extra-embryonic membranes; ESC, embryonic stem cell; ES-deriv, embryonic stem-derived; iPSC, induced pluripotent stem cell; HSC, hematopoietic stem cell. Recent integrative epigenomics study (<xref rid="B39" ref-type="bibr">39</xref>) (EpiMap) leverages enhancer sharing tree to investigate tissue enrichment of T2D-related SNPs and indicates that T2D is polyfactorial trait enriched in up to 18 tissue categories out of 33 tested. Sequence-based LOGO-2002 shows similar diversity of enrichment with significant tissue overlaps. Islets only constitute ∼1% of the pancreas and specific annotations of islet epigenome are absent in ENCODE and Roadmap Epigenomes Project (<xref rid="B51" ref-type="bibr">51</xref>). Thus, analyzing the pancreas organs alone fails to provide reliable information of islet epigenomes. Thurner (<xref rid="B51" ref-type="bibr">51</xref>) and Varshney (<xref rid="B52" ref-type="bibr">52</xref>) have specifically annotated promoter/enhancer state of islets, which are regarded as typical T2D relevant cell types. Specifically, 10 functional SNPs identified by LOGO-3357 are overlapping with islets-specific promoter/enhancer state with at least five activated chromatin features (<italic toggle="yes">E</italic>-value &lt; 1<inline-formula><tex-math id="M00035" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–5</sup>) (Table <xref rid="tbl1" ref-type="table">1</xref>). The disruption of regulatory function is consistent with a previous report that parts of T2D-related risk variants are considered to act through primary effects on beta-cell function. For instance, T2D-risk allele at rs9693089 (FAM167A locus) locates at the active enhancer state of islet sample identified by Varshney (<xref rid="B52" ref-type="bibr">52</xref>) and has been reported to be associated with very low-density lipoprotein (VLDL) synthesis by Kraja (<xref rid="B53" ref-type="bibr">53</xref>). The corresponding activated chromatin features include H3K4me3, H3K4me1, H3K27ac. Even though LOGO-3357 is not specifically trained on islet chromatin marks, this experiment demonstrates that deep learning based methods have the potential of providing extra informativeness using sequence alone as input (<xref rid="B54" ref-type="bibr">54</xref>).</p>
      <table-wrap position="float" id="tbl1">
        <label>Table 1.</label>
        <caption>
          <p>Significant SNPs identified by LOGO-3357 overlapped with islet promoter/enhancer regions<sup>a</sup></p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="1" colspan="1">RS ID</th>
              <th rowspan="1" colspan="1">Locus</th>
              <th rowspan="1" colspan="1">Significant marks<sup>b</sup></th>
              <th rowspan="1" colspan="1">min E-value<sup>c</sup></th>
              <th rowspan="1" colspan="1">GWAS <italic toggle="yes">P</italic>-value<sup>d</sup></th>
              <th rowspan="1" colspan="1">GWAS odds<sup>d</sup></th>
              <th rowspan="1" colspan="1">1000G AF</th>
              <th rowspan="1" colspan="1">Paper (PMID)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">rs9693089<break/>
chr8:11298385 A-G</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">FAM167A</italic>
              </td>
              <td rowspan="1" colspan="1">H3K4me3<break/>
H3K4me1<break/>
H3K27ac</td>
              <td rowspan="1" colspan="1">0.000001</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">0.68111</td>
              <td rowspan="1" colspan="1">23192668</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">rs4735337<break/>
chr8:95973465 T-C</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">TP53INP1</italic>
              </td>
              <td rowspan="1" colspan="1">H3K4me3<break/>
H3K4me1<break/>
DNase-seq</td>
              <td rowspan="1" colspan="1">0.000001</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">0.552516</td>
              <td rowspan="1" colspan="1">25393876</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">rs1126899<break/>
chr7:130021488 G-C</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">CPA1</italic>
              </td>
              <td rowspan="1" colspan="1">H3K4me1</td>
              <td rowspan="1" colspan="1">0.000001</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">0.582268</td>
              <td rowspan="1" colspan="1">-</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">rs11774700<break/>
chr8:118220270 T-C</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">LOC105375716</italic>
              </td>
              <td rowspan="1" colspan="1">HNF4G</td>
              <td rowspan="1" colspan="1">0.000001</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">0.271965</td>
              <td rowspan="1" colspan="1">21188353</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">rs163800<break/>
chr20:57578508 T-C</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">CTSZ</italic>
              </td>
              <td rowspan="1" colspan="1">H3K27ac</td>
              <td rowspan="1" colspan="1">0.000001</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">0.000399361</td>
              <td rowspan="1" colspan="1">29795304</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">rs4383259<break/>
chr19:53661337 A-G</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">ZNF347</italic>
              </td>
              <td rowspan="1" colspan="1">H3K4me1<break/>
H3K27ac</td>
              <td rowspan="1" colspan="1">0.000001</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">0.752196</td>
              <td rowspan="1" colspan="1">24306210</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">rs3176447<break/>
chr1:51433687 T-A</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">CDKN2C</italic>
              </td>
              <td rowspan="1" colspan="1">DNase-seq</td>
              <td rowspan="1" colspan="1">0.000001</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">0.0740815</td>
              <td rowspan="1" colspan="1">21145615</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">rs11671664<break/>
chr19:46172278 G-A</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">GIPR</italic>
              </td>
              <td rowspan="1" colspan="1">H3K27me3</td>
              <td rowspan="1" colspan="1">0.000001</td>
              <td rowspan="1" colspan="1">3E-12</td>
              <td rowspan="1" colspan="1">4.22[2.73–5.71]</td>
              <td rowspan="1" colspan="1">0.155152</td>
              <td rowspan="1" colspan="1">27480816</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">rs998451<break/>
chr2:135429288 G-A</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">TMEM163</italic>
              </td>
              <td rowspan="1" colspan="1">CEBPB</td>
              <td rowspan="1" colspan="1">0.000001</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">0.10643</td>
              <td rowspan="1" colspan="1">24843659</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">rs1776897<break/>
chr6:34195011 G-T</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">EP300</td>
              <td rowspan="1" colspan="1">0.000004</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">-</td>
              <td rowspan="1" colspan="1">0.776757</td>
              <td rowspan="1" colspan="1">27104953</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="T1TFN1">
            <p><sup>a</sup>Islet promoter and enhancer regions are annotated by Thurner (<xref rid="B51" ref-type="bibr">51</xref>) and Varshney (<xref rid="B52" ref-type="bibr">52</xref>).</p>
          </fn>
          <fn id="T1TFN2">
            <p><sup>b</sup>Significant Marks means all chromatin marks with E-value &lt; 1 × 10<sup>–5</sup>.</p>
          </fn>
          <fn id="T1TFN3">
            <p><sup>c</sup>Min E-value means the minimum E-value of corresponding chromatin mark activated by LOGO-3357.</p>
          </fn>
          <fn id="T1TFN4">
            <p><sup>d</sup>GWAS <italic toggle="yes">P</italic>-value and GWAS odds are only shown for lead SNP reported from GWAS Catalog.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Second, we demonstrate that LOGO can be a sequence-based tool to help interpret those GWAS signals with possible population bias or sample size limitations. The statistical power of GWAS relies heavily on sample size, allele frequency and effect size of candidate SNPs (<xref rid="B55" ref-type="bibr">55</xref>). GWAS with inadequate sample size can result in a multitude of nominally significant loci (<italic toggle="yes">P</italic>-value &lt; 0.05). This problem is mainly mitigated by expanding the sample size or conducting meta-analysis across cohorts or ethnic groups. Sequence based LOGO model is expected not to be affected by allele frequency or population bias and can evaluate both common and rare variants <italic toggle="yes">ab initio</italic>. We illustrate this potential using the following examples. In study GCST005414 (<xref rid="B56" ref-type="bibr">56</xref>), rs340874 (PROX1 locus) reaches nominally significant (<italic toggle="yes">P</italic>-value = 1<inline-formula><tex-math id="M00036" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–7</sup>). However, this SNP achieves genome-wide significant in study GCST009379 (<xref rid="B49" ref-type="bibr">49</xref>)/GCST006867 (<xref rid="B57" ref-type="bibr">57</xref>) (<italic toggle="yes">P</italic>-value = 2<inline-formula><tex-math id="M00037" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–22</sup> and 8<inline-formula><tex-math id="M00038" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–18</sup>, respectively) with larger sample size. PROX1 has been reported to be associated with after-meal metabolism (<xref rid="B58" ref-type="bibr">58</xref>), non-esterified fatty acids, and glucose metabolism (<xref rid="B59" ref-type="bibr">59</xref>), which is also validated in both Japanese and Chinese populations (<xref rid="B60" ref-type="bibr">60</xref>,<xref rid="B61" ref-type="bibr">61</xref>) (MAF = 0.376). LOGO-3357 can directly identify rs340874 as a functional significant SNP (1 activated feature with <italic toggle="yes">E</italic>-value ≤ 1<inline-formula><tex-math id="M00039" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–5</sup>, transcription factor POLR2A). PROX1 is reported to be a target gene of the POLR2A transcription factor from the ENCODE Transcription Factor Targets dataset. Rs896854 (TP53INP1 locus) is perceived to be associated with lipid levels of the Chinese population with nominal significant signal (<italic toggle="yes">P</italic>-value = 2<inline-formula><tex-math id="M00040" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–6</sup>) in the study GCST004894 (<xref rid="B62" ref-type="bibr">62</xref>) and genome-wide significant signal (<italic toggle="yes">P</italic>-value = 1<inline-formula><tex-math id="M00041" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–9</sup>) in the study GCST000712 (<xref rid="B63" ref-type="bibr">63</xref>). Rs516946 (ANK1 locus) is reported in several independent studies to be correlated with decrease of insulin level and dysfunction of pancreatic islet cells at a nearby site (<xref rid="B64" ref-type="bibr">64</xref>). Again, LOGO-3357 can identify both rs896854 (1 activated feature with <italic toggle="yes">E</italic>-value ≤ 1<inline-formula><tex-math id="M00042" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–5</sup>, Blood &amp; T-cell, H3K9me3) and rs516946 (1 activated feature with <italic toggle="yes">E</italic>-value ≤ 1<inline-formula><tex-math id="M00043" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–5</sup>, Other, DNase-seq) to be functional. Furthermore, we evaluate another 43 regulatory variants with posterior probability of association (PPA) &gt;80% in a recent fine-mapping study (<xref rid="B49" ref-type="bibr">49</xref>). 2 SNPs are identified as functional significant by LOGO-3357 (rs340874 at PROX1 locus and rs76549217 at ANKH locus). Another largest-scale T2D meta-analysis study accumulates 1.4 million samples and discovered 318 new loci (<xref rid="B65" ref-type="bibr">65</xref>), out of which LOGO-3357 can identify 14 SNPs to be functional. It is worth mentioning that all these 14 SNPs do not reach genome-wide significance in other populations except European ancestry. This result further indicates the unbiased predictive power of LOGO. 16 reported SNPs validated by LOGO-3357 with corresponding activated features are listed in <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S14</xref>. We demonstrate that LOGO fine-tuned on chromatin features can help interpret GWAS non-coding SNPs and provide hints regarding underlying tissue-specific regulatory mechanism.</p>
    </sec>
    <sec id="SEC3-5">
      <title>Introducing locality-sensitive encoding scheme and convolution facilitates prioritizing functional variants for both inherited diseases and complex traits or diseases</title>
      <p>Next, we evaluate whether fine-tuning LOGO can be used to develop functional predictor of pathogenic regulatory single-nucleotide variants (SNV) or common GWAS phenotype-associated SNPs. We define two schemes of fine-tuning: end-to-end training on the binary label of deleteriousness (LOGO-E2E) and two-stage training of chromatin features prediction followed by variant prioritization (LOGO-C2P) (<xref rid="B6" ref-type="bibr">6</xref>,<xref rid="B40" ref-type="bibr">40</xref>). Perturbation of molecular phenotypes can serve as an indicator of potential deleteriousness inspired by DeepSEA. We compare LOGO against six common predictors, including evolution-based method (GERP) (<xref rid="B66" ref-type="bibr">66</xref>), sequence-based predictor based on chromatin effect signals with four evolutionary conservation features (DeepSEA) (<xref rid="B6" ref-type="bibr">6</xref>), functional genome-based method (Funseq2) (<xref rid="B67" ref-type="bibr">67</xref>), evolutionary method incorporating functional genome features (LINSIGHT) (<xref rid="B68" ref-type="bibr">68</xref>), machine learning based classifier (CADD) (<xref rid="B69" ref-type="bibr">69</xref>) and genome diversity metric (CDTS) (<xref rid="B70" ref-type="bibr">70</xref>). It is noted that DeepSEA and CADD can provide allele-specific evaluations, whereas others assign identical scores to all alternative variants. Our predictors, LOGO-E2E and LOGO-C2P, are designed to capture allelic effect.</p>
      <p>For variants associated with inherited human diseases, we extract a dataset from Human Gene Mutation Database (version 2019–03) (<xref rid="B71" ref-type="bibr">71</xref>) to define positive examples of stringent regulatory mutations. We construct negative controls from 1000 Genomes Project (<xref rid="B72" ref-type="bibr">72</xref>) SNPs by stringent frequency and population control, resulting in 3498 pathogenic regulatory mutations and 3,690 negatives (total 7,188 variants, details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S6</xref>). We use 10-fold cross-validation to make a robust comparison. For each fold, test variants are ensured to be scorable across methods. To increase stringency, we consider two schemes of negative sets selection: random sampling (unrestricted), and negative samples matched to positive ones within 1 kb (restricted, total 6532 variants). Dataset construction details are illustrated in <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S6</xref>.</p>
      <p>For LOGO-E2E, we use three layers to encode variant presence and allelic information at specific position, including the Ref layer, Alt layer and Variant Type layer. Ref layer is used to encode 1,000-bp context with 6-mer-1-stride setting. (Model architecture details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S4</xref>) Alt layer is used to encode alternative allele at certain position to enforce the model to see directional alteration. Another Variant Type layer is set as default for SNV. By this means, we explicitly encode the alternative allele, ensure the ALT allele is always the effect allele. Each variant with surrounding context of certain length will be encoded as a matrix input containing ‘Ref’, ‘Alt’ and ‘Type’ information. 1-dimension convolutional layer is added before fed token embeddings into LOGO to learn the binary deleteriousness effect of the target variant. Fine-tuning LOGO in this way is expected to learn allelic pathogenicity. For LOGO-C2P, we follow similar pipelines in DeepSEA’s functional SNP prioritization part and firstly use previously trained LOGO-919/LOGO-2002 to generate chromatin effect features for both reference and alternative allele. We then conduct the same absolute difference and relative log fold change transformation as DeepSEA and feed these features into boosted logistic regression model to train the classifier at the second stage. It is worth mentioning that we discard the z-score transformation used in DeepSEA-C2P classifier. We also assess the difference between preserving or removing evolutionary conservation features. The original scores of LINSIGHT, CADD, FunSeq2, GERP, CDTS and DeepSEA functional significant score are used to obtain the binary classification result with full range of thresholds.</p>
      <p>In the end-to-end setting, LOGO-E2E outperforms all other methods in restricted negative control scenario (AUROC = 0.722) (Figure <xref rid="F4" ref-type="fig">4B</xref>) and performs the second in the scenario of unrestricted negative control (AUROC = 0.823). It is consistent with previous finding that restricted scenario poses more difficulties for distinguishing functional sites from surroundings than separating functional regions from genome background. Nonetheless, LOGO-E2E leverages an Alt token layer to enforce the model to explicitly encode allele position and directional mutation event to be distinguished from nearby unchanged context, which equips the model with allelic specificity under 1,000-bp context. For the less challenging unrestricted task, LOGO-E2E performs slightly worse than LINSIGHT(AUROC = 0.847), one possible reason might be that LOGO has not been trained on population genomic data with conservation information to witness enough genome diversity from human and other related outgroup species. To overcome these shortcomings, LOGO-2002-C2P incorporates four evolutionary conservation features as in DeepSEA (PhastCons scores (<xref rid="B73" ref-type="bibr">73</xref>), PhyloP scores (<xref rid="B74" ref-type="bibr">74</xref>), and GERP++ neutral evolution (<xref rid="B75" ref-type="bibr">75</xref>) and rejected substitution scores (<xref rid="B66" ref-type="bibr">66</xref>)) and achieves the highest performance (AUROC = 0.883) (Figure <xref rid="F4" ref-type="fig">4C</xref>) in the scenario of the unrestricted negative control.</p>
      <p>It is noted that all compared methods except DeepSEA-C2P are not specifically trained on the HGMD dataset. To avoid potential over-fitting controversy and assess the generalizability of LOGO-C2P, we extract from ClinVar database (<xref rid="B76" ref-type="bibr">76</xref>) to define an independent test set with 177 highly confident non-coding pathogenic SNVs (Data details in Methods and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S5</xref>, all splicing variants removed). Analysis with LOGO-E2E, LOGO-919-C2P and LOGO-2002-C2P are compared with that of CADD, LINSIGHT, GERP, Funseq2, CDTS, DeepSEA and DeepSEA-C2P (Figure <xref rid="F4" ref-type="fig">4D</xref>). DeepSEA and DeepSEA-w/o Evo means DeepSEA derived functional significant score including and excluding four evolutionary features, respectively. DeepSEA-C2P represents a logistic regression model based on 919 chromatin marks and 4 evolutionary features. LOGO-E2E means LOGO fine-tuned on binary label of allelic deleteriousness in an end-to-end manner. LOGO-919-C2P and LOGO-2002-C2P stand for LOGO fine-tuned on prediction of 919/2002 chromatin features with four evolutionary features followed by allele-specific variant prioritization via logistic regression model. LOGO-2002-C2P ranks third (AUROC = 0.927) and significantly outperforms CDTS (AUROC = 0.734) but is slightly worse than LINSIGHT (AUROC = 0.946) and CADD(AUROC = 0.966). CDTS solely relies on 11,257 whole-genome sequences to obtain 7-mer constraint under 550-bp context of human species, whose lack of interspecies conservation leads to poorer performance to evaluate fitness consequence of inherited disease related variants. LOGO-E2E (AUROC = 0.769) is only trained on 3498 HGMD variants yet performs better than genome diversity based CDTS, which again proves end-to-end fine-tuning architecture captures some intrinsic features of non-coding genome by only using a few annotated examples. LOGO-C2P is only trained on HGMD dataset and proved to be well generalizable on the ClinVar dataset. LINSIGHT is trained on human polymorphism data from 54 unrelated individuals and three outgroup species divergence data from aligned primates genomes conditioned on 48 genomic features, revealing the utility of incorporating genome diversity information to interpret non-coding genome. CADD is trained with more than 60 genome annotations on a much larger dataset (<italic toggle="yes">n</italic> = 30 million) containing fixed or nearly fixed variants in human populations but is absent in human-ape ancestor as proxy-neutral variants and matched proxy-deleterious variants, which is essentially designed for binary classification of fitness consequence. The superior performance of LOGO-C2P, LINSIGHT and CADD shows that evolutionary information is likely to be powerful to identify regulatory pathogenic variants that tend to be under strong purifying selection.</p>
      <p>We conduct another benchmark experiment to prioritize complex trait or disease-associated variants. GWAS variants are generally of weaker functional impact than HGMD mutations. We construct a positive test set by extracting all genome-wide significant variants (<italic toggle="yes">P</italic>-value &lt; 5<inline-formula><tex-math id="M00044" notation="LaTeX">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$ \times$\end{document}</tex-math></inline-formula>10<sup>–8</sup>) replicated in at least two independent studies from GWAS Catalog followed by retaining SNPs overlapped with ENCODE candidate cis-Regulatory Elements (ccREs) (<xref rid="B2" ref-type="bibr">2</xref>) and fixation index (<italic toggle="yes">F</italic><sub>ST</sub>) lower than 0.01 to ensure little genetic differentiation (<xref rid="B77" ref-type="bibr">77</xref>,<xref rid="B78" ref-type="bibr">78</xref>). We ensure that all test variants have never been used in previous HGMD experiment, resulting in 2,731 positive GWAS SNPs and 704 negative controls. We bootstrap 10 times to obtain balanced held-out test set of 1408 variants (Data details in <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S6</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S6</xref>). All predictors show reduced performance. Compared with more deleterious HGMD mutations under significant purifying selection, common GWAS-associated variants have smaller effect size with lower evolutionary conservation, thereby plausibly more difficult to predict. LOGO-C2P-2002 is the top performer (AUROC = 0.841) (Figure <xref rid="F4" ref-type="fig">4D</xref>) across all methods We show that LOGO-C2P-2002 has the advantages of considering both chromatin effects and evolutionary constraint at base-resolution. Though LOGO-C2P is solely fine-tuned on HGMD mutations, the result proves its domain transferability from inherited diseases to common phenotypes. The second-best predictor is LOGO-919-C2P (AUROC = 0.832), indicating the benefit of broad collection of chromatin features. LOGO-919-C2P outperforms DeepSEA-C2P (AUROC = 0.804), which again demonstrates the edge of attention-based Transformer over CNN-based architecture. For these two independent evaluations, LOGO-C2P performs relatively better than CADD and LINSIGHT in GWAS domain than ClinVar domain, which suggests that chromatin features are more informative for complex traits while evolutionary information is more important for inherited diseases. This is consistent with the hypothesis that highly deleterious mutations of genetic diseases are subject to stronger selection than complex disease loci (<xref rid="B79" ref-type="bibr">79</xref>). Recent EpiMap results also emphasize the central role of dense, rich, high-resolution epigenomic annotations to investigate regulatory circuitry of complex disease. LOGO-C2P exhibits its capability of integrating sequence context, regulatory annotation, and evolutionary constraint either explicitly or implicitly at different levels. It is noted that CDTS, which solely relies on human genetic diversity, shows poorer performance in both rare and common disease scenarios. We argue that the statistical test of 7-mer regional tolerance is not powerful enough to capture complex semantics underlying human genome sequence, even though more than 10,000 human genomes are incorporated (<xref rid="B70" ref-type="bibr">70</xref>). For the variant prioritization task, we also benchmark against Basenji and achieve better performance on a small dataset. Details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S12</xref>.</p>
      <p>In order to demonstrate the utility of incorporating 1D convolution, we conduct ablation experiments on GWAS Catalog SNPs benchmark dataset and obtain better result than convolution-excluded version (Details can be found in <xref rid="sup1" ref-type="supplementary-material">Supplementary Text S7</xref>, <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S13</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S11</xref>). The convolution module adds three kinds of channel information, 2-mer, 3-mer and 5-mer, which can effectively provide more diverse contextual information and help the model clearly distinguish the variant change before and after the mutation. In addition, the convolution operation reduces the weight updating frequency and makes parameter updating more stable during the fine-tuning process, which is well suited for variant effects prediction task at base-resolution.</p>
      <p>Furthermore, we explore LOGO performance of prioritizing pathogenicity of small insertion or deletion variants (Indels). We fine-tune LOGO in a similar way with LOGO-E2E using 3-mer tokenization with 1000-bp context (LOGO-E2E-Indel) on a much larger dataset from CADD Developmental release: v1.4 with 3,675,207 indels, including similar number of human-derived variants and simulations (<xref rid="B69" ref-type="bibr">69</xref>). We evaluate model performance of LOGO-E2E-Indel against LINSIGHT, CADD and DeepSEA-w/o Evo (excluding evolutionary features) on independent test set, consisting of 5869 non-coding Indels (&lt;48 bp) from ClinVar recent release (clinvar_20201003), including 5556 positive samples (defined as pathogenic and likely pathogenic in ClinVar) and 313 negative samples (defined as benign and likely benign in ClinVar). LOGO-E2E-Indel achieves the best performance (AUROC = 0.743) across all compared methods (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S7</xref>). These results indicate that LOGO-E2E can effectively utilize the learned semantic representations from pre-training and shows stronger generalizability for downstream classification tasks than CADD, which is trained in a fully supervised manner.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="SEC4">
    <title>DISCUSSION</title>
    <p>Genome sequence contains tremendous biological information regarding the species to which it belongs. Even though a multitude of high-throughput biochemical assays have been used to characterize the sequences, the complex nature of genome poses tremendous challenges to well interpret it. It is impractical to exhaustively perform functional annotations at every position in all conditions, and current assay design is believed to only cover the tip of the iceberg due to the limitations of existing hypothesis. A substantial gap remains between the outcomes of these experiments and a comprehensive understanding of the whole genome, especially those regulatory regions. New computational approaches are in pressing need to help interpret the underlying code. Motivated by recent huge progress in the field of NLP and CV, we propose a light language model called LOGO, utilizing ALBERT-version Transformer architecture for sequence labelling, and integrating convolution with a novel input encoding scheme for base-resolution interpretation.</p>
    <p>Learning from raw reference genome successfully equips the model with strong adaptability across various downstream tasks by fine-tuning. No explicit annotation label is given during pre-training stage, and we have shown that the intrinsic bidirectional representations learned by the model can easily extend to sequence labelling tasks. In chromatin features prediction task, LOGO achieves higher accuracy than DeepSEA with significantly reduced parameters in much shorter computing time. Facing the needs of continuously growing number of functional annotations, we demonstrate that supervised multitask learning incurs problem of parameter explosion and tedious architecture tuning, while LOGO can efficiently extend to more abundant features with marginally increased parameters and trivial modification. Sequence-based chromatin effects prediction is informative to characterize GWAS SNPs via identifying certain disruption of regulatory function. These results offer a strong justification that developing pre-trained language model can enable accurate, fast, scalable, and robust genome modelling. The community can benefit from simply and economically fine-tuning the pre-trained LOGO for specific chromatin profiles of intertest with trivial effort. By initializing model with pre-trained weights, only one additional output layer needs to be modified instead of extensive architecture tuning. We also show that fine-tuning LOGO with an explicit ref/alt token encoding strategy and convolutional operation proves powerful to prioritize functional non-coding variants associated with human disease at base-resolution.</p>
    <p>It is noted that LOGO is only trained on human reference genome hg19. We envision that introducing genome diversity in pre-training stage can further boost representation power. This can be done by feeding LOGO with all currently identified variants across human populations and from other related outgroup species, which is expected to automatically learn evolutionary conservation and context-dependent constraint across the genome. The learned representation will in turn facilitates variant function prediction and evolutionary landscape discovery. We make an analogy between biological sequence and human language that genome possesses diversified combinations of words or phrases without compromising intrinsic grammar constraints. Overall, LOGO offers a versatile strategy to represent both global and local patterns of the human genome and sheds light on unearthing more value of ever-growing WGS data in the boom of national genome project.</p>
    <p>We hypothesize that there exist many dimensions not yet captured by LOGO. The intrinsic property of naïve self-attention based Transformer model leads to its incompetency to capture even longer-range context (<xref rid="B80" ref-type="bibr">80</xref>), which is essential for modeling distal regulatory dependency across human genome. Recently, Enformer (<xref rid="B47" ref-type="bibr">47</xref>) combines dilated convolutions and Transformers to model interactions up to 100 kb away and successfully links remote enhancers to target genes. In the future, we would like to explore whether designing advanced LOGO via incorporating hierarchical interactive mechanism (<xref rid="B81" ref-type="bibr">81</xref>) would solve the problem. For example, encoding a 1 Mb DNA sequence with ‘sentence Transformer’ and then feeding it to ‘document Transformer’. LOGO can also be fine-tuned on tissue or cell-type specific expression profiles to investigate variant effects, potentially shedding light on eQTL fine-mapping and cis-regulatory evolution. Furthermore, there could be alternative ways to construct underlying vocabulary and define pre-training objectives with a further optimized masking strategy. We already show that injecting knowledge post hoc into the model can help boost performance. On the other hand, we anticipate that a large amount of existing somewhat noisy knowledgebase can be utilized to further boost the effectiveness of deep learning model. For example, sequence annotation databases (<xref rid="B82" ref-type="bibr">82</xref>,<xref rid="B83" ref-type="bibr">83</xref>) and biological networks (<xref rid="B84" ref-type="bibr">84</xref>) can be introduced systematically and structurally to guide self-supervised representation learning of genome sequence or inspire novel knowledge-guided masking strategy design. This will in turn help construct a better downstream prediction model in a more interpretable manner. In addition, LOGO can be reconfigured into a generative version, potentially be used to improve <italic toggle="yes">in silico</italic> mutagenesis efficiency and assess artificially designed sequences in the field of genome editing and synthetic biology. Integrating adversarial feedback loop of functional constraint into language model can potentially aid perturbation experiment and rational <italic toggle="yes">de novo</italic> design of new regulatory circuit (<xref rid="B85" ref-type="bibr">85</xref>,<xref rid="B86" ref-type="bibr">86</xref>).</p>
  </sec>
  <sec sec-type="data-availability" id="SEC5">
    <title>DATA AVAILABILITY</title>
    <p>All datasets in this study are derived from published resources and can be generated following protocols described in Methods. Demo data is available on Github at <ext-link xlink:href="https://github.com/melobio/LOGO" ext-link-type="uri">https://github.com/melobio/LOGO</ext-link>. LOGO functional scores for all noncoding SNPs can be found on figshare at <ext-link xlink:href="https://doi.org/10.6084/m9.figshare.19149827.v2" ext-link-type="uri">https://doi.org/10.6084/m9.figshare.19149827.v2</ext-link>.</p>
  </sec>
  <sec id="SEC6">
    <title>CODE AVAILABILITY</title>
    <p>LOGO is written in Python. The source code is available on Github at <ext-link xlink:href="https://github.com/melobio/LOGO" ext-link-type="uri">https://github.com/melobio/LOGO</ext-link>.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>gkac326_Supplemental_File</label>
      <media xlink:href="gkac326_supplemental_file.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ACK1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>We thank Zhenzhong Lan from Westlake University for discussions regarding pre-trained language model. We thank Peng Cheng Laboratory, Shenzhen, Guangdong, China for providing GPU resources to train the model.</p>
    <p><italic toggle="yes">Author contributions</italic>: M.Y. conceived the problem and designed the study. H.M.Y. and F.M. supervised the work. M.Y., L.C.H., H.P.H., and H.T. performed deep learning experiments. L.C.H, H.T. and N.Z. performed bioinformatics analysis. J.H.W. provided detailed guidance on analysis of disease-related regulatory mutations. M.Y. wrote the manuscript, and other authors made modifications.</p>
  </ack>
  <sec id="SEC8">
    <title>SUPPLEMENTARY DATA</title>
    <p><ext-link xlink:href="https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkac326#supplementary-data" ext-link-type="uri">Supplementary Data</ext-link> are available at NAR Online.</p>
  </sec>
  <sec id="SEC9">
    <title>FUNDING</title>
    <p>This research is supported by Guangdong Provincial Academician Workstation of BGI Synthetic Genomics [2017B090904014]; Jihong Wu is supported by Program of Shanghai Academic Research Leader [20XD1401100] and Program for Outstanding Medical Academic Leader [2019LJ01].</p>
    <p><italic toggle="yes">Conflict of interest statement</italic>. F.M. declares the following competing interests: stock holdings in MGI.</p>
  </sec>
  <ref-list id="REF1">
    <title>REFERENCES</title>
    <ref id="B1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><collab>ENCODE Project Consortium</collab><article-title>An integrated encyclopedia of DNA elements in the human genome</article-title>. <source>Nature</source>. <year>2012</year>; <volume>489</volume>:<fpage>57</fpage>.<pub-id pub-id-type="pmid">22955616</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Moore</surname><given-names>J.E.</given-names></string-name>, <string-name><surname>Purcaro</surname><given-names>M.J.</given-names></string-name>, <string-name><surname>Pratt</surname><given-names>H.E.</given-names></string-name>, <string-name><surname>Epstein</surname><given-names>C.B.</given-names></string-name>, <string-name><surname>Shoresh</surname><given-names>N.</given-names></string-name>, <string-name><surname>Adrian</surname><given-names>J.</given-names></string-name>, <string-name><surname>Hardison</surname><given-names>R.C.</given-names></string-name>, <string-name><surname>Gingeras</surname><given-names>T.R.</given-names></string-name>, <string-name><surname>Stamatoyannopoulos</surname><given-names>J.A.</given-names></string-name>, <string-name><surname>Weng</surname><given-names>Z.</given-names></string-name></person-group><article-title>Expanded encyclopaedias of DNA elements in the human and mouse genomes</article-title>. <source>Nature</source>. <year>2020</year>; <volume>583</volume>:<fpage>699</fpage>–<lpage>710</lpage>.<pub-id pub-id-type="pmid">32728249</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kundaje</surname><given-names>A.</given-names></string-name>, <string-name><surname>Meuleman</surname><given-names>W.</given-names></string-name>, <string-name><surname>Ernst</surname><given-names>J.</given-names></string-name>, <string-name><surname>Bilenky</surname><given-names>M.</given-names></string-name>, <string-name><surname>Yen</surname><given-names>A.</given-names></string-name>, <string-name><surname>Heravi-Moussavi</surname><given-names>A.</given-names></string-name>, <string-name><surname>Stamatoyannopoulos</surname><given-names>J.A.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>T.</given-names></string-name>, <string-name><surname>Kellis</surname><given-names>M.</given-names></string-name></person-group><article-title>Integrative analysis of 111 reference human epigenomes</article-title>. <source>Nature</source>. <year>2015</year>; <volume>518</volume>:<fpage>317</fpage>–<lpage>330</lpage>.<pub-id pub-id-type="pmid">25693563</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Visscher</surname><given-names>P.M.</given-names></string-name>, <string-name><surname>Wray</surname><given-names>N.R.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Q.</given-names></string-name>, <string-name><surname>Sklar</surname><given-names>P.</given-names></string-name>, <string-name><surname>McCarthy</surname><given-names>M.I.</given-names></string-name>, <string-name><surname>Brown</surname><given-names>M.A.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>J.</given-names></string-name></person-group><article-title>10 years of GWAS discovery: biology, function, and translation</article-title>. <source>Am. J. Hum. Genet</source>. <year>2017</year>; <volume>101</volume>:<fpage>5</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">28686856</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Eraslan</surname><given-names>G.</given-names></string-name>, <string-name><surname>Avsec</surname><given-names>Ž.</given-names></string-name>, <string-name><surname>Gagneur</surname><given-names>J.</given-names></string-name>, <string-name><surname>Theis</surname><given-names>F.J.</given-names></string-name></person-group><article-title>Deep learning: new computational modelling techniques for genomics</article-title>. <source>Nat. Rev. Genet</source>. <year>2019</year>; <volume>20</volume>:<fpage>389</fpage>–<lpage>403</lpage>.<pub-id pub-id-type="pmid">30971806</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Troyanskaya</surname><given-names>O.G.</given-names></string-name></person-group><article-title>Predicting effects of noncoding variants with deep learning–based sequence model</article-title>. <source>Nat. Methods</source>. <year>2015</year>; <volume>12</volume>:<fpage>931</fpage>–<lpage>934</lpage>.<pub-id pub-id-type="pmid">26301843</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <label>7.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mikolov</surname><given-names>T.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>K.</given-names></string-name>, <string-name><surname>Corrado</surname><given-names>G.</given-names></string-name>, <string-name><surname>Dean</surname><given-names>J.</given-names></string-name></person-group><article-title>Efficient estimation of word representations in vector space</article-title>. <year>2013</year>; <comment>arXiv doi:</comment><comment>07 September 2013, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</uri>.</mixed-citation>
    </ref>
    <ref id="B8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cybenko</surname><given-names>G.</given-names></string-name></person-group><article-title>Approximation by superpositions of a sigmoidal function</article-title>. <source>Math. Control Signals Syst.</source><year>1989</year>; <volume>2</volume>:<fpage>303</fpage>–<lpage>314</lpage>.</mixed-citation>
    </ref>
    <ref id="B9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Elman</surname><given-names>J.L.</given-names></string-name></person-group><article-title>Finding structure in time</article-title>. <source>Cogn. Sci.</source><year>1990</year>; <volume>14</volume>:<fpage>179</fpage>–<lpage>211</lpage>.</mixed-citation>
    </ref>
    <ref id="B10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hochreiter</surname><given-names>S.</given-names></string-name>, <string-name><surname>Schmidhuber</surname><given-names>J.</given-names></string-name></person-group><article-title>Long short-term memory</article-title>. <source>Neural Comput.</source><year>1997</year>; <volume>9</volume>:<fpage>1735</fpage>–<lpage>1780</lpage>.<pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <label>11.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Vaswani</surname><given-names>A.</given-names></string-name>, <string-name><surname>Shazeer</surname><given-names>N.</given-names></string-name>, <string-name><surname>Parmar</surname><given-names>N.</given-names></string-name>, <string-name><surname>Uszkoreit</surname><given-names>J.</given-names></string-name>, <string-name><surname>Jones</surname><given-names>L.</given-names></string-name>, <string-name><surname>Gomez</surname><given-names>A.N.</given-names></string-name>, <string-name><surname>Polosukhin</surname><given-names>I.</given-names></string-name></person-group><article-title>Attention is all you need</article-title>. <year>2017</year>; <comment>arXiv doi:</comment><comment>06 December 2017, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</uri>.</mixed-citation>
    </ref>
    <ref id="B12">
      <label>12.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Peters</surname><given-names>M.E.</given-names></string-name>, <string-name><surname>Neumann</surname><given-names>M.</given-names></string-name>, <string-name><surname>Iyyer</surname><given-names>M.</given-names></string-name>, <string-name><surname>Gardner</surname><given-names>M.</given-names></string-name>, <string-name><surname>Clark</surname><given-names>C.</given-names></string-name>, <string-name><surname>Lee</surname><given-names>K.</given-names></string-name>, <string-name><surname>Zettlemoyer</surname><given-names>L.</given-names></string-name></person-group><article-title>Deep contextualized word representations</article-title>. <year>2018</year>; <comment>arXiv doi:</comment><comment>22 March 2018, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1802.05365">https://arxiv.org/abs/1802.05365</uri>.</mixed-citation>
    </ref>
    <ref id="B13">
      <label>13.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Howard</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ruder</surname><given-names>S.</given-names></string-name></person-group><article-title>Universal language model fine-tuning for text classification</article-title>. <year>2018</year>; <comment>arXiv doi:</comment><comment>23 May 2018, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1801.06146">https://arxiv.org/abs/1801.06146</uri>.</mixed-citation>
    </ref>
    <ref id="B14">
      <label>14.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Radford</surname><given-names>A.</given-names></string-name>, <string-name><surname>Narasimhan</surname><given-names>K.</given-names></string-name>, <string-name><surname>Salimans</surname><given-names>T.</given-names></string-name>, <string-name><surname>Sutskever</surname><given-names>I.</given-names></string-name></person-group><article-title>Improving language understanding by generative pre-training</article-title>. <year>2018</year>; <uri xlink:href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</uri>.</mixed-citation>
    </ref>
    <ref id="B15">
      <label>15.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Devlin</surname><given-names>J.</given-names></string-name>, <string-name><surname>Chang</surname><given-names>M.W.</given-names></string-name>, <string-name><surname>Lee</surname><given-names>K.</given-names></string-name>, <string-name><surname>Toutanova</surname><given-names>K.</given-names></string-name></person-group><article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>. <year>2018</year>; <comment>arXiv doi:</comment><comment>24 May 2019, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</uri>.</mixed-citation>
    </ref>
    <ref id="B16">
      <label>16.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Dai</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Carbonell</surname><given-names>J.</given-names></string-name>, <string-name><surname>Salakhutdinov</surname><given-names>R.</given-names></string-name>, <string-name><surname>Le</surname><given-names>Q.V.</given-names></string-name></person-group><article-title>Xlnet: generalized autoregressive pretraining for language understanding</article-title>. <year>2019</year>; <comment>arXiv doi:</comment><comment>02 January 2020, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1906.08237">https://arxiv.org/abs/1906.08237</uri>.</mixed-citation>
    </ref>
    <ref id="B17">
      <label>17.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Dong</surname><given-names>L.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>N.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>W.</given-names></string-name>, <string-name><surname>Wei</surname><given-names>F.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>X.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Hon</surname><given-names>H.W.</given-names></string-name></person-group><article-title>Unified language model pre-training for natural language understanding and generation</article-title>. <year>2019</year>; <comment>arXiv doi:</comment><comment>15 October 2019, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1905.03197">https://arxiv.org/abs/1905.03197</uri>.</mixed-citation>
    </ref>
    <ref id="B18">
      <label>18.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Song</surname><given-names>K.</given-names></string-name>, <string-name><surname>Tan</surname><given-names>X.</given-names></string-name>, <string-name><surname>Qin</surname><given-names>T.</given-names></string-name>, <string-name><surname>Lu</surname><given-names>J.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>T.Y.</given-names></string-name></person-group><article-title>Mass: masked sequence to sequence pre-training for language generation</article-title>. <year>2019</year>; <comment>arXiv doi:</comment><comment>21 June 2019, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1905.02450">https://arxiv.org/abs/1905.02450</uri>.</mixed-citation>
    </ref>
    <ref id="B19">
      <label>19.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>X.</given-names></string-name>, <string-name><surname>He</surname><given-names>P.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>W.</given-names></string-name>, <string-name><surname>Gao</surname><given-names>J.</given-names></string-name></person-group><article-title>Multi-task deep neural networks for natural language understanding</article-title>. <year>2019</year>; <comment>arXiv doi:</comment><comment>30 May 2019, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1901.11504">https://arxiv.org/abs/1901.11504</uri>.</mixed-citation>
    </ref>
    <ref id="B20">
      <label>20.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lample</surname><given-names>G.</given-names></string-name>, <string-name><surname>Conneau</surname><given-names>A.</given-names></string-name></person-group><article-title>Cross-lingual language model pretraining</article-title>. <year>2019</year>; <comment>arXiv doi:</comment><comment>22 January 2019, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1901.07291">https://arxiv.org/abs/1901.07291</uri>.</mixed-citation>
    </ref>
    <ref id="B21">
      <label>21.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Lan</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>M.</given-names></string-name>, <string-name><surname>Goodman</surname><given-names>S.</given-names></string-name>, <string-name><surname>Gimpel</surname><given-names>K.</given-names></string-name>, <string-name><surname>Sharma</surname><given-names>P.</given-names></string-name>, <string-name><surname>Soricut</surname><given-names>R.</given-names></string-name></person-group><article-title>Albert: a lite bert for self-supervised learning of language representations</article-title>. <year>2019</year>; <comment>arXiv doi:</comment><comment>09 February 2020, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1909.11942">https://arxiv.org/abs/1909.11942</uri>.</mixed-citation>
    </ref>
    <ref id="B22">
      <label>22.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Ott</surname><given-names>M.</given-names></string-name>, <string-name><surname>Goyal</surname><given-names>N.</given-names></string-name>, <string-name><surname>Du</surname><given-names>J.</given-names></string-name>, <string-name><surname>Joshi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>D.</given-names></string-name>, <string-name><surname>Stoyanov</surname><given-names>V.</given-names></string-name></person-group><article-title>Roberta: a robustly optimized bert pretraining approach</article-title>. <year>2019</year>; <comment>arXiv doi:</comment><comment>26 July 2019, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1907.11692">https://arxiv.org/abs/1907.11692</uri>.</mixed-citation>
    </ref>
    <ref id="B23">
      <label>23.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Clark</surname><given-names>K.</given-names></string-name>, <string-name><surname>Luong</surname><given-names>M.T.</given-names></string-name>, <string-name><surname>Le</surname><given-names>Q.V.</given-names></string-name>, <string-name><surname>Manning</surname><given-names>C.D.</given-names></string-name></person-group><article-title>Electra: Pre-training text encoders as discriminators rather than generators</article-title>. <year>2020</year>; <comment>arXiv doi:</comment><comment>23 March 2020, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/2003.10555">https://arxiv.org/abs/2003.10555</uri>.</mixed-citation>
    </ref>
    <ref id="B24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Qiu</surname><given-names>X.</given-names></string-name>, <string-name><surname>Sun</surname><given-names>T.</given-names></string-name>, <string-name><surname>Xu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Shao</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Dai</surname><given-names>N.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>X.</given-names></string-name></person-group><article-title>Pre-trained models for natural language processing: a survey</article-title>. <source>Sci. China: Technol. Sci.</source><year>2020</year>; <volume>63</volume>:<fpage>1872</fpage>–<lpage>1897</lpage>.</mixed-citation>
    </ref>
    <ref id="B25">
      <label>25.</label>
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Beltagy</surname><given-names>I.</given-names></string-name>, <string-name><surname>Lo</surname><given-names>K.</given-names></string-name>, <string-name><surname>Cohan</surname><given-names>A.</given-names></string-name></person-group><article-title>SciBERT: a pretrained language model for scientific text</article-title>. <year>2019</year>; <comment>arXiv doi:</comment><comment>10 September 2019, preprint: not peer reviewed</comment><uri xlink:href="https://arxiv.org/abs/1903.10676">https://arxiv.org/abs/1903.10676</uri>.</mixed-citation>
    </ref>
    <ref id="B26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lee</surname><given-names>J.</given-names></string-name>, <string-name><surname>Yoon</surname><given-names>W.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>S.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>D.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>S.</given-names></string-name>, <string-name><surname>So</surname><given-names>C.H.</given-names></string-name>, <string-name><surname>Kang</surname><given-names>J.</given-names></string-name></person-group><article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>. <source>Bioinformatics</source>. <year>2020</year>; <volume>36</volume>:<fpage>1234</fpage>–<lpage>1240</lpage>.<pub-id pub-id-type="pmid">31501885</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rives</surname><given-names>A.</given-names></string-name>, <string-name><surname>Goyal</surname><given-names>S.</given-names></string-name>, <string-name><surname>Meier</surname><given-names>J.</given-names></string-name>, <string-name><surname>Guo</surname><given-names>D.</given-names></string-name>, <string-name><surname>Ott</surname><given-names>M.</given-names></string-name>, <string-name><surname>Zitnick</surname><given-names>C.L.</given-names></string-name>, <string-name><surname>Fergus</surname><given-names>R.</given-names></string-name></person-group><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>. <source>Proc. Natl Acad. Sci. U.S.A.</source><year>2021</year>; <volume>118</volume>:<fpage>e2016239118</fpage>.<pub-id pub-id-type="pmid">33876751</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ji</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>H.</given-names></string-name>, <string-name><surname>Davuluri</surname><given-names>R.V.</given-names></string-name></person-group><article-title>DNABERT: pre-trained bidirectional encoder representations from transformers model for DNA-language in genome</article-title>. <source>Bioinformatics</source>. <year>2021</year>; <volume>37</volume>:<fpage>2112</fpage>–<lpage>2120</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>d'Ascoli</surname><given-names>S.</given-names></string-name>, <string-name><surname>Touvron</surname><given-names>H.</given-names></string-name>, <string-name><surname>Leavitt</surname><given-names>M.</given-names></string-name>, <string-name><surname>Morcos</surname><given-names>A.</given-names></string-name>, <string-name><surname>Biroli</surname><given-names>G.</given-names></string-name>, <string-name><surname>Sagun</surname><given-names>L.</given-names></string-name></person-group><article-title>Convit: improving vision transformers with soft convolutional inductive biases</article-title>. <source>PMLR</source>. <year>2021</year>; <volume>139</volume>:<fpage>2286</fpage>–<lpage>2296</lpage>.</mixed-citation>
    </ref>
    <ref id="B30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dai</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>H.</given-names></string-name>, <string-name><surname>Le</surname><given-names>Q.V.</given-names></string-name>, <string-name><surname>Tan</surname><given-names>M.</given-names></string-name></person-group><article-title>CoAtNet: marrying convolution and attention for all data sizes</article-title>. <source>NeurIPS</source>. <year>2021</year>; <volume>34</volume>:<fpage>3965</fpage>–<lpage>3977</lpage>.</mixed-citation>
    </ref>
    <ref id="B31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Umarov</surname><given-names>R.</given-names></string-name>, <string-name><surname>Kuwahara</surname><given-names>H.</given-names></string-name>, <string-name><surname>Li</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Gao</surname><given-names>X.</given-names></string-name>, <string-name><surname>Solovyev</surname><given-names>V.</given-names></string-name></person-group><article-title>Promoter analysis and prediction in the human genome using sequence-based deep learning models</article-title>. <source>Bioinformatics</source>. <year>2019</year>; <volume>35</volume>:<fpage>2730</fpage>–<lpage>2737</lpage>.<pub-id pub-id-type="pmid">30601980</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dreos</surname><given-names>R.</given-names></string-name>, <string-name><surname>Ambrosini</surname><given-names>G.</given-names></string-name>, <string-name><surname>Groux</surname><given-names>R.</given-names></string-name>, <string-name><surname>Cavin Périer</surname><given-names>R.</given-names></string-name>, <string-name><surname>Bucher</surname><given-names>P.</given-names></string-name></person-group><article-title>The eukaryotic promoter database in its 30th year: focus on non-vertebrate organisms</article-title>. <source>Nucleic Acids Res.</source><year>2017</year>; <volume>45</volume>:<fpage>D51</fpage>–<lpage>D55</lpage>.<pub-id pub-id-type="pmid">27899657</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Benson</surname><given-names>D.A.</given-names></string-name>, <string-name><surname>Cavanaugh</surname><given-names>M.</given-names></string-name>, <string-name><surname>Clark</surname><given-names>K.</given-names></string-name>, <string-name><surname>Karsch-Mizrachi</surname><given-names>I.</given-names></string-name>, <string-name><surname>Ostell</surname><given-names>J.</given-names></string-name>, <string-name><surname>Pruitt</surname><given-names>K.D.</given-names></string-name>, <string-name><surname>Sayers</surname><given-names>E.W.</given-names></string-name></person-group><article-title>GenBank</article-title>. <source>Nucleic Acids Res.</source><year>2018</year>; <volume>46</volume>:<fpage>D41</fpage>–<lpage>D47</lpage>.<pub-id pub-id-type="pmid">29140468</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>W.</given-names></string-name>, <string-name><surname>Wong</surname><given-names>W.H.</given-names></string-name>, <string-name><surname>Jiang</surname><given-names>R.</given-names></string-name></person-group><article-title>DeepTACT: predicting 3D chromatin contacts via bootstrapping deep learning</article-title>. <source>Nucleic Acids Res.</source><year>2019</year>; <volume>47</volume>:<fpage>e60</fpage>.<pub-id pub-id-type="pmid">30869141</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Noguchi</surname><given-names>S.</given-names></string-name>, <string-name><surname>Arakawa</surname><given-names>T.</given-names></string-name>, <string-name><surname>Fukuda</surname><given-names>S.</given-names></string-name>, <string-name><surname>Furuno</surname><given-names>M.</given-names></string-name>, <string-name><surname>Hasegawa</surname><given-names>A.</given-names></string-name>, <string-name><surname>Hori</surname><given-names>F.</given-names></string-name>, <string-name><surname>Wolvetang</surname><given-names>E.</given-names></string-name></person-group><article-title>FANTOM5 CAGE profiles of human and mouse samples</article-title>. <source>Scientific Data</source>. <year>2017</year>; <volume>4</volume>:<fpage>170112</fpage>.<pub-id pub-id-type="pmid">28850106</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cunningham</surname><given-names>F.</given-names></string-name>, <string-name><surname>Amode</surname><given-names>M.R.</given-names></string-name>, <string-name><surname>Barrell</surname><given-names>D.</given-names></string-name>, <string-name><surname>Beal</surname><given-names>K.</given-names></string-name>, <string-name><surname>Billis</surname><given-names>K.</given-names></string-name>, <string-name><surname>Brent</surname><given-names>S.</given-names></string-name>, <string-name><surname>Flicek</surname><given-names>P.</given-names></string-name></person-group><article-title>Ensembl 2015</article-title>. <source>Nucleic Acids Res.</source><year>2015</year>; <volume>43</volume>:<fpage>D662</fpage>–<lpage>D669</lpage>.<pub-id pub-id-type="pmid">25352552</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Javierre</surname><given-names>B.M.</given-names></string-name>, <string-name><surname>Burren</surname><given-names>O.S.</given-names></string-name>, <string-name><surname>Wilder</surname><given-names>S.P.</given-names></string-name>, <string-name><surname>Kreuzhuber</surname><given-names>R.</given-names></string-name>, <string-name><surname>Hill</surname><given-names>S.M.</given-names></string-name>, <string-name><surname>Sewitz</surname><given-names>S.</given-names></string-name>, <string-name><surname>Fraser</surname><given-names>P.</given-names></string-name></person-group><article-title>Lineage-specific genome architecture links enhancers and non-coding disease variants to target gene promoters</article-title>. <source>Cell</source>. <year>2016</year>; <volume>167</volume>:<fpage>1369</fpage>–<lpage>1384</lpage>.<pub-id pub-id-type="pmid">27863249</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Theesfeld</surname><given-names>C.L.</given-names></string-name>, <string-name><surname>Yao</surname><given-names>K.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>K.M.</given-names></string-name>, <string-name><surname>Wong</surname><given-names>A.K.</given-names></string-name>, <string-name><surname>Troyanskaya</surname><given-names>O.G.</given-names></string-name></person-group><article-title>Deep learning sequence-based ab initio prediction of variant effects on expression and disease risk</article-title>. <source>Nat. Genet.</source><year>2018</year>; <volume>50</volume>:<fpage>1171</fpage>–<lpage>1179</lpage>.<pub-id pub-id-type="pmid">30013180</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boix</surname><given-names>C.A.</given-names></string-name>, <string-name><surname>James</surname><given-names>B.T.</given-names></string-name>, <string-name><surname>Park</surname><given-names>Y.P.</given-names></string-name>, <string-name><surname>Meuleman</surname><given-names>W.</given-names></string-name>, <string-name><surname>Kellis</surname><given-names>M.</given-names></string-name></person-group><article-title>Regulatory genomic circuitry of human disease loci by integrative epigenomics</article-title>. <source>Nature</source>. <year>2021</year>; <volume>590</volume>:<fpage>300</fpage>–<lpage>307</lpage>.<pub-id pub-id-type="pmid">33536621</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Park</surname><given-names>C.Y.</given-names></string-name>, <string-name><surname>Theesfeld</surname><given-names>C.L.</given-names></string-name>, <string-name><surname>Wong</surname><given-names>A.K.</given-names></string-name>, <string-name><surname>Yuan</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Scheckel</surname><given-names>C.</given-names></string-name>, <string-name><surname>Troyanskaya</surname><given-names>O.G.</given-names></string-name></person-group><article-title>Whole-genome deep-learning analysis identifies contribution of noncoding mutations to autism risk</article-title>. <source>Nat. Genet.</source><year>2019</year>; <volume>51</volume>:<fpage>973</fpage>–<lpage>980</lpage>.<pub-id pub-id-type="pmid">31133750</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Richter</surname><given-names>F.</given-names></string-name>, <string-name><surname>Morton</surname><given-names>S.U.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>S.W.</given-names></string-name>, <string-name><surname>Kitaygorodsky</surname><given-names>A.</given-names></string-name>, <string-name><surname>Wasson</surname><given-names>L.K.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>K.M.</given-names></string-name>, <string-name><surname>Gelb</surname><given-names>B.D.</given-names></string-name></person-group><article-title>Genomic analyses implicate noncoding de novo variants in congenital heart disease</article-title>. <source>Nat. Genet.</source><year>2020</year>; <volume>52</volume>:<fpage>769</fpage>–<lpage>777</lpage>.<pub-id pub-id-type="pmid">32601476</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>K.M.</given-names></string-name>, <string-name><surname>Cofer</surname><given-names>E.M.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Troyanskaya</surname><given-names>O.G.</given-names></string-name></person-group><article-title>Selene: a pytorch-based deep learning library for sequence data</article-title>. <source>Nat. Methods</source>. <year>2019</year>; <volume>16</volume>:<fpage>315</fpage>–<lpage>318</lpage>.<pub-id pub-id-type="pmid">30923381</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schaid</surname><given-names>D.J.</given-names></string-name>, <string-name><surname>Chen</surname><given-names>W.</given-names></string-name>, <string-name><surname>Larson</surname><given-names>N.B.</given-names></string-name></person-group><article-title>From genome-wide associations to candidate causal variants by statistical fine-mapping</article-title>. <source>Nat. Rev. Genet.</source><year>2018</year>; <volume>19</volume>:<fpage>491</fpage>–<lpage>504</lpage>.<pub-id pub-id-type="pmid">29844615</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buniello</surname><given-names>A.</given-names></string-name>, <string-name><surname>MacArthur</surname><given-names>J.A.L.</given-names></string-name>, <string-name><surname>Cerezo</surname><given-names>M.</given-names></string-name>, <string-name><surname>Harris</surname><given-names>L.W.</given-names></string-name>, <string-name><surname>Hayhurst</surname><given-names>J.</given-names></string-name>, <string-name><surname>Malangone</surname><given-names>C.</given-names></string-name>, <string-name><surname>Parkinson</surname><given-names>H.</given-names></string-name></person-group><article-title>The NHGRI-EBI GWAS catalog of published genome-wide association studies, targeted arrays and summary statistics 2019</article-title>. <source>Nucleic Acids Res.</source><year>2019</year>; <volume>47</volume>:<fpage>D1005</fpage>–<lpage>D1012</lpage>.<pub-id pub-id-type="pmid">30445434</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Machiela</surname><given-names>M.J.</given-names></string-name>, <string-name><surname>Chanock</surname><given-names>S.J.</given-names></string-name></person-group><article-title>LDlink: a web-based application for exploring population-specific haplotype structure and linking correlated alleles of possible functional variants</article-title>. <source>Bioinformatics</source>. <year>2015</year>; <volume>31</volume>:<fpage>3555</fpage>–<lpage>3557</lpage>.<pub-id pub-id-type="pmid">26139635</pub-id></mixed-citation>
    </ref>
    <ref id="B46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Arloth</surname><given-names>J.</given-names></string-name>, <string-name><surname>Eraslan</surname><given-names>G.</given-names></string-name>, <string-name><surname>Andlauer</surname><given-names>T.F.</given-names></string-name>, <string-name><surname>Martins</surname><given-names>J.</given-names></string-name>, <string-name><surname>Iurato</surname><given-names>S.</given-names></string-name>, <string-name><surname>Kühnel</surname><given-names>B.</given-names></string-name>, <string-name><surname>Mueller</surname><given-names>N.S.</given-names></string-name></person-group><article-title>DeepWAS: multivariate genotype-phenotype associations by directly integrating regulatory information using deep learning</article-title>. <source>PLoS Comput. Biol.</source><year>2020</year>; <volume>16</volume>:<fpage>e1007616</fpage>.<pub-id pub-id-type="pmid">32012148</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kelley</surname><given-names>D.R.</given-names></string-name>, <string-name><surname>Reshef</surname><given-names>Y.A.</given-names></string-name>, <string-name><surname>Bileschi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Belanger</surname><given-names>D.</given-names></string-name>, <string-name><surname>McLean</surname><given-names>C.Y.</given-names></string-name>, <string-name><surname>Snoek</surname><given-names>J.</given-names></string-name></person-group><article-title>Sequential regulatory activity prediction across chromosomes with convolutional neural networks</article-title>. <source>Genome Res.</source><year>2018</year>; <volume>28</volume>:<fpage>739</fpage>–<lpage>750</lpage>.<pub-id pub-id-type="pmid">29588361</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Avsec</surname><given-names>Ž.</given-names></string-name>, <string-name><surname>Agarwal</surname><given-names>V.</given-names></string-name>, <string-name><surname>Visentin</surname><given-names>D.</given-names></string-name>, <string-name><surname>Ledsam</surname><given-names>J.R.</given-names></string-name>, <string-name><surname>Grabska-Barwinska</surname><given-names>A.</given-names></string-name>, <string-name><surname>Taylor</surname><given-names>K.R.</given-names></string-name>, <string-name><surname>Assael</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Jumper</surname><given-names>J.</given-names></string-name></person-group><article-title>Effective gene expression prediction from sequence by integrating long-range interactions</article-title>. <source>Nat. Methods</source>. <year>2021</year>; <volume>18</volume>:<fpage>1196</fpage>–<lpage>1203</lpage>.<pub-id pub-id-type="pmid">34608324</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mahajan</surname><given-names>A.</given-names></string-name>, <string-name><surname>Taliun</surname><given-names>D.</given-names></string-name>, <string-name><surname>Thurner</surname><given-names>M.</given-names></string-name>, <string-name><surname>Robertson</surname><given-names>N.R.</given-names></string-name>, <string-name><surname>Torres</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Rayner</surname><given-names>N.W.</given-names></string-name>, <string-name><surname>McCarthy</surname><given-names>M.I.</given-names></string-name></person-group><article-title>Fine-mapping type 2 diabetes loci to single-variant resolution using high-density imputation and islet-specific epigenome maps</article-title>. <source>Nat. Genet.</source><year>2018</year>; <volume>50</volume>:<fpage>1505</fpage>–<lpage>1513</lpage>.<pub-id pub-id-type="pmid">30297969</pub-id></mixed-citation>
    </ref>
    <ref id="B50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pickrell</surname><given-names>J.K.</given-names></string-name></person-group><article-title>Joint analysis of functional genomic data and genome-wide association studies of 18 human traits</article-title>. <source>Am. J. Hum. Genet.</source><year>2014</year>; <volume>94</volume>:<fpage>559</fpage>–<lpage>573</lpage>.<pub-id pub-id-type="pmid">24702953</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Thurner</surname><given-names>M.</given-names></string-name>, <string-name><surname>Van De Bunt</surname><given-names>M.</given-names></string-name>, <string-name><surname>Torres</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Mahajan</surname><given-names>A.</given-names></string-name>, <string-name><surname>Nylander</surname><given-names>V.</given-names></string-name>, <string-name><surname>Bennett</surname><given-names>A.J.</given-names></string-name>, <string-name><surname>McCarthy</surname><given-names>M.I.</given-names></string-name></person-group><article-title>Integration of human pancreatic islet genomic data refines regulatory mechanisms at type 2 diabetes susceptibility loci</article-title>. <source>Elife</source>. <year>2018</year>; <volume>7</volume>:<fpage>e31977</fpage>.<pub-id pub-id-type="pmid">29412141</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Varshney</surname><given-names>A.</given-names></string-name>, <string-name><surname>Scott</surname><given-names>L.J.</given-names></string-name>, <string-name><surname>Welch</surname><given-names>R.P.</given-names></string-name>, <string-name><surname>Erdos</surname><given-names>M.R.</given-names></string-name>, <string-name><surname>Chines</surname><given-names>P.S.</given-names></string-name>, <string-name><surname>Narisu</surname><given-names>N.</given-names></string-name><collab>NISC Comparative Sequencing Program</collab></person-group><article-title>Genetic regulatory signatures underlying islet gene expression and type 2 diabetes</article-title>. <source>Proc. Natl. Acad. Sci. U.S.A.</source><year>2017</year>; <volume>114</volume>:<fpage>2301</fpage>–<lpage>2306</lpage>.<pub-id pub-id-type="pmid">28193859</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kraja</surname><given-names>A.T.</given-names></string-name>, <string-name><surname>Borecki</surname><given-names>I.B.</given-names></string-name>, <string-name><surname>Tsai</surname><given-names>M.Y.</given-names></string-name>, <string-name><surname>Ordovas</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Hopkins</surname><given-names>P.N.</given-names></string-name>, <string-name><surname>Lai</surname><given-names>C.Q.</given-names></string-name>, <string-name><surname>Arnett</surname><given-names>D.K.</given-names></string-name></person-group><article-title>Genetic analysis of 16 NMR-lipoprotein fractions in humans, the GOLDN study</article-title>. <source>Lipids</source>. <year>2013</year>; <volume>48</volume>:<fpage>155</fpage>–<lpage>165</lpage>.<pub-id pub-id-type="pmid">23192668</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dey</surname><given-names>K.K.</given-names></string-name>, <string-name><surname>Van de Geijn</surname><given-names>B.</given-names></string-name>, <string-name><surname>Kim</surname><given-names>S.S.</given-names></string-name>, <string-name><surname>Hormozdiari</surname><given-names>F.</given-names></string-name>, <string-name><surname>Kelley</surname><given-names>D.R.</given-names></string-name>, <string-name><surname>Price</surname><given-names>A.L.</given-names></string-name></person-group><article-title>Evaluating the informativeness of deep learning annotations for human complex diseases</article-title>. <source>Nat. Commun.</source><year>2020</year>; <volume>11</volume>:<fpage>4703</fpage>.<pub-id pub-id-type="pmid">32943643</pub-id></mixed-citation>
    </ref>
    <ref id="B55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sham</surname><given-names>P.C.</given-names></string-name>, <string-name><surname>Purcell</surname><given-names>S.M.</given-names></string-name></person-group><article-title>Statistical power and significance testing in large-scale genetic studies</article-title>. <source>Nat. Rev. Genet</source>. <year>2014</year>; <volume>15</volume>:<fpage>335</fpage>–<lpage>346</lpage>.<pub-id pub-id-type="pmid">24739678</pub-id></mixed-citation>
    </ref>
    <ref id="B56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bonàs-Guarch</surname><given-names>S.</given-names></string-name>, <string-name><surname>Guindo-Martínez</surname><given-names>M.</given-names></string-name>, <string-name><surname>Miguel-Escalada</surname><given-names>I.</given-names></string-name>, <string-name><surname>Grarup</surname><given-names>N.</given-names></string-name>, <string-name><surname>Sebastian</surname><given-names>D.</given-names></string-name>, <string-name><surname>Rodriguez-Fos</surname><given-names>E.</given-names></string-name>, <string-name><surname>Torrents</surname><given-names>D.</given-names></string-name></person-group><article-title>Re-analysis of public genetic data reveals a rare X-chromosomal variant associated with type 2 diabetes</article-title>. <source>Nat. Commun</source>. <year>2018</year>; <volume>9</volume>:<fpage>321</fpage>.<pub-id pub-id-type="pmid">29358691</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Xue</surname><given-names>A.</given-names></string-name>, <string-name><surname>Wu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Zhu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>F.</given-names></string-name>, <string-name><surname>Kemper</surname><given-names>K.E.</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>J.</given-names></string-name></person-group><article-title>Genome-wide association analyses identify 143 risk variants and putative regulatory mechanisms for type 2 diabetes</article-title>. <source>Nat. Commun.</source><year>2018</year>; <volume>9</volume>:<fpage>2941</fpage>.<pub-id pub-id-type="pmid">30054458</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Adamska-Patruno</surname><given-names>E.</given-names></string-name>, <string-name><surname>Godzien</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ciborowski</surname><given-names>M.</given-names></string-name>, <string-name><surname>Samczuk</surname><given-names>P.</given-names></string-name>, <string-name><surname>Bauer</surname><given-names>W.</given-names></string-name>, <string-name><surname>Siewko</surname><given-names>K.</given-names></string-name>, <string-name><surname>Kretowski</surname><given-names>A.</given-names></string-name></person-group><article-title>The type 2 diabetes susceptibility PROX1 gene variants are associated with postprandial plasma metabolites profile in non-diabetic men</article-title>. <source>Nutrients</source>. <year>2019</year>; <volume>11</volume>:<fpage>882</fpage>.</mixed-citation>
    </ref>
    <ref id="B59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kretowski</surname><given-names>A.</given-names></string-name>, <string-name><surname>Adamska</surname><given-names>E.</given-names></string-name>, <string-name><surname>Maliszewska</surname><given-names>K.</given-names></string-name>, <string-name><surname>Wawrusiewicz-Kurylonek</surname><given-names>N.</given-names></string-name>, <string-name><surname>Citko</surname><given-names>A.</given-names></string-name>, <string-name><surname>Goscik</surname><given-names>J.</given-names></string-name>, <string-name><surname>Gorska</surname><given-names>M.</given-names></string-name></person-group><article-title>The rs340874 PROX1 type 2 diabetes mellitus risk variant is associated with visceral fat accumulation and alterations in postprandial glucose and lipid metabolism</article-title>. <source>Genes Nutr</source>. <year>2015</year>; <volume>10</volume>:<fpage>4</fpage>.<pub-id pub-id-type="pmid">25601634</pub-id></mixed-citation>
    </ref>
    <ref id="B60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fujita</surname><given-names>H.</given-names></string-name>, <string-name><surname>Hara</surname><given-names>K.</given-names></string-name>, <string-name><surname>Shojima</surname><given-names>N.</given-names></string-name>, <string-name><surname>Horikoshi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Iwata</surname><given-names>M.</given-names></string-name>, <string-name><surname>Hirota</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Kadowaki</surname><given-names>T.</given-names></string-name></person-group><article-title>Variations with modest effects have an important role in the genetic background of type 2 diabetes and diabetes-related traits</article-title>. <source>J. Hum. Genet.</source><year>2012</year>; <volume>57</volume>:<fpage>776</fpage>–<lpage>779</lpage>.<pub-id pub-id-type="pmid">22992776</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <label>61.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hu</surname><given-names>C.</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>R.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>C.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>J.</given-names></string-name>, <string-name><surname>Ma</surname><given-names>X.</given-names></string-name>, <string-name><surname>Hou</surname><given-names>X.</given-names></string-name>, <string-name><surname>Jia</surname><given-names>W.</given-names></string-name></person-group><article-title>Variants from GIPR, TCF7L2, DGKB, MADD, CRY2, GLIS3, PROX1, SLC30A8 and IGF1 are associated with glucose metabolism in the Chinese</article-title>. <source>PLoS One</source>. <year>2010</year>; <volume>5</volume>:<fpage>e15542</fpage>.<pub-id pub-id-type="pmid">21103350</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhao</surname><given-names>W.</given-names></string-name>, <string-name><surname>Rasheed</surname><given-names>A.</given-names></string-name>, <string-name><surname>Tikkanen</surname><given-names>E.</given-names></string-name>, <string-name><surname>Lee</surname><given-names>J.J.</given-names></string-name>, <string-name><surname>Butterworth</surname><given-names>A.S.</given-names></string-name>, <string-name><surname>Howson</surname><given-names>J.M.</given-names></string-name><collab>EPIC-Interact Consortium</collab></person-group><article-title>Identification of new susceptibility loci for type 2 diabetes and shared etiological pathways with coronary heart disease</article-title>. <source>Nat. Genet.</source><year>2017</year>; <volume>49</volume>:<fpage>1450</fpage>.<pub-id pub-id-type="pmid">28869590</pub-id></mixed-citation>
    </ref>
    <ref id="B63">
      <label>63.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Voight</surname><given-names>B.F.</given-names></string-name>, <string-name><surname>Scott</surname><given-names>L.J.</given-names></string-name>, <string-name><surname>Steinthorsdottir</surname><given-names>V.</given-names></string-name>, <string-name><surname>Morris</surname><given-names>A.P.</given-names></string-name>, <string-name><surname>Dina</surname><given-names>C.</given-names></string-name>, <string-name><surname>Welch</surname><given-names>R.P.</given-names></string-name>, <string-name><surname>Thorsteinsdottir</surname><given-names>U.</given-names></string-name></person-group><article-title>Twelve type 2 diabetes susceptibility loci identified through large-scale association analysis</article-title>. <source>Nat. Genet.</source><year>2010</year>; <volume>42</volume>:<fpage>579</fpage>.<pub-id pub-id-type="pmid">20581827</pub-id></mixed-citation>
    </ref>
    <ref id="B64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harder</surname><given-names>M.N.</given-names></string-name>, <string-name><surname>Ribel-Madsen</surname><given-names>R.</given-names></string-name>, <string-name><surname>Justesen</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Sparsø</surname><given-names>T.</given-names></string-name>, <string-name><surname>Andersson</surname><given-names>E.A.</given-names></string-name>, <string-name><surname>Grarup</surname><given-names>N.</given-names></string-name>, <string-name><surname>Pedersen</surname><given-names>O.</given-names></string-name></person-group><article-title>Type 2 diabetes risk alleles near BCAR1 and in ANK1 associate with decreased β-cell function whereas risk alleles near ANKRD55 and GRB14 associate with decreased insulin sensitivity in the danish inter99 cohort</article-title>. <source>J. Clin. Endocrinol. Metab.</source><year>2013</year>; <volume>98</volume>:<fpage>E801</fpage>–<lpage>E806</lpage>.<pub-id pub-id-type="pmid">23457408</pub-id></mixed-citation>
    </ref>
    <ref id="B65">
      <label>65.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Vujkovic</surname><given-names>M.</given-names></string-name>, <string-name><surname>Keaton</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Lynch</surname><given-names>J.A.</given-names></string-name>, <string-name><surname>Miller</surname><given-names>D.R.</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>J.</given-names></string-name>, <string-name><surname>Tcheandjieu</surname><given-names>C.</given-names></string-name>, <string-name><surname>Saleheen</surname><given-names>D.</given-names></string-name></person-group><article-title>Discovery of 318 new risk loci for type 2 diabetes and related vascular outcomes among 1.4 million participants in a multi-ancestry meta-analysis</article-title>. <source>Nat. Genet</source>. <year>2020</year>; <volume>52</volume>:<fpage>680</fpage>–<lpage>691</lpage>.<pub-id pub-id-type="pmid">32541925</pub-id></mixed-citation>
    </ref>
    <ref id="B66">
      <label>66.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davydov</surname><given-names>E.V.</given-names></string-name>, <string-name><surname>Goode</surname><given-names>D.L.</given-names></string-name>, <string-name><surname>Sirota</surname><given-names>M.</given-names></string-name>, <string-name><surname>Cooper</surname><given-names>G.M.</given-names></string-name>, <string-name><surname>Sidow</surname><given-names>A.</given-names></string-name>, <string-name><surname>Batzoglou</surname><given-names>S.</given-names></string-name></person-group><article-title>Identifying a high fraction of the human genome to be under selective constraint using GERP++</article-title>. <source>PLoS Comput. Biol.</source><year>2010</year>; <volume>6</volume>:<fpage>e1001025</fpage>.<pub-id pub-id-type="pmid">21152010</pub-id></mixed-citation>
    </ref>
    <ref id="B67">
      <label>67.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fu</surname><given-names>Y.</given-names></string-name>, <string-name><surname>Liu</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Lou</surname><given-names>S.</given-names></string-name>, <string-name><surname>Bedford</surname><given-names>J.</given-names></string-name>, <string-name><surname>Mu</surname><given-names>X.J.</given-names></string-name>, <string-name><surname>Yip</surname><given-names>K.Y.</given-names></string-name>, <string-name><surname>Gerstein</surname><given-names>M.</given-names></string-name></person-group><article-title>FunSeq2: a framework for prioritizing noncoding regulatory variants in cancer</article-title>. <source>Genome Biol.</source><year>2014</year>; <volume>15</volume>:<fpage>480</fpage>.<pub-id pub-id-type="pmid">25273974</pub-id></mixed-citation>
    </ref>
    <ref id="B68">
      <label>68.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>Y.F.</given-names></string-name>, <string-name><surname>Gulko</surname><given-names>B.</given-names></string-name>, <string-name><surname>Siepel</surname><given-names>A.</given-names></string-name></person-group><article-title>Fast, scalable prediction of deleterious noncoding variants from functional and population genomic data</article-title>. <source>Nat. Genet.</source><year>2017</year>; <volume>49</volume>:<fpage>618</fpage>–<lpage>624</lpage>.<pub-id pub-id-type="pmid">28288115</pub-id></mixed-citation>
    </ref>
    <ref id="B69">
      <label>69.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rentzsch</surname><given-names>P.</given-names></string-name>, <string-name><surname>Witten</surname><given-names>D.</given-names></string-name>, <string-name><surname>Cooper</surname><given-names>G.M.</given-names></string-name>, <string-name><surname>Shendure</surname><given-names>J.</given-names></string-name>, <string-name><surname>Kircher</surname><given-names>M.</given-names></string-name></person-group><article-title>CADD: predicting the deleteriousness of variants throughout the human genome</article-title>. <source>Nucleic Acids Res.</source><year>2019</year>; <volume>47</volume>:<fpage>D886</fpage>–<lpage>D894</lpage>.<pub-id pub-id-type="pmid">30371827</pub-id></mixed-citation>
    </ref>
    <ref id="B70">
      <label>70.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Di Iulio</surname><given-names>J.</given-names></string-name>, <string-name><surname>Bartha</surname><given-names>I.</given-names></string-name>, <string-name><surname>Wong</surname><given-names>E.H.</given-names></string-name>, <string-name><surname>Yu</surname><given-names>H.C.</given-names></string-name>, <string-name><surname>Lavrenko</surname><given-names>V.</given-names></string-name>, <string-name><surname>Yang</surname><given-names>D.</given-names></string-name>, <string-name><surname>Telenti</surname><given-names>A.</given-names></string-name></person-group><article-title>The human noncoding genome defined by genetic diversity</article-title>. <source>Nat. Genet.</source><year>2018</year>; <volume>50</volume>:<fpage>333</fpage>–<lpage>337</lpage>.<pub-id pub-id-type="pmid">29483654</pub-id></mixed-citation>
    </ref>
    <ref id="B71">
      <label>71.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stenson</surname><given-names>P.D.</given-names></string-name>, <string-name><surname>Ball</surname><given-names>E.V.</given-names></string-name>, <string-name><surname>Mort</surname><given-names>M.</given-names></string-name>, <string-name><surname>Phillips</surname><given-names>A.D.</given-names></string-name>, <string-name><surname>Shiel</surname><given-names>J.A.</given-names></string-name>, <string-name><surname>Thomas</surname><given-names>N.S.</given-names></string-name>, <string-name><surname>Cooper</surname><given-names>D.N.</given-names></string-name></person-group><article-title>Human gene mutation database (HGMD®): 2003 update</article-title>. <source>Hum. Mutat.</source><year>2003</year>; <volume>21</volume>:<fpage>577</fpage>–<lpage>581</lpage>.<pub-id pub-id-type="pmid">12754702</pub-id></mixed-citation>
    </ref>
    <ref id="B72">
      <label>72.</label>
      <mixed-citation publication-type="journal"><collab>1000 Genomes Project Consortium</collab><article-title>A global reference for human genetic variation</article-title>. <source>Nature</source>. <year>2015</year>; <volume>526</volume>:<fpage>68</fpage>.<pub-id pub-id-type="pmid">26432245</pub-id></mixed-citation>
    </ref>
    <ref id="B73">
      <label>73.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Siepel</surname><given-names>A.</given-names></string-name>, <string-name><surname>Bejerano</surname><given-names>G.</given-names></string-name>, <string-name><surname>Pedersen</surname><given-names>J.S.</given-names></string-name>, <string-name><surname>Hinrichs</surname><given-names>A.S.</given-names></string-name>, <string-name><surname>Hou</surname><given-names>M.</given-names></string-name>, <string-name><surname>Rosenbloom</surname><given-names>K.</given-names></string-name>, <string-name><surname>Haussler</surname><given-names>D.</given-names></string-name></person-group><article-title>Evolutionarily conserved elements in vertebrate, insect, worm, and yeast genomes</article-title>. <source>Genome Res.</source><year>2005</year>; <volume>15</volume>:<fpage>1034</fpage>–<lpage>1050</lpage>.<pub-id pub-id-type="pmid">16024819</pub-id></mixed-citation>
    </ref>
    <ref id="B74">
      <label>74.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pollard</surname><given-names>K.S.</given-names></string-name>, <string-name><surname>Hubisz</surname><given-names>M.J.</given-names></string-name>, <string-name><surname>Rosenbloom</surname><given-names>K.R.</given-names></string-name>, <string-name><surname>Siepel</surname><given-names>A.</given-names></string-name></person-group><article-title>Detection of nonneutral substitution rates on mammalian phylogenies</article-title>. <source>Genome Res.</source><year>2010</year>; <volume>20</volume>:<fpage>110</fpage>–<lpage>121</lpage>.<pub-id pub-id-type="pmid">19858363</pub-id></mixed-citation>
    </ref>
    <ref id="B75">
      <label>75.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cooper</surname><given-names>G.M.</given-names></string-name>, <string-name><surname>Stone</surname><given-names>E.A.</given-names></string-name>, <string-name><surname>Asimenos</surname><given-names>G.</given-names></string-name>, <string-name><surname>Green</surname><given-names>E.D.</given-names></string-name>, <string-name><surname>Batzoglou</surname><given-names>S.</given-names></string-name>, <string-name><surname>Sidow</surname><given-names>A.</given-names></string-name></person-group><article-title>Distribution and intensity of constraint in mammalian genomic sequence</article-title>. <source>Genome Res.</source><year>2005</year>; <volume>15</volume>:<fpage>901</fpage>–<lpage>913</lpage>.<pub-id pub-id-type="pmid">15965027</pub-id></mixed-citation>
    </ref>
    <ref id="B76">
      <label>76.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Landrum</surname><given-names>M.J.</given-names></string-name>, <string-name><surname>Lee</surname><given-names>J.M.</given-names></string-name>, <string-name><surname>Riley</surname><given-names>G.R.</given-names></string-name>, <string-name><surname>Jang</surname><given-names>W.</given-names></string-name>, <string-name><surname>Rubinstein</surname><given-names>W.S.</given-names></string-name>, <string-name><surname>Church</surname><given-names>D.M.</given-names></string-name>, <string-name><surname>Maglott</surname><given-names>D.R.</given-names></string-name></person-group><article-title>ClinVar: public archive of relationships among sequence variation and human phenotype</article-title>. <source>Nucleic Acids Res.</source><year>2014</year>; <volume>42</volume>:<fpage>D980</fpage>–<lpage>D985</lpage>.<pub-id pub-id-type="pmid">24234437</pub-id></mixed-citation>
    </ref>
    <ref id="B77">
      <label>77.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhou</surname><given-names>L.</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>F.</given-names></string-name></person-group><article-title>Prioritization and functional assessment of noncoding variants associated with complex diseases</article-title>. <source>Genome Medicine</source>. <year>2018</year>; <volume>10</volume>:<fpage>53</fpage>.<pub-id pub-id-type="pmid">29996888</pub-id></mixed-citation>
    </ref>
    <ref id="B78">
      <label>78.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wright</surname><given-names>S.</given-names></string-name></person-group><article-title>The interpretation of population structure by F-statistics with special regard to systems of mating</article-title>. <source>Evolution</source>. <year>1965</year>; <volume>19</volume>:<fpage>395</fpage>–<lpage>420</lpage>.</mixed-citation>
    </ref>
    <ref id="B79">
      <label>79.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Quintana-Murci</surname><given-names>L.</given-names></string-name></person-group><article-title>Understanding rare and common diseases in the context of human evolution</article-title>. <source>Genome Biol.</source><year>2016</year>; <volume>17</volume>:<fpage>225</fpage>.<pub-id pub-id-type="pmid">27821149</pub-id></mixed-citation>
    </ref>
    <ref id="B80">
      <label>80.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tay</surname><given-names>Yi</given-names></string-name>, <string-name><surname>Bahri</surname><given-names>D.</given-names></string-name>, <string-name><surname>Metzler</surname><given-names>D.</given-names></string-name>, <string-name><surname>Juan</surname><given-names>Da-C</given-names></string-name>, <string-name><surname>Zhao</surname><given-names>Z.</given-names></string-name>, <string-name><surname>Zheng</surname><given-names>C.</given-names></string-name></person-group><article-title>Synthesizer: rethinking self-attention for transformer models</article-title>. <source>PMLR</source>. <year>2020</year>; <volume>139</volume>:<fpage>10183</fpage>–<lpage>10192</lpage>.</mixed-citation>
    </ref>
    <ref id="B81">
      <label>81.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname><given-names>C.</given-names></string-name>, <string-name><surname>Wu</surname><given-names>F.</given-names></string-name>, <string-name><surname>Qi</surname><given-names>T.</given-names></string-name>, <string-name><surname>Huang</surname><given-names>Y.</given-names></string-name></person-group><article-title>Hi-Transformer: hierarchical interactive transformer for efficient and effective long document modeling</article-title>. <source>ACL</source>. <year>2021</year>; <volume>2</volume>:<fpage>848</fpage>–<lpage>853</lpage>.</mixed-citation>
    </ref>
    <ref id="B82">
      <label>82.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ward</surname><given-names>L.D.</given-names></string-name>, <string-name><surname>Kellis</surname><given-names>M.</given-names></string-name></person-group><article-title>HaploReg v4: systematic mining of putative causal variants, cell types, regulators and target genes for human complex traits and disease</article-title>. <source>Nucleic Acids Res.</source><year>2016</year>; <volume>44</volume>:<fpage>D877</fpage>–<lpage>D881</lpage>.<pub-id pub-id-type="pmid">26657631</pub-id></mixed-citation>
    </ref>
    <ref id="B83">
      <label>83.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lonsdale</surname><given-names>J.</given-names></string-name>, <string-name><surname>Thomas</surname><given-names>J.</given-names></string-name>, <string-name><surname>Salvatore</surname><given-names>M.</given-names></string-name>, <string-name><surname>Phillips</surname><given-names>R.</given-names></string-name>, <string-name><surname>Lo</surname><given-names>E.</given-names></string-name>, <string-name><surname>Shad</surname><given-names>S.</given-names></string-name>, <string-name><surname>Moore</surname><given-names>H.F.</given-names></string-name></person-group><article-title>The genotype-tissue expression (GTEx) project</article-title>. <source>Nat. Genet.</source><year>2013</year>; <volume>45</volume>:<fpage>580</fpage>–<lpage>585</lpage>.<pub-id pub-id-type="pmid">23715323</pub-id></mixed-citation>
    </ref>
    <ref id="B84">
      <label>84.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greene</surname><given-names>C.S.</given-names></string-name>, <string-name><surname>Krishnan</surname><given-names>A.</given-names></string-name>, <string-name><surname>Wong</surname><given-names>A.K.</given-names></string-name>, <string-name><surname>Ricciotti</surname><given-names>E.</given-names></string-name>, <string-name><surname>Zelaya</surname><given-names>R.A.</given-names></string-name>, <string-name><surname>Himmelstein</surname><given-names>D.S.</given-names></string-name>, <string-name><surname>Troyanskaya</surname><given-names>O.G.</given-names></string-name></person-group><article-title>Understanding multicellular function and disease with human tissue-specific networks</article-title>. <source>Nat. Genet.</source><year>2015</year>; <volume>47</volume>:<fpage>569</fpage>–<lpage>576</lpage>.<pub-id pub-id-type="pmid">25915600</pub-id></mixed-citation>
    </ref>
    <ref id="B85">
      <label>85.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Alley</surname><given-names>E.C.</given-names></string-name>, <string-name><surname>Khimulya</surname><given-names>G.</given-names></string-name>, <string-name><surname>Biswas</surname><given-names>S.</given-names></string-name>, <string-name><surname>AlQuraishi</surname><given-names>M.</given-names></string-name>, <string-name><surname>Church</surname><given-names>G.M.</given-names></string-name></person-group><article-title>Unified rational protein engineering with sequence-based deep representation learning</article-title>. <source>Nat. Methods</source>. <year>2019</year>; <volume>16</volume>:<fpage>1315</fpage>–<lpage>1322</lpage>.<pub-id pub-id-type="pmid">31636460</pub-id></mixed-citation>
    </ref>
    <ref id="B86">
      <label>86.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Repecka</surname><given-names>D.</given-names></string-name>, <string-name><surname>Jauniskis</surname><given-names>V.</given-names></string-name>, <string-name><surname>Karpus</surname><given-names>L.</given-names></string-name>, <string-name><surname>Rembeza</surname><given-names>E.</given-names></string-name>, <string-name><surname>Rokaitis</surname><given-names>I.</given-names></string-name>, <string-name><surname>Zrimec</surname><given-names>J.</given-names></string-name>, <string-name><surname>Zelezniak</surname><given-names>A.</given-names></string-name></person-group><article-title>Expanding functional protein sequence spaces using generative adversarial networks</article-title>. <source>Nat. Mach. Intell.</source><year>2021</year>; <volume>3</volume>:<fpage>324</fpage>–<lpage>333</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
