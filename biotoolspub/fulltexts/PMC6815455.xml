<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6815455</article-id>
    <article-id pub-id-type="publisher-id">3135</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-3135-4</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Comprehensive ensemble in QSAR prediction for drug discovery</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Kwon</surname>
          <given-names>Sunyoung</given-names>
        </name>
        <address>
          <email>nicesilsil@naver.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Bae</surname>
          <given-names>Ho</given-names>
        </name>
        <address>
          <email>hobae@snu.ac.kr</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jo</surname>
          <given-names>Jeonghee</given-names>
        </name>
        <address>
          <email>page1024@snu.ac.kr</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2367-197X</contrib-id>
        <name>
          <surname>Yoon</surname>
          <given-names>Sungroh</given-names>
        </name>
        <address>
          <email>sryoon@snu.ac.kr</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0470 5905</institution-id><institution-id institution-id-type="GRID">grid.31501.36</institution-id><institution>Department of Electrical and Computer Engineering, Seoul National University, </institution></institution-wrap>Seoul, 08826 South Korea </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0470 5905</institution-id><institution-id institution-id-type="GRID">grid.31501.36</institution-id><institution>Interdisciplinary Program in Bioinformatics, Seoul National University, </institution></institution-wrap>Seoul, 08826 South Korea </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 5313 0634</institution-id><institution-id institution-id-type="GRID">grid.497243.f</institution-id><institution>Clova AI Research, NAVER Corp., </institution></institution-wrap>Seongnam, 13561 South Korea </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0470 5905</institution-id><institution-id institution-id-type="GRID">grid.31501.36</institution-id><institution>Biological Sciences, Seoul National University, </institution></institution-wrap>Seoul, 08826 South Korea </aff>
      <aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0470 5905</institution-id><institution-id institution-id-type="GRID">grid.31501.36</institution-id><institution>ASRI and INMC, Seoul National University, </institution></institution-wrap>Seoul, 08826 South Korea </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0470 5905</institution-id><institution-id institution-id-type="GRID">grid.31501.36</institution-id><institution>Institute of Engineering Research, Seoul National University, </institution></institution-wrap>Seoul, 08826 South Korea </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>26</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>26</day>
      <month>10</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>521</elocation-id>
    <history>
      <date date-type="received">
        <day>2</day>
        <month>5</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>9</day>
        <month>10</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Quantitative structure-activity relationship (QSAR) is a computational modeling method for revealing relationships between structural properties of chemical compounds and biological activities. QSAR modeling is essential for drug discovery, but it has many constraints. Ensemble-based machine learning approaches have been used to overcome constraints and obtain reliable predictions. Ensemble learning builds a set of diversified models and combines them. However, the most prevalent approach random forest and other ensemble approaches in QSAR prediction limit their model diversity to a single subject.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">The proposed ensemble method consistently outperformed thirteen individual models on 19 bioassay datasets and demonstrated superiority over other ensemble approaches that are limited to a single subject. The comprehensive ensemble method is publicly available at <ext-link ext-link-type="uri" xlink:href="http://data.snu.ac.kr/QSAR/">http://data.snu.ac.kr/QSAR/</ext-link>.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">We propose a comprehensive ensemble method that builds multi-subject diversified models and combines them through second-level meta-learning. In addition, we propose an end-to-end neural network-based individual classifier that can automatically extract sequential features from a simplified molecular-input line-entry system (SMILES). The proposed individual models did not show impressive results as a single model, but it was considered the most important predictor when combined, according to the interpretation of the meta-learning.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Ensemble-learning</kwd>
      <kwd>Meta-learning</kwd>
      <kwd>Drug-prediction</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Quantitative structure-activity relationship (QSAR) is a computational or mathematical modeling method to reveal relationships between biological activities and the structural properties of chemical compounds. The underlying principle is that variations in structural properties cause different biological activities [<xref ref-type="bibr" rid="CR1">1</xref>]. Structural properties refer to physico-chemical properties, and biological activities correspond to pharmacokinetic properties such as absorption, distribution, metabolism, excretion, and toxicity.</p>
    <p>QSAR modeling helps prioritize a large number of chemicals in terms of their desired biological activities as an in silico methodology and, as a result, significantly reduces the number of candidate chemicals to be tested with in vivo experiments. QSAR modeling has served as an inevitable process in the pharmaceutical industry, but many constraints are involved [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. QSAR data may involve a very large number of chemicals (more than hundreds of thousands); each chemical can be represented by a variety of descriptors; commonly used fingerprints are very sparse (most of the values are zero), and some features are highly correlated; it is assumed that the dataset contains some errors because relationships are assessed through in situ experiments.</p>
    <p>Due to these constraints, it has become difficult for QSAR-based model prediction to achieve a reliable prediction score. Consequently, machine learning approaches have been applied to QSAR prediction. Linear regression models [<xref ref-type="bibr" rid="CR4">4</xref>] and Bayesian neural networks [<xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR7">7</xref>] have been used for QSAR prediction. Random forest (RF) [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>] is most commonly used algorithm with a high level of predictability, simplicity, and robustness. RF is a kind of ensemble method based on multiple decision trees that can prevent the overfitting from a single decision tree. RF is considered to be the gold standard in this field [<xref ref-type="bibr" rid="CR2">2</xref>]; thus, newly proposed QSAR prediction methods ofen have their performance compared to RF.</p>
    <p>The Merck Kaggle competition in 2012 turned people’s attentions to neural networks. The winning team used multi-task neural networks (MTNNs) [<xref ref-type="bibr" rid="CR10">10</xref>]. The fundamental learning structure is based on plain feed-forward neural networks; it avoids overfitting by learning multiple bioassays simultaneously. The team obtained results that consistently outperformed RF. Despite achieving high performance using a multi-task neural network, the team ultimately used an ensemble that combined different methods.</p>
    <p>Both RF and the aforementioned technique from the Kaggle competition used ensemble learning, a technique which builds a set of learning models and combines multiple models to produce final predictions. Theoretically and empirically, it has been shown that the predictive power of ensemble learning surpasses that of a single individual learner if the individual algorithms are accurate and diverse [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR14">14</xref>]. Ensemble learning manages the strengths and weaknesses of individual learners, similar to how people consider diverse opinions when faced with critical issues.</p>
    <p>Ensemble methods, including neural network ensemble based on bootstrap sampling in QSAR (<italic>data sampling ensemble</italic>) [<xref ref-type="bibr" rid="CR15">15</xref>]; ensemble against different learning methods for drug-drug interaction [<xref ref-type="bibr" rid="CR16">16</xref>], Bayesian ensemble model with different QSAR tools (<italic>method ensemble</italic>) [<xref ref-type="bibr" rid="CR7">7</xref>], ensemble learning based qualitative and quantitative SAR models [<xref ref-type="bibr" rid="CR17">17</xref>], Hybrid QSAR prediction model with various learning methods [<xref ref-type="bibr" rid="CR18">18</xref>], ensembles with different boosting methods [<xref ref-type="bibr" rid="CR19">19</xref>], Hybridizing feature selection and feature learning in QSAR modeling [<xref ref-type="bibr" rid="CR20">20</xref>], and ensemble against diverse chemicals for carcinogenicity prediction (<italic>representation ensembles</italic>) [<xref ref-type="bibr" rid="CR21">21</xref>] have been extensively used in drug (chemical) research. However, these ensemble approaches limit model diversity to a single subject, such as data sampling, method, and input representation (drug-specific).</p>
    <p>To overcome this limitation, we propose a multi-subject comprehensive ensemble with a new type of individual classifier based on 1D-CNNs and RNNs. The detailed key characteristics and contributions of our proposed methods are as follows: 
<list list-type="bullet"><list-item><p>Instead of limiting ensemble diversity to a single subject, we combine multi-subject individual models comprehensively. This ensemble is used for combinations of bagging, methods, and chemical compound input representations.</p></list-item><list-item><p>We propose a new type of individual QSAR classifier that is an end-to-end neural network model based on one-dimensional convolutional neural networks (1D-CNNs) and recurrent neural networks (RNNs). It automatically extracts sequential features from a simplified molecular-input line-entry system (SMILES).</p></list-item><list-item><p>We combine a set of models using second-level combined learning (meta-learning) and provide an interpretation regarding the importance of individual models through their learned weights.</p></list-item></list></p>
    <p>To validate our proposed method, we tested 19 bioassays specified in [<xref ref-type="bibr" rid="CR10">10</xref>]. In our experiments, we confirmed the superiority of our proposed method by comparing individual models, limited ensemble approaches, and other combining techniques. Further, we identified the importance of the proposed end-to-end individual classifier through an interpretation of second-level meta-learning.</p>
  </sec>
  <sec id="Sec2" sec-type="results">
    <title>Results</title>
    <sec id="Sec3">
      <title>Experimental setup</title>
      <sec id="Sec4">
        <title>Dataset</title>
        <p>A bioassay is a biochemical test to determine or estimate the potency of a chemical compound on targets and has been used for a variety of purposes, including drug development, and environmental impact analysis. In our experiment, we used 19 bioassays downloaded from the PubChem open chemistry database [<xref ref-type="bibr" rid="CR22">22</xref>], which are listed in Table <xref rid="Tab1" ref-type="table">1</xref>. All bioassays are those specified in [<xref ref-type="bibr" rid="CR10">10</xref>]. The purpose of the paper was to address multi-task effects; thus, a number of experimental assays are closely related, such as the 1851, 46321*, 48891*, and 6517** series.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Details of the bioassay datasets used in the experiments</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Assay ID</th><th align="left">Description of BioAssay</th><th align="left"># Active</th><th align="left"># Inactive</th></tr></thead><tbody><tr><td align="left">1851_1a2</td><td align="justify">Cytochrome P450 Panel Assay, cyp1a2</td><td align="left">5,902</td><td align="left">6,974</td></tr><tr><td align="left">1851_2c19</td><td align="justify">Cytochrome P450 Panel Assay, cyp2c19</td><td align="left">5,840</td><td align="left">7,135</td></tr><tr><td align="left">1851_2c9</td><td align="justify">Cytochrome P450 Panel Assay, cyp2c9</td><td align="left">4,065</td><td align="left">8,361</td></tr><tr><td align="left">1851_2d6</td><td align="justify">Cytochrome P450 Panel Assay, cyp2d6</td><td align="left">2,601</td><td align="left">10,826</td></tr><tr><td align="left">1851_3a4</td><td align="justify">Cytochrome P450 Panel Assay, cyp3a4</td><td align="left">5,175</td><td align="left">7,446</td></tr><tr><td align="left">1915</td><td align="justify">Streptokinase Expression Inhibition</td><td align="left">2,219</td><td align="left">1,017</td></tr><tr><td align="left">2358</td><td align="justify">Inhibitors of Protein Phosphatase 1 (PP1)</td><td align="left">1,006</td><td align="left">934</td></tr><tr><td align="left">463213</td><td align="justify">Inhibitors of tim10-1 yeast</td><td align="left">4,138</td><td align="left">3,234</td></tr><tr><td align="left">463215</td><td align="justify">Inhibitors of tim10 yeast</td><td align="left">2,941</td><td align="left">1,695</td></tr><tr><td align="left">488912</td><td align="justify">Inhibitors of Sentrin-specific protease 8</td><td align="left">2,491</td><td align="left">3,705</td></tr><tr><td align="left">488915</td><td align="justify">Inhibitors of Sentrin-specific protease 6</td><td align="left">3,568</td><td align="left">2,628</td></tr><tr><td align="left">488917</td><td align="justify">Inhibitors of Sentrin-specific protease 7</td><td align="left">4,283</td><td align="left">1,913</td></tr><tr><td align="left">488918</td><td align="justify">Inhibitors of Sentrin-specific proteases</td><td align="left">3,691</td><td align="left">2,505</td></tr><tr><td align="left">492992</td><td align="justify">Inhibitors of KCNK9 <sup>∗</sup></td><td align="left">2,094</td><td align="left">2,820</td></tr><tr><td align="left">504607</td><td align="justify">Inhibitors of Mdm2/MdmX interaction</td><td align="left">4,825</td><td align="left">1,406</td></tr><tr><td align="left">624504</td><td align="justify">Inhibitors of the mtPTP <sup><italic>†</italic></sup></td><td align="left">3,944</td><td align="left">1,090</td></tr><tr><td align="left">651739</td><td align="justify">Inhibition of T.cruzi proliferation</td><td align="left">4,043</td><td align="left">1,322</td></tr><tr><td align="left">651744</td><td align="justify">NIH/3T3 (mouse embryonic fibroblast) toxicity</td><td align="left">3,099</td><td align="left">2,303</td></tr><tr><td align="left">652065</td><td align="justify">Molecules that bind r(CAG) RNA repeats</td><td align="left">2,965</td><td align="left">1,286</td></tr></tbody></table><table-wrap-foot><p>The 19 bioassays are those specified in [<xref ref-type="bibr" rid="CR10">10</xref>]</p><p><sup>∗</sup>Two-pore domain potassium channel</p><p><sup><italic>†</italic></sup>Mitochondrial permeability transition pore</p></table-wrap-foot></table-wrap>
</p>
        <p>From each bioassay, we extracted a PubChem chemical ID and activity outcome (active or inactive). We only used duplicate chemicals once, and we excluded inconsistent chemicals that had both active and inactive outcomes. A class imbalance ratio between active and inactive ranged from 1:1.1 to 1:4.2 depending on the dataset; most bioassays are imbalanced, with an average ratio of 1:2.</p>
      </sec>
      <sec id="Sec5">
        <title>Representation of chemical compounds</title>
        <p>In our experiment, we used three types of molecular fingerprints PubChem [<xref ref-type="bibr" rid="CR22">22</xref>], ECFP [<xref ref-type="bibr" rid="CR23">23</xref>], MACCS [<xref ref-type="bibr" rid="CR24">24</xref>], and string type SMILES [<xref ref-type="bibr" rid="CR25">25</xref>]. Because SMILES is a sequential string type descriptor, it is not a proper form for conventional learning methods. We used an end-to-end 1D-CNN and RNN which are capable of handling a sequential forms. On the other hand, a binary vector type fingerprint consists of 1’s and 0’s in a form of non-sequential form. Thus, conventional machine learning approaches such as plain feed-forward neural network are used.</p>
        <p>The SMILES and PubChem fingerprint were retrieved from the preprocessed chemical IDs using PubChemPy [<xref ref-type="bibr" rid="CR26">26</xref>], and ECFP and MACCS fingerprints were retrieved from SMILES using RDKit [<xref ref-type="bibr" rid="CR27">27</xref>].</p>
      </sec>
      <sec id="Sec6">
        <title>Experimental configuration and environment</title>
        <p>We followed the same experimental settings and performance measures as described for the multi-task neural network [<xref ref-type="bibr" rid="CR10">10</xref>]. We randomly divided the dataset into two parts: 75% of the dataset was used as a training set, and the other 25% was used as a testing set. The training dataset was also randomly partitioned into five portions: one for validation, and the remaining four for training (5-fold cross-validation). The prediction probabilities from the 5-fold validations were concatenated as <italic>P</italic>, and were then used as inputs for the second-level learning.</p>
        <p>We ran our experiments on Ubuntu 14.04 (3.5GHz Intel i7-5930K CPU and GTX Titan X Maxwell(12GB) GPU). We used the Keras library package (version 2.0.6) for neural network implementation, the Scikit-learn library package (version 0.18) for conventional machine learning methods, and PubChemPy (version 1.0.3) and RDKit (version 1.0.3) for input representation preparation of the chemical compounds.</p>
      </sec>
    </sec>
    <sec id="Sec7">
      <title>Performance comparison with other approaches</title>
      <sec id="Sec8">
        <title>Performance comparison with individual models</title>
        <p>We compared our comprehensive ensemble method with 13 individual models: the 12 models from the combination of three types of fingerprints (PubChem, ECFP, and MACCS) and four types of learning methods (RF, SVM, GBM, and NN), and a SMILES-NN combination.</p>
        <p>As shown in Table <xref rid="Tab2" ref-type="table">2</xref>, the comprehensive ensemble showed the best performance across all datasets, followed by ECFP-RF and PubChem-RF. We can see that the top-3 AUCs (represented in bold) are dispersed across the chemical compound representations and learning methods, except for PubChem-SVM, ECFP-GBM, and MACCS-SVM. The individual SMILES-NN models were within the top-3 ranks of the three datasets. In terms of learning methodology, RF showed the highest number of top-3 AUC values followed by NN, GBM, and SVM. In terms of chemical compound representation, ECFP showed the highest number of top-3 AUC values followed by PubChem, SMILES (compared proportionally), and MACCS. In terms of the averaged AUC, the comprehensive ensemble showed the best performance (0.814), followed by ECFP-RF (0.798) and PubChem-RF (0.794). The MACCS-SVM combination showed the lowest AUC value (0.736). Aside from the best (proposed ensemble) and the worst (MACCS-SVM) methods, all average AUC values were less than 0.80. Predictability depends on the combination of learning method and input representation. Although SVM showed better performance than GBM in ECFP, GBM showed better performance than SVM in MACCS.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performance comparison between the proposed comprehensive ensemble and the individual models on 19 bioassay datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">BioAssay</th><th align="left" colspan="4">PubChem fingerprint</th><th align="left" colspan="4">ECFP fingerprint</th><th align="left" colspan="4">MACCS fingerprint</th><th align="left">SMILES</th><th align="left">comprehensive</th></tr><tr><th align="left"/><th align="left">RF</th><th align="left">SVM</th><th align="left">GBM</th><th align="left">NN</th><th align="left">RF</th><th align="left">SVM</th><th align="left">GBM</th><th align="left">NN</th><th align="left">RF</th><th align="left">SVM</th><th align="left">GBM</th><th align="left">NN</th><th align="left">NN</th><th align="left">ensemble</th></tr></thead><tbody><tr><td align="left">1851_1a2</td><td align="left"><bold>0.921</bold></td><td align="left">0.896</td><td align="left">0.900</td><td align="left"><bold>0.921</bold></td><td align="left">0.919</td><td align="left">0.906</td><td align="left">0.882</td><td align="left">0.920</td><td align="left">0.912</td><td align="left">0.879</td><td align="left">0.894</td><td align="left">0.912</td><td align="left"><bold>0.922</bold></td><td align="left"><bold>0.934</bold></td></tr><tr><td align="left">1851_2c19</td><td align="left">0.871</td><td align="left">0.852</td><td align="left">0.848</td><td align="left">0.872</td><td align="left">0.882</td><td align="left">0.871</td><td align="left">0.854</td><td align="left"><bold>0.880</bold></td><td align="left">0.874</td><td align="left">0.842</td><td align="left">0.850</td><td align="left"><bold>0.885</bold></td><td align="left">0.875</td><td align="left"><bold>0.900</bold></td></tr><tr><td align="left">1851_2c9</td><td align="left">0.871</td><td align="left">0.857</td><td align="left">0.851</td><td align="left">0.873</td><td align="left"><bold>0.880</bold></td><td align="left">0.866</td><td align="left">0.843</td><td align="left"><bold>0.880</bold></td><td align="left">0.858</td><td align="left">0.828</td><td align="left">0.840</td><td align="left">0.870</td><td align="left">0.877</td><td align="left"><bold>0.898</bold></td></tr><tr><td align="left">1851_2d6</td><td align="left">0.858</td><td align="left">0.847</td><td align="left">0.832</td><td align="left"><bold>0.869</bold></td><td align="left"><bold>0.867</bold></td><td align="left">0.850</td><td align="left">0.833</td><td align="left">0.856</td><td align="left">0.854</td><td align="left">0.816</td><td align="left">0.830</td><td align="left">0.852</td><td align="left">0.846</td><td align="left"><bold>0.884</bold></td></tr><tr><td align="left">1851_3a4</td><td align="left">0.877</td><td align="left">0.868</td><td align="left">0.865</td><td align="left">0.887</td><td align="left"><bold>0.891</bold></td><td align="left">0.887</td><td align="left">0.855</td><td align="left"><bold>0.895</bold></td><td align="left">0.867</td><td align="left">0.832</td><td align="left">0.851</td><td align="left">0.875</td><td align="left"><bold>0.891</bold></td><td align="left"><bold>0.914</bold></td></tr><tr><td align="left">1915</td><td align="left">0.754</td><td align="left">0.692</td><td align="left">0.709</td><td align="left">0.722</td><td align="left">0.731</td><td align="left">0.700</td><td align="left">0.700</td><td align="left">0.712</td><td align="left"><bold>0.758</bold></td><td align="left">0.716</td><td align="left">0.736</td><td align="left"><bold>0.741</bold></td><td align="left">0.701</td><td align="left"><bold>0.755</bold></td></tr><tr><td align="left">2358</td><td align="left"><bold>0.787</bold></td><td align="left">0.705</td><td align="left">0.736</td><td align="left">0.770</td><td align="left">0.780</td><td align="left">0.767</td><td align="left">0.722</td><td align="left">0.761</td><td align="left">0.774</td><td align="left">0.731</td><td align="left">0.763</td><td align="left"><bold>0.775</bold></td><td align="left">0.697</td><td align="left"><bold>0.803</bold></td></tr><tr><td align="left">463213</td><td align="left">0.673</td><td align="left">0.639</td><td align="left">0.652</td><td align="left">0.651</td><td align="left"><bold>0.685</bold></td><td align="left">0.652</td><td align="left">0.644</td><td align="left">0.661</td><td align="left"><bold>0.668</bold></td><td align="left">0.642</td><td align="left">0.655</td><td align="left">0.651</td><td align="left">0.636</td><td align="left"><bold>0.689</bold></td></tr><tr><td align="left">463215</td><td align="left">0.620</td><td align="left">0.576</td><td align="left">0.592</td><td align="left">0.604</td><td align="left">0.617</td><td align="left">0.585</td><td align="left">0.598</td><td align="left">0.595</td><td align="left"><bold>0.629</bold></td><td align="left">0.600</td><td align="left"><bold>0.630</bold></td><td align="left">0.625</td><td align="left">0.587</td><td align="left"><bold>0.627</bold></td></tr><tr><td align="left">488912</td><td align="left">0.679</td><td align="left">0.643</td><td align="left">0.634</td><td align="left">0.668</td><td align="left"><bold>0.693</bold></td><td align="left">0.654</td><td align="left">0.668</td><td align="left">0.675</td><td align="left">0.667</td><td align="left">0.634</td><td align="left">0.650</td><td align="left"><bold>0.673</bold></td><td align="left">0.644</td><td align="left"><bold>0.698</bold></td></tr><tr><td align="left">488915</td><td align="left">0.718</td><td align="left">0.686</td><td align="left">0.679</td><td align="left">0.713</td><td align="left"><bold>0.731</bold></td><td align="left">0.693</td><td align="left">0.680</td><td align="left"><bold>0.708</bold></td><td align="left">0.692</td><td align="left">0.659</td><td align="left">0.680</td><td align="left">0.693</td><td align="left">0.679</td><td align="left"><bold>0.735</bold></td></tr><tr><td align="left">488917</td><td align="left"><bold>0.808</bold></td><td align="left">0.777</td><td align="left">0.759</td><td align="left">0.805</td><td align="left"><bold>0.814</bold></td><td align="left">0.788</td><td align="left">0.760</td><td align="left">0.799</td><td align="left">0.788</td><td align="left">0.726</td><td align="left">0.752</td><td align="left">0.786</td><td align="left">0.780</td><td align="left"><bold>0.834</bold></td></tr><tr><td align="left">488918</td><td align="left">0.762</td><td align="left">0.745</td><td align="left">0.735</td><td align="left"><bold>0.778</bold></td><td align="left"><bold>0.778</bold></td><td align="left">0.766</td><td align="left">0.729</td><td align="left">0.767</td><td align="left">0.737</td><td align="left">0.690</td><td align="left">0.708</td><td align="left">0.742</td><td align="left">0.746</td><td align="left"><bold>0.799</bold></td></tr><tr><td align="left">492992</td><td align="left"><bold>0.829</bold></td><td align="left">0.784</td><td align="left">0.783</td><td align="left">0.800</td><td align="left"><bold>0.849</bold></td><td align="left">0.807</td><td align="left">0.802</td><td align="left">0.822</td><td align="left">0.825</td><td align="left">0.726</td><td align="left">0.759</td><td align="left">0.790</td><td align="left">0.802</td><td align="left"><bold>0.845</bold></td></tr><tr><td align="left">504607</td><td align="left"><bold>0.694</bold></td><td align="left">0.678</td><td align="left"><bold>0.692</bold></td><td align="left">0.686</td><td align="left">0.690</td><td align="left">0.668</td><td align="left">0.673</td><td align="left">0.656</td><td align="left">0.676</td><td align="left">0.640</td><td align="left">0.662</td><td align="left">0.655</td><td align="left">0.649</td><td align="left"><bold>0.721</bold></td></tr><tr><td align="left">624504</td><td align="left"><bold>0.884</bold></td><td align="left">0.850</td><td align="left">0.857</td><td align="left">0.867</td><td align="left"><bold>0.884</bold></td><td align="left">0.858</td><td align="left">0.858</td><td align="left">0.861</td><td align="left">0.872</td><td align="left">0.832</td><td align="left">0.862</td><td align="left">0.876</td><td align="left">0.868</td><td align="left"><bold>0.897</bold></td></tr><tr><td align="left">651739</td><td align="left">0.791</td><td align="left">0.770</td><td align="left">0.773</td><td align="left">0.781</td><td align="left"><bold>0.802</bold></td><td align="left">0.782</td><td align="left">0.771</td><td align="left">0.788</td><td align="left">0.779</td><td align="left">0.729</td><td align="left">0.759</td><td align="left">0.754</td><td align="left"><bold>0.792</bold></td><td align="left"><bold>0.804</bold></td></tr><tr><td align="left">651744</td><td align="left">0.884</td><td align="left">0.862</td><td align="left">0.872</td><td align="left">0.885</td><td align="left"><bold>0.889</bold></td><td align="left">0.883</td><td align="left">0.875</td><td align="left">0.896</td><td align="left">0.869</td><td align="left">0.829</td><td align="left">0.843</td><td align="left">0.853</td><td align="left"><bold>0.899</bold></td><td align="left"><bold>0.901</bold></td></tr><tr><td align="left">652065</td><td align="left"><bold>0.800</bold></td><td align="left">0.752</td><td align="left">0.782</td><td align="left">0.780</td><td align="left"><bold>0.785</bold></td><td align="left">0.775</td><td align="left">0.758</td><td align="left">0.774</td><td align="left">0.776</td><td align="left">0.736</td><td align="left">0.759</td><td align="left">0.772</td><td align="left">0.763</td><td align="left"><bold>0.826</bold></td></tr><tr><td align="left">average</td><td align="left"><bold>0.794</bold></td><td align="left">0.762</td><td align="left">0.766</td><td align="left">0.786</td><td align="left"><bold>0.798</bold></td><td align="left">0.777</td><td align="left">0.763</td><td align="left">0.784</td><td align="left">0.783</td><td align="left">0.741</td><td align="left">0.762</td><td align="left">0.778</td><td align="left">0.771</td><td align="left"><bold>0.814</bold></td></tr></tbody></table><table-wrap-foot><p>Each value shows the averaged AUC from twenty repeated experiments on the test set (bold: top 3 AUC on each dataset), and the last row shows the averaged AUC calculated from 19 AUC results</p></table-wrap-foot></table-wrap>
</p>
        <p>Statistical analysis with paired <italic>t</italic>-tests was performed to evaluate differences between the means of paired outcomes. The AUC scores of the comprehensive ensembles were compared with the top-scored AUC from the individual classifier in each dataset from the five fold cross-validation. Assuming that two output scores <italic>y</italic><sub>1</sub> and <italic>y</italic><sub>2</sub> follow normal distributions, the difference between these two scores should also follow a normal distribution. The null hypothesis of no difference between the means of two output scores, calculated as <italic>d</italic>=<italic>y</italic><sub>1</sub>−<italic>y</italic><sub>2</sub>, indicates that the distribution of this difference has mean 0 and variance <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sigma ^{2}_{d}$\end{document}</tex-math><mml:math id="M2"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq1.gif"/></alternatives></inline-formula>. The comprehensive ensemble achieved an AUC score exceeding the top-scored AUC from an individual classifier in 16 out of 19 PubChem bioassays as shown in Table <xref rid="Tab3" ref-type="table">3</xref>. Let <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\bar {d}, s_{d}$\end{document}</tex-math><mml:math id="M4"><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq2.gif"/></alternatives></inline-formula>, <italic>n</italic> denote the mean difference, the standard deviation of the differences, and the number of samples, respectively. The results are significant at a p-value of 8.2×10<sup>−7</sup>, where the t value is calculated by <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$t_{d} = \frac {\bar {d}} {\frac {s_{d}}{\sqrt {n}}} \sim t_{n-1}.$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac><mml:mo>∼</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>.</mml:mi></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq3.gif"/></alternatives></inline-formula>
<table-wrap id="Tab3"><label>Table 3</label><caption><p>The AUC scores of the ensemble classifier and the best single classifier for 19 PubChem assays</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Assay ID</th><th align="left">The Best Single Classifier (AUC)</th><th align="left">The Ensemble Classifier (AUC)</th></tr></thead><tbody><tr><td align="justify">1851_1a2</td><td align="justify">0.922</td><td align="justify">0.934</td></tr><tr><td align="justify">1851_2c19</td><td align="justify">0.885</td><td align="justify">0.900</td></tr><tr><td align="justify">1851_2c9</td><td align="justify">0.88</td><td align="justify">0.898</td></tr><tr><td align="justify">1851_2d6</td><td align="justify">0.867</td><td align="justify">0.884</td></tr><tr><td align="justify">1851_3a4</td><td align="justify">0.895</td><td align="justify">0.914</td></tr><tr><td align="justify">1915</td><td align="justify">0.758</td><td align="justify">0.755</td></tr><tr><td align="justify">2358</td><td align="justify">0.787</td><td align="justify">0.803</td></tr><tr><td align="justify">463213</td><td align="justify">0.685</td><td align="justify">0.689</td></tr><tr><td align="justify">463215</td><td align="justify">0.630</td><td align="justify">0.627</td></tr><tr><td align="justify">488912</td><td align="justify">0.693</td><td align="justify">0.698</td></tr><tr><td align="justify">488915</td><td align="justify">0.731</td><td align="justify">0.735</td></tr><tr><td align="justify">488917</td><td align="justify">0.814</td><td align="justify">0.834</td></tr><tr><td align="justify">488918</td><td align="justify">0.778</td><td align="justify">0.799</td></tr><tr><td align="justify">492992</td><td align="justify">0.849</td><td align="justify">0.845</td></tr><tr><td align="justify">504607</td><td align="justify">0.694</td><td align="justify">0.721</td></tr><tr><td align="justify">624504</td><td align="justify">0.884</td><td align="justify">0.897</td></tr><tr><td align="justify">651739</td><td align="justify">0.802</td><td align="justify">0.804</td></tr><tr><td align="justify">651744</td><td align="justify">0.899</td><td align="justify">0.901</td></tr><tr><td align="justify">652065</td><td align="justify">0.800</td><td align="justify">0.826</td></tr></tbody></table></table-wrap>
</p>
      </sec>
      <sec id="Sec9">
        <title>Performance comparison with other ensemble approaches</title>
        <p>In addition to a comparison with individual models, we compared the proposed ensemble method with other ensemble approaches based on the ensemble subject and combining technique, as shown in Table <xref rid="Tab4" ref-type="table">4</xref>.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Performance comparison with other ensemble approaches</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">BioAssay</th><th align="left" colspan="8">limited ensemble</th><th align="left" colspan="2">comprehensive ensemble</th></tr><tr><th align="left"/><th align="left" colspan="3">method ensemble</th><th align="left" colspan="5">representation ensemble</th><th align="left"/><th align="left"/></tr><tr><th align="left"/><th align="left">PubChem</th><th align="left">ECFP</th><th align="left">MACCS</th><th align="left">RF</th><th align="left">SVM</th><th align="left">GBM</th><th align="left">NN</th><th align="left">NN (+SMILES) <sup>∗</sup></th><th align="left">average</th><th align="left">meta-learning</th></tr></thead><tbody><tr><td align="left">1851_1a2</td><td align="left">0.921</td><td align="left">0.922</td><td align="left">0.910</td><td align="left">0.931</td><td align="left">0.920</td><td align="left">0.907</td><td align="left"><bold>0.937</bold></td><td align="left"><bold>0.941</bold></td><td align="left">0.934</td><td align="left"><bold>0.943</bold></td></tr><tr><td align="left">1851_2c19</td><td align="left">0.875</td><td align="left">0.889</td><td align="left">0.879</td><td align="left">0.893</td><td align="left">0.887</td><td align="left">0.869</td><td align="left"><bold>0.902</bold></td><td align="left"><bold>0.905</bold></td><td align="left">0.900</td><td align="left"><bold>0.908</bold></td></tr><tr><td align="left">1851_2c9</td><td align="left">0.878</td><td align="left">0.885</td><td align="left">0.866</td><td align="left">0.888</td><td align="left">0.882</td><td align="left">0.865</td><td align="left"><bold>0.899</bold></td><td align="left"><bold>0.905</bold></td><td align="left">0.898</td><td align="left"><bold>0.908</bold></td></tr><tr><td align="left">1851_2d6</td><td align="left">0.870</td><td align="left">0.869</td><td align="left">0.853</td><td align="left">0.880</td><td align="left">0.869</td><td align="left">0.852</td><td align="left"><bold>0.884</bold></td><td align="left"><bold>0.886</bold></td><td align="left"><bold>0.884</bold></td><td align="left"><bold>0.892</bold></td></tr><tr><td align="left">1851_3a4</td><td align="left">0.890</td><td align="left">0.902</td><td align="left">0.874</td><td align="left">0.898</td><td align="left">0.901</td><td align="left">0.881</td><td align="left">0.913</td><td align="left"><bold>0.919</bold></td><td align="left"><bold>0.914</bold></td><td align="left"><bold>0.920</bold></td></tr><tr><td align="left">1915</td><td align="left">0.729</td><td align="left">0.721</td><td align="left">0.750</td><td align="left"><bold>0.766</bold></td><td align="left">0.728</td><td align="left">0.739</td><td align="left">0.747</td><td align="left">0.750</td><td align="left"><bold>0.755</bold></td><td align="left"><bold>0.764</bold></td></tr><tr><td align="left">2358</td><td align="left">0.758</td><td align="left">0.781</td><td align="left">0.780</td><td align="left"><bold>0.805</bold></td><td align="left">0.780</td><td align="left">0.772</td><td align="left"><bold>0.805</bold></td><td align="left">0.803</td><td align="left">0.803</td><td align="left"><bold>0.807</bold></td></tr><tr><td align="left">463213</td><td align="left">0.669</td><td align="left">0.672</td><td align="left">0.669</td><td align="left"><bold>0.689</bold></td><td align="left">0.671</td><td align="left">0.666</td><td align="left">0.682</td><td align="left">0.684</td><td align="left"><bold>0.689</bold></td><td align="left"><bold>0.694</bold></td></tr><tr><td align="left">463215</td><td align="left">0.604</td><td align="left">0.603</td><td align="left"><bold>0.639</bold></td><td align="left"><bold>0.636</bold></td><td align="left">0.604</td><td align="left">0.623</td><td align="left">0.623</td><td align="left">0.624</td><td align="left">0.627</td><td align="left"><bold>0.634</bold></td></tr><tr><td align="left">488912</td><td align="left">0.674</td><td align="left">0.682</td><td align="left">0.676</td><td align="left"><bold>0.698</bold></td><td align="left">0.668</td><td align="left">0.667</td><td align="left">0.695</td><td align="left"><bold>0.698</bold></td><td align="left"><bold>0.698</bold></td><td align="left"><bold>0.700</bold></td></tr><tr><td align="left">488915</td><td align="left">0.720</td><td align="left">0.719</td><td align="left">0.699</td><td align="left">0.731</td><td align="left">0.711</td><td align="left">0.700</td><td align="left">0.732</td><td align="left"><bold>0.737</bold></td><td align="left"><bold>0.735</bold></td><td align="left"><bold>0.739</bold></td></tr><tr><td align="left">488917</td><td align="left">0.811</td><td align="left">0.815</td><td align="left">0.785</td><td align="left">0.824</td><td align="left">0.808</td><td align="left">0.782</td><td align="left">0.832</td><td align="left"><bold>0.838</bold></td><td align="left"><bold>0.834</bold></td><td align="left"><bold>0.841</bold></td></tr><tr><td align="left">488918</td><td align="left">0.777</td><td align="left">0.783</td><td align="left">0.743</td><td align="left">0.780</td><td align="left">0.782</td><td align="left">0.752</td><td align="left">0.793</td><td align="left"><bold>0.799</bold></td><td align="left"><bold>0.799</bold></td><td align="left"><bold>0.801</bold></td></tr><tr><td align="left">492992</td><td align="left">0.820</td><td align="left">0.829</td><td align="left">0.795</td><td align="left"><bold>0.854</bold></td><td align="left">0.818</td><td align="left">0.812</td><td align="left">0.836</td><td align="left">0.845</td><td align="left"><bold>0.845</bold></td><td align="left"><bold>0.862</bold></td></tr><tr><td align="left">504607</td><td align="left"><bold>0.710</bold></td><td align="left">0.687</td><td align="left">0.682</td><td align="left">0.708</td><td align="left">0.701</td><td align="left">0.703</td><td align="left">0.698</td><td align="left">0.706</td><td align="left"><bold>0.721</bold></td><td align="left"><bold>0.726</bold></td></tr><tr><td align="left">624504</td><td align="left">0.879</td><td align="left">0.875</td><td align="left">0.867</td><td align="left">0.896</td><td align="left">0.880</td><td align="left">0.878</td><td align="left">0.892</td><td align="left"><bold>0.900</bold></td><td align="left"><bold>0.897</bold></td><td align="left"><bold>0.904</bold></td></tr><tr><td align="left">651739</td><td align="left">0.795</td><td align="left"><bold>0.806</bold></td><td align="left">0.774</td><td align="left">0.800</td><td align="left">0.776</td><td align="left">0.783</td><td align="left">0.803</td><td align="left"><bold>0.807</bold></td><td align="left">0.804</td><td align="left"><bold>0.809</bold></td></tr><tr><td align="left">651744</td><td align="left">0.892</td><td align="left"><bold>0.902</bold></td><td align="left">0.868</td><td align="left">0.890</td><td align="left">0.882</td><td align="left">0.879</td><td align="left">0.899</td><td align="left"><bold>0.905</bold></td><td align="left">0.901</td><td align="left"><bold>0.909</bold></td></tr><tr><td align="left">652065</td><td align="left">0.795</td><td align="left">0.791</td><td align="left">0.784</td><td align="left">0.807</td><td align="left">0.804</td><td align="left">0.803</td><td align="left">0.813</td><td align="left"><bold>0.822</bold></td><td align="left"><bold>0.826</bold></td><td align="left"><bold>0.832</bold></td></tr><tr><td align="left">average</td><td align="left">0.793</td><td align="left">0.796</td><td align="left">0.784</td><td align="left">0.809</td><td align="left">0.793</td><td align="left">0.786</td><td align="left">0.810</td><td align="left"><bold>0.814</bold></td><td align="left"><bold>0.814</bold></td><td align="left"><bold>0.821</bold></td></tr></tbody></table><table-wrap-foot><p>All AUC values except those in the last two columns are based on limited subject ensembles, while the AUC values in the last two columns are from the comprehensive ensemble. The first three columns are method ensembles that consider various methods by fixing them to a target molecular fingerprint. The next five columns are representation ensembles that consider various chemical compound representations by fixing them to a learning method. Except for the final meta-learning approach, combining is based on uniform averaging. Each value is the averaged AUC from five repeated experiments (bold: top 3)</p><p><sup>∗</sup>NN(+SMILES) is a representation ensemble that combines a set of models trained on a diversified input representation of fingerprints (PubChem, ECFP, MACCS) and SMILES-based on NN</p></table-wrap-foot></table-wrap>
</p>
        <p>The first three columns showe the method ensemble, which combines predictions from RF, SVM, GBM, and NN by fixing them to a particular chemical representation. The ensembles based on PubChem, ECFP, and MACCS showed AUC values of 0.793, 0.796, and 0.784, which are 0.016, 0.015, and 0.018 higher than the average AUC value for the four individual methods based on those representations, respectively. The next five columns show the representation ensembles, which combine the PubChem, ECFP, and MACCS molecular representations by fixing them to a particular learning method. As with the method ensembles, the representation ensembles outperformed the average results from the individual representation models based on their learning methods. In particular, the NN-based individual models showed lower AUCs values than the RF-based models, but the NN-based combined representation ensemble showed a higher AUC value than the RF-based ensemble.</p>
        <p>Bagging is an easy-to-develop and powerful technique for class imbalance problems [<xref ref-type="bibr" rid="CR28">28</xref>]. Figure <xref rid="Fig1" ref-type="fig">1</xref>a shows the effectiveness of bagging by comparing a plain neural network (NN) with a bootstrap aggregated neural network (NN-bagging) and a neural network-based representation ensemble (NN-representation ensemble). As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, bagging improved the AUC in both ensemble techniques. As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b, the improved AUC by bagging was correlated with the imbalance ratio of the dataset (Pearson’s r=0.69, p-value= 1.1×10<sup>−3</sup>). The results showed greater improvement with a higher imbalance ratio.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Ensemble effects on class-imbalanced datasets. <bold>a</bold> Improved average AUC value produced by neural network bagging (NN-bagging) and neural network-based representation ensemble (NN-representation ensemble) over three fingerprints. <bold>b</bold> Pearson’s correlation (r=0.69, p-value=1.1x 10<sup>−3</sup>) between the improved AUC values from NN-bagging and the class imbalance ratio. The class imbalance ratio was calculated from the number of active and inactive chemicals, as shown in Table <xref rid="Tab1" ref-type="table">1</xref></p></caption><graphic xlink:href="12859_2019_3135_Fig1_HTML" id="MO1"/></fig>
</p>
        <p>The proposed multi-subject comprehensive ensemble combines all models regardless of learning method or representation: 12 models consisting of the unique combinations of representations (PubChem, ECFP, and MACCS) and learning methods (RF, SVM, GBM, and NN) and the newly proposed SMILES-NN model. All ensembles except for the last column combined the various models by uniform averaging. The comprehensive ensemble outperformed all limited ensemble approaches based on average combining.</p>
        <p>In terms of the combination technique, we compared simple uniform averaging with the proposed meta-learning techniques in both comprehensive ensembles. The results of the comprehensive ensemble from Table <xref rid="Tab2" ref-type="table">2</xref> are presented in the second to the last column of Table <xref rid="Tab4" ref-type="table">4</xref>. The last column in Table <xref rid="Tab4" ref-type="table">4</xref> shows the performance comparison between meta-learning and the comprehensive ensemble. The multi-task neural networks [<xref ref-type="bibr" rid="CR10">10</xref>] achieved state-of-the-art performance on 19 PubChem bioassays with performance measurement of the AUC. As shown in Table <xref rid="Tab5" ref-type="table">5</xref>, our approach outperformed multi-task learning in 13 out of 19 PubChem bioassays. From “<xref rid="Sec24" ref-type="sec">Convolutional and recurrent neural networks</xref>” section, this result was statistically significant at a p-value of 3.9×10<sup>−8</sup> in 13 out of 19 datasets and resulted in a higher mean AUC value for the meta-learning network than for the multi-task network.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Performance comparison between multi-task [<xref ref-type="bibr" rid="CR10">10</xref>] and meta-learning neural networks</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Assay ID</th><th align="left">Multi-task</th><th align="left">Proposed (Meta-learning)</th></tr></thead><tbody><tr><td align="left">1851_1a2</td><td align="left">0.938</td><td align="left"><bold>0.943</bold></td></tr><tr><td align="left">1851_2c19</td><td align="left">0.903</td><td align="left"><bold>0.908</bold></td></tr><tr><td align="left">1851_2c9</td><td align="left">0.907</td><td align="left"><bold>0.908</bold></td></tr><tr><td align="left">1851_2d6</td><td align="left">0.861</td><td align="left"><bold>0.892</bold></td></tr><tr><td align="left">1851_3a4</td><td align="left">0.897</td><td align="left"><bold>0.920</bold></td></tr><tr><td align="left">1915</td><td align="left">0.750</td><td align="left"><bold>0.764</bold></td></tr><tr><td align="left">2358</td><td align="left">0.751</td><td align="left"><bold>0.807</bold></td></tr><tr><td align="left">463213</td><td align="left">0.676</td><td align="left"><bold>0.694</bold></td></tr><tr><td align="left">463215</td><td align="left">0.654</td><td align="left">0.634</td></tr><tr><td align="left">488912</td><td align="left">0.816</td><td align="left">0.700</td></tr><tr><td align="left">488915</td><td align="left">0.873</td><td align="left">0.739</td></tr><tr><td align="left">488917</td><td align="left">0.894</td><td align="left">0.841</td></tr><tr><td align="left">488918</td><td align="left">0.842</td><td align="left">0.801</td></tr><tr><td align="left">492992</td><td align="left">0.829</td><td align="left"><bold>0.862</bold></td></tr><tr><td align="left">504607</td><td align="left">0.670</td><td align="left"><bold>0.726</bold></td></tr><tr><td align="left">624504</td><td align="left">0.889</td><td align="left"><bold>0.904</bold></td></tr><tr><td align="left">651739</td><td align="left">0.825</td><td align="left">0.809</td></tr><tr><td align="left">651744</td><td align="left">0.900</td><td align="left"><bold>0.909</bold></td></tr><tr><td align="left">652065</td><td align="left">0.792</td><td align="left"><bold>0.832</bold></td></tr></tbody></table><table-wrap-foot><p>The mean AUC values for both neural networks are shown (bold: top AUC on each dataset)</p></table-wrap-foot></table-wrap>
</p>
      </sec>
      <sec id="Sec10">
        <title>Performance comparison on other dataset</title>
        <p>The Drug Therapeutics Program (DTP) AIDS Antiviral Screen developed an HIV dataset for over 40,000 compounds. These results are categorized into three groups: confirmed inactive (CI), confirmed active (CA) and confirmed moderately active (CM). Following previous research [<xref ref-type="bibr" rid="CR29">29</xref>], we also combined the latter two labels (CA and CM), resulting it a classification task to discriminate inactive and active.</p>
        <p>We evaluated our meta-learning neural network on the HIV dataset following identical experimental settings as described in MoleculeNet [<xref ref-type="bibr" rid="CR29">29</xref>]. The HIV dataset was divided by scaffold-based splitting into training, validation, and test sets at a ratio of 80:10:10. Scaffold-based splitting separates structurally different molecules into different subgroups [<xref ref-type="bibr" rid="CR29">29</xref>]. For the performance metrics, we used AU-ROC, accuracy, Matthews correlation coefficient (MCC), and F1-score. Accuracy, MCC, and F1-score were defined as follows: 
<disp-formula id="Equa"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} &amp;\texttt{Accuracy} = \frac{TP+TN}{TP+TN+FP+FN} \\ &amp;\texttt{MCC} = \frac{TP*TN-FP*FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}} \\ &amp;\texttt{F1-score} = \frac{2TP}{2TP+FP+FN} \\ \end{array} $$ \end{document}</tex-math><mml:math id="M8"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mtext mathvariant="monospace">Accuracy</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mtext mathvariant="monospace">MCC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>∗</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>−</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext><mml:mo>∗</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mtext mathvariant="italic">TN</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"/><mml:mtd class="align-2"><mml:mtext mathvariant="monospace">F1-score</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mtext mathvariant="italic">TP</mml:mtext></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mtext mathvariant="italic">TP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FP</mml:mtext><mml:mo>+</mml:mo><mml:mtext mathvariant="italic">FN</mml:mtext></mml:mrow></mml:mfrac><mml:mspace width="2em"/></mml:mtd><mml:mtd><mml:mspace width="2em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="align-1"/></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_3135_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <italic>TP</italic>, <italic>FP</italic>, <italic>FN</italic>, and <italic>TN</italic> represent the number of true positives, false positives, false negatives, and true negatives, respectively. Table <xref rid="Tab6" ref-type="table">6</xref> shows the results for the comparison between multi-task [<xref ref-type="bibr" rid="CR10">10</xref>] and meta-learning on the various performance metrics. For meta-learning, we applied our neural networks described in Section 2.3.4 to the multi-task neural network. We repeated the experiments 100 times and calculated the mean test score. In terms of AU-ROC, both neural networks performed similarly, however, meta-learning outperformed multi-task learning in other metrics.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Performance comparison with Multi-task neural networks [<xref ref-type="bibr" rid="CR10">10</xref>] on HIV datasets [<xref ref-type="bibr" rid="CR29">29</xref>]</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">AUC</th><th align="left">Accuracy</th><th align="left">MCC</th><th align="left">F1-score</th></tr></thead><tbody><tr><td align="left">Multi-task [<xref ref-type="bibr" rid="CR10">10</xref>]</td><td align="left">0.714 ±0.007</td><td align="left">0.947 ±0.009</td><td align="left">0.260 ±0.020</td><td align="left">0.972 ±0.005</td></tr><tr><td align="left">Meta-learning</td><td align="left">0.714 ±0.007</td><td align="left">0.964 ±0.001</td><td align="left">0.269 ±0.026</td><td align="left">0.982 ±0.001</td></tr></tbody></table><table-wrap-foot><p>The table shows the average test set of various measures for Multi-task neural networks and Meta-learning neural networks</p></table-wrap-foot></table-wrap>
</p>
      </sec>
    </sec>
    <sec id="Sec11">
      <title>Meta-learning and interpretation of model importance</title>
      <p>We made a final decision through meta-learning using the predictions from independent first-level models as input. Any learning algorithm could be used as a meta-learner. We used SVM, which achieved the highest average AUC value in further experiments compared with NN, RF, GBM, and ordinary regression.</p>
      <p>We interpreted the importance of the models through their learned weights. In the process of meta-learning, a weight is assigned to each model, and this weight could be interpreted as the model importance. As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, the degree of darkness for each method is slightly different depending on the dataset, just as the best prediction method and representation depends on the datasets (Table <xref rid="Tab2" ref-type="table">2</xref>). A darker color indicates a higher weight and importance. PubChem-SVM, ECFP-GBM, and MACCS-SVM showed low importance, while SMILES-NN and ECFP-RF showed high importance throughout the dataset. The SMILES-NN model did not show as high a performance as an individual model, but it was regarded as the most important model.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Interpretation of model importance through meta-learning. Weights through meta-learning were used to interpret model importance. Darker green indicates a highly weighted and significant model, while lighter yellow indicates a less weighted and less significant model</p></caption><graphic xlink:href="12859_2019_3135_Fig2_HTML" id="MO2"/></fig>
</p>
    </sec>
  </sec>
  <sec id="Sec12" sec-type="discussion">
    <title>Discussion</title>
    <p>Ensemble learning can improve predictability, but it requires a set of diversified hypotheses; bagging requires a set of randomly sampled datasets, a method ensemble needs to exploit diverse learning methods, and a representation ensemble needs to prepare diversified input representations. A comprehensive ensemble requires diversified datasets, methods, and representations across multi-subjects; thus, it has difficulties in preparation and learning efficiency for these hypotheses.</p>
    <p>Diversity is a crucial condition for ensemble learning. RF was superior to NN among the individual models, but NN outperformed RF in the representation ensemble. This is presumably due to model variation diversities caused by random initialization and random dropout of the neural network. In addition to model variation diversity, SMILES seems to contribute to ensemble representation diversity. The SMILES-based model did not show impressive results as an individual model, but it was considered the most important predictor when combined.</p>
    <p>The proposed comprehensive ensemble exploits diversities across multi-subjects and exhibits improved predictability compared to the individual models. In particular, the neural network and SMILES contribute to diversity and are considered important factors when combined. However, the proposed ensemble approach has difficulties associated with these diversities.</p>
  </sec>
  <sec id="Sec13" sec-type="conclusion">
    <title>Conclusions</title>
    <p>We proposed a multi-subject comprehensive ensemble due to the difficulties and importance of QSAR problems. In our experiments, the proposed ensemble method consistently outperformed all individual models, and it exhibited superiority over limited subject ensemble approaches and uniform averaging. As part of our future work, we will focus on analyzing as few hypotheses as possible or combinations of hypotheses while maintaining the ensemble effect.</p>
  </sec>
  <sec id="Sec14">
    <title>Methods</title>
    <sec id="Sec15">
      <title>Ensemble learning</title>
      <p>Ensemble learning builds a set of diversified models and combines them. Theoretically and empirically, numerous studies have demonstrated that ensemble learning usually yields higher accuracy than individual models [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR30">30</xref>–<xref ref-type="bibr" rid="CR32">32</xref>]; a collection of weak models (inducers) can be combined to produce a single strong ensemble model.</p>
      <sec id="Sec16">
        <title>Framework</title>
        <p>Ensemble learning can be divided into <italic>independent</italic> and <italic>dependent</italic> frameworks for building ensembles [<xref ref-type="bibr" rid="CR33">33</xref>]. In the independent framework, also called the randomization-based approach, individual inducers can be trained independently in parallel. On the other hand, in the dependent framework (also called the boosting-based approach), base inducers are affected sequentially by previous inducers. In terms of individual learning, we used both independent and dependent frameworks, <italic>e.g.</italic>, RF and gradient boosting, respectively. In terms of combining learning, we treated the individual inducers independently.</p>
      </sec>
      <sec id="Sec17">
        <title>Diversity</title>
        <p>Diversity is well known as a crucial condition for ensemble learning [<xref ref-type="bibr" rid="CR34">34</xref>, <xref ref-type="bibr" rid="CR35">35</xref>]. Diversity leads to uncorrelated inducers, which in turn improves the final prediction performance [<xref ref-type="bibr" rid="CR36">36</xref>]. In this paper, we focus on the following three types of diversity. 
<list list-type="bullet"><list-item><p><italic>Dataset diversity</italic></p><p>The original dataset can be diversified by sampling. Random sampling with replacement (bootstrapping) from an original dataset can generate multiple datasets with different levels of variation. If the original and bootstrap datasets are the same size (<italic>n</italic>), the bootstrap datasets are expected to have (<inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$1-\frac {1}{e}$\end{document}</tex-math><mml:math id="M10"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq4.gif"/></alternatives></inline-formula>) (≈63.2<italic>%</italic> for <italic>n</italic>) unique samples in the original data, with the remainder being duplicated. Dataset variation results in different prediction, even with the same algorithm, which produces <italic>homogeneous</italic> base inducers. Bagging (bootstrap aggregating) belongs to this category and is known to improve unstable or relatively large variance-error factors [<xref ref-type="bibr" rid="CR37">37</xref>].</p></list-item><list-item><p><italic>Learning method diversity</italic></p><p>Diverse learning algorithms that produce <italic>heterogeneous</italic> inducers yield different predictions for the same problem. Combining the predictions from <italic>heterogeneous</italic> inducers leads to improved performance that is difficult to achieve with a single inducer. Ensemble combining of diverse methods is prevalently used as a final technique in competitions, that presented in [<xref ref-type="bibr" rid="CR10">10</xref>]. We attempted to combine popular learning methods, including random forest (RF) [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR38">38</xref>], support vector machine (SVM) [<xref ref-type="bibr" rid="CR39">39</xref>], gradient boosting machine (GBM) [<xref ref-type="bibr" rid="CR40">40</xref>], and neural network (NN).</p></list-item><list-item><p><italic>Input representation diversity</italic></p><p>Drugs (chemical compounds) can be expressed with diverse representations. The diversified input representations produce different types of input features and lead to different predictions. [<xref ref-type="bibr" rid="CR21">21</xref>] demonstrated improved performance by applying ensemble learning to a diverse set of molecular fingerprints. We used diverse representations from PubChem [<xref ref-type="bibr" rid="CR22">22</xref>], ECFP [<xref ref-type="bibr" rid="CR23">23</xref>], and MACCS [<xref ref-type="bibr" rid="CR24">24</xref>] fingerprints and from a simplified molecular input line entry system (SMILES) [<xref ref-type="bibr" rid="CR25">25</xref>].</p></list-item></list></p>
      </sec>
      <sec id="Sec18">
        <title>Combining a set of models</title>
        <p>For the final decision, ensemble learning should combine predictions from multiple inducers. There are two main combination methods: weighting (non-learning) and meta-learning. Weighting method, such as majority voting and averaging, have been frequently used for their convenience and are useful for homogeneous inducers. Meta-learning methods, such as <italic>stacking</italic> [<xref ref-type="bibr" rid="CR41">41</xref>], are a learning-based methods (second-level learning) that use predictions from first-level inducers and are usually employed in heterogeneous inducers. For example, let <italic>f</italic><sub><italic>θ</italic></sub> be a classifier of an individual QSAR classifier with parameter <italic>θ</italic>, trained for a single subject (drug-specific task) <italic>p</italic>(<italic>X</italic>) with dataset <italic>X</italic> that outputs <italic>y</italic> given an input <italic>x</italic>. The optimal <italic>θ</italic> can be achieved by 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \theta^{*} = \text{argmax}_{\theta}\mathbb{E}_{(x,y)\in X}[p_{\theta}(y|x)]  $$ \end{document}</tex-math><mml:math id="M12"><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>argmax</mml:mtext></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">𝔼</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:math><graphic xlink:href="12859_2019_3135_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>Then, the second-level learning will learn to maximize output <italic>y</italic> by learning how to update the individual QSAR classifier <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\phantom {\dot {i}\!}f_{\theta ^{*}}$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq5.gif"/></alternatives></inline-formula>. “<xref rid="Sec21" ref-type="sec">First-level: individual learning</xref>” section details the first-level learning and, “<xref rid="Sec25" ref-type="sec">Second-level: combined learning</xref>” section details the second-level learning.</p>
      </sec>
    </sec>
    <sec id="Sec19">
      <title>Chemical compound representation</title>
      <p>Chemical compounds can be expressed with various types of chemical descriptors that represent their structural information. One representative type of chemical compound descriptor is a molecular fingerprint. Molecular fingerprints are encoded representations of a molecular structure as a bit-string; these have been studied and used in drug discovery for a long time. Depending on the transformation to a bit-string, there are several types of molecular fingerprints: structure key-based, topological or path-based, circular, and hybrid [<xref ref-type="bibr" rid="CR42">42</xref>]. Structure key-based fingerprints, such as PubChem [<xref ref-type="bibr" rid="CR22">22</xref>] and MACCS [<xref ref-type="bibr" rid="CR24">24</xref>], encode molecular structures based on the presence of substructures or features. Circular fingerprints, such as ECFP [<xref ref-type="bibr" rid="CR23">23</xref>], encode molecular structures based on hashing fragments up to a specific radius.</p>
      <p>Another chemical compound representation is the simplified molecular-input line-entry system (SMILES) [<xref ref-type="bibr" rid="CR25">25</xref>], which is a string type notation expressing a chemical compound structure with characters, <italic>e.g.</italic>, C,O, or N for atoms, = for bonds, and (,) for a ring structure. SMILES is generated by the symbol nodes encountered in a 2D structure in a depth-first search in terms of a graph-based computational procedure. The generated SMILES can be reconverted into a 2D or 3D representation of the chemical compound.</p>
      <p>Examples of SMILES and molecular fingerprints of leucine, which is an essential amino acid for hemoglobin formation, are as follows: 
<list list-type="bullet"><list-item><p>SMILES string: CC(C)CC(C(=O)O)N</p></list-item><list-item><p>PubChem fingerprint: 1,1,0,0,0,0,0,0,0,1,1,0,0,0,1,0,⋯</p></list-item><list-item><p>ECFP fingerprint: 0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,⋯</p></list-item><list-item><p>MACCS fingerprint: 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,⋯</p><p>(Most values in this molecular fingerprint are zero).</p></list-item></list></p>
      <p>Figure <xref rid="Fig3" ref-type="fig">3</xref> shows the two-levels of learning procedure. First-level learning is an individual learning level from diversified learning algorithms and chemical compound representations. The prediction probabilities produced from first-level learning models are used as inputs for second-level learning. Second-level learning makes the final decision by learning the importance of individual models produced from the first-level predictions.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Learning procedure of the proposed comprehensive ensemble. The individual <italic>i</italic>-th learning algorithm <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {L}_{i}$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq6.gif"/></alternatives></inline-formula> outputs its prediction probability <italic>P</italic><sub><italic>i</italic></sub> for the training dataset through 5-fold cross-validation. The <italic>n</italic> diverse learning algorithms produce <italic>n</italic> prediction probabilities (<italic>P</italic><sub>1</sub>,<italic>P</italic><sub>2</sub>,⋯,<italic>P</italic><sub><italic>n</italic></sub>). The probabilities are concatenated and then used as input to the second-level learning algorithm <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\boldsymbol {\mathcal {L}}$\end{document}</tex-math><mml:math id="M18"><mml:mi mathvariant="bold-script">ℒ</mml:mi></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq7.gif"/></alternatives></inline-formula>, which makes a final decision <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {y}$\end{document}</tex-math><mml:math id="M20"><mml:mi>ŷ</mml:mi></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq8.gif"/></alternatives></inline-formula>. <bold>a</bold> First-level learning. <bold>b</bold> Second-level learning</p></caption><graphic xlink:href="12859_2019_3135_Fig3_HTML" id="MO3"/></fig>
</p>
    </sec>
    <sec id="Sec20">
      <title>Notation</title>
      <p>The notation used in our paper is as follows: 
<list list-type="bullet"><list-item><p><bold>x</bold>: preprocessed chemical compound-representation input, where <bold>x</bold> can be a particular type of molecular fingerprints or SMILES.</p></list-item><list-item><p><bold>h</bold>: hidden representation</p></list-item><list-item><p><inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {L}$\end{document}</tex-math><mml:math id="M22"><mml:mi mathvariant="script">ℒ</mml:mi></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq9.gif"/></alternatives></inline-formula>: first-level individual learning algorithm (<inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {L}_{i}$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq10.gif"/></alternatives></inline-formula>: <italic>i</italic>-th algorithm, <italic>i</italic>={1,⋯,<italic>n</italic>})</p></list-item><list-item><p><inline-formula id="IEq11"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\boldsymbol {\mathcal {L}}$\end{document}</tex-math><mml:math id="M26"><mml:mi mathvariant="bold-script">ℒ</mml:mi></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq11.gif"/></alternatives></inline-formula>: second-level learning algorithm</p></list-item><list-item><p><italic>P</italic>: predicted probability from the individual model (<italic>P</italic><sub><italic>i</italic></sub>: predicted probability from the <inline-formula id="IEq12"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {L}_{i}$\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq12.gif"/></alternatives></inline-formula>)</p></list-item><list-item><p><inline-formula id="IEq13"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {y}$\end{document}</tex-math><mml:math id="M30"><mml:mi>ŷ</mml:mi></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq13.gif"/></alternatives></inline-formula>: final predicted decision from the second-level learning</p></list-item><list-item><p><italic>σ</italic>: activation function (<italic>σ</italic><sub><italic>s</italic></sub>: sigmoid, <italic>σ</italic><sub><italic>r</italic></sub>: rectified linear unit (ReLU), and <italic>σ</italic><sub><italic>t</italic></sub>: hyperbolic tangent)</p></list-item><list-item><p><italic>n</italic>: total number of individual algorithms</p></list-item></list></p>
    </sec>
    <sec id="Sec21">
      <title>First-level: individual learning</title>
      <p>With a combination of learning algorithms and chemical compound input representations, we generated thirteen kinds of individual learning models: nine models from conventional machine learning methods, three models from a plain feed-forward neural network, and one model from the 1D-CNN and RNN-based newly proposed neural network model.</p>
      <sec id="Sec22">
        <title>Conventional machine learning methods</title>
        <p>Among the conventional machine learning methods, we used SVM, RF, and GBM with three types of molecular fingerprints, resulting in nine combination models consisting of all unique pairs of learning algorithms (SVM, RF, and GBM) and fingerprints (PubChem, ECFP, and MACCS). We set the penalty parameter to 0.05 for the linear SVM, and the number of estimators was set to 100 for RF and GBM based on a grid search and experimental efficiency. The prediction probabilities from these learning methods are used as inputs for second-level learning. However, SVM outputs a signed distance to the hyperplane rather than a probability. Thus, we applied a probability calibration method to convert the SVM results into probabilistic outputs.</p>
      </sec>
      <sec id="Sec23">
        <title>Plain feed-forward neural network</title>
        <p>We used a plain feed-forward neural network (NN) for the vector-type fingerprints: PubChem-NN, ECFP-NN, and MACCS-NN. The neural network structure consists of three fully connected layers (Fcl) with 512, 64, and 1 units in each layer and using, the ReLU, tanh, and sigmoid activation functions, respectively, 
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ P= \sigma_{s}(\mathbf{Fcl}(\sigma_{t}(\mathbf{Fcl}(\sigma_{r}(\mathbf{Fcl}(\mathbf{x})))))).   $$ \end{document}</tex-math><mml:math id="M32"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Fcl</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Fcl</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Fcl</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mi>.</mml:mi></mml:math><graphic xlink:href="12859_2019_3135_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>The sigmoid activation function outputs a probability for binary classification. We used the Adam optimizer [<xref ref-type="bibr" rid="CR43">43</xref>] with binary cross-entropy loss (learning rate: 0.001, epoch: 30, and mini-batch size: 256).</p>
      </sec>
      <sec id="Sec24">
        <title>Convolutional and recurrent neural networks</title>
        <p>To learn key features through end-to-end neural network learning automatically, we used a SMILES string as input and exploited the neural network structures of the 1D-CNNs and RNNs. A CNN is used to recognize the short-term dependencies, and an RNN is used as the next layer to learn long-term dependencies from the recognized local patterns.</p>
        <p>As illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref> of the preprocessing step, the input SMILES strings were preprocessed with one-hot encoding [<xref ref-type="bibr" rid="CR44">44</xref>–<xref ref-type="bibr" rid="CR46">46</xref>], which sets only the corresponding symbol to 1 and others to 0. The input is truncated/padded to a maximum length of 100. We only consider the most frequent nine characters in SMILES and treat the remaining symbols as OTHERS, thus the encoding dimension was reduced to 10.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Proposed CNN + RNN model. The input SMILES strings are converted with one-hot encoding and truncated to a maximum length of 100. The preprocessed input is subsequently fed to the CNN layer without pooling, and the outputs are directly fed into the GRU layer</p></caption><graphic xlink:href="12859_2019_3135_Fig4_HTML" id="MO4"/></fig>
</p>
        <p>As illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref> of the neural networks step, the preprocessed input <bold>x</bold> was fed into the CNN layer without pooling (CNN filter length: 17, number of filters: 384). Then, the outputs from the CNN were fed into the GRU layer (dimension: 9, structure: many-to-many). 
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \mathbf{h}= \sigma_{t}(\mathbf{GRU}(\sigma_{r}(\mathbf{Conv}(\mathbf{x})))),   $$ \end{document}</tex-math><mml:math id="M34"><mml:mi mathvariant="bold">h</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">GRU</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Conv</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2019_3135_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <bold>h</bold> is the output of GRU layer, <italic>σ</italic><sub><italic>r</italic></sub> is the ReLU, and <italic>σ</italic><sub><italic>t</italic></sub> is the hyperbolic tangent. The output <bold>h</bold> was flattened and then fed into a fully connected neural network. 
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ P= \sigma_{s}(\mathbf{Fcl}(\sigma_{r}(\mathbf{Fcl}(\mathbf{h}_{\text{\texttt{flatten}}})))),   $$ \end{document}</tex-math><mml:math id="M36"><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Fcl</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">Fcl</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mtext>flatten</mml:mtext></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2019_3135_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <italic>P</italic> is the output probability from the sigmoid activation function for binary classification. The output <italic>P</italic> is subsequently used for second-level learning as in the last step in Fig. <xref rid="Fig4" ref-type="fig">4</xref>.</p>
        <p>We used dropout for each layer (CNN: 0.9, RNN: 0.6, first Fcl: 0.6) and an Adam optimizer (learning rate: 0.001, epoch: 120, mini-batch size: 256) with binary cross-entropy. Most of these hyperparameters were empirically determined.</p>
      </sec>
    </sec>
    <sec id="Sec25">
      <title>Second-level: combined learning</title>
      <p>We combined the first-level predictions generated from the set of individual models to obtain the final decision.</p>
      <p>We have <italic>n</italic> individual learning algorithms <inline-formula id="IEq14"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {L}_{i}$\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq14.gif"/></alternatives></inline-formula>, where <italic>i</italic>={1,⋯,<italic>n</italic>}, and the <italic>i</italic>-th model outputs the prediction probability <italic>P</italic><sub><italic>i</italic></sub> for a given <bold>x</bold>. We can determine the final prediction <inline-formula id="IEq15"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {y}$\end{document}</tex-math><mml:math id="M40"><mml:mi>ŷ</mml:mi></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq15.gif"/></alternatives></inline-formula> by weighting, <italic>w</italic><sub><italic>i</italic></sub>: 
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \hat{y}=\sum_{i=1}^{n}w_{i}P_{i}(\mathbf{x}),   $$ \end{document}</tex-math><mml:math id="M42"><mml:mi>ŷ</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2019_3135_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where if the weight <italic>w</italic><sub><italic>i</italic></sub>=1/<italic>n</italic>,∀<italic>i</italic> indicates, <italic>uniform averaging</italic>.</p>
      <p>As another technique, we can combine the first-level output predictions through meta-learning. The performance of individual methods varies depending on each dataset as shown in “<xref rid="Sec8" ref-type="sec">Performance comparison with individual models</xref>” section; there is no invincible universal method. The learned weights from the individual models are applied to the corresponding datasets. Thus, we use learning based combining methods (meta-learning) rather than simple averaging or voting. 
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} \hat{y}&amp;=\boldsymbol{\mathcal{L}}(\mathcal{L}_{1}(\mathbf{x}), \mathcal{L}_{2}(\mathbf{x}),\cdots,\mathcal{L}_{n}(\mathbf{x})) \end{array} $$ \end{document}</tex-math><mml:math id="M44"><mml:mtable class="align" columnalign="left"><mml:mtr><mml:mtd class="align-1"><mml:mi>ŷ</mml:mi></mml:mtd><mml:mtd class="align-2"><mml:mo>=</mml:mo><mml:mi mathvariant="bold-script">ℒ</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mspace width="0.3em"/><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mspace width="2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_3135_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ7">
          <label>7</label>
          <alternatives>
            <tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\begin{array}{*{20}l} &amp;=\boldsymbol{\mathcal{L}} \left ([P_{1}, P_{2}, \cdots, P_{n}] \right),  \end{array} $$ \end{document}</tex-math>
            <mml:math id="M46">
              <mml:mtable class="align" columnalign="left">
                <mml:mtr>
                  <mml:mtd class="align-1"/>
                  <mml:mtd class="align-2">
                    <mml:mo>=</mml:mo>
                    <mml:mi mathvariant="bold-script">ℒ</mml:mi>
                    <mml:mfenced close=")" open="(" separators="">
                      <mml:mrow>
                        <mml:mo>[</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>P</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>P</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>,</mml:mo>
                        <mml:mo>⋯</mml:mo>
                        <mml:mspace width="0.3em"/>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>P</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>n</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>]</mml:mo>
                      </mml:mrow>
                    </mml:mfenced>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="2em"/>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_3135_Article_Equ7.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>where <inline-formula id="IEq16"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\boldsymbol {\mathcal {L}}$\end{document}</tex-math><mml:math id="M48"><mml:mi mathvariant="bold-script">ℒ</mml:mi></mml:math><inline-graphic xlink:href="12859_2019_3135_Article_IEq16.gif"/></alternatives></inline-formula> is a second-level learning algorithm, and any machine learning method can be applied this level. All <italic>P</italic><sub><italic>i</italic></sub>, where <italic>i</italic>={1,2,⋯,<italic>n</italic>} are concatenated and used as inputs. The model importance imposes a weight <italic>w</italic><sub><italic>i</italic></sub> on <italic>P</italic><sub><italic>i</italic></sub> and is determined through meta-learning.</p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>1D-CNNs</term>
        <def>
          <p>One-dimensional convolutional neural networks</p>
        </def>
      </def-item>
      <def-item>
        <term>AU-PRC</term>
        <def>
          <p>Area under the curve of the receiver operating characteristic curve</p>
        </def>
      </def-item>
      <def-item>
        <term>AUC</term>
        <def>
          <p>Area under the curve</p>
        </def>
      </def-item>
      <def-item>
        <term>GBM</term>
        <def>
          <p>Gradient boosting machine</p>
        </def>
      </def-item>
      <def-item>
        <term>GRU</term>
        <def>
          <p>Gated recurrent units</p>
        </def>
      </def-item>
      <def-item>
        <term>HTS</term>
        <def>
          <p>High throughput screening</p>
        </def>
      </def-item>
      <def-item>
        <term>MTNN</term>
        <def>
          <p>Multi-task neural networks</p>
        </def>
      </def-item>
      <def-item>
        <term>NN</term>
        <def>
          <p>Neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>QSAR</term>
        <def>
          <p>Quantitative structure-activity relationship</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p>Random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>RNNs</term>
        <def>
          <p>Recurrent neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>SMILES</term>
        <def>
          <p>simplified molecular-input line-entry system</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p>Support vector machine</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
Sunyoung Kwon and Ho Bae are equal contributors.
</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Sunyoung Kwon and Ho Bae contributed equally to this work.</p>
    </fn>
  </fn-group>
  <ack>
    <p>The authors would like to thank the anonymous reviewers of this manuscript for their helpful comments and suggestions.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>SK and HB designed and carried out experiments, performed analysis, and wrote the manuscript. JJ participated in experiments and editing the manuscript. SY conceived and supervised the research and edited the manuscript. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Publication costs were funded by Seoul National University. This research was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (Ministry of Science and ICT) [2014M3C9A3063541, 2018R1A2B3001628], the Brain Korea 21 Plus Project in 2018, and the Korea Health Technology R&amp;D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health and Welfare, Republic of Korea [HI15C3224]. The funding bodies did not play any roles in the design of the study and collection, analysis, and interpretation of data and in writing the manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The datasets generated and/or analyzed during the current study are available at <ext-link ext-link-type="uri" xlink:href="http://data.snu.ac.kr/QSAR/">http://data.snu.ac.kr/QSAR/</ext-link>.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Verma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Khedkar</surname>
            <given-names>VM</given-names>
          </name>
          <name>
            <surname>Coutinho</surname>
            <given-names>EC</given-names>
          </name>
        </person-group>
        <article-title>3d-qsar in drug design-a review</article-title>
        <source>Curr Top Med Chem</source>
        <year>2010</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>95</fpage>
        <lpage>115</lpage>
        <pub-id pub-id-type="pmid">19929826</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Sheridan</surname>
            <given-names>RP</given-names>
          </name>
          <name>
            <surname>Liaw</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dahl</surname>
            <given-names>GE</given-names>
          </name>
          <name>
            <surname>Svetnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Deep neural nets as a method for quantitative structure–activity relationships</article-title>
        <source>J Chem Inf Model</source>
        <year>2015</year>
        <volume>55</volume>
        <issue>2</issue>
        <fpage>263</fpage>
        <lpage>74</lpage>
        <?supplied-pmid 25635324?>
        <pub-id pub-id-type="pmid">25635324</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Golbraikh</surname>
            <given-names>Alexander</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Xiang S.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Hao</given-names>
          </name>
          <name>
            <surname>Tropsha</surname>
            <given-names>Alexander</given-names>
          </name>
        </person-group>
        <article-title>Predictive QSAR Modeling: Methods and Applications in Drug Discovery and Chemical Risk Assessment</article-title>
        <source>Handbook of Computational Chemistry</source>
        <year>2016</year>
        <publisher-loc>Dordrecht</publisher-loc>
        <publisher-name>Springer Netherlands</publisher-name>
        <fpage>1</fpage>
        <lpage>48</lpage>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Luco</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Ferretti</surname>
            <given-names>FH</given-names>
          </name>
        </person-group>
        <article-title>Qsar based on multiple linear regression and pls methods for the anti-hiv activity of a large group of hept derivatives</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>1997</year>
        <volume>37</volume>
        <issue>2</issue>
        <fpage>392</fpage>
        <lpage>401</lpage>
        <?supplied-pmid 9090857?>
        <pub-id pub-id-type="pmid">9090857</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Burden</surname>
            <given-names>FR</given-names>
          </name>
          <name>
            <surname>Winkler</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <article-title>Robust qsar models using bayesian regularized neural networks</article-title>
        <source>J Med Chem</source>
        <year>1999</year>
        <volume>42</volume>
        <issue>16</issue>
        <fpage>3183</fpage>
        <lpage>7</lpage>
        <?supplied-pmid 10447964?>
        <pub-id pub-id-type="pmid">10447964</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Burden</surname>
            <given-names>FR</given-names>
          </name>
          <name>
            <surname>Ford</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Whitley</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Winkler</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <article-title>Use of automatic relevance determination in qsar studies using bayesian neural networks</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>2000</year>
        <volume>40</volume>
        <issue>6</issue>
        <fpage>1423</fpage>
        <lpage>30</lpage>
        <?supplied-pmid 11128101?>
        <pub-id pub-id-type="pmid">11128101</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pradeep</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Povinelli</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Merrill</surname>
            <given-names>SJ</given-names>
          </name>
        </person-group>
        <article-title>An ensemble model of qsar tools for regulatory risk assessment</article-title>
        <source>J Cheminformatics</source>
        <year>2016</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>48</fpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Svetnik</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Liaw</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tong</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Culberson</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Sheridan</surname>
            <given-names>RP</given-names>
          </name>
          <name>
            <surname>Feuston</surname>
            <given-names>BP</given-names>
          </name>
        </person-group>
        <article-title>Random forest: a classification and regression tool for compound classification and qsar modeling</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>2003</year>
        <volume>43</volume>
        <issue>6</issue>
        <fpage>1947</fpage>
        <lpage>58</lpage>
        <?supplied-pmid 14632445?>
        <pub-id pub-id-type="pmid">14632445</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zakharov</surname>
            <given-names>AV</given-names>
          </name>
          <name>
            <surname>Varlamova</surname>
            <given-names>EV</given-names>
          </name>
          <name>
            <surname>Lagunin</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Dmitriev</surname>
            <given-names>AV</given-names>
          </name>
          <name>
            <surname>Muratov</surname>
            <given-names>EN</given-names>
          </name>
          <name>
            <surname>Fourches</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kuz’min</surname>
            <given-names>VE</given-names>
          </name>
          <name>
            <surname>Poroikov</surname>
            <given-names>VV</given-names>
          </name>
          <name>
            <surname>Tropsha</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Nicklaus</surname>
            <given-names>MC</given-names>
          </name>
        </person-group>
        <article-title>Qsar modeling and prediction of drug–drug interactions</article-title>
        <source>Mol Pharm</source>
        <year>2016</year>
        <volume>13</volume>
        <issue>2</issue>
        <fpage>545</fpage>
        <lpage>56</lpage>
        <?supplied-pmid 26669717?>
        <pub-id pub-id-type="pmid">26669717</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <mixed-citation publication-type="other">Dahl GE, Jaitly N, Salakhutdinov R. Multi-task neural networks for qsar predictions. arXiv preprint. 2014. arXiv:1406.1231.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <mixed-citation publication-type="other">Dietterich TG. Ensemble methods in machine learning In: Goos G, Hartmanis J, Van Leeuwen JP, editors. International Workshop on Multiple Classifier Systems. Springer: 2000. p. 1–15.</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hansen</surname>
            <given-names>LK</given-names>
          </name>
          <name>
            <surname>Salamon</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Neural network ensembles</article-title>
        <source>IEEE Trans Pattern Anal Mach Intell</source>
        <year>1990</year>
        <volume>12</volume>
        <issue>10</issue>
        <fpage>993</fpage>
        <lpage>1001</lpage>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ju</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bibaut</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>van der Laan</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The relative performance of ensemble methods with deep convolutional neural networks for image classification</article-title>
        <source>J Appl Stat</source>
        <year>2018</year>
        <volume>45</volume>
        <issue>15</issue>
        <fpage>2800</fpage>
        <lpage>18</lpage>
        <?supplied-pmid 31631918?>
        <pub-id pub-id-type="pmid">31631918</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ezzat</surname>
            <given-names>Ali</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Min</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Xiaoli</given-names>
          </name>
          <name>
            <surname>Kwoh</surname>
            <given-names>Chee-Keong</given-names>
          </name>
        </person-group>
        <article-title>Computational Prediction of Drug-Target Interactions via Ensemble Learning</article-title>
        <source>Methods in Molecular Biology</source>
        <year>2018</year>
        <publisher-loc>New York, NY</publisher-loc>
        <publisher-name>Springer New York</publisher-name>
        <fpage>239</fpage>
        <lpage>254</lpage>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Agrafiotis</surname>
            <given-names>DK</given-names>
          </name>
          <name>
            <surname>Cedeno</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Lobanov</surname>
            <given-names>VS</given-names>
          </name>
        </person-group>
        <article-title>On the use of neural network ensembles in qsar and qspr</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>2002</year>
        <volume>42</volume>
        <issue>4</issue>
        <fpage>903</fpage>
        <lpage>11</lpage>
        <?supplied-pmid 12132892?>
        <pub-id pub-id-type="pmid">12132892</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thomas</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Neves</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Solt</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Tikk</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Leser</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Relation extraction for drug-drug interactions using ensemble learning</article-title>
        <source>Training</source>
        <year>2011</year>
        <volume>4</volume>
        <issue>2,402</issue>
        <fpage>21</fpage>
        <lpage>425</lpage>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Basant</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Singh</surname>
            <given-names>KP</given-names>
          </name>
        </person-group>
        <article-title>Predicting human intestinal absorption of diverse chemicals using ensemble learning based qsar modeling approaches</article-title>
        <source>Comput Biol Chem</source>
        <year>2016</year>
        <volume>61</volume>
        <fpage>178</fpage>
        <lpage>96</lpage>
        <?supplied-pmid 26881740?>
        <pub-id pub-id-type="pmid">26881740</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Sedykh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Developing enhanced blood–brain barrier permeability models: integrating external bio-assay data in qsar modeling</article-title>
        <source>Pharm Res</source>
        <year>2015</year>
        <volume>32</volume>
        <issue>9</issue>
        <fpage>3055</fpage>
        <lpage>65</lpage>
        <?supplied-pmid 25862462?>
        <pub-id pub-id-type="pmid">25862462</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Afolabi</surname>
            <given-names>LT</given-names>
          </name>
          <name>
            <surname>Saeed</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hashim</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Petinrin</surname>
            <given-names>OO</given-names>
          </name>
        </person-group>
        <article-title>Ensemble learning method for the prediction of new bioactive molecules</article-title>
        <source>PloS ONE</source>
        <year>2018</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>0189538</fpage>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Ponzoni I, Sebastián-Pérez V, Requena-Triguero C, Roca C, Martínez MJ, Cravero F, Díaz MF, Páez JA, Arrayás RG, Adrio J, et al.Hybridizing feature selection and feature learning approaches in qsar modeling for drug discovery. Sci Rep. 2017; 7(1):2403.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Ai</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Yin</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Carcinopred-el: Novel models for predicting the carcinogenicity of chemicals using molecular fingerprints and ensemble learning methods</article-title>
        <source>Sci Rep</source>
        <year>2017</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>2118</fpage>
        <?supplied-pmid 28522849?>
        <pub-id pub-id-type="pmid">28522849</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Suzek</surname>
            <given-names>TO</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bryant</surname>
            <given-names>SH</given-names>
          </name>
        </person-group>
        <article-title>Pubchem: a public information system for analyzing bioactivities of small molecules</article-title>
        <source>Nucleic Acids Res</source>
        <year>2009</year>
        <volume>37</volume>
        <issue>suppl 2</issue>
        <fpage>623</fpage>
        <lpage>33</lpage>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Morgan</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service</article-title>
        <source>J Chem Doc</source>
        <year>1965</year>
        <volume>5</volume>
        <issue>2</issue>
        <fpage>107</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Durant</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Leland</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Henry</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Nourse</surname>
            <given-names>JG</given-names>
          </name>
        </person-group>
        <article-title>Reoptimization of mdl keys for use in drug discovery</article-title>
        <source>J Chem Inf Comput Sci</source>
        <year>2002</year>
        <volume>42</volume>
        <issue>6</issue>
        <fpage>1273</fpage>
        <lpage>80</lpage>
        <?supplied-pmid 12444722?>
        <pub-id pub-id-type="pmid">12444722</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Weininger</surname>
            <given-names>David</given-names>
          </name>
        </person-group>
        <article-title>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules</article-title>
        <source>Journal of Chemical Information and Modeling</source>
        <year>1988</year>
        <volume>28</volume>
        <issue>1</issue>
        <fpage>31</fpage>
        <lpage>36</lpage>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <mixed-citation publication-type="other">Swain M. PubChemPy: a way to interact with PubChem in Python. 2014.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <mixed-citation publication-type="other">Landrum G. Rdkit: Open-source cheminformatics. 2006. <ext-link ext-link-type="uri" xlink:href="https://pubchempy.readthedocs.io/en/latest/">https://pubchempy.readthedocs.io/en/latest/</ext-link>. Accessed 4 Mar 2012.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Galar</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Fernandez</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Barrenechea</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Bustince</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Herrera</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches</article-title>
        <source>IEEE Trans Syst Man Cybern Part C (Appl Rev)</source>
        <year>2012</year>
        <volume>42</volume>
        <issue>4</issue>
        <fpage>463</fpage>
        <lpage>84</lpage>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ramsundar</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Feinberg</surname>
            <given-names>EN</given-names>
          </name>
          <name>
            <surname>Gomes</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Geniesse</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Pappu</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Leswing</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Pande</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Moleculenet: a benchmark for molecular machine learning</article-title>
        <source>Chem Sci</source>
        <year>2018</year>
        <volume>9</volume>
        <issue>2</issue>
        <fpage>513</fpage>
        <lpage>30</lpage>
        <?supplied-pmid 29629118?>
        <pub-id pub-id-type="pmid">29629118</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wan</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>KK</given-names>
          </name>
        </person-group>
        <article-title>A novel hierarchical selective ensemble classifier with bioinformatics application</article-title>
        <source>Artif Intell Med</source>
        <year>2017</year>
        <volume>83</volume>
        <fpage>82</fpage>
        <lpage>90</lpage>
        <?supplied-pmid 28245947?>
        <pub-id pub-id-type="pmid">28245947</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>M-W</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>C-W</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>W-C</given-names>
          </name>
          <name>
            <surname>Ke</surname>
            <given-names>S-W</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>C-F</given-names>
          </name>
        </person-group>
        <article-title>Svm and svm ensembles in breast cancer prediction</article-title>
        <source>PloS ONE</source>
        <year>2017</year>
        <volume>12</volume>
        <issue>1</issue>
        <fpage>0161501</fpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>A deep learning-based multi-model ensemble method for cancer prediction</article-title>
        <source>Comput Methods Prog Biomed</source>
        <year>2018</year>
        <volume>153</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rokach</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Ensemble-based classifiers</article-title>
        <source>Artif Intell Rev</source>
        <year>2010</year>
        <volume>33</volume>
        <issue>1-2</issue>
        <fpage>1</fpage>
        <lpage>39</lpage>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tumer</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Error correlation and error reduction in ensemble classifiers</article-title>
        <source>Connect Sci</source>
        <year>1996</year>
        <volume>8</volume>
        <issue>3-4</issue>
        <fpage>385</fpage>
        <lpage>404</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <mixed-citation publication-type="other">Krogh A, Vedelsby J. Neural network ensembles, cross validation, and active learning. In: NIPS: 1995. p. 231–8.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <mixed-citation publication-type="other">Hu X. Using rough sets theory and database operations to construct a good ensemble of classifiers for data mining applications. In: Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference On. IEEE: 2001. p. 233–40. 10.1109/icdm.2001.989524.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Bagging predictors</article-title>
        <source>Mach Learn</source>
        <year>1996</year>
        <volume>24</volume>
        <issue>2</issue>
        <fpage>123</fpage>
        <lpage>40</lpage>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Learn</source>
        <year>2001</year>
        <volume>45</volume>
        <issue>1</issue>
        <fpage>5</fpage>
        <lpage>32</lpage>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Vapnik</surname>
            <given-names>Vladimir N.</given-names>
          </name>
        </person-group>
        <source>The Nature of Statistical Learning Theory</source>
        <year>2000</year>
        <publisher-loc>New York, NY</publisher-loc>
        <publisher-name>Springer New York</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <article-title>Greedy function approximation: a gradient boosting machine</article-title>
        <source>Ann Stat</source>
        <year>2001</year>
        <volume>29</volume>
        <fpage>1189</fpage>
        <lpage>232</lpage>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wolpert</surname>
            <given-names>DH</given-names>
          </name>
        </person-group>
        <article-title>Stacked generalization</article-title>
        <source>Neural Netw</source>
        <year>1992</year>
        <volume>5</volume>
        <issue>2</issue>
        <fpage>241</fpage>
        <lpage>59</lpage>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cereto-Massagué</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ojeda</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Valls</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Mulero</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Garcia-Vallvé</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Pujadas</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Molecular fingerprint similarity search in virtual screening</article-title>
        <source>Methods</source>
        <year>2015</year>
        <volume>71</volume>
        <fpage>58</fpage>
        <lpage>63</lpage>
        <?supplied-pmid 25132639?>
        <pub-id pub-id-type="pmid">25132639</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <mixed-citation publication-type="other">Kingma D, Ba J. Adam: A method for stochastic optimization. arXiv preprint. 2014. arXiv:1412.6980.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Winter</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Montanari</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Noé</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Clevert</surname>
            <given-names>D-A</given-names>
          </name>
        </person-group>
        <article-title>Learning continuous and data-driven molecular descriptors by translating equivalent chemical representations</article-title>
        <source>Chem Sci</source>
        <year>2019</year>
        <volume>10</volume>
        <issue>6</issue>
        <fpage>1692</fpage>
        <lpage>701</lpage>
        <?supplied-pmid 30842833?>
        <pub-id pub-id-type="pmid">30842833</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peric</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Sierra</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Martí</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Cruañas</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Garau</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Quantitative structure–activity relationship (qsar) prediction of (eco) toxicity of short aliphatic protic ionic liquids</article-title>
        <source>Ecotoxicol Environ Saf</source>
        <year>2015</year>
        <volume>115</volume>
        <fpage>257</fpage>
        <lpage>62</lpage>
        <?supplied-pmid 25728357?>
        <pub-id pub-id-type="pmid">25728357</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choi</surname>
            <given-names>J-S</given-names>
          </name>
          <name>
            <surname>Ha</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Trinh</surname>
            <given-names>TX</given-names>
          </name>
          <name>
            <surname>Yoon</surname>
            <given-names>TH</given-names>
          </name>
          <name>
            <surname>Byun</surname>
            <given-names>H-G</given-names>
          </name>
        </person-group>
        <article-title>Towards a generalized toxicity prediction model for oxide nanomaterials using integrated data from different sources</article-title>
        <source>Sci Rep</source>
        <year>2018</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>6110</fpage>
        <?supplied-pmid 29666463?>
        <pub-id pub-id-type="pmid">29666463</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
