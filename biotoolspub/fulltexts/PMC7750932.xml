<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7750932</article-id>
    <article-id pub-id-type="pmid">32592464</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa593</article-id>
    <article-id pub-id-type="publisher-id">btaa593</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Systems Biology</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Multiple kernel learning for integrative consensus clustering of omic datasets</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-1605-652X</contrib-id>
        <name>
          <surname>Cabassi</surname>
          <given-names>Alessandra</given-names>
        </name>
        <aff>
          <institution>MRC Biostatistics Unit, University of Cambridge, Cambridge CB2 0SR, UK</institution>
        </aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5931-7489</contrib-id>
        <name>
          <surname>Kirk</surname>
          <given-names>Paul D W</given-names>
        </name>
        <xref rid="btaa593-cor1" ref-type="corresp"/>
        <aff>
          <institution>MRC Biostatistics Unit, University of Cambridge, Cambridge CB2 0SR, UK</institution>
        </aff>
        <aff>
          <institution>Cambridge Institute of Therapeutic Immunology &amp; Infectious Disease, University of Cambridge, Cambridge CB2 0AW, UK</institution>
        </aff>
        <!--paul.kirk@mrc-bsu.cam.ac.uk-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Xu</surname>
          <given-names>Jinbo</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btaa593-cor1">To whom correspondence should be addressed. E-mail: <email>paul.kirk@mrc-bsu.cam.ac.uk</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>9</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-06-27">
      <day>27</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <volume>36</volume>
    <issue>18</issue>
    <fpage>4789</fpage>
    <lpage>4796</lpage>
    <history>
      <date date-type="received">
        <day>01</day>
        <month>3</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>18</day>
        <month>5</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>19</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa593.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Diverse applications—particularly in tumour subtyping—have demonstrated the importance of integrative clustering techniques for combining information from multiple data sources. Cluster Of Clusters Analysis (COCA) is one such approach that has been widely applied in the context of tumour subtyping. However, the properties of COCA have never been systematically explored, and its robustness to the inclusion of noisy datasets is unclear.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We rigorously benchmark COCA, and present Kernel Learning Integrative Clustering (KLIC) as an alternative strategy. KLIC frames the challenge of combining clustering structures as a multiple kernel learning problem, in which different datasets each provide a <italic toggle="yes">weighted</italic> contribution to the final clustering. This allows the contribution of noisy datasets to be down-weighted relative to more informative datasets. We compare the performances of KLIC and COCA in a variety of situations through simulation studies. We also present the output of KLIC and COCA in real data applications to cancer subtyping and transcriptional module discovery.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>R packages <italic toggle="yes">klic</italic> and <italic toggle="yes">coca</italic> are available on the Comprehensive R Archive Network.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>UK Medical Research Council</institution>
          </institution-wrap>
        </funding-source>
        <award-id>MC_UU_00002/10</award-id>
        <award-id>MC_UU_00002/13</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institute for Health Research</institution>
            <institution-id institution-id-type="DOI">10.13039/501100000272</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Cambridge Biomedical Research Centre at the Cambridge University Hospitals NHS Foundation Trust</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NHS</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>NIHR</institution>
            <institution-id institution-id-type="DOI">10.13039/100006662</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Department of Health and Social Care)]</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>RESCUER</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>European Union's Horizon 2020</institution>
          </institution-wrap>
        </funding-source>
        <award-id>847912</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Thanks to technological advances, both the availability and the diversity of omic datasets have hugely increased in recent years (<xref rid="btaa593-B23" ref-type="bibr">Manzoni <italic toggle="yes">et al.</italic>, 2018</xref>). These datasets provide information on multiple levels of biological systems, going from the genomic and epigenomic level, to gene and protein expression level, up to the metabolomic level, accompanied by phenotype information. Many publications have highlighted the importance of integrating information from diverse omic datasets in order to provide novel biomedical insight. For example, numerous studies by The Cancer Genome Atlas (TCGA) consortium have demonstrated the value of combining multiple omic datasets in order to define cancer subtypes (see e.g. <xref rid="btaa593-B40" ref-type="bibr">The Cancer Genome Atlas Research Network, 2011</xref>, <xref rid="btaa593-B41" ref-type="bibr">2012</xref>).</p>
    <p>Many existing statistical and computational tools have been applied to this problem and many others have been developed specifically for this. One of the first statistical methods applied to integrative clustering for cancer subtypes was <italic toggle="yes">iCluster</italic> (<xref rid="btaa593-B36" ref-type="bibr">Shen <italic toggle="yes">et al.</italic>, 2009</xref>, <xref rid="btaa593-B37" ref-type="bibr">2013</xref>). iCluster finds a partitioning of the tumours into different subtypes by projecting the available datasets onto a common latent space, maximizing the correlation between data types. Another statistical method for integrative clustering is <italic toggle="yes">Multiple Dataset Integration</italic> (MDI; see <xref rid="btaa593-B17" ref-type="bibr">Kirk <italic toggle="yes">et al.</italic>, 2012</xref>; <xref rid="btaa593-B24" ref-type="bibr">Mason <italic toggle="yes">et al.</italic>, 2016</xref>). It is based on Dirichlet-multinomial mixture models in which the allocation of observations to clusters in one dataset influences the allocation of observations in another, while allowing different datasets to have different numbers of clusters. Similarly, <italic toggle="yes">Bayesian Consensus Clustering</italic> (BCC) is based on a Dirichlet mixture model that assigns a different probability model to each dataset. Again, samples belong to different partitions, each given by a different data type, but here they also adhere loosely to an overall clustering (<xref rid="btaa593-B22" ref-type="bibr">Lock and Dunson, 2013</xref>). More recently, <xref rid="btaa593-B7" ref-type="bibr">Gabasová <italic toggle="yes">et al.</italic> (2017b</xref>) developed <italic toggle="yes">Clusternomics</italic>, a mixture model over all possible combinations of cluster assignments on the level of individual datasets that allows to model different degrees of dependence between clusters across datasets.</p>
    <p>Integrative clustering methods can be broadly classified as either <italic toggle="yes">joint modelling</italic> or <italic toggle="yes">two-step</italic> approaches. The former simultaneously consider all datasets together (e.g. MDI or BCC). The latter, which we consider here, are composed of two steps: first, the clustering structure in each dataset is analyzed independently; then an integration step is performed to find a common clustering structure that combines the individual ones. These approaches have sometimes also been referred to as <italic toggle="yes">sequential analysis</italic> methods (<xref rid="btaa593-B18" ref-type="bibr">Kristensen <italic toggle="yes">et al.</italic>, 2014</xref>).</p>
    <p>Cluster Of Clusters Analysis (COCA) is a particular two-step approach, which has grown in popularity since its first introduction in <xref rid="btaa593-B41" ref-type="bibr">The Cancer Genome Atlas Research Network (2012)</xref>. As we explain in Section 2.1, COCA proceeds by first clustering each of the datasets separately, and then building a binary matrix that encodes the cluster allocations of each observation in each dataset. This binary matrix is then used as the input to a CC algorithm (<xref rid="btaa593-B27" ref-type="bibr">Monti <italic toggle="yes">et al.</italic>, 2003</xref>; <xref rid="btaa593-B44" ref-type="bibr">Wilkerson and Hayes, 2010</xref>), which returns a single, global clustering structure, together with an assessment of its stability. The idea is that this global clustering structure both combines and summarizes the clustering structures of the individual datasets. Despite its widespread use, to the best of our knowledge the COCA algorithm has never previously been systematically explored. In what follows, we elucidate the algorithm underlying COCA, and highlight some of its limitations. We show that one key limitation is that the combination of the clustering structures from each dataset is <italic toggle="yes">unweighted</italic>, making the output of the algorithm sensitive to the inclusion of poor quality datasets.</p>
    <p>An alternative class of approaches for integrating multiple omic datasets is provided by those based on <italic toggle="yes">kernel methods</italic> (see, among others, <xref rid="btaa593-B20" ref-type="bibr">Lanckriet <italic toggle="yes">et al.</italic>, 2004b</xref>; <xref rid="btaa593-B21" ref-type="bibr">Lewis <italic toggle="yes">et al.</italic>, 2006</xref>, for ‘omic dataset’ applications). In these, a kernel function (which defines similarities between different units of observation) is associated with each dataset. These may be straightforwardly combined in order to define an overall similarity between different units of observation, which incorporates similarity information from each dataset. Determining an optimal (weighted) combination of kernels is known as <italic toggle="yes">multiple kernel learning</italic> (MKL); see, e.g. <xref rid="btaa593-B2" ref-type="bibr">Bach <italic toggle="yes">et al.</italic> (2004)</xref>, <xref rid="btaa593-B9" ref-type="bibr">Gönen and Alpayd (2011)</xref>, <xref rid="btaa593-B19" ref-type="bibr">Lanckriet <italic toggle="yes">et al.</italic> (2004a</xref>), <xref rid="btaa593-B39" ref-type="bibr">Strauß <italic toggle="yes">et al.</italic> (2019)</xref>, <xref rid="btaa593-B43" ref-type="bibr">Wang <italic toggle="yes">et al.</italic> (2017)</xref>, <xref rid="btaa593-B47" ref-type="bibr">Yu <italic toggle="yes">et al.</italic> (2010)</xref>. A challenge associated with these approaches is how best to define the kernel function(s), for which there may be many choices.</p>
    <p>Here, we combine ideas from COCA and MKL in order to propose a new Kernel Learning Integrative Clustering (KLIC) method that addresses the limitations of COCA (Section 2.2). Key to our approach is the result that the <italic toggle="yes">consensus matrix</italic> returned by CC is a valid kernel matrix (Section 2.2.3). This insight allows us to make use of the full range of MKL approaches in order to combine consensus matrices derived from different omic datasets. We perform simulation studies to illustrate our proposed approach and compare it with COCA. Finally, we show how KLIC and COCA compare in two practical applications: multiplatform tumour subtyping, where the goal is to stratify patients, and transcriptional module discovery, where genes are the statistical observations that we want to cluster.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Cluster of clusters analysis</title>
      <p><italic toggle="yes">COCA</italic> (<xref rid="btaa593-B41" ref-type="bibr">The Cancer Genome Atlas Research Network, 2012</xref>) is an integrative clustering method that was first introduced in a breast cancer study by <xref rid="btaa593-B41" ref-type="bibr">The Cancer Genome Atlas Research Network (2012)</xref> and quickly became a popular tool in cancer studies (see e.g. <xref rid="btaa593-B1" ref-type="bibr">Aure <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btaa593-B14" ref-type="bibr">Hoadley <italic toggle="yes">et al.</italic>, 2014</xref>). It makes use of <italic toggle="yes">CC</italic> (<xref rid="btaa593-B27" ref-type="bibr">Monti <italic toggle="yes">et al.</italic>, 2003</xref>), an algorithm that was originally developed to assess the stability of the clusters obtained with any clustering algorithm.</p>
      <sec>
        <label>2.1.1</label>
        <title>Consensus clustering</title>
        <p>We recall here the main features of CC in order to be able to explain the functioning of COCA. As originally formulated, CC is an approach for assessing the robustness of the clustering structure present in a single dataset (<xref rid="btaa593-B27" ref-type="bibr">Monti <italic toggle="yes">et al.</italic>, 2003</xref>; <xref rid="btaa593-B44" ref-type="bibr">Wilkerson and Hayes, 2010</xref>). The idea behind CC is that, by re-sampling multiple times the items that we want to cluster and then applying the same clustering algorithm to each of the subsets of items, we assess the robustness of the clustering structure that the algorithm detects, both to perturbations of the data and (where relevant) to the stochasticity of the clustering algorithm. To do this, CC makes use of the concepts of co-clustering matrix and consensus matrix, which we recall here.</p>
        <p>Given a set of items <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> that we seek to cluster and a clustering <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> such that <italic toggle="yes">c<sub>i</sub></italic> is the label of the cluster to which item <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> has been assigned, the corresponding <italic toggle="yes">co-clustering matrix</italic> (or <italic toggle="yes">connectivity matrix</italic>) is an <italic toggle="yes">N </italic>×<italic toggle="yes"> N</italic> matrix <italic toggle="yes">C</italic> such that the <italic toggle="yes">ij</italic>th element <italic toggle="yes">C<sub>ij</sub></italic> is equal to one if <italic toggle="yes">c<sub>i</sub></italic> = <italic toggle="yes">c<sub>j</sub></italic>, and zero otherwise. Let <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>H</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> be a list of perturbed datasets obtained by re-sampling subsets of items and/or covariates from the original dataset <italic toggle="yes">X</italic>. If <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the subset of the indices of the observations <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> present in <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, then the co-clustering matrix has <italic toggle="yes">ij</italic>-th element equal to one if <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">c<sub>i</sub></italic> = <italic toggle="yes">c<sub>j</sub></italic>, zero otherwise. We denote by <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> the co-clustering matrix corresponding to dataset <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> where the items have been assigned to <italic toggle="yes">K</italic> classes using a clustering algorithm.</p>
        <p>The <italic toggle="yes">consensus matrix</italic> <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>Δ</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is an <italic toggle="yes">N </italic>×<italic toggle="yes"> N</italic> matrix with elements
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>Δ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mi>min</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="double-struck">I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> if both items <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic> are present in dataset <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p>
        <p>Thus, CC performs multiple runs of a (stochastic) clustering algorithm (e.g. <italic toggle="yes">k</italic>-means, hierarchical clustering etc.) to assess the stability of the discovered clusters, with the consensus matrix providing a convenient summary of the CC analysis. If all the elements of the consensus matrix are close to either one or zero, this means that every pair of items is either almost always assigned to the same cluster, or almost always assigned to different clusters. Therefore, consensus matrices with all the elements close to either zero or one indicate stable clusters. In the framework of CC, these matrices can also be used to determine the number of clusters, by computing and comparing the consensus matrices <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mo>Δ</mml:mo></mml:mrow><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> for a range of numbers of clusters <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">K</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> of interest and then pick the value of <italic toggle="yes">K</italic> that gives the consensus matrix with the greater proportion of elements close to either zero or one (<xref rid="btaa593-B27" ref-type="bibr">Monti <italic toggle="yes">et al.</italic>, 2003</xref>).</p>
      </sec>
      <sec>
        <label>2.1.2</label>
        <title>Cluster Of Clusters Analysis</title>
        <p>In contrast to CC (which we emphasize is concerned with assessing clustering stability when analyzing a single dataset), the main goal of COCA is to summarize the clusterings found in <italic toggle="yes">different</italic> omic datasets, by identifying a ‘global’ clustering across the datasets that is intended to summarize the clustering structures identified in each of the individual datasets. In the first step, a clustering <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is produced independently for each dataset <italic toggle="yes">X<sub>m</sub></italic>, <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>, each with a different number of clusters <italic toggle="yes">K<sub>m</sub></italic>. We define <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. Then, the clusters are summarized into a Matrix Of Clusters (MOC) of size <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, with elements:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>MOC</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mo> </mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>otherwise</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where by <italic toggle="yes">m<sub>k</sub></italic> we denote the <italic toggle="yes">k</italic>th cluster in dataset <italic toggle="yes">m</italic>, <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>. The MOC matrix is then used as input to CC together with a fixed global number of clusters <italic toggle="yes">K</italic>. The resulting consensus matrix is then used as the similarity matrix for a hierarchical clustering method (or any other distance-based clustering algorithm).</p>
        <p>The global number of clusters <italic toggle="yes">K</italic> is not always known. In <xref rid="btaa593-B41" ref-type="bibr">The Cancer Genome Atlas Research Network (2012)</xref>, where COCA was introduced, the global number of clusters was chosen as in <xref rid="btaa593-B27" ref-type="bibr">Monti <italic toggle="yes">et al.</italic> (2003)</xref>, as explained above: CC was performed with different values of <italic toggle="yes">K</italic> and then the one that gave the ‘best’ consensus matrices were considered. Instead, <xref rid="btaa593-B1" ref-type="bibr">Aure <italic toggle="yes">et al.</italic> (2017)</xref> suggest to choose the value of <italic toggle="yes">K</italic> that maximizes the average silhouette (<xref rid="btaa593-B31" ref-type="bibr">Rousseeuw, 1987</xref>) of the final clustering, since this was found to give more sensible results.</p>
        <p>Since the construction of the MOC matrix just requires the cluster allocations, COCA has the advantage of allowing clusterings derived from different sources to be combined, even if the original datasets are unavailable or unwieldy. However, this method is unweighted, since all the clusters found in the first step have the same influence on the final clustering. Moreover, the objective function that is optimized by the algorithm is unclear.</p>
        <p>In what follows, we describe an alternative way of performing integrative clustering that takes into account not only the clusterings in each dataset, but also the information about the similarities between items that are extracted from different types of data. Additionally, the new method allows weights to be given to each source of information, according to how useful it is for defining the final clustering.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Kernel learning integrative clustering</title>
      <p>Before introducing the new methodology, we recall the main principles behind the methods that we use to combine similarity matrices.</p>
      <sec>
        <label>2.2.1</label>
        <title>Kernel methods</title>
        <p>Using kernel methods, it is possible to model non-linear relationships between the data points with a low computational complexity, thanks to the so-called <italic toggle="yes">kernel trick.</italic> For this reason, these have been widely used to extend many traditional algorithms to the non-linear framework, such as PCA (<xref rid="btaa593-B33" ref-type="bibr">Schölkopf <italic toggle="yes">et al.</italic>, 1998</xref>), linear discriminant analysis (<xref rid="btaa593-B3" ref-type="bibr">Baudat and Anouar, 2000</xref>; <xref rid="btaa593-B25" ref-type="bibr">Mika <italic toggle="yes">et al.</italic>, 1999</xref>; <xref rid="btaa593-B30" ref-type="bibr">Roth and Steinhage, 2000</xref>) and ridge regression (<xref rid="btaa593-B5" ref-type="bibr">Friedman <italic toggle="yes">et al.</italic>, 2001</xref>; <xref rid="btaa593-B34" ref-type="bibr">Shawe-Taylor and Cristianini, 2004</xref>).</p>
        <p>A <italic toggle="yes">positive definite kernel</italic> or, more simply, a <italic toggle="yes">kernel δ</italic> is a symmetric map <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:mo>δ</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math></inline-formula> for which for all <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi></mml:mrow></mml:math></inline-formula>, the matrix Δ with entries <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>Δ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>δ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is positive semi-definite. The matrix Δ is called the <italic toggle="yes">kernel matrix</italic> or <italic toggle="yes">Gram matrix</italic>. Kernel methods proceed by embedding the observations into a higher-dimensional feature space <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mi mathvariant="script">H</mml:mi></mml:math></inline-formula> endowed with an inner product <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mo>·</mml:mo><mml:mo>,</mml:mo><mml:mo>·</mml:mo><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi mathvariant="script">H</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and induced norm <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mo>‖</mml:mo><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mi mathvariant="script">H</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, making use of a map <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:math></inline-formula>. Using Mercer’s theorem, it can be shown that for any positive semi-definite kernel function, <italic toggle="yes">δ</italic>, there exists a corresponding feature map, <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo>:</mml:mo><mml:mi mathvariant="script">X</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:math></inline-formula> (see e.g. <xref rid="btaa593-B42" ref-type="bibr">Vapnik, 1998</xref>). That is, for each kernel <italic toggle="yes">δ</italic>, there exists a feature map <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mo>ϕ</mml:mo></mml:math></inline-formula> taking value in some inner product space <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mi mathvariant="script">H</mml:mi></mml:math></inline-formula> such that <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:mo>δ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>ϕ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi mathvariant="script">H</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In practice, it is therefore often sufficient to specify a positive semi-definite kernel matrix, Δ, in order to allow us to apply kernel methods such as those presented in the following sections. For a more detailed discussion of kernel methods, see e.g. <xref rid="btaa593-B34" ref-type="bibr">Shawe-Taylor and Cristianini (2004)</xref>.</p>
      </sec>
      <sec>
        <label>2.2.2</label>
        <title>Localized multiple kernel <italic toggle="yes">k</italic>-means clustering</title>
        <p>Kernel <italic toggle="yes">k</italic>-means is a generalization of the <italic toggle="yes">k</italic>-means algorithm of <xref rid="btaa593-B38" ref-type="bibr">Steinhaus (1956)</xref> to the kernel framework (<xref rid="btaa593-B8" ref-type="bibr">Girolami, 2002</xref>). The kernel trick is used to reformulate the problem of minimizing the sum of squared distances between each point and the corresponding cluster centre (in the feature space) as a trace maximization problem that only requires knowing the Gram matrix corresponding to the kernel of interest. Optimal cluster allocations can then be efficiently determined using kernel Principal Component Analysis (PCA) . More details on kernel <italic toggle="yes">k</italic>-means can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
        <p>The clustering algorithm used here is the extension of the kernel <italic toggle="yes">k</italic>-means approach to MKL (<xref rid="btaa593-B9" ref-type="bibr">Gönen and Alpayd, 2011</xref>) with sample-specific weights (<xref rid="btaa593-B10" ref-type="bibr">Gönen and Margolin, 2014</xref>) aimed at removing sample-specific noise. We consider multiple datasets <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> each with a different mapping function <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>P</mml:mi></mml:msup><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">H</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and corresponding kernel <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>δ</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">H</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and kernel matrix Δ<sub><italic toggle="yes">m</italic></sub>. Then, if we define <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mo>Θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>′</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>′</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:mo>Θ</mml:mo><mml:mo>∈</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is a vector of kernel weights with elements <italic toggle="yes">θ<sub>im</sub></italic> such that <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>, the kernel function of this multiple feature problem is a convex sum of the single kernels:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>δ</mml:mo></mml:mrow><mml:mo>Θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mo>Θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mo>ϕ</mml:mo></mml:mrow><mml:mo>Θ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">H</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>δ</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>We denote the corresponding kernel matrix by <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>Δ</mml:mo></mml:mrow><mml:mo>Θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>. The idea of localized multiple kernel <italic toggle="yes">k</italic>-means is to replace the Gram matrix used in kernel <italic toggle="yes">k</italic>-means by this weighted matrix. The optimization strategy proposed by <xref rid="btaa593-B10" ref-type="bibr">Gönen and Margolin (2014)</xref> is based on the idea that, for some fixed vector of weights Θ, this is a standard kernel <italic toggle="yes">k</italic>-means problem. Therefore, they develop a two-step optimization strategy: (i) given a fixed vector of weights <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mo>Θ</mml:mo></mml:math></inline-formula>, solve the optimization problem as in the case of one kernel, with kernel matrix given by <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>δ</mml:mo></mml:mrow><mml:mo>Θ</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and then (ii) minimize the objective function with respect to the kernel weights, keeping the assignment variables fixed. This is a convex quadratic programming (QP) problem that can be solved with any standard QP solver up to a moderate number of kernels <italic toggle="yes">M</italic>.</p>
      </sec>
      <sec>
        <label>2.2.3</label>
        <title>Identifying consensus matrices as kernels</title>
        <p>We prove that the consensus matrices defined in Section 2.1 are positive semi-definite, and hence that they can be used as input for any kernel-based clustering method, including the integrative clustering method presented in the next section. Given any <italic toggle="yes">N </italic>×<italic toggle="yes"> N</italic> co-clustering matrix <italic toggle="yes">C</italic>, we can reorder the rows and columns to obtain a block-diagonal matrix with blocks <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>where <italic toggle="yes">K</italic> is the total number of clusters and <italic toggle="yes">J<sub>k</sub></italic> is an <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> matrix of ones, with <italic toggle="yes">n<sub>k</sub></italic> being the number of items in cluster <italic toggle="yes">k</italic>. It is straightforward to show that the eigenvalues of a block-diagonal matrix are simply the eigenvalues of its blocks. Since each block is a matrix of ones, the eigenvalues of each block are nonnegative, and so any co-clustering matrix <italic toggle="yes">C</italic> is positive semi-definite. Moreover, given any set of <italic toggle="yes">λ<sub>m</sub></italic>, <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> non-negative, and co-clustering matrices <italic toggle="yes">C<sub>m</sub></italic>, <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula>, then <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is positive semi-definite, because if <italic toggle="yes">λ</italic> is a nonnegative scalar, and <italic toggle="yes">C</italic> is positive semi-definite, then <italic toggle="yes">λC</italic> is also positive semi-definite and the sum of positive semi-definite matrices is a positive semi-definite matrix. Since every consensus matrix is of the form <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mrow><mml:mo>λ</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we can conclude that any consensus matrix is positive semi-definite.</p>
      </sec>
      <sec>
        <label>2.2.4</label>
        <title>Kernel Learning Integrative Clustering</title>
        <p>We recall from Section 2.2.1 that any positive semi-definite matrix defines a feature map <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:mo>ϕ</mml:mo><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mi>P</mml:mi></mml:msup><mml:mo>→</mml:mo><mml:mi mathvariant="script">H</mml:mi></mml:mrow></mml:math></inline-formula> and is therefore a valid kernel matrix. The integrative clustering method that we introduce here is based on the idea that we can identify the consensus matrices produced by CC as kernels. That is, one can perform CC on each dataset to produce a consensus matrix Δ<sub><italic toggle="yes">m</italic></sub> for each <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. This is a kernel <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>Δ</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where the <italic toggle="yes">ij</italic>th element corresponds to the similarity between items <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>. Therefore, these matrices Δ<sub><italic toggle="yes">m</italic></sub> can be combined through the (localized) multiple kernel <italic toggle="yes">k</italic>-means algorithm described in Section 2.2.2. This allows a weight to be obtained for each kernel, as well as a global clustering <bold><italic toggle="yes">c</italic></bold> of the items. We note that this algorithm could also be applied using more than one similarity matrix per dataset, and also using kernel matrices other than (or in addition to) consensus matrices.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Examples</title>
    <sec>
      <title>3.1 Simulated data</title>
      <p>To assess the KLIC algorithm described in Section 2.2.4 and to compare it with COCA, we perform a range of simulation studies. We generate several synthetic datasets, each composed of data belonging to six different clusters of equal size. Each dataset has total number of observations equal to 300. Each observation <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is generated from a bivariate normal with mean <italic toggle="yes">ks</italic> for each variable, where <italic toggle="yes">k</italic> denotes the cluster to which the observation belongs and <italic toggle="yes">s</italic> the separation level of the dataset. Higher values of <italic toggle="yes">s</italic> give clearer clustering structures. The variance covariance matrix is the identity matrix.</p>
      <p>We consider the following settings:
</p>
      <list list-type="order">
        <list-item>
          <p><italic toggle="yes">Similar datasets.</italic> We generate four datasets that have the same clustering structure and cluster separability <italic toggle="yes">s</italic>. We denote the datasets by A, B, C and D. The goal of this experiment is to show that using localized kernel <italic toggle="yes">k</italic>-means on multiple consensus matrices leads to better results than those obtained using just one consensus matrix. To demonstrate how we may deal with irrelevant variables, we also repeat this experiment adding to each dataset 13 variables centred at zero that have no clustering structure, i.e.
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>50</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>k</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mrow><mml:mn>13</mml:mn></mml:mrow></mml:munder><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo>∀</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">I</italic> is the 15 × 15 identity matrix.</p>
        </list-item>
        <list-item>
          <p><italic toggle="yes">Datasets with different levels of noise.</italic> In this case we utilize four datasets that have the same clustering structure, but different levels of cluster separability <italic toggle="yes">s</italic>. We denote the datasets by 0 for ‘no cluster separability’, 1 ‘low cluster separability’, 2 ‘medium cluster separability’ and 3 ‘high cluster separability’ (<xref rid="btaa593-F1" ref-type="fig">Fig. 1</xref>). We use this example to show how the weights are allocated to each consensus matrix and why it is important to assign lower weights to datasets that are noisy or not relevant.
<fig position="float" id="btaa593-F1"><label>Fig. 1.</label><caption><p>Consensus matrices of the synthetic data with different levels of noise, going from ‘no cluster separability’ to ‘high cluster separability’. Blue indicates high similarity. The colours of the bar to the right of each matrix indicate the cluster labels</p></caption><graphic xlink:href="btaa593f1" position="float"/></fig></p>
        </list-item>
      </list>
      <p>We repeat each experiment 100 times. For each synthetic dataset, we use CC to obtain the consensus matrices. For simplicity, we set <italic toggle="yes">K </italic>=<italic toggle="yes"> </italic>6. As for the clustering algorithm, we use <italic toggle="yes">k</italic>-means clustering with Euclidean distance, which we found to work well in practice. The <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> contains additional simulation settings. In particular, we consider a wide range of separability values for the setting with four similar datasets and the integration of datasets with nested clusters. Moreover, we perform a short sensitivity analysis of the choice or tuning options for the <italic toggle="yes">k</italic>-means algorithm.</p>
    </sec>
    <sec>
      <title>3.2 Multiplatform analysis of 12 cancer types</title>
      <p><xref rid="btaa593-B14" ref-type="bibr">Hoadley <italic toggle="yes">et al.</italic> (2014)</xref> performed a multiplatform integrative analysis of 3,527 tumour samples from 12 different tumour types, and used COCA to identify 11 integrated tumour subtypes. To do so, they applied different clustering algorithms to each data type separately: DNA copy number, DNA methylation, mRNA expression, microRNA expression and protein expression. They then combined the five sets of clusters obtained in this way using COCA. The final clusters are highly correlated with the tissue-of-origin of each tumour sample, but some cancer types coalesce into the same clusters. The clusters obtained in this way were shown to be prognostic and to give independent information from the tissue-of-origin.</p>
      <p>Here, we use the same data to try to replicate their analysis, and compare the clusters obtained with COCA to those obtained with KLIC. To facilitate future analyses by other researchers, we have made available our scripts for processing and analyzing these datasets using the freely available R statistical programming language (<xref rid="btaa593-B28" ref-type="bibr">R Core Team, 2020</xref>), which include scripts that seek to replicate the original analysis of <xref rid="btaa593-B14" ref-type="bibr">Hoadley <italic toggle="yes">et al.</italic> (2014)</xref>, at <ext-link xlink:href="https://github.com/acabassi/klic-pancancer-analysis" ext-link-type="uri">https://github.com/acabassi/klic-pancancer-analysis</ext-link>.</p>
    </sec>
    <sec>
      <title>3.3 Transcriptional module discovery</title>
      <p><italic toggle="yes">Transcriptional modules</italic> are groups (i.e. clusters) of genes that share a common biological function and are co-regulated by a common set of transcription factors. It has been recognized that integrative clustering methods can be useful for discovering transcriptional modules, by combining gene expression datasets with datasets that provide information about transcription factor binding (<xref rid="btaa593-B15" ref-type="bibr">Ihmels <italic toggle="yes">et al.</italic>, 2002</xref>; <xref rid="btaa593-B32" ref-type="bibr">Savage <italic toggle="yes">et al.</italic>, 2010</xref>).</p>
      <p>Here, we consider transcriptional module discovery for yeast (<italic toggle="yes">Saccharomyces cerevisiae</italic>). We integrate the expression dataset of <xref rid="btaa593-B11" ref-type="bibr">Granovskaia <italic toggle="yes">et al.</italic> (2010)</xref> that contains measurements related to 551 genes whose expression profiles have been measured at 41 different time points of the cell cycle with the ChIP-chip dataset of <xref rid="btaa593-B12" ref-type="bibr">Harbison <italic toggle="yes">et al.</italic> (2004)</xref> which provides binding information for 117 transcriptional regulators for the same genes. The latter was discretized as in <xref rid="btaa593-B17" ref-type="bibr">Kirk <italic toggle="yes">et al.</italic> (2012)</xref> and <xref rid="btaa593-B32" ref-type="bibr">Savage <italic toggle="yes">et al.</italic> (2010)</xref>.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Results</title>
    <sec>
      <title>4.1 Simulated data</title>
      <p>In Section 4.1, we apply the developed methods to the synthetic datasets. In Section 4.1.2, we compare the performances of our method for integrative clustering to COCA and other integrative clustering algorithms.</p>
      <sec>
        <label>4.1.1</label>
        <title>Kernel Learning Integrative Clustering</title>
        <p>We apply KLIC to the synthetic datasets presented in Section 3.1.</p>
        <sec>
          <title>4.1.1.1 Similar datasets</title>
          <p>First, we run the kernel <italic toggle="yes">k</italic>-means algorithm on each of the consensus matrices that have the same clustering structure and noise level. To assess the quality of the clustering, we compare the clustering found with the true one using the adjusted Rand index (ARI; <xref rid="btaa593-B29" ref-type="bibr">Rand, 1971</xref>), which is equal to one if they are equal and is equal to zero if we observe as many similarities between the two partitions of the data as it is expected by chance. Then we run KLIC on multiple datasets. In <xref rid="btaa593-F2" ref-type="fig">Figure 2</xref>, the box plots of the ARI obtained combining the four datasets together using KLIC (column ‘A + B+C + D’) and the box plots of the average weights assigned by the KLIC algorithm to the observations in each dataset are reported. We observe that as expected, combining together more datasets helps recovering the clustering structure better than just taking the matrices one at a time. This is because localized kernel <italic toggle="yes">k</italic>-means allows to give different weights to each observation. Therefore, if data point <italic toggle="yes">n</italic> is hard to classify in Dataset <italic toggle="yes">d</italic><sub>1</sub>, but not in Dataset <italic toggle="yes">d</italic><sub>2,</sub> we will have <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mo>θ</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. However, on average the weights are divided equally between the datasets. This reflects the fact that all datasets have the same dispersion and, as a consequence, they contain on average the same amount of information about the clustering structure.
</p>
          <fig position="float" id="btaa593-F2">
            <label>Fig. 2.</label>
            <caption>
              <p>Results of applying KLIC to four similar datasets. Left: ARI of KLIC applied to each dataset separately (columns ‘A’, ‘B’, ‘C’ and ‘D’) and to all four datasets together (column ‘A + B+C + D’). The ARI is higher in the last column because KLIC can combine information from all the datasets to find a global clustering. Right: kernel weights associated to each dataset, when applying KLIC to all four datasets together. The algorithm is able to recognize that each dataset contains the same amount of information regarding the global clustering, and therefore assigns on average the same weight to each dataset</p>
            </caption>
            <graphic xlink:href="btaa593f2" position="float"/>
          </fig>
        </sec>
        <sec>
          <title>4.1.1.2 Datasets with different levels of noise</title>
          <p>Here we use the datasets shown in <xref rid="btaa593-F1" ref-type="fig">Figure 1</xref>, that have the same clustering structure (six clusters of the same size each) but different levels of cluster separability. We consider four different settings, each time combining three out of the four synthetic datasets. <xref rid="btaa593-F3" ref-type="fig">Figure 3</xref> shows the box plots of the ARI obtained using kernel <italic toggle="yes">k</italic>-means on the datasets taken one at a time (columns ‘0’, ‘1’, ‘2’, ‘3’) and the ARI obtained using KLIC on each subset of datasets (columns “0 + 1 + 2”, “0 + 1 + 3”, “0 + 2 + 3”, “1 + 2 + 3”). As expected, the consensus matrices with clearer clustering structure give higher values of the ARI on average. Moreover, the ARI obtained combining three matrices with different levels of cluster separability is on average the same or higher as in the case when only the “best” matrix is considered. This is because larger weights are assigned to the datasets that have clearer clustering structure. In the bottom part of <xref rid="btaa593-F3" ref-type="fig">Figure 3</xref> are reported the box plots of the average weights given by the localized multiple kernel <italic toggle="yes">k</italic>-means to the observations in each dataset. It is easy to see that each time the matrix with best cluster separability has higher weights than the other two.
</p>
          <fig position="float" id="btaa593-F3">
            <label>Fig. 3.</label>
            <caption>
              <p>Results of applying KLIC to datasets with different levels of noise (‘0’ indicates the dataset that has no cluster separability, ‘1’ the dataset with low cluster separability, and so on). Top: ARI of KLIC applied to each dataset separately (columns ‘0’, ‘1’, ‘2’ and ‘3’) and to subsets of three of those datasets (columns ‘0 + 1 + 2’, ‘0 + 1 + 3’, ‘0 + 2 + 3’ and ‘1 + 2 + 3’). Bottom: kernel weights associated to each dataset in each of the experiments with multiple datasets, ordered by cluster separability. For example, the first subset is ‘0 + 1 + 2’ so the weights marked as ‘first’ are those assigned to dataset ‘0’, ‘second’ are those assigned to ‘1’ and so on. For each subset of datasets the weights of the noisier datasets (‘first’ and ‘second’) are lower than those of the ‘best’ dataset in the subset (‘third’). This is reflected in an increased ARI in each subset, compared with applying KLIC to those datasets separately</p>
            </caption>
            <graphic xlink:href="btaa593f3" position="float"/>
          </fig>
        </sec>
      </sec>
      <sec>
        <label>4.1.2</label>
        <title>Comparison between KLIC, COCA and other methods</title>
        <p>We compare the performance of KLIC to the one obtained using COCA, as well as to two other comparable integrative clustering algorithms for which implementations are readily available; namely, iCluster and Clusternomics. Additionally, we compare with localized multiple kernel <italic toggle="yes">k</italic>-means using standard radial basis function (RBF) kernels. We use the same synthetic datasets as in the previous section.</p>
        <p>For COCA, we use the <italic toggle="yes">k</italic>-means algorithm with Euclidean distance, fixing the number of clusters to be equal to the true one, to find the clustering labels of each dataset. Many other clustering algorithms can be used, but this is the one that gives the best results among the most common ones. To find the global clustering, we build the consensus matrices using 1000 re-samplings of the data, each time with 80% of the observations and all the features. The final clustering is done using hierarchical clustering with average linkage on the consensus matrix. The iCluster model is fitted using the tune.iCluster2 function of the R package <italic toggle="yes">iCluster</italic> (<xref rid="btaa593-B35" ref-type="bibr">Shen, 2012</xref>), with number of clusters set to six. For Clusternomics we use the contextCluster function of the R package <italic toggle="yes">clusternomics</italic>  <xref rid="btaa593-B6" ref-type="bibr">Gabasová (2017a</xref>), providing the true number of clusters both for the partial and global clusterings. To assess the impact of RBF kernel parameter choice, we consider two ways to set the free parameter, <italic toggle="yes">σ</italic>, of the kernel. In one setting we fix <italic toggle="yes">σ </italic>= 1, a common default value. In the second setting, <italic toggle="yes">σ</italic> is tuned for each dataset to maximize the average ARI between the clustering obtained with kernel <italic toggle="yes">k</italic>-means using the RBF kernel and the true clusters (more information about this procedure can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>). Although this procedure clearly could not be applied in practice (where the true clustering is unknown), it is used here to determine a putative upper bound on the performances of MKL with this kernel.</p>
        <sec>
          <title>4.1.2.1 Similar datasets</title>
          <p>We combine four datasets that have the same clustering structure and cluster separability. In <xref rid="btaa593-F4" ref-type="fig">Figure 4</xref>, the ARI of all considered methods applied to 100 sets of data of this type is shown. In the first setting, where only variables relevant for the clustering are present, the localized multiple kernel <italic toggle="yes">k</italic>-means with RBF kernel has the highest median ARI, followed by COCA and KLIC. To cluster the data that include noisy variables, we replace the <italic toggle="yes">k</italic>-means algorithm by the sparse <italic toggle="yes">k</italic>-means feature selection framework of <xref rid="btaa593-B45" ref-type="bibr">Witten and Tibshirani (2010)</xref> in COCA and KLIC, using the R package <italic toggle="yes">sparcl</italic> (<xref rid="btaa593-B46" ref-type="bibr">Witten and Tibshirani, 2018</xref>). Thanks to this, the performances of these two methods are not affected by the presence of irrelevant variables. COCA, in particular, has the highest median ARI, followed by KLIC. This shows that both methods work well in the case of multiple datasets that have the same clustering structure and level of noise and, in contrast to the four other methods considered here, can be straightforwardly modified to deal with the presence of irrelevant features.
</p>
          <fig position="float" id="btaa593-F4">
            <label>Fig. 4.</label>
            <caption>
              <p>Comparison between KLIC, COCA and other clustering algorithms. The labels ‘RBF opt.’ and ‘RBF fixed’ refer to the MKL method using an RBF kernel with either <italic toggle="yes">σ</italic> optimized or fixed at 1 (see text). Top: ARI obtained with each clustering algorithm using four datasets having the same clustering structure and cluster separability (as in <xref rid="btaa593-F2" ref-type="fig">Fig. 2</xref>). Bottom: ARI obtained with COCA and KLIC for each of the subsets of heterogeneous datasets considered in <xref rid="btaa593-F3" ref-type="fig">Figure 3</xref>. The high ARI obtained with KLIC in all settings shows the advantage of using this method, especially when some of the datasets are noisy</p>
            </caption>
            <graphic xlink:href="btaa593f4" position="float"/>
          </fig>
        </sec>
        <sec>
          <title>4.1.2.2 Datasets with different levels of noise</title>
          <p>We also compare the behaviour of all methods in the presence of multiple datasets with the same clustering structure, but different levels of cluster separability. The ARI is shown in <xref rid="btaa593-F4" ref-type="fig">Figure 4</xref>. We observe that, in each of the four simulation settings, KLIC and the optimized version of localized multiple kernel <italic toggle="yes">k</italic>-means with RBF kernel have the highest ARI scores. The reason for this is that COCA, iCluster and Clusternomics are not weighted methods, so their ability to recover the true clustering structure is decreased by adding noisy datasets. Instead, we have shown in the previous section that KLIC allows to give lower weights to the noisiest datasets, achieving better performances. We emphasize that the optimal values of the RBF parameters have been determined making use of the true cluster labels, which will not be possible in real applications. The performance achieved when the RBF kernel parameter, <italic toggle="yes">σ</italic>, is fixed to 1 may therefore be more representative of what can be achieved in practice.</p>
          <p>Overall, these comparisons suggest that KLIC may be a good default choice, since it can be run in such a way that it is robust to both the inclusion of noisy variables (via the choice of an appropriate clustering algorithm) and of noisy datasets.</p>
        </sec>
      </sec>
    </sec>
    <sec>
      <title>4.2 Multiplatform analysis of 12 cancer types</title>
      <p>The first step of the data analysis is dedicated to replicating the analysis performed by <xref rid="btaa593-B14" ref-type="bibr">Hoadley <italic toggle="yes">et al.</italic> (2014)</xref>. The DNA copy number, DNA methylation, mRNA expression, microRNA expression, and protein expression data were preprocessed in the same way as <xref rid="btaa593-B14" ref-type="bibr">Hoadley <italic toggle="yes">et al.</italic> (2014)</xref> did. We then clustered the tumour samples independently for each dataset, using the same clustering algorithm as in the original paper. We compared the clusters we obtained to those reported by <xref rid="btaa593-B14" ref-type="bibr">Hoadley <italic toggle="yes">et al.</italic> (2014)</xref> for different number of clusters, and we found that the best correspondence was given by choosing the same number of clusters as in the original paper, except for the microRNA expression data, for which we found the best number of clusters to be seven (instead of 15). <xref rid="btaa593-F5" ref-type="fig">Figure 5</xref> (left) shows the MOC matrix formed by these clusters and the resulting COCA clusters. As can be seen from the <xref rid="btaa593-F5" ref-type="fig">Figure 5</xref>, each dataset has some missing observations. The corresponding entries in the MOC matrix were set to zero. We chose the number of clusters that maximizes the silhouette, as suggested by Aure <italic toggle="yes">et al.</italic> (2017), which is 10.
</p>
      <fig position="float" id="btaa593-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>Multiplatform analysis of 12 cancer types. Left: matrix-of-clusters of the pan-cancer data, each row corresponds to a cluster in one of the dataset, and each column corresponds to a tumour sample. Coloured cells show which tumours belong to each cluster. Grey cells indicate missing observations. Centre: weighted similarity matrix. Right: Coincidence matrix comparing the clusters given by COCA and KLIC</p>
        </caption>
        <graphic xlink:href="btaa593f5" position="float"/>
      </fig>
      <p>We then applied KLIC to the preprocessed data, building one consensus matrix for each dataset, using the same clustering algorithm and number of clusters as for COCA. We assigned weight zero to every missing observation (more details on how to use KLIC with incomplete data can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>). The weighted consensus matrix is shown in <xref rid="btaa593-F5" ref-type="fig">Figure 5</xref> (centre). The weights assigned on average to the observations in each dataset are as follows: copy number 31.4%, methylation 19.2%, miRNA 17.8%, mRNA 16.4% and protein 15.2%.</p>
      <p>Similarly to what was observed by <xref rid="btaa593-B14" ref-type="bibr">Hoadley <italic toggle="yes">et al.</italic> (2014)</xref>, both the clusters obtained using COCA and KLIC correspond well with the tissue-of-origin classification of the tumours. However, there are a few differences between the two: the coincidence matrix is shown in <xref rid="btaa593-F5" ref-type="fig">Figure 5</xref> (right). Further details on how we tried to replicate the data analysis of <xref rid="btaa593-B14" ref-type="bibr">Hoadley <italic toggle="yes">et al.</italic> (2014)</xref> and how we applied KLIC to these data can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
    </sec>
    <sec>
      <title>4.3 Transcriptional module discovery</title>
      <p>We clustered the 551 genes based on the gene expression and transcription factor data using KLIC. For each dataset, the consensus matrices were obtained as explained in Section 2.1. The clustering algorithms used in this step were partitioning around medoids (PAMs; <xref rid="btaa593-B16" ref-type="bibr">Kaufman and Rousseeuw, 2009</xref>) with the correlations between data points as distances for the gene expression data and Bayesian hierarchical clustering (BHC) for the transcription factor data (<xref rid="btaa593-B4" ref-type="bibr">Cooke <italic toggle="yes">et al.</italic>, 2011</xref>; <xref rid="btaa593-B13" ref-type="bibr">Heller and Ghahramani, 2005</xref>). The consensus matrices obtained in this way were then used as input to KLIC. The algorithm was run with number of clusters ranging from 2 to 20. We found that the silhouette is maximized by setting the number of clusters to four. <xref rid="btaa593-F6" ref-type="fig">Figure 6</xref> shows the weighted kernel matrix given by KLIC where the rows and columns are sorted by final cluster. Next to it are reported the data, where the observations are in the same order as in the kernel matrix. The clusters obtained independently on each dataset are also shown on the right of each plot. The kernel matrices of each dataset can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.
</p>
      <fig position="float" id="btaa593-F6">
        <label>Fig. 6.</label>
        <caption>
          <p>Transcriptional module discovery, KLIC output. Left: weighted kernel matrix obtained with KLIC, where each row and column corresponds to a gene, and final clusters. Centre: transcription factor data, where each row represents a gene and each column a transcription factor, black dots correspond to transcription factors that are believed to be able to bind to the promoter region of the corresponding gene with high confidence; clusters obtained using BHC on the transcription factor data and weight assigned by KLIC to each data point. Right: gene expression data, where each row is a gene and each column a time point, clusters obtained using PAM on the gene expression data, and weights assigned by KLIC to each data point</p>
        </caption>
        <graphic xlink:href="btaa593f6" position="float"/>
      </fig>
      <p>We also applied COCA to this dataset, with the initial clusters for each dataset obtained with the same clustering algorithms as those used for the consensus matrices. The metrics used to choose the number of clusters for the initial clustering of the expression data are reported in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>. BHC does not require the number of clusters to be set by the user. For the final clustering the number of clusters was chosen in order to maximize the silhouette, considering all values between 2 and 10. This resulted in choosing the 10-cluster solution.</p>
      <p>In order to assess the quality of the clusters, we make use of the Gene Ontology Term Overlap (GOTO) scores of <xref rid="btaa593-B26" ref-type="bibr">Mistry and Pavlidis (2008)</xref>. Each score is an indication of the number of annotations that, on average, are shared by genes belonging to the same clusters. These are available for three different ontologies: biological process, molecular function and cellular component. More details on these scores and how they are calculated can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> of <xref rid="btaa593-B17" ref-type="bibr">Kirk <italic toggle="yes">et al.</italic> (2012)</xref>. We report in <xref rid="btaa593-T1" ref-type="table">Table 1</xref>, the GOTO scores of both KLIC and COCA clusters, for both number of clusters selected by KLIC (4) and COCA (10). We also show the scores obtained clustering each dataset separately. We observe that, while in the case of four clusters no information is lost by combining the datasets; by dividing data into 10 clusters one obtains more biologically meaningful clusters. Moreover, KLIC does a better job at combining the datasets, by better exploiting the information contained in the data and down-weighting the kernel of the ChIP dataset, which contains less information. More details about the kernel matrices and weights can be found in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.
</p>
      <table-wrap position="float" id="btaa593-T1">
        <caption>
          <p>Table 1. GOTO scores for different sets of data, clustering algorithms and numbers of clusters</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Clusters</th>
              <th rowspan="1" colspan="1">Dataset(s)</th>
              <th rowspan="1" colspan="1">Algorithm</th>
              <th rowspan="1" colspan="1">GOTO BP</th>
              <th rowspan="1" colspan="1">GOTO MF</th>
              <th rowspan="1" colspan="1">GOTO CC</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">8</td>
              <td rowspan="1" colspan="1">ChIP</td>
              <td rowspan="1" colspan="1">BHC</td>
              <td rowspan="1" colspan="1">6.09</td>
              <td rowspan="1" colspan="1">0.90</td>
              <td rowspan="1" colspan="1">8.33</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">Expression</td>
              <td rowspan="1" colspan="1">PAM</td>
              <td rowspan="1" colspan="1">6.12</td>
              <td rowspan="1" colspan="1">0.91</td>
              <td rowspan="1" colspan="1">8.41</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">ChIP + Expr.</td>
              <td rowspan="1" colspan="1">COCA</td>
              <td rowspan="1" colspan="1">6.12</td>
              <td rowspan="1" colspan="1">0.91</td>
              <td rowspan="1" colspan="1">8.41</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">4</td>
              <td rowspan="1" colspan="1">ChIP + Expr.</td>
              <td rowspan="1" colspan="1">KLIC</td>
              <td rowspan="1" colspan="1">6.12</td>
              <td rowspan="1" colspan="1">0.91</td>
              <td rowspan="1" colspan="1">8.41</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">ChIP + Expr.</td>
              <td rowspan="1" colspan="1">COCA</td>
              <td rowspan="1" colspan="1">6.28</td>
              <td rowspan="1" colspan="1">0.93</td>
              <td rowspan="1" colspan="1">8.51</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">10</td>
              <td rowspan="1" colspan="1">ChIP + Expr.</td>
              <td rowspan="1" colspan="1">KLIC</td>
              <td rowspan="1" colspan="1">6.32</td>
              <td rowspan="1" colspan="1">0.95</td>
              <td rowspan="1" colspan="1">8.53</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: ‘BP’ stands for Biological Process ontology; ‘MF’ for Molecular Function; and ‘CC’ for Cellular Component.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>5 Discussion</title>
    <p>In the first part of this work, we have given the algorithm for COCA, a widely used method in integrative clustering of genomic data, highlighting the main issues of using this method. We have also presented KLIC, a novel approach to integrative clustering, that allows multiple datasets to be combined to find a global clustering of the data and is well suited for the analysis of large datasets, such as those often encountered in genomics applications. A defining difference between KLIC and COCA is that, while COCA performs a combination of the clusters found in each dataset, KLIC uses the similarities between data points observed in each dataset to perform the integrative step. Moreover, KLIC weights each dataset individually, which allows more informative datasets to be up-weighted relative to less informative ones, as demonstrated in our simulation study. Finally, we have used KLIC to integrate multiple omic datasets, in two different real world applications, finding biologically meaningful clusters. The results compare favourably to those obtained with COCA.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the UK Medical Research Council [MC_UU_00002/10 and MC_UU_00002/13] and the National Institute for Health Research [Cambridge Biomedical Research Centre at the Cambridge University Hospitals NHS Foundation Trust (The views expressed are those of the authors and not necessarily those of the NHS, the NIHR or the Department of Health and Social Care)]. Partly funded by the RESCUER project. RESCUER has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No. 847912.</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btaa593_supplementary_data</label>
      <media xlink:href="btaa593_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa593-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Aure</surname><given-names>M.R.</given-names></string-name></person-group>  <etal>et al</etal>; OSBREAC (<year>2017</year>) 
<article-title>Integrative clustering reveals a novel split in the luminal A subtype of breast cancer with impact on outcome</article-title>. <source>Breast Cancer Res</source>., <volume>19</volume>, <fpage>44</fpage>.<pub-id pub-id-type="pmid">28356166</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bach</surname><given-names>F.R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2004</year>) 
<article-title>Multiple kernel learning, conic duality, and the SMO algorithm</article-title>. In: <italic toggle="yes">Proceedings of the 21st International Conference on Machine Learning</italic>, p. 6. ACM Press, New York.</mixed-citation>
    </ref>
    <ref id="btaa593-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baudat</surname><given-names>G.</given-names></string-name>, <string-name><surname>Anouar</surname><given-names>F.</given-names></string-name></person-group> (<year>2000</year>) 
<article-title>Generalized discriminant analysis using a kernel approach</article-title>. <source>Neural Computa</source>., <volume>12</volume>, <fpage>2385</fpage>–<lpage>2404</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cooke</surname><given-names>E.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2011</year>) 
<article-title>Bayesian hierarchical clustering for microarray time series data with replicates and outlier measurements</article-title>. <source>BMC Bioinformatics</source>, <volume>12</volume>, <fpage>399</fpage>.<pub-id pub-id-type="pmid">21995452</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Friedman</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2001</year>). <source>The Elements of Statistical Learning</source>. Vol. <volume>1</volume>. 
<publisher-name>Springer Series in Statistics, New York</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa593-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Gabasová</surname><given-names>E.</given-names></string-name></person-group> (<year>2017</year>a) <italic toggle="yes">clusternomics: Integrative Clustering for Heterogeneous Biomedical Datasets.</italic> R package version 0.1.2. Repository: CRAN. <ext-link xlink:href="https://CRAN.R-project.org/package=clusternomics" ext-link-type="uri">https://CRAN.R-project.org/package=clusternomics</ext-link>.</mixed-citation>
    </ref>
    <ref id="btaa593-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gabasová</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>b) 
<article-title>Clusternomics: integrative context-dependent clustering for heterogeneous datasets</article-title>. <source>PLoS Comput. Biol</source>., <volume>13</volume>, <fpage>e1005781</fpage>.<pub-id pub-id-type="pmid">29036190</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Girolami</surname><given-names>M.</given-names></string-name></person-group> (<year>2002</year>) 
<article-title>Mercer kernel-based clustering in feature space</article-title>. <source>IEEE Trans. Neural Netw</source>., <volume>13</volume>, <fpage>780</fpage>–<lpage>784</lpage>.<pub-id pub-id-type="pmid">18244475</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gönen</surname><given-names>M.</given-names></string-name> and <string-name><surname>Alpaydın</surname><given-names>E.</given-names></string-name></person-group> (<year>2011</year>) 
<article-title>Multiple kernel learning algorithms</article-title>. <source>J. Mach. Learn. Res</source>., <volume>12</volume>, <fpage>2211</fpage>–<lpage>2268</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gönen</surname><given-names>M.</given-names></string-name>, <string-name><surname>Margolin</surname><given-names>A.A.</given-names></string-name></person-group> (<year>2014</year>). <part-title>Localized data fusion for kernel k-means clustering with application to cancer biology.</part-title> In: <source>Advances in Neural Information Processing Systems</source>, pp. <fpage>1305</fpage>–<lpage>1313</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Granovskaia</surname><given-names>M.V.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) 
<article-title>High-resolution transcription atlas of the mitotic cell cycle in budding yeast</article-title>. <source>Genome Biol</source>., <volume>11</volume>, <fpage>R24</fpage>.<pub-id pub-id-type="pmid">20193063</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Harbison</surname><given-names>C.T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2004</year>) 
<article-title>Transcriptional regulatory code of a eucaryotic genome</article-title>. <source>Nature</source>, <volume>431</volume>, <fpage>99</fpage>–<lpage>104</lpage>.<pub-id pub-id-type="pmid">15343339</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B13">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Heller</surname><given-names>K.A.</given-names></string-name></person-group> and Ghahramani,Z. (2005) Bayesian hierarchical clustering. In: <source>Proceedings of the 22nd international conference on Machine learning</source> , pp <fpage>297</fpage>–<lpage>304</lpage>. ACM Press, New York</mixed-citation>
    </ref>
    <ref id="btaa593-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hoadley</surname><given-names>K.A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) 
<article-title>Multiplatform analysis of 12 cancer types reveals molecular classification within and across tissues of origin</article-title>. <source>Cell</source>, <volume>158</volume>, <fpage>929</fpage>–<lpage>944</lpage>.<pub-id pub-id-type="pmid">25109877</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ihmels</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2002</year>) 
<article-title>Revealing modular organization in the yeast transcriptional network</article-title>. <source>Nat. Genet</source>., <volume>31</volume>, <fpage>370</fpage>–<lpage>377</lpage>.<pub-id pub-id-type="pmid">12134151</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kaufman</surname><given-names>L.</given-names></string-name>, <string-name><surname>Rousseeuw</surname><given-names>P.J.</given-names></string-name></person-group> (<year>2009</year>). <source>Finding Groups in Data: An Introduction to Cluster Analysis</source>. Vol. <volume>344</volume>. 
<publisher-name>John Wiley &amp; Sons, Hoboken, New Jersey</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btaa593-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kirk</surname><given-names>P.D.W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) 
<article-title>Bayesian correlated clustering to integrate multiple datasets</article-title>. <source>Bioinformatics</source>, <volume>28</volume>, <fpage>3290</fpage>–<lpage>3297</lpage>.<pub-id pub-id-type="pmid">23047558</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kristensen</surname><given-names>V.N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) 
<article-title>Principles and methods of integrative genomic analyses in cancer</article-title>. <source>Nat. Rev. Cancer</source>, <volume>14</volume>, <fpage>299</fpage>–<lpage>313</lpage>.<pub-id pub-id-type="pmid">24759209</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lanckriet</surname><given-names>G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2004</year>a) 
<article-title>Learning the kernel matrix with semidefinite programming</article-title>. <source>J. Mach. Learn. Res.</source>  <volume>5</volume>, <fpage>27</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lanckriet</surname><given-names>G.R.G.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2004</year>b) 
<article-title>A statistical framework for genomic data fusion</article-title>. <source>Bioinformatics</source>, <volume>20</volume>, <fpage>2626</fpage>–<lpage>2635</lpage>.<pub-id pub-id-type="pmid">15130933</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lewis</surname><given-names>D.P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2006</year>) 
<article-title>Support vector machine learning from heterogeneous data: an empirical analysis using protein sequence and structure</article-title>. <source>Bioinformatics</source>, <volume>22</volume>, <fpage>2753</fpage>–<lpage>2760</lpage>.<pub-id pub-id-type="pmid">16966363</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lock</surname><given-names>E.F.</given-names></string-name>, <string-name><surname>Dunson</surname><given-names>D.B.</given-names></string-name></person-group> (<year>2013</year>) 
<article-title>Bayesian consensus clustering</article-title>. <source>Bioinformatics</source>, <volume>29</volume>, <fpage>2610</fpage>–<lpage>2616</lpage>.<pub-id pub-id-type="pmid">23990412</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Manzoni</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Genome, transcriptome and proteome: the rise of omics data and their integration in biomedical sciences</article-title>. <source>Brief. Bioinform</source>., <volume>19</volume>, <fpage>286</fpage>–<lpage>302</lpage>.<pub-id pub-id-type="pmid">27881428</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mason</surname><given-names>S.A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>MDI-GPU: accelerating integrative modelling for genomic-scale data using GP-GPU computing</article-title>. <source>Stat. Appl. Genet. Mol. Biol</source>., <volume>15</volume>, <fpage>83</fpage>–<lpage>86</lpage>.<pub-id pub-id-type="pmid">26910751</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B25">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mika</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>1999</year>) 
<article-title>Fisher discriminant analysis with kernels</article-title>. In: <italic toggle="yes">Neural Networks for Signal Processing IX, 1999. Proceedings of the 1999 IEEE Signal Processing Society Workshop</italic>, pp. 41–48. IEEE, New York.</mixed-citation>
    </ref>
    <ref id="btaa593-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Mistry</surname><given-names>M.</given-names></string-name>, <string-name><surname>Pavlidis</surname><given-names>P.</given-names></string-name></person-group> (<year>2008</year>) 
<article-title>Gene Ontology term overlap as a measure of gene functional similarity</article-title>. <source>BMC Bioinformatics</source>, <volume>9</volume>, <fpage>327</fpage>.<pub-id pub-id-type="pmid">18680592</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Monti</surname><given-names>S.</given-names></string-name></person-group> (<year>2003</year>) 
<article-title>Consensus clustering: a resampling-based method for class discovery and visualization of gene</article-title>. <source>Mach. Learn</source>., <volume>52</volume>, <fpage>91</fpage>–<lpage>118</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B28">
      <mixed-citation publication-type="book">R Core Team. (<year>2020</year>) <source>R: A Language and Environment for Statistical Computing</source>. 
<publisher-name>R Foundation for Statistical Computing</publisher-name>, 
<publisher-loc>Vienna, Austria</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btaa593-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rand</surname><given-names>W.M.</given-names></string-name></person-group> (<year>1971</year>) 
<article-title>Objective criteria for the evaluation of clustering methods</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>66</volume>, <fpage>846</fpage>–<lpage>850</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Roth</surname><given-names>V.</given-names></string-name>, <string-name><surname>Steinhage</surname><given-names>V.</given-names></string-name></person-group> (<year>2000</year>). <part-title>Nonlinear discriminant analysis using kernel functions.</part-title> In: <source>Advances in Neural Information Processing Systems</source>, pp. <fpage>568</fpage>–<lpage>574</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rousseeuw</surname><given-names>P.J.</given-names></string-name></person-group> (<year>1987</year>) 
<article-title>Silhouettes: a graphical aid to the interpretation and validation of cluster analysis</article-title>. <source>J. Comput. Appl. Math</source>., <volume>20</volume>, <fpage>53</fpage>–<lpage>65</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Savage</surname><given-names>R.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) 
<article-title>Discovering transcriptional modules by Bayesian data integration</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>i158</fpage>–<lpage>i167</lpage>.<pub-id pub-id-type="pmid">20529901</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schölkopf</surname><given-names>B.</given-names></string-name></person-group>  <etal>et al</etal> (<year>1998</year>) 
<article-title>Nonlinear component analysis as a kernel eigenvalue problem</article-title>. <source>Neural Comput</source>., <volume>10</volume>, <fpage>1299</fpage>–<lpage>1319</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Shawe-Taylor</surname><given-names>J.</given-names></string-name>, <string-name><surname>Cristianini</surname><given-names>N.</given-names></string-name></person-group> (<year>2004</year>) <source>Kernel Methods for Pattern Analysis</source>. 
<publisher-name>Cambridge University Press</publisher-name>, 
<publisher-loc>Cambridge</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btaa593-B35">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Shen</surname><given-names>R.</given-names></string-name></person-group> (<year>2012</year>). <italic toggle="yes">iCluster: Integrative clustering of multiple genomic data types.</italic> R package version 2.1.0. Repository: CRAN. <ext-link xlink:href="https://CRAN.R-project.org/package=iCluster" ext-link-type="uri">https://CRAN.R-project.org/package=iCluster</ext-link></mixed-citation>
    </ref>
    <ref id="btaa593-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) 
<article-title>Integrative clustering of multiple genomic data types using a joint latent variable model with application to breast and lung cancer subtype analysis</article-title>. <source>Bioinformatics</source>, <volume>25</volume>, <fpage>2906</fpage>–<lpage>2912</lpage>.<pub-id pub-id-type="pmid">19759197</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Shen</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) 
<article-title>Sparse integrative clustering of multiple omics data sets</article-title>. <source>Ann. Appl. Stat</source>., <volume>7</volume>, <fpage>269</fpage>–<lpage>294</lpage>.<pub-id pub-id-type="pmid">24587839</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Steinhaus</surname><given-names>W.H.D.</given-names></string-name></person-group> (<year>1956</year>) 
<article-title>Sur la division des corps matériels en parties</article-title>. <source>Bull. L’Acad. Polonaise Sci</source>., <volume>IV</volume>, <fpage>801</fpage>–<lpage>804</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Strauß</surname><given-names>M.E.</given-names></string-name></person-group>  <etal>et al</etal> (20<year>20</year>) 
<article-title>GPseudoClust: deconvolution of shared pseudo-trajectories at single-cell resolution</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>1484</fpage>–<lpage>1491</lpage>.<pub-id pub-id-type="pmid">31608923</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B40">
      <mixed-citation publication-type="journal">The Cancer Genome Atlas Research Network. (<year>2011</year>) 
<article-title>Integrated genomic analyses of ovarian carcinoma</article-title>. <source>Nature</source>, <volume>474</volume>, <fpage>609</fpage>.<pub-id pub-id-type="pmid">21720365</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B41">
      <mixed-citation publication-type="journal">The Cancer Genome Atlas Research Network. (<year>2012</year>) 
<article-title>Comprehensive molecular portraits of human breast tumours</article-title>. <source>Nature</source>, <volume>487</volume>, <fpage>61</fpage>–<lpage>70</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa593-B42">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Vapnik</surname><given-names>V.N.</given-names></string-name></person-group> (<year>1998</year>) <source>Statistical Learning Theory</source>. 
<publisher-name>Wiley</publisher-name>, 
<publisher-loc>New York</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btaa593-B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>B.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Visualization and analysis of single-cell RNA-seq data by kernel-based similarity learning</article-title>. <source>Nat. Methods</source>, <volume>14</volume>, <fpage>414</fpage>–<lpage>416</lpage>.<pub-id pub-id-type="pmid">28263960</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wilkerson</surname><given-names>M.D.</given-names></string-name>, <string-name><surname>Hayes</surname><given-names>D.N.</given-names></string-name></person-group> (<year>2010</year>) 
<article-title>ConsensusClusterPlus: a class discovery tool with confidence assessments and item tracking</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>1572</fpage>–<lpage>1573</lpage>.<pub-id pub-id-type="pmid">20427518</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Witten</surname><given-names>D.M.</given-names></string-name>, <string-name><surname>Tibshirani</surname><given-names>R.</given-names></string-name></person-group> (<year>2010</year>) 
<article-title>A framework for feature selection in clustering</article-title>. <source>J. Am. Stat. Assoc</source>., <volume>105</volume>, <fpage>713</fpage>–<lpage>726</lpage>.<pub-id pub-id-type="pmid">20811510</pub-id></mixed-citation>
    </ref>
    <ref id="btaa593-B46">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Witten</surname><given-names>D.M.</given-names></string-name>, <string-name><surname>Tibshirani</surname><given-names>R.</given-names></string-name></person-group> (<year>2018</year>) <italic toggle="yes">sparcl: Perform Sparse Hierarchical Clustering and Sparse K-Means Clustering.</italic> R package version 1.0.4. Repository: CRAN. <ext-link xlink:href="https://CRAN.R-project.org/package=sparcl" ext-link-type="uri">https://CRAN.R-project.org/package=sparcl</ext-link></mixed-citation>
    </ref>
    <ref id="btaa593-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yu</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) 
<article-title>L2-norm multiple kernel learning and its application to biomedical data fusion</article-title>. <source>BMC Bioinformatics</source>, <volume>11</volume>, <fpage>309</fpage>.<pub-id pub-id-type="pmid">20529363</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
