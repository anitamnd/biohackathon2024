<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Digit Imaging</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Digit Imaging</journal-id>
    <journal-title-group>
      <journal-title>Journal of Digital Imaging</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0897-1889</issn>
    <issn pub-type="epub">1618-727X</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6646456</article-id>
    <article-id pub-id-type="pmid">31089974</article-id>
    <article-id pub-id-type="publisher-id">232</article-id>
    <article-id pub-id-type="doi">10.1007/s10278-019-00232-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><italic>RIL-Contour</italic>: a Medical Imaging Dataset Annotation Tool for and with Deep Learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Philbrick</surname>
          <given-names>Kenneth A.</given-names>
        </name>
        <address>
          <email>Philbrick.Kenneth@mayo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Weston</surname>
          <given-names>Alexander D.</given-names>
        </name>
        <address>
          <email>Weston.Alexander@mayo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Akkus</surname>
          <given-names>Zeynettin</given-names>
        </name>
        <address>
          <email>Akkus.Zeynettin@mayo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kline</surname>
          <given-names>Timothy L.</given-names>
        </name>
        <address>
          <email>Kline.Timothy@mayo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Korfiatis</surname>
          <given-names>Panagiotis</given-names>
        </name>
        <address>
          <email>Korfiatis.Panagiotis@mayo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sakinis</surname>
          <given-names>Tomas</given-names>
        </name>
        <address>
          <email>Sakinis.Tomas@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kostandy</surname>
          <given-names>Petro</given-names>
        </name>
        <address>
          <email>KostandyPetro@mayo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Boonrod</surname>
          <given-names>Arunnit</given-names>
        </name>
        <address>
          <email>Boonrod.Arunnit@mayo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zeinoddini</surname>
          <given-names>Atefeh</given-names>
        </name>
        <address>
          <email>Zeinoddini.Atefeh@mayo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Takahashi</surname>
          <given-names>Naoki</given-names>
        </name>
        <address>
          <email>Takahashi.Naoki@mayo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Erickson</surname>
          <given-names>Bradley J.</given-names>
        </name>
        <address>
          <email>bje@mayo.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0459 167X</institution-id><institution-id institution-id-type="GRID">grid.66875.3a</institution-id><institution>Radiology Informatics Laboratory, Department of Radiology, </institution><institution>Mayo Clinic, </institution></institution-wrap>Rochester, MN USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0389 8485</institution-id><institution-id institution-id-type="GRID">grid.55325.34</institution-id><institution>Oslo University Hospital, </institution></institution-wrap>Oslo, Norway </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0470 0856</institution-id><institution-id institution-id-type="GRID">grid.9786.0</institution-id><institution>Radiology Department, </institution><institution>Khon Kaen University, </institution></institution-wrap>Khon Kaen, 40002 Thailand </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>14</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <volume>32</volume>
    <issue>4</issue>
    <fpage>571</fpage>
    <lpage>581</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Deep-learning algorithms typically fall within the domain of supervised artificial intelligence and are designed to “learn” from annotated data. Deep-learning models require large, diverse training datasets for optimal model convergence. The effort to curate these datasets is widely regarded as a barrier to the development of deep-learning systems. We developed <italic>RIL-Contour</italic> to accelerate medical image annotation for and with deep-learning. A major goal driving the development of the software was to create an environment which enables clinically oriented users to utilize deep-learning models to rapidly annotate medical imaging. <italic>RIL-Contour</italic> supports using fully automated deep-learning methods, semi-automated methods, and manual methods to annotate medical imaging with voxel and/or text annotations. To reduce annotation error, <italic>RIL-Contour</italic> promotes the standardization of image annotations across a dataset. <italic>RIL-Contour</italic> accelerates medical imaging annotation through the process of annotation by iterative deep learning (AID). The underlying concept of AID is to iteratively annotate, train, and utilize deep-learning models during the process of dataset annotation and model development. To enable this, <italic>RIL-Contour</italic> supports workflows in which multiple-image analysts annotate medical images, radiologists approve the annotations, and data scientists utilize these annotations to train deep-learning models. To automate the feedback loop between data scientists and image analysts, <italic>RIL-Contour</italic> provides mechanisms to enable data scientists to push deep newly trained deep-learning models to other users of the software. <italic>RIL-Contour</italic> and the AID methodology accelerate dataset annotation and model development by facilitating rapid collaboration between analysts, radiologists, and engineers.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Deep-learning</kwd>
      <kwd>Medical image annotation</kwd>
      <kwd>Annotation by iterative deep learing (AID)</kwd>
      <kwd>Segmentation</kwd>
      <kwd>Classification</kwd>
      <kwd>Software tools</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Society for Imaging Informatics in Medicine 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Deep-learning algorithms typically fall within the domain of supervised artificial intelligence and are designed to “learn” from annotated data [<xref ref-type="bibr" rid="CR1">1</xref>]. Deep-learning models require large, diverse training datasets for optimal model convergence. The ImageNet dataset used to train powerful general-purpose deep-learning image classifiers contains millions of unique images each annotated to describe the objects contained within the image [<xref ref-type="bibr" rid="CR2">2</xref>]. While usually smaller, datasets used to train powerful medical image classifiers typically contain hundreds-to-thousands of annotated images [<xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR7">7</xref>]. The effort required to curate these training datasets is widely regarded as a major barrier to the development of deep-learning systems.</p>
    <p id="Par3">Numerous software tools have been developed to annotate medical imaging [<xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR18">18</xref>]. These tools commonly provide manual, semi-automated, and fully automated methods to label imaging. Semi-automated methods typically utilize traditional image processing techniques such as thresholding or edge detection [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]. Fully automated methods are typically built upon semi-automated techniques and human-designed algorithms which encode domain-specific knowledge [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]. Development of these algorithms is time consuming and the computational time associated with running many of them can be substantial.</p>
    <p id="Par4">Deep-learning algorithms “learn” to identify objects of interest in imaging data [<xref ref-type="bibr" rid="CR1">1</xref>]. Utilizing deep-learning–based approaches for medical imaging annotation does not require the development of traditional human engineered algorithms. In many cases, deep-learning approaches to image analysis have been found to meet or exceed the performance of traditional algorithms [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>]. The computational time required to perform inference utilizing deep-learning models is often lower than traditional approaches. This suggests that implementing a deep-learning–based approach for dataset annotation may meet or exceed the performance of traditional human-designed annotation algorithms.</p>
    <p id="Par5">Medical image annotation software often does not provide tools that standardize the annotations used across datasets. Many annotation tools create annotations on an ad hoc basis. These tools place the burden of maintaining consistency in annotation labels on the analyst and have inspired efforts to standardize annotation lexicon [<xref ref-type="bibr" rid="CR22">22</xref>]. Errors or variability in data annotation increases the size of the dataset required for deep-learning model convergence to a “correct” generalizable solution [<xref ref-type="bibr" rid="CR23">23</xref>]. Errors specifically in the definition of the test dataset make it difficult to determine “true” model performance as model divergence from the test dataset may be appropriate.</p>
    <p id="Par6">Once created, annotation metadata must be associated in some fashion with the original imaging. Errors here result in annotation data loss and/or dataset corruption. The Digital Imaging and Communications in Medicine (DICOM) standard provides one solution to these challenges by enabling annotation metadata to be non-destructively embedded directly within medical imaging. This, however, alters the imaging files and can complicate using the same imaging for multiple annotation projects. Alternatively, if annotation data are not embedded within imaging then annotation metadata must be saved and associated in some fashion with the original imaging. Content management systems have historically provided a partial solution to these data management challenges. These systems provide database-like mechanisms to store and manage imaging and its associated metadata [<xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR25">25</xref>]. However, annotation tools are usually stand-alone and not well integrated with content management systems. This lack of integration complicates workflows by requiring the image analyst to manage the movement of data between the content management system and the annotation software. The addition of these workflow steps results in the inability to guarantee that annotation metadata is correctly captured by a content management system.</p>
  </sec>
  <sec id="Sec2">
    <title>Software Overview</title>
    <p id="Par7">We developed <italic>Radiology Informatics Laboratory Contour (RIL-Contour)</italic> to accelerate medical image annotation for and with deep learning. A major goal driving the development of the software was to create an environment which enables clinically oriented users to focus on annotating imaging datasets using deep-learning methods and not on the underlying challenges associated with data transformation or management. Unlike annotation tools designed to annotate single images, <italic>RIL-Contour</italic> facilitates the consistent annotation of large medical imaging datasets required for developing deep-learning models and promotes collaborative dataset annotation by supporting concurrent multiuser workflows.</p>
    <p id="Par8"><italic>RIL-Contour</italic> defines voxel and imaging annotation definitions at the “dataset level” to enforce consistency of annotation definitions across all images in a dataset. This is similar to the concept of annotation template definitions used in other software [<xref ref-type="bibr" rid="CR11">11</xref>]. <italic>RIL-Contour</italic> supports the use of deep-learning models to automatically perform voxel and text annotation of imaging. Additionally, <italic>RIL-Contour</italic> provides mechanisms to perform advanced deep-learning model visualization to aid image analysts and data scientists in understanding deep-learning models and provides methods to automate quantification of Dice and Jaccard metrics for deep-learning segmentation models.</p>
    <p id="Par9"><italic>RIL-Contour</italic> stores annotation metadata independently from imaging to enable imaging to be used in multiple annotation projects and to guarantee that the act of annotation does not alter image data files. <italic>RIL-Contour</italic> manages the storage of annotation metadata. While not common, other annotation tools provide similar functionality [<xref ref-type="bibr" rid="CR11">11</xref>]. For datasets stored on a file system, <italic>RIL-Contour</italic> automatically maintains the file association between annotation metadata and imaging. Alternatively, <italic>RIL-Contour</italic> can be linked with a Medical Imaging Research Management and Associated Information Database (MIRMAID) content management system [<xref ref-type="bibr" rid="CR24">24</xref>]. In this later configuration, <italic>RIL-Contour</italic> will silently retrieve imaging on demand and push and pull annotation metadata to and from the content management system.</p>
    <sec id="Sec3">
      <title>RIL-Contour User Interface</title>
      <p id="Par10">Upon loading, <italic>RIL-Conto</italic>ur presents two windows, the dataset project viewer (Fig. <xref rid="Fig1" ref-type="fig">1(a)</xref>) and the dataset annotation window (Fig. <xref rid="Fig1" ref-type="fig">1(b)</xref>). The dataset project viewer displays a list of the imaging files associated with a project. The project viewer is designed to simplify the complexity of working with large datasets. From the user’s perspective, the dataset project viewer displays files in a hierarchy which mirror the datasets storage on the file system or for a content-managed dataset in a DICOM-inspired Patient → Study → Series hierarchy. The menus shown on the dataset project viewer window broadly provide access to dialogs which control project-wide settings (e.g., annotation definitions) and dialogs that perform operations across the project’s dataset (e.g., exporting data). Series which have been edited are bolded, allowing the user to quickly identify annotated imaging, and textual annotations associated with imaging can be shown as optional columns.<fig id="Fig1"><label>Fig. 1</label><caption><p>Screenshot of (<bold>a</bold>) dataset project viewer and (<bold>b</bold>) dataset annotation window. The dataset project viewer (<bold>a</bold>) shows a representative imaging project consisting of multiple data series, each consisting of one or more imaging exams, each further consisting of imaging series datasets. Imaging with annotation data is bolded and the selected series is shown in blue. The dataset annotation window (<bold>b</bold>) shows the currently selected dataset and the selected dataset’s annotations. Dataset slices in reference to the primary annotation view are shown in the slice viewer. Slice voxel annotations are indicated by the presence of one or more colors within the slice indicator. Voxel annotation along the axis perpendicular to the annotation view is shown to the right. The axis shown in the annotation view defaults to the projection with the greatest in slice voxel resolution and can be manually selected using the orientation view projection drop down.</p></caption><graphic xlink:href="10278_2019_232_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par11">The dataset annotation window is the primary interface through which annotation is performed (Fig. <xref rid="Fig1" ref-type="fig">1(b)</xref>). <italic>RIL-Contour</italic> supports voxel annotations to define regions of interest (ROI) within images and text annotations to describe location-independent features or observations (e.g., image quality, presence of features, comments). <italic>RIL-Contour</italic> supports associating voxel ROI annotations with RadLex identification numbers (RadLex ID) to enable ROI definitions to be associated with a universally identifiable numerical nomenclature [<xref ref-type="bibr" rid="CR22">22</xref>]. All dataset annotation operations are saved automatically as they are performed. For file system–based projects, the software supports versioning image annotations and supports common related versioning operations (e.g., viewing a version change history and rolling back to a previous version). To enable multiple users to utilize the same source imaging for independent annotation projects, <italic>RIL-Contour</italic> supports saving annotation data in an independent location on the file system or within a MIRMAID content management system [<xref ref-type="bibr" rid="CR24">24</xref>].</p>
    </sec>
    <sec id="Sec4">
      <title>Voxel Annotations</title>
      <p id="Par12"><italic>RIL-Contour</italic> supports “area” and “point” voxel annotations to define ROIs within images (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). Area annotations describe multi-voxel patches that can be used to either train an algorithm for segmentation or for classification. These annotations are often defined on multiple slices, and thus can represent multi-slice volumes. Point annotations describe the location of point(s) of interest within a series and can be used to define anatomical locations within a series or specifying the presence or absence of feature(s) within a slice. Descriptive statistics for a selected annotation can be shown through the statistics window (Fig. <xref rid="Fig3" ref-type="fig">3</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><p>Screenshot of (<bold>a</bold>) ROI manager dialog window and (<bold>b</bold>) ROI editor dialog window. All existing ROIs defined for a project are shown in the project ROI editor window. ROI editor (<bold>b</bold>)—the editor window allows the user to change the name, RadLex ID, and color for any ROI.</p></caption><graphic xlink:href="10278_2019_232_Fig2_HTML" id="MO2"/></fig><fig id="Fig3"><label>Fig. 3</label><caption><p>Statistics window displays descriptive statistics for the selected ROI with reference to the entire volume or selected slice.</p></caption><graphic xlink:href="10278_2019_232_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par13">Manual definition of ROI is performed using the voxel annotation tools and filters. These tools and filters support common drawing operations (e.g., painting, erasing, filling, dilation, erosion, and undo/redo). <italic>RIL-Contour</italic> supports labeling voxels with multiple annotation labels, e.g., a voxel could be annotated as both kidney and tumor. Alternatively, the software can be set to enforce a one ROI per voxel mapping; e.g., a voxel could be annotated as a kidney or tumor but not both (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). All manual segmentation tools support threshold-based application to selectively perform the desired annotation operation on voxels within a defined value range. The paint brush and eraser tools support cross-slice painting to automatically extend the operation to a predetermined number of adjacent slices. The histogram shown on the statistics window (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) can be useful in determining the threshold range exhibited by a partially annotated tissue. The combination of threshold-based painting and multi-slice painting facilitates rapid manual segmentation of tissues which exhibit values which strongly differentiate them from surrounding structures. Finally, all ROI annotations support locking to prevent the ROI from being modified using manual, semi-automated, and fully automated deep-learning techniques.</p>
      <p id="Par14"><italic>RIL-Contour</italic> supports semi-automated ROI generation and edge refinement using the Minimal Interaction Rapid Organ Segmentation (MIROS) algorithm [<xref ref-type="bibr" rid="CR26">26</xref>]. This algorithm was developed to refine the boundary of high-contrast organs (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). Semi-automated edge refinement can be performed for a single slice using the “Snap Contour” and for multiple slices using the “Auto-Contouring” or “Batch Contouring” user interfaces (Fig. <xref rid="Fig1" ref-type="fig">1(b)</xref>). Slice segmentations generated wholly using semi-automated methods are illustrated within the slice viewer by a lighter version of the ROI’s color to differentiate them from user-edited annotations (Fig. <xref rid="Fig1" ref-type="fig">1(b)</xref>).<fig id="Fig4"><label>Fig. 4</label><caption><p>Example of semi-automated edge refinement of the kidney; (<bold>a</bold>) manual segmentation; (<bold>b</bold>) segmentation following semi-automated edge refinement.</p></caption><graphic xlink:href="10278_2019_232_Fig4_HTML" id="MO4"/></fig></p>
    </sec>
    <sec id="Sec5">
      <title>Text Annotations</title>
      <p id="Par15"><italic>RIL-Contour</italic> supports descriptive text annotations to capture non-voxel-based observations. Text annotations can be restricted to a predefined set of values to standardize annotations. All text annotations can be shown as optional columns in the dataset project viewer to identify images in the dataset containing the text annotation.</p>
    </sec>
    <sec id="Sec6">
      <title>Import and Exporting Annotations</title>
      <p id="Par16"><italic>RIL-Contour</italic> supports importing and exporting ROI voxel annotation data to and from binary file masks. To define multiple overlapping ROI in a single binary voxel mask, files can be written out as the “binary or” of the overlapping ROI mask values. For masks exported to the file system, annotation masks are written in a hierarchy that mirrors the original dataset. Masks exported to the file system are accompanied by a data file describing the binary mask, e.g., mapping between the ROI mask value and a RadLex ID. Copies of the original input imaging and original <italic>RIL-Contour</italic> annotation data file can optionally also be written out. For content-managed workflows, binary masks can be exported back into a MIRMAID content management system or to the file system. Alternatively, descriptive statistics of voxel annotations and tables listing the text annotations associated with imaging can be exported in Excel (Microsoft, Redmond, WA) or comma-separated value (CSV) format.</p>
    </sec>
    <sec id="Sec7">
      <title>Concurrent User Annotation and Multiuser Workflows</title>
      <p id="Par17"><italic>RIL-Contour</italic> supports concurrent dataset annotation by multiple users. For datasets stored in a MIRMAID content management system, <italic>RIL-Contour</italic> utilizes locking mechanisms to enable multiple users to concurrently annotate independent imaging series. For datasets stored on the file system, <italic>RIL-Contour</italic> supports series locking and additionally supports multiuser workflows which define series-specific user-level rights to generate annotations for imaging and define the set of other software users to which a user can assign image annotation rights to. These workflows are designed to enable multiple people to work concurrently to annotate, review, and utilize the data for machine-learning purposes. Figure <xref rid="Fig5" ref-type="fig">5</xref> illustrates an example annotation workflow in which multiple-image analysts generate annotations, the generated annotations are reviewed, and the resulting annotations are used by data scientists to train a deep-learning model. To support these workflows, <italic>RIL-Contour</italic> automatically versions series annotations when series ownership changes. <italic>RIL-Contour</italic> multiuser workflows are described in a YAML file which can be optionally embedded within a <italic>RIL-Contour</italic> project description file or specified at run time through a command line option.<fig id="Fig5"><label>Fig. 5</label><caption><p>Example collaborative multiuser annotation workflow illustrating the controlled annotation of an individual series (red) by multiple users. Unannotated series assigned to analyst group at the start of the project. Analysts acquire unannotated series for annotation from the analyst pool. Analysts can (<bold>a</bold>) return partially annotated series to the Analyst’s pool for further editing by other analysts or (<bold>b</bold>) assign the annotated series to Reviewer’s pool; series can no longer be acquired by an analyst. (<bold>c)</bold> Reviewer 1 acquires the series from the Reviewer’s pool, if the annotations look correct, (<bold>d</bold>) reviewer assigns image to Scientist pool. Alternatively, not pictured, if the reviewer deemed the annotations poor, they could have re-assigned the series back to the analysts pool or to a specific analyst. (<bold>e)</bold> Scientists use available curated dataset to train deep-learning model. (<bold>f)</bold> Trained deep-learning model pushed to analysts to perform draft dataset annotation as an example of implementing the AID dataset annotation methodology.</p></caption><graphic xlink:href="10278_2019_232_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec8">
      <title>Deep-Learning Powered Annotation</title>
      <p id="Par18"><italic>RIL-Contour</italic> supports utilizing trained deep-learning models to perform fully automated annotation. <italic>RIL-Contour</italic> utilizes a “no-coding” plugin architecture to make it relatively simple to deploy deep-learning models in the software. The plugin interface is designed to run deep-learning models developed in Keras running on Tensorflow. The plugin execution framework instantiates models on demand. The time required to load a model is related to the model complexity. Once loaded, the computational costs associated with model inference for most models are typically low enough that models can be run on a standard modern CPU.</p>
      <p id="Par19"><italic>RIL-Contour</italic> defines the preprocessing operations (e.g., normalization, mapping model output to annotation settings) required for model inference in metadata which it stores alongside an HDF5 file that describes the model’s weights and optionally architecture. To enable model metadata to be defined with little-to-no coding, <italic>RIL-Contour</italic> provides a model creation wizard that resides inside of a model manager dialog that steps users through the definition of the requisite metadata (Fig. <xref rid="Fig6" ref-type="fig">6</xref>).<fig id="Fig6"><label>Fig. 6</label><caption><p>Importing a deep-learning model into <italic>RIL-Contour</italic>. Metadata required to load ML model is defined in the model wizard; (<bold>a</bold>) defines model name and loads model and weights (HDF5 file) and optionally defines custom python model loading code; (<bold>b</bold>) defines affine transformations required to transform slice input into the model; (<bold>c</bold>) defines image normalization to perform prior to model execution; and (<bold>d</bold>) links model output with custom <italic>RIL-Contour</italic> annotations.</p></caption><graphic xlink:href="10278_2019_232_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par20">The <italic>RIL-Contour</italic> model manager supports model versioning and model sharing. Model versioning is designed to enable models to be easily updated with a new set of learned weights and/or architecture while maintaining a history of prior model configurations. The software supports importing and exporting models with their definition metadata and has functions to automate model discovery to enable models to be automatically imported into the software as they are made available. This feature has been designed to enable data scientists to “push” new and updated deep-learning models to other users of the software (Fig. <xref rid="Fig5" ref-type="fig">5</xref>).</p>
    </sec>
    <sec id="Sec9">
      <title>Understanding Model Inference</title>
      <p id="Par21"><italic>RIL-Contour</italic> supports the interactive generation of visualizations which identify the regions of images that models identify when performing prediction (Fig. <xref rid="Fig7" ref-type="fig">7</xref>) [<xref ref-type="bibr" rid="CR4">4</xref>]. The software supports a variety of state-of-the-art visualization approaches such as saliency maps, class activation maps (CAM), gradient class activation maps (Grad-CAM), and saliency activation maps (SAM) [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR27">27</xref>–<xref ref-type="bibr" rid="CR30">30</xref>]. These techniques allow analysts without a data science background to quickly and intuitively understand the regions of an image that a deep-learning model responds to. Each of the model visualization techniques employed within <italic>RIL-Contour</italic> generates an “activation” metrics for each voxel. To enable users to rapidly focus on meaningful regions of activation, <italic>RIL-Contour</italic> performs automatic thresholding to hide low-intensity background activations; this setting can also be dynamically adjusted by the analyst.<fig id="Fig7"><label>Fig. 7</label><caption><p>Representative interactive model visualizations generated in <italic>RIL-Contour</italic> illustrating the regions of an image that the model strongly activated on when performing inference; (<bold>a</bold>) saliency activation map (SAM); (<bold>b</bold>) saliency map visualizations of a deep-learning model designed to classify CT contrast enhancement. Visualizations shown using a rainbow color pallet; red = high, purple = low. This visualization indicates that portions of the left and right kidney are being used by the model to identify the imaging’s renal contrast enhancement phase.</p></caption><graphic xlink:href="10278_2019_232_Fig7_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec10">
      <title>Deep-Learning Model Segmentation Model Metrics</title>
      <p id="Par22"><italic>RIL-Contour</italic> supports automated quantification of Dice and Jacard segmentation metrics between a deep-learning model’s predictions and image segmentations defined in the software. Metrics are computed on a per-slice basis for slices selected in the software. Slice segmentations metrics are summarized as volume segmentation metrics.</p>
    </sec>
    <sec id="Sec11">
      <title>Annotation by Iterative Deep Learning</title>
      <p id="Par23">The time required to curate large datasets is a major roadblock to developing novel deep-learning models. <italic>RIL-Contour</italic> can accelerate data annotation through the process of annotation by iterative deep learning (AID). AID accelerates dataset annotation by utilizing deep-learning models to generate draft annotations. AID is based on the observation that it is typically faster for humans to edit or correct a good-but-not-perfect image annotation than to generate one entirely from scratch.</p>
      <p id="Par24">Using the AID process, dataset annotation begins with an entirely unannotated dataset. From this, a small subset of the data is selected and annotated using traditional methods. This initial annotated dataset is then used to train a “development” deep-learning model to perform the desired annotation. This “development” model is then deployed from within <italic>RIL-Contour</italic> to generate draft annotations for the next set of training data. The newly created draft annotations are then corrected as necessary from within the <italic>RIL-Contour</italic> and the now expanded annotated dataset is exported from the software and used to train the next “development” model. This process is repeated iteratively until the entire dataset is annotated or until a model is created with sufficient accuracy that further iteration is no longer required. The AID methodology is illustrated in Figs. <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig8" ref-type="fig">8</xref>. Conceptually, AID is described as a cycle. However, given sufficient human resources, model development and dataset annotation can be conducted concurrently (Fig. <xref rid="Fig5" ref-type="fig">5</xref>); new models can be developed as new data becomes available and deep-learning annotation models can be utilized as they are created.<fig id="Fig8"><label>Fig. 8</label><caption><p>Annotation by iterative deep learning (AID).</p></caption><graphic xlink:href="10278_2019_232_Fig8_HTML" id="MO8"/></fig></p>
      <p id="Par25">We have utilized <italic>RIL-Contour</italic> for multiple annotation projects. These projects have used the software for annotation of MRI, CT, and US imaging collected at the head, chest, and abdomen to generate annotations of brain, abdominal organs, tumors, and other tissues and to generate annotations that categorically classify the presence or absence of tumors in imaging or the contrast enhancement phase of a CT series. To date, we have used RIL-Contour to perform data annotation for over 12 projects. We report several case studies to illustrate how <italic>RIL-Contour</italic> can be used to accelerate medical image annotation.</p>
      <p id="Par26">Our largest project to date involves segmenting 35 unique organs and tissues in CT volumes of the abdomen. Project staff consists of 17 image analysts, 5 radiologists, and 3 data scientists who coordinate solely through <italic>RIL-Contour</italic>. Qualitatively, AID methodology greatly decreased the human time required to annotate new series for this project. Initially, starting from minimal base annotations, annotators required approximately 40 h to fully segment the abdominal organs in a series. At present, we have 99 annotated volumes annotated. The AID methodology has decreased average volume annotation time to approximal 8 h per series, 80% reduction in annotation time.</p>
      <p id="Par27">In another example, we created a novel dataset to train a deep-learning model to locate the vertebral bodies. Seven analysts utilized the software to define the desired anatomy. The entire project, which involved segmentation of 132 cases, took less than a week from conception to successful conclusion.</p>
      <p id="Par28">In another example, we utilized <italic>RIL-Contour</italic> to categorically annotate the contrast enhancement phase of abdominal CT imaging. Annotations were generated by 3 radiologists. Three thousand images were annotated. These annotations were used to train a contrast enhancement prediction model [<xref ref-type="bibr" rid="CR4">4</xref>]. A <italic>RIL-Contour</italic> plugin for this model is shared on GitLab (see “<xref rid="Sec14" ref-type="sec">Software Availability</xref>”).</p>
      <p id="Par29">We have found <italic>RIL-Contour</italic> to be a useful tool for deploying deep-learning models to collaborators who may have little-to-no experience with machine learning. In a recent example, we utilized <italic>RIL-Contour</italic> to correlate body composition, in particular visceral adiposity, with waist-hip measurements taken at our clinic. <italic>RIL-Contour’s</italic> no-coding interface allowed our collaborator, who had no experience coding, to utilize deep-learning models to perform automated segmentation after an hour of training.</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Discussion</title>
    <p id="Par30">The development of deep-learning models for medical imaging typically requires the annotation of hundreds-to-thousands of images [<xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR7">7</xref>]. This process is time consuming and potentially error prone. Software tools which facilitate rapid accurate image annotation and annotation review are needed to accelerate the development of deep-learning datasets and models.</p>
    <p id="Par31"><italic>RIL-Contour</italic> has been designed with the goals of accelerating the annotation of medical imaging for deep learning. <italic>RIL-Contour</italic> contour accomplishes this by (1) providing a tool that simplifies the challenges of working with large imagining datasets in a collaborative research environment, (2) by providing a tool that enables deep-learning models to be utilized directly from within the software to perform fully automated annotation, and (3) by providing a tool that facilitates the visualization of and understanding of deep-learning models.</p>
    <p id="Par32">Variability or errors in dataset annotation increase the size of the training dataset required for accurate deep-learning model convergence [<xref ref-type="bibr" rid="CR23">23</xref>]. A strategy utilized by other medical imaging software has been to standardize definition of annotations across the images in a dataset using templates [<xref ref-type="bibr" rid="CR11">11</xref>]. <italic>RIL-Contour</italic> adopts a similar strategy to ensure consistency in the definition of annotations in a dataset. This design paradigm guarantees that a given ROI will have the same name, RadLex ID, and voxel mask value for all images in a <italic>RIL-Contour</italic> dataset and that text annotations will fall within a predefined set of values.</p>
    <p id="Par33">Few medical imaging research annotation tools are designed to manage the association between imaging and annotation metadata when the metadata is not stored directly within the source imaging. A notable expectation is the work of Rubin et al. [<xref ref-type="bibr" rid="CR11">11</xref>]. Content management systems such as <italic>MIRMAID</italic> and Extensible Neuroimaging Archive Toolkit (<italic>XNAT)</italic> provide systems to accomplish this [<xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR25">25</xref>]. However, in working with most annotation software, these systems typically require the data analyst to manually move data between annotation software and the content management system. These additional steps add workflow complexity and are potentially error prone. <italic>RIL-Contour</italic> provides a mechanism to manage the association between imaging data and annotation metadata for datasets stored on the file system or within a MIRMAID content management system. These interfaces are designed to minimize workflow complexity and empower the data analyst to focus on data annotation and review and not on the management of imaging and metadata.</p>
    <p id="Par34"><italic>RIL-Contour</italic> is designed to simplify the application of deep-learning models for the purposes of medical image annotation. <italic>RIL-Contour</italic> utilizes a plugin engine to load and run deep-learning models at run time. The <italic>RIL-Contour</italic> engine supports models developed in <italic>Keras</italic> running on <italic>Tensorflow</italic>. Future support for additional platforms is planned. To execute a model, the plugin engine loads the model at run time, from source or an HDF5 file, normalizes and transforms the input imaging to match the model’s requirements, runs the model, and, for segmentations, transforms the model output into <italic>RIL-Contour</italic> voxel annotations. The plugin engine enables <italic>RIL-Contour</italic> to interact directly with models. This allows <italic>RIL-Contour</italic> to provide a graphical user interface (GUI) model definition wizard which walks users through the process of importing a deep-learning model based, in part, on the underlying architecture of the model and enables the software to provide model visualization features which rely on the ability to rewrite a model and compute the output and gradient of arbitrary model layers.</p>
    <p id="Par35">To our knowledge, <italic>DeepInfer</italic> is the only other medical image annotation tool developed to facilitate automated image annotation using deep learning [<xref ref-type="bibr" rid="CR31">31</xref>]. <italic>DeepInfer</italic> is a <italic>3D Slicer</italic> plugin which enables <italic>3D Slicer</italic> to utilize deep-learning models to perform fully automated image annotation [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR31">31</xref>]. In terms of functionality, <italic>RIL-Contour</italic> and <italic>DeepInfer</italic> both automate the application of deep-learning models for the purposes of data annotation. <italic>DeepInfer</italic> utilizes a Docker-based execution engine to run deep-learning models. Due to its Docker-based design, <italic>DeepInfer</italic> does not directly interact with models and as a result cannot directly perform the model modifications required for the generation of advanced visualizations.</p>
    <p id="Par36">The <italic>RIL-Contour</italic> plugin interface currently supports two-dimensional models and patch-wise application of three-dimensional models for segmentation or classification. Support for whole volume three-dimensional models is planned. The generation of CAM visualizations requires CAM-specific model architecture, within network SAM and Grad-CAM layer visualizations are supported for both convolutional and activation layers with non-linear activation functions [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR30">30</xref>].</p>
    <p id="Par37">The effort required to curate training datasets for deep learning is widely regarded as a major barrier to the development of deep-learning models. Numerous groups have attempted to accelerate machine-learning model training through processes designed to optimize the creation of training datasets [<xref ref-type="bibr" rid="CR32">32</xref>–<xref ref-type="bibr" rid="CR34">34</xref>]. Deep-learning methods have been proposed to accelerate interactive segmentation and to propagate segmentations across slices [<xref ref-type="bibr" rid="CR35">35</xref>]. Other techniques, auto-annotation and pseudo-annotation, utilize multiple instance learning to automatically identify meaningful annotations from a set of predetermined noisy labels; labels that both correctly and incorrectly label data [<xref ref-type="bibr" rid="CR36">36</xref>–<xref ref-type="bibr" rid="CR38">38</xref>].</p>
    <p id="Par38">Here, we propose the AID methodology to accelerate human-driven data annotation of medical imaging. AID is an example of how artificial intelligence can be used to augment and accelerate human performance while retaining human supervision. AID methodology is similar to a classification-based annotation system described for natural world images [<xref ref-type="bibr" rid="CR32">32</xref>]. The underlying premise behind AID is that a machine-learning model can be used during the construction of a supervised training dataset and that the amount of human correction required following application of a model will be approximately proportional with the overall size and diversity of the model's training dataset. <italic>RIL-Contour</italic> is designed to facilitate AID by (1) enabling deep-learning models to be applied to annotation images from within the software, (2) by providing mechanism from within the software to edit deep-learning derived annotations, (3) by providing a mechanism to export data to promote rapid model training, (4) by supporting concurrent workflows, and (5) by providing mechanisms which automate the sharing of deep-learning models between users of the software.</p>
    <p id="Par39">A limitation of <italic>RIL-Contour</italic> is the software has been designed to facilitate annotation of imaging stored in the Neuroimaging Informatics Technology Initiative (NIfTI) file format [<xref ref-type="bibr" rid="CR39">39</xref>]. There are numerous tools (e.g., dcm2niix, MRIConvert) which can be used to convert DICOM imaging to the NIfTI file format. The NIfTI file format is a simpler format than the DICOM file format [<xref ref-type="bibr" rid="CR39">39</xref>]. The NIfTI file format has been designed to encapsulate multi-dimensional imaging data within a single file. At present, there is a well-developed Python API to reliably read and write the file format, there are a number of medical imaging tools which read and write the format, and the format is extensively utilized within medical imaging research community [<xref ref-type="bibr" rid="CR8">8</xref>–<xref ref-type="bibr" rid="CR10">10</xref>, <xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR39">39</xref>]. A major limitation of the NIfTI file format is that it fails to capture much of the metadata commonly stored within DICOM files. To overcome this limitation, <italic>RIL-Contour</italic> supports the association of additional imaging metadata as a secondary CSV file and supports reading and writing this additional metadata from a MIRMAID content management system [<xref ref-type="bibr" rid="CR24">24</xref>]. A focus of future development efforts is to add support in RIL-Contour to natively support datasets stored in DICOM.</p>
    <p id="Par40"><italic>RIL-Contour</italic> exports annotated voxel data as NIfTI files aligned to match the orientation and alignment of the source imaging. Additional non-imaging metadata is exported as tabular data in CSV and Excel format. These representations are programmatically convenient to work with. However, they do not facilitate broad data interoperability. The DICOM file format is capable of describing both imaging and metadata (contours, points, binary masks, and non-imaging data). The DICOM format is fully capable of encapsulating the metadata generated using RIL-Contour. A focus of future development efforts is to add support in <italic>RIL-Contour</italic> to export annotated datasets in the DICOM format to facilitate the utilization of <italic>RIL-Contour</italic> annotated datasets in other software packages.</p>
  </sec>
  <sec id="Sec13">
    <title>Conclusion</title>
    <p id="Par41">Deep-learning models are widely believed to require large training datasets for generalizable model convergence. The time required to annotate such datasets is a major barrier to the development of these models. We have developed the software <italic>RIL-Contour</italic> to accelerate medical imaging dataset annotation for deep learning. <italic>RIL-Contour</italic> provides annotation mechanisms designed to standardize annotation definitions and provides tools to easily apply deep-learning models to perform fully automated text and voxel annotation. <italic>RIL-Contour</italic> supports collaborative workflows and has been designed to accelerate annotation through the process of AID—a process through which deep-learning models are iteratively trained and utilized to generate draft annotation for a dataset that can then be edited as necessary.</p>
  </sec>
  <sec id="Sec14">
    <title>Software Availability</title>
    <p id="Par42">The source code for <italic>RIL-Contour</italic> and example deep-learning model plugins trained to identify the renal contrast enhancement phase of CT imaging and to perform patch-based kidney segmentation are publicly available on Gitlab at (<ext-link ext-link-type="uri" xlink:href="https://gitlab.com/Philbrick/rilcontour">https://gitlab.com/Philbrick/rilcontour</ext-link>). The software is distributed under a BSD style license. The software is provided “as is” and is intended for research purposes only. The software is installable using the Anaconda 3.6 package manager. License and installation instructions are available on Gitlab. The software is written in Python and utilizes common libraries for core functionality. Utilization of the machine-learning interface requires the additional installation of the OpenCV, Keras, and Tensorflow packages. The software is designed to work with data stored in the NIfTI format. Supplemental Python code has been published in the Gitlab archive demonstrating the use of dcm2nii to convert DICOM datasets to NIfTI. The <italic>RIL-Contour</italic> is broadly compatible with Python 2.7+ and Python 3.6+. Interaction with a MIRMAID content management system requires Python 2.7.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <p>The following image analysts have made a significant contribution to one or more annotation projects utilizing <italic>RIL-Contour</italic>: Lindsey Anding, Cailin Austin, Isabel Bazley, Lucas Betts, Clare Buntrock, Margaret Cantlon, Nicholas DeBlois, Lauren Karras, Rachel Marks, Kristina Monson, Sallie Perkins, Cole Rokke, Jill Tryon, Bailey Ullom, Angela Weiler, and Paul Weishaar.</p>
  </ack>
  <notes>
    <title>Compliance with Ethical Standards</title>
    <notes notes-type="COI-statement">
      <title>Conflict of Interest</title>
      <p id="Par43">The authors declare that they have no conflict of interest.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <fpage>436</fpage>
        <lpage>444</lpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Russakovsky</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Su</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Krause</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Satheesh</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Karpathy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Khosla</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bernstein</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Berg</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Fei-Fei</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>ImageNet large scale visual recognition challenge</article-title>
        <source>Int J Comput Vis</source>
        <year>2015</year>
        <volume>115</volume>
        <issue>3</issue>
        <fpage>211</fpage>
        <lpage>252</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Weston AD, et al: Automated abdominal segmentation of CT scans for body composition analysis using deep learning<italic>.</italic> Radiology 181432, 2018</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Philbrick</surname>
            <given-names>KA</given-names>
          </name>
          <name>
            <surname>Yoshida</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Inoue</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Akkus</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Kline</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Korfiatis</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Takahashi</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Erickson</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>What does deep learning see? Insights from a classifier trained to predict contrast enhancement phase from CT images</article-title>
        <source>Am J Roentgenol</source>
        <year>2018</year>
        <volume>211</volume>
        <issue>6</issue>
        <fpage>1184</fpage>
        <lpage>1193</lpage>
        <pub-id pub-id-type="doi">10.2214/AJR.18.20331</pub-id>
        <pub-id pub-id-type="pmid">30403527</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Korfiatis</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kline</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Lachance</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Parney</surname>
            <given-names>IF</given-names>
          </name>
          <name>
            <surname>Buckner</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Erickson</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>Residual deep convolutional neural network predicts MGMT methylation status</article-title>
        <source>J Digit Imaging</source>
        <year>2017</year>
        <volume>30</volume>
        <fpage>622</fpage>
        <lpage>628</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-017-0009-z</pub-id>
        <pub-id pub-id-type="pmid">28785873</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Akkus</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Ali</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Sedlář</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Predicting deletion of chromosomal arms 1p/19q in low-grade gliomas from MR images using machine intelligence</article-title>
        <source>J Digit Imaging</source>
        <year>2017</year>
        <volume>30</volume>
        <fpage>469</fpage>
        <lpage>476</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-017-9984-3</pub-id>
        <pub-id pub-id-type="pmid">28600641</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Rajpurkar P, et al: Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning<italic>.</italic> arXiv preprint arXiv:1711.05225, 2017</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rueden</surname>
            <given-names>CT</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>ImageJ2: ImageJ for the next generation of scientific image data</article-title>
        <source>BMC Bioinformatics</source>
        <year>2017</year>
        <volume>18</volume>
        <issue>1</issue>
        <fpage>529</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1934-z</pub-id>
        <pub-id pub-id-type="pmid">29187165</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kikinis</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pieper</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Vosburgh</surname>
            <given-names>KG</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Jolesz</surname>
            <given-names>FA</given-names>
          </name>
        </person-group>
        <article-title>3D Slicer: A platform for subject-specific image analysis, visualization, and clinical support</article-title>
        <source>Intraoperative Imaging and Image-Guided Therapy</source>
        <year>2014</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer New York</publisher-name>
        <fpage>277</fpage>
        <lpage>289</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kline</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Edwards</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Korfiatis</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Akkus</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Torres</surname>
            <given-names>VE</given-names>
          </name>
          <name>
            <surname>Erickson</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>Semiautomated segmentation of polycystic kidneys in T2-weighted MR images</article-title>
        <source>Am J Roentgenol</source>
        <year>2016</year>
        <volume>207</volume>
        <issue>3</issue>
        <fpage>605</fpage>
        <lpage>613</lpage>
        <pub-id pub-id-type="doi">10.2214/AJR.15.15875</pub-id>
        <pub-id pub-id-type="pmid">27341140</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rubin</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Willrett</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>O’Connor</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Hage</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kurtz</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Moreira</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <article-title>Automated tracking of quantitative assessments of tumor burden in clinical trials</article-title>
        <source>Transl Oncol</source>
        <year>2014</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>23</fpage>
        <lpage>35</lpage>
        <pub-id pub-id-type="doi">10.1593/tlo.13796</pub-id>
        <pub-id pub-id-type="pmid">24772204</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yushkevich</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Piven</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hazlett</surname>
            <given-names>HC</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>RG</given-names>
          </name>
          <name>
            <surname>Ho</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Gerig</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability</article-title>
        <source>NeuroImage</source>
        <year>2006</year>
        <volume>31</volume>
        <issue>3</issue>
        <fpage>1116</fpage>
        <lpage>1128</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.015</pub-id>
        <pub-id pub-id-type="pmid">16545965</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fischl</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>FreeSurfer</article-title>
        <source>NeuroImage</source>
        <year>2012</year>
        <volume>62</volume>
        <issue>2</issue>
        <fpage>774</fpage>
        <lpage>781</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id>
        <pub-id pub-id-type="pmid">22248573</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Papademetris</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>BioImage Suite: An integrated medical image analysis suite: An update</article-title>
        <source>Insight J</source>
        <year>2006</year>
        <volume>2006</volume>
        <fpage>209</fpage>
        <lpage>209</lpage>
        <pub-id pub-id-type="pmid">25364771</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>van Zijl</surname>
            <given-names>PCM</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Pearlson</surname>
            <given-names>GD</given-names>
          </name>
          <name>
            <surname>Mori</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>DtiStudio: Resource program for diffusion tensor computation and fiber bundle tracking</article-title>
        <source>Comput Methods Prog Biomed</source>
        <year>2006</year>
        <volume>81</volume>
        <issue>2</issue>
        <fpage>106</fpage>
        <lpage>116</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2005.08.004</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <mixed-citation publication-type="other">McAuliffe MJ, et al: Medical image processing, analysis and visualization in clinical research. In: Proceedings 14th IEEE Symposium on Computer-Based Medical Systems. CBMS, 2001</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jenkinson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Beckmann</surname>
            <given-names>CF</given-names>
          </name>
          <name>
            <surname>Behrens</surname>
            <given-names>TEJ</given-names>
          </name>
          <name>
            <surname>Woolrich</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>SM</given-names>
          </name>
        </person-group>
        <article-title>FSL</article-title>
        <source>NeuroImage</source>
        <year>2012</year>
        <volume>62</volume>
        <issue>2</issue>
        <fpage>782</fpage>
        <lpage>790</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.015</pub-id>
        <pub-id pub-id-type="pmid">21979382</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Takahashi</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Sugimoto</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Psutka</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Moynagh</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Carter</surname>
            <given-names>RE</given-names>
          </name>
        </person-group>
        <article-title>Validation study of a new semi-automated software program for CT body composition analysis</article-title>
        <source>Abdom Radiol</source>
        <year>2017</year>
        <volume>42</volume>
        <issue>9</issue>
        <fpage>2369</fpage>
        <lpage>2375</lpage>
        <pub-id pub-id-type="doi">10.1007/s00261-017-1123-6</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Carvalho LE, Sobieranski AC, von Wangenheim A: 3D segmentation algorithms for computerized tomographic imaging: A systematic literature review<italic>.</italic> J Digit Imaging 1–52, 2018</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cheng</surname>
            <given-names>J-Z</given-names>
          </name>
          <name>
            <surname>Ni</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>YH</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tiu</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>YC</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>CS</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>CM</given-names>
          </name>
        </person-group>
        <article-title>Computer-aided diagnosis with deep learning architecture: Applications to breast lesions in US images and pulmonary nodules in CT scans</article-title>
        <source>Sci Rep</source>
        <year>2016</year>
        <volume>6</volume>
        <fpage>24454</fpage>
        <pub-id pub-id-type="doi">10.1038/srep24454</pub-id>
        <pub-id pub-id-type="pmid">27079888</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wachinger</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Reuter</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>DeepNAT: Deep convolutional neural network for segmenting neuroanatomy</article-title>
        <source>NeuroImage</source>
        <year>2018</year>
        <volume>170</volume>
        <fpage>434</fpage>
        <lpage>445</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.02.035</pub-id>
        <pub-id pub-id-type="pmid">28223187</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>KC</given-names>
          </name>
        </person-group>
        <article-title>Standard lexicons, coding systems and ontologies for interoperability and semantic computation in imaging</article-title>
        <source>J Digit Imaging</source>
        <year>2018</year>
        <volume>31</volume>
        <issue>3</issue>
        <fpage>353</fpage>
        <lpage>360</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-018-0069-8</pub-id>
        <pub-id pub-id-type="pmid">29725962</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Agarwal</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Podchiyska</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Banda</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Goel</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>TI</given-names>
          </name>
          <name>
            <surname>Minty</surname>
            <given-names>EP</given-names>
          </name>
          <name>
            <surname>Sweeney</surname>
            <given-names>TE</given-names>
          </name>
          <name>
            <surname>Gyang</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>NH</given-names>
          </name>
        </person-group>
        <article-title>Learning statistical models of phenotypes using noisy labeled training data</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2016</year>
        <volume>23</volume>
        <issue>6</issue>
        <fpage>1166</fpage>
        <lpage>1173</lpage>
        <pub-id pub-id-type="doi">10.1093/jamia/ocw028</pub-id>
        <pub-id pub-id-type="pmid">27174893</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Korfiatis</surname>
            <given-names>PD</given-names>
          </name>
          <name>
            <surname>Kline</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Blezek</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Langer</surname>
            <given-names>SG</given-names>
          </name>
          <name>
            <surname>Ryan</surname>
            <given-names>WJ</given-names>
          </name>
          <name>
            <surname>Erickson</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>MIRMAID: A content management system for medical image analysis research</article-title>
        <source>RadioGraphics</source>
        <year>2015</year>
        <volume>35</volume>
        <issue>5</issue>
        <fpage>1461</fpage>
        <lpage>1468</lpage>
        <pub-id pub-id-type="doi">10.1148/rg.2015140031</pub-id>
        <pub-id pub-id-type="pmid">26284301</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Marcus</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>TR</given-names>
          </name>
          <name>
            <surname>Ramaratnam</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Buckner</surname>
            <given-names>RL</given-names>
          </name>
        </person-group>
        <article-title>The extensible neuroimaging archive toolkit</article-title>
        <source>Neuroinformatics</source>
        <year>2007</year>
        <volume>5</volume>
        <issue>1</issue>
        <fpage>11</fpage>
        <lpage>33</lpage>
        <pub-id pub-id-type="doi">10.1385/NI:5:1:11</pub-id>
        <pub-id pub-id-type="pmid">17426351</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kline</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Korfiatis</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Edwards</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Bae</surname>
            <given-names>KT</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chapman</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Mrug</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Grantham</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Landsittel</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bennett</surname>
            <given-names>WM</given-names>
          </name>
          <name>
            <surname>King</surname>
            <given-names>BF</given-names>
          </name>
          <name>
            <surname>Harris</surname>
            <given-names>PC</given-names>
          </name>
          <name>
            <surname>Torres</surname>
            <given-names>VE</given-names>
          </name>
          <name>
            <surname>Erickson</surname>
            <given-names>BJ</given-names>
          </name>
          <collab>CRISP Investigators</collab>
        </person-group>
        <article-title>Image texture features predict renal function decline in patients with autosomal dominant polycystic kidney disease</article-title>
        <source>Kidney Int</source>
        <year>2017</year>
        <volume>92</volume>
        <issue>5</issue>
        <fpage>1206</fpage>
        <lpage>1216</lpage>
        <pub-id pub-id-type="doi">10.1016/j.kint.2017.03.026</pub-id>
        <pub-id pub-id-type="pmid">28532709</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Selvaraju RR, et al.: Grad-CAM: Why did you say that? arXiv [stat.ML], 2016</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Selvaraju RR, et al: Grad-cam: Visual explanations from deep networks via gradient-based localization. v3(8)7, 2016<italic>.</italic> See <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1610.02391">https://arxiv.org/abs/1610.02391</ext-link></mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Simonyan K, Vedaldi A, Zisserman A: Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. arXiv [cs.CV], 2013</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Zhou B, et al: Learning Deep Features for Discriminative Localization. arXiv [cs.CV], 2015</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Mehrtash A, et al: DeepInfer: Open-source deep learning deployment toolkit for image-guided therapy. In: Proceedings of SPIE--the International Society for Optical Engineering, 10135. 101351K, 2017</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Yu F, et al: Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop<italic>.</italic> arXiv preprint arXiv:1506.03365, 2015</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <mixed-citation publication-type="other">Zhou Z, et al: Integrating active learning and transfer learning for carotid intima-media thickness video interpretation. 2018</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Russakovsky O, Li L, Fei-Fei L: Best of both worlds: Human-machine collaboration for object annotation. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Sakinis T, et al: Interactive segmentation of medical images through fully convolutional neural networks<italic>.</italic> arXiv preprint arXiv:1903.08205, 2019.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <mixed-citation publication-type="other">Wu J, et al: Deep multiple instance learning for image classification and auto-annotation. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2015</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <mixed-citation publication-type="other">Xu Y, et al: Deep learning of feature representation with multiple instance learning for medical image analysis. In: 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2014</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <mixed-citation publication-type="other">Mettes P, Snoek CG, Chang S-F: Localizing actions from video labels and pseudo-annotations. arXiv preprint arXiv:1707.09143, 2017</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Morgan</surname>
            <given-names>PS</given-names>
          </name>
          <name>
            <surname>Ashburner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Rorden</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>The first step for neuroimaging data analysis: DICOM to NIfTI conversion</article-title>
        <source>J Neurosci Methods</source>
        <year>2016</year>
        <volume>264</volume>
        <fpage>47</fpage>
        <lpage>56</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.03.001</pub-id>
        <pub-id pub-id-type="pmid">26945974</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
