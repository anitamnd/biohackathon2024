<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_MEDIMA101537 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEgr6 jpg ?>
<?FILEgr7 jpg ?>
<?FILEfx1 jpg ?>
<?FILEsi1 svg ?>
<?properties open_access?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Med Image Anal</journal-id>
    <journal-id journal-id-type="iso-abbrev">Med Image Anal</journal-id>
    <journal-title-group>
      <journal-title>Medical Image Analysis</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1361-8415</issn>
    <issn pub-type="epub">1361-8423</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6839613</article-id>
    <article-id pub-id-type="publisher-id">S1361-8415(19)30075-1</article-id>
    <article-id pub-id-type="doi">10.1016/j.media.2019.101537</article-id>
    <article-id pub-id-type="publisher-id">101537</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Evaluation of algorithms for Multi-Modality Whole Heart Segmentation: An open-access grand challenge</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0001">
        <name>
          <surname>Zhuang</surname>
          <given-names>Xiahai</given-names>
        </name>
        <email>zxh@fudan.edu.cn</email>
        <xref rid="cor0001" ref-type="corresp">⁎</xref>
        <xref rid="aff0001" ref-type="aff">a</xref>
        <xref rid="aff0002" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au0002">
        <name>
          <surname>Li</surname>
          <given-names>Lei</given-names>
        </name>
        <email>lilei.sky@sjtu.edu.cn</email>
        <xref rid="cor0002" ref-type="corresp">⁎⁎</xref>
        <xref rid="aff0003" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author" id="au0003">
        <name>
          <surname>Payer</surname>
          <given-names>Christian</given-names>
        </name>
        <xref rid="aff0004" ref-type="aff">d</xref>
      </contrib>
      <contrib contrib-type="author" id="au0004">
        <name>
          <surname>Štern</surname>
          <given-names>Darko</given-names>
        </name>
        <xref rid="aff0005" ref-type="aff">e</xref>
      </contrib>
      <contrib contrib-type="author" id="au0005">
        <name>
          <surname>Urschler</surname>
          <given-names>Martin</given-names>
        </name>
        <xref rid="aff0005" ref-type="aff">e</xref>
      </contrib>
      <contrib contrib-type="author" id="au0006">
        <name>
          <surname>Heinrich</surname>
          <given-names>Mattias P.</given-names>
        </name>
        <xref rid="aff0006" ref-type="aff">f</xref>
      </contrib>
      <contrib contrib-type="author" id="au0007">
        <name>
          <surname>Oster</surname>
          <given-names>Julien</given-names>
        </name>
        <xref rid="aff0007" ref-type="aff">g</xref>
      </contrib>
      <contrib contrib-type="author" id="au0008">
        <name>
          <surname>Wang</surname>
          <given-names>Chunliang</given-names>
        </name>
        <xref rid="aff0008" ref-type="aff">h</xref>
      </contrib>
      <contrib contrib-type="author" id="au0009">
        <name>
          <surname>Smedby</surname>
          <given-names>Örjan</given-names>
        </name>
        <xref rid="aff0008" ref-type="aff">h</xref>
      </contrib>
      <contrib contrib-type="author" id="au0010">
        <name>
          <surname>Bian</surname>
          <given-names>Cheng</given-names>
        </name>
        <xref rid="aff0009" ref-type="aff">i</xref>
      </contrib>
      <contrib contrib-type="author" id="au0011">
        <name>
          <surname>Yang</surname>
          <given-names>Xin</given-names>
        </name>
        <xref rid="aff0010" ref-type="aff">j</xref>
      </contrib>
      <contrib contrib-type="author" id="au0012">
        <name>
          <surname>Heng</surname>
          <given-names>Pheng-Ann</given-names>
        </name>
        <xref rid="aff0010" ref-type="aff">j</xref>
      </contrib>
      <contrib contrib-type="author" id="au0013">
        <name>
          <surname>Mortazi</surname>
          <given-names>Aliasghar</given-names>
        </name>
        <xref rid="aff0011" ref-type="aff">k</xref>
      </contrib>
      <contrib contrib-type="author" id="au0014">
        <name>
          <surname>Bagci</surname>
          <given-names>Ulas</given-names>
        </name>
        <xref rid="aff0011" ref-type="aff">k</xref>
      </contrib>
      <contrib contrib-type="author" id="au0015">
        <name>
          <surname>Yang</surname>
          <given-names>Guanyu</given-names>
        </name>
        <xref rid="aff0012" ref-type="aff">l</xref>
      </contrib>
      <contrib contrib-type="author" id="au0016">
        <name>
          <surname>Sun</surname>
          <given-names>Chenchen</given-names>
        </name>
        <xref rid="aff0012" ref-type="aff">l</xref>
      </contrib>
      <contrib contrib-type="author" id="au0017">
        <name>
          <surname>Galisot</surname>
          <given-names>Gaetan</given-names>
        </name>
        <xref rid="aff0013" ref-type="aff">m</xref>
      </contrib>
      <contrib contrib-type="author" id="au0018">
        <name>
          <surname>Ramel</surname>
          <given-names>Jean-Yves</given-names>
        </name>
        <xref rid="aff0013" ref-type="aff">m</xref>
      </contrib>
      <contrib contrib-type="author" id="au0019">
        <name>
          <surname>Brouard</surname>
          <given-names>Thierry</given-names>
        </name>
        <xref rid="aff0013" ref-type="aff">m</xref>
      </contrib>
      <contrib contrib-type="author" id="au0020">
        <name>
          <surname>Tong</surname>
          <given-names>Qianqian</given-names>
        </name>
        <xref rid="aff0014" ref-type="aff">n</xref>
      </contrib>
      <contrib contrib-type="author" id="au0021">
        <name>
          <surname>Si</surname>
          <given-names>Weixin</given-names>
        </name>
        <xref rid="aff0015" ref-type="aff">o</xref>
      </contrib>
      <contrib contrib-type="author" id="au0022">
        <name>
          <surname>Liao</surname>
          <given-names>Xiangyun</given-names>
        </name>
        <xref rid="aff0016" ref-type="aff">p</xref>
      </contrib>
      <contrib contrib-type="author" id="au0023">
        <name>
          <surname>Zeng</surname>
          <given-names>Guodong</given-names>
        </name>
        <xref rid="aff0003" ref-type="aff">c</xref>
        <xref rid="aff0017" ref-type="aff">q</xref>
      </contrib>
      <contrib contrib-type="author" id="au0024">
        <name>
          <surname>Shi</surname>
          <given-names>Zenglin</given-names>
        </name>
        <xref rid="aff0017" ref-type="aff">q</xref>
      </contrib>
      <contrib contrib-type="author" id="au0025">
        <name>
          <surname>Zheng</surname>
          <given-names>Guoyan</given-names>
        </name>
        <xref rid="aff0003" ref-type="aff">c</xref>
        <xref rid="aff0017" ref-type="aff">q</xref>
      </contrib>
      <contrib contrib-type="author" id="au0026">
        <name>
          <surname>Wang</surname>
          <given-names>Chengjia</given-names>
        </name>
        <xref rid="aff0018" ref-type="aff">r</xref>
        <xref rid="aff0019" ref-type="aff">s</xref>
      </contrib>
      <contrib contrib-type="author" id="au0027">
        <name>
          <surname>MacGillivray</surname>
          <given-names>Tom</given-names>
        </name>
        <xref rid="aff0019" ref-type="aff">s</xref>
      </contrib>
      <contrib contrib-type="author" id="au0028">
        <name>
          <surname>Newby</surname>
          <given-names>David</given-names>
        </name>
        <xref rid="aff0018" ref-type="aff">r</xref>
        <xref rid="aff0019" ref-type="aff">s</xref>
      </contrib>
      <contrib contrib-type="author" id="au0029">
        <name>
          <surname>Rhode</surname>
          <given-names>Kawal</given-names>
        </name>
        <xref rid="aff0020" ref-type="aff">t</xref>
      </contrib>
      <contrib contrib-type="author" id="au0030">
        <name>
          <surname>Ourselin</surname>
          <given-names>Sebastien</given-names>
        </name>
        <xref rid="aff0020" ref-type="aff">t</xref>
      </contrib>
      <contrib contrib-type="author" id="au0031">
        <name>
          <surname>Mohiaddin</surname>
          <given-names>Raad</given-names>
        </name>
        <xref rid="aff0021" ref-type="aff">u</xref>
        <xref rid="aff0022" ref-type="aff">v</xref>
      </contrib>
      <contrib contrib-type="author" id="au0032">
        <name>
          <surname>Keegan</surname>
          <given-names>Jennifer</given-names>
        </name>
        <xref rid="aff0021" ref-type="aff">u</xref>
        <xref rid="aff0022" ref-type="aff">v</xref>
      </contrib>
      <contrib contrib-type="author" id="au0033">
        <name>
          <surname>Firmin</surname>
          <given-names>David</given-names>
        </name>
        <xref rid="aff0021" ref-type="aff">u</xref>
        <xref rid="aff0022" ref-type="aff">v</xref>
      </contrib>
      <contrib contrib-type="author" id="au0034">
        <name>
          <surname>Yang</surname>
          <given-names>Guang</given-names>
        </name>
        <email>g.yang@imperial.ac.uk</email>
        <xref rid="cor0003" ref-type="corresp">‡</xref>
        <xref rid="aff0021" ref-type="aff">u</xref>
        <xref rid="aff0022" ref-type="aff">v</xref>
      </contrib>
    </contrib-group>
    <aff id="aff0001"><label>a</label>School of Data Science, Fudan University, Shanghai, 200433, China</aff>
    <aff id="aff0002"><label>b</label>Fudan-Xinzailing Joint Research Center for Big Data, Fudan University, Shanghai, 200433, China</aff>
    <aff id="aff0003"><label>c</label>School of Biomedical Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China</aff>
    <aff id="aff0004"><label>d</label>Institute of Computer Graphics and Vision, Graz University of Technology, Graz, 8010, Austria</aff>
    <aff id="aff0005"><label>e</label>Ludwig Boltzmann Institute for Clinical Forensic Imaging, Graz, 8010, Austria</aff>
    <aff id="aff0006"><label>f</label>Institute of Medical Informatics, University of Lubeck, Lubeck, 23562, Germany</aff>
    <aff id="aff0007"><label>g</label>Inserm, Université de Lorraine, IADI, U1254, Nancy, France</aff>
    <aff id="aff0008"><label>h</label>Department of Biomedical Engineering and Health Systems, KTH Royal Institute of Technology, Stockholm SE-14152, Sweden</aff>
    <aff id="aff0009"><label>i</label>School of Biomed. Eng., Health Science Centre, Shenzhen University, Shenzhen, 518060, China</aff>
    <aff id="aff0010"><label>j</label>Dept. of Comp. Sci. and Eng., The Chinese University of Hong Kong, Hong Kong, China</aff>
    <aff id="aff0011"><label>k</label>Center for Research in Computer Vision (CRCV), University of Central Florida, Orlando, 32816, U.S.</aff>
    <aff id="aff0012"><label>l</label>School of Computer Science and Engineering, Southeast University, Nanjing, 210096, China</aff>
    <aff id="aff0013"><label>m</label>LIFAT (EA6300), Université de Tours, 64 avenue Jean Portalis, Tours, 37200, France</aff>
    <aff id="aff0014"><label>n</label>School of Computer Science, Wuhan University, Wuhan, 430072, China</aff>
    <aff id="aff0015"><label>o</label>Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, SIAT, Shenzhen, China</aff>
    <aff id="aff0016"><label>p</label>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, 518055, China</aff>
    <aff id="aff0017"><label>q</label>Institute for Surgical Technology &amp; Biomechanics, University of Bern, Bern, 3014, Switzerland</aff>
    <aff id="aff0018"><label>r</label>BHF Centre for Cardiovascular Science, University of Edinburgh, Edinburgh, U.K.</aff>
    <aff id="aff0019"><label>s</label>Edinburgh Imaging Facility QMRI, University of Edinburgh, Edinburgh, U.K.</aff>
    <aff id="aff0020"><label>t</label>School of Biomedical Engineering and Imaging Sciences, Kings College London, London, U.K.</aff>
    <aff id="aff0021"><label>u</label>Cardiovascular Research Centre, Royal Brompton Hospital, London, SW3 6NP, U.K.</aff>
    <aff id="aff0022"><label>v</label>National Heart and Lung Institute, Imperial College London, London, SW7 2AZ, London, U.K.</aff>
    <author-notes>
      <corresp id="cor0001"><label>⁎</label>Corresponding author. <email>zxh@fudan.edu.cn</email></corresp>
      <corresp id="cor0002"><label>⁎⁎</label>Corresponding author. <email>lilei.sky@sjtu.edu.cn</email></corresp>
      <corresp id="cor0003"><label>‡</label>Corresponding author. <email>g.yang@imperial.ac.uk</email></corresp>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>1</day>
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="ppub">.-->
    <pub-date pub-type="ppub">
      <month>12</month>
      <year>2019</year>
    </pub-date>
    <volume>58</volume>
    <elocation-id>101537</elocation-id>
    <history>
      <date date-type="received">
        <day>19</day>
        <month>2</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>3</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>22</day>
        <month>7</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 The Authors</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="CC BY" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract abstract-type="author-highlights" id="absh001">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="lst0002">
          <list-item id="lstitem0001">
            <label>•</label>
            <p id="p0001">This work presents the methodologies and evaluation results for the WHS algorithms selected from the submissions to the Multi-Modality Whole Heart Segmentation (MM-WHS) challenge, in conjunction with MICCAI 2017.</p>
          </list-item>
          <list-item id="lstitem0002">
            <label>•</label>
            <p id="p0002">This work introduces the related information to the challenge, discusses the results from the conventional methods and deep learning-based algorithms, and provides insights to the future research.</p>
          </list-item>
          <list-item id="lstitem0003">
            <label>•</label>
            <p id="p0003">The challenge provides a fair and intuitive comparison framework for methods developed and being developed for WHS.</p>
          </list-item>
          <list-item id="lstitem0004">
            <label>•</label>
            <p id="p0004">The challenge provides the training datasets with manually delineated ground truths and evaluation for an ongoing development of MM-WHS algorithms.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <abstract abstract-type="graphical" id="absh0002">
      <title>Graphical abstract</title>
      <p><fig id="fig0008" position="anchor"><graphic xlink:href="fx1"/></fig>This manuscript presents the methodologies and evaluation results for the WHS algorithms selected from the submissions to the Multi-Modality Whole Heart Segmentation (MMWHS) challenge, in conjunction with MICCAI-STACOM 2017. The challenge provides 120 three-dimensional cardiac images covering the whole heart, including 60 CT and 60 MRI volumes, all acquired in clinical environments with manual delineation. Ten algorithms for CT data and eleven algorithms for MRI data, submitted from twelve groups, have been evaluated. The results show that many of the deep learning (DL) based methods achieved high accuracy, even though the number of training datasets were limited. Several of them also reported poor results in the blinded evaluation, probably due to over fitting in their training. The conventional algorithms, mainly based on multi-atlas segmentation, demonstrated robust and stable performance, even though the accuracy is not as good as the best DL method in CT segmentation. The challenge, including provision of the annotated training data and the blinded evaluation for submitted algorithms on the test data, continues as an ongoing benchmarking resource.</p>
    </abstract>
    <abstract id="abs0001">
      <p>Knowledge of whole heart anatomy is a prerequisite for many clinical applications. Whole heart segmentation (WHS), which delineates substructures of the heart, can be very valuable for modeling and analysis of the anatomy and functions of the heart. However, automating this segmentation can be challenging due to the large variation of the heart shape, and different image qualities of the clinical data. To achieve this goal, an initial set of training data is generally needed for constructing priors or for training. Furthermore, it is difficult to perform comparisons between different methods, largely due to differences in the datasets and evaluation metrics used. This manuscript presents the methodologies and evaluation results for the WHS algorithms selected from the submissions to the Multi-Modality Whole Heart Segmentation (MM-WHS) challenge, in conjunction with MICCAI 2017. The challenge provided 120 three-dimensional cardiac images covering the whole heart, including 60 CT and 60 MRI volumes, all acquired in clinical environments with manual delineation. Ten algorithms for CT data and eleven algorithms for MRI data, submitted from twelve groups, have been evaluated. The results showed that the performance of CT WHS was generally better than that of MRI WHS. The segmentation of the substructures for different categories of patients could present different levels of challenge due to the difference in imaging and variations of heart shapes. The deep learning (DL)-based methods demonstrated great potential, though several of them reported poor results in the blinded evaluation. Their performance could vary greatly across different network structures and training strategies. The conventional algorithms, mainly based on multi-atlas segmentation, demonstrated good performance, though the accuracy and computational efficiency could be limited. The challenge, including provision of the annotated training data and the blinded evaluation for submitted algorithms on the test data, continues as an ongoing benchmarking resource via its homepage (<ext-link ext-link-type="uri" xlink:href="http://www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs/" id="intrrf0001">www.sdspeople.fudan.edu.cn/zhuangxiahai/0/mmwhs/</ext-link>).</p>
    </abstract>
    <kwd-group id="keys0001">
      <title>Keywords</title>
      <kwd>Whole Heart Segmentation</kwd>
      <kwd>Multi-modality</kwd>
      <kwd>Benchmark</kwd>
      <kwd>Challenge</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec0001">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0006">According to the World Health Organization, cardiovascular diseases (CVDs) are the leading cause of death globally (<xref rid="bib0018" ref-type="bibr">Mendis et al., 2011</xref>). Medical imaging has revolutionized modern medicine and healthcare, and imaging and computing technologies have become increasingly important for the diagnosis and treatments of CVDs. Computed tomography (CT), magnetic resonance imaging (MRI), positron emission tomography (PET), single photon emission computed tomography (SPECT), and ultrasound (US) have been used extensively for physiologic understanding and diagnostic purposes in cardiology (<xref rid="bib0013" ref-type="bibr">Kang et al., 2012</xref>). Among these, CT and MRI are particularly used to provide clear anatomical information of the heart. Cardiac MRI has the advantages of being free from ionizing radiation, acquiring images with good contrast between soft tissues and with relatively high spatial resolution (<xref rid="bib0025" ref-type="bibr">Nikolaou et al., 2011</xref>). In contrast, cardiac CT, though involves ionizing radiation, is fast, low cost, and generally of high quality (<xref rid="bib0032" ref-type="bibr">Roberts et al., 2008</xref>).</p>
    <p id="p0007">To quantify the morphological and pathological changes, it is commonly a prerequisite to segment the important structures from the cardiac medical images. Whole heart segmentation (WHS) aims to extract each of the individual whole heart substructures, including the left ventricle (LV), right ventricle (RV), left atrium (LA), right atrium (RA), myocardium of LV (Myo), ascending aorta (AO) or the whole aorta, and the pulmonary artery (PA) (<xref rid="bib0051" ref-type="bibr">Zhuang, 2013</xref>), as <xref rid="fig0001" ref-type="fig">Fig. 1</xref> shows. The applications of WHS are numerous. The results can be used to directly compute the functional indices such as ejection fraction. Additionally, the geometrical information is useful in surgical guidance such as in radio-frequency ablation of the LA. However, the manual delineation of whole heart is labor-intensive and tedious, needing almost 8 hours for a single subject (<xref rid="bib0054" ref-type="bibr">Zhuang and Shen, 2016</xref>). Thus, automating the segmentation from multi-modality images, referred to as MM-WHS, is highly desired but still challenging, mainly due to the following reasons (<xref rid="bib0051" ref-type="bibr">Zhuang, 2013</xref>). First, the shape of the heart varies through the cardiac cycle as the heart contracts and relaxes. It also varies greatly from subject to subject, especially for those with pathological and physiological changes. Second, the appearance and image quality can be variable. For example, the enhancement patterns of the CT images can differ significantly for different scanners or acquisition sessions. Also, motion artifacts, poor contrast-to-noise ratio and signal-to-noise ratio, commonly presented in the clinical data, can significantly deteriorate the image quality and consequently challenge the task.<fig id="fig0001"><label>Fig. 1</label><caption><p>Examples of cardiac images and WHS results: (a) displays the three orthogonal views of a cardiac CT image and its corresponding WHS result, (b) shows example cardiac MRI data and the WHS result. LV: left ventricle; RV: right ventricle; LA: left atrium; RA: right atrium; Myo: myocardium of LV; AO: ascending aorta; PA: pulmonary artery.</p></caption><alt-text id="at0001">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p>
    <sec id="sec0002">
      <label>1.1</label>
      <title>State-of-the-art for Whole Heart Segmentation</title>
      <p id="p0008">In the last ten years, a variety of WHS techniques have been proposed for cardiac CT and MRI data. Detailed reviews of previously published algorithms can be found in <xref rid="bib0013" ref-type="bibr">Kang et al. (2012)</xref>, <xref rid="bib0051" ref-type="bibr">Zhuang (2013)</xref> and <xref rid="bib0029" ref-type="bibr">Peng et al. (2016)</xref>. <xref rid="bib0013" ref-type="bibr">Kang et al. (2012)</xref> reviewed several modalities and corresponding segmentation algorithms for the diagnosis and treatments of CVDs. They summarized the roles and characteristics of different modalities of cardiac imaging and the parameter correlation between them. Furthermore, they categorized the WHS techniques into four, i.e., (1) boundary-driven techniques, (2) region-based techniques, (3) graph-cuts techniques, and (4) model fitting techniques. The advantages and disadvantages of each category were analyzed and summarized. <xref rid="bib0051" ref-type="bibr">Zhuang (2013)</xref> discussed the challenges and methodologies of the fully automatic WHS. Particularly, the work summarized two key techniques, i.e., the construction of prior models and the fitting procedure for segmentation propagation, for achieving this goal. Based on the types of prior models, the segmentation methods can be divided into two groups, namely the deformable model based methods and the atlas-based approaches. The fitting procedure can be decomposed into three stages, including localizing the whole heart, initializing the substructures, and refining the boundary delineation. Hence, this review paper by <xref rid="bib0051" ref-type="bibr">Zhuang (2013)</xref> mainly analyzes the algorithms based on the classification of prior models and fitting algorithms for the WHS from different modality images. <xref rid="bib0029" ref-type="bibr">Peng et al. (2016)</xref> reviewed both the methodologies of WHS and the structural and functional indices of the heart for clinical assessments. In their work, the WHS approaches were classified into three categories, i.e., image-driven techniques, model-driven techniques, and the direct estimation-based methods.</p>
      <p id="p0009">The three topic review papers mentioned above mainly cover publications before 2015. A collection of recent works not included by them are summarized in <xref rid="tbl0001" ref-type="table">Table 1</xref>. Among these works, <xref rid="bib0052" ref-type="bibr">Zhuang et al. (2015)</xref> proposed an atlas ranking and selection scheme based on conditional entropy for the multi-atlas based WHS of CT. <xref rid="bib0050" ref-type="bibr">Zhou et al. (2017)</xref> developed a set of CT atlases labeled with 15 cardiac substructures. These atlases were then used for automatic WHS of CT via the multi-atlas segmentation (MAS) framework. <xref rid="bib0004" ref-type="bibr">Cai et al. (2017)</xref> developed a method with window width-level adjustment to pre-process CT data, which generates images with clear anatomical structures for WHS. They applied a Gaussian filter-based multi-resolution scheme to eliminate the discontinuity in the down-sampling decomposition for whole heart image registration. <xref rid="bib0055" ref-type="bibr">Zuluaga et al. (2013)</xref> developed a MAS scheme for both CT and MRI WHS. The proposed method ranked and selected optimal atlases based on locally normalized cross correlation. <xref rid="bib0026" ref-type="bibr">Pace et al. (2015)</xref> proposed a patch-based interactive algorithm to extract the heart based on a manual initialization from experts. The method employs active learning to identify the areas that require user interaction. <xref rid="bib0054" ref-type="bibr">Zhuang and Shen (2016)</xref> developed a multi-modality MAS framework for WHS of cardiac MRI, which used a set of atlases built from both CT and MRI. They proposed modality invariant metrics for computing the global image similarity and the local similarity. The global image similarity was used to rank and select atlases, from the multi-modality atlas pool, for segmenting a target image, and the local similarity metrics were proposed for the patch-based label fusion, where a multi-scale patch strategy was developed to obtain a promising performance.<table-wrap position="float" id="tbl0001"><label>Table 1</label><caption><p>Summary of previous WHS methods for multi-modality images. PIS: patch-based interactive segmentation; FIMH: International Conference on Functional Imaging and Modeling of the Heart; MICCAI: International Conference on Medical Image Computing and Computer-assisted Intervention; MedPhys: Medical Physics; MedIA: Medical Image Analysis; RadiotherOncol: Radiotherapy and Oncology.</p></caption><alt-text id="at0008">Table 1</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Reference</th><th align="left" valign="top">Data</th><th align="left" valign="top">Method</th><th align="left" valign="top">Runtime</th><th align="left" valign="top">Dice</th></tr></thead><tbody><tr><td valign="top"><xref rid="bib0055" ref-type="bibr">Zuluaga et al. (2013)</xref>, FIMH</td><td valign="top">8 CT, 23 MRI</td><td valign="top">MAS</td><td valign="top">60 min, 30 min</td><td valign="top">0.89 ± 0.04, 0.91 ± 0.03</td></tr><tr><td valign="top"><xref rid="bib0052" ref-type="bibr">Zhuang et al. (2015)</xref>, MedPhys</td><td valign="top">30 CT</td><td valign="top">MAS</td><td valign="top">13.2 min</td><td valign="top">0.92 ± 0.02</td></tr><tr><td valign="top"><xref rid="bib0026" ref-type="bibr">Pace et al. (2015)</xref>, MICCAI</td><td valign="top">20 MRI</td><td valign="top">PIS + Active learning</td><td valign="top">N/A</td><td valign="top">N/A</td></tr><tr><td valign="top"><xref rid="bib0054" ref-type="bibr">Zhuang and Shen (2016)</xref>, MedIA</td><td valign="top">20 CT, 20 MRI</td><td valign="top">Multi-modality MAS</td><td valign="top">12.58 min</td><td valign="top">0.90 ± 0.03</td></tr><tr><td valign="top"><xref rid="bib0050" ref-type="bibr">Zhou et al. (2017)</xref>, RadiotherOncol</td><td valign="top">31 CT</td><td valign="top">MAS</td><td valign="top">10 min</td><td valign="top">N/A</td></tr><tr><td valign="top"><xref rid="bib0004" ref-type="bibr">Cai et al. (2017)</xref>, Neurocomputing</td><td valign="top">14 CT</td><td valign="top">Gaussian filter-based</td><td valign="top">N/A</td><td valign="top">N/A</td></tr></tbody></table></table-wrap></p>
      <p id="p0010">In conclusion, WHS based on the MAS framework, referred to as MA-WHS, has been well researched in recent years. MAS segments an unknown target image by propagating and fusing the labels from multiple annotated atlases using image registration techniques. The performance relies on the registration algorithms for label propagation and the fusion strategy to combine the segmentation results from the multiple atlases. Both of these two key steps are generally computationally expensive.</p>
      <p id="p0011">Recently, deep learning (DL)-based methods have shown great promise in medical image analysis. They have achieved superior performance in various imaging modalities and different clinical applications (<xref rid="bib0034" ref-type="bibr">Roth, Lu, Seff, Cherry, Hoffman, Wang, Liu, Turkbey, Summers, 2014</xref>, <xref rid="bib0035" ref-type="bibr">Shen, Wu, Suk, 2017</xref>). For cardiac segmentation, <xref rid="bib0001" ref-type="bibr">Avendi et al. (2016)</xref> proposed a DL algorithm for LV segmentation. <xref rid="bib0024" ref-type="bibr">Ngo et al. (2017)</xref> trained multiple layers of a deep belief network to localize the LV, and to define the endocardial and epicardial borders, followed by the distance regularized level set. Recently, <xref rid="bib0037" ref-type="bibr">Tan et al. (2018)</xref> designed a fully automated convolutional neural network (CNN) architecture for pixel-wise labeling of both the LV and RV with impressive performance, and <xref rid="bib0020" ref-type="bibr">Mo et al. (2018)</xref> proposed a deep Poincare map-based method for LV segmentation. DL methods have the potential to provide faster and more accurate segmentation, compared to the conventional approaches, such as the deformable model based algorithms and MAS methods. However, little work has been reported to date using DL for WHS, probably due to the limitation of training data and complexity of the segmentation task.</p>
    </sec>
    <sec id="sec0003">
      <label>1.2</label>
      <title>Motivation and contribution</title>
      <p id="p0012">Due to the above mentioned challenges, we organized the competition of MM-WHS, providing 120 multi-modality whole heart images for developing new WHS algorithms, as well as validating existing ones. We also presented a fair evaluation and comparison framework for participants. In total, twelve groups who submitted their results and methods were selected, and they all agreed to contribute to this work, a benchmark for WHS of two modalities, i.e., CT and MRI. In this work, we introduce the related information, elaborate on the methodologies of these selective submissions, discuss the results and provide insights into future research.</p>
      <p id="p0013">The rest of this paper is organized as follows. <xref rid="sec0004" ref-type="sec">Section 2</xref> provides details of the materials and evaluation framework. <xref rid="sec0009" ref-type="sec">Section 3</xref> introduces the evaluated methods for benchmarking. <xref rid="sec0022" ref-type="sec">Section 4</xref> presents the results, followed by discussions in <xref rid="sec0023" ref-type="sec">Section 5</xref>. We conclude this work in <xref rid="sec0030" ref-type="sec">Section 6</xref>.</p>
    </sec>
  </sec>
  <sec id="sec0004">
    <label>2</label>
    <title>Materials and setup</title>
    <sec id="sec0005">
      <label>2.1</label>
      <title>Data acquisition</title>
      <p id="p0014">All the CT and MRI data have been anonymized in agreement with the local regional ethics committee before being released to the MM-WHS challenge, and they were acquired in real clinical environments. The cardiac CT/CTA data were obtained from two state-of-the-art 64-slice CT scanners (Philips Medical Systems, Netherlands) using a standard coronary CT angiography protocol at two sites in Shanghai, China. All the data cover the whole heart from the upper abdomen to the aortic arch. The in-plane resolution of the axial slices is 0.78 × 0.78 mm, and the average slice thickness is 1.60 mm. The cardiac MRI data were acquired from two hospitals in London, UK. One set of data was acquired from St. Thomas Hospital on a 1.5T Philips scanner (Philips Healthcare, Best, The Netherlands), and the other was from Royal Brompton Hospital on a Siemens Magnetom Avanto 1.5T scanner (Siemens Medical Systems, Erlangen, Germany). In both sites, a navigator-gated 3D balanced steady state free precession (b-SSFP) sequence was used for free-breathing whole heart imaging. The data were acquired at a resolution of around (1.6 ∼ 2) × (1.6 ∼ 2) × (2 ∼ 3.2) mm, and reconstructed to half of its acquisition resolution, i.e., about (0.8 ∼ 1) × (0.8 ∼ 1) × (1 ∼ 1.6) mm.</p>
      <p id="p0015">In total, we provided 120 multi-modality whole heart images from multiple sites, including 60 cardiac CT and 60 cardiac MRI. For each modality, we selected 20 images to form the training set, and the remaining 40 to form the test set. For the CT data, we used random sampling to divide the data into the two sets. This is because the CT data were acquired from the two sites of the same hospital, and the data were equally distributed to the two sites. For the MRI data, they were acquired from two different hospitals. One provided 19 images, and the other provided 41 images. We divided the data from each hospital into two subsets, one for training (about one third) and the other for test (about two thirds). We then combined them to form the training set of 20 cases and test set of 40 cases. The pathologies involved in the MRI data covered a wide range of cardiac diseases, including myocardium infarction, atrial fibrillation (AF), tricuspid regurgitation, aortic valve stenosis, Alagille syndrome, Williams syndrome, dilated cardiomyopathy, aortic coarctation, and Tetralogy of Fallot. For analyzing the WHS performance with respect to different pathologies, we divided them into three categories, i.e., congenital heart disease (CHD) cases, AF patients, and <italic>Others</italic>. The numbers of subjects of these three categories in the training set are respectively 7, 6 and 7, and the numbers in the test set are respectively 9, 13, 18. Please refer to <xref rid="sec0026" ref-type="sec">Section 5.3</xref> for details of discussion.</p>
    </sec>
    <sec id="sec0006">
      <label>2.2</label>
      <title>Definition and gold standard</title>
      <p id="p0016">The WHS in this work aims to delineate and extract the seven substructures of the heart (<xref rid="bib0051" ref-type="bibr">Zhuang, 2013</xref>). These are:<list list-type="simple" id="lst0001"><list-item id="lstitem0006"><label>(1)</label><p id="p0017">The LV blood cavity, also referred to as the LV. The boundary between the LV and LA is defined by the plane of the mitral valve annulus, and the boundary between the LV and aorta is defined by the plane of the aortic valve annulus. The papillary muscles are included in the LV, according to the recommendation of cardiologists.</p></list-item><list-item id="lstitem0007"><label>(2)</label><p id="p0018">The RV blood cavity, also referred to as the RV. The boundary between the RV and RA is defined by the plane of the tricuspid valve annulus, and the boundary between the RV and PA is defined by the plane of the pulmonary valve annulus.</p></list-item><list-item id="lstitem0008"><label>(3)</label><p id="p0019">The LA blood cavity, also referred to as the LA. LA solely consists of the blood pool within the endocardium of the LA cavity, excluding the pulmonary veins (PVs) and left atrial appendage. The boundaries between the LA and PVs are determined by following each PV distally to the LA body and truncating at the point when there is no clear vein to follow (<xref rid="bib0038" ref-type="bibr">Tobon-Gomez et al., 2015</xref>).</p></list-item><list-item id="lstitem0009"><label>(4)</label><p id="p0020">The RA blood cavity, also referred to as the RA. The boundaries between the RA and superior/ inferior vena cava are determined at the point when there is no clear vena cava to follow, similar to the definition of boundaries between the LA and PVs.</p></list-item><list-item id="lstitem0010"><label>(5)</label><p id="p0021">The myocardium of the LV, referred to as the Myo. Myo has two surfaces, i.e., the epicardial surface (Epi) and the endocardial surface of the LV.</p></list-item><list-item id="lstitem0011"><label>(6)</label><p id="p0022">The AO trunk from the aortic valve to the superior level of the atria, also referred to as the AO. In our training data, the provided manual segmentation generally covers the whole ascending aorta to include the aortic arch. This means the distal end of the segmented great vessel exceeds the cutting point of the definition. However, in the evaluation we only consider the major trunk by manually cutting off the part of aorta which exceeds the superior level of the atria. We do this to avoid biased evaluation due to the inconsistent definition of the distal end of a great vessel.</p></list-item><list-item id="lstitem0012"><label>(7)</label><p id="p0023">The PA trunk from the pulmonary valve to the bifurcation point, also referred to as the PA. Similar to AO, for the training data we provide the manual segmentation which exceeds the distal end of the definition. However, for the test data we truncate the segmentation at the bifurcation point of the pulmonary artery before evaluating the accuracy of a result.</p></list-item></list></p>
      <p id="p0024">The four blood pool cavities, i.e., LV, RV, LA and RA, are also referred to as the four chambers.</p>
      <p id="p0025">Manual labeling was adopted for generating the gold standard segmentation. This was done slice-by-slice using the ITK-SNAP software (<xref rid="bib0048" ref-type="bibr">Yushkevich et al., 2006</xref>), either by clinicians or by students who majored in biomedical engineering or medical physics and were familiar with the whole heart anatomy. Each manual segmentation result was examined by a senior researcher specialized in cardiac imaging with experience of more than five years, and modifications were made where required. The sagittal and coronal views were visualized simultaneously to check the consistency and smoothness of the segmentation, although the manual delineation was mainly performed in the axial views. For each 3D image, it took approximately 6–10 h for the observer to complete the manual segmentation of the whole heart.</p>
    </sec>
    <sec id="sec0007">
      <label>2.3</label>
      <title>Evaluation metrics</title>
      <p id="p0026">We employed four widely used metrics to evaluate the accuracy of a segmentation result (<xref rid="bib0051" ref-type="bibr">Zhuang, 2013</xref>): the Dice score, Jaccard index, surface-to-surface distance (SD), and Hausdorff Distance (HD). For WHS evaluation, the generalized metrics were used, which are expected to be more objective (<xref rid="bib0005" ref-type="bibr">Crum, Camara, Hill, 2006</xref>, <xref rid="bib0051" ref-type="bibr">Zhuang, 2013</xref>).</p>
      <p id="p0027">For each modality, the data were split into two sets, i.e., the training set (20 CT and 20 MRI) and the test set (40 CT and 40 MRI). For the training data, both the images and the corresponding gold standard were released to the participants for building, training and cross-validating their models. For the test data, only the CT and MRI images were released. Once the participants developed their algorithms, they could submit their segmentation results on the test data to the challenge moderators for a final independent evaluation. To avoid parameter tuning via multiple submissions, the organizers only allowed a maximum of two evaluations of segmentation accuracies for one algorithm.</p>
    </sec>
    <sec id="sec0008">
      <label>2.4</label>
      <title>Participants</title>
      <p id="p0028">Twelve algorithms (teams) were selected for this benchmark work. Nine of them provided results for both CT and MRI data, one experimented only on the CT data and two worked solely on the MRI data.</p>
      <p id="p0029">All the 12 teams agreed to include their results in this paper. To simplify the description below, we used the team abbreviations referring to both the teams and their corresponding methods and results. The evaluated methods are elaborated on in <xref rid="sec0009" ref-type="sec">Section 3</xref>, and the key contributions of the teams are summarized in <xref rid="tbl0002" ref-type="table">Table 2</xref>. Note that the three methods, highlighted with an asterisk (*), were submitted after the deadline of the challenge. To be fair to the groups who submitted before the deadline, we excluded the late submissions from ranking and competing for the awards of the challenge. However, for this manuscript we include all the high quality submissions, to maximize the number of methods for benchmark and quality of the paper.<table-wrap position="float" id="tbl0002"><label>Table 2</label><caption><p>Summary of submitted methods.</p></caption><alt-text id="at0009">Table 2</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Teams</th><th align="left" valign="top">Tasks</th><th align="left" valign="top">Key elements in methods</th><th align="left" valign="top">Teams</th><th align="left" valign="top">Tasks</th><th align="left" valign="top">Key elements in methods</th></tr></thead><tbody><tr><td valign="top">GUT</td><td valign="top">CT, MRI</td><td valign="top">Two-step CNN, combined with anatomical label configurations.</td><td valign="top">UOL</td><td valign="top">MRI</td><td valign="top">MAS and discrete registration, to adapt the large shape variations.</td></tr><tr><td valign="top">KTH</td><td valign="top">CT, MRI</td><td valign="top">Multi-view U-Nets combining hierarchical shape prior.</td><td valign="top">CUHK1</td><td valign="top">CT, MRI</td><td valign="top">3D fully connected network (FCN) with the gradient flow optimization and Dice loss function.</td></tr><tr><td valign="top">SEU</td><td valign="top">CT</td><td valign="top">Conventional MAS-based method.</td><td valign="top">CUHK2</td><td valign="top">CT, MRI</td><td valign="top">Hybrid loss guided FCN.</td></tr><tr><td valign="top">UCF</td><td valign="top">CT, MRI</td><td valign="top">Multi-object multi-planar CNN with an adaptive fusion method.</td><td valign="top">UT</td><td valign="top">CT, MRI</td><td valign="top">Local probabilistic atlases coupled with a topological graph.</td></tr><tr><td valign="top">SIAT</td><td valign="top">CT, MRI</td><td valign="top">3D U-Net network learn multi-modality features.</td><td valign="top">UB2<xref rid="tblfn1" ref-type="table-fn">⁎</xref></td><td valign="top">MRI</td><td valign="top">Multi-scale fully convolutional Dense-Nets.</td></tr><tr><td valign="top">UB1<xref rid="tblfn1" ref-type="table-fn">⁎</xref></td><td valign="top">CT, MRI</td><td valign="top">Dilated residual networks.</td><td valign="top">UOE<xref rid="tblfn1" ref-type="table-fn">⁎</xref></td><td valign="top">CT, MRI</td><td valign="top">Two-stage concatenated U-Net.</td></tr></tbody></table><table-wrap-foot><fn id="tblfn1"><label>⁎</label><p id="np0001">Teams submitted results after the challenge deadline are indicated using Asterisk (*).</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
  </sec>
  <sec id="sec0009">
    <label>3</label>
    <title>Evaluated methods</title>
    <p id="p0030">In this section, we elaborate on the twelve benchmarked algorithms. <xref rid="tbl0002" ref-type="table">Table 2</xref> provides the summary for reference.</p>
    <sec id="sec0010">
      <label>3.1</label>
      <title>Graz University of Technology (GUT)</title>
      <p id="p0031"><xref rid="bib0028" ref-type="bibr">Payer et al. (2017)</xref> propose a fully automatic whole heart segmentation, based on multi-label CNN and using volumetric kernels, which consists of two separate CNNs: one to localize the heart, referred to as localization CNN, and the other to segment the fine detail of the whole heart structure within a small region of interest (ROI), referred to as segmentation CNN. The localization CNN is designed to predict the approximate center of the bounding box around all heart substructures, based on the U-Net (<xref rid="bib0033" ref-type="bibr">Ronneberger et al., 2015</xref>) and heatmap regression (<xref rid="bib0027" ref-type="bibr">Payer et al., 2016</xref>). A fixed physical size ROI is then cropped around the predicted center, ensuring that it can enclose all interested substructures of the heart. Within the cropped ROI, the multi-label segmentation CNN predicts the label of each pixel. In this method, the segmentation CNN works on high-resolution ROI, while the localization CNN works on the low resolution images. This two-step CNN pipeline helps to mitigate the intensive memory and runtime generally required by the volumetric kernels equipped 3D CNNs.</p>
    </sec>
    <sec id="sec0011">
      <label>3.2</label>
      <title>University of Lubeck (UOL)</title>
      <p id="p0032"><xref rid="bib0012" ref-type="bibr">Heinrich and Oster (2017)</xref> propose a multi-atlas registration approach for WHS of MRI, as <xref rid="fig0002" ref-type="fig">Fig. 2</xref> shows. This method adopts a discrete registration, which can capture large shape variations across different scans (<xref rid="bib0011" ref-type="bibr">Heinrich et al., 2013b</xref>). Moreover, it can ensure the alignment of anatomical structures by using dense displacement sampling and graphical model-based optimization (<xref rid="bib0010" ref-type="bibr">Heinrich et al., 2013a</xref>). Due to the use of contrast-invariant features (<xref rid="bib0043" ref-type="bibr">Xu et al., 2016</xref>), the multi-atlas registration can implicitly deal with the challenging varying intensity distributions due to different acquisition protocols. Within this method, one can register all the training atlases to an unseen test image. The warped atlas label images are then combined by means of weighted label fusion. Finally, an edge-preserving smoothing of the generated probability maps is performed using the multi-label random walk algorithm, as implemented and parameterized in <xref rid="bib0009" ref-type="bibr">Heinrich and Blendowski (2016)</xref>.<fig id="fig0002"><label>Fig. 2</label><caption><p>Multi-atlas registration and label fusion with regularization proposed by <xref rid="bib0012" ref-type="bibr">Heinrich and Oster (2017)</xref>.</p></caption><alt-text id="at0002">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p>
    </sec>
    <sec id="sec0012">
      <label>3.3</label>
      <title>KTH Royal Institute of Technology (KTH)</title>
      <p id="p0033"><xref rid="bib0042" ref-type="bibr">Wang and Smedby (2017)</xref> propose an automatic WHS framework combining CNN with statistical shape priors. The additional shape information, also called shape context (<xref rid="bib0017" ref-type="bibr">Mahbod et al., 2018</xref>), is used to provide explicit 3D shape knowledge to the CNN. The method uses a random forest based landmark detection to detect the ROI. The statistical shape models are created using the segmentation masks of the 20 training CT images. The probability map is generated from three 2D U-Nets learned from the multi-view slices of the 3D training images. To estimate the shape of each subregion of heart, a hierarchical shape prior guided segmentation algorithm (<xref rid="bib0041" ref-type="bibr">Wang and Smedby, 2014</xref>) is then performed on the probability map. This shape information is represented using volumetric shape models, i.e., signed distance maps of the corresponding shapes. Finally, the estimated shape information is used as an extra channel, to train a new set of multi-view U-Nets for the final segmentation of the whole heart.</p>
    </sec>
    <sec id="sec0013">
      <label>3.4</label>
      <title>The Chinese University of Hong Kong, Method No. 1 (CUHK1)</title>
      <p id="p0034"><xref rid="bib0045" ref-type="bibr">Yang et al. (2017b)</xref> apply a general and fully automatic framework based on a 3D fully convolutional network (FCN). The framework is reinforced in the following aspects. First, an initialization is achieved by inheriting the knowledge from a 3D convolutional network trained on the large-scale Sports-1M video dataset (<xref rid="bib0040" ref-type="bibr">Tran et al., 2015</xref>). Then, the gradient flow is applied by shortening the back-propagation path and employing several auxiliary loss functions on the shallow layers of the network. This is to tackle the low efficiency and over-fitting issues when directly training the deep 3D FCNs, due to the gradient vanishing problem in shallow layers. Finally, the Dice similarity coefficient based loss function (<xref rid="bib0019" ref-type="bibr">Milletari et al., 2016</xref>) is included into a multi-class variant to balance the training for all classes.</p>
    </sec>
    <sec id="sec0014">
      <label>3.5</label>
      <title>University of Central Florida (UCF)</title>
      <p id="p0035"><xref rid="bib0022" ref-type="bibr">Mortazi et al. (2017a)</xref> propose a multi-object multi-planar CNN (MO-MP-CNN) method based on an encoder-decoder CNN. The multiple CNNs (<xref rid="bib0023" ref-type="bibr">Mortazi et al., 2017b</xref>) are trained from three different views, i.e., axial, sagittal, and coronal views, in 2D manners. An adaptive fusion method is then employed to combine the multiple outputs to refine the delineation. Furthermore, they apply a connected component analysis (CCA) on the final segmentation, to estimate the reliable (true positive) and unreliable (false positives) regions. Let <italic>n</italic> denote the number of classes in the images and <italic>m</italic> denote the number of components in each class, then the CCA could be performed as follows,<disp-formula id="eq0001"><label>(1)</label><mml:math id="M1" altimg="si1.svg"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>11</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>∪</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi mathvariant="bold">o</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo>&amp;</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>11</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>∩</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic>S</italic> indicates the segmentation result, <italic>i</italic> ∈ <italic>m</italic> and <italic>j</italic> ∈ <italic>n</italic>. The differences between the reliable and unreliable regions are used to guide the reliability of the segmentation process, namely the higher the difference, the more reliable the segmentation.</p>
    </sec>
    <sec id="sec0015">
      <label>3.6</label>
      <title>The Chinese University of Hong Kong, method no. 2 (CUHK2)</title>
      <p id="p0036"><xref rid="bib0046" ref-type="bibr">Yang et al. (2017c)</xref> propose to employ a 3D FCN for an end-to-end dense labeling, as <xref rid="fig0003" ref-type="fig">Fig. 3</xref> shows. The proposed network is coupled with several auxiliary loss functions in a deep supervision mechanism, to tackle the potential gradient vanishing problem and class imbalance in training. The network learns a spatial-temporal knowledge from a large-scale video dataset, and then transfer to initialize the shallow convolutional layers in the down-sampling path (<xref rid="bib0040" ref-type="bibr">Tran et al., 2015</xref>). For the class imbalance issue, a hybrid loss is proposed (<xref rid="bib0019" ref-type="bibr">Milletari et al., 2016</xref>), combining two complementary components: (1) volume-size weighted cross entropy loss (<italic>wCross</italic>) to preserve branch details such as the PA trunks. (2) multi-class Dice similarity coefficient loss (<italic>mDSC</italic>) to compact anatomy segmentation. Then, the proposed network can be well trained to simultaneously segment different heart substructures, and generate a segmentation in a dense but detail-preserved format.<fig id="fig0003"><label>Fig. 3</label><caption><p>A schematic illustration of the method developed by <xref rid="bib0046" ref-type="bibr">Yang et al. (2017c)</xref>. Digits represent the number of feature volumes in each layer. Volume with dotted line is for concatenation.</p></caption><alt-text id="at0003">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p>
    </sec>
    <sec id="sec0016">
      <label>3.7</label>
      <title>Southeast University (SEU)</title>
      <p id="p0037"><xref rid="bib0044" ref-type="bibr">Yang et al. (2017a)</xref> develop a MAS-based method for WHS of CT images. The proposed method consists of the following major steps. Firstly, an ROI detection is performed on atlas images and label images, which are down-sampled and resized to crop and generate a heart mask. Then, an affine registration is used to globally align the target image with the atlas images, followed by a nonrigid registration to refine alignment of local details. In addition, an atlas ranking step is applied by using mutual information as the similarity criterion, and those atlases with low similarity are discarded. A non-rigid registration is further performed by minimizing the dissimilarity within the heart substructures using the adaptive stochastic gradient descent method. Finally, the propagated labels are fused with different weights according to the similarities between the deformed atlases and the target image.</p>
    </sec>
    <sec id="sec0017">
      <label>3.8</label>
      <title>University of Tours (UT)</title>
      <p id="p0038"><xref rid="bib0007" ref-type="bibr">Galisot et al. (2017)</xref> propose an incremental and interactive atlas-based WHS method, combining several local probabilistic atlases based on a topological graph. The training images are used to construct the probabilistic atlases, for each of the substructures of the heart. The graph is used to encode the priori knowledge to incrementally extract different ROIs. The priori knowledge about the shape and intensity distributions of substructures is stored as features to the nodes of the graph. The spatial relationships between these anatomical structures are also learned and stored as the edges of the graph. In the case of multi-modality data, multiple graphs are constructed, for example two graphs are built for the CT and MRI images, respectively. A pixelwise classification method combining hidden Markov random field is developed to integrate the probability map information. To correct the misclassifications, a post-correction is performed based on the Adaboost scheme.</p>
    </sec>
    <sec id="sec0018">
      <label>3.9</label>
      <title>Shenzhen Institutes of Advanced Technology (SIAT)</title>
      <p id="p0039"><xref rid="bib0039" ref-type="bibr">Tong et al. (2017)</xref> develop a deeply-supervised end-to-end 3D U-Net for fully automatic WHS. The training dataset are artificially augmented by considering each ROI of the heart substructure independently. To reduce false positives from the surrounding tissues, a 3D U-Net is first trained to coarsely detect and segment the whole heart structure. To take full advantage of multi-modality information so that features of different substructures could be better extracted, the cardiac CT and MRI data are fused. Both the size and the intensity range of the different modality images are normalized before training the 3D U-Net model. Finally, the detected ROI is refined to achieve the final WHS, which is performed by a pixel-wise classification fashion using the 3D U-Net.</p>
    </sec>
    <sec id="sec0019">
      <label>3.10</label>
      <title>University of Bern, method no. 1 (UB1*)</title>
      <p id="p0040">This method designs a voxelwise dilated residual network, referred as VoxDResNet, to segment the whole heart structures from 3D MRI images. It can be used to generate a semantic segmentation of an arbitrary-sized volume data after training. Conventional FCN methods integrate multi-scale contextual information by reducing the spatial resolution via successive pooling and sub-sampling layers, for semantic segmentation. By contrast, the proposed method achieves the same goal using dilated convolution kernels, without decreasing the spatial resolution of the network output. Additionally, residual learning is incorporated as pixel-wise dilated residual modules to alleviate the degrading problem, and the WHS accuracy can be further improved by avoiding gridding artifacts introduced by the dilation (<xref rid="bib0047" ref-type="bibr">Yu et al., 2017</xref>).</p>
    </sec>
    <sec id="sec0020">
      <label>3.11</label>
      <title>University of Bern, method no. 2 (UB2*)</title>
      <p id="p0041">This method includes a multi-scale pixel-wise fully convolutional Dense-Nets for 3D WHS of MRI images, which could directly map a whole volume of data to its volume-wise labels after training. The multi-scale context and multi-scale deep supervision strategies are adopted, to enhance feature learning. The deep neural network is an encoder (contracting path)-decoder (expansive path) architecture. The encoder is focused on feature learning, while the decoder is used to generate the segmentation results. Skip connection is employed to recover spatial context loss in the down-sampling path. To further boost feature learning in the contracting path, multi-scale contextual information is incorporated. Two down-scaled branch classifiers are inserted into the network to alleviate the potential gradient vanishing problem. Thus, more efficient gradients can be back-propagated from loss function to the shallow layers.</p>
    </sec>
    <sec id="sec0021">
      <label>3.12</label>
      <title>University of Edinburgh (UOE*)</title>
      <p id="p0042"><xref rid="bib0042" ref-type="bibr">Wang and Smedby (2017)</xref> develop a two-stage concatenated U-Net framework that simultaneously detects an ROI of the heart and classifies pixels into different substructures without losing the original resolution. The first U-Net uses a down-sampled 3D volume to produce a coarse prediction of the pixel labels, which is then re-sampled to the original resolution. The architecture of the second U-Net is inspired by the super-resolution CNN (SRCNN) (<xref rid="bib0006" ref-type="bibr">Dong et al., 2016</xref>) with skipping connections and recursive units (<xref rid="bib0016" ref-type="bibr">Kim et al., 2016</xref>). It inputs a two-channel 4D volume, consisting of the output of the first U-Net and the original data. In the test phase, a dynamic-tile layer is introduced between the two U-Nets to crop an ROI from both the input and output volume of the first U-Net. This layer is removed when performing an end-to-end training to simplify the implementation. Unlike the other U-Net based architecture, the proposed method can directly perform a prediction on the images with their original resolutions, thanks to the SRCNN-like network architecture.</p>
    </sec>
  </sec>
  <sec id="sec0022">
    <label>4</label>
    <title>Results</title>
    <p id="p0043"><xref rid="tbl0003" ref-type="table">Tables 3</xref> and <xref rid="tbl0004" ref-type="table">4</xref> present the quantitative results of the evaluated algorithms on the CT and MRI datasets, respectively. The mean Dice scores of the evaluated methods for MM-WHS are respectively 0.872 ± 0.087 (CT) and 0.824 ± 0.102 (MRI), and the mean HDs are respectively 37.684 ± 17.026 mm (CT) and 39.209 ± 23.435 mm (MRI). In general, the evaluated algorithms obtain better WHS accuracies for CT than for MRI, using the four metrics. <xref rid="sec0025" ref-type="sec">Section 5.2</xref> provides a discussion of the difference between modalities.<table-wrap position="float" id="tbl0003"><label>Table 3</label><caption><p>Results of the ten evaluated algorithms on CT dataset.</p></caption><alt-text id="at0010">Table 3</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Teams</th><th align="left" valign="top">Dice</th><th align="left" valign="top">Jaccard</th><th align="left" valign="top">SD (mm)</th><th align="left" valign="top">HD (mm)</th><th align="left" valign="top">DL/MAS</th></tr></thead><tbody><tr><td valign="top">GUT</td><td valign="top"><bold>0.908 ± 0.086</bold></td><td valign="top"><bold>0.832 ± 0.037</bold></td><td valign="top"><bold>1.117 ± 0.250</bold></td><td valign="top"><bold>25.242 ± 10.813</bold></td><td valign="top">DL</td></tr><tr><td valign="top">KTH</td><td valign="top">0.894 ± 0.030</td><td valign="top">0.810 ± 0.048</td><td valign="top">1.387 ± 0.516</td><td valign="top">31.146 ± 13.203</td><td valign="top">DL</td></tr><tr><td valign="top">CUHK1</td><td valign="top">0.890 ± 0.049</td><td valign="top">0.805 ± 0.074</td><td valign="top">1.432 ± 0.590</td><td valign="top">29.006 ± 15.804</td><td valign="top">DL</td></tr><tr><td valign="top">CUHK2</td><td valign="top">0.886 ± 0.047</td><td valign="top">0.798 ± 0.072</td><td valign="top">1.681 ± 0.593</td><td valign="top">41.974 ± 16.287</td><td valign="top">DL</td></tr><tr><td valign="top">UCF</td><td valign="top">0.879 ± 0.079</td><td valign="top">0.792 ± 0.106</td><td valign="top">1.538 ± 1.006</td><td valign="top">28.481 ± 11.434</td><td valign="top">DL</td></tr><tr><td valign="top">SEU</td><td valign="top">0.879 ± 0.023</td><td valign="top">0.784 ± 0.036</td><td valign="top">1.705 ± 0.399</td><td valign="top">34.129 ± 12.528</td><td valign="top">MAS</td></tr><tr><td valign="top">SIAT</td><td valign="top">0.849 ± 0.061</td><td valign="top">0.742 ± 0.086</td><td valign="top">1.925 ± 0.924</td><td valign="top">44.880 ± 16.084</td><td valign="top">DL</td></tr><tr><td valign="top">UT</td><td valign="top">0.838 ± 0.152</td><td valign="top">0.742 ± 0.161</td><td valign="top">4.812 ± 13.604</td><td valign="top">34.634 ± 12.351</td><td valign="top">MAS</td></tr><tr><td valign="top">UB1*</td><td valign="top">0.887 ± 0.030</td><td valign="top">0.798 ± 0.048</td><td valign="top">1.443 ± 0.302</td><td valign="top">55.426 ± 10.924</td><td valign="top">DL</td></tr><tr><td valign="top">UOE*</td><td valign="top">0.806 ± 0.159</td><td valign="top">0.697 ± 0.166</td><td valign="top">4.197 ± 7.780</td><td valign="top">51.922 ± 17.482</td><td valign="top">DL</td></tr><tr><td align="left" rowspan="3" valign="top">Average</td><td valign="top">0.859 ± 0.108</td><td valign="top">0.763 ± 0.118</td><td valign="top">3.259 ± 9.748</td><td valign="top">34.382 ± 12.468</td><td valign="top">MAS</td></tr><tr><td valign="top">0.875 ± 0.083</td><td valign="top">0.784 ± 0.010</td><td valign="top">1.840 ± 2.963</td><td valign="top">38.510 ± 17.890</td><td valign="top">DL</td></tr><tr><td valign="top">0.872 ± 0.087</td><td valign="top">0.780 ± 0.102</td><td valign="top">2.124 ± 5.133</td><td valign="top">37.684 ± 17.026</td><td valign="top">ALL</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl0004"><label>Table 4</label><caption><p>Results of the eleven evaluated algorithms on MRI dataset.</p></caption><alt-text id="at0011">Table 4</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Teams</th><th align="left" valign="top">Dice</th><th align="left" valign="top">Jaccard</th><th align="left" valign="top">SD (mm)</th><th align="left" valign="top">HD (mm)</th><th align="left" valign="top">DL/MAS</th></tr></thead><tbody><tr><td valign="top">UOL</td><td valign="top">0.870 ± 0.035</td><td valign="top">0.772 ± 0.054</td><td valign="top">1.700 ± 0.649</td><td valign="top"><bold>28.535 ± 13.220</bold></td><td valign="top">MAS</td></tr><tr><td valign="top">GUT</td><td valign="top">0.863 ± 0.043</td><td valign="top">0.762 ± 0.064</td><td valign="top">1.890 ± 0.781</td><td valign="top">30.227 ± 14.046</td><td valign="top">DL</td></tr><tr><td valign="top">KTH</td><td valign="top">0.855 ± 0.069</td><td valign="top">0.753 ± 0.094</td><td valign="top">1.963 ± 1.012</td><td valign="top">30.201 ± 13.216</td><td valign="top">DL</td></tr><tr><td valign="top">UCF</td><td valign="top">0.818 ± 0.096</td><td valign="top">0.701 ± 0.118</td><td valign="top">3.040 ± 3.097</td><td valign="top">40.092 ± 21.119</td><td valign="top">DL</td></tr><tr><td valign="top">UT</td><td valign="top">0.817 ± 0.059</td><td valign="top">0.695 ± 0.081</td><td valign="top">2.420 ± 0.925</td><td valign="top">30.938 ± 12.190</td><td valign="top">MAS</td></tr><tr><td valign="top">CUHK2</td><td valign="top">0.810 ± 0.071</td><td valign="top">0.687 ± 0.091</td><td valign="top">2.385 ± 0.944</td><td valign="top">33.101 ± 13.804</td><td valign="top">DL</td></tr><tr><td valign="top">CUHK1</td><td valign="top">0.783 ± 0.097</td><td valign="top">0.653 ± 0.117</td><td valign="top">3.233 ± 1.783</td><td valign="top">44.837 ± 15.658</td><td valign="top">DL</td></tr><tr><td valign="top">SIAT</td><td valign="top">0.674 ± 0.182</td><td valign="top">0.532 ± 0.178</td><td valign="top">9.776 ± 6.366</td><td valign="top">92.889 ± 18.001</td><td valign="top">DL</td></tr><tr><td valign="top">UB2*</td><td valign="top"><bold>0.874 ± 0.039</bold></td><td valign="top"><bold>0.778 ± 0.060</bold></td><td valign="top"><bold>1.631 ± 0.580</bold></td><td valign="top">28.995 ± 13.030</td><td valign="top">DL</td></tr><tr><td valign="top">UB1*</td><td valign="top">0.869 ± 0.058</td><td valign="top">0.773 ± 0.079</td><td valign="top">1.757 ± 0.814</td><td valign="top">30.018 ± 14.156</td><td valign="top">DL</td></tr><tr><td valign="top">UOE*</td><td valign="top">0.832 ± 0.081</td><td valign="top">0.720 ± 0.105</td><td valign="top">2.472 ± 1.892</td><td valign="top">41.465 ± 16.758</td><td valign="top">DL</td></tr><tr><td align="left" rowspan="3" valign="top">Average</td><td valign="top">0.844 ± 0.047</td><td valign="top">0.734 ± 0.072</td><td valign="top">2.060 ± 0.876</td><td valign="top">29.737 ± 12.771</td><td valign="top">MAS</td></tr><tr><td valign="top">0.820 ± 0.107</td><td valign="top">0.707 ± 0.127</td><td valign="top">3.127 ± 3.640</td><td valign="top">41.314 ± 24.711</td><td valign="top">DL</td></tr><tr><td valign="top">0.824 ± 0.102</td><td valign="top">0.711 ± 0.125</td><td valign="top">2.933 ± 3.339</td><td valign="top">39.209 ± 23.435</td><td valign="top">ALL</td></tr></tbody></table></table-wrap></p>
    <p id="p0044">For the CT data, the results are generally promising. The best Dice score (0.908 ± 0.086) and the best HD (25.242 ± 10.813 mm) were both achieved by GUT, which is a DL-based algorithm with anatomical label configurations. For the MRI data, the best Dice score (0.874 ± 0.039) was obtained by UB2*, which is a DL-based method and a delayed submission; and the best HD (28.535 ± 13.220 mm) was achieved by UOL, an MAS-based algorithm. Here, the average accuracy of MAS (two teams) was better than that of the DL-based segmentation (nine teams) in all evaluation metrics. However, the number of MAS-based approaches is limited, namely two, and the performance across different DL methods was variable, similar to the results from the CT experiment. For example, the top four DL methods by Dice scores, i.e., GUT, KTH, UB1* and UB2*, achieved comparable mean Dice scores to that of UOL (<italic>p</italic>=0.157, <italic>p</italic>=0.073, <italic>p</italic>=0.903 and <italic>p</italic>=0.448), but the other DL approaches generated much poorer results (<italic>p</italic> &lt; 0.001). The discussion of different methodologies will be given in <xref rid="sec0027" ref-type="sec">Section 5.4</xref>.</p>
    <p id="p0045"><xref rid="fig0004" ref-type="fig">Fig. 4</xref> shows the boxplots of the evaluated algorithms on CT data. One can see that they achieved relatively accurate segmentation for all substructures of the heart, except for the PA whose variability in terms of shape and appearance is notably greater. For GUT, KTH, CUHK1, UB1*, and CUHK2, the delineation of PA is reasonably good with the mean Dice score larger than 0.80. <xref rid="fig0005" ref-type="fig">Fig. 5</xref> presents the boxplots on the MRI data. The five methods, i.e., UB2*, UOL, UB1*, GUT, and KTH, all demonstrate good Dice scores on the segmentation of four chambers and LV myocardium. Similar to the conclusion drawn from <xref rid="tbl0003" ref-type="table">Tables 3</xref> and <xref rid="tbl0004" ref-type="table">4</xref>, the segmentation on the CT images is generally better than that on the MRI data as indicated by the quantitative evaluation metrics.<fig id="fig0004"><label>Fig. 4</label><caption><p>Boxplot of Dice scores of the whole heart segmentation on CT dataset by the ten methods.</p></caption><alt-text id="at0004">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig><fig id="fig0005"><label>Fig. 5</label><caption><p>Boxplot of Dice scores of the whole heart segmentation on MRI dataset by the eleven methods.</p></caption><alt-text id="at0005">Fig. 5</alt-text><graphic xlink:href="gr5"/></fig></p>
    <p id="p0046"><xref rid="fig0006" ref-type="fig">Fig. 6</xref> shows the 3D visualization of the cases with the median and worst WHS Dice scores by the evaluated methods on the CT data. Most of the median cases look reasonably good, though some contain patchy noise; and the worst cases require significant improvements. Specifically, UOE* median case contains significant amount of misclassification in AO, and parts of the LV are labeled as LA in the UOE* and SIAT median cases. In the worst cases, the CUHK1 and CUHK2 results do not have a complete shape of the RV; KTH and SIAT contain a large amount of misclassification, particularly in myocardium; UCF mistakes the RA as LV; UOE* only segments the LA, and UT generates a result with wrong orientation.<fig id="fig0006"><label>Fig. 6</label><caption><p>3D visualization of the WHS results of the median and worse cases in the CT test dataset by the ten evaluated methods. The color bar indicates the correspondence of substructures. Note that the colors of Myo and LV in 3D visualization do not look exactly the same as the keys in the color bar, due to the 50% transparency setting for Myo rendering and the addition effect from two colors (LV and 50% Myo) for LV rendering, respectively. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</p></caption><alt-text id="at0006">Fig. 6</alt-text><graphic xlink:href="gr6"/></fig></p>
    <p id="p0047"><xref rid="fig0007" ref-type="fig">Fig. 7</xref> visualizes the median and worst results on MRI WHS. Compared with the CT results, even the median cases of MRI cases are poor. For example, the SIAT method could perform well on most of the CT cases, but failed to generate acceptable results for most of the MRI images, including the median cases presented in the figure. The worst cases of UOE*, CUHK2 and UB1 miss at least one substructure, and UCF and SIAT results do not contain any complete substructure of the whole heart. In conclusion, the CT segmentation results look better than the MRI results, which is consistent with the quantitative results. Also, one can see from <xref rid="fig0006" ref-type="fig">Figs. 6</xref> and <xref rid="fig0007" ref-type="fig">7</xref> that the resulting shape from the two MAS-based methods looks more realistic, even though the segmentation could sometimes be very poor or even a failure, such as the worst MRI case by UOL and the worst CT case by UT.<fig id="fig0007"><label>Fig. 7</label><caption><p>3D visualization of the WHS results of the median and worse cases in the MRI test dataset by the eleven evaluated methods.</p></caption><alt-text id="at0007">Fig. 7</alt-text><graphic xlink:href="gr7"/></fig></p>
    <p id="p0048">The computational complexity of a DL method in the testing stage is related to the complexity of the network. In addition, DL methods can be implemented with the help of a GPU. Hence, the WHS of a case can be done within seconds or a minute on average. By contrast, the conventional approaches are commonly implemented with iterated optimization procedures, such as the atlas-to-target registration in the MAS, and thus could be computationally expensive. However, due to the difference of implementation and hardware an objective comparison between the evaluated methods can be difficult. For reference, we summarize the information regarding to the implementation details and their average run time in <xref rid="tbl0005" ref-type="table">Table 5</xref>.<table-wrap position="float" id="tbl0005"><label>Table 5</label><caption><p>Details on the average run time and computer systems used for the evaluated methods. T: average run time; Proc: average run time includes the pre- and post-processing of the images for the DL-based methods.</p></caption><alt-text id="at0012">Table 5</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Teams</th><th align="left" valign="top">T (MRI)</th><th align="left" valign="top">T (CT)</th><th align="left" valign="top">Proc</th><th align="left" valign="top">GPU</th><th align="left" valign="top">CPU and RAM</th><th align="left" valign="top">Programming language</th></tr></thead><tbody><tr><td valign="top">GUT</td><td valign="top">21 s</td><td valign="top">104 s</td><td valign="top">Y</td><td valign="top">GTX TITAN X; 12GB</td><td valign="top">Intel i7-4820K; 32GB</td><td valign="top">Python, C++</td></tr><tr><td valign="top">UOL</td><td valign="top">N/A</td><td valign="top">N/A</td><td valign="top">N/A</td><td valign="top">N/A</td><td valign="top">N/A</td><td valign="top">N/A</td></tr><tr><td valign="top">KTH</td><td valign="top">7 min</td><td valign="top">5 min</td><td valign="top">Y</td><td valign="top">GTX1080; 8GB</td><td valign="top">Intel Xeon E5 1620; 32GB</td><td valign="top">Python, C++</td></tr><tr><td valign="top">CUHK1</td><td valign="top">68.55 s</td><td valign="top">87.38 s</td><td valign="top">N</td><td valign="top">TITAN X (PASCAL); 12GB</td><td valign="top">Intel i5-6500; 16GB</td><td valign="top">Python + TensorFlow</td></tr><tr><td valign="top">SEU</td><td valign="top">N/A</td><td valign="top">20 min</td><td valign="top">N/A</td><td valign="top">N/A</td><td valign="top">Intel 7900X; 16G</td><td valign="top">Python + Elastix</td></tr><tr><td valign="top">CUHK2</td><td valign="top">66.03 s</td><td valign="top">89.79 s</td><td valign="top">N</td><td valign="top">TITAN X (PASCAL); 12GB</td><td valign="top">Intel i5-6500; 16GB</td><td valign="top">Python + TensorFlow</td></tr><tr><td valign="top">UCF</td><td valign="top">17 s</td><td valign="top">50 s</td><td valign="top">N</td><td valign="top">TITAN XP; 12GB</td><td valign="top">Intel Xeon E5-2630 v3; N/A</td><td valign="top">Python + TensorFlow</td></tr><tr><td valign="top">UT</td><td valign="top">14 min</td><td valign="top">21 min</td><td valign="top">N/A</td><td valign="top">N/A</td><td valign="top">Intel Core i7-4600; 16GB</td><td valign="top">C++, Cli</td></tr><tr><td valign="top">SIAT</td><td valign="top">7 s</td><td valign="top">11 s</td><td valign="top">N</td><td valign="top">GTX TITAN X; 12GB</td><td valign="top">Intel Core i5-7640X; 32GB</td><td valign="top">Python</td></tr><tr><td valign="top">UB2*</td><td valign="top">30 s</td><td valign="top">N/A</td><td valign="top">N</td><td valign="top">GTX 1080 Ti; 11GB</td><td valign="top">Intel(R) i7; 32GB</td><td valign="top">Python + TensorFlow</td></tr><tr><td valign="top">UB1*</td><td valign="top">28 s</td><td valign="top">23 s</td><td valign="top">N</td><td valign="top">GTX 1080 Ti; 11GB</td><td valign="top">Intel(R) i7; 32GB</td><td valign="top">Python + TensorFlow</td></tr><tr><td valign="top">UOE*</td><td valign="top">0.11 s</td><td valign="top">0.22 s</td><td valign="top">N</td><td valign="top">Telsa K80; 24GB</td><td valign="top">Intel Xeon E5-2686 v4; 64GB</td><td valign="top">Python + TensorFlow</td></tr></tbody></table></table-wrap></p>
  </sec>
  <sec id="sec0023">
    <label>5</label>
    <title>Discussion</title>
    <sec id="sec0024">
      <label>5.1</label>
      <title>Overall performance of the evaluated algorithms</title>
      <p id="p0049">The segmentation accuracies reported for the four chambers are generally good, but the segmentation of the other substructures demonstrates more challenges. For example, one can see from <xref rid="fig0004" ref-type="fig">Figs. 4</xref> and <xref rid="fig0005" ref-type="fig">5</xref> that in CT WHS the PA segmentation is much poorer compared to other substructures; in MRI WHS, the segmentation of myocardium, AO and PA appears to be more difficult. One reason could be that these regions have much larger variation in terms of shapes and image appearance across different scans. Particularly, the diverse pathologies can result in heterogeneous intensity of the myocardium and the blood, please refer to <xref rid="sec0026" ref-type="sec">Section 5.3</xref> for a detailed discussion.</p>
      <p id="p0050">Another reason could be the ambiguity in the manual delineations which are used as the ground truth for training of learning-based algorithms. This is likely to be greater for MR data than CT, as the image quality of whole heart MRI is generally lower (poorer contrast and signal-to-noise ratio). <xref rid="tbl0006" ref-type="table">Table 6</xref> shows the inter- and intra-observer variabilities in the manual delineation derived from a subset of MRI data. These are computed from the mean of 6 subjects (for inter-observer) and 4 subjects (for intra-observer), respectively. The inter-observer variabilities are comparable to the mean dice scores of the highest ranked methods (UOL and UB2*) in <xref rid="tbl0004" ref-type="table">Table 4</xref>. Furthermore, the observer variation studies confirm that it can be more challenging to achieve consistent segmentation results on certain substructures, even for experienced observers. For example, in the variation studies the mean Dice scores of PA and Myo are much worse than those of the other substructures, particularly of LV and RV, which agrees with the different performance of the automatic methods in these substructures. Note that each of the gold standard segmentation used in this work was done by one rater, which is a limitation since the variability of manual segmentation between observers could be considerably large.<table-wrap position="float" id="tbl0006"><label>Table 6</label><caption><p>The inter-observer (Inter-Ob) and intra-observer (Intra-Ob) variabilities of the MRI segmentation in Dice scores (%).</p></caption><alt-text id="at0013">Table 6</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" valign="top">LV</th><th align="left" valign="top">Myo</th><th align="left" valign="top">RV</th><th align="left" valign="top">LA</th></tr></thead><tbody><tr><td valign="top">Inter-Ob</td><td valign="top">93.7 ± 1.33</td><td valign="top">81.1 ± 2.90</td><td valign="top">90.1 ± 1.96</td><td valign="top">83.7 ± 4.58</td></tr><tr><td valign="top">Intra-Ob</td><td valign="top">94.2 ± 0.84</td><td valign="top">83.9 ± 1.23</td><td valign="top">91.2 ± 2.59</td><td valign="top">86.8 ± 3.23</td></tr><tr><td/><td valign="top">RA</td><td valign="top">AO</td><td valign="top">PA</td><td valign="top">WHS</td></tr><tr><td valign="top">Inter-Ob</td><td valign="top">85.8 ± 3.10</td><td valign="top">87.6 ± 5.24</td><td valign="top">76.3 ± 14.34</td><td valign="top">87.8 ± 1.36</td></tr><tr><td valign="top">Intra-Ob</td><td valign="top">87.2 ± 2.48</td><td valign="top">91.1 ± 1.65</td><td valign="top">82.6 ± 3.77</td><td valign="top">89.5 ± 1.03</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec0025">
      <label>5.2</label>
      <title>Discussion of different modalities: CT versus MRI</title>
      <p id="p0051">The MRI WHS is generally more challenging than the CT WHS, which is confirmed by the results presented in this work. The mean generalized Dice score of CT WHS is evidently better than that of MRI WHS averaged from the evaluated algorithms, namely 0.872 ± 0.087 (CT) versus 0.824 ± 0.102 (MRI), and <italic>p</italic>-value is 0.011 after the false discovery rate (FDR) correction (<xref rid="bib0002" ref-type="bibr">Benjamini et al., 2001</xref>). There is a significant difference between the mean HDs of the two modalities, namely 34.382 ± 12.468 mm (CT) versus 39.209 ± 23.435 mm (MRI) (<italic>p</italic> &lt; 0.01, after FDR correction). One can further confirm this by comparing the results for these two tasks in <xref rid="tbl0003" ref-type="table">Tables 3</xref> and <xref rid="tbl0004" ref-type="table">4</xref>, as nine methods have been evaluated on both of the CT and MRI test data, and the same algorithms generally achieve better accuracies for CT data. Similar conclusion can also be drawn for the individual substructures as well as for the whole heart, when one compares the boxplots of segmentation Dice scores between <xref rid="fig0004" ref-type="fig">Figs. 4</xref> and <xref rid="fig0005" ref-type="fig">5</xref>.</p>
    </sec>
    <sec id="sec0026">
      <label>5.3</label>
      <title>Discussion of different pathologies</title>
      <p id="p0052">The pathologies of patients in this study cover a wide range of cardiac diseases. In particular, the MRI data include patients with CHD and AF, in whom the heart shape and size can vary considerably and in whom image quality can be more variable. We have therefore categorized the pathologies into three subgroups, i.e., CHD, AF and <italic>Others</italic>, and discuss the WHS performance for each.</p>
      <p id="p0053">The average WHS Dice scores of the evaluated methods on these three categories were respectively 0.817 ± 0.129 (CHD), 0.790 ± 0.103 (AF), 0.853 ± 0.072 (<italic>Others</italic>), as presented in <xref rid="tbl0007" ref-type="table">Table 7</xref>. The <italic>p</italic>-values of the WHS Dice scores after FDR correction are as follows, <italic>p</italic>=0.001 between AF and CHD, <italic>p</italic>=0.005 between AF and <italic>Others</italic>, and <italic>p</italic>=0.017 between CHD and <italic>Others</italic>, indicating significant difference between these categories. One can see that the WHS result from the category of <italic>Others</italic> was evidently better than the other two with statistical significance.<table-wrap position="float" id="tbl0007"><label>Table 7</label><caption><p>The performance of each substructure and WHS on different pathologies of the MRI in Dice scores (%).</p></caption><alt-text id="at0014">Table 7</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" valign="top">LV</th><th align="left" valign="top">Myo</th><th align="left" valign="top">RV</th><th align="left" valign="top">LA</th></tr></thead><tbody><tr><td valign="top">AF</td><td valign="top">80.4 ± 17.9</td><td valign="top">71.8 ± 13.4</td><td valign="top">71.5 ± 15.8</td><td valign="top">84.4 ± 9.7</td></tr><tr><td valign="top">CHD</td><td valign="top">85.5 ± 16.6</td><td valign="top">69.6 ± 18.1</td><td valign="top">87.5 ± 11.3</td><td valign="top">78.2 ± 18.4</td></tr><tr><td valign="top"><italic>Others</italic></td><td valign="top">91.2 ± 7.7</td><td valign="top">79.6 ± 8.2</td><td valign="top">88.6 ± 7.7</td><td valign="top">81.9 ± 1.14</td></tr><tr><td/><td valign="top">RA</td><td valign="top">AO</td><td valign="top">PA</td><td valign="top">WHS</td></tr><tr><td valign="top">AF</td><td valign="top">84.7 ± 10.1</td><td valign="top">76.5 ± 18.3</td><td valign="top">71.4 ± 20.7</td><td valign="top">79.0 ± 10.3</td></tr><tr><td valign="top">CHD</td><td valign="top">83.5 ± 11.3</td><td valign="top">80.9 ± 10.3</td><td valign="top">67.7 ± 23.4</td><td valign="top">81.7 ± 12.9</td></tr><tr><td valign="top"><italic>Others</italic></td><td valign="top">81.2 ± 15.7</td><td valign="top">83.4 ± 13.4</td><td valign="top">73.4 ± 15.5</td><td valign="top">85.3 ± 7.2</td></tr></tbody></table></table-wrap></p>
      <p id="p0054">For the CHD cohort, the evaluated methods tended to achieve less accurate results, especially in the substructures of LA, Myo and PA, probably due to large shape variations of the heart in these patients. For the AF patients, because of the irregular heart rhythm and shortness of breath, the image quality can be degraded, which could result in less accurate WHS results for the evaluated methods. Interestingly, we have found that the LA segmentation from AF patients was particularly more accurate (<italic>p</italic>=0.007, after FDR correction), and the ventricle segmentation, i.e., LV and RV, was much worse (<italic>p</italic>=0.025 and <italic>p</italic>=0.012, after FDR correction). This could be owing to the fact that the LA was larger for AF patients, and therefore could be easier to be recognized from the images by the algorithms. We therefore conclude that the segmentation of the substructures for different categories of patients can present different levels of challenges due to the difference in image quality and shape variations of the heart.</p>
    </sec>
    <sec id="sec0027">
      <label>5.4</label>
      <title>Discussion of different methodologies</title>
      <p id="p0055">As <xref rid="tbl0003" ref-type="table">Tables 3</xref> and <xref rid="tbl0004" ref-type="table">4</xref> summarize, 9 out of the 11 benchmarked CT WHS methods and 8 out of the 10 MRI WHS algorithms are based on deep neural networks. Overall, the DL-based approaches have shown great potentials, particularly in CT WHS. However, several reported poor results based on their mean HDs as well as Dice scores, such as SIAT, UB1* and UOE* for CT WHS, and UCF, CUHK1 and SIAT for MRI WHS. The boxplots of Dice scores in <xref rid="fig0004" ref-type="fig">Figs. 4</xref> and <xref rid="fig0005" ref-type="fig">5</xref> confirm that some of the DL methods have very large interquartile ranges and outliers. <xref rid="fig0006" ref-type="fig">Figs. 6</xref> and <xref rid="fig0007" ref-type="fig">7</xref> visualize the 3D segmentation results of the median and the worst cases of each method. One can see that the resulting heart shapes of several cases are totally unrealistic, such as the worst CT case of UOE*, the median and worst MRI cases of SIAT, and the worst MRI cases of CUHK1 and UCF.</p>
      <p id="p0056">The performance of the DL methods could vary greatly across different network structures and training strategies, as summarized in <xref rid="tbl0008" ref-type="table">Table 8</xref>. One can see that most of the DL-based approaches are 3D-based networks, except for KTH and UCF, which were based on multi-view 2D networks. The performance of 2D networks was comparable to that of 3D networks. For example, no significant difference was found between KTH and the two top performing 3D network-based methods, i.e., UB1* and GUT, in the MRI WHS Dice scores, as neither of the <italic>p</italic>-values was less than 0.1. This may be owing to the increased number of training data by using 2D networks, since one 3D image can be split into tens to hundreds of 2D slices. Additionally, the DL-based approaches are generally based on U-Net or FCN, except for UCF and UB2* which were based on encoder-decoder CNN. Two teams, i.e., CUHK1 and CUHK2, employed a pre-trained network to avoid overfitting. However, no significant difference was found between the different network architectures in terms of WHS Dice scores, and neither was there between the methods using pre-trained models and those which did not, as none of the <italic>p</italic>-values was less than 0.5. Only one team, i.e., KTH, embedded shape priors into the deep learning framework, which demonstrated good potential in improving the segmentation performance. Finally, SIAT was the only method to train the network using both the CT and MRI data, but the resulting network did not perform well. Hence, it is still an open question in terms of how to improve the generalization ability of a segmentation network by using multi-modality training data.<table-wrap position="float" id="tbl0008"><label>Table 8</label><caption><p>Summary of the DL-based methods. The abbreviations are as follows, Dim: dimension; MS: multi-stage; E-D: encode-decode CNN; MM-train: trained on multi-modality datasets.</p></caption><alt-text id="at0015">Table 8</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Teams</th><th align="left" valign="top">Dim</th><th align="left" valign="top">MS</th><th align="left" valign="top">Network</th><th align="left" valign="top">Prior</th><th align="left" valign="top">Pre-train</th><th align="left" valign="top">MM-train</th></tr></thead><tbody><tr><td valign="top">GUT</td><td valign="top">3D</td><td valign="top">Y</td><td valign="top">U-Net</td><td valign="top">N</td><td valign="top">N</td><td valign="top">N</td></tr><tr><td valign="top">KTH</td><td valign="top">2D</td><td valign="top">Y</td><td valign="top">U-Net</td><td valign="top">Y</td><td valign="top">N</td><td valign="top">N</td></tr><tr><td valign="top">CUHK1</td><td valign="top">3D</td><td valign="top">N</td><td valign="top">FCN</td><td valign="top">N</td><td valign="top">Y</td><td valign="top">N</td></tr><tr><td valign="top">CUHK2</td><td valign="top">3D</td><td valign="top">N</td><td valign="top">FCN</td><td valign="top">N</td><td valign="top">Y</td><td valign="top">N</td></tr><tr><td valign="top">UCF</td><td valign="top">2D</td><td valign="top">N</td><td valign="top">E-D</td><td valign="top">N</td><td valign="top">N</td><td valign="top">N</td></tr><tr><td valign="top">SIAT</td><td valign="top">3D</td><td valign="top">Y</td><td valign="top">U-Net</td><td valign="top">N</td><td valign="top">N</td><td valign="top">Y</td></tr><tr><td valign="top">UB2*</td><td valign="top">3D</td><td valign="top">N</td><td valign="top">E-D</td><td valign="top">N</td><td valign="top">N</td><td valign="top">N</td></tr><tr><td valign="top">UB1*</td><td valign="top">3D</td><td valign="top">N</td><td valign="top">FCN</td><td valign="top">N</td><td valign="top">N</td><td valign="top">N</td></tr><tr><td valign="top">UOE*</td><td valign="top">3D</td><td valign="top">Y</td><td valign="top">U-Net</td><td valign="top">N</td><td valign="top">N</td><td valign="top">N</td></tr></tbody></table></table-wrap></p>
      <p id="p0057">The conventional methods, mainly based on MAS in the evaluated methods, could generate stable results with more realistic shapes, though they were not necessarily competitive in terms of mean accuracies and computation efficiency. Particularly, in MRI WHS the MAS-based methods achieved no worse mean accuracies compared to the DL-based approaches, though only two MAS methods were submitted for evaluation. Finally, the advantages and potential limitations of all the evaluated methods are summarized in <xref rid="tbl0009" ref-type="table">Table 9</xref>.<table-wrap position="float" id="tbl0009"><label>Table 9</label><caption><p>Summary of the advantages and limitations of the twelve evaluated methods.</p></caption><alt-text id="at0016">Table 9</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Method</th><th align="left" valign="top">Strengths</th><th align="left" valign="top">Limitations</th></tr></thead><tbody><tr><td valign="top">GUT</td><td valign="top">- Combining localization and segmentation CNNs to reduce the requirements of memory and computation time.<break/>- Good segmentation performance for both CT and MRI.</td><td valign="top">- Based on an automatically localized landmark in the center of the heart, the cropping of a fixed physical size ROI is required for segmentation.</td></tr><tr><td valign="top">UOL</td><td valign="top">- The discrete registration can capture large shape variations across scans. <break/>- The regularization is used to obtain smooth surfaces that are important for mesh generation and motion or electrophysiological modelling.</td><td valign="top">- Only tested on the MRI data.<break/>- The automatic cropping of ROI sometimes do not cover the whole heart.</td></tr><tr><td valign="top">KTH</td><td valign="top">- Combining shape context information with orthogonal U-Nets for more consistent segmentation in 3-D views.<break/>- Good segmentation performance, particularly for CT.</td><td valign="top">- Potential of overfitting because the U-Nets rely much on the shape context channels.<break/>- Weighting factors of the shape context generation are determined empirically.</td></tr><tr><td valign="top">CUHK1</td><td valign="top">- Pre-trained 3-D Network provides good initialization and reduces overfitting.<break/>- Auxiliary loss functions are used to promote gradient flow and ease the training procedure.<break/>- Tackling the class-imbalance problem using a multi-class Dice based metric.</td><td valign="top">- The introduced hyperparameters need determining empirically.<break/>- Relatively poor performance in MRI WHS.</td></tr><tr><td valign="top">UCF</td><td valign="top">- Multi-planar information reinforce the segmentation along the three orthogonal planes.<break/>- Multiple 3-D CNNs require less memory compared to a 3-D CNN.</td><td valign="top">- The softmax function in the last layer could cause information loss due to class normalization.</td></tr><tr><td valign="top">CUHK2</td><td valign="top">- Coupling the 3-D FCN with transfer learning and deep supervision mechanism to tackle potential training difficulties caused by overfitting and vanishing gradient.<break/>- Enhance local contrast and reduce the image inhomogeneity.</td><td valign="top">- Relatively poor performance in MRI WHS.</td></tr><tr><td valign="top">SEU</td><td valign="top">- Three-step multi-atlas image registration method is lightweight for computing resources.<break/>- The method can be easily deployed.</td><td valign="top">- Only tested on the CT data.</td></tr><tr><td valign="top">UT</td><td valign="top">- The proposed incremental segmentation method is based on local atlases and allows users to perform partial and incremental segmentation.</td><td valign="top">- The registration of MRI atlas can be inaccurate, and the evaluated segmentation accuracy is low.</td></tr><tr><td valign="top">SIAT</td><td valign="top">- Combining a 3-D U-Net with a ROI detection to alleviate the impact of surrounding tissues and reduce the computational complexity.<break/>- Fusing MRI and CT images to increase the training samples and take full advantage of multimodality information so that features of different substructures can be better extracted.</td><td valign="top">- Poor segmentation performance, particularly for MRI data.</td></tr><tr><td valign="top">UB1*</td><td valign="top">- The focal loss and Dice loss are well encapsulated into a complementary learning objective to segment both hard and easy classes.</td><td valign="top">- Late submission of the WHS results.</td></tr><tr><td valign="top">UB2*</td><td valign="top">- Multi-scale context and multi-scale deep supervision are employed to enhance feature learning and to alleviate the potential gradient vanishing problem during training.<break/>- Reliable performance on the tested MR data.</td><td valign="top">- Late submission of the WHS results.<break/>- Only tested on the MRI data.</td></tr><tr><td valign="top">UOE*</td><td valign="top">- The proposed two-stage U-Net framework can directly segment the images with their original resolution.</td><td valign="top">- Late submission of the WHS results.<break/>- Poor performance, particularly for CT data.</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec0028">
      <label>5.5</label>
      <title>Comparisons with the literature</title>
      <p id="p0058"><xref rid="tbl0001" ref-type="table">Table 1</xref> summarizes the WHS results from recent literature. Previous works were mainly based on conventional segmentation algorithms, such as MAS which achieved the most competitive performance before the introduction of DL-based methodologies. The best mean Dice score of WHS was around 0.90 for CT images from the literature, though it is important to notice that objective inter-work comparisons can be difficult, due to the difference in the evaluation metrics, implementations and study group pathologies. This is comparable to the results from the best performing methods in this challenge. For MRI data, both <xref rid="bib0055" ref-type="bibr">Zuluaga et al. (2013)</xref> and <xref rid="bib0054" ref-type="bibr">Zhuang and Shen (2016)</xref> reported a mean WHS Dice score of around 0.90, which is evidently better than the blinded evaluation results from this challenge. This could be attributed to the usage of the multi-modality atlases in the previous works, which improved the WHS by having more prior knowledge. By contrast, in this study only SIAT used multi-modality images to train their neural network, and the result was not promising. Hence, how to effectively train a neural network with multi-modality images remains an open question.</p>
      <p id="p0059"><xref rid="tbl0010" ref-type="table">Table 10</xref> summarizes the recent public datasets for cardiac segmentation, which mainly focus on specific substructures of the heart. <xref rid="bib0031" ref-type="bibr">Radau et al. (2009)</xref>, <xref rid="bib0036" ref-type="bibr">Suinesiaputra et al. (2011)</xref>, <xref rid="bib0030" ref-type="bibr">Petitjean et al. (2015)</xref> and <xref rid="bib0003" ref-type="bibr">Bernard et al. (2018)</xref> organized the challenges for segmenting the left, right or both ventricles. <xref rid="bib0021" ref-type="bibr">Moghari et al. (2016)</xref> organized a challenge for the segmentation of blood pool and myocardium from 3D MRI data. This work was aimed to offer pre-procedural planning of children with complex CHD. <xref rid="bib0015" ref-type="bibr">Karim et al. (2013)</xref>, <xref rid="bib0038" ref-type="bibr">Tobon-Gomez et al. (2015)</xref>, <xref rid="bib0014" ref-type="bibr">Karim et al. (2018)</xref> and <xref rid="bib0049" ref-type="bibr">Zhao and Xiong (2018)</xref> provided data for benchmarking algorithms of LA, LA wall, or LA scar segmentation for patients suffering from AF. <xref rid="bib0053" ref-type="bibr">Zhuang et al. (2019)</xref> organized a challenge for benchmarking the segmentation of ventricles and myocardium from multi-sequence cardiac MRI. Transfer learning or domain adaptation was particularly emphasized to achieve the segmentation of LGE MRI with the knowledge from other MRI sequences.<table-wrap position="float" id="tbl0010"><label>Table 10</label><caption><p>Summary of the previous challenges related to cardiac segmentation from MICCAI society.</p></caption><alt-text id="at0017">Table 10</alt-text><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Organizers/refernece</th><th align="left" valign="top">Year</th><th align="left" valign="top">Data</th><th align="left" valign="top">Target</th><th align="left" valign="top">Pathology</th></tr></thead><tbody><tr><td valign="top"><xref rid="bib0031" ref-type="bibr">Radau et al. (2009)</xref></td><td valign="top">2009</td><td valign="top">45 cine MRI</td><td valign="top">LV</td><td valign="top">hypertrophy, infarction</td></tr><tr><td valign="top"><xref rid="bib0036" ref-type="bibr">Suinesiaputra et al. (2011)</xref></td><td valign="top">2011</td><td valign="top">200 cine MRI</td><td valign="top">LV</td><td valign="top">myocardial infarction</td></tr><tr><td valign="top"><xref rid="bib0015" ref-type="bibr">Karim et al. (2013)</xref></td><td valign="top">2013</td><td valign="top">60 MRI</td><td valign="top">LA scar</td><td valign="top">atrial fibrillation</td></tr><tr><td valign="top"><xref rid="bib0030" ref-type="bibr">Petitjean et al. (2015)</xref></td><td valign="top">2012</td><td valign="top">48 cine MRI</td><td valign="top">RV</td><td valign="top">congenital heart disease</td></tr><tr><td valign="top"><xref rid="bib0038" ref-type="bibr">Tobon-Gomez et al. (2015)</xref></td><td valign="top">2013</td><td valign="top">30 CT + 30 MRI</td><td valign="top">LA</td><td valign="top">atrial fibrillation</td></tr><tr><td valign="top"><xref rid="bib0014" ref-type="bibr">Karim et al. (2018)</xref></td><td valign="top">2016</td><td valign="top">10 CT + 10 MRI</td><td valign="top">LA wall</td><td valign="top">atrial fibrillation</td></tr><tr><td valign="top"><xref rid="bib0021" ref-type="bibr">Moghari et al. (2016)</xref></td><td valign="top">2016</td><td valign="top">20 MRI</td><td valign="top">Blood pool, Myo</td><td valign="top">congenital heart disease</td></tr><tr><td valign="top"><xref rid="bib0003" ref-type="bibr">Bernard et al. (2018)</xref></td><td valign="top">2017</td><td valign="top">150 cine MRI</td><td valign="top">Ventricles</td><td valign="top">infarction, dilated/ hypertrophic</td></tr><tr><td/><td/><td/><td/><td valign="top">cardiomyopathy, abnormal RV</td></tr><tr><td valign="top"><xref rid="bib0049" ref-type="bibr">Zhao and Xiong (2018)</xref></td><td valign="top">2018</td><td valign="top">150 LGE-MRI</td><td valign="top">LA</td><td valign="top">atrial fibrillation</td></tr><tr><td valign="top"><xref rid="bib0053" ref-type="bibr">Zhuang et al. (2019)</xref></td><td valign="top">2019</td><td valign="top">45 multi-modal MRI</td><td valign="top">Ventricles</td><td valign="top">cardiomyopathy</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="sec0029">
      <label>5.6</label>
      <title>Progress and challenges</title>
      <p id="p0060">The MM-WHS challenge provides an open-access dataset and ongoing evaluation framework for researchers, to develop and compare their algorithms. Both the conventional methods and the new DL-based algorithms have made great progress, as shown in this paper. It is worth mentioning that the best performing DL methods have demonstrated great potential of generating accurate and reliable WHS results, such as GUT, UB1* and UB2*, even though they had limited training data (20 CT and 20 MRI). Despite this, there are limitations need to be overcome, particularly from the methodological point of view.</p>
      <p id="p0061">WHS of MRI is more challenging than that of CT. In general, the image quality of MRI data is poorer than that of CT data, in terms of contrast-to-noise ratio, signal-to-noise ratio and spatial resolution. In some patients, there can be blurring and/or ghosting artifacts due to the poorly corrected respiratory motion. In addition, the MRI datasets in this study included the particularly challenging cases from patients with CHD and AF. The former had large and challenging variations in cardiac anatomy, and the latter tented to have degraded image quality due to the irregular heart rhythm and shortness of breath of the patients. Enlarging the size of training data is commonly pursued to improve the learning-based segmentation algorithms. However, availability of whole heart training images can be difficult as well. One potential solution is to use artificial training data, such as by means of data augmentation or image synthesis using generative adversarial networks (<xref rid="bib0008" ref-type="bibr">Goodfellow et al., 2014</xref>). Furthermore, shape constraints can be incorporated into the training and prediction framework, which is particularly useful for the DL-based methods to avoid generating results of unrealistic shapes.</p>
    </sec>
  </sec>
  <sec id="sec0030">
    <label>6</label>
    <title>Conclusion</title>
    <p id="p0062">Knowledge of the detailed anatomy of the heart structure is clinically important as it is closely related to cardiac function and patient symptoms. Manual WHS is labor-intensive and also suffers from poor reproducibility. A fully automated multi-modality WHS is therefore highly in demand. However, achieving this goal is still challenging, mainly due to the variable quality of whole heart images, complex structure of the heart and large variation of the shape. This manuscript describes the MM-WHS challenge which provides 120 clinical CT/ MRI images, elaborates on the methodologies of twelve evaluated methods, and analyzes their results.</p>
    <p id="p0063">The challenge provides the same training data and test dataset for all the submitted methods. Note that these data are also open to researchers in future. The evaluation has been performed by the organizers, blind to the participants for a fair comparison. The results show that WHS of CT has been more successful than that of MRI from the twelve submissions. For segmentation of the substructures, the four chambers are generally easy to segment. By contrast, the great vessels, including aorta and pulmonary artery, still need more efforts to achieve good results. The performance of the DL-based methods submitted to this challenge was variable, with the best performing methods achieving high accuracy while the lowest performing methods were poor. The conventional atlas-based approaches generally performed well, though only 2 of the 11 MRI WHS methods and 2 of the 10 CT WHS algorithms submitted were none-DL-based. The hybrid methods, combining deep learning with prior information from either the multi-modality atlases or shape information of the heart substructures, should have good potential and be worthy of future exploration.</p>
  </sec>
  <sec id="sec0031">
    <title>Authors contributions</title>
    <p id="p0064">XZ initialized the challenge event, provided the 60 CT images, 41 MRI images (with KR and SO) of the 60 MRI images, and the manual segmentations of all the 120 images. GY, RM, JK, and DF provided the other 19 MRI images. XZ, GY and LL organized the challenge event, and LL evaluated all the submitted segmentation results. CP, DS, MU, MPH, JO, CW, OS, CB, XY, PAH, AM, UB, JB, GYu, CS, GG, JYR, TB, QT, WS, and XL were participants of the MM-WHS challenge and contributed equally. GZ, ZS, CW, TM and DN submitted their results after the deadline of the challenge. All the participants provided their results for evaluation and the description of their algorithms. All the authors have read and approved the publication of this work.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0065">We wish to confirm that there are no known conflicts of interest associated with this publication and there has been no signification financial support for this work that could have influenced its outcome. We confirm that the manuscript has been read and approved by all named authors and that there are no other of authors listed in the manuscript has been approved by all of us. We confirm that we have given due consideration to the protection of intellectual property associated with this work and that there are no impediments to publication, including the timing of publication, with respect to intellectual property. In so doing we confirm that we have followed the regulations of our institutions concerning intellectual property.</p>
  </sec>
</body>
<back>
  <ref-list id="bib001">
    <title>References</title>
    <ref id="bib0001">
      <element-citation publication-type="journal" id="sbref0001">
        <person-group person-group-type="author">
          <name>
            <surname>Avendi</surname>
            <given-names>M.R.</given-names>
          </name>
          <name>
            <surname>Kheradvar</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Jafarkhani</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>A combined deep-learning and deformable-model approach to fully automatic segmentation of the left ventricle in cardiac MRI</article-title>
        <source>Med. Image Anal.</source>
        <volume>30</volume>
        <year>2016</year>
        <fpage>108</fpage>
        <lpage>119</lpage>
        <pub-id pub-id-type="pmid">26917105</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0002">
      <element-citation publication-type="journal" id="sbref0002">
        <person-group person-group-type="author">
          <name>
            <surname>Benjamini</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Yekutieli</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>The control of the false discovery rate in multiple testing under dependency</article-title>
        <source>Ann. Stat.</source>
        <volume>29</volume>
        <issue>4</issue>
        <year>2001</year>
        <fpage>1165</fpage>
        <lpage>1188</lpage>
      </element-citation>
    </ref>
    <ref id="bib0003">
      <element-citation publication-type="journal" id="sbref0003">
        <person-group person-group-type="author">
          <name>
            <surname>Bernard</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Lalande</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zotti</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cervenansky</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>P.-A.</given-names>
          </name>
          <name>
            <surname>Cetin</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Lekadir</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Camara</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Ballester</surname>
            <given-names>M.A.G.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: Is the problem solved?</article-title>
        <source>IEEE Trans. Med. Imag.</source>
        <volume>37</volume>
        <issue>11</issue>
        <year>2018</year>
        <fpage>2514</fpage>
        <lpage>2525</lpage>
      </element-citation>
    </ref>
    <ref id="bib0004">
      <element-citation publication-type="journal" id="sbref0004">
        <person-group person-group-type="author">
          <name>
            <surname>Cai</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ou</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>A framework combining window width-level adjustment and Gaussian filter-based multi-resolution for automatic whole heart segmentation</article-title>
        <source>Neurocomputing</source>
        <volume>220</volume>
        <year>2017</year>
        <fpage>138</fpage>
        <lpage>150</lpage>
      </element-citation>
    </ref>
    <ref id="bib0005">
      <element-citation publication-type="journal" id="sbref0005">
        <person-group person-group-type="author">
          <name>
            <surname>Crum</surname>
            <given-names>W.R.</given-names>
          </name>
          <name>
            <surname>Camara</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Hill</surname>
            <given-names>D.L.G.</given-names>
          </name>
        </person-group>
        <article-title>Generalized overlap measures for evaluation and validation in medical image analysis</article-title>
        <source>IEEE Trans. Med. Imag.</source>
        <volume>25</volume>
        <issue>11</issue>
        <year>2006</year>
        <fpage>1451</fpage>
        <lpage>1461</lpage>
      </element-citation>
    </ref>
    <ref id="bib0006">
      <element-citation publication-type="journal" id="sbref0006">
        <person-group person-group-type="author">
          <name>
            <surname>Dong</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Loy</surname>
            <given-names>C.C.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Image super-resolution using deep convolutional networks</article-title>
        <source>IEEE Trans. Pattern Anal. Mach.Intell.</source>
        <volume>38</volume>
        <issue>2</issue>
        <year>2016</year>
        <fpage>295</fpage>
        <lpage>307</lpage>
        <pub-id pub-id-type="pmid">26761735</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0007">
      <element-citation publication-type="book" id="sbref0007">
        <person-group person-group-type="author">
          <name>
            <surname>Galisot</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Brouard</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ramel</surname>
            <given-names>J.-Y.</given-names>
          </name>
        </person-group>
        <chapter-title>Local probabilistic atlases and a posteriori correction for the segmentation of heart images</chapter-title>
        <source>International Workshop on Statistical Atlases and Computational Models of the Heart</source>
        <year>2017</year>
        <fpage>207</fpage>
        <lpage>214</lpage>
      </element-citation>
    </ref>
    <ref id="bib0008">
      <element-citation publication-type="book" id="sbref0008">
        <person-group person-group-type="author">
          <name>
            <surname>Goodfellow</surname>
            <given-names>I.</given-names>
          </name>
          <name>
            <surname>Pouget-Abadie</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Mirza</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Warde-Farley</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ozair</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Courville</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <chapter-title>Generative adversarial nets</chapter-title>
        <source>Advances in neural information processing systems</source>
        <year>2014</year>
        <fpage>2672</fpage>
        <lpage>2680</lpage>
      </element-citation>
    </ref>
    <ref id="bib0009">
      <element-citation publication-type="book" id="sbref0009">
        <person-group person-group-type="author">
          <name>
            <surname>Heinrich</surname>
            <given-names>M.P.</given-names>
          </name>
          <name>
            <surname>Blendowski</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <chapter-title>Multi-organ segmentation using vantage point forests and binary context features</chapter-title>
        <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>
        <year>2016</year>
        <fpage>598</fpage>
        <lpage>606</lpage>
      </element-citation>
    </ref>
    <ref id="bib0010">
      <element-citation publication-type="book" id="sbref0010">
        <person-group person-group-type="author">
          <name>
            <surname>Heinrich</surname>
            <given-names>M.P.</given-names>
          </name>
          <name>
            <surname>Jenkinson</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bartlomiej W.</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Brady</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Schnabel</surname>
            <given-names>J.A.</given-names>
          </name>
        </person-group>
        <chapter-title>Towards realtime multimodal fusion for image-guided interventions using self-similarities</chapter-title>
        <source>International conference on medical image computing and computer-assisted intervention</source>
        <year>2013</year>
        <fpage>187</fpage>
        <lpage>194</lpage>
      </element-citation>
    </ref>
    <ref id="bib0011">
      <element-citation publication-type="journal" id="sbref0011">
        <person-group person-group-type="author">
          <name>
            <surname>Heinrich</surname>
            <given-names>M.P.</given-names>
          </name>
          <name>
            <surname>Jenkinson</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Brady</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Schnabel</surname>
            <given-names>J.A.</given-names>
          </name>
        </person-group>
        <article-title>MRF-Based deformable registration and ventilation estimation of lung CT</article-title>
        <source>IEEE Trans. Med. Imag.</source>
        <volume>32</volume>
        <issue>7</issue>
        <year>2013</year>
        <fpage>1239</fpage>
        <lpage>1248</lpage>
      </element-citation>
    </ref>
    <ref id="bib0012">
      <element-citation publication-type="book" id="sbref0012">
        <person-group person-group-type="author">
          <name>
            <surname>Heinrich</surname>
            <given-names>M.P.</given-names>
          </name>
          <name>
            <surname>Oster</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <chapter-title>MRI whole heart segmentation using discrete nonlinear registration and fast non-local fusion</chapter-title>
        <source>International Workshop on Statistical Atlases and Computational Models of the Heart</source>
        <year>2017</year>
        <fpage>233</fpage>
        <lpage>241</lpage>
      </element-citation>
    </ref>
    <ref id="bib0013">
      <element-citation publication-type="journal" id="sbref0013">
        <person-group person-group-type="author">
          <name>
            <surname>Kang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Woo</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kuo</surname>
            <given-names>C.C.J.</given-names>
          </name>
          <name>
            <surname>Slomka</surname>
            <given-names>P.J.</given-names>
          </name>
          <name>
            <surname>Dey</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Germano</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Heart chambers and whole heart segmentation techniques: a review</article-title>
        <source>J. Electron. Imag.</source>
        <volume>21</volume>
        <issue>1</issue>
        <year>2012</year>
        <fpage>010901</fpage>
      </element-citation>
    </ref>
    <ref id="bib0014">
      <element-citation publication-type="journal" id="sbref0014">
        <person-group person-group-type="author">
          <name>
            <surname>Karim</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>L.-E.</given-names>
          </name>
          <name>
            <surname>Inoue</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Tao</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Housden</surname>
            <given-names>R.J.</given-names>
          </name>
          <name>
            <surname>Bhagirath</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Duval</surname>
            <given-names>J.-L.</given-names>
          </name>
          <name>
            <surname>Varela</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Behar</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Algorithms for left atrial wall segmentation and thickness–evaluation on an open-source CT and MRI image database</article-title>
        <source>Med. Image Anal.</source>
        <volume>50</volume>
        <year>2018</year>
        <fpage>36</fpage>
        <lpage>53</lpage>
        <pub-id pub-id-type="pmid">30208355</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0015">
      <element-citation publication-type="journal" id="sbref0015">
        <person-group person-group-type="author">
          <name>
            <surname>Karim</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Housden</surname>
            <given-names>R.J.</given-names>
          </name>
          <name>
            <surname>Balasubramaniam</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Perry</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Uddin</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Al-Beyatti</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Palkhi</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Acheampong</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Obom</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of current algorithms for segmentation of scar tissue from late gadolinium enhancement cardiovascular magnetic resonance of the left atrium: an open-access grand challenge</article-title>
        <source>J. Cardiovasc. Magnetic Reson.</source>
        <volume>15</volume>
        <issue>1</issue>
        <year>2013</year>
        <fpage>105</fpage>
      </element-citation>
    </ref>
    <ref id="bib0016">
      <element-citation publication-type="book" id="sbref0016">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kwon Lee</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Mu Lee</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <chapter-title>Deeply-recursive convolutional network for image super-resolution</chapter-title>
        <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
        <year>2016</year>
        <fpage>1637</fpage>
        <lpage>1645</lpage>
      </element-citation>
    </ref>
    <ref id="bib0017">
      <element-citation publication-type="journal" id="sbref0017">
        <person-group person-group-type="author">
          <name>
            <surname>Mahbod</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Chowdhury</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Smedby</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <article-title>Automatic brain segmentation using artificial neural networks with shape context</article-title>
        <source>Pattern Recognit. Lett.</source>
        <volume>101</volume>
        <year>2018</year>
        <fpage>74</fpage>
        <lpage>79</lpage>
      </element-citation>
    </ref>
    <ref id="bib0018">
      <element-citation publication-type="book" id="sbref0018">
        <person-group person-group-type="author">
          <name>
            <surname>Mendis</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Puska</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Norrving</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <chapter-title>Global Atlas on Cardiovascular Disease Prevention and Control</chapter-title>
        <year>2011</year>
        <publisher-name>World Health Organization</publisher-name>
      </element-citation>
    </ref>
    <ref id="bib0019">
      <element-citation publication-type="book" id="sbref0019">
        <person-group person-group-type="author">
          <name>
            <surname>Milletari</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Navab</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ahmadi</surname>
            <given-names>S.-A.</given-names>
          </name>
        </person-group>
        <chapter-title>V-net: Fully convolutional neural networks for volumetric medical image segmentation</chapter-title>
        <source>International Conference on 3D Vision</source>
        <year>2016</year>
        <fpage>565</fpage>
        <lpage>571</lpage>
      </element-citation>
    </ref>
    <ref id="bib0020">
      <element-citation publication-type="book" id="sbref0020">
        <person-group person-group-type="author">
          <name>
            <surname>Mo</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>McIlwraith</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Guo</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <chapter-title>The deep poincaré map: a novel approach for left ventricle segmentation</chapter-title>
        <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>
        <year>2018</year>
        <fpage>561</fpage>
        <lpage>568</lpage>
      </element-citation>
    </ref>
    <ref id="bib0021">
      <mixed-citation publication-type="other" id="othref0001">Moghari, M.H., Pace, D.F., Akhondi-Asl, A., Powell, A.J., 2016. HVSMR 2016: MICCAI workshop on whole-heart and great vessel segmentation from 3D cardiovascular MRI in congenital heart disease. <ext-link ext-link-type="uri" xlink:href="http://segchd.csail.mit.edu/index.html" id="intrrf0002">http://segchd.csail.mit.edu/index.html</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0022">
      <element-citation publication-type="book" id="sbref0021">
        <person-group person-group-type="author">
          <name>
            <surname>Mortazi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Burt</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bagci</surname>
            <given-names>U.</given-names>
          </name>
        </person-group>
        <chapter-title>Multi-planar deep segmentation networks for cardiac substructures from MRI and CT</chapter-title>
        <source>International Workshop on Statistical Atlases and Computational Models of the Heart</source>
        <year>2017</year>
        <fpage>199</fpage>
        <lpage>206</lpage>
      </element-citation>
    </ref>
    <ref id="bib0023">
      <element-citation publication-type="book" id="sbref0022">
        <person-group person-group-type="author">
          <name>
            <surname>Mortazi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Karim</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Rhode</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Burt</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Bagci</surname>
            <given-names>U.</given-names>
          </name>
        </person-group>
        <chapter-title>CardiacNET: segmentation of left atrium and proximal pulmonary veins from MRI using multi-view CNN</chapter-title>
        <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>
        <year>2017</year>
        <fpage>377</fpage>
        <lpage>385</lpage>
      </element-citation>
    </ref>
    <ref id="bib0024">
      <element-citation publication-type="journal" id="sbref0023">
        <person-group person-group-type="author">
          <name>
            <surname>Ngo</surname>
            <given-names>T.A.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Carneiro</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Combining deep learning and level set for the automated segmentation of the left ventricle of the heart from cardiac cine magnetic resonance</article-title>
        <source>Med. Image Anal.</source>
        <volume>35</volume>
        <year>2017</year>
        <fpage>159</fpage>
        <lpage>171</lpage>
        <pub-id pub-id-type="pmid">27423113</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0025">
      <element-citation publication-type="journal" id="sbref0024">
        <person-group person-group-type="author">
          <name>
            <surname>Nikolaou</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Alkadhi</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Bamberg</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Leschka</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wintersperger</surname>
            <given-names>B.J.</given-names>
          </name>
        </person-group>
        <article-title>MRI and CT in the diagnosis of coronary artery disease: indications and applications</article-title>
        <source>InsightsImag.</source>
        <volume>2</volume>
        <issue>1</issue>
        <year>2011</year>
        <fpage>9</fpage>
        <lpage>24</lpage>
      </element-citation>
    </ref>
    <ref id="bib0026">
      <element-citation publication-type="book" id="sbref0025">
        <person-group person-group-type="author">
          <name>
            <surname>Pace</surname>
            <given-names>D.F.</given-names>
          </name>
          <name>
            <surname>Dalca</surname>
            <given-names>A.V.</given-names>
          </name>
          <name>
            <surname>Geva</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Powell</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Moghari</surname>
            <given-names>M.H.</given-names>
          </name>
          <name>
            <surname>Golland</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <chapter-title>Interactive whole-heart segmentation in congenital heart disease</chapter-title>
        <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>
        <year>2015</year>
        <fpage>80</fpage>
        <lpage>88</lpage>
      </element-citation>
    </ref>
    <ref id="bib0027">
      <element-citation publication-type="book" id="sbref0026">
        <person-group person-group-type="author">
          <name>
            <surname>Payer</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Štern</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bischof</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Urschler</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <chapter-title>Regressing heatmaps for multiple landmark localization using CNNs</chapter-title>
        <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>
        <year>2016</year>
        <fpage>230</fpage>
        <lpage>238</lpage>
      </element-citation>
    </ref>
    <ref id="bib0028">
      <element-citation publication-type="book" id="sbref0027">
        <person-group person-group-type="author">
          <name>
            <surname>Payer</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Štern</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bischof</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Urschler</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <chapter-title>Multi-label whole heart segmentation using CNNs and anatomical label configurations</chapter-title>
        <source>International Workshop on Statistical Atlases and Computational Models of the Heart</source>
        <year>2017</year>
        <fpage>190</fpage>
        <lpage>198</lpage>
      </element-citation>
    </ref>
    <ref id="bib0029">
      <element-citation publication-type="journal" id="sbref0028">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Lekadir</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Gooya</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Shao</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Petersen</surname>
            <given-names>S.E.</given-names>
          </name>
          <name>
            <surname>Frangi</surname>
            <given-names>A.F.</given-names>
          </name>
        </person-group>
        <article-title>A review of heart chamber segmentation for structural and functional analysis using cardiac magnetic resonance imaging</article-title>
        <source>Magnetic Reson. Mater. Phys. Biol. Med.</source>
        <volume>29</volume>
        <issue>2</issue>
        <year>2016</year>
        <fpage>155</fpage>
        <lpage>195</lpage>
      </element-citation>
    </ref>
    <ref id="bib0030">
      <element-citation publication-type="journal" id="sbref0029">
        <person-group person-group-type="author">
          <name>
            <surname>Petitjean</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Zuluaga</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Bai</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Dacher</surname>
            <given-names>J.-N.</given-names>
          </name>
          <name>
            <surname>Grosgeorge</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Caudron</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ruan</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Ayed</surname>
            <given-names>I.B.</given-names>
          </name>
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.-C.</given-names>
          </name>
        </person-group>
        <article-title>Right ventricle segmentation from cardiac MRI: a collation study</article-title>
        <source>Med. Image Anal.</source>
        <volume>19</volume>
        <issue>1</issue>
        <year>2015</year>
        <fpage>187</fpage>
        <lpage>202</lpage>
        <pub-id pub-id-type="pmid">25461337</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0031">
      <element-citation publication-type="journal" id="sbref0030">
        <person-group person-group-type="author">
          <name>
            <surname>Radau</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Connelly</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Paul</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Dick</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Wright</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Evaluation framework for algorithms segmenting short axis cardiac MRI</article-title>
        <source>MIDAS J.-Cardiac MR Left Ventricle Segmentation Challenge</source>
        <volume>49</volume>
        <year>2009</year>
      </element-citation>
    </ref>
    <ref id="bib0032">
      <element-citation publication-type="journal" id="sbref0031">
        <person-group person-group-type="author">
          <name>
            <surname>Roberts</surname>
            <given-names>W.T.</given-names>
          </name>
          <name>
            <surname>Bax</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Davies</surname>
            <given-names>L.C.</given-names>
          </name>
        </person-group>
        <article-title>Cardiac CT and CT coronary angiography: technology and application</article-title>
        <source>Heart</source>
        <volume>94</volume>
        <issue>6</issue>
        <year>2008</year>
        <fpage>781</fpage>
        <lpage>792</lpage>
        <pub-id pub-id-type="pmid">18480352</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0033">
      <element-citation publication-type="book" id="sbref0032">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <chapter-title>U-net: convolutional networks for biomedical image segmentation</chapter-title>
        <source>International Conference on Medical image computing and computer-assisted intervention</source>
        <year>2015</year>
        <fpage>234</fpage>
        <lpage>241</lpage>
      </element-citation>
    </ref>
    <ref id="bib0034">
      <element-citation publication-type="book" id="sbref0033">
        <person-group person-group-type="author">
          <name>
            <surname>Roth</surname>
            <given-names>H.R.</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Seff</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Cherry</surname>
            <given-names>K.M.</given-names>
          </name>
          <name>
            <surname>Hoffman</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Turkbey</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Summers</surname>
            <given-names>R.M.</given-names>
          </name>
        </person-group>
        <chapter-title>A new 2.5 D representation for lymph node detection using random sets of deep convolutional neural network observations</chapter-title>
        <source>International conference on medical image computing and computer-assisted intervention</source>
        <year>2014</year>
        <fpage>520</fpage>
        <lpage>527</lpage>
      </element-citation>
    </ref>
    <ref id="bib0035">
      <element-citation publication-type="journal" id="sbref0034">
        <person-group person-group-type="author">
          <name>
            <surname>Shen</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Suk</surname>
            <given-names>H.-I.</given-names>
          </name>
        </person-group>
        <article-title>Deep learning in medical image analysis</article-title>
        <source>Annu. Rev. Biomed. Eng.</source>
        <volume>19</volume>
        <year>2017</year>
        <fpage>221</fpage>
        <lpage>248</lpage>
        <pub-id pub-id-type="pmid">28301734</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0036">
      <element-citation publication-type="book" id="sbref0035">
        <person-group person-group-type="author">
          <name>
            <surname>Suinesiaputra</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Cowan</surname>
            <given-names>B.R.</given-names>
          </name>
          <name>
            <surname>Finn</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Fonseca</surname>
            <given-names>C.G.</given-names>
          </name>
          <name>
            <surname>Kadish</surname>
            <given-names>A.H.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>D.C.</given-names>
          </name>
          <name>
            <surname>Medrano-Gracia</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Warfield</surname>
            <given-names>S.K.</given-names>
          </name>
          <name>
            <surname>Tao</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Young</surname>
            <given-names>A.A.</given-names>
          </name>
        </person-group>
        <chapter-title>Left ventricular segmentation challenge from cardiac MRI: a collation study</chapter-title>
        <source>International Workshop on Statistical Atlases and Computational Models of the Heart</source>
        <year>2011</year>
        <fpage>88</fpage>
        <lpage>97</lpage>
      </element-citation>
    </ref>
    <ref id="bib0037">
      <element-citation publication-type="journal" id="sbref0036">
        <person-group person-group-type="author">
          <name>
            <surname>Tan</surname>
            <given-names>L.K.</given-names>
          </name>
          <name>
            <surname>McLaughlin</surname>
            <given-names>R.A.</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Abdul Aziz</surname>
            <given-names>Y.F.</given-names>
          </name>
          <name>
            <surname>Liew</surname>
            <given-names>Y.M.</given-names>
          </name>
        </person-group>
        <article-title>Fully automated segmentation of the left ventricle in cine cardiac MRI using neural network regression</article-title>
        <source>J. Magnetic Reson. Imag.</source>
        <volume>48</volume>
        <issue>1</issue>
        <year>2018</year>
        <fpage>140</fpage>
        <lpage>152</lpage>
      </element-citation>
    </ref>
    <ref id="bib0038">
      <element-citation publication-type="journal" id="sbref0037">
        <person-group person-group-type="author">
          <name>
            <surname>Tobon-Gomez</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Geers</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Peters</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Weese</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Pinto</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Karim</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Ammar</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Daoudi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Margeta</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sandoval</surname>
            <given-names>Z.</given-names>
          </name>
        </person-group>
        <article-title>Benchmark for algorithms segmenting the left atrium from 3D CT and MRI datasets</article-title>
        <source>IEEE Trans. Med. Imag.</source>
        <volume>34</volume>
        <issue>7</issue>
        <year>2015</year>
        <fpage>1460</fpage>
        <lpage>1473</lpage>
      </element-citation>
    </ref>
    <ref id="bib0039">
      <element-citation publication-type="book" id="sbref0038">
        <person-group person-group-type="author">
          <name>
            <surname>Tong</surname>
            <given-names>Q.</given-names>
          </name>
          <name>
            <surname>Ning</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Si</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <chapter-title>3D deeply-supervised U-Net based whole heart segmentation</chapter-title>
        <source>International Workshop on Statistical Atlases and Computational Models of the Heart</source>
        <year>2017</year>
        <fpage>224</fpage>
        <lpage>232</lpage>
      </element-citation>
    </ref>
    <ref id="bib0040">
      <element-citation publication-type="book" id="sbref0039">
        <person-group person-group-type="author">
          <name>
            <surname>Tran</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Bourdev</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Fergus</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Torresani</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Paluri</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <chapter-title>Learning spatiotemporal features with 3D convolutional networks</chapter-title>
        <source>Proceedings of the IEEE international conference on computer vision</source>
        <year>2015</year>
        <fpage>4489</fpage>
        <lpage>4497</lpage>
      </element-citation>
    </ref>
    <ref id="bib0041">
      <element-citation publication-type="book" id="sbref0040">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Smedby</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <chapter-title>Automatic multi-organ segmentation in non-enhanced CT datasets using hierarchical shape priors</chapter-title>
        <source>International Conference on Pattern Recognition</source>
        <year>2014</year>
        <fpage>3327</fpage>
        <lpage>3332</lpage>
      </element-citation>
    </ref>
    <ref id="bib0042">
      <element-citation publication-type="book" id="sbref0041">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Smedby</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <chapter-title>Automatic whole heart segmentation using deep learning and shape context</chapter-title>
        <source>International Workshop on Statistical Atlases and Computational Models of the Heart</source>
        <year>2017</year>
        <fpage>242</fpage>
        <lpage>249</lpage>
      </element-citation>
    </ref>
    <ref id="bib0043">
      <element-citation publication-type="journal" id="sbref0042">
        <person-group person-group-type="author">
          <name>
            <surname>Xu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>C.P.</given-names>
          </name>
          <name>
            <surname>Heinrich</surname>
            <given-names>M.P.</given-names>
          </name>
          <name>
            <surname>Modat</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Abramson</surname>
            <given-names>R.G.</given-names>
          </name>
          <name>
            <surname>Landman</surname>
            <given-names>B.A.</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of six registration methods for the human abdomen on clinically acquired CT</article-title>
        <source>IEEE Trans. Biomed. Eng.</source>
        <volume>63</volume>
        <issue>8</issue>
        <year>2016</year>
        <fpage>1563</fpage>
        <lpage>1572</lpage>
        <pub-id pub-id-type="pmid">27254856</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0044">
      <element-citation publication-type="book" id="sbref0043">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Shu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Dillenseger</surname>
            <given-names>J.-l.</given-names>
          </name>
        </person-group>
        <chapter-title>Automatic whole heart segmentation in CT images based on multi-atlas image registration</chapter-title>
        <source>International Workshop on Statistical Atlases and Computational Models of the Heart</source>
        <year>2017</year>
        <fpage>250</fpage>
        <lpage>257</lpage>
      </element-citation>
    </ref>
    <ref id="bib0045">
      <element-citation publication-type="book" id="sbref0044">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Bian</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ni</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>P.-A.</given-names>
          </name>
        </person-group>
        <chapter-title>3D convolutional networks for fully automatic fine-grained whole heart partition</chapter-title>
        <source>International Workshop on Statistical Atlases and Computational Models of the Heart</source>
        <year>2017</year>
        <fpage>181</fpage>
        <lpage>189</lpage>
      </element-citation>
    </ref>
    <ref id="bib0046">
      <element-citation publication-type="book" id="sbref0045">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Bian</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ni</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>P.-A.</given-names>
          </name>
        </person-group>
        <chapter-title>Hybrid loss guided convolutional networks for whole heart parsing</chapter-title>
        <source>International Workshop on Statistical Atlases and Computational Models of the Heart</source>
        <year>2017</year>
        <fpage>215</fpage>
        <lpage>223</lpage>
      </element-citation>
    </ref>
    <ref id="bib0047">
      <element-citation publication-type="book" id="sbref0046">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Koltun</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Funkhouser</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <chapter-title>Dilated residual networks</chapter-title>
        <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
        <year>2017</year>
        <fpage>472</fpage>
        <lpage>480</lpage>
      </element-citation>
    </ref>
    <ref id="bib0048">
      <element-citation publication-type="journal" id="sbref0047">
        <person-group person-group-type="author">
          <name>
            <surname>Yushkevich</surname>
            <given-names>P.A.</given-names>
          </name>
          <name>
            <surname>Piven</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Hazlett</surname>
            <given-names>H.C.</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>R.G.</given-names>
          </name>
          <name>
            <surname>Ho</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Gee</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Gerig</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>User-guided 3d active contour segmentation of anatomical structures: significantly improved efficiency and reliability</article-title>
        <source>Neuroimage</source>
        <volume>31</volume>
        <issue>3</issue>
        <year>2006</year>
        <fpage>1116</fpage>
        <lpage>1128</lpage>
        <pub-id pub-id-type="pmid">16545965</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0049">
      <mixed-citation publication-type="other" id="othref0002">Zhao, J., Xiong, Z., 2018. 2018 atrial segmentation challenge. <ext-link ext-link-type="uri" xlink:href="http://atriaseg2018.cardiacatlas.org/" id="intrrf0003">http://atriaseg2018.cardiacatlas.org/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0050">
      <element-citation publication-type="journal" id="sbref0048">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Milgrom</surname>
            <given-names>S.A.</given-names>
          </name>
          <name>
            <surname>Pinnix</surname>
            <given-names>C.C.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Gomez</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Cardiac atlas development and validation for automatic segmentation of cardiac substructures</article-title>
        <source>Radiother. Oncol.</source>
        <volume>122</volume>
        <issue>1</issue>
        <year>2017</year>
        <fpage>66</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="pmid">27939201</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0051">
      <element-citation publication-type="journal" id="sbref0049">
        <person-group person-group-type="author">
          <name>
            <surname>Zhuang</surname>
            <given-names>X.</given-names>
          </name>
        </person-group>
        <article-title>Challenges and methodologies of fully automatic whole heart segmentation: a review</article-title>
        <source>J. Healthcare Eng.</source>
        <volume>4</volume>
        <issue>3</issue>
        <year>2013</year>
        <fpage>371</fpage>
        <lpage>407</lpage>
      </element-citation>
    </ref>
    <ref id="bib0052">
      <element-citation publication-type="journal" id="sbref0050">
        <person-group person-group-type="author">
          <name>
            <surname>Zhuang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Bai</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Zhan</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Qian</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Lian</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Rueckert</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Multiatlas whole heart segmentation of CT data using conditional entropy for atlas ranking and selection</article-title>
        <source>Med. Phys.</source>
        <volume>42</volume>
        <issue>7</issue>
        <year>2015</year>
        <fpage>3822</fpage>
        <lpage>3833</lpage>
        <pub-id pub-id-type="pmid">26133584</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0053">
      <mixed-citation publication-type="other" id="othref0003">Zhuang, X., Li, L., Xu, J., Zhou, Y., Luo, X., 2019. Multi-sequence cardiac MR segmentation challenge. <ext-link ext-link-type="uri" xlink:href="https://zmiclab.github.io/mscmrseg19/" id="intrrf0004">https://zmiclab.github.io/mscmrseg19/</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib0054">
      <element-citation publication-type="journal" id="sbref0051">
        <person-group person-group-type="author">
          <name>
            <surname>Zhuang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Multi-scale patch and multi-modality atlases for whole heart segmentation of MRI</article-title>
        <source>Med. Image Anal.</source>
        <volume>31</volume>
        <year>2016</year>
        <fpage>77</fpage>
        <lpage>87</lpage>
        <pub-id pub-id-type="pmid">26999615</pub-id>
      </element-citation>
    </ref>
    <ref id="bib0055">
      <element-citation publication-type="book" id="sbref0052">
        <person-group person-group-type="author">
          <name>
            <surname>Zuluaga</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Cardoso</surname>
            <given-names>M.J.</given-names>
          </name>
          <name>
            <surname>Modat</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ourselin</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <chapter-title>Multi-atlas propagation whole heart segmentation from MRI and CTA using a local normalised correlation coefficient criterion</chapter-title>
        <source>International Conference on Functional Imaging and Modeling of the Heart</source>
        <year>2013</year>
        <fpage>174</fpage>
        <lpage>181</lpage>
      </element-citation>
    </ref>
  </ref-list>
  <ack id="ack0001">
    <title>Acknowledgements</title>
    <p>This work was funded in part by the <funding-source id="gs00001">National Natural Science Foundation of China</funding-source> (NSFC) grant (61971142), the <funding-source id="gs00002">Science and Technology Commission of Shanghai Municipality</funding-source> grant (17JC1401600) and the <funding-source id="gs00003">British Heart Foundation</funding-source> Project grant (PG/16/78/32402).</p>
  </ack>
</back>
