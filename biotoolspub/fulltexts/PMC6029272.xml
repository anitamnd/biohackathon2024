<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6029272</article-id>
    <article-id pub-id-type="publisher-id">2253</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-018-2253-8</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>HapCHAT: adaptive haplotype assembly for efficiently leveraging high coverage in long reads</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Beretta</surname>
          <given-names>Stefano</given-names>
        </name>
        <address>
          <email>stefano.beretta@disco.unimib.it</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4329-0234</contrib-id>
        <name>
          <surname>Patterson</surname>
          <given-names>Murray D.</given-names>
        </name>
        <address>
          <email>murray.patterson@unimib.it</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zaccaria</surname>
          <given-names>Simone</given-names>
        </name>
        <address>
          <email>zaccaria@princeton.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Della Vedova</surname>
          <given-names>Gianluca</given-names>
        </name>
        <address>
          <email>gianluca@dellavedova.org</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Bonizzoni</surname>
          <given-names>Paola</given-names>
        </name>
        <address>
          <email>paola.bonizzoni@unimib.it</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2174 1754</institution-id><institution-id institution-id-type="GRID">grid.7563.7</institution-id><institution>Department of Informatics, Systems, and Communication, University of Milano-Bicocca, </institution></institution-wrap>Milan, Italy </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2097 5006</institution-id><institution-id institution-id-type="GRID">grid.16750.35</institution-id><institution>Department of Computer Science, Princeton University, </institution></institution-wrap>Princeton, New Jersey, USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>3</day>
      <month>7</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>3</day>
      <month>7</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2018</year>
    </pub-date>
    <volume>19</volume>
    <elocation-id>252</elocation-id>
    <history>
      <date date-type="received">
        <day>15</day>
        <month>11</month>
        <year>2017</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>6</month>
        <year>2018</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2018</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver(<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Haplotype assembly is the process of assigning the different alleles of the variants covered by mapped sequencing reads to the two haplotypes of the genome of a human individual. Long reads, which are nowadays cheaper to produce and more widely available than ever before, have been used to reduce the fragmentation of the assembled haplotypes since their ability to span several variants along the genome. These long reads are also characterized by a high error rate, an issue which may be mitigated, however, with larger sets of reads, when this error rate is uniform across genome positions. Unfortunately, current state-of-the-art dynamic programming approaches designed for long reads deal only with limited coverages.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>Here, we propose a new method for assembling haplotypes which combines and extends the features of previous approaches to deal with long reads and higher coverages. In particular, our algorithm is able to dynamically adapt the estimated number of errors at each variant site, while minimizing the total number of error corrections necessary for finding a feasible solution. This allows our method to significantly reduce the required computational resources, allowing to consider datasets composed of higher coverages. The algorithm has been implemented in a freely available tool, HapCHAT: <bold>Hap</bold>lotype Assembly <bold>C</bold>overage <bold>H</bold>andling by <bold>A</bold>dapting <bold>T</bold>hresholds. An experimental analysis on sequencing reads with up to 60 × coverage reveals improvements in accuracy and recall achieved by considering a higher coverage with lower runtimes.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>Our method leverages the long-range information of sequencing reads that allows to obtain assembled haplotypes fragmented in a lower number of unphased haplotype blocks. At the same time, our method is also able to deal with higher coverages to better correct the errors in the original reads and to obtain more accurate haplotypes as a result.</p>
      </sec>
      <sec>
        <title>Availability</title>
        <p>HapCHAT is available at <ext-link ext-link-type="uri" xlink:href="http://hapchat.algolab.eu">http://hapchat.algolab.eu</ext-link>under the GNU Public License (GPL).</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Single individual haplotyping</kwd>
      <kwd>Long reads</kwd>
      <kwd>High coverage</kwd>
      <kwd>Haplotype assembly</kwd>
      <kwd>Minimum error correction</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Fondazione Cariplo (IT)</institution>
        </funding-source>
        <award-id>2013?0955</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2018</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Due to the diploid nature of the human genome, i.e., it has two copies of its genome, called <italic>haplotypes</italic>, genomic variants appear on either of these two copies. Knowing the specific haplotype on which each of the genomic variants occurs has a strong impact on various studies in genetics, from population genomics [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>], to clinical and medical genetics [<xref ref-type="bibr" rid="CR3">3</xref>], or to the effects of compound heterozygosity [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR4">4</xref>].</p>
    <p>More specifically, the variations between two haplotypes of the genome are, for the most part, in the form of heterozygous <italic>Single Nucleotide Variants</italic> (SNVs), i.e., single genomic positions where the haplotypes contain two distinct alleles. Since a direct experimental reconstruction of the haplotypes is not yet cost effective [<xref ref-type="bibr" rid="CR5">5</xref>] or require methods that have not yet gained widespread adoption [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>], computational methods aim to perform this task starting from sequencing reads mapped to a reference human genome. In fact, sequencing reads usually cover multiple SNV positions on the genome, hence providing information about the corresponding alleles that co-occur on a haplotype. In particular, <italic>haplotype assembly</italic> is the computational approach aiming to partition the reads into two sets such that all the reads belonging to the same set are assigned to the same haplotype.</p>
    <p>Due to the availability of curated, high quality haplotype reference panels on a large population of individuals [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>], computational methods for statistically inferring the haplotypes of an individual from these panels are widely used [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR10">10</xref>]. The accuracy of these methods, however, depends heavily on the size and diversity of the population used to compile the panels, entailing poor performance on rare variants, while <italic>de novo</italic> variants are completely missed. These types of variants appear in the sequencing reads of the individual, making read-based haplotype assembly the obvious solution.</p>
    <p>The combinatorial <italic>Minimum Error Correction</italic> (MEC) problem is the most commonly cited formulation of haplotype assembly [<xref ref-type="bibr" rid="CR11">11</xref>]. Under the principle of parsimony, MEC aims to find the minimum number of corrections to the values of sequencing reads in order to be able to partition the reads into two haplotypes. Unfortunately, this problem is NP-hard [<xref ref-type="bibr" rid="CR11">11</xref>] and it is even hard to approximate [<xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR14">14</xref>]. As such, several heuristics for haplotype assembly have been proposed [<xref ref-type="bibr" rid="CR15">15</xref>–<xref ref-type="bibr" rid="CR19">19</xref>]. Beyond that, several exact methods have been proposed, including <italic>Integer Linear Programming</italic> (ILP) approaches [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>], and <italic>Dynamic Programming</italic> (DP) approaches which are <italic>Fixed-Parameter Tractable</italic> (FPT) in some parameter [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR22">22</xref>]. These methods achieve good results on datasets obtained using the traditional short sequencing reads. However, short reads do not allow to span more than a few SNV positions along the genome, rendering them inadequate for reconstructing long regions of the two haplotypes. In fact, the short range information provided by these reads does not allow to link many – if any – SNVs together. Consequently, the resulting assembled haplotypes are fragmented into many short haplotype blocks that remain unphased, relative to each other [<xref ref-type="bibr" rid="CR23">23</xref>].</p>
    <p>The advent of third generation sequencing technologies introduces a new kind of sequencing reads, called <italic>long reads</italic>, that are able to cover much longer portions of the genome [<xref ref-type="bibr" rid="CR24">24</xref>–<xref ref-type="bibr" rid="CR26">26</xref>]. Each read may span several positions along the genome and the long-range information provided by these reads allow to link several SNVs. This results in the possibility of obtaining longer haplotype blocks that assign more variants to the corresponding haplotype [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>]. Current third generation sequencing platforms offered by Pacific Biosciences (PacBio) [<xref ref-type="bibr" rid="CR29">29</xref>] and Oxford Nanopore Technologies (ONT) [<xref ref-type="bibr" rid="CR30">30</xref>] are now able to produce reads of tens to hundreds of kilobasepairs (kbp) in length, and are much more capable of capturing together more variants than the short reads that are commonplace today. While PacBio technologies are characterized by a high error rate (substitution error rate up to 5% and indel rate up to 10%), this is uniformly distributed along the genome positions [<xref ref-type="bibr" rid="CR24">24</xref>, <xref ref-type="bibr" rid="CR25">25</xref>, <xref ref-type="bibr" rid="CR31">31</xref>] – something we can take advantage of. Oxford Nanopore Technologies, on the other hand, have an even higher error rate which is also not uniformly distributed [<xref ref-type="bibr" rid="CR32">32</xref>]. Traditional approaches that have been designed for short reads fail when they are applied to these long reads, even when considering low coverages, as demonstrated in [<xref ref-type="bibr" rid="CR33">33</xref>]. This is due to the fact that these approaches scale poorly with increasing read length [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR22">22</xref>].</p>
    <p>Recently, two methods have been proposed to specifically deal with long reads and their characteristics, namely WhatsHap [<xref ref-type="bibr" rid="CR33">33</xref>, <xref ref-type="bibr" rid="CR34">34</xref>] and HapCol [<xref ref-type="bibr" rid="CR35">35</xref>]. On the one hand, WhatsHap introduces a dynamic programming algorithm that is fixed parameter tractable, with <italic>coverage</italic> as the parameter, where coverage is the maximum number of reads covering any genome position. Hence, this algorithm is able to leverage the long-range information of long reads since its runtime is independent of the read length, but unfortunately it can deal only with datasets of limited coverages – up to 20×, and hence resorts to pruning datasets with higher coverage [<xref ref-type="bibr" rid="CR33">33</xref>]. A parallel version of WhatsHap has been recently proposed showing the capability to deal with higher coverages of up to 25× [<xref ref-type="bibr" rid="CR36">36</xref>]. Although WhatsHap computes the theoretically optimal solution to the MEC problem, minimizing the overall number of corrections in the input reads, this could result, however, in columns having an unrealistically large number of corrections, which may not be coherent with how the errors are truly distributed in the actual reads.</p>
    <p>On the other hand, HapCol proposes an approach that exploits the uniform distribution of sequencing errors characterizing long reads. In particular, the authors propose a new formulation of the MEC problem where the maximum number of corrections is bounded in every column and is computed from the expected error rate [<xref ref-type="bibr" rid="CR35">35</xref>]. HapCol has been shown to be able to deal with datasets of higher coverages compared to WhatsHap. However, the presence of genome positions containing more errors than expected (due to errors in the alignment or repetitive regions) is a problem for this approach. As a result, even HapCol was effectively limited to deal with instances of relatively low coverages up to 25–30×, since even the presence of few outliers forces the algorithm to change the global behavior, or to fail.</p>
    <p>As a result, both the methods proposed for haplotype assembly from long reads, WhatsHap and HapCol, have issues managing datasets with increasing coverages. However, considering a higher number of reads covering each position is indeed the most reliable way to face the high error rate characterizing the sequencing reads produced by third generation sequencing technologies. In fact, long reads generated by the PacBio platform share a limited number of errors on any given SNV position that they cover because errors are almost uniformly distributed across genome positions. Therefore, increasing the coverage mitigates the effects of sequencing errors and may allow to reconstruct haplotypes of higher quality.</p>
    <p>In this work we propose a new method which combines and extends the main features of the previous WhatsHap and HapCol, and aims to deal with datasets of higher coverages while being robust to the presence of noise and outliers. In particular, we re-design the approach proposed in [<xref ref-type="bibr" rid="CR35">35</xref>] by allowing also the dynamic adaption of the estimated error rate and, consequently, the maximum number of corrections that are allowed in each position. This allows the handling of columns that require more errors than expected, while avoiding the exploration of scenarios that involve a number of corrections that is much higher than necessary for a site. This is coupled with a merging procedure which merges pairs of reads that are highly likely to originate from the same haplotype, allowing this method to scale to significantly higher values of coverage. The method has been implemented in HapCHAT: <bold>Hap</bold>lotype Assembly <bold>C</bold>overage <bold>H</bold>andling by <bold>A</bold>dapting <bold>T</bold>hresholds that is freely available at <ext-link ext-link-type="uri" xlink:href="http://github.com/AlgoLab/HapCHAT">http://github.com/AlgoLab/HapCHAT</ext-link>. An experimental analysis on real and simulated sequencing reads with up to 60×coverage reveals that we are able to leverage high coverage towards better predictions in terms of both accuracy (switch error rate) and recall (QAN50 score — the Quality Adjusted N50 score, see Discussion Section), as we see an upward trend in both, as coverage increases. This trend is the most stark in the case of recall, which is where it counts the most, since the ultimate goal of haplotype assembly is indeed to assemble the longest haplotype blocks possible.</p>
    <p>We compare our method to some of the state-of-the-art methods in haplotype assembly, including HapCol [<xref ref-type="bibr" rid="CR35">35</xref>]; the newest version of WhatsHap [<xref ref-type="bibr" rid="CR37">37</xref>], to which many features have since been added; and HapCUT2 [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. We show that HapCHAT is comparable to or better than any tool in terms of both accuracy and recall, while requiring an amount of computational resources (time and memory) that is on the same or a lower order of magnitude of any comparable (in terms of accuracy or recall) tool in every case. These results confirm that high coverage can indeed be leveraged in order to deal with the high error rate of long reads in order to take advantage of their long-range information.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p>In this section, we highlight the new insight of HapCHAT for the assembly of single individual haplotypes, with the specific goal of processing high coverage in long read datasets. We first need some preliminary definitions.</p>
    <sec id="Sec3">
      <title>Preliminaries</title>
      <p>Let <italic>v</italic> be a vector, then <italic>v</italic>[<italic>i</italic>] denotes the value of <italic>v</italic> at position <italic>i</italic>. A <italic>haplotype</italic> is a vector <italic>h</italic>∈{0,1}<sup><italic>m</italic></sup>. Given two haplotypes of an individual, say <italic>h</italic><sub>1</sub>, <italic>h</italic><sub>2</sub>, the position <italic>j</italic> is <italic>heterozygous</italic> if <italic>h</italic><sub>1</sub>[<italic>j</italic>]≠<italic>h</italic><sub>2</sub>[<italic>j</italic>], otherwise <italic>j</italic> is <italic>homozygous</italic>. A <italic>fragment</italic> is a vector <italic>f</italic> of length <italic>l</italic> belonging to {0,1,−}<sup><italic>l</italic></sup>. Given a fragment <italic>f</italic>, position <italic>j</italic> is a <italic>hole</italic> if <italic>f</italic>[<italic>j</italic>]=−, while a <italic>gap</italic> is a maximal sub-vector of <italic>f</italic> of holes, i.e., a gap is preceded and followed by a non-hole element (or by a boundary of the fragment).</p>
      <p>A <italic>fragment matrix</italic> is a matrix <italic>M</italic> that consists of <italic>n</italic> rows (fragments) and <italic>m</italic> columns (SNVs). We denote as <italic>L</italic> the maximum length for all the fragments in <italic>M</italic>, and as <italic>M</italic><sub><italic>j</italic></sub> the <italic>j</italic>-th column of <italic>M</italic>. Notice that each column of <italic>M</italic> is a vector in {0,1,−}<sup><italic>n</italic></sup> while each row is a vector in {0,1,−}<sup><italic>m</italic></sup>.</p>
      <p>Given two row vectors <italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub> belonging to {0,1,−}<sup><italic>m</italic></sup>, <italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub> are in <italic>conflict</italic> if there exists a position <italic>j</italic>, with 1≤<italic>j</italic>≤<italic>m</italic>, such that <italic>r</italic><sub>1</sub>[<italic>j</italic>]≠<italic>r</italic><sub>2</sub>[<italic>j</italic>] and <italic>r</italic><sub>1</sub>[<italic>j</italic>],<italic>r</italic><sub>2</sub>[<italic>j</italic>]≠−, otherwise <italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub> are in <italic>agreement</italic>. A fragment matrix <italic>M</italic> is <italic>conflict free</italic> if and only if there exist two haplotypes <italic>h</italic><sub>1</sub>, <italic>h</italic><sub>2</sub> such that each row of <italic>M</italic> is in agreement with one of <italic>h</italic><sub>1</sub> and <italic>h</italic><sub>2</sub>. Equivalently, <italic>M</italic> is conflict free if and only if there exists a <italic>bipartition</italic> (<italic>P</italic><sub>1</sub>,<italic>P</italic><sub>2</sub>) of the fragments in <italic>M</italic> such that each pair of fragments in <italic>P</italic><sub>1</sub> is in agreement and each pair of fragments in <italic>P</italic><sub>2</sub> is in agreement. A <italic>k</italic>-<italic>correction</italic> of a column <italic>M</italic><sub><italic>j</italic></sub>, is obtained from <italic>M</italic><sub><italic>j</italic></sub> by flipping at most <italic>k</italic> values that are different from −. A column of a matrix is called <italic>homozygous</italic> if it contains no 0 or no 1, otherwise (if it contains both 0 and 1) it is called <italic>heterozygous</italic>. We say that a fragment <italic>i</italic> is <italic>active</italic> on a column <italic>M</italic><sub><italic>j</italic></sub>, if <italic>M</italic><sub><italic>j</italic></sub>[<italic>i</italic>]=0 or <italic>M</italic><sub><italic>j</italic></sub>[<italic>i</italic>]=1. The <italic>active fragments</italic> of a column <italic>M</italic><sub><italic>j</italic></sub> are the set <italic>a</italic><italic>c</italic><italic>t</italic><italic>i</italic><italic>v</italic><italic>e</italic>(<italic>M</italic><sub><italic>j</italic></sub>)={<italic>i</italic>:<italic>M</italic><sub><italic>j</italic></sub>[<italic>i</italic>]≠−}. The <italic>coverage</italic> of the column <italic>M</italic><sub><italic>j</italic></sub> is defined as the number <italic>c</italic><italic>o</italic><italic>v</italic><sub><italic>j</italic></sub> of fragments that are active on <italic>M</italic><sub><italic>j</italic></sub>, that is <italic>c</italic><italic>o</italic><italic>v</italic><sub><italic>j</italic></sub>=|<italic>a</italic><italic>c</italic><italic>t</italic><italic>i</italic><italic>v</italic><italic>e</italic>(<italic>M</italic><sub><italic>j</italic></sub>)|. In the following, we indicate as <italic>c</italic><italic>o</italic><italic>v</italic> the maximum coverage over all the columns of <italic>M</italic>. Given two columns <italic>M</italic><sub><italic>i</italic></sub> and <italic>M</italic><sub><italic>j</italic></sub>, we denote by <italic>a</italic><italic>c</italic><italic>t</italic><italic>i</italic><italic>v</italic><italic>e</italic>(<italic>M</italic><sub><italic>i</italic></sub>,<italic>M</italic><sub><italic>j</italic></sub>) the intersection <italic>a</italic><italic>c</italic><italic>t</italic><italic>i</italic><italic>v</italic><italic>e</italic>(<italic>M</italic><sub><italic>i</italic></sub>)∩<italic>a</italic><italic>c</italic><italic>t</italic><italic>i</italic><italic>v</italic><italic>e</italic>(<italic>M</italic><sub><italic>j</italic></sub>). Moreover, we will write <italic>M</italic><sub><italic>i</italic></sub>≈<italic>M</italic><sub><italic>j</italic></sub>, and say that <italic>M</italic><sub><italic>i</italic></sub>, <italic>M</italic><sub><italic>j</italic></sub> are in accordance [<xref ref-type="bibr" rid="CR13">13</xref>], if <italic>M</italic><sub><italic>i</italic></sub>[<italic>r</italic>]=<italic>M</italic><sub><italic>y</italic></sub>[<italic>r</italic>] for each <italic>r</italic>∈<italic>a</italic><italic>c</italic><italic>t</italic><italic>i</italic><italic>v</italic><italic>e</italic>(<italic>M</italic><sub><italic>i</italic></sub>,<italic>M</italic><sub><italic>j</italic></sub>), or <italic>M</italic><sub><italic>i</italic></sub>[<italic>r</italic>]≠<italic>M</italic><sub><italic>y</italic></sub>[<italic>r</italic>] for each <italic>r</italic>∈<italic>a</italic><italic>c</italic><italic>t</italic><italic>i</italic><italic>v</italic><italic>e</italic>(<italic>M</italic><sub><italic>i</italic></sub>,<italic>M</italic><sub><italic>j</italic></sub>). Notice that <italic>M</italic><sub><italic>i</italic></sub>≈<italic>M</italic><sub><italic>j</italic></sub> means that these two columns are compatible, that is, they induce no conflict. Moreover, <italic>d</italic>(<italic>M</italic><sub><italic>i</italic></sub>,<italic>M</italic><sub><italic>j</italic></sub>) denotes the minimum number of corrections to make columns <italic>M</italic><sub><italic>i</italic></sub> and <italic>M</italic><sub><italic>j</italic></sub> in accordance.</p>
      <p>The <italic>Minimum Error Correction</italic> (MEC) problem [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR38">38</xref>], given a matrix <italic>M</italic> of fragments, asks to find a conflict free matrix <italic>C</italic> obtained from <italic>M</italic> with the minimum number of corrections. In this work, we consider the variant of the MEC problem, called <italic>k</italic>-cMEC (<italic>k-constrained</italic> MEC) in which the number of corrections per column is bounded by an integer <italic>k</italic> [<xref ref-type="bibr" rid="CR35">35</xref>]. More precisely, we want a <italic>k-correction matrix</italic><italic>D</italic> for <italic>M</italic> where each column <italic>C</italic><sub><italic>j</italic></sub> is a <italic>k</italic>-correction of column <italic>M</italic><sub><italic>j</italic></sub>, minimizing the total number of corrections. We recall that in this paper we will consider only matrices where all columns are heterozygous.</p>
      <p>Now, let us briefly recall the dynamic programming approach to solve the <italic>k</italic>-cMEC problem [<xref ref-type="bibr" rid="CR35">35</xref>]. This approach computes a bidimensional array <italic>D</italic>[<italic>j</italic>,<italic>C</italic><sub><italic>j</italic></sub>] for each column <italic>j</italic>≥1 and each possible heterozygous <italic>k</italic>-correction <italic>C</italic><sub><italic>j</italic></sub> of <italic>M</italic><sub><italic>j</italic></sub>, where each entry <italic>D</italic>[<italic>j</italic>,<italic>C</italic><sub><italic>j</italic></sub>] contains the minimum number of corrections to obtain a <italic>k</italic>-correction matrix <italic>C</italic> for <italic>M</italic> on columns <italic>M</italic><sub>1</sub>,…,<italic>M</italic><sub><italic>j</italic></sub> such that the columns <italic>C</italic><sub><italic>j</italic></sub> are heterozygous. For the sake of simplicity, we pose <italic>D</italic>[0,·]=0. For 0&lt;<italic>j</italic>≤<italic>m</italic>, the recurrence equation for <italic>D</italic>[<italic>j</italic>,<italic>C</italic><sub><italic>j</italic></sub>] is the following, where <italic>δ</italic><sub><italic>j</italic></sub> is the set of all heterozygous <italic>k</italic>-corrections of the column <italic>M</italic><sub><italic>j</italic></sub>. 
<disp-formula id="Equa"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$D[j, C_{j}] = \min_{C_{j-1} \in \delta_{j-1}, C_{j} \approx C_{j-1}} \bigg\{ D[j-1, C_{j-1}] + d(M_{j}, C_{j}) \bigg\}. $$ \end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>D</mml:mi><mml:mo>[</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mstyle mathsize="2.03em"><mml:mfenced close="" open="{" separators=""><mml:mrow/></mml:mfenced></mml:mstyle><mml:mi>D</mml:mi><mml:mo>[</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>+</mml:mo><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mstyle mathsize="2.03em"><mml:mfenced close="" open="}" separators=""><mml:mrow/></mml:mfenced></mml:mstyle><mml:mi>.</mml:mi></mml:mrow></mml:math><graphic xlink:href="12859_2018_2253_Article_Equa.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>For the complete description of the dynamic programming recurrence we refer the reader to [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR35">35</xref>]. In fact, as reported in the original HapCol paper [<xref ref-type="bibr" rid="CR35">35</xref>], this FPT algorithm is exponential in the number <italic>k</italic> of allowed corrections in each position. Therefore, we developed a preprocessing step which merges reads belonging to the same haplotype based on a graph clustering method. Moreover, we also improved the HapCol method by introducing a heuristic procedure to cope with problematic positions, i.e. those requiring more than <italic>k</italic> corrections.</p>
      <p>As anticipated, the combination of all these improvements allowed the possibility of reconstructing haplotypes using higher coverage reads (w.r.t. the original HapCol method), while reducing the runtimes. We now detail these two improvements.</p>
    </sec>
    <sec id="Sec4">
      <title>Preprocessing</title>
      <p>The first step of our pipeline is to merge pairs of fragments that, with high probability, originate from the same haplotype. With <italic>p</italic> we denote the (average) probability that any single base has been read incorrectly, i.e., that a nucleotide in the input BAM (<italic>Binary Alignment Map</italic>: a binary version of the <italic>Sequence Alignment Map</italic> (SAM) format) file is wrong — we recall that <italic>p</italic>≈0.15 and that errors are uniformly distributed for PacBio reads. Let <italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub> be two reads that share <italic>m</italic>+<italic>x</italic> sites, where they agree on <italic>m</italic> of those sites and disagree on the other <italic>x</italic> sites. For this pair of reads, we compute a likelihood under the hypothesis that the reads originate from the same haplotype, and a likelihood under the hypothesis that the reads originate from different haplotypes. We then compute the ratio of these two likelihoods. This idea is similar to the one adopted in [<xref ref-type="bibr" rid="CR39">39</xref>], but our use is different.</p>
      <p>Then, the probability of obtaining the two reads <italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub> under the hypothesis that they originate from the same haplotype is approximately <italic>p</italic><sub><italic>s</italic></sub>(<italic>r</italic><sub>1</sub>,<italic>r</italic><sub>2</sub>)=(1−<italic>p</italic>)<sup>2<italic>m</italic></sup><italic>p</italic><sup><italic>x</italic></sup>(1−<italic>p</italic>/3)<sup><italic>x</italic></sup>, that is we assume that we have no error in the shared part and exactly one error on the other sites. Similarly, the probability of obtaining the two reads <italic>r</italic><sub>1</sub> and <italic>r</italic><sub>2</sub> under the hypothesis that they originate from two different haplotypes is approximately <italic>p</italic><sub><italic>d</italic></sub>(<italic>r</italic><sub>1</sub>,<italic>r</italic><sub>2</sub>)=<italic>p</italic><sup><italic>m</italic></sup>(1−<italic>p</italic>/3)<sup><italic>m</italic></sup>(1−<italic>p</italic>/3)<sup><italic>x</italic></sup>(1−<italic>p</italic>)<sup><italic>x</italic></sup>, that is we assume that there is exactly one error in the sites with same value and at most an error in the sites with different values.</p>
      <p>A simple approach to reduce the size of the instance is to merge all pairs (<italic>r</italic><sub>1</sub>,<italic>r</italic><sub>2</sub>) of fragments such that <italic>p</italic><sub><italic>s</italic></sub>(<italic>r</italic><sub>1</sub>,<italic>r</italic><sub>2</sub>) is sufficiently large. But that would also merge some pairs of fragments whose probability <italic>p</italic><sub><italic>d</italic></sub> is too large. Since we want to be conservative in merging fragments, we partition the fragment set into clusters such that <italic>p</italic><sub><italic>s</italic></sub>/<italic>p</italic><sub><italic>d</italic></sub>≥10<sup>6</sup> for each pair of fragments in the cluster. This threshold was obtained empirically, in order to achieve the best performance in terms of quality of the predictions in the performed experimental analysis. Then, for each site, the character that is the result of a merge is chosen applying a majority rule, weighted by the Phred score of each symbol. Notice that the merging heuristic of ProbHap [<xref ref-type="bibr" rid="CR39">39</xref>] considers only the ratio to determine when to merge two reads, while we analyze all pairs of reads to determine which <italic>sets</italic> of reads to merge.</p>
    </sec>
    <sec id="Sec5">
      <title>Adaptive k-cMEC</title>
      <p>Here, we describe how we modified the HapCol dynamic programming recurrence in order to deal with problematic columns for which the maximum allowed number of corrections is not enough to obtain a solution. As stated in the original HapCol paper [<xref ref-type="bibr" rid="CR35">35</xref>], the number <italic>k</italic><sub><italic>j</italic></sub> of corrections for each column <italic>M</italic><sub><italic>j</italic></sub> is computed, based on its coverage <italic>c</italic><italic>o</italic><italic>v</italic><sub><italic>j</italic></sub> and on two input parameters: <italic>ε</italic> (average <italic>error-rate</italic>) and <italic>α</italic> (the probability that the column <italic>M</italic><sub><italic>j</italic></sub> has more than <italic>k</italic><sub><italic>j</italic></sub> errors). The idea is that the number of errors in a column <italic>j</italic> follows a binomial distribution, and hence we allow the lowest value <italic>k</italic><sub><italic>j</italic></sub> such that the probability of having more than <italic>k</italic><sub><italic>j</italic></sub> errors (with error rate <italic>ε</italic>) is at most <italic>α</italic>. This is done in order to bound the value of <italic>k</italic>, which is fundamental since HapCol implements an FPT algorithm that is exponential in the maximum number of allowed corrections. For this reason, we would prefer to have low values of <italic>k</italic><sub><italic>j</italic></sub>. A side effect of this approach is that, when all solutions of an instance contain a column with more than <italic>k</italic><sub><italic>j</italic></sub> errors, HapCol is not able to find a solution. Therefore, we developed a heuristic procedure which has the final goal of guaranteeing that a solution is found, by slightly increasing the allowed number of errors beyond <italic>k</italic><sub><italic>j</italic></sub>, such that a solution exists for this number. We recall that the recurrence equation governing the original dynamic programming approach considers all <italic>k</italic><sub><italic>j</italic></sub>-correction <italic>C</italic><sub><italic>j</italic></sub>∈<italic>δ</italic><sub><italic>j</italic></sub>. We slightly modify the definition of <italic>k</italic>-corrections to cope with those problematic columns, by increasing the number of allowed corrections. Let <italic>C</italic><sub><italic>j</italic>,<italic>k</italic></sub> be a <italic>k</italic>-correction of <italic>M</italic><sub><italic>j</italic></sub> with exactly <italic>k</italic> corrections, let <italic>z</italic><sub><italic>j</italic>,0</sub>=<italic>k</italic><sub><italic>j</italic></sub> and <italic>z</italic><sub><italic>j</italic>,<italic>i</italic></sub>=<italic>z</italic><sub><italic>j</italic>,<italic>i</italic>−1</sub>+⌊log2(<italic>z</italic><sub><italic>j</italic>,<italic>i</italic>−1</sub>)+1⌋, i.e., each term is obtained from the previous one by adding a logarithmic term, to guarantee that the number of allowed corrections does not grow too quickly. Then <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$k_{j}^{*} = \min _{i: D[j, C_{j,z_{j,i-1}}] \neq \infty }\left \{ z_{j,i}\right \}$\end{document}</tex-math><mml:math id="M4"><mml:msubsup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:mi>D</mml:mi><mml:mo>[</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>]</mml:mo><mml:mo>≠</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:munder><mml:mfenced close="}" open="{" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2018_2253_Article_IEq1.gif"/></alternatives></inline-formula> if <italic>i</italic>&gt;0, where <italic>D</italic>[·,·]≠<italic>∞</italic> means it is a feasible correction. Starting from this notation, the new set of possible corrections of column <italic>M</italic><sub><italic>j</italic></sub> is 
<disp-formula id="Equb"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$\delta_{j} = \big\{ C_{j,k} : 1\leq k \leq k_{j}^{*}\big\}. $$ \end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle mathsize="1.19em"><mml:mfenced close="" open="{" separators=""><mml:mrow/></mml:mfenced></mml:mstyle><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>k</mml:mi><mml:mo>≤</mml:mo><mml:msubsup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mstyle mathsize="1.19em"><mml:mfenced close="" open="}" separators=""><mml:mrow/></mml:mfenced></mml:mstyle><mml:mi>.</mml:mi></mml:mrow></mml:math><graphic xlink:href="12859_2018_2253_Article_Equb.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Notice that the sequence of <italic>z</italic><sub><italic>j</italic>,<italic>i</italic></sub> is monotonically increasing with <italic>i</italic>, hence we can compute <inline-formula id="IEq2"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$k_{j}^{*}$\end{document}</tex-math><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2018_2253_Article_IEq2.gif"/></alternatives></inline-formula> by starting with <italic>k</italic><sub><italic>j</italic></sub> and increasing it until we are able to find a <inline-formula id="IEq3"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$k_{j}^{*}$\end{document}</tex-math><mml:math id="M10"><mml:msubsup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2018_2253_Article_IEq3.gif"/></alternatives></inline-formula>-correction for the column <italic>M</italic><sub><italic>j</italic></sub>. The dynamic programming equation is unchanged, but our new construction of the set <italic>δ</italic><sub><italic>j</italic></sub> guarantees that we are always able to compute a solution. Moreover, just as for HapCol, we cannot guarantee that we solve optimally the instance of the MEC problem.</p>
      <p>One of the key points of this procedure is how we increment <italic>z</italic><sub><italic>j</italic>,<italic>i</italic></sub>, that is by adding a logarithmic quantity. This guarantees a balance between finding a low value of <inline-formula id="IEq4"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$k_{j}^{*}$\end{document}</tex-math><mml:math id="M12"><mml:msubsup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2018_2253_Article_IEq4.gif"/></alternatives></inline-formula> and the running time needed for the computation.</p>
    </sec>
  </sec>
  <sec id="Sec6">
    <title>Results and Discussion</title>
    <p>We now describe the results of our experiments. In the first subsection, we describe the data that we use, or simulate. Then we detail the experiments that we set up in order to compare our tool with others in the next subsection. Finally, we present and discuss the results of these experiments.</p>
    <sec id="Sec7">
      <title>Data description</title>
      <p>The Genome in a Bottle (GIAB) Consortium has released publicly available high-quality sequencing data for seven individuals, using eleven different technologies [<xref ref-type="bibr" rid="CR40">40</xref>–<xref ref-type="bibr" rid="CR42">42</xref>]. Since our goal is to assess the performance of different single-individual haplotype phasing methods, we study chromosome 1 of the Ashkenazim individual NA24385, as well as chromosomes 1–22 of individual NA12878.</p>
      <p>The Ashkenazim individual is the son in a mother-father-son trio. We downloaded from GIAB the genotype variants call sets NIST_CallsIn2Technologies_05182015, a set of variants for each individual of this trio that have been called by at least two independent variant calling technologies. In order to be able to compare against methods that use reference panels or information from multiple individuals, <italic>e.g</italic>, a trio, for single-individual haplotype phasing, we considered all the bi-allelic SNVs of the chromosome that: (a) appear also in the 1000 Genomes reference panel <ext-link ext-link-type="uri" xlink:href="https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3.tgz">https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3.tgz</ext-link>, and (b) have been called in all three individuals of the Ashkenazim trio, i.e., also in the mother and the father. For chromosome 1, this resulted in 140744 SNVs, of which 48023 are heterozygous. We refer to this set of SNVs as the set of benchmark SNVs for this dataset – the set is in the form of a VCF (<italic>Variant Call Format</italic>) file. Since the authors of [<xref ref-type="bibr" rid="CR43">43</xref>] also studied this trio, and have made the pipeline for collecting and generating their data publicly available at <ext-link ext-link-type="uri" xlink:href="https://bitbucket.org/whatshap/phasing-comparison-experiments/">https://bitbucket.org/whatshap/phasing-comparison-experiments/</ext-link>, we use or modify parts of this pipeline to generate our data as detailed in the following.</p>
      <p>As for the individual NA12878, we downloaded the latest high confidence phased VCF of GIAB for hg37 (human genome version 37), available at <ext-link ext-link-type="uri" xlink:href="ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh37/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz">ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh37/HG001_GRCh37_ GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz</ext-link>, and used all SNVs in this file as our set of benchmark SNVs for the respective chromosomes.</p>
      <sec id="Sec8">
        <title>GIAB PacBio Reads</title>
        <p>One of the more recent technologies producing long reads – those which are the most informative for read-based phasing – is the Pacific Biosciences (PacBio) platform. PacBio is one of the eleven technologies on which GIAB provides sequencing reads.</p>
        <p>We hence downloaded the set of aligned PacBio reads from <ext-link ext-link-type="uri" xlink:href="ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_MtSinai_NIST/MtSinai_blasr_bam_GRCh37/hg002_gr37_1.bam">ftp: //ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_MtSinai_NIST/MtSinai_blasr_bam_GRCh37/hg002_gr37_1.bam</ext-link>for chromosome 1 of the Ashkenazim individual, which has an average coverage of 60.2× and an average mapped read length of 8687 bp (basepairs). We then downsampled the read set to average coverages of 25×, 30×, 35×, 40×, 45×, 50×, 55×, and 60×. This was done using the DownsampleSam subcommand of Picard Tools, which randomly downsamples a read set by selecting each read with probability <italic>p</italic>. We downsample recursively, so that each downsampled read set with a given average coverage is a subset of any downsampled read set with an average coverage higher than this set.</p>
        <p>As for individual NA12878, we downloaded the set of aligned PacBio reads <ext-link ext-link-type="uri" xlink:href="ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NA12878_PacBio_MtSinai/sorted_final_merged.bam">ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NA12878_PacBio_MtSinai/sorted_final_merged.bam</ext-link>, which comprises chromosomes 1–22. The average coverages (resp., mapped read lengths) ranged between 26.9 and 44.2 (resp., 4746 and 5285), so we did not perform any downsampling for this dataset.</p>
        <p>As a phasing benchmark for the Ashkenazim chromosome 1, we used the latest high confidence trio-phased VCF of GIAB for hg37, available at <ext-link ext-link-type="uri" xlink:href="ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/latest/GRCh37/HG002_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-22_v.3.3.2_highconf_triophased.vcf.gz">ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/latest/GRCh37/ HG002_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-22_v.3.3.2_highconf_triophased.vcf.gz</ext-link>. As for chromosomes 1–22 of the individual NA12878, we used the (original, i.e., phased version of the) high confidence phased VCF mentioned in the previous section.</p>
      </sec>
      <sec id="Sec9">
        <title>Simulated PacBio Data</title>
        <p>Aside from the PacBio data described in the previous section, we also produce and run our experiments on a simulated read set for chromosome 1 of the Ashkenazim individual. Reference panels may leave out some variants with low allele frequency – a good reason for doing read-based phasing – and statistical methods might be susceptible to systematic bias in the data. For these reasons, we complement our study with an experimental analysis on simulated reads, as follows.</p>
        <p>We first obtain a pair of “true” haplotypes off of which we simulate reads. This is obtained from the output of the population-based phasing tool SHAPEITv2-r837 [<xref ref-type="bibr" rid="CR44">44</xref>] with default parameters on the 1000 Genomes reference panel, the corresponding genetic map <ext-link ext-link-type="uri" xlink:href="http://www.shapeit.fr/files/genetic_map_b37.tar.gz">http://www.shapeit.fr/files/genetic_map_b37.tar.gz</ext-link>, and the unphased genotypes, i.e., the set of benchmark SNVs of this chromosome.</p>
        <p>Given the phasing by SHAPEIT, we incorporate the (benchmark) SNVs of the first haplotype of this phasing into the reference genome (hg37) by flipping the variant sites that are the alternative allele in this haplotype. The second haplotype is obtained analogously. Using these two true haplotypes as the input, we produce a corresponding set of reads for this haplotype using PBSIM [<xref ref-type="bibr" rid="CR45">45</xref>], a PacBio-specific read simulator. We input to PBSIM the optional parameters --depth 60 so that our simulated reads have sufficient coverage, and as --sample-fastq a sample of the original GIAB PacBio reads described in the previous section, so that our simulated reads have the same length and accuracy profile as the corresponding real read set. We align the resulting simulated reads to the reference genome using BWA-MEM 0.7.12-r1039 [<xref ref-type="bibr" rid="CR46">46</xref>] with optional parameter -x pacbio. Finally, this pair of aligned read sets, representing the reads coming off of each haplotype is merged using the MergeSamFiles subcommand of Picard Tools, obtaining the final simulated read set. In the same way as we have done with the read sets for the real Chromosome 1, we downsample to average coverages 25×, 30×, 35×, 40×, 45×, 50×, 55×, and 60×.</p>
        <p>To summarize, the data we use or simulate regards both real and simulated reads on chromosome 1 of the Ashkenazim individual for a set of 8 average coverages, for a total of 16 read sets, each in the form of a BAM file. The autosomes of individual NA12878 adds an additional 22 read sets, each in the form of a BAM file. It is on these 38 read sets, along with their corresponding set of benchmark SNVs – in the form of VCF files – that we carry out our experiments, as described in the following section.</p>
      </sec>
    </sec>
    <sec id="Sec10">
      <title>Experimental Setup</title>
      <p>We compare our tool HapCHAT to the most recent state-of-the-art read-based phasing methods of WhatsHap [<xref ref-type="bibr" rid="CR34">34</xref>, <xref ref-type="bibr" rid="CR37">37</xref>], HapCol [<xref ref-type="bibr" rid="CR35">35</xref>], HapCUT2 [<xref ref-type="bibr" rid="CR17">17</xref>], ProbHap [<xref ref-type="bibr" rid="CR39">39</xref>], ReFHap [<xref ref-type="bibr" rid="CR19">19</xref>] and FastHare [<xref ref-type="bibr" rid="CR15">15</xref>] by running them all on the data described in the previous subsection. Recall that, as detailed in the introduction, WhatsHap, HapCol and HapCHAT are approaches with a core phasing algorithm that is FPT either in the coverage or in the number of errors at each SNV site. Hence the coverage must first be reduced to some target maximum coverage before its core algorithm can be run. Each run of a tool on a dataset is given a time limit of one day, and a memory limit of 64GB. We now describe the details of how we parameterized each tool for comparison in what follows.</p>
      <sec id="Sec11">
        <title>WhatsHap</title>
        <p>For each read set, we provide to WhatsHap (version 0.13) the corresponding BAM and VCF file. We run WhatsHap on this input pair on otherwise default settings, with the exception of providing it the reference genome (hg37) via the optional parameter --reference. This allows WhatsHap to run in <italic>realignment mode</italic>, which has been shown to significantly boost accuracy predictions for noisy read sets such as PacBio, as detailed in [<xref ref-type="bibr" rid="CR37">37</xref>]. In particular, this mode is well suited to handle the abundant indel errors in the input reads. WhatsHap has a built-in read selection procedure [<xref ref-type="bibr" rid="CR47">47</xref>] which subsequently prunes to a default maximum coverage of 15 before the core phasing algorithm is called. The default value has been selected by the authors of WhatsHap to provide the best trade-off between quality of the results and runtime [<xref ref-type="bibr" rid="CR48">48</xref>]. Additionally, we run WhatsHap in realignment mode as above, but fixing to target maximum coverage 20 by providing the additional optional parameter -H 20. It is the resulting set of phasings by WhatsHap, in the form of phased VCF, that we use for the basis of comparison with the other methods.</p>
      </sec>
      <sec id="Sec12">
        <title>HapCol</title>
        <p>For each read set, together with the VCF file of the corresponding chromosome, we convert it to the custom input format for HapCol. Since HapCol does not have a read selection procedure – something it does need for data at 35× (or higher) coverage (<italic>cf.</italic> the Introduction) — we then apply the read selection procedure of [<xref ref-type="bibr" rid="CR47">47</xref>] to prune this set to the target maximum coverages of 15×, 20×, 25×, and 30×. On these resulting input files, we run HapCol with its default value of <italic>α</italic>=0.01 (and of <italic>ε</italic>=0.05) (<italic>cf.</italic> the subsection on Adaptive k-cMEC or [<xref ref-type="bibr" rid="CR35">35</xref>] for details on the meaning of <italic>α</italic> and <italic>ε</italic>). Since HapCol is not adaptive, but we want to give it a chance to obtain a solution on its instance, should a given <italic>α</italic> be infeasible (<italic>cf.</italic> the subsection on Adaptive k-cMEC), we continue to rerun HapCol with an <italic>α</italic> of one tenth the size of the previous until a solution exists. HapCol outputs a pair of binary strings representing the phasing, which we then convert to phased VCF. Note that we did not further attempt any higher maximum coverages, because at maximum coverage 30, HapCol either exceeded one day of runtime or 64GB of memory on every dataset. It is this set of resulting phasings (phased VCF files) that we use to compare with the other methods.</p>
      </sec>
      <sec id="Sec13">
        <title>ProbHap, RefHap and FastHare</title>
        <p>For each read set, we use the extractHAIRS program that is distributed with the original HapCut [<xref ref-type="bibr" rid="CR16">16</xref>] to convert its BAM / VCF pair into the custom input format for these methods. We then ran each method on these instances with default settings, each producing a custom input which is then converted to a phased VCF with the subcommand hapcut2vcf of the WhatsHap toolbox.</p>
      </sec>
      <sec id="Sec14">
        <title>HapCUT2</title>
        <p>For each read set, we use the extractHAIRS program that comes with HapCUT2, with parameter --pacbio 1, which activates a newly-developed realignment procedure for pacbio reads, to convert its BAM / VCF pair into the custom input format for HapCUT2. We then ran HapCUT2 on the resulting instances with default settings, each producing a custom output which is then converted to phased VCF with the subcommand hapcut2vcf of the WhatsHap toolbox.</p>
      </sec>
      <sec id="Sec15">
        <title>HapCHAT</title>
        <p>For each read set, we provide to HapCHAT the corresponding BAM and VCF file. We run HapCHAT on this input pair on otherwise default settings, with the exception of providing it the reference genome (hg37) via the optional parameter --reference. This allows HapCHAT to run in realignment mode like with WhatsHap, thanks to the partial integration of HapCHAT into the WhatsHap codebase. We then apply our merging step as described in the subsection Preprocessing, which reduces the coverage. If necessary, the reads are further selected via a greedy selection approach (based on the Phred score), with ties broken at random, to downsample each dataset to the target maximum coverages of 15×, 20×, 25×, and 30×. It is the resulting phasings, in phased VCF format, for which the comparison of HapCHAT to other methods is based.</p>
      </sec>
    </sec>
    <sec id="Sec16">
      <title>Experimental results and discussion</title>
      <p>The times reported here do not include the time necessary to read the input (BAM) file, which is more-or-less the same for each method. The results are summarized in Tables <xref rid="Tab1" ref-type="table">1</xref>, <xref rid="Tab2" ref-type="table">2</xref>, <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref>, <xref rid="Tab5" ref-type="table">5</xref>, <xref rid="Tab6" ref-type="table">6</xref>, <xref rid="Tab7" ref-type="table">7</xref>, <xref rid="Tab8" ref-type="table">8</xref>, <xref rid="Tab9" ref-type="table">9</xref>, <xref rid="Tab10" ref-type="table">10</xref>, <xref rid="Tab11" ref-type="table">11</xref>, <xref rid="Tab12" ref-type="table">12</xref>, <xref rid="Tab13" ref-type="table">13</xref>, <xref rid="Tab14" ref-type="table">14</xref>, <xref rid="Tab15" ref-type="table">15</xref>, <xref rid="Tab16" ref-type="table">16</xref>, <xref rid="Tab17" ref-type="table">17</xref>, and Figs. <xref rid="Fig1" ref-type="fig">1</xref>,<xref rid="Fig2" ref-type="fig">2</xref> and <xref rid="Fig3" ref-type="fig">3</xref>.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Switch error rate and Hamming distance as a function of running time. As achieved by HapCHAT and WhatsHap at different maximum coverages on the real Ashkenazim Chromosome 1 dataset. For each tool and each maximum coverage, we represent a point for each of the 8 possible values of the average coverage</p></caption><graphic xlink:href="12859_2018_2253_Fig1_HTML" id="MO1"/></fig>
<fig id="Fig2"><label>Fig. 2</label><caption><p>Quality measures on the real Ashkenazim Chromosome 1 dataset. We present the bar plots showing the measures of switch error percentage and QAN50 achieved by HapCHAT, WhatsHap, and HapCUT2 on the Ashkenazim Chromosome 1 dataset at different coverage values</p></caption><graphic xlink:href="12859_2018_2253_Fig2_HTML" id="MO2"/></fig>
<fig id="Fig3"><label>Fig. 3</label><caption><p>Quality measures on the real NA12878 dataset. We present the bar plots showing the measures of switch error percentage and QAN50 achieved by HapCHAT, WhatsHap, and HapCUT2 on the different chromosome datasets of NA12878</p></caption><graphic xlink:href="12859_2018_2253_Fig3_HTML" id="MO3"/></fig>
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Switch error percentage on the real Ashkenazim dataset, Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Avg.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left">Cov.</th><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">25</td><td align="left"><bold>0.334</bold></td><td align="left">0.662</td><td align="left">0.342</td><td align="left">0.342</td><td align="left">0.342</td><td align="left">2.813</td><td align="left">3.303</td><td align="left">3.547</td></tr><tr><td align="left">30</td><td align="left">0.324</td><td align="left">0.623</td><td align="left">0.337</td><td align="left">0.333</td><td align="left"><bold>0.308</bold></td><td align="left">2.420</td><td align="left">2.980</td><td align="left">3.133</td></tr><tr><td align="left">35</td><td align="left"><bold>0.320</bold></td><td align="left">0.601</td><td align="left">0.324</td><td align="left">0.332</td><td align="left">0.333</td><td align="left">2.221</td><td align="left">-</td><td align="left">2.933</td></tr><tr><td align="left">40</td><td align="left"><bold>0.324</bold></td><td align="left">0.575</td><td align="left">0.336</td><td align="left">0.332</td><td align="left">0.332</td><td align="left">2.027</td><td align="left">-</td><td align="left">2.691</td></tr><tr><td align="left">45</td><td align="left"><bold>0.323</bold></td><td align="left">0.533</td><td align="left">0.348</td><td align="left">0.336</td><td align="left">0.328</td><td align="left">1.932</td><td align="left">-</td><td align="left">2.522</td></tr><tr><td align="left">50</td><td align="left"><bold>0.323</bold></td><td align="left">0.490</td><td align="left">0.340</td><td align="left"><bold>0.323</bold></td><td align="left">0.327</td><td align="left">1.864</td><td align="left">-</td><td align="left">2.303</td></tr><tr><td align="left">55</td><td align="left"><bold>0.323</bold></td><td align="left">0.452</td><td align="left">0.327</td><td align="left">0.331</td><td align="left"><bold>0.323</bold></td><td align="left">1.774</td><td align="left">-</td><td align="left">2.268</td></tr><tr><td align="left">60</td><td align="left">0.327</td><td align="left">0.452</td><td align="left">0.326</td><td align="left"><bold>0.322</bold></td><td align="left"><bold>0.322</bold></td><td align="left">1.740</td><td align="left">-</td><td align="left">2.123</td></tr></tbody></table><table-wrap-foot><p>For each dataset, its row identified by its average coverage (Avg. Cov.). We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare. The best result (lowest value) for each dataset is boldfaced</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Hamming distance on the real Ashkenazim dataset, Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Avg.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left">Cov.</th><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">25</td><td align="left">0.54</td><td align="left">2.41</td><td align="left">0.64</td><td align="left">0.84</td><td align="left"><bold>0.44</bold></td><td align="left">3.96</td><td align="left">3.42</td><td align="left">5.53</td></tr><tr><td align="left">30</td><td align="left">0.35</td><td align="left">2.18</td><td align="left">0.64</td><td align="left">0.60</td><td align="left"><bold>0.24</bold></td><td align="left">3.46</td><td align="left">3.41</td><td align="left">5.38</td></tr><tr><td align="left">35</td><td align="left"><bold>0.36</bold></td><td align="left">2.02</td><td align="left">0.37</td><td align="left">0.42</td><td align="left">0.37</td><td align="left">3.99</td><td align="left">-</td><td align="left">5.62</td></tr><tr><td align="left">40</td><td align="left"><bold>0.37</bold></td><td align="left">1.66</td><td align="left">0.45</td><td align="left">0.44</td><td align="left"><bold>0.37</bold></td><td align="left">3.10</td><td align="left">-</td><td align="left">5.08</td></tr><tr><td align="left">45</td><td align="left">0.38</td><td align="left">1.80</td><td align="left">0.43</td><td align="left">0.42</td><td align="left"><bold>0.37</bold></td><td align="left">3.02</td><td align="left">-</td><td align="left">4.49</td></tr><tr><td align="left">50</td><td align="left">0.41</td><td align="left">1.47</td><td align="left">0.41</td><td align="left">0.38</td><td align="left"><bold>0.35</bold></td><td align="left">2.84</td><td align="left">-</td><td align="left">4.32</td></tr><tr><td align="left">55</td><td align="left">0.40</td><td align="left">0.87</td><td align="left"><bold>0.36</bold></td><td align="left">0.41</td><td align="left">0.37</td><td align="left">3.28</td><td align="left">-</td><td align="left">4.67</td></tr><tr><td align="left">60</td><td align="left">0.39</td><td align="left">1.25</td><td align="left"><bold>0.34</bold></td><td align="left">0.36</td><td align="left">0.35</td><td align="left">3.60</td><td align="left">-</td><td align="left">5.06</td></tr></tbody></table><table-wrap-foot><p>For each dataset, its row identified by its average coverage (Avg. Cov.). We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare. The best result (lowest value) for each dataset is boldfaced</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab3"><label>Table 3</label><caption><p>QAN50 on the real Ashkenazim dataset, Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Avg.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left">Cov.</th><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">25</td><td align="left">79452</td><td align="left">76856</td><td align="left"><bold>79515</bold></td><td align="left"><bold>79515</bold></td><td align="left">78192</td><td align="left">48097</td><td align="left">45492</td><td align="left">45445</td></tr><tr><td align="left">30</td><td align="left"><bold>80662</bold></td><td align="left">80150</td><td align="left">80426</td><td align="left">80426</td><td align="left">80150</td><td align="left">52713</td><td align="left">50806</td><td align="left">49308</td></tr><tr><td align="left">35</td><td align="left"><bold>81842</bold></td><td align="left">81464</td><td align="left">81757</td><td align="left">81757</td><td align="left">81464</td><td align="left">54182</td><td align="left">-</td><td align="left">51766</td></tr><tr><td align="left">40</td><td align="left"><bold>83968</bold></td><td align="left">82758</td><td align="left">83802</td><td align="left">83802</td><td align="left">83263</td><td align="left">57589</td><td align="left">-</td><td align="left">55014</td></tr><tr><td align="left">45</td><td align="left"><bold>87267</bold></td><td align="left">86001</td><td align="left"><bold>87267</bold></td><td align="left"><bold>87267</bold></td><td align="left">86001</td><td align="left">59161</td><td align="left">-</td><td align="left">57008</td></tr><tr><td align="left">50</td><td align="left">89669</td><td align="left">89738</td><td align="left"><bold>89858</bold></td><td align="left"><bold>89858</bold></td><td align="left">89306</td><td align="left">60380</td><td align="left">-</td><td align="left">59447</td></tr><tr><td align="left">55</td><td align="left"><bold>91434</bold></td><td align="left"><bold>91434</bold></td><td align="left">91224</td><td align="left">91224</td><td align="left">90718</td><td align="left">62652</td><td align="left">-</td><td align="left">59582</td></tr><tr><td align="left">60</td><td align="left">94913</td><td align="left">92938</td><td align="left"><bold>95818</bold></td><td align="left"><bold>95818</bold></td><td align="left">92565</td><td align="left">64710</td><td align="left">-</td><td align="left">62655</td></tr></tbody></table><table-wrap-foot><p>For each dataset, its row identified by its average coverage (Avg. Cov.). We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare. The best result (highest value) for each dataset is boldfaced</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Time in seconds of the tools on real Ashkenazim datasets of Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Avg.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left">Cov.</th><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">25</td><td align="left">591</td><td align="left">39456</td><td align="left">1115</td><td align="left">9278</td><td align="left">1563</td><td align="left">80</td><td align="left">43573</td><td align="left">3</td></tr><tr><td align="left">30</td><td align="left">1292</td><td align="left">46564</td><td align="left">1031</td><td align="left">10753</td><td align="left">1596</td><td align="left">196</td><td align="left">72696</td><td align="left">4</td></tr><tr><td align="left">35</td><td align="left">2193</td><td align="left">50071</td><td align="left">1122</td><td align="left">11959</td><td align="left">1888</td><td align="left">308</td><td align="left">-</td><td align="left">4</td></tr><tr><td align="left">40</td><td align="left">3095</td><td align="left">50301</td><td align="left">1247</td><td align="left">12570</td><td align="left">2160</td><td align="left">499</td><td align="left">-</td><td align="left">5</td></tr><tr><td align="left">45</td><td align="left">3888</td><td align="left">51570</td><td align="left">1308</td><td align="left">12735</td><td align="left">2388</td><td align="left">822</td><td align="left">-</td><td align="left">6</td></tr><tr><td align="left">50</td><td align="left">4579</td><td align="left">53030</td><td align="left">1395</td><td align="left">12996</td><td align="left">2731</td><td align="left">1192</td><td align="left">-</td><td align="left">8</td></tr><tr><td align="left">55</td><td align="left">5103</td><td align="left">54012</td><td align="left">1534</td><td align="left">13252</td><td align="left">2983</td><td align="left">1777</td><td align="left">-</td><td align="left">9</td></tr><tr><td align="left">60</td><td align="left">5550</td><td align="left">53496</td><td align="left">1605</td><td align="left">13469</td><td align="left">3216</td><td align="left">2493</td><td align="left">-</td><td align="left">13</td></tr></tbody></table><table-wrap-foot><p>For each dataset, its row identified by its average coverage (Avg. Cov.). We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Peak of RAM usage in Megabytes of the tools on real Ashkenazim datasets of Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Avg.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left">Cov.</th><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">25</td><td align="left">1370</td><td align="left">2263</td><td align="left">930</td><td align="left">5510</td><td align="left">3266</td><td align="left">3005</td><td align="left">4693</td><td align="left">3005</td></tr><tr><td align="left">30</td><td align="left">1661</td><td align="left">2562</td><td align="left">931</td><td align="left">6195</td><td align="left">3270</td><td align="left">3005</td><td align="left">5355</td><td align="left">3005</td></tr><tr><td align="left">35</td><td align="left">1966</td><td align="left">2908</td><td align="left">931</td><td align="left">6513</td><td align="left">3276</td><td align="left">3005</td><td align="left">-</td><td align="left">3005</td></tr><tr><td align="left">40</td><td align="left">2291</td><td align="left">3231</td><td align="left">931</td><td align="left">6483</td><td align="left">3279</td><td align="left">3005</td><td align="left">-</td><td align="left">3005</td></tr><tr><td align="left">45</td><td align="left">2636</td><td align="left">3190</td><td align="left">952</td><td align="left">6937</td><td align="left">3283</td><td align="left">3005</td><td align="left">-</td><td align="left">3005</td></tr><tr><td align="left">50</td><td align="left">3158</td><td align="left">3286</td><td align="left">1007</td><td align="left">7144</td><td align="left">3287</td><td align="left">3005</td><td align="left">-</td><td align="left">3005</td></tr><tr><td align="left">55</td><td align="left">3549</td><td align="left">3479</td><td align="left">1042</td><td align="left">7229</td><td align="left">3292</td><td align="left">3005</td><td align="left">-</td><td align="left">3005</td></tr><tr><td align="left">60</td><td align="left">3968</td><td align="left">5412</td><td align="left">1073</td><td align="left">7430</td><td align="left">3296</td><td align="left">3005</td><td align="left">-</td><td align="left">3005</td></tr></tbody></table><table-wrap-foot><p>For each dataset, its row identified by its average coverage (Avg. Cov.). We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Switch error percentage on simulated datasets of Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Avg.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left">Cov.</th><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">25</td><td align="left"><bold>0.035</bold></td><td align="left">0.218</td><td align="left"><bold>0.035</bold></td><td align="left">0.039</td><td align="left">0.037</td><td align="left">1.081</td><td align="left">1.487</td><td align="left">2.112</td></tr><tr><td align="left">30</td><td align="left"><bold>0.028</bold></td><td align="left">0.181</td><td align="left">0.035</td><td align="left">0.031</td><td align="left">0.037</td><td align="left">0.725</td><td align="left">1.166</td><td align="left">1.430</td></tr><tr><td align="left">35</td><td align="left"><bold>0.028</bold></td><td align="left">0.161</td><td align="left">0.033</td><td align="left">0.037</td><td align="left">0.037</td><td align="left">0.537</td><td align="left">0.879</td><td align="left">1.086</td></tr><tr><td align="left">40</td><td align="left"><bold>0.026</bold></td><td align="left">0.148</td><td align="left"><bold>0.026</bold></td><td align="left">0.030</td><td align="left">0.037</td><td align="left">0.425</td><td align="left">-</td><td align="left">0.901</td></tr><tr><td align="left">45</td><td align="left"><bold>0.022</bold></td><td align="left">0.139</td><td align="left">0.024</td><td align="left">0.024</td><td align="left"><bold>0.022</bold></td><td align="left">0.404</td><td align="left">-</td><td align="left">0.781</td></tr><tr><td align="left">50</td><td align="left">0.020</td><td align="left">0.134</td><td align="left">0.020</td><td align="left">0.024</td><td align="left"><bold>0.018</bold></td><td align="left">0.324</td><td align="left">-</td><td align="left">0.586</td></tr><tr><td align="left">55</td><td align="left">0.022</td><td align="left">0.126</td><td align="left">0.024</td><td align="left">0.022</td><td align="left"><bold>0.018</bold></td><td align="left">0.273</td><td align="left">-</td><td align="left">0.565</td></tr><tr><td align="left">60</td><td align="left"><bold>0.020</bold></td><td align="left">0.108</td><td align="left"><bold>0.020</bold></td><td align="left">0.024</td><td align="left">0.022</td><td align="left">0.248</td><td align="left">-</td><td align="left">0.470</td></tr></tbody></table><table-wrap-foot><p>For each dataset, its row identified by its average coverage (Avg. Cov.). We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare. The best result (lowest value) for each dataset is boldfaced</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab7"><label>Table 7</label><caption><p>Hamming Distance percentage on simulated datasets of Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Avg.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left">Cov.</th><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">25</td><td align="left">0.35</td><td align="left">1.30</td><td align="left">0.42</td><td align="left">0.42</td><td align="left"><bold>0.33</bold></td><td align="left">3.43</td><td align="left">2.66</td><td align="left">5.95</td></tr><tr><td align="left">30</td><td align="left"><bold>0.33</bold></td><td align="left">1.92</td><td align="left">0.43</td><td align="left">0.42</td><td align="left">0.38</td><td align="left">2.52</td><td align="left">2.43</td><td align="left">4.55</td></tr><tr><td align="left">35</td><td align="left"><bold>0.27</bold></td><td align="left">1.37</td><td align="left">0.34</td><td align="left">0.55</td><td align="left">0.37</td><td align="left">1.92</td><td align="left">2.09</td><td align="left">3.93</td></tr><tr><td align="left">40</td><td align="left">0.26</td><td align="left">1.18</td><td align="left"><bold>0.24</bold></td><td align="left">0.41</td><td align="left">0.27</td><td align="left">1.86</td><td align="left">-</td><td align="left">3.31</td></tr><tr><td align="left">45</td><td align="left">0.34</td><td align="left">1.02</td><td align="left">0.32</td><td align="left">0.34</td><td align="left"><bold>0.27</bold></td><td align="left">1.95</td><td align="left">-</td><td align="left">3.12</td></tr><tr><td align="left">50</td><td align="left"><bold>0.27</bold></td><td align="left">1.18</td><td align="left">0.78</td><td align="left">0.81</td><td align="left">0.73</td><td align="left">1.42</td><td align="left">-</td><td align="left">3.14</td></tr><tr><td align="left">55</td><td align="left">0.28</td><td align="left">1.13</td><td align="left">0.76</td><td align="left">0.76</td><td align="left"><bold>0.20</bold></td><td align="left">1.48</td><td align="left">-</td><td align="left">3.19</td></tr><tr><td align="left">60</td><td align="left"><bold>0.13</bold></td><td align="left">1.26</td><td align="left">0.17</td><td align="left">0.16</td><td align="left">0.57</td><td align="left">1.49</td><td align="left">-</td><td align="left">3.49</td></tr></tbody></table><table-wrap-foot><p>For each dataset, its row identified by its average coverage (Avg. Cov.). We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare. The best result (lowest value) for each dataset is boldfaced</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab8"><label>Table 8</label><caption><p>QAN50 results of the tools on real simulated datasets of Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Avg.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left">Cov.</th><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">25</td><td align="left"><bold>87890</bold></td><td align="left">85002</td><td align="left">87581</td><td align="left">87581</td><td align="left">51183</td><td align="left">50325</td><td align="left">49121</td><td align="left">45846</td></tr><tr><td align="left">30</td><td align="left"><bold>93454</bold></td><td align="left">87599</td><td align="left">92831</td><td align="left">92565</td><td align="left">57323</td><td align="left">56745</td><td align="left">54412</td><td align="left">52138</td></tr><tr><td align="left">35</td><td align="left"><bold>96311</bold></td><td align="left">92483</td><td align="left">96167</td><td align="left">95611</td><td align="left">61204</td><td align="left">60612</td><td align="left">59047</td><td align="left">56881</td></tr><tr><td align="left">40</td><td align="left"><bold>97810</bold></td><td align="left">95818</td><td align="left"><bold>97810</bold></td><td align="left">97270</td><td align="left">64979</td><td align="left">64535</td><td align="left">-</td><td align="left">60748</td></tr><tr><td align="left">45</td><td align="left"><bold>100826</bold></td><td align="left">98674</td><td align="left"><bold>100826</bold></td><td align="left"><bold>100826</bold></td><td align="left">68274</td><td align="left">66973</td><td align="left">-</td><td align="left">64003</td></tr><tr><td align="left">50</td><td align="left"><bold>103348</bold></td><td align="left">100826</td><td align="left"><bold>103348</bold></td><td align="left"><bold>103348</bold></td><td align="left">73159</td><td align="left">73256</td><td align="left">-</td><td align="left">69457</td></tr><tr><td align="left">55</td><td align="left">105243</td><td align="left">103348</td><td align="left"><bold>106341</bold></td><td align="left"><bold>106341</bold></td><td align="left">74273</td><td align="left">74402</td><td align="left">-</td><td align="left">71058</td></tr><tr><td align="left">60</td><td align="left">107121</td><td align="left">105243</td><td align="left"><bold>107569</bold></td><td align="left"><bold>107569</bold></td><td align="left">76497</td><td align="left">76497</td><td align="left">-</td><td align="left">73256</td></tr></tbody></table><table-wrap-foot><p>For each dataset, its row identified by its average coverage (Avg. Cov.). We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare. The best result (highest value) for each dataset is boldfaced</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab9"><label>Table 9</label><caption><p>Time in seconds of the tools on simulated datasets of Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Avg.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left">Cov.</th><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">25</td><td align="left">572</td><td align="left">38863</td><td align="left">1027</td><td align="left">9686</td><td align="left">205</td><td align="left">44</td><td align="left">33988</td><td align="left">3</td></tr><tr><td align="left">30</td><td align="left">1317</td><td align="left">47367</td><td align="left">883</td><td align="left">11095</td><td align="left">238</td><td align="left">91</td><td align="left">56165</td><td align="left">3</td></tr><tr><td align="left">35</td><td align="left">2167</td><td align="left">18813</td><td align="left">954</td><td align="left">11650</td><td align="left">286</td><td align="left">167</td><td align="left">80061</td><td align="left">4</td></tr><tr><td align="left">40</td><td align="left">3052</td><td align="left">20007</td><td align="left">1048</td><td align="left">12760</td><td align="left">323</td><td align="left">269</td><td align="left">-</td><td align="left">5</td></tr><tr><td align="left">45</td><td align="left">3754</td><td align="left">56403</td><td align="left">1161</td><td align="left">12678</td><td align="left">367</td><td align="left">423</td><td align="left">-</td><td align="left">6</td></tr><tr><td align="left">50</td><td align="left">4399</td><td align="left">57135</td><td align="left">1170</td><td align="left">12860</td><td align="left">412</td><td align="left">672</td><td align="left">-</td><td align="left">6</td></tr><tr><td align="left">55</td><td align="left">4882</td><td align="left">56745</td><td align="left">1287</td><td align="left">13174</td><td align="left">467</td><td align="left">1019</td><td align="left">-</td><td align="left">7</td></tr><tr><td align="left">60</td><td align="left">5277</td><td align="left">21070</td><td align="left">1336</td><td align="left">13407</td><td align="left">496</td><td align="left">1536</td><td align="left">-</td><td align="left">9</td></tr></tbody></table><table-wrap-foot><p>For each dataset, its row identified by its average coverage (Avg. Cov.). We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab10"><label>Table 10</label><caption><p>Peak of RAM usage in Megabytes of the tools on simulated datasets of Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Avg.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left">Cov.</th><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">25</td><td align="left">1378</td><td align="left">2180</td><td align="left">930</td><td align="left">5161</td><td align="left">3262</td><td align="left">3007</td><td align="left">4284</td><td align="left">3007</td></tr><tr><td align="left">30</td><td align="left">1667</td><td align="left">4187</td><td align="left">930</td><td align="left">6117</td><td align="left">3266</td><td align="left">3008</td><td align="left">5320</td><td align="left">3008</td></tr><tr><td align="left">35</td><td align="left">1984</td><td align="left">2134</td><td align="left">931</td><td align="left">6558</td><td align="left">3270</td><td align="left">3008</td><td align="left">5709</td><td align="left">3008</td></tr><tr><td align="left">40</td><td align="left">2315</td><td align="left">2186</td><td align="left">932</td><td align="left">6780</td><td align="left">3272</td><td align="left">3009</td><td align="left">-</td><td align="left">3009</td></tr><tr><td align="left">45</td><td align="left">2665</td><td align="left">5037</td><td align="left">932</td><td align="left">7043</td><td align="left">3276</td><td align="left">3010</td><td align="left">-</td><td align="left">3010</td></tr><tr><td align="left">50</td><td align="left">3180</td><td align="left">5223</td><td align="left">932</td><td align="left">7058</td><td align="left">3279</td><td align="left">3010</td><td align="left">-</td><td align="left">3010</td></tr><tr><td align="left">55</td><td align="left">3591</td><td align="left">5483</td><td align="left">996</td><td align="left">7212</td><td align="left">3282</td><td align="left">3011</td><td align="left">-</td><td align="left">3011</td></tr><tr><td align="left">60</td><td align="left">4009</td><td align="left">2374</td><td align="left">1039</td><td align="left">7294</td><td align="left">3286</td><td align="left">3011</td><td align="left">-</td><td align="left">3011</td></tr></tbody></table><table-wrap-foot><p>For each dataset, its row identified by its average coverage (Avg. Cov.). We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab11"><label>Table 11</label><caption><p>Switch error percentage on datasets of NA12878</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Chrom.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left"/><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">1</td><td align="left">1.929</td><td align="left">-</td><td align="left">1.926</td><td align="left">1.924</td><td align="left"><bold>1.920</bold></td><td align="left">-</td><td align="left">-</td><td align="left">2.191</td></tr><tr><td align="left">2</td><td align="left">0.038</td><td align="left">-</td><td align="left">0.050</td><td align="left">0.035</td><td align="left"><bold>0.030</bold></td><td align="left">-</td><td align="left">-</td><td align="left">0.374</td></tr><tr><td align="left">3</td><td align="left">0.044</td><td align="left">-</td><td align="left">0.045</td><td align="left">0.039</td><td align="left"><bold>0.031</bold></td><td align="left">-</td><td align="left">-</td><td align="left">0.381</td></tr><tr><td align="left">4</td><td align="left">2.042</td><td align="left">-</td><td align="left">2.052</td><td align="left">2.048</td><td align="left"><bold>2.033</bold></td><td align="left">-</td><td align="left">-</td><td align="left">2.237</td></tr><tr><td align="left">5</td><td align="left">1.829</td><td align="left">-</td><td align="left">1.828</td><td align="left"><bold>1.824</bold></td><td align="left">1.825</td><td align="left">-</td><td align="left">-</td><td align="left">1.998</td></tr><tr><td align="left">6</td><td align="left">1.991</td><td align="left">-</td><td align="left">1.990</td><td align="left">1.991</td><td align="left"><bold>1.983</bold></td><td align="left">-</td><td align="left">-</td><td align="left">2.205</td></tr><tr><td align="left">7</td><td align="left"><bold>0.659</bold></td><td align="left">-</td><td align="left">0.669</td><td align="left">0.666</td><td align="left">0.660</td><td align="left">-</td><td align="left">-</td><td align="left">0.924</td></tr><tr><td align="left">8</td><td align="left"><bold>1.743</bold></td><td align="left">-</td><td align="left">1.746</td><td align="left">1.748</td><td align="left">1.749</td><td align="left">-</td><td align="left">-</td><td align="left">1.992</td></tr><tr><td align="left">9</td><td align="left">1.966</td><td align="left">-</td><td align="left">1.965</td><td align="left">1.966</td><td align="left"><bold>1.940</bold></td><td align="left">2.140</td><td align="left">-</td><td align="left">2.187</td></tr><tr><td align="left">10</td><td align="left">0.949</td><td align="left">-</td><td align="left">0.949</td><td align="left">0.948</td><td align="left"><bold>0.939</bold></td><td align="left">1.171</td><td align="left">-</td><td align="left">1.232</td></tr><tr><td align="left">11</td><td align="left">2.092</td><td align="left">-</td><td align="left">2.101</td><td align="left">2.101</td><td align="left"><bold>2.081</bold></td><td align="left">2.282</td><td align="left">-</td><td align="left">2.325</td></tr><tr><td align="left">12</td><td align="left"><bold>0.041</bold></td><td align="left">-</td><td align="left">0.055</td><td align="left">0.048</td><td align="left">0.043</td><td align="left">0.319</td><td align="left">-</td><td align="left">0.405</td></tr><tr><td align="left">13</td><td align="left">0.051</td><td align="left">-</td><td align="left">0.036</td><td align="left">0.049</td><td align="left"><bold>0.029</bold></td><td align="left">0.285</td><td align="left">-</td><td align="left">0.349</td></tr><tr><td align="left">14</td><td align="left">0.034</td><td align="left">-</td><td align="left">0.042</td><td align="left">0.039</td><td align="left"><bold>0.032</bold></td><td align="left">0.347</td><td align="left">-</td><td align="left">0.421</td></tr><tr><td align="left">15</td><td align="left">0.055</td><td align="left">0.331</td><td align="left">0.069</td><td align="left">0.065</td><td align="left"><bold>0.043</bold></td><td align="left">0.358</td><td align="left">-</td><td align="left">0.427</td></tr><tr><td align="left">16</td><td align="left"><bold>0.022</bold></td><td align="left">0.289</td><td align="left"><bold>0.022</bold></td><td align="left">0.029</td><td align="left">0.027</td><td align="left">0.322</td><td align="left">-</td><td align="left">0.420</td></tr><tr><td align="left">17</td><td align="left">0.055</td><td align="left">0.277</td><td align="left">0.071</td><td align="left">0.067</td><td align="left"><bold>0.047</bold></td><td align="left">0.337</td><td align="left">-</td><td align="left">0.426</td></tr><tr><td align="left">18</td><td align="left">1.895</td><td align="left">-</td><td align="left">1.879</td><td align="left"><bold>1.876</bold></td><td align="left">1.889</td><td align="left">2.072</td><td align="left">-</td><td align="left">2.122</td></tr><tr><td align="left">19</td><td align="left">2.629</td><td align="left">-</td><td align="left">2.642</td><td align="left">2.644</td><td align="left"><bold>2.616</bold></td><td align="left">2.807</td><td align="left">-</td><td align="left">2.914</td></tr><tr><td align="left">20</td><td align="left"><bold>0.043</bold></td><td align="left">0.277</td><td align="left">0.046</td><td align="left"><bold>0.043</bold></td><td align="left"><bold>0.043</bold></td><td align="left">0.412</td><td align="left">-</td><td align="left">0.451</td></tr><tr><td align="left">21</td><td align="left">0.033</td><td align="left">-</td><td align="left">0.044</td><td align="left">0.041</td><td align="left"><bold>0.030</bold></td><td align="left">0.364</td><td align="left">-</td><td align="left">0.408</td></tr><tr><td align="left">22</td><td align="left">2.102</td><td align="left">2.323</td><td align="left">2.106</td><td align="left">2.114</td><td align="left"><bold>2.068</bold></td><td align="left">2.378</td><td align="left">-</td><td align="left">2.452</td></tr></tbody></table><table-wrap-foot><p>Each row corresponds to a chromosome. The dataset consists of all reads aligned to the chromosome. We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare. The best result (lowest value) for each dataset is boldfaced</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab12"><label>Table 12</label><caption><p>Hamming Distance percentage on datasets of NA12878</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Chrom.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left"/><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">1</td><td align="left">2.12</td><td align="left">-</td><td align="left"><bold>1.92</bold></td><td align="left">2.10</td><td align="left">2.11</td><td align="left">-</td><td align="left">-</td><td align="left">6.16</td></tr><tr><td align="left">2</td><td align="left">0.51</td><td align="left">-</td><td align="left">0.49</td><td align="left"><bold>0.29</bold></td><td align="left">0.77</td><td align="left">-</td><td align="left">-</td><td align="left">4.91</td></tr><tr><td align="left">3</td><td align="left"><bold>0.32</bold></td><td align="left">-</td><td align="left">0.42</td><td align="left">0.42</td><td align="left">0.48</td><td align="left">-</td><td align="left">-</td><td align="left">4.74</td></tr><tr><td align="left">4</td><td align="left">2.47</td><td align="left">-</td><td align="left">2.15</td><td align="left">2.18</td><td align="left"><bold>2.00</bold></td><td align="left">-</td><td align="left">-</td><td align="left">6.44</td></tr><tr><td align="left">5</td><td align="left">2.33</td><td align="left">-</td><td align="left">2.53</td><td align="left">2.22</td><td align="left"><bold>1.98</bold></td><td align="left">-</td><td align="left">-</td><td align="left">6.56</td></tr><tr><td align="left">6</td><td align="left">3.39</td><td align="left">-</td><td align="left">3.02</td><td align="left">3.20</td><td align="left"><bold>2.82</bold></td><td align="left">-</td><td align="left">-</td><td align="left">7.15</td></tr><tr><td align="left">7</td><td align="left">1.16</td><td align="left">-</td><td align="left"><bold>1.10</bold></td><td align="left"><bold>1.10</bold></td><td align="left">1.36</td><td align="left">-</td><td align="left">-</td><td align="left">5.05</td></tr><tr><td align="left">8</td><td align="left">2.44</td><td align="left">-</td><td align="left">2.46</td><td align="left">2.54</td><td align="left"><bold>2.02</bold></td><td align="left">-</td><td align="left">-</td><td align="left">6.14</td></tr><tr><td align="left">9</td><td align="left">2.45</td><td align="left">-</td><td align="left">2.31</td><td align="left">2.49</td><td align="left"><bold>2.11</bold></td><td align="left">5.68</td><td align="left">-</td><td align="left">6.23</td></tr><tr><td align="left">10</td><td align="left">1.19</td><td align="left">-</td><td align="left">1.16</td><td align="left">1.18</td><td align="left"><bold>0.93</bold></td><td align="left">3.89</td><td align="left">-</td><td align="left">5.29</td></tr><tr><td align="left">11</td><td align="left">2.08</td><td align="left">-</td><td align="left">2.06</td><td align="left">2.06</td><td align="left"><bold>1.99</bold></td><td align="left">4.25</td><td align="left">-</td><td align="left">5.08</td></tr><tr><td align="left">12</td><td align="left">0.43</td><td align="left">-</td><td align="left">0.48</td><td align="left"><bold>0.38</bold></td><td align="left">0.51</td><td align="left">2.92</td><td align="left">-</td><td align="left">5.54</td></tr><tr><td align="left">13</td><td align="left">0.41</td><td align="left">-</td><td align="left">0.63</td><td align="left">0.57</td><td align="left"><bold>0.35</bold></td><td align="left">4.01</td><td align="left">-</td><td align="left">4.84</td></tr><tr><td align="left">14</td><td align="left">0.21</td><td align="left">-</td><td align="left">0.48</td><td align="left">0.58</td><td align="left"><bold>0.17</bold></td><td align="left">3.01</td><td align="left">-</td><td align="left">3.24</td></tr><tr><td align="left">15</td><td align="left"><bold>0.23</bold></td><td align="left">3.39</td><td align="left">0.24</td><td align="left">0.34</td><td align="left">0.34</td><td align="left">4.18</td><td align="left">-</td><td align="left">5.49</td></tr><tr><td align="left">16</td><td align="left"><bold>0.24</bold></td><td align="left">2.09</td><td align="left">0.45</td><td align="left">0.88</td><td align="left">0.28</td><td align="left">1.65</td><td align="left">-</td><td align="left">2.87</td></tr><tr><td align="left">17</td><td align="left">0.50</td><td align="left">2.84</td><td align="left">0.38</td><td align="left">0.79</td><td align="left"><bold>0.20</bold></td><td align="left">2.89</td><td align="left">-</td><td align="left">4.61</td></tr><tr><td align="left">18</td><td align="left">1.80</td><td align="left">-</td><td align="left">1.67</td><td align="left"><bold>1.65</bold></td><td align="left">1.68</td><td align="left">4.77</td><td align="left">-</td><td align="left">8.10</td></tr><tr><td align="left">19</td><td align="left">3.19</td><td align="left">-</td><td align="left">3.14</td><td align="left">3.40</td><td align="left"><bold>2.99</bold></td><td align="left">4.37</td><td align="left">-</td><td align="left">7.32</td></tr><tr><td align="left">20</td><td align="left">1.37</td><td align="left">3.47</td><td align="left">0.16</td><td align="left"><bold>0.10</bold></td><td align="left"><bold>0.16</bold></td><td align="left">2.99</td><td align="left">-</td><td align="left">4.07</td></tr><tr><td align="left">21</td><td align="left"><bold>0.10</bold></td><td align="left">-</td><td align="left"><bold>0.10</bold></td><td align="left"><bold>0.10</bold></td><td align="left">1.95</td><td align="left">5.37</td><td align="left">-</td><td align="left">4.22</td></tr><tr><td align="left">22</td><td align="left"><bold>1.82</bold></td><td align="left">4.92</td><td align="left">1.84</td><td align="left">1.83</td><td align="left"><bold>1.82</bold></td><td align="left">4.83</td><td align="left">-</td><td align="left">6.52</td></tr></tbody></table><table-wrap-foot><p>Each row corresponds to a chromosome. The dataset consists of all reads aligned to the chromosome. We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare. The best result (lowest value) for each dataset is boldfaced</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab13"><label>Table 13</label><caption><p>QAN50 results of the tools on datasets of NA12878</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Chrom.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left"/><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">1</td><td align="left">91098</td><td align="left">-</td><td align="left">91668</td><td align="left"><bold>91677</bold></td><td align="left">89249</td><td align="left">-</td><td align="left">-</td><td align="left">84863</td></tr><tr><td align="left">2</td><td align="left">210603</td><td align="left">-</td><td align="left">210098</td><td align="left"><bold>211732</bold></td><td align="left"><bold>211732</bold></td><td align="left">-</td><td align="left">-</td><td align="left">177388</td></tr><tr><td align="left">3</td><td align="left"><bold>229835</bold></td><td align="left">-</td><td align="left">227732</td><td align="left"><bold>229835</bold></td><td align="left">229655</td><td align="left">-</td><td align="left">-</td><td align="left">170494</td></tr><tr><td align="left">4</td><td align="left">90639</td><td align="left">-</td><td align="left"><bold>91034</bold></td><td align="left">90639</td><td align="left">89868</td><td align="left">-</td><td align="left">-</td><td align="left">84861</td></tr><tr><td align="left">5</td><td align="left">99011</td><td align="left">-</td><td align="left">99012</td><td align="left"><bold>99567</bold></td><td align="left">98900</td><td align="left">-</td><td align="left">-</td><td align="left">91745</td></tr><tr><td align="left">6</td><td align="left"><bold>94780</bold></td><td align="left">-</td><td align="left">94200</td><td align="left"><bold>94780</bold></td><td align="left">93894</td><td align="left">-</td><td align="left">-</td><td align="left">85483</td></tr><tr><td align="left">7</td><td align="left"><bold>156573</bold></td><td align="left">-</td><td align="left">155773</td><td align="left">155773</td><td align="left">155209</td><td align="left">-</td><td align="left">-</td><td align="left">135095</td></tr><tr><td align="left">8</td><td align="left">90928</td><td align="left">-</td><td align="left"><bold>91069</bold></td><td align="left">90836</td><td align="left">90661</td><td align="left">-</td><td align="left">-</td><td align="left">84076</td></tr><tr><td align="left">9</td><td align="left">85172</td><td align="left">-</td><td align="left"><bold>85655</bold></td><td align="left">85469</td><td align="left"><bold>85655</bold></td><td align="left">82917</td><td align="left">-</td><td align="left">80957</td></tr><tr><td align="left">10</td><td align="left">123171</td><td align="left">-</td><td align="left">123171</td><td align="left"><bold>123224</bold></td><td align="left">122317</td><td align="left">114172</td><td align="left">-</td><td align="left">112861</td></tr><tr><td align="left">11</td><td align="left">84153</td><td align="left">-</td><td align="left">84108</td><td align="left">84108</td><td align="left"><bold>84237</bold></td><td align="left">81526</td><td align="left">-</td><td align="left">79057</td></tr><tr><td align="left">12</td><td align="left">224308</td><td align="left">-</td><td align="left">224308</td><td align="left"><bold>228356</bold></td><td align="left">224308</td><td align="left">190161</td><td align="left">-</td><td align="left">174540</td></tr><tr><td align="left">13</td><td align="left"><bold>229318</bold></td><td align="left">-</td><td align="left">228310</td><td align="left">228310</td><td align="left">227286</td><td align="left">178173</td><td align="left">-</td><td align="left">175124</td></tr><tr><td align="left">14</td><td align="left"><bold>243192</bold></td><td align="left">-</td><td align="left"><bold>243192</bold></td><td align="left">227040</td><td align="left">220294</td><td align="left">186476</td><td align="left">-</td><td align="left">181826</td></tr><tr><td align="left">15</td><td align="left"><bold>180874</bold></td><td align="left">153527</td><td align="left">173950</td><td align="left">176529</td><td align="left">176529</td><td align="left">147339</td><td align="left">-</td><td align="left">138185</td></tr><tr><td align="left">16</td><td align="left"><bold>193611</bold></td><td align="left">160049</td><td align="left"><bold>193611</bold></td><td align="left">190884</td><td align="left">189342</td><td align="left">158848</td><td align="left">-</td><td align="left">152960</td></tr><tr><td align="left">17</td><td align="left">162690</td><td align="left">151262</td><td align="left"><bold>163789</bold></td><td align="left"><bold>163789</bold></td><td align="left">162328</td><td align="left">140216</td><td align="left">-</td><td align="left">133887</td></tr><tr><td align="left">18</td><td align="left">93705</td><td align="left">-</td><td align="left"><bold>94210</bold></td><td align="left">93705</td><td align="left"><bold>94210</bold></td><td align="left">87076</td><td align="left">-</td><td align="left">83383</td></tr><tr><td align="left">19</td><td align="left"><bold>62662</bold></td><td align="left">-</td><td align="left"><bold>62662</bold></td><td align="left">62568</td><td align="left">62233</td><td align="left">59716</td><td align="left">-</td><td align="left">58694</td></tr><tr><td align="left">20</td><td align="left">165921</td><td align="left">163062</td><td align="left">165921</td><td align="left"><bold>176807</bold></td><td align="left">165921</td><td align="left">140498</td><td align="left">-</td><td align="left">140034</td></tr><tr><td align="left">21</td><td align="left">222171</td><td align="left">-</td><td align="left"><bold>222769</bold></td><td align="left">221786</td><td align="left">222171</td><td align="left">149165</td><td align="left">-</td><td align="left">146675</td></tr><tr><td align="left">22</td><td align="left">82618</td><td align="left">73223</td><td align="left"><bold>85112</bold></td><td align="left">82618</td><td align="left"><bold>85112</bold></td><td align="left">72117</td><td align="left">-</td><td align="left">70718</td></tr></tbody></table><table-wrap-foot><p>Each row corresponds to a chromosome. The dataset consists of all reads aligned to the chromosome. We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare. The best result (lowest value) for each dataset is boldfaced</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab14"><label>Table 14</label><caption><p>Time in seconds on datasets of NA12878</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Chrom.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left"/><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">1</td><td align="left">20183</td><td align="left">-</td><td align="left">3300</td><td align="left">41626</td><td align="left">6301</td><td align="left">-</td><td align="left">-</td><td align="left">980</td></tr><tr><td align="left">2</td><td align="left">21913</td><td align="left">-</td><td align="left">3686</td><td align="left">46937</td><td align="left">6758</td><td align="left">-</td><td align="left">-</td><td align="left">1075</td></tr><tr><td align="left">3</td><td align="left">19325</td><td align="left">-</td><td align="left">2994</td><td align="left">38040</td><td align="left">5536</td><td align="left">-</td><td align="left">-</td><td align="left">776</td></tr><tr><td align="left">4</td><td align="left">21744</td><td align="left">-</td><td align="left">3031</td><td align="left">40083</td><td align="left">5998</td><td align="left">-</td><td align="left">-</td><td align="left">862</td></tr><tr><td align="left">5</td><td align="left">18416</td><td align="left">-</td><td align="left">2943</td><td align="left">36674</td><td align="left">5169</td><td align="left">-</td><td align="left">-</td><td align="left">790</td></tr><tr><td align="left">6</td><td align="left">17792</td><td align="left">-</td><td align="left">2658</td><td align="left">35189</td><td align="left">5640</td><td align="left">-</td><td align="left">-</td><td align="left">759</td></tr><tr><td align="left">7</td><td align="left">14321</td><td align="left">-</td><td align="left">2409</td><td align="left">32550</td><td align="left">4429</td><td align="left">-</td><td align="left">-</td><td align="left">744</td></tr><tr><td align="left">8</td><td align="left">15930</td><td align="left">-</td><td align="left">2421</td><td align="left">29902</td><td align="left">4578</td><td align="left">-</td><td align="left">-</td><td align="left">669</td></tr><tr><td align="left">9</td><td align="left">11307</td><td align="left">-</td><td align="left">1886</td><td align="left">23586</td><td align="left">3369</td><td align="left">86913</td><td align="left">-</td><td align="left">635</td></tr><tr><td align="left">10</td><td align="left">13943</td><td align="left">-</td><td align="left">2244</td><td align="left">27638</td><td align="left">3914</td><td align="left">86941</td><td align="left">-</td><td align="left">670</td></tr><tr><td align="left">11</td><td align="left">13291</td><td align="left">-</td><td align="left">1983</td><td align="left">25419</td><td align="left">3916</td><td align="left">86833</td><td align="left">-</td><td align="left">567</td></tr><tr><td align="left">12</td><td align="left">12684</td><td align="left">-</td><td align="left">1916</td><td align="left">25865</td><td align="left">4054</td><td align="left">86814</td><td align="left">-</td><td align="left">554</td></tr><tr><td align="left">13</td><td align="left">11100</td><td align="left">-</td><td align="left">1474</td><td align="left">20288</td><td align="left">2952</td><td align="left">86686</td><td align="left">-</td><td align="left">406</td></tr><tr><td align="left">14</td><td align="left">9017</td><td align="left">-</td><td align="left">1265</td><td align="left">17658</td><td align="left">2644</td><td align="left">86684</td><td align="left">-</td><td align="left">384</td></tr><tr><td align="left">15</td><td align="left">6934</td><td align="left">63221</td><td align="left">1114</td><td align="left">14218</td><td align="left">2102</td><td align="left">86700</td><td align="left">-</td><td align="left">368</td></tr><tr><td align="left">16</td><td align="left">7426</td><td align="left">69771</td><td align="left">1265</td><td align="left">16323</td><td align="left">2589</td><td align="left">86783</td><td align="left">-</td><td align="left">461</td></tr><tr><td align="left">17</td><td align="left">6460</td><td align="left">54037</td><td align="left">956</td><td align="left">12312</td><td align="left">1832</td><td align="left">86669</td><td align="left">-</td><td align="left">312</td></tr><tr><td align="left">18</td><td align="left">8440</td><td align="left">-</td><td align="left">1152</td><td align="left">15794</td><td align="left">2497</td><td align="left">86671</td><td align="left">-</td><td align="left">353</td></tr><tr><td align="left">19</td><td align="left">3625</td><td align="left">-</td><td align="left">826</td><td align="left">10368</td><td align="left">1617</td><td align="left">86668</td><td align="left">-</td><td align="left">296</td></tr><tr><td align="left">20</td><td align="left">5878</td><td align="left">55032</td><td align="left">827</td><td align="left">11815</td><td align="left">1594</td><td align="left">86600</td><td align="left">-</td><td align="left">243</td></tr><tr><td align="left">21</td><td align="left">3561</td><td align="left">-</td><td align="left">560</td><td align="left">7508</td><td align="left">1308</td><td align="left">86585</td><td align="left">-</td><td align="left">226</td></tr><tr><td align="left">22</td><td align="left">2835</td><td align="left">31617</td><td align="left">505</td><td align="left">7059</td><td align="left">1002</td><td align="left">86568</td><td align="left">-</td><td align="left">195</td></tr></tbody></table><table-wrap-foot><p>Each row corresponds to a chromosome. The dataset consists of all reads aligned to the chromosome. We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab15"><label>Table 15</label><caption><p>Peak of RAM usage in Megabytes of the tools on datasets of NA12878</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Chrom.</th><th align="left">HapCHAT</th><th align="left">HapCol</th><th align="left">WhatsHap</th><th align="left">WhatsHap</th><th align="left">HapCUT2</th><th align="left">ReFHap</th><th align="left">ProbHap</th><th align="left">FastHare</th></tr><tr><th align="left"/><th align="left"/><th align="left"/><th align="left">cov. 15x</th><th align="left">cov. 20x</th><th align="left"/><th align="left"/><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">1</td><td align="left">6361</td><td align="left">-</td><td align="left">2983</td><td align="left">13259</td><td align="left">3351</td><td align="left">-</td><td align="left">-</td><td align="left">3050</td></tr><tr><td align="left">2</td><td align="left">7082</td><td align="left">-</td><td align="left">3173</td><td align="left">13938</td><td align="left">3362</td><td align="left">-</td><td align="left">-</td><td align="left">3056</td></tr><tr><td align="left">3</td><td align="left">6180</td><td align="left">-</td><td align="left">2669</td><td align="left">12672</td><td align="left">3329</td><td align="left">-</td><td align="left">-</td><td align="left">3041</td></tr><tr><td align="left">4</td><td align="left">7531</td><td align="left">-</td><td align="left">2685</td><td align="left">12959</td><td align="left">3334</td><td align="left">-</td><td align="left">-</td><td align="left">3046</td></tr><tr><td align="left">5</td><td align="left">5882</td><td align="left">-</td><td align="left">2551</td><td align="left">12364</td><td align="left">3320</td><td align="left">-</td><td align="left">-</td><td align="left">3033</td></tr><tr><td align="left">6</td><td align="left">5649</td><td align="left">-</td><td align="left">2325</td><td align="left">12120</td><td align="left">3312</td><td align="left">-</td><td align="left">-</td><td align="left">3031</td></tr><tr><td align="left">7</td><td align="left">4597</td><td align="left">-</td><td align="left">2080</td><td align="left">11167</td><td align="left">3309</td><td align="left">-</td><td align="left">-</td><td align="left">3022</td></tr><tr><td align="left">8</td><td align="left">5075</td><td align="left">-</td><td align="left">2091</td><td align="left">11164</td><td align="left">3302</td><td align="left">-</td><td align="left">-</td><td align="left">3023</td></tr><tr><td align="left">9</td><td align="left">3583</td><td align="left">-</td><td align="left">1639</td><td align="left">9345</td><td align="left">3285</td><td align="left">17915</td><td align="left">-</td><td align="left">3009</td></tr><tr><td align="left">10</td><td align="left">4059</td><td align="left">-</td><td align="left">1819</td><td align="left">10164</td><td align="left">3303</td><td align="left">9766</td><td align="left">-</td><td align="left">3017</td></tr><tr><td align="left">11</td><td align="left">3965</td><td align="left">-</td><td align="left">1814</td><td align="left">10135</td><td align="left">3290</td><td align="left">9632</td><td align="left">-</td><td align="left">3018</td></tr><tr><td align="left">12</td><td align="left">4011</td><td align="left">-</td><td align="left">1787</td><td align="left">10229</td><td align="left">3288</td><td align="left">13984</td><td align="left">-</td><td align="left">3016</td></tr><tr><td align="left">13</td><td align="left">7950</td><td align="left">-</td><td align="left">1449</td><td align="left">8982</td><td align="left">3267</td><td align="left">8371</td><td align="left">-</td><td align="left">3006</td></tr><tr><td align="left">14</td><td align="left">2857</td><td align="left">-</td><td align="left">1281</td><td align="left">8198</td><td align="left">3261</td><td align="left">10024</td><td align="left">-</td><td align="left">2998</td></tr><tr><td align="left">15</td><td align="left">2232</td><td align="left">8437</td><td align="left">1077</td><td align="left">7370</td><td align="left">3257</td><td align="left">8302</td><td align="left">-</td><td align="left">2993</td></tr><tr><td align="left">16</td><td align="left">3116</td><td align="left">19698</td><td align="left">1128</td><td align="left">7703</td><td align="left">3263</td><td align="left">10328</td><td align="left">-</td><td align="left">2995</td></tr><tr><td align="left">17</td><td align="left">7844</td><td align="left">7845</td><td align="left">962</td><td align="left">6737</td><td align="left">3253</td><td align="left">5941</td><td align="left">-</td><td align="left">2990</td></tr><tr><td align="left">18</td><td align="left">3542</td><td align="left">-</td><td align="left">1152</td><td align="left">7810</td><td align="left">3254</td><td align="left">15868</td><td align="left">-</td><td align="left">2995</td></tr><tr><td align="left">19</td><td align="left">1721</td><td align="left">-</td><td align="left">793</td><td align="left">6055</td><td align="left">3244</td><td align="left">8808</td><td align="left">-</td><td align="left">2983</td></tr><tr><td align="left">20</td><td align="left">8496</td><td align="left">9966</td><td align="left">865</td><td align="left">6612</td><td align="left">3242</td><td align="left">7973</td><td align="left">-</td><td align="left">2985</td></tr><tr><td align="left">21</td><td align="left">1329</td><td align="left">-</td><td align="left">611</td><td align="left">5211</td><td align="left">3229</td><td align="left">7852</td><td align="left">-</td><td align="left">2977</td></tr><tr><td align="left">22</td><td align="left">3324</td><td align="left">7782</td><td align="left">542</td><td align="left">4912</td><td align="left">3225</td><td align="left">7904</td><td align="left">-</td><td align="left">2975</td></tr></tbody></table><table-wrap-foot><p>Each row corresponds to a chromosome. The dataset consists of all reads aligned to the chromosome. We report the results obtained by running the tools with maximum coverage 30 × for HapCHAT, 25 × for HapCol, 15 × and 20 × for WhatsHap. No maximum coverage was set for HapCUT2, ReFHap, ProbHap, and FastHare</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab16"><label>Table 16</label><caption><p>List of SNV positions when the adaptive procedure of subsection Adaptive k-cMEC was activated for real Ashkenazim and simulated datasets of Chromosome 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="2">Chr.1</th><th align="left">4 to 7</th><th align="left">5 to 8</th></tr><tr><th align="left">Data</th><th align="left">Avg. Cov.</th><th align="left"/><th align="left"/></tr></thead><tbody><tr><td align="left">Ashkenazim</td><td align="left">Cov. 25</td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left">Cov. 30</td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left">Cov. 35</td><td align="left">35556</td><td align="left"/></tr><tr><td align="left"/><td align="left">Cov. 40</td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left">Cov. 45</td><td align="left">35581</td><td align="left"/></tr><tr><td align="left"/><td align="left">Cov. 50</td><td align="left">35593, 42897</td><td align="left"/></tr><tr><td align="left"/><td align="left">Cov. 55</td><td align="left">3528</td><td align="left"/></tr><tr><td align="left"/><td align="left">Cov. 60</td><td align="left">46338, 46339</td><td align="left"/></tr><tr><td align="left">Simulated</td><td align="left">Cov. 25</td><td align="left">35569, 38788</td><td align="left">26778</td></tr><tr><td align="left"/><td align="left">Cov. 30</td><td align="left">35594, 38815, 38817</td><td align="left">26800</td></tr><tr><td align="left"/><td align="left">Cov. 35</td><td align="left">38827</td><td align="left">26811</td></tr><tr><td align="left"/><td align="left">Cov. 40</td><td align="left">38837</td><td align="left">38834, 38835, 38836</td></tr><tr><td align="left"/><td align="left">Cov. 45</td><td align="left">38844</td><td align="left">38842</td></tr><tr><td align="left"/><td align="left">Cov. 50</td><td align="left"/><td align="left">38849</td></tr><tr><td align="left"/><td align="left">Cov. 55</td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left">Cov. 60</td><td align="left"/><td align="left"/></tr></tbody></table><table-wrap-foot><p>For each dataset, its row is identified by its average coverage (Avg. Cov.). The positions in column ‘4 to 7’ are those for which the number of corrections was increased from 4 to 7, and similarly for the column ‘5 to 8’</p></table-wrap-foot></table-wrap>
<table-wrap id="Tab17"><label>Table 17</label><caption><p>Comparison of the switch error positions on the Ashkenazim datasets of Chromosome 1 obtained with HapCHAT</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Ashkenazim</th><th align="left">Cov. 25</th><th align="left">Cov. 30</th><th align="left">Cov. 35</th><th align="left">Cov. 40</th><th align="left">Cov. 45</th><th align="left">Cov. 50</th><th align="left">Cov. 55</th></tr></thead><tbody><tr><td align="left">Cov. 30</td><td align="left">75/2/4</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">Cov. 35</td><td align="left">72/4/7</td><td align="left">74/2/3</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">Cov. 40</td><td align="left">71/6/8</td><td align="left">74/4/4</td><td align="left">75/2/1</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">Cov. 45</td><td align="left">71/6/8</td><td align="left">73/4/4</td><td align="left">75/2/1</td><td align="left">77/0/0</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">Cov. 50</td><td align="left">70/7/9</td><td align="left">72/5/5</td><td align="left">73/4/3</td><td align="left">75/2/2</td><td align="left">75/2/2</td><td align="left"/><td align="left"/></tr><tr><td align="left">Cov. 55</td><td align="left">71/6/8</td><td align="left">73/4/4</td><td align="left">73/4/3</td><td align="left">75/2/2</td><td align="left">75/2/2</td><td align="left">75/2/2</td><td align="left"/></tr><tr><td align="left">Cov. 60</td><td align="left">71/7/8</td><td align="left">73/5/4</td><td align="left">73/5/3</td><td align="left">75/3/2</td><td align="left">75/3/2</td><td align="left">75/3/2</td><td align="left">76/2/1</td></tr></tbody></table><table-wrap-foot><p>For each pair of datasets having different coverages, we report the number of positions in which a switch error occurred as follows: those in common between the two datasets, those only found in the dataset of the row, and those only found the dataset of the column, respectively</p></table-wrap-foot></table-wrap></p>
      <p>The accuracy of the predictions obtained from the experiments and measured in terms of switch error percentages is summarized in Tables <xref rid="Tab1" ref-type="table">1</xref>, <xref rid="Tab6" ref-type="table">6</xref> and <xref rid="Tab11" ref-type="table">11</xref>. We have also assessed the accuracy of the predictions by computing the Hamming distance percentages — Tables <xref rid="Tab2" ref-type="table">2</xref>, <xref rid="Tab7" ref-type="table">7</xref> and <xref rid="Tab12" ref-type="table">12</xref>. Each true haplotype is a mosaic of the predicted haplotypes. A switch error is the boundary (that is two consecutive SNV positions) between two portions of such a mosaic. The switch error percentage is the ratio between the number of switch errors and the number of phased SNVs minus one (expressed as a percentage). It is immediate to notice that HapCHAT, WhatsHap, and HapCUT2 compute the best predictions, all of them being very close. Figures <xref rid="Fig2" ref-type="fig">2</xref> and <xref rid="Fig3" ref-type="fig">3</xref> give bar chart representations of switch error rates for just these three methods on all real datasets. We point out that HapCHAT (resp., HapCUT2) computes the best switch error rates for almost all instances of the real and simulated Ashkenazim (resp., NA12878) datasets.</p>
      <p>Although the switch error is one of the most widely adopted measures used to evaluate the quality of the phased haplotypes, it does not take into account the <italic>recall</italic>, or the completeness of the haplotype – that is, the size of the phased haplotype blocks recovered. While N50 is the classical median size of an assembled haplotype block in terms of length in basepairs (bp) from the literature on assembly, [<xref ref-type="bibr" rid="CR49">49</xref>] introduced the <italic>adjusted</italic> N50, that is AN50 score which normalizes each block in terms of the number of phased SNVs appearing on a block. In order to account for completeness <italic>and</italic> quality, [<xref ref-type="bibr" rid="CR50">50</xref>] introduced the notion of <italic>quality</italic> AN50, that is the QAN50 score, where assembled haplotype blocks are fractured at each switch error, and then AN50 is taken on the resulting sub-blocks. This is an important measure because it is closest to the objective of haplotype assembly – to reassemble the longest (error-free) haplotype blocks possible. We hence computed QAN50 scores for all methods, as summarized in Tables <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab8" ref-type="table">8</xref>, and <xref rid="Tab13" ref-type="table">13</xref>. It is immediate to notice that HapCHAT and WhatsHap have the best QAN50 scores, more precisely HapCHAT (resp., WhatsHap) computes the best QAN50 scores for almost all instances of the real and simulated Ashkenazim (resp., NA12878) datasets. HapCUT2 is a close second: despite its good switch error rate, it has consistently lower QAN50 scores.</p>
      <p>This could possibly be explained by [<xref ref-type="bibr" rid="CR17">17</xref>]: “HapCUT2 implements likelihood-based strategies for pruning low-confidence variants to reduce mismatch errors and splitting blocks at poor linkages to reduce switch errors (see Methods). These postprocessing steps allow a user to improve accuracy of the haplotypes at the cost of reducing completeness and contiguity.” – indeed their switch error rate tends to be consistently the best for the NA12878 dataset at least, the tradeoff being that QAN50 score is consistently lower than the best method in all cases. Figures <xref rid="Fig2" ref-type="fig">2</xref> and <xref rid="Fig3" ref-type="fig">3</xref> give bar chart representations of QAN50 scores for HapCHAT, WhatsHap and HapCUT2 on all real datasets.</p>
      <p>Since HapCHAT and WhatsHap can be influenced by a maximum coverage parameter, we did a deeper analysis of these two methods at different values of such parameter. The plots in Fig. <xref rid="Fig1" ref-type="fig">1</xref> represent the quality of the predictions computed by WhatsHap and HapCHAT as a function of the running time, for Chromosome 1 on the Ashkenazim dataset. Besides the switch error rate, we have also investigated the Hamming distance, that is the number of phase-calls that are different from the ground truth. Both plots confirm that HapCHAT computes predictions that are at least as good as those of WhatsHap (and clearly better in terms of Hamming distance) with a comparable runtime. We decided to include in the Tables the comparison of WhatsHap at both 20x and 15x max coverage, while 20x is the maximum coverage that we could test for WhatsHap – 15x is suggested by the authors as the default value for running WhatsHap and achieve the best trade off between accuracy and running time [<xref ref-type="bibr" rid="CR48">48</xref>]. Observe in Fig. <xref rid="Fig1" ref-type="fig">1</xref> that with 20x max coverage WhatsHap obtains better predictions — close to those by HapCHAT — but with a much higher runtime.</p>
      <p>It is possible to observe from Tables <xref rid="Tab4" ref-type="table">4</xref>, <xref rid="Tab5" ref-type="table">5</xref>, <xref rid="Tab9" ref-type="table">9</xref> and <xref rid="Tab10" ref-type="table">10</xref> that although both time and memory used by HapCHAT is growing with the (average) coverage, with higher coverage the rate at which the time increments is decreasing. Similarly, also the memory increment is almost linear with respect to the growth of the coverage of the datasets. On the other hand, while the changes of time and memory required by HapCol and WhatsHap to process higher coverages remain similar. Contrary to HapCHAT, because HapCol and WhatsHap are not adaptive (see intro for more details) that is they do not change their behaviour w.r.t. increasing average coverage, they must be run at a uniform <italic>maximum</italic> coverage of 25 and 15, respectively, and exhibit similar runtimes and memory usage for all datasets. HapCHAT, on the other hand, processes these datasets at the higher uniform maximum coverage of 30, and because it <italic>adapts</italic> to this increased average coverage, we see this linear trend in increased resource usage, as expected. Finally, we point out that HapCUT2, ReFHap, and FastHare require always the same memory, since it does not depend on the coverage, and the time grows linearly, while ProbHap exhibits a behavior reflecting the coverage increment, especially in terms of memory consumption.</p>
      <p>An analysis of Tables <xref rid="Tab1" ref-type="table">1</xref> and <xref rid="Tab6" ref-type="table">6</xref> towards finding the effect of average coverage shows that there is a trend of improving predictions with higher average coverage, but this improvement is irregular. Since those irregularities are more common for HapCHAT than for the other tools, we have produced Table <xref rid="Tab17" ref-type="table">17</xref> which gives a more detailed breakdown of how the switch error is changing as a result of increasing coverage. More precisely, we have found that only in one case the erroneous sites at higher coverage is a subset of the erroneous sites at lower coverage. This shows a higher sensitivity of HapCHAT to changing (in this case sampling) instances. On the other hand, the quality measure given by the QAN50 reported in Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab8" ref-type="table">8</xref> and also summarized in Fig. <xref rid="Fig2" ref-type="fig">2</xref> shows that there is a regular increase of the QAN50 for all the data sets consistent with the increase of the coverage.</p>
      <p>Table <xref rid="Tab16" ref-type="table">16</xref> reports for each of the 16 Ashkenazim datasets, the SNV sites when the adaptive procedure of subsection Adaptive <italic>k</italic>-cMEC was activated. Interestingly, it is only in the Simulated dataset that the number of corrections needed to be increased from 5 to 8 – the rest needing an increase only to 7 (from 4) – indicating that it contains more unanticipated errors than the real datasets. Indeed this demonstrates that this adaptive procedure is an improvement over HapCol, recalling that each time this procedure is invoked, HapCol fails by definition. An added benefit of this procedure is that it can serve as an indicator of the quality of the read set to be phased. More specifically, it can serve as an indicator of the quality of the variant calling itself — indeed it is a third type of accuracy prediction, on top of switch error and Hamming distance — one that can be used to integrate the predictions of several tools to obtain higher quality variant calls [<xref ref-type="bibr" rid="CR41">41</xref>, <xref ref-type="bibr" rid="CR42">42</xref>]. We plan to investigate further this advantage in future developments of HapCHAT.</p>
    </sec>
  </sec>
  <sec id="Sec17" sec-type="conclusion">
    <title>Conclusions</title>
    <p>We have presented HapCHAT, a tool that is able to phase high coverage PacBio reads. We have compared HapCHAT to WhatsHap, HapCol, HapCUT2, ReFHap, ProbHap and FastHare on on real and simulated whole-chromosome datasets, with average coverage up to 60×. The real datasets have been taken from the GIAB project. Our experimental comparison shows that HapCHAT has accuracy and recall that are comparable with those of WhatsHap and HapCUT2, and better than all other tools. At the same time, HapCHAT requires an amount of computational resources that is on the same order of magnitude as WhatsHap and HapCUT2. In particular, our QAN50 scores are almost consistently better than all other tools, showing that we reconstruct the longest, least fragmented haplotype blocks – the ultimate aim of haplotype assembly. Trying our dynamic programming approach with even <italic>longer</italic> reads, such as those bolstered with Hi-C information [<xref ref-type="bibr" rid="CR51">51</xref>] would hence be an interesting future endeavour, to see how far we can push this method for assembling haplotypes.</p>
    <p>Introducing the capability of adapting the number of errors permitted in each column allows HapCHAT to achieve a better fit than HapCol of the number of corrections needed at each variant site. Still, the current approach allows such adaptation only for the current column. Coupling this step with backtracking could result in fewer overall corrections.</p>
    <p>Another direction of research is to fully consider the parent-sibling relations in trios, as done in [<xref ref-type="bibr" rid="CR43">43</xref>] or in [<xref ref-type="bibr" rid="CR52">52</xref>] here. This is especially relevant, since most of the GIAB data is on trios.</p>
    <p>Finally, we are working on the integration of HapCHAT with the WhatsHap tool to provide a more powerful haplotype phasing method able to combine the strengths of the two approaches.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>AN50</term>
        <def>
          <p>Adjusted N50</p>
        </def>
      </def-item>
      <def-item>
        <term>BAM</term>
        <def>
          <p>Binary Alignment Map (binary version of SAM)</p>
        </def>
      </def-item>
      <def-item>
        <term>bp</term>
        <def>
          <p>basepairs</p>
        </def>
      </def-item>
      <def-item>
        <term>FPT</term>
        <def>
          <p>Fixed Parameter Tractable</p>
        </def>
      </def-item>
      <def-item>
        <term>HapCHAT</term>
        <def>
          <p><bold>Hap</bold>lotype Assembly <bold>C</bold>overage <bold>H</bold>andling by <bold>A</bold>dapting</p>
        </def>
      </def-item>
      <def-item>
        <term><bold>T</bold>hresholds; hg37</term>
        <def>
          <p>human genome version 37</p>
        </def>
      </def-item>
      <def-item>
        <term>ILP</term>
        <def>
          <p>Integer Linear Programming</p>
        </def>
      </def-item>
      <def-item>
        <term>GIAB</term>
        <def>
          <p>Genome in a Bottle</p>
        </def>
      </def-item>
      <def-item>
        <term>GNU</term>
        <def>
          <p>GNU’s Not Unix</p>
        </def>
      </def-item>
      <def-item>
        <term>GPL</term>
        <def>
          <p>GNU Public License</p>
        </def>
      </def-item>
      <def-item>
        <term><italic>k</italic>-cMEC</term>
        <def>
          <p><italic>k</italic>-constrained MEC, see [<xref ref-type="bibr" rid="CR35">35</xref>]</p>
        </def>
      </def-item>
      <def-item>
        <term>kbp</term>
        <def>
          <p>kilobasepairs</p>
        </def>
      </def-item>
      <def-item>
        <term>MEC</term>
        <def>
          <p>Minimum Error Correction</p>
        </def>
      </def-item>
      <def-item>
        <term>NA12878</term>
        <def>
          <p>an individual, see [<xref ref-type="bibr" rid="CR41">41</xref>]</p>
        </def>
      </def-item>
      <def-item>
        <term>N50</term>
        <def>
          <p>median length of an assembled haplotype block</p>
        </def>
      </def-item>
      <def-item>
        <term>ONT</term>
        <def>
          <p>Oxford Nanopore Technologies</p>
        </def>
      </def-item>
      <def-item>
        <term>PacBio</term>
        <def>
          <p><bold>Pac</bold>ific <bold>Bio</bold>sciences</p>
        </def>
      </def-item>
      <def-item>
        <term>QAN50</term>
        <def>
          <p>Quality Adjusted N50</p>
        </def>
      </def-item>
      <def-item>
        <term>SAM</term>
        <def>
          <p>Sequence Alignment Map (a file format)</p>
        </def>
      </def-item>
      <def-item>
        <term>SNV</term>
        <def>
          <p>Single Nucleotide Polymorphism</p>
        </def>
      </def-item>
      <def-item>
        <term>VCF</term>
        <def>
          <p>Variant Call Format</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>Stefano Beretta and Murray D. Patterson contributed equally to this work.</p>
    </fn>
    <fn>
      <p>
        <bold>Availability of data and materials</bold>
      </p>
      <p>The PacBio long reads data that we used are publicly available at <ext-link ext-link-type="uri" xlink:href="ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_MtSinai_NIST/MtSinai_blasr_bam_GRCh37/hg002_gr37_1.bam">ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_MtSinai_NIST/MtSinai_blasr_bam_GRCh37/hg002_gr37_1.bam</ext-link>and <ext-link ext-link-type="uri" xlink:href="ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NA12878_PacBio_MtSinai/sorted_final_merged.bam">ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/NA12878_PacBio_MtSinai/sorted_final_merged.bam</ext-link></p>
      <p>The simulated datasets that we have used can be downloaded at: <ext-link ext-link-type="uri" xlink:href="https://drive.google.com/drive/folders/0BxqLPsY2hmAXMlowZF9JQllZNEU">https://drive.google.com/drive/folders/0BxqLPsY2hmAXMlowZF9JQllZNEU</ext-link>. The BAM files are in archive HapCHAT-experiments_bam--simulated.tar.g, while the corresponding VCFs can be found at HapCHAT-experiments.tar.gz.</p>
      <p>Instructions on how to use and replicate the experiments can be found at <ext-link ext-link-type="uri" xlink:href="http://hapchat.algolab.eu">http://hapchat.algolab.eu</ext-link></p>
    </fn>
  </fn-group>
  <ack>
    <p>We thank Tobias Marschall and Marcel Martin for inspiring discussions and for comments on earlier versions of this manuscript. We also thank the anonymous reviewers for pointing out during the revision process the new realignment feature of HapCut2 that allowed us to extend to experimental analysis and to use the QAN50 measure that helped the analysis and comparison of the tools.</p>
    <sec id="d29e10030">
      <title>Funding</title>
      <p>We acknowledge the support of the Cariplo Foundation grant 2013–0955 (Modulation of anti cancer immune response by regulatory non-coding RNAs). This funding body did not play any role in the design of the study nor collection, analysis, nor interpretation of data nor in writing the manuscript.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>All authors devised the preprocessing step, and GDV implemented it with the help of SB. All authors devised the adaptive procedure, and SB implemented it with the help of SZ. All authors designed the experiments and analyzed the results. MDP did the experiments, and SB generated the figures and tables. All authors wrote and revised the manuscript. All authors read and approved the final version of the manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec id="d29e10041">
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable</p>
    </sec>
    <sec id="d29e10046">
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="d29e10051">
      <title>Publisher’s Note</title>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Browning</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Browning</surname>
            <given-names>BL</given-names>
          </name>
        </person-group>
        <article-title>Haplotype phasing: existing methods and new developments</article-title>
        <source>Nature Reviews Genetics</source>
        <year>2011</year>
        <volume>12</volume>
        <issue>10</issue>
        <fpage>703</fpage>
        <lpage>714</lpage>
        <pub-id pub-id-type="doi">10.1038/nrg3054</pub-id>
        <?supplied-pmid 21921926?>
        <pub-id pub-id-type="pmid">21921926</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <mixed-citation publication-type="other">Tewhey R, Bansal V, Torkamani A, Topol EJ, Schork NJ. The importance of phase information for human genomics. Nature Reviews Genetics. 2011; 3:215–223. 10.1038/nrg2950.</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <mixed-citation publication-type="other">Glusman G, Cox H. C, Roach J. C. Whole-genome haplotyping approaches and genomic medicine. Genome Medicine. 2014; 6(9):73. 10.1186/s13073-014-0073-7.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <mixed-citation publication-type="other">Roach J. C, Glusman G, Smit AFA, Huff CD, Hubley R, Shannon PT, Rowen L, Pant KP, Goodman N, Bamshad M, Shendure J, Drmanac R, Jorde LB, Hood L, Galas DJ. Analysis of genetic inheritance in a family quartet by whole-genome sequencing. Science. 2010; 328(5978):636–639. 10.1126/science.1186802.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kuleshov</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Pushkarev</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Blauwkamp</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kertesz</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Whole-genome haplotyping using long reads and statistical methods</article-title>
        <source>Nature Biotechnology</source>
        <year>2014</year>
        <volume>32</volume>
        <issue>3</issue>
        <fpage>261</fpage>
        <lpage>266</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.2833</pub-id>
        <?supplied-pmid 24561555?>
        <pub-id pub-id-type="pmid">24561555</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <mixed-citation publication-type="other">Porubský D, Sanders AD, Wietmarschen Nv, Falconer E, Hills M, Spierings DCJ, Bevova MR, Guryev V, Lansdorp PM. Direct chromosome-length haplotyping by single-cell sequencing. Genome Res. 2016.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Porubsky</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Garg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sanders</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Korbel</surname>
            <given-names>JO</given-names>
          </name>
          <name>
            <surname>Guryev</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Lansdorp</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Marschall</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Dense and accurate whole-chromosome haplotyping of individual genomes</article-title>
        <source>Nat. Commun</source>
        <year>2017</year>
        <volume>8</volume>
        <issue>1</issue>
        <fpage>1293</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-017-01389-4</pub-id>
        <?supplied-pmid 29101320?>
        <pub-id pub-id-type="pmid">29101320</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <mixed-citation publication-type="other">Loh P-R, Danecek P, Palamara PF, Fuchsberger C, Reshef YA, Finucane HK, Schoenherr S, Forer L, McCarthy S, Abecasis GR, Durbin R, Price AL. Reference-based phasing using the haplotype reference consortium panel. Nature Genetics. 2016; 48(11):1443–1448. 10.1038/ng.3679.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <mixed-citation publication-type="other">O’Connell J, Sharp K, Shrine N, Wain L, Hall I, Tobin M, Zagury J-F, Delaneau O, Marchini J. Haplotype estimation for biobank-scale data sets. Nature Genetics. 2016; 48(7):817–820. 10.1038/ng.3583.</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Stephens</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Modeling linkage disequilibrium and identifying recombination hotspots using single-nucleotide polymorphism data</article-title>
        <source>Genetics</source>
        <year>2003</year>
        <volume>165</volume>
        <issue>4</issue>
        <fpage>2213</fpage>
        <lpage>2233</lpage>
        <?supplied-pmid 14704198?>
        <pub-id pub-id-type="pmid">14704198</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lippert</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Schwartz</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lancia</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Istrail</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Algorithmic strategies for the single nucleotide polymorphism haplotype assembly problem</article-title>
        <source>Briefings in Bioinformatics</source>
        <year>2002</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>23</fpage>
        <lpage>31</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/3.1.23</pub-id>
        <?supplied-pmid 12002221?>
        <pub-id pub-id-type="pmid">12002221</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cilibrasi</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Van Iersel</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Kelk</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tromp</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>The complexity of the single individual SNP haplotyping problem</article-title>
        <source>Algorithmica</source>
        <year>2007</year>
        <volume>49</volume>
        <issue>1</issue>
        <fpage>13</fpage>
        <lpage>36</lpage>
        <pub-id pub-id-type="doi">10.1007/s00453-007-0029-z</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <mixed-citation publication-type="other">Bonizzoni P, Dondi R, Klau GW, Pirola Y, Pisanti N, Zaccaria S. On the fixed parameter tractability and approximability of the minimum error correction problem. In: 26th Annual Symposium on Combinatorial Pattern Matching (CPM). LNCS: 2015. p. 100–113.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bonizzoni</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Dondi</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Klau</surname>
            <given-names>GW</given-names>
          </name>
          <name>
            <surname>Pirola</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Pisanti</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Zaccaria</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>On the minimum error correction problem for haplotype assembly in diploid and polyploid genomes</article-title>
        <source>Journal of Computational Biology</source>
        <year>2016</year>
        <volume>23</volume>
        <issue>9</issue>
        <fpage>718</fpage>
        <lpage>736</lpage>
        <pub-id pub-id-type="doi">10.1089/cmb.2015.0220</pub-id>
        <?supplied-pmid 27280382?>
        <pub-id pub-id-type="pmid">27280382</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <mixed-citation publication-type="other">Panconesi A, Sozio M. Fast hare: A fast heuristic for single individual SNP haplotype reconstruction. In: Algorithms in Bioinformatics, 4th International Workshop, WABI 2004, Bergen, Norway, September 17-21, 2004, Proceedings: 2004. p. 266–277.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bansal</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Bafna</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>HapCUT: an efficient and accurate algorithm for the haplotype assembly problem</article-title>
        <source>Bioinformatics</source>
        <year>2008</year>
        <volume>24</volume>
        <issue>16</issue>
        <fpage>153</fpage>
        <lpage>159</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btn298</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <mixed-citation publication-type="other">Edge P, Bafna V, Bansal V. HapCUT2: robust and accurate haplotype assembly for diverse sequencing technologies. Genome Research. 2016; 213462(116). 10.1101/gr.213462.116.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mazrouee</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>FastHap: fast and accurate single individual haplotype reconstruction using fuzzy conflict graphs</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>17</issue>
        <fpage>371</fpage>
        <lpage>378</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <mixed-citation publication-type="other">Duitama J, Huebsch T, McEwen G, Suk E-K, Hoehe MR. ReFHap: a reliable and fast algorithm for single individual haplotyping. In: BCB. ACM: 2010. p. 160–169.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fouilhoux</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Mahjoub</surname>
            <given-names>A. R</given-names>
          </name>
        </person-group>
        <article-title>Solving VLSI design and DNA sequencing problems using bipartization of graphs</article-title>
        <source>Computational Optimization and Applications</source>
        <year>2012</year>
        <volume>51</volume>
        <issue>2</issue>
        <fpage>749</fpage>
        <lpage>781</lpage>
        <pub-id pub-id-type="doi">10.1007/s10589-010-9355-1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Z-Z</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Exact algorithms for haplotype assembly from whole-genome sequence data</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>16</issue>
        <fpage>1938</fpage>
        <lpage>45</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt349</pub-id>
        <?supplied-pmid 23782612?>
        <pub-id pub-id-type="pmid">23782612</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Pipatsrisawat</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Darwiche</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Eskin</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Optimal algorithms for haplotype assembly from whole-genome sequence data</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <issue>12</issue>
        <fpage>183</fpage>
        <lpage>190</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btq215</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <mixed-citation publication-type="other">Chaisson MJP, Sanders AD, Zhao X, Malhotra A, Porubsky D, Rausch T, Gardner EJ, Rodriguez O, Guo L, Collins RL, Fan X, Wen J, Handsaker RE, Fairley S, Kronenberg ZN, Kong X, Hormozdiari F, Lee D, Wenger AM, Hastie A, Antaki D, Audano P, Brand H, Cantsilieris S, Cao H, Cerveira E, Chen C, Chen X, Chin C-S, Chong Z, Chuang NT, Church DM, Clarke L, Farrell A, Flores J, Galeev T, David G, Gujral M, Guryev V, Haynes-Heaton W, Korlach J, Kumar S, Kwon JY, Lee JE, Lee J, Lee W-P, Lee SP, Marks P, Valud-Martinez K, Meiers S, Munson KM, Navarro F, Nelson BJ, Nodzak C, Noor A, Kyriazopoulou-Panagiotopoulou S, Pang A, Qiu Y, Rosanio G, Ryan M, Stutz A, Spierings DCJ, Ward A, Welsch AE, Xiao M, Xu W, Zhang C, Zhu Q, Zheng-Bradley X, Jun G, Ding L, Koh CL, Ren B, Flicek P, Chen K, Gerstein MB, Kwok P-Y, Lansdorp PM, Marth G, Sebat J, Shi X, Bashir A, Ye K, Devine SE, Talkowski M, Mills RE, Marschall T, Korbel J, Eichler EE, Lee C. Multi-platform discovery of haplotype-resolved structural variation in human genomes. bioRxiv. 2017. 10.1101/193144.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carneiro</surname>
            <given-names>MO</given-names>
          </name>
          <name>
            <surname>Russ</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ross</surname>
            <given-names>MG</given-names>
          </name>
          <name>
            <surname>Gabriel</surname>
            <given-names>SB</given-names>
          </name>
          <name>
            <surname>Nusbaum</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>DePristo</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Pacific Biosciences sequencing technology for genotyping and variation discovery in human data</article-title>
        <source>BMC Genomics</source>
        <year>2012</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>375</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2164-13-375</pub-id>
        <?supplied-pmid 22863213?>
        <pub-id pub-id-type="pmid">22863213</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roberts</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Carneiro</surname>
            <given-names>MO</given-names>
          </name>
          <name>
            <surname>Schatz</surname>
            <given-names>MC</given-names>
          </name>
        </person-group>
        <article-title>The advantages of SMRT sequencing</article-title>
        <source>Genome Biology</source>
        <year>2013</year>
        <volume>14</volume>
        <issue>6</issue>
        <fpage>405</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2013-14-6-405</pub-id>
        <?supplied-pmid 23822731?>
        <pub-id pub-id-type="pmid">23822731</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <mixed-citation publication-type="other">Sedlazeck FJ, Lee H, Darby CA, Schatz MC. Piercing the dark matter: bioinformatics of long-range sequencing and mapping. Nat. Rev. Genet. 2018.</mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kuleshov</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Whole-genome haplotyping using long reads and statistical methods</article-title>
        <source>Nature Biotechnology</source>
        <year>2014</year>
        <volume>32</volume>
        <issue>3</issue>
        <fpage>261</fpage>
        <lpage>266</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.2833</pub-id>
        <?supplied-pmid 24561555?>
        <pub-id pub-id-type="pmid">24561555</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <mixed-citation publication-type="other">Ip CLC, Loose M, Tyson JR, de Cesare M, Brown BL, Jain M, Leggett RM, Eccles DA, Zalunin V, Urban JM, Piazza P, Bowden RJ, Paten B, Mwaigwisya S, Batty EM, Simpson JT, Snutch TP, Birney E, Buck D, Goodwin S, Jansen HJ, O’Grady J, Olsen HE. MinION analysis and reference consortium: Phase 1 data release and analysis. F1000 Research. 2015; 4. 10.12688/f1000research.7201.1.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rhoads</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Au</surname>
            <given-names>KF</given-names>
          </name>
        </person-group>
        <article-title>Pacbio sequencing and its applications</article-title>
        <source>Genomics, Proteomics and Bioinformatics</source>
        <year>2015</year>
        <volume>13</volume>
        <issue>5</issue>
        <fpage>278</fpage>
        <lpage>289</lpage>
        <pub-id pub-id-type="doi">10.1016/j.gpb.2015.08.002</pub-id>
        <?supplied-pmid 26542840?>
        <pub-id pub-id-type="pmid">26542840</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>HE</given-names>
          </name>
          <name>
            <surname>Paten</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Akeson</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The oxford nanopore minion: delivery of nanopore sequencing to the genomics community</article-title>
        <source>Genome Biology</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>239</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-016-1103-0</pub-id>
        <?supplied-pmid 27887629?>
        <pub-id pub-id-type="pmid">27887629</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jain</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Fiddes</surname>
            <given-names>IT</given-names>
          </name>
          <name>
            <surname>Miga</surname>
            <given-names>KH</given-names>
          </name>
          <name>
            <surname>Olsen</surname>
            <given-names>HE</given-names>
          </name>
          <name>
            <surname>Paten</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Akeson</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Improved data analysis for the minion nanopore sequencer</article-title>
        <source>Nature methods</source>
        <year>2015</year>
        <volume>12</volume>
        <fpage>351</fpage>
        <lpage>356</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.3290</pub-id>
        <?supplied-pmid 25686389?>
        <pub-id pub-id-type="pmid">25686389</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <mixed-citation publication-type="other">Cretu Stancu M, van Roosmalen MJ, Renkens I, Nieboer MM, Middelkamp S, de Ligt J, Pregno G, Giachino D, Mandrile G, Espejo Valle-Inclan J, Korzelius J, de Bruijn E, Cuppen E, Talkowski ME, Marschall T, de Ridder J, Kloosterman WP. Mapping and phasing of structural variation in patient genomes using nanopore sequencing. Nature Communications. 2017; 8(1326). 10.1038/s41467-017-01343-4.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <mixed-citation publication-type="other">Patterson M, Marschall T, Pisanti N, van Iersel L, Stougie L, Klau G. W, Schönhuth A. WhatsHap: Haplotype assembly for future-generation sequencing reads. In: RECOMB. LNCS: 2014. p. 237–249.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Patterson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Marschall</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Pisanti</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>van Iersel</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Stougie</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Klau</surname>
            <given-names>GW</given-names>
          </name>
          <name>
            <surname>Schönhuth</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>WhatsHap: Weighted haplotype assembly for future-generation sequencing reads</article-title>
        <source>Journal of Computational Biology</source>
        <year>2015</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>498</fpage>
        <lpage>509</lpage>
        <pub-id pub-id-type="doi">10.1089/cmb.2014.0157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <mixed-citation publication-type="other">Pirola Y, Zaccaria S, Dondi R, Klau GW, Pisanti N, Bonizzoni P. HapCol: accurate and memory-efficient haplotype assembly from long reads. Bioinformatics. 2016; 32(11):1610–1617. 10.1093/bioinformatics/btv495.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <mixed-citation publication-type="other">Bracciali A, Aldinucci M, Patterson M, Marschall T, Pisanti N, Merelli I, Torquati M. PWHATSHAP: efficient haplotyping for future generation sequencing. BMC Bioinformatics. 2016; 17(11):342. 10.1186/s12859-016-1170-y.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <mixed-citation publication-type="other">Martin M, Patterson M, Garg S, Fischer SO, Pisanti N, Klau GW, Schoenhuth A, Marschall T. WhatsHap: fast and accurate read-based phasing. 2016.</mixed-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bonizzoni</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Della Vedova</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Dondi</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>The haplotyping problem: An overview of computational models and solutions</article-title>
        <source>Journal of Computer Science and Technolgy</source>
        <year>2003</year>
        <volume>18</volume>
        <issue>6</issue>
        <fpage>675</fpage>
        <lpage>688</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02945456</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kuleshov</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Probabilistic single-individual haplotyping</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <issue>17</issue>
        <fpage>379</fpage>
        <lpage>385</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu484</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zook</surname>
            <given-names>JM</given-names>
          </name>
        </person-group>
        <article-title>Integrating human sequence data sets provides a resource of benchmark SNP and indel genotype calls</article-title>
        <source>Nature Biotechnology</source>
        <year>2014</year>
        <volume>32</volume>
        <fpage>246</fpage>
        <lpage>251</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.2835</pub-id>
        <?supplied-pmid 24531798?>
        <pub-id pub-id-type="pmid">24531798</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <mixed-citation publication-type="other">Zook JM, Catoe D, McDaniel J, Vang L, Spies N, Sidow A, Weng Z, Liu Y, Mason CE, Alexander N, Henaff E, McIntyre ABR, Chandramohan D, Chen F, Jaeger E, Moshrefi A, Pham K, Stedman W, Liang T, Saghbini M, Dzakula Z, Hastie A, Cao H, Deikus G, Schadt E, Sebra R, Bashir A, Truty RM, Chang CC, Gulbahce N, Zhao K, Ghosh S, Hyland F, Fu Y, Chaisson M, Xiao C, Trow J, Sherry ST, Zaranek AW, Ball M, Bobe J, Estep P, Church GM, Marks P, Kyriazopoulou-Panagiotopoulou S, Zheng GXY, Schnall-Levin M, Ordonez HS, Mudivarti PA, Giorda K, Sheng Y, Rypdal KB, Salit M. Extensive sequencing of seven human genomes to characterize benchmark reference materials. Scientific Data. 2016; 3(160025). 10.1038/sdata.2016.25.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <mixed-citation publication-type="other">Kalman L, Datta V, Williams M, Zook JM, Salit ML, Han J-Y. Development and characterization of reference materials for genetic testing: Focus on public partnerships. Annals of Laboratory Medicine. 2016; 36(6):513–520. 10.3343/alm.2016.36.6.513.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Garg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Martin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Marschall</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Read-based phasing of related individuals</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <issue>12</issue>
        <fpage>234</fpage>
        <lpage>242</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btw276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Delaneau</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>Haplotype estimation using sequencing reads</article-title>
        <source>American Journal of Human Genetics</source>
        <year>2013</year>
        <volume>93</volume>
        <fpage>687</fpage>
        <lpage>696</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ajhg.2013.09.002</pub-id>
        <?supplied-pmid 24094745?>
        <pub-id pub-id-type="pmid">24094745</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hamada</surname>
            <given-names>YOKAM</given-names>
          </name>
        </person-group>
        <article-title>PBSIM: Pacbio reads simluator–toward accurate genome assembly</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>29</volume>
        <fpage>119</fpage>
        <lpage>121</lpage>
        <?supplied-pmid 23129296?>
        <pub-id pub-id-type="pmid">23129296</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46</label>
      <mixed-citation publication-type="other">Li H. Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM. ArXiv e-prints. 2013. 1303.3997.</mixed-citation>
    </ref>
    <ref id="CR47">
      <label>47</label>
      <mixed-citation publication-type="other">Fischer S. O, Marschall T. Selecting Reads for Haplotype Assembly. bioRxiv. 2016; 046771. 10.1101/046771.</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48</label>
      <mixed-citation publication-type="other">Marschall T. personal communication. 2018.</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lo</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bashir</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bansal</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Bafna</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Strobe sequence design for haplotype assembly</article-title>
        <source>BMC Bioinformatics</source>
        <year>2011</year>
        <volume>12</volume>
        <issue>Suppl. 1</issue>
        <fpage>24</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-12-S1-S24</pub-id>
        <pub-id pub-id-type="pmid">21244715</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Duitama</surname>
            <given-names>J</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fosmid-based whole genome haplotyping of a HapMap trio child: evaluation of single individual haplotyping techniques</article-title>
        <source>Nucleic Acids Research</source>
        <year>2012</year>
        <volume>40</volume>
        <fpage>2041</fpage>
        <lpage>2053</lpage>
        <pub-id pub-id-type="doi">10.1093/nar/gkr1042</pub-id>
        <?supplied-pmid 22102577?>
        <pub-id pub-id-type="pmid">22102577</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51</label>
      <mixed-citation publication-type="other">Hi-c: a comprehensive technique to capture the conformation of genomes. Methods. 2012; 58(3):268–276. 10.1016/j.ymeth.2012.05.001.</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52</label>
      <mixed-citation publication-type="other">Pirola Y, Bonizzoni P, Jiang T. An efficient algorithm for haplotype inference on pedigrees with recombinations and mutations. IEEE/ACM Trans. Comput. Biology Bioinform. 2012; 9(1):12–25. 10.1109/TCBB.2011.51.</mixed-citation>
    </ref>
  </ref-list>
</back>
