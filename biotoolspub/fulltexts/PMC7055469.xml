<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Gigascience</journal-id>
    <journal-id journal-id-type="iso-abbrev">Gigascience</journal-id>
    <journal-id journal-id-type="publisher-id">gigascience</journal-id>
    <journal-title-group>
      <journal-title>GigaScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2047-217X</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7055469</article-id>
    <article-id pub-id-type="doi">10.1093/gigascience/giaa012</article-id>
    <article-id pub-id-type="publisher-id">giaa012</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Technical Note</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DeepPod: a convolutional neural network based quantification of fruit number in <italic>Arabidopsis</italic></article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-2962-6010</contrib-id>
        <name>
          <surname>Hamidinekoo</surname>
          <given-names>Azam</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
        <xref ref-type="author-notes" rid="afn1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Garzón-Martínez</surname>
          <given-names>Gina A</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
        <xref ref-type="author-notes" rid="afn1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ghahremani</surname>
          <given-names>Morteza</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
        <xref ref-type="aff" rid="aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Corke</surname>
          <given-names>Fiona M K</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zwiggelaar</surname>
          <given-names>Reyer</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-6027-1919</contrib-id>
        <name>
          <surname>Doonan</surname>
          <given-names>John H</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-4898-6679</contrib-id>
        <name>
          <surname>Lu</surname>
          <given-names>Chuan</given-names>
        </name>
        <!--<email>cul@aber.ac.uk</email>-->
        <xref ref-type="aff" rid="aff1">1</xref>
        <xref ref-type="corresp" rid="cor1"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><label>1</label><institution>Department of Computer Science, Aberystwyth University</institution>, Aberystwyth, Ceredigion SY233DB, <country country="GB">UK</country></aff>
    <aff id="aff2"><label>2</label><institution>National Plant Phenomics Centre, Institute of Biological, Environmental and Rural Sciences</institution>, Aberystwyth University, Aberystwyth, Ceredigion SY233EB, <country country="GB">UK</country></aff>
    <author-notes>
      <corresp id="cor1"><bold>Correspondence address</bold>. Chuan Lu, Department of Computer Science, Aberystwyth University, Aberystwyth, Ceredigion SY233DB, UK. E-mail: <email>cul@aber.ac.uk</email></corresp>
      <fn id="afn1">
        <p>Contributed equally.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-03-04">
      <day>04</day>
      <month>3</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>04</day>
      <month>3</month>
      <year>2020</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>9</volume>
    <issue>3</issue>
    <elocation-id>giaa012</elocation-id>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>7</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>18</day>
        <month>11</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="giaa012.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="abs1">
        <title>Background</title>
        <p>High-throughput phenotyping based on non-destructive imaging has great potential in plant biology and breeding programs. However, efficient feature extraction and quantification from image data remains a bottleneck that needs to be addressed. Advances in sensor technology have led to the increasing use of imaging to monitor and measure a range of plants including the model <italic>Arabidopsis thaliana</italic>. These extensive datasets contain diverse trait information, but feature extraction is often still implemented using approaches requiring substantial manual input.</p>
      </sec>
      <sec id="abs2">
        <title>Results</title>
        <p>The computational detection and segmentation of individual fruits from images is a challenging task, for which we have developed DeepPod, a patch-based 2-phase deep learning framework. The associated manual annotation task is simple and cost-effective without the need for detailed segmentation or bounding boxes. Convolutional neural networks (CNNs) are used for classifying different parts of the plant inflorescence, including the tip, base, and body of the siliques and the stem inflorescence. In a post-processing step, different parts of the same silique are joined together for silique detection and localization, whilst taking into account possible overlapping among the siliques. The proposed framework is further validated on a separate test dataset of 2,408 images. Comparisons of the CNN-based prediction with manual counting (<italic>R</italic><sup>2</sup> = 0.90) showed the desired capability of methods for estimating silique number.</p>
      </sec>
      <sec id="abs3">
        <title>Conclusions</title>
        <p>The DeepPod framework provides a rapid and accurate estimate of fruit number in a model system widely used by biologists to investigate many fundemental processes underlying growth and reproduction</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="keywords">
      <kwd>plant phenotyping</kwd>
      <kwd>image analysis</kwd>
      <kwd>deep learning</kwd>
      <kwd>object detection</kwd>
      <kwd>fruit counting</kwd>
      <kwd>
        <italic>Arabidopsis</italic>
      </kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">Biotechnology and Biological Sciences Research Council</named-content>
          <named-content content-type="funder-identifier">10.13039/501100000268</named-content>
        </funding-source>
        <award-id>BB/CAP1730/1</award-id>
        <award-id>BB/P013376/1</award-id>
        <award-id>BB/P003095/1</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Science Foundation</named-content>
          <named-content content-type="funder-identifier">10.13039/100000001</named-content>
        </funding-source>
        <award-id>1340112</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="13"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1">
    <title>Introduction</title>
    <p>Photometrics (imaging followed by computationally assisted feature extraction and measurement) promises to revolutionize biological research and agricultural production systems [<xref rid="bib1" ref-type="bibr">1–5</xref>]. Automation of workflows remains a key challenge in the scaling of these approaches to cope with the requirements of large genetic experiments or, indeed, food production systems. Phenotyping aims to measure observable plant features, often as a response to environmental cues and/or variability between individuals. Traditionally, phenotyping has been a labour-intensive and costly process, usually manual and often destructive. High-throughput phenotyping technologies aim to address this problem by the use of non-destructive approaches either in glasshouses [<xref rid="bib1" ref-type="bibr">1</xref>, <xref rid="bib2" ref-type="bibr">2</xref>, <xref rid="bib6" ref-type="bibr">6</xref>] or directly in the field [<xref rid="bib4" ref-type="bibr">4</xref>, <xref rid="bib7" ref-type="bibr">7</xref>] integrating imaging, robotics, spectroscopy, high-tech sensors, and high-performance computing [<xref rid="bib3" ref-type="bibr">3</xref>, <xref rid="bib8" ref-type="bibr">8</xref>].</p>
    <p>Imaging has the potential to generate an enormous volume of data in real time, while image analysis to extract useful information is currently the main bottleneck. The extraction of quantitative traits relies on the development and use of improved software techniques. Machine learning tools have been used to identify patterns in large biological datasets [<xref rid="bib8" ref-type="bibr">8–12</xref>]. Recently, deep learning tools have been applied to accurately extract features from plant images [<xref rid="bib13" ref-type="bibr">13–21</xref>].</p>
    <p>Model organisms have been widely used to dissect different biological processes and provide a useful means to test and develop new methods that can subsequently be more widely applied to crop and ecological scenarios. <italic>Arabidopsis thaliana</italic> is a small, flowering plant widely used to address questions related to plant genetics, molecular biology, evolution, ecology, and physiology, among others [<xref rid="bib22" ref-type="bibr">22–24</xref>]. The seedling produces a small rosette that increases in size by addition of leaves. The central meristem produces an inflorescence that produces flowers and then fruits. The fruits are also known as pods or siliques [<xref rid="bib24" ref-type="bibr">24</xref>]. The measurement of traits, such as growth rate, flowering, and fruit number, is key to evaluate plant performance and reproductive fitness [<xref rid="bib25" ref-type="bibr">25</xref>]. However, many high-throughput imaging studies focus on growth dynamics of the rosette [<xref rid="bib9" ref-type="bibr">9</xref>, <xref rid="bib26" ref-type="bibr">26–28</xref>], despite the importance of fruit production in reproductive and evolutionary processes [<xref rid="bib2" ref-type="bibr">2</xref>, <xref rid="bib29" ref-type="bibr">29–31</xref>].</p>
    <p>This work demonstrates that deep learning can be used to estimate fruit number from images. In particular, we have developed DeepPod, a framework for <italic>Arabidopsis</italic> silique detection that involves a deep neural network for patch-based classification and an object reconstructor for silique localization and counting. The framework has been validated using a separate dataset of 2,408 images from biological experiments. This allowed the analysis of large numbers of plants' inflorescences in an accurate and effective way, providing a cost-effective alternative to manual counting.</p>
  </sec>
  <sec id="sec2">
    <title>Background</title>
    <p>Convolutional neural networks (CNNs) have become the dominant type of models for image classification [<xref rid="bib32" ref-type="bibr">32</xref>]. The input for a CNN, typically an image, can be represented as a 3D array of height × width × channels. A CNN contains convolutional layers, where inputs are passed through various filters for extracting features that are arranged as feature maps, prior to using the fully connected layers for classification or regression. The weights or parameters of the filters are shared among the neurons of the convolutional layers [<xref rid="bib33" ref-type="bibr">33</xref>], not only to encourage detection of repeated patterns in the image but also to reduce the number of parameters for the network to learn. Other types of layers such as pooling are also often used in combination with convolutional layers to reduce the dimensionality of feature maps. A CNN can be trained using a back-propagation algorithm to update the weights in an iterative process, in order to minimize the loss function that measures the discrepancy between the predicted output and actual output for the training examples. What makes CNNs particularly attractive in computer vision is that they can directly extract features from images without the need for time-consuming, hand-crafted pre-processing or feature extraction steps, unlike classical machine learning approaches [<xref rid="bib34" ref-type="bibr">34</xref>].</p>
    <p>Recent publications have reported the application of deep learning in various plant phenotyping tasks such as leaf counting, age estimation, mutant classification, disease detection, fruit classification, and plant organ localization [<xref rid="bib13" ref-type="bibr">13–16</xref>, <xref rid="bib18" ref-type="bibr">18–21</xref>]. Mohanty et al. [<xref rid="bib14" ref-type="bibr">14</xref>] trained deep CNNs to identify 14 crop species and 26 diseases using a publicly available plant disease dataset. They built models with architectures of AlexNet [<xref rid="bib35" ref-type="bibr">35</xref>] and GoogleNet [<xref rid="bib36" ref-type="bibr">36</xref>] using transfer learning. Wang et al. [<xref rid="bib20" ref-type="bibr">20</xref>] used CNNs to establish disease severity in apple black rot images. Deep learning meta-architectures have also been considered for more complex scenarios. Fuentes et al. [<xref rid="bib19" ref-type="bibr">19</xref>] demonstrated a combination of CNNs and deep feature extractors to recognize different diseases and pests in tomatoes, which dealt with inter- and intra-class variations. Deep learning was also used for cassava disease detection via mobile devices [<xref rid="bib21" ref-type="bibr">21</xref>]. Pawara et al. [<xref rid="bib18" ref-type="bibr">18</xref>] applied CNNs to classify leaves, fruits, and flowers in field images. They compared the performance of classical classifiers to CNNs, where architectures such as GoogleNet and AlexNet gave the best results in the plant-related datasets used. Namin et al.[<xref rid="bib16" ref-type="bibr">16</xref>] proposed a convolutional neural network–long short-term memory (CNN-LSTM) framework for plant classification using temporal sequence of images. In particular, the model features were learned using CNNs and the plant growth variation over time was modeled with LSTMs. Ubbens et al. [<xref rid="bib15" ref-type="bibr">15</xref>] used CNNs for regression to perform leaf counting. They used rendered images of synthetic plants to augment an <italic>Arabidopsis</italic> rosette dataset and concluded that the augmentation with high-quality 3D synthetic plants improved the performance of leaf counting while real and synthetic plants could be interchangeably used for training a neural network. Pound et al. [<xref rid="bib13" ref-type="bibr">13</xref>] demonstrated wheat root and shoot feature identification and localization using 2 different standard CNN architectures for patch classification. For shoot features, they found that the leaf tips represented the hardest classification problem compared to the leaf base owing to the existing variations in orientation, size, shape, and colour of tips in their dataset. Further reconstruction from the classification results of the overlapping patches allows localization of separate structural regions such as leaf tips and bases. However the objects of interest as a whole (such as leaves) are yet to be identified in order to extract more morphological features (e.g., leaf length and shape).</p>
    <p>Our proposed framework treats the silique (or pod) counting problem as an object detection and segmentation problem followed by counting. One popular approach of deep learning frameworks for object detection is to train a single CNN to jointly perform object classification and localization tasks, where the object localizations are usually defined by bounding boxes. Examples of such networks include Fast-RCNN (regional-CNN), SSD (single shot multibox detector), and YOLO (you only look once) [<xref rid="bib37" ref-type="bibr">37</xref>]. However training of such networks requires labelled data with detailed segmentation or bounding boxes of individual objects, which are usually obtained through a tedious manual process. Moreover, the image size allowed for the network input is limited owing to the complexity of network architecture and the available memory.</p>
    <p>In our case, the resolution of the raw images needs to be sufficiently high in order to preserve details of pods that are small and narrow, often overlapping. A single image can also contain a wide variation in the number of fruits from 0 to near 400, which poses further challenges for deep learning models when the available labelled data are limited.</p>
    <p>To address these issues, we adopted an alternative approach that performs patch-based classification and localization in 2 separate phases. The first step is to classify a region of a suitable size in the original image into different parts of the inflorescences. In the localization phase, each original image will be scanned and each patch classified as silique/not silique (i.e., as 1 of the 4 classes including the tip, base, or body of siliques, and the stem inflorescence). Given an accurate classification of patches as silique/not silique, one could then estimate the number of siliques and their lengths to a good precision. The manual annotation task for the proposed framework was simple, involving collection of sufficient pixels from different defined structural parts of the plant.</p>
  </sec>
  <sec id="sec3">
    <title>Data Acquisition</title>
    <p>A set of 2,552 images of mature inflorescences taken from a subset of the Multiparent Advanced Generation Inter-Cross (MAGIC) RIL (Recombinant Inbred Line) population [<xref rid="bib38" ref-type="bibr">38</xref>] were used to establish and test the CNN pipeline. A subset (referred to as Set-1 = 144 images) of this dataset was randomly selected for manual annotation and then used to train 1 shallow and 1 deep CNN. A total of 2,408 images (referred to as Set-2) were used to test the performance of the selected model. Information about the dedicated data for different tasks is presented in Table <xref rid="tbl1" ref-type="table">1</xref>.</p>
    <table-wrap id="tbl1" orientation="portrait" position="float">
      <label>Table 1.</label>
      <caption>
        <p>Information about the dedicated data for different tasks</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th align="left" rowspan="1" colspan="1">Dataset name</th>
            <th align="left" rowspan="1" colspan="1">No. of images</th>
            <th align="left" rowspan="1" colspan="1">Provided annotation</th>
            <th align="left" rowspan="1" colspan="1">Used task</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">Set-1</td>
            <td rowspan="1" colspan="1">144</td>
            <td rowspan="1" colspan="1">Silique main structural elements, silique count</td>
            <td rowspan="1" colspan="1">Developing classification model, developing counting pipeline</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Set-2</td>
            <td rowspan="1" colspan="1">2,408</td>
            <td rowspan="1" colspan="1">Silique count</td>
            <td rowspan="1" colspan="1">Evaluating counting pipeline</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>Plants were grown on an automatic watering platform within the National Plant Phenomics Centre (NPPC) (Aberystwyth University, UK) in 6-cm diameter pots half-filled with vermiculite; the upper half was filled with 30% grit/sand: 70% Levington F1 (peat-based compost). The vermiculite was used to restrict plant growth. Pots were filled to a uniform weight. Each plant was automatically weighed and irrigated from above to a 75% gravimetric water content daily.</p>
    <p>The mature inflorescence or stem of each plant, with attached fruits, was harvested and imaged in a flatbed scanner (Plustek, OpticPro A320, Ewige Weide 13 22926 Ahrensburg, Germany). Images were saved at 300 dpi and stored in .PNG format with image size equal to 3,600 × 5,100 pixels. The image file name includes the identification number for the line (e.g., ATxxx_001xxx represents RIL001) according to Kover et al. [<xref rid="bib38" ref-type="bibr">38</xref>]. A sample image is shown in Fig. <xref ref-type="fig" rid="fig1">1</xref>. Manual counting of viable fruits in images was undertaken by a single person to minimize operator variation. ImageJ [<xref rid="bib39" ref-type="bibr">39</xref>] was used to track the counting by setting a label to each fruit as it was counted.</p>
    <fig id="fig1" orientation="portrait" position="float">
      <label>Figure 1</label>
      <caption>
        <p>An illustrative example of image and features (the important parts of the plants) annotated for patch-based classification.</p>
      </caption>
      <graphic xlink:href="giaa012fig1"/>
    </fig>
  </sec>
  <sec id="sec4">
    <title>Patch-based Classification using CNNs</title>
    <sec id="sec4-1">
      <title>Data preparation for model development</title>
      <sec id="sec4-1-1">
        <title>Data annotation</title>
        <p>An annotation tool with a GUI was built (in MATLAB) to assist with manual annotation of different parts of the inflorescence. Fig. <xref ref-type="fig" rid="fig2">2</xref> shows the schematic of this GUI with some screenshots of annotation. The user selects the class type (tip, base, body of the silique, and stem) and clicks on the respective parts on each input image. The annotated parts (points clicked) were saved as defined locations based on image coordinates. An example annotated image illustrating the predefined parts of the silique (tip, base, body, and stem) is given in Fig. <xref ref-type="fig" rid="fig3">3</xref>. This tool was used to manually annotate Set-1, which was used to develop the patch classifiers (see Section Patch-Based Classification Problem).</p>
        <fig id="fig2" orientation="portrait" position="float">
          <label>Figure 2</label>
          <caption>
            <p>The developed GUI used for manually annotating plant parts.</p>
          </caption>
          <graphic xlink:href="giaa012fig2"/>
        </fig>
        <fig id="fig3" orientation="portrait" position="float">
          <label>Figure 3</label>
          <caption>
            <p>Example annotated images (from left to right) for tip, base, body, and stem. Our annotation approach only requires sampling of pixels/points for the 4 main structural regions. Although most tips and bases have been annotated (see the left 2 panels), only a small portion of points for body or stems have been sampled and labelled (see the right 2 panels).</p>
          </caption>
          <graphic xlink:href="giaa012fig3"/>
        </fig>
        <p>The main advantages of this annotation platform include its relatively low cost and ease of use. Compared to other annotation approaches that require detailed segmentation, polygons, or bounding boxes, this approach requires annotation of just 4 main structural elements. Using this platform, Set-1 was manually annotated by a single person in a total of 36 working hours.</p>
        <p>Table <xref rid="tbl2" ref-type="table">2</xref> shows the number of annotations performed per class (before augmentation). This dataset was used in the initial training step for classifying whole inflorescences into defined parts. To prepare patches for classification, Set-1 was randomly split into training, validation, and test sets as rounded of 65%, 20%, and 15% of the 144 images.</p>
        <table-wrap id="tbl2" orientation="portrait" position="float">
          <label>Table 2.</label>
          <caption>
            <p>Summary statistics for data annotation performed on Set-1</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1">Feature</th>
                <th align="left" rowspan="1" colspan="1">No. of manual annotations</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Silique tip</td>
                <td rowspan="1" colspan="1">7,299</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Silique base</td>
                <td rowspan="1" colspan="1">8,058</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Silique body</td>
                <td rowspan="1" colspan="1">11,187</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Stem</td>
                <td rowspan="1" colspan="1">10,266</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="sec4-1-2">
        <title>Patch generation and augmentation</title>
        <p>An approach similar to that proposed by Pound et al. [<xref rid="bib13" ref-type="bibr">13</xref>] has been followed for image patch generation and augmentation. Using the annotated data to prepare training samples, square bounding box patches were extracted while being centred at the manually annotated points. Subsequently, data augmentation [<xref rid="bib40" ref-type="bibr">40</xref>, <xref rid="bib41" ref-type="bibr">41</xref>] was performed to increase the amount of training data via specific transformations, as well as considering frames different from the centred ones. The patches of size 50 × 50 were first extracted. Then, random 32 × 32 pixel crops followed by random mirroring or rotation were performed. For pre-processing, we normalized the data using the channel means and standard deviations on the training set. For validation samples, no augmentation was undertaken and the 32 × 32 patches centred at the annotated points were extracted. Fig. <xref ref-type="fig" rid="fig4">4</xref> shows various examples of each class that were used in the training procedure.</p>
        <fig id="fig4" orientation="portrait" position="float">
          <label>Figure 4</label>
          <caption>
            <p>Example extracted patches using manual annotations. <italic>From top to bottom:</italic> samples of base, body, stem, and tip, respectively. Note that the key structural elements are not always centred in the patches as a result of the random cropping process for patch extraction.</p>
          </caption>
          <graphic xlink:href="giaa012fig4"/>
        </fig>
      </sec>
    </sec>
    <sec id="sec4-2">
      <title>Data preparation for testing</title>
      <p>The training patches were centred at the annotated points; this was followed by augmentation, as described earlier. To prepare test samples, the difference in the pixel intensity distribution between the testing data and the training/validation data (that were used during training time) was taken into account. First, the whole image was scanned over with a sliding window and tiled into 32 × 32 patches with 50% overlap in both the vertical and horizontal directions (see Fig. <xref ref-type="fig" rid="fig7">7</xref>). Most pixels within the area of interest (plant area) would hence be included in 4 different patches. The patches belonging to the white background (lacking plant pixels) were excluded by thresholding.</p>
      <p>The rationale behind selecting overlapping regions was (i) to increase the number of patches by a factor of 4 compared to without overlapping and (ii) to make the patch classification more robust by combining multiple predictions.</p>
      <p>When the model is applied to test data, the difference between the sample distribution for training and that for testing, i.e., presence of potential covariate and dataset shift, can adversely affect the model generalization performance. To address this issue, each test image was also to be normalized using the channel-wise mean and standard deviation of the training set.</p>
      <p>Then the resultant patches were fed to the trained networks and the classification outcomes for each sample patch (tip, base, stem, body) were computed.</p>
    </sec>
    <sec id="sec4-3">
      <title>Building CNN classifiers</title>
      <p>In the next step, CNN-based classifiers were built to take extracted patches of interest as input, and to output probability scores for different labels {0, 1, 2, 3} indicating the probability that the input patch contains a base, body, stem, and tip, respectively.</p>
      <sec id="sec4-3-1">
        <title>Network architecture</title>
        <p>LeNet is a pioneering CNN that was proposed to classify handwriting digits [<xref rid="bib42" ref-type="bibr">42</xref>]. LeNet's architecture [<xref rid="bib43" ref-type="bibr">43</xref>] consists of 2 sets of convolutional and pooling layers stacked on top of each other, followed by 2 fully connected layers and finally ending with a Softmax layer (see Fig. <xref ref-type="fig" rid="fig5">5</xref>). LeNet is a simple shallow network and has been chosen as a baseline model in this study, considering the potentially higher computational resource needs for running more complex deep learning models.</p>
        <fig id="fig5" orientation="portrait" position="float">
          <label>Figure 5</label>
          <caption>
            <p>LeNet architecture.</p>
          </caption>
          <graphic xlink:href="giaa012fig5"/>
        </fig>
        <p>DenseNet is a model notable for its key characteristic of bypassing signals from preceding layers to subsequent layers that enforces optimal information flow in the form of feature maps. Amongst DenseNet variants [<xref rid="bib44" ref-type="bibr">44</xref>], DenseNet-Basic is a successful model proposed for the CIFAR10 [<xref rid="bib34" ref-type="bibr">34</xref>] image classification challenge. Hereafter, DenseNet-Basic is referred to as “DenseNet.” A simple DenseNet is made up of a total of <italic>L</italic> layers, while each layer is responsible for implementing a specific non-linear transformation, which is a composite function of different operations such as batch normalization, rectified linear unit, pooling, and convolution [<xref rid="bib42" ref-type="bibr">42</xref>, <xref rid="bib44" ref-type="bibr">44</xref>]. Within a dense block that consists of multiple densely connected layers with such composite functions, all layers are directly connected to each other, and each layer receives inputs (i.e., feature maps) from all preceding layers (as illustrated in the middle row of Fig. <xref ref-type="fig" rid="fig6">6</xref>). The number of feature maps generated from the composite function layer is usually fixed and is called the growth rate (<italic>k</italic>) for the DenseNet.</p>
        <fig id="fig6" orientation="portrait" position="float">
          <label>Figure 6</label>
          <caption>
            <p>The DenseNet-Basic architecture used for patch-based <italic>Arabidopsis</italic> structural part classification. The feature map sizes in the 3 dense blocks were 32 × 32, 16 × 16, and 8 × 8, respectively.</p>
          </caption>
          <graphic xlink:href="giaa012fig6"/>
        </fig>
        <p>To facilitate down-sampling for CNNs, the network used for our experiment consisted of multiple dense blocks. These dense blocks were connected to each other through transition layers (composed of a batch normalization layer, a 1 × 1 convolutional layer, a dropout layer, and a 2 × 2 average pooling layer as shown in the bottom row of Fig. <xref ref-type="fig" rid="fig6">6</xref>).</p>
        <p>The growth rate (<italic>k</italic>) was set to 12 for all dense blocks in order to generate narrow layers within the overall DenseNet structure (i.e., 3 dense blocks with equal number of layers and 2 transition layers). A relatively small growth rate (of 12) was found to be sufficient to obtain satisfying results on our target datasets. The initial convolution layer incorporated 16 convolutions of size 3 × 3 on the input images. The number of feature maps in all other layers follows the setting for <italic>k</italic>.</p>
        <p>At the end of the last dense block (third dense block), a global average pooling was performed to minimize over-fitting by reducing the total number of parameters in the model. The final Softmax classifier of 4 output nodes will predict the probability for each class on the basis of the extracted features in the network. The rest of the model’s parameters with regards to the kernel, stride, and padding sizes were kept as default as detailed in Huang et al. [<xref rid="bib44" ref-type="bibr">44</xref>].</p>
      </sec>
      <sec id="sec4-3-2">
        <title>Training</title>
        <p>In our experiments with LeNet and DenseNet, a configuration similar to that in Huang et al. [<xref rid="bib44" ref-type="bibr">44</xref>] has been applied. Both models were trained via a stochastic gradient descent solver with the parameters set to Gamma = 0.1 (for the learning rate decreasing factor), momentum = 0.9 (for weight update from the previous iteration), and weight-decay factor = 10<sup>−5</sup>. We trained LeNet and DenseNet with mini-batches of size 64 and 8 (according to our hardware specifications), respectively. Both models were trained using an initial learning rate of 0.001 with 33% step-down policy. LeNet was trained for 15 epochs, and DenseNet, for 30 epochs. In our implementation, the LeNet and DenseNet models pretrained on the CIFAR10 dataset [<xref rid="bib34" ref-type="bibr">34</xref>] were used to initialize the weights, whilst the networks were fine-tuned using prepared training data from the silique dataset. In the pre-processing step for each model, the mean patch calculated on the training set patches was subtracted for each sample patch being fed.</p>
        <p>All CNN training and testing was performed within the Caffe framework [<xref rid="bib45" ref-type="bibr">45</xref>]. The computations were carried out using an NVIDIA GeForce GTX 1080 GPU, Intel Core i7-4790 processor, and Ubuntu 16.04 operating system.</p>
        <p>Table <xref rid="tbl3" ref-type="table">3</xref> shows the classification accuracy and loss for both networks on the validation data from Set-1 after training.</p>
        <table-wrap id="tbl3" orientation="portrait" position="float">
          <label>Table 3.</label>
          <caption>
            <p>Classification results on the validation samples</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th align="left" rowspan="1" colspan="1">LeNet</th>
                <th align="left" rowspan="1" colspan="1">DenseNet</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Accuracy</td>
                <td rowspan="1" colspan="1">80.55%</td>
                <td rowspan="1" colspan="1">86.80%</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Loss</td>
                <td rowspan="1" colspan="1">0.64</td>
                <td rowspan="1" colspan="1">0.37</td>
              </tr>
            </tbody>
          </table>
        </table-wrap>
      </sec>
      <sec id="sec4-3-3">
        <title>Performance on patch-based classification</title>
        <p>In the initial evaluation, we used the test samples in our model development data Set-1 to evaluate the classification and detection performance of both the shallow and deep networks. The aim of this comparative evaluation was to choose the best model for correct classification of patches and estimating silique counts on the smaller development dataset.</p>
        <p>The classification results of both networks are presented in Tables <xref rid="tbl4" ref-type="table">4</xref> and <xref rid="tbl5" ref-type="table">5</xref> in terms of a confusion matrix, per-class precision and recall, and total classification precision and recall. Note that only annotated patches have been considered for this evaluation. The DenseNet network has higher representational power due to its deeper architecture and its use of features of multiple levels for classification in comparison to the LeNet network; its efficacy in the learning task has also been evidenced by its higher accuracy in classifying plant parts (as shown in Tables <xref rid="tbl4" ref-type="table">4</xref> and <xref rid="tbl5" ref-type="table">5</xref>).</p>
        <table-wrap id="tbl4" orientation="portrait" position="float">
          <label>Table 4.</label>
          <caption>
            <p>Performance of patch-based classification on the testing images for model development using LeNet network</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="2" align="left" colspan="1">Predicted</th>
                <th colspan="4" align="center" rowspan="1">Actual</th>
                <th rowspan="2" align="center" colspan="1">Precision (%)</th>
                <th rowspan="2" align="center" colspan="1">Recall (%)</th>
              </tr>
              <tr>
                <th align="left" rowspan="1" colspan="1">Base</th>
                <th align="left" rowspan="1" colspan="1">Body</th>
                <th align="left" rowspan="1" colspan="1">Stem</th>
                <th align="left" rowspan="1" colspan="1">Tip</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Base</td>
                <td rowspan="1" colspan="1">344</td>
                <td rowspan="1" colspan="1">12</td>
                <td rowspan="1" colspan="1">52</td>
                <td rowspan="1" colspan="1">4</td>
                <td rowspan="1" colspan="1">74.1</td>
                <td rowspan="1" colspan="1">83.5</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Body</td>
                <td rowspan="1" colspan="1">15</td>
                <td rowspan="1" colspan="1">280</td>
                <td rowspan="1" colspan="1">26</td>
                <td rowspan="1" colspan="1">30</td>
                <td rowspan="1" colspan="1">79.5</td>
                <td rowspan="1" colspan="1">79.8</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Stem</td>
                <td rowspan="1" colspan="1">14</td>
                <td rowspan="1" colspan="1">29</td>
                <td rowspan="1" colspan="1">270</td>
                <td rowspan="1" colspan="1">4</td>
                <td rowspan="1" colspan="1">75.8</td>
                <td rowspan="1" colspan="1">85.2</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Tip</td>
                <td rowspan="1" colspan="1">91</td>
                <td rowspan="1" colspan="1">31</td>
                <td rowspan="1" colspan="1">8</td>
                <td rowspan="1" colspan="1">169</td>
                <td rowspan="1" colspan="1">81.6</td>
                <td rowspan="1" colspan="1">56.5</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="req-158215116907980440">
              <p>Total precision = 77.8%; total recall = 76.2%.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <table-wrap id="tbl5" orientation="portrait" position="float">
          <label>Table 5.</label>
          <caption>
            <p>Performance of patch-based classification on the testing images for model development using DenseNet network</p>
          </caption>
          <table frame="hsides" rules="groups">
            <thead>
              <tr>
                <th rowspan="2" align="left" colspan="1">Predicted</th>
                <th colspan="4" align="center" rowspan="1">Actual</th>
                <th rowspan="2" align="center" colspan="1">Precision (%)</th>
                <th rowspan="2" align="center" colspan="1">Recall (%)</th>
              </tr>
              <tr>
                <th align="left" rowspan="1" colspan="1">Base</th>
                <th align="left" rowspan="1" colspan="1">Body</th>
                <th align="left" rowspan="1" colspan="1">Stem</th>
                <th align="left" rowspan="1" colspan="1">Tip</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">Base</td>
                <td rowspan="1" colspan="1">392</td>
                <td rowspan="1" colspan="1">4</td>
                <td rowspan="1" colspan="1">14</td>
                <td rowspan="1" colspan="1">2</td>
                <td rowspan="1" colspan="1">93.6</td>
                <td rowspan="1" colspan="1">95.1</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Body</td>
                <td rowspan="1" colspan="1">15</td>
                <td rowspan="1" colspan="1">290</td>
                <td rowspan="1" colspan="1">13</td>
                <td rowspan="1" colspan="1">33</td>
                <td rowspan="1" colspan="1">93.2</td>
                <td rowspan="1" colspan="1">82.6</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Stem</td>
                <td rowspan="1" colspan="1">11</td>
                <td rowspan="1" colspan="1">14</td>
                <td rowspan="1" colspan="1">290</td>
                <td rowspan="1" colspan="1">2</td>
                <td rowspan="1" colspan="1">91.5</td>
                <td rowspan="1" colspan="1">91.5</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">Tip</td>
                <td rowspan="1" colspan="1">1</td>
                <td rowspan="1" colspan="1">3</td>
                <td rowspan="1" colspan="1">0</td>
                <td rowspan="1" colspan="1">295</td>
                <td rowspan="1" colspan="1">88.9</td>
                <td rowspan="1" colspan="1">98.7</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="req-158215166320080440">
              <p>Total precision = 91.8%; total recall = 92%.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
    </sec>
  </sec>
  <sec id="sec5">
    <title>Post-processing for Silique Localization and Counting</title>
    <sec id="sec5-1">
      <title>Image reconstruction</title>
      <p>Given the classification of various patches in an image, post-processing can be applied to reconstruct the image and detect probable silique appearances. The plant regions are first identified from the background (including borders) using simple thresholding methods. Then the plant regions are further segmented into 4 classes based on labelling of the patches of interest.</p>
      <p>Because the patches for a test image are generated with <inline-formula><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$50\%$\end{document}</tex-math></inline-formula> overlap along both the horizontal and vertical directions, each patch consists of 4 squares of equal size (16 × 16), called sub-patches. Each sub-patch has 4 class predictions from 4 adjacent patches, the final decision is inferred through majority vote, and the label for each pixel in the sub-patch was determined accordingly (see Fig. <xref ref-type="fig" rid="fig7">7</xref>). In case of a tied vote for several classes, the average probability of those classes for the image will be assigned to the sub-patch and its pixels.</p>
      <fig id="fig7" orientation="portrait" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Flow chart of the sub-patch labelling step for image reconstruction.</p>
        </caption>
        <graphic xlink:href="giaa012fig7"/>
      </fig>
    </sec>
    <sec id="sec5-2">
      <title>Silique counting</title>
      <p>To count siliques in the reconstructed image, a silique is defined as an area composed of 3 interconnected parts: 1 tip, 1 body, and 1 base in such a way that the body is located between the tip and the base (Fig. <xref ref-type="fig" rid="fig1">1</xref>). In the areas where tips and bodies presenting shared borders were initially identified, these tip-body areas were extended through shared borders to search for the connected tips, which eventually established a combined area for a silique object.</p>
      <p>In practice, many touching or overlapping siliques were observed in the captured images, which was a problem for detecting individual siliques accurately. In the case where 1 silique object area contained multiple tips or bases, the angle between the potentially overlaid siliques was calculated, using a cross-product between the different vectors linking the bases to the corresponding tips. For example, for the case of 2 siliques overlaying (often with the same apparent base or tip), the centres of tips and bases were computed; then using a cross-product, the centres were connected in order to calculate the angle between overlaid siliques. If the measured angle was larger than a predetermined threshold, the region was considered as 2 distinctive siliques, otherwise as a single silique. The value of the threshold was set to 0.05 rad in our experiments according to the resolution of the images. See <xref ref-type="supplementary-material" rid="sup14">supplementary Figure S1</xref> for an illustrative example on how to detect/count individual siliques with overlapping regions.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec6">
    <title>Test Results for Silique Counting</title>
    <sec id="sec6-1" sec-type="results">
      <title>Results on the test data for model development</title>
      <p>Fig. <xref ref-type="fig" rid="fig8">8</xref> shows the results of image reconstruction for several randomly selected images after patch classification (using the DenseNet network), with colours indicating different structural parts of the plant.</p>
      <fig id="fig8" orientation="portrait" position="float">
        <label>Figure 8</label>
        <caption>
          <p>Three example results of labelling on the reconstructed plant images based the DenseNet patch-based classification. Tips, bodies, bases, and stems are indicated in red, green, blue, and white, respectively.</p>
        </caption>
        <graphic xlink:href="giaa012fig8"/>
      </fig>
      <p>Table <xref rid="tbl6" ref-type="table">6</xref> reports the performance of silique count prediction using the 2 different trained networks. In this table, the correlation coefficient (for the linear relationship between the prediction and the manual counts) shows that the prediction using the deeper model (DenseNet) is more accurate than that using the shallower model (LeNet). This linear correlation can be better seen in Fig. <xref ref-type="fig" rid="fig9">9</xref>, which shows the scatter plots of the actual vs automated silique counts. We also examined the distribution of the errors (actual − prediction) in silique counting; see Fig. <xref ref-type="fig" rid="fig10">10</xref> for the histograms of errors for the 2 trained models. It appears that both LeNet and DenseNet underestimated the counts compared to manual counting in most cases.</p>
      <fig id="fig9" orientation="portrait" position="float">
        <label>Figure 9</label>
        <caption>
          <p>Predicted counts using the 2 models using validation and testing samples. <italic>R</italic><sup>2</sup> = 0.90 for the LeNet-based model and <italic>R</italic><sup>2</sup> = 0.95 for the DenseNet-based model.</p>
        </caption>
        <graphic xlink:href="giaa012fig9"/>
      </fig>
      <fig id="fig10" orientation="portrait" position="float">
        <label>Figure 10</label>
        <caption>
          <p>The histograms of errors in silique count prediction for the LeNet and DenseNet models.</p>
        </caption>
        <graphic xlink:href="giaa012fig10"/>
      </fig>
      <table-wrap id="tbl6" orientation="portrait" position="float">
        <label>Table 6.</label>
        <caption>
          <p>Performance for silique count prediction compared to manual counting on the 22 test images for model development</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th align="left" rowspan="1" colspan="1">Metric</th>
              <th align="left" rowspan="1" colspan="1">LeNet</th>
              <th align="left" rowspan="1" colspan="1">DenseNet</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Correlation coefficient</td>
              <td rowspan="1" colspan="1">0.932</td>
              <td rowspan="1" colspan="1">0.954</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Root mean squared error</td>
              <td rowspan="1" colspan="1">20.35</td>
              <td rowspan="1" colspan="1">12.45</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Comparing a shallow and a deep network for classifying image patches, we concluded that the classification results and the quality of the count estimation show improvement from using the deeper architecture. Therefore, DenseNet has been selected for identifying siliques because it seemed to be more robust to the variations in shape and size. This is probably in part a consequence of using a training set of images from diverse individuals harvested at different stages of silique maturation.</p>
    </sec>
    <sec id="sec6-2" sec-type="results">
      <title>Results on the separate test data</title>
      <p>To further evaluate the proposed framework, we used a separate large dataset of 2,408 images available within the NPPC. The scatter plot in Fig. <xref ref-type="fig" rid="fig11">11</xref> shows a high positive correlation (Pearson correlation coefficient <italic>R</italic><sup>2</sup> = 0.90) between the manual counts and automated counts. With the reconstructed silique objects, additional morphological features could be extracted including silique length. Predicted silique number and statistics for silique length (mean, maximum, and minimum) per image are reported in <xref ref-type="supplementary-material" rid="sup14">Supplementary Data S1</xref>. Preliminary validation of the mean length estimate has been given in <xref ref-type="supplementary-material" rid="sup14">Supplementary Figure S2</xref> and <xref ref-type="supplementary-material" rid="sup14">Supplementary Data S2 and S3</xref>.</p>
      <fig id="fig11" orientation="portrait" position="float">
        <label>Figure 11</label>
        <caption>
          <p>Predicted silique count and manual counting from Set-2 testing samples including 2,408 images. <italic>R</italic><sup>2</sup> = 0.90.</p>
        </caption>
        <graphic xlink:href="giaa012fig11"/>
      </fig>
      <p>The CNN-based prediction tends to underestimate compared to actual manual counting. To better understand where the problem lies, detailed detection results have also been visualized; see Fig. <xref ref-type="fig" rid="fig12">12</xref> for some random examples. It seems that the current-post processing method might have difficulty in detecting some small or overlapping siliques.</p>
      <fig id="fig12" orientation="portrait" position="float">
        <label>Figure 12</label>
        <caption>
          <p>Results of the DenseNet framework applied to some random samples from the larger testing dataset. <italic>From left to right</italic>: original plant images, sub-patch labelling and image reconstruction (Tips, bodies, bases, and stems are indicated in red, green, blue, and white, respectively), and silique region detection (in white).</p>
        </caption>
        <graphic xlink:href="giaa012fig12"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec7">
    <title>Discussion</title>
    <p>A recent computer vision approach to fruit number estimation involves linear regression using selected skeleton descriptors (such as junction numbers and number of triple points) extracted after segmentation and 2D skeletonization, resulting in a validation correlation of <italic>R</italic><sup>2</sup> = 0.91 between observed and automated values for the best-performing model on the 100 examples [<xref rid="bib2" ref-type="bibr">2</xref>] in a development dataset. When applied on the dataset from a separate experiment, although the model can qualitatively capture the main phenotype under investigation, its accuracy against the manual counts decreased significantly to <italic>R</italic><sup>2</sup> of 0.7. This suggests that this regression approach to fruit counting might not be generalized to other conditions as effectively as our object recognition approach. Apparently this non-deep learning approach used only “handcrafted” global features, with resulting models more specific to the conditions for training, whereas our approach used both local features (for patch classification) and some more global features (for object reconstruction).</p>
    <p>On the basis of our test results on silique counting, we expect our method to be useful for species with similar fruit morphology such as canola (oilseed rape) and other brassicas. However, the CNN will most likely need to be fine-tuned for diverse silique morphology and imaging conditions.</p>
    <p>There are several promising directions for future work for which the developed software can be improved such as the detection of other traits like silique length or branch number. These 2 traits have been reported to be a good proxy of seed number and therefore could be important for estimating productivity [<xref rid="bib46" ref-type="bibr">46</xref>]. The following considerations should be taken into account in future to improve the classification and detection performance: 
<list list-type="roman-lower"><list-item><p>The robustness of the representations in both networks relied largely on the quality and quantity of the training and test data. Increased variety in the training samples (along with artificial augmentation) should provide more robust learned representations and may facilitate extension to other species.</p></list-item><list-item><p>Deep learning models can take the whole image or the patches as input. In this study, a patch-based classifier was used and the image was scanned over with a sliding window, classifying the patches. However, feeding all patches to the network was time-consuming and the designated patch overlap produces substantial redundancy. To overcome these issues, deep neural networks taking the whole image as input for object detection can be explored.</p></list-item><list-item><p>Generative adversarial networks [<xref rid="bib47" ref-type="bibr">47</xref>] have been widely used in segmentation problems on real-world [<xref rid="bib48" ref-type="bibr">48</xref>, <xref rid="bib49" ref-type="bibr">49</xref>] and medical data (see our recent application of these models on medical images [<xref rid="bib50" ref-type="bibr">50</xref>, <xref rid="bib51" ref-type="bibr">51</xref>]). To avoid the need for post-processing (which affects the performance), different types of generative adversarial networks should be investigated.</p></list-item><list-item><p>DeepPod can be used to accelerate the development of even more robust fruit recognition approaches. DeepPod can rapidly provide more annotated images because the output of the proposed DenseNet model can be used to automatically generate detailed fruit annotation suggestions. A human annotator would then focus on correcting false-negative results (by adding missed siliques) and false-positive results (or removing falsely detected ones) instead of spending so much time on marking each fruit contour individually.</p></list-item></list></p>
  </sec>
  <sec sec-type="conclusions" id="sec8">
    <title>Conclusion</title>
    <p>In summary, we have developed DeepPod, an image-based deep learning framework for fruit counting. We have demonstrated DeepPod’s effectiveness in silique detection and counting for <italic>Arabidopsis</italic>, as well as challenges due to presence of overlapping siliques and variability in fruit morphology. The pipeline developed has been shown to be cost-effective in image annotation for model development. To further improve the pipeline, more robust and scale-invariant methods will be investigated for object detection and for extraction of more morphological traits. In addition, active learning and transfer learning could be applied for more effective data annotation and machine learning modelling.</p>
  </sec>
  <sec id="sec9">
    <title>Availability of Source Code and Requirements</title>
    <p>
      <list list-type="bullet">
        <list-item>
          <p>Project name: DeepPod</p>
        </list-item>
        <list-item>
          <p>Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/AzmHmd/DeepPod.git">https://github.com/AzmHmd/DeepPod.git</ext-link></p>
        </list-item>
        <list-item>
          <p>Operating system(s): Platform independent</p>
        </list-item>
        <list-item>
          <p>Programming language: MATLAB</p>
        </list-item>
        <list-item>
          <p>Other requirements: CUDA version 8.0, CuDNN version v5.1, BLAS: atlas, Caffe version 1.0.0-rc3, DIGITS version 5.1-dev, MATLAB version 9.3 or above</p>
        </list-item>
        <list-item>
          <p>License: MIT</p>
        </list-item>
        <list-item>
          <p>The annotation toolbox (also included in DeepPod project) has been registered in SciCrunch as <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/scicrunch/resolver/SCR_017413">RRID:SCR_017413</ext-link></p>
        </list-item>
      </list>
    </p>
  </sec>
  <sec sec-type="materials" id="sec10">
    <title>Availability of Supporting Data and Materials</title>
    <p>The dataset for model development (Set-1, including 144 raw images and their annotations, and manual silique counts) and the dataset for testing (Set-2, including 2,408 raw images and their manual silique counts) are available in the Aberystwyth research data repository [<xref rid="bib53_934_1582153552855" ref-type="bibr">53</xref>]. Snapshots of our code and other supporting data are available in the <italic>GigaScience</italic> repository, GigaDB [<xref rid="bib52" ref-type="bibr">52</xref>].</p>
  </sec>
  <sec sec-type="supplementary-material" id="sec11">
    <title>Additional Files</title>
    <p><bold>Supplementary Figure S1:</bold> An illustrative example on identification of individual siliques with overlapping regions</p>
    <p><bold>Supplementary Figure S2:</bold> Comparison of predicted mean silique length with manual estimate</p>
    <p><bold>Supplementary Data S1:</bold> CSV file reporting the predicted silique count, mean, and range of silique length (in pixels) for each image in Set-2</p>
    <p><bold>Supplementary Data S2:</bold> CSV file reporting the manual measurement of individual silique length (2,359 siliques out of the 32 images randomly selected from Set-2)</p>
    <p><bold>Supplementary Data S3:</bold> CSV file for the data used to evaluate the predicted mean silique length with the manual measure for the 32 annotated images in Set-2</p>
    <supplementary-material content-type="local-data" id="sup1">
      <label>giaa012_GIGA-D-19-00251_Original_Submission</label>
      <media xlink:href="giaa012_giga-d-19-00251_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup2">
      <label>giaa012_GIGA-D-19-00251_Revision_1</label>
      <media xlink:href="giaa012_giga-d-19-00251_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup3">
      <label>giaa012_GIGA-D-19-00251_Revision_2</label>
      <media xlink:href="giaa012_giga-d-19-00251_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup4">
      <label>giaa012_GIGA-D-19-00251_Revision_3</label>
      <media xlink:href="giaa012_giga-d-19-00251_revision_3.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup5">
      <label>giaa012_Response_to_Reviewer_Comments_Original_Submission</label>
      <media xlink:href="giaa012_response_to_reviewer_comments_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup6">
      <label>giaa012_Response_to_Reviewer_Comments_Revision_1</label>
      <media xlink:href="giaa012_response_to_reviewer_comments_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup7">
      <label>giaa012_Response_to_Reviewer_Comments_Revision_2</label>
      <media xlink:href="giaa012_response_to_reviewer_comments_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup8">
      <label>giaa012_Reviewer_1_Report_Original_Submission</label>
      <caption>
        <p>Chris Armit -- 7/31/2019 Reviewed</p>
      </caption>
      <media xlink:href="giaa012_reviewer_1_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup9">
      <label>giaa012_Reviewer_1_Report_Revision_1</label>
      <caption>
        <p>Chris Armit -- 11/26/2019 Reviewed</p>
      </caption>
      <media xlink:href="giaa012_reviewer_1_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup10">
      <label>giaa012_Reviewer_2_Report_Original_Submission</label>
      <caption>
        <p>Dijun Chen -- 8/8/2019 Reviewed</p>
      </caption>
      <media xlink:href="giaa012_reviewer_2_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup11">
      <label>giaa012_Reviewer_2_Report_Revision_1</label>
      <caption>
        <p>Dijun Chen -- 11/26/2019 Reviewed</p>
      </caption>
      <media xlink:href="giaa012_reviewer_2_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup12">
      <label>giaa012_Reviewer_3_Report_Original_Submission</label>
      <caption>
        <p>Andrew French -- 8/21/2019 Reviewed</p>
      </caption>
      <media xlink:href="giaa012_reviewer_3_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup13">
      <label>giaa012_Reviewer_3_Report_Revision_1</label>
      <caption>
        <p>Andrew French -- 12/6/2019 Reviewed</p>
      </caption>
      <media xlink:href="giaa012_reviewer_3_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup14">
      <label>giaa012_Supplemental_Files</label>
      <media xlink:href="giaa012_supplemental_files.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <sec id="sec13">
    <title>Abbreviations</title>
    <p>Caffe: Convolutional Architecture for Fast Feature Embedding; CNN: convolutional neural network; GUI: graphical user interface; LSTM: long short-term memory; NPPC: National Plant Phenomic Centre; RCNN: regional CNN; SSD: Single shot multibox detector; YOLO: you only look once.</p>
  </sec>
  <sec id="sec15">
    <title>Competing Interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec id="sec16">
    <title>Authors' Contributions</title>
    <p>J.H.D., F.C., and C.L. designed the study and provided the images. G.G.-M. performed manual counting and manual annotations. A.H. developed the annotation toolbox and performed the deep learning and data analysis, testing, and evaluation tasks. A.H. and M.G. carried out the post-processing analysis. A.H. and G.G.-M. drafted the manuscript. All the authors provided comments and corrected the manuscript.</p>
  </sec>
  <sec id="sec17">
    <title>Acknowledgements</title>
    <p>The authors gratefully acknowledge Sandy Spence and Alun Jones for their support and maintenance of the GPU and the systems used for this research. We acknowledge Dr Jay Biernaskie from University of Oxford for allowing us to use a previously unpublished image dataset (Set-1 and Set-2). We also thank the team of the National Plant Phenomics Centre, mainly Lina Avila Clasen for her help in acquiring the images and manual counting; Jason Brook for running the PSI small plant platform, and Karen Askew for assisting with plant care. G.G.-M. acknowledges receipt of a AberDoc scholarship from Aberystwyth University; J.H.D., F.C., and C.L. funding from BBSRC (grants BB/CAP1730/1, BB/P013376/1, and BB/P003095/1; and J.H.D. acknowledge support from the National Science Foundation (cROP project: 1340112).</p>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="bib1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Neumann</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Friedel</surname><given-names>S</given-names></name>, <etal>et al</etal>.</person-group><article-title>Dissecting the phenotypic components of crop plant growth and drought responses based on high-throughput image analysis</article-title>. <source>Plant Cell</source>. <year>2014</year>;<volume>26</volume>(<issue>12</issue>):<fpage>4636</fpage>–<lpage>55</lpage>.<pub-id pub-id-type="pmid">25501589</pub-id></mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vasseur</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Bresson</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>G</given-names></name>, <etal>et al</etal>.</person-group><article-title>Image-based methods for phenotyping growth dynamics and fitness components in <italic>Arabidopsis thaliana</italic></article-title>. <source>Plant Methods</source>. <year>2018</year>;<volume>14</volume>(<issue>1</issue>):<fpage>63</fpage>.<pub-id pub-id-type="pmid">30065776</pub-id></mixed-citation>
    </ref>
    <ref id="bib3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Furbank</surname><given-names>RT</given-names></name>, <name name-style="western"><surname>Tester</surname><given-names>M</given-names></name></person-group><article-title>Phenomics–technologies to relieve the phenotyping bottleneck</article-title>. <source>Trends Plant Sci</source>. <year>2011</year>;<volume>16</volume>(<issue>12</issue>):<fpage>635</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">22074787</pub-id></mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pauli</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Andrade-Sanchez</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Carmo-Silva</surname><given-names>AE</given-names></name>, <etal>et al</etal>.</person-group><article-title>Field-based high-throughput plant phenotyping reveals the temporal patterns of quantitative trait loci associated with stress-responsive traits in cotton</article-title>. <source>G3 (Bethesda)</source>. <year>2016</year>;<volume>6</volume>(<issue>4</issue>):<fpage>865</fpage>–<lpage>79</lpage>.<pub-id pub-id-type="pmid">26818078</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shakoor</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Lee</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Mockler</surname><given-names>TC</given-names></name></person-group><article-title>High throughput phenotyping to accelerate crop breeding and monitoring of diseases in the field</article-title>. <source>Curr Opin Plant Biol</source>. <year>2017</year>;<volume>38</volume>:<fpage>184</fpage>–<lpage>92</lpage>.<pub-id pub-id-type="pmid">28738313</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <label>6.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Camargo</surname><given-names>AV</given-names></name>, <name name-style="western"><surname>Mott</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Gardner</surname><given-names>KA</given-names></name>, <etal>et al</etal>.</person-group><article-title>Determining phenological patterns associated with the onset of senescence in a wheat MAGIC mapping population</article-title>. <source>Front Plant Sci</source>. <year>2016</year>;<volume>7</volume>:<fpage>1540</fpage>.<pub-id pub-id-type="pmid">27822218</pub-id></mixed-citation>
    </ref>
    <ref id="bib7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liebisch</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Kirchgessner</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Schneider</surname><given-names>D</given-names></name>, <etal>et al</etal>.</person-group><article-title>Remote, aerial phenotyping of maize traits with a mobile multi-sensor approach</article-title>. <source>Plant Methods</source>. <year>2015</year>;<volume>11</volume>(<issue>1</issue>):<fpage>9</fpage>.<pub-id pub-id-type="pmid">25793008</pub-id></mixed-citation>
    </ref>
    <ref id="bib8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Ganapathysubramanian</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Singh</surname><given-names>AK</given-names></name>, <etal>et al</etal>.</person-group><article-title>Machine learning for high-throughput stress phenotyping in plants</article-title>. <source>Trends Plant Sci</source>. <year>2016</year>;<volume>21</volume>(<issue>2</issue>):<fpage>110</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">26651918</pub-id></mixed-citation>
    </ref>
    <ref id="bib9">
      <label>9.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Pape</surname><given-names>JM</given-names></name>, <name name-style="western"><surname>Klukas</surname><given-names>C</given-names></name></person-group><article-title>Utilizing machine learning approaches to improve the prediction of leaf counts and individual leaf segmentation of rosette plant images</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Tsaftaris</surname><given-names>SA</given-names></name>, <name name-style="western"><surname>Scharr</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Pridmore</surname><given-names>T</given-names></name></person-group>, eds. <source>Proceedings of the Computer Vision Problems in Plant Phenotyping (CVPPP)</source>. <publisher-name>BMVA</publisher-name>; <year>2015</year>:<fpage>3.1</fpage>–<lpage>12</lpage>.</mixed-citation>
    </ref>
    <ref id="bib10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Naik</surname><given-names>HS</given-names></name>, <name name-style="western"><surname>Zhang</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Lofquist</surname><given-names>A</given-names></name>, <etal>et al</etal>.</person-group><article-title>A real-time phenotyping framework using machine learning for plant stress severity rating in soybean</article-title>. <source>Plant Methods</source>. <year>2017</year>;<volume>13</volume>(<issue>1</issue>):<fpage>23</fpage>.<pub-id pub-id-type="pmid">28405214</pub-id></mixed-citation>
    </ref>
    <ref id="bib11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Atkinson</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Lobet</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Noll</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>Combining semi-automated image analysis techniques with machine learning algorithms to accelerate large-scale genetic studies</article-title>. <source>Gigascience</source>. <year>2017</year>;<volume>6</volume>(<issue>10</issue>), doi:<pub-id pub-id-type="doi">10.1093/gigascience/gix084</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Arinkin</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Digel</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Porst</surname><given-names>D</given-names></name>, <etal>et al</etal>.</person-group><article-title>Phenotyping date palm varieties via leaflet cross-sectional imaging and artificial neural network application</article-title>. <source>BMC Bioinformatics</source>. <year>2014</year>;<volume>15</volume>(<issue>1</issue>):<fpage>55</fpage>.<pub-id pub-id-type="pmid">24564551</pub-id></mixed-citation>
    </ref>
    <ref id="bib13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pound</surname><given-names>MP</given-names></name>, <name name-style="western"><surname>Atkinson</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Townsend</surname><given-names>AJ</given-names></name>, <etal>et al</etal>.</person-group><article-title>Deep machine learning provides state-of-the-art performance in image-based plant phenotyping</article-title>. <source>Gigascience</source>. <year>2017</year>;<volume>6</volume>(<issue>10</issue>), doi:<pub-id pub-id-type="doi">10.1093/gigascience/gix083</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mohanty</surname><given-names>SP</given-names></name>, <name name-style="western"><surname>Hughes</surname><given-names>DP</given-names></name>, <name name-style="western"><surname>Salathé</surname><given-names>M</given-names></name></person-group><article-title>Using deep learning for image-based plant disease detection</article-title>. <source>Front Plant Sci</source>. <year>2016</year>;<volume>7</volume>:<fpage>1419</fpage>.<pub-id pub-id-type="pmid">27713752</pub-id></mixed-citation>
    </ref>
    <ref id="bib15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ubbens</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Cieslak</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Prusinkiewicz</surname><given-names>P</given-names></name>, <etal>et al</etal>.</person-group><article-title>The use of plant models in deep learning: an application to leaf counting in rosette plants</article-title>. <source>Plant Methods</source>. <year>2018</year>;<volume>14</volume>(<issue>1</issue>):<fpage>6</fpage>.<pub-id pub-id-type="pmid">29375647</pub-id></mixed-citation>
    </ref>
    <ref id="bib16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Namin</surname><given-names>ST</given-names></name>, <name name-style="western"><surname>Esmaeilzadeh</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Najafi</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>Deep phenotyping: deep learning for temporal phenotype/genotype classification</article-title>. <source>Plant Methods</source>. <year>2018</year>;<volume>14</volume>(<issue>1</issue>):<fpage>66</fpage>.<pub-id pub-id-type="pmid">30087695</pub-id></mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ubbens</surname><given-names>JR</given-names></name>, <name name-style="western"><surname>Stavness</surname><given-names>I</given-names></name></person-group><article-title>Deep plant phenomics: a deep learning platform for complex plant phenotyping tasks</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>1190</fpage>.<pub-id pub-id-type="pmid">28736569</pub-id></mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pawara</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Okafor</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Surinta</surname><given-names>O</given-names></name>, <etal>et al</etal>.</person-group><article-title>Comparing local descriptors and bags of visual words to deep convolutional neural networks for plant recognition</article-title>. In: <source>Proceedings of the 6th International Conference on Pattern Recognition Applications and Methods (ICPRAM)</source>. <year>2017</year>:<fpage>479</fpage>–<lpage>86</lpage>.</mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fuentes</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Yoon</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>SC</given-names></name>, <etal>et al</etal>.</person-group><article-title>A robust deep-learning-based detector for real-time tomato plant diseases and pests recognition</article-title>. <source>Sensors</source>. <year>2017</year>;<volume>17</volume>(<issue>9</issue>):<fpage>2022</fpage>.</mixed-citation>
    </ref>
    <ref id="bib20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Sun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>J</given-names></name></person-group><article-title>Automatic image-based plant disease severity estimation using deep learning</article-title>. <source>Comput Intell Neurosci</source>. <year>2017</year>;<volume>2017</volume>:<fpage>2917536</fpage>.<pub-id pub-id-type="pmid">28757863</pub-id></mixed-citation>
    </ref>
    <ref id="bib21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ramcharan</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Baranowski</surname><given-names>K</given-names></name>, <name name-style="western"><surname>McCloskey</surname><given-names>P</given-names></name>, <etal>et al</etal>.</person-group><article-title>Transfer learning for image-based cassava disease detection</article-title>. <source>Front Plant Sci</source>. <year>2017</year>;<volume>8</volume>:<fpage>1852</fpage>.<pub-id pub-id-type="pmid">29163582</pub-id></mixed-citation>
    </ref>
    <ref id="bib22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mitchell-Olds</surname><given-names>T</given-names></name></person-group><article-title><italic>Arabidopsis thalian a</italic> and its wild relatives: a model system for ecology and evolution</article-title>. <source>Trends Ecol Evol</source>. <year>2001</year>;<volume>16</volume>(<issue>12</issue>):<fpage>693</fpage>–<lpage>700</lpage>.</mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koornneef</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Meinke</surname><given-names>D</given-names></name></person-group><article-title>The development of <italic>Arabidopsis</italic> as a model plant</article-title>. <source>Plant J</source>. <year>2010</year>;<volume>61</volume>(<issue>6</issue>):<fpage>909</fpage>–<lpage>21</lpage>.<pub-id pub-id-type="pmid">20409266</pub-id></mixed-citation>
    </ref>
    <ref id="bib24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krämer</surname><given-names>U</given-names></name></person-group><article-title>Planting molecular functions in an ecological context with <italic>Arabidopsis thaliana</italic></article-title>. <source>Elife</source>. <year>2015</year>, doi:<pub-id pub-id-type="doi">10.7554/eLife.06100</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Reboud</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Le Corre</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Scarcelli</surname><given-names>N</given-names></name>, <etal>et al</etal>.</person-group><article-title>Natural variation among accessions of <italic>Arabidopsis thaliana</italic>: beyond the flowering date, what morphological traits are relevant to study adaptation</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Cronk</surname><given-names>QCB</given-names></name>, <name name-style="western"><surname>Whitton</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ree</surname><given-names>RH</given-names></name><etal>et al</etal>., <etal>et al</etal>.</person-group><source>Plant Adaptation: Molecular Genetics and Ecology</source>. <publisher-loc>Ottawa, ONT, Canada</publisher-loc>: <publisher-name>NRC Research Press</publisher-name>; <year>2004</year>:<fpage>135</fpage>–<lpage>42</lpage>.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bac-Molenaar</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Vreugdenhil</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Granier</surname><given-names>C</given-names></name>, <etal>et al</etal>.</person-group><article-title>Genome-wide association mapping of growth dynamics detects time-specific and general quantitative trait loci</article-title>. <source>J Exp Bot</source>. <year>2015</year>;<volume>66</volume>(<issue>18</issue>):<fpage>5567</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">25922493</pub-id></mixed-citation>
    </ref>
    <ref id="bib27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bac-Molenaar</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Granier</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Keurentjes</surname><given-names>JJ</given-names></name>, <etal>et al</etal>.</person-group><article-title>Genome-wide association mapping of time-dependent growth responses to moderate drought stress in <italic>Arabidopsis</italic></article-title>. <source>Plant Cell &amp; Environ</source>. <year>2016</year>;<volume>39</volume>(<issue>1</issue>):<fpage>88</fpage>–<lpage>102</lpage>.</mixed-citation>
    </ref>
    <ref id="bib28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Minervini</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Abdelsamea</surname><given-names>MM</given-names></name>, <name name-style="western"><surname>Tsaftaris</surname><given-names>SA</given-names></name></person-group><article-title>Image-based plant phenotyping with incremental learning and active contours</article-title>. <source>Ecol Informatics</source>. <year>2014</year>;<volume>23</volume>:<fpage>35</fpage>–<lpage>48</lpage>.</mixed-citation>
    </ref>
    <ref id="bib29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Augustin</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Haxhimusa</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Busch</surname><given-names>W</given-names></name>, <etal>et al</etal>.</person-group><article-title>A framework for the extraction of quantitative traits from 2D images of mature <italic>Arabidopsis thaliana</italic></article-title>. <source>Mach Vis Appl</source>. <year>2016</year>;<volume>27</volume>(<issue>5</issue>):<fpage>647</fpage>–<lpage>61</lpage>.</mixed-citation>
    </ref>
    <ref id="bib30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bush</surname><given-names>MS</given-names></name>, <name name-style="western"><surname>Crowe</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Zheng</surname><given-names>T</given-names></name>, <etal>et al</etal>.</person-group><article-title>The RNA helicase, eIF 4A-1, is required for ovule development and cell size homeostasis in <italic>Arabidopsis</italic></article-title>. <source>Plant J</source>. <year>2015</year>;<volume>84</volume>(<issue>5</issue>):<fpage>989</fpage>–<lpage>1004</lpage>.<pub-id pub-id-type="pmid">26493293</pub-id></mixed-citation>
    </ref>
    <ref id="bib31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Nibau</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Phillips</surname><given-names>DW</given-names></name>, <etal>et al</etal>.</person-group><article-title>CDKG1 protein kinase is essential for synapsis and male meiosis at high ambient temperature in <italic>Arabidopsis thaliana</italic></article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2014</year>;<volume>111</volume>(<issue>6</issue>):<fpage>2182</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">24469829</pub-id></mixed-citation>
    </ref>
    <ref id="bib32">
      <label>32.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Kavukcuoglu</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Farabet</surname><given-names>C</given-names></name></person-group><article-title>Convolutional networks and applications in vision</article-title>. In: <source>Proceedings of IEEE International Symposium on Circuits and Systems (ISCAS)</source>. <year>2010</year>:<fpage>253</fpage>–<lpage>56</lpage>.</mixed-citation>
    </ref>
    <ref id="bib33">
      <label>33.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Simonyan</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Zisserman</surname><given-names>A</given-names></name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title>. <source>arXiv</source>. <year>2014</year>:<fpage>1409.1556</fpage>.</mixed-citation>
    </ref>
    <ref id="bib34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Learning multiple layers of features from tiny images</article-title>. <comment>Technical Report, University of Toronto</comment><year>2009</year>.</mixed-citation>
    </ref>
    <ref id="bib35">
      <label>35.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Krizhevsky</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sutskever</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title>. In: <source>Advances in Neural Information Processing Systems</source>. <year>2012</year>:<fpage>1097</fpage>–<lpage>105</lpage>.</mixed-citation>
    </ref>
    <ref id="bib36">
      <label>36.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Szegedy</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Jia</surname><given-names>Y</given-names></name>, <etal>et al</etal>.</person-group><article-title>Going deeper with convolutions</article-title>. In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2015</year>:<fpage>1</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="bib37">
      <label>37.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Redmon</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Divvala</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Girshick</surname><given-names>R</given-names></name>, <etal>et al</etal>.</person-group><article-title>You only look once: Unified, real-time object detection</article-title>. In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>. <year>2016</year>:<fpage>779</fpage>–<lpage>88</lpage>.</mixed-citation>
    </ref>
    <ref id="bib38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kover</surname><given-names>PX</given-names></name>, <name name-style="western"><surname>Valdar</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Trakalo</surname><given-names>J</given-names></name>, <etal>et al</etal>.</person-group><article-title>A multiparent advanced generation inter-cross to fine-map quantitative traits in <italic>Arabidopsis thaliana</italic></article-title>. <source>PLoS Genet</source>. <year>2009</year>;<volume>5</volume>(<issue>7</issue>):<fpage>e1000551</fpage>.<pub-id pub-id-type="pmid">19593375</pub-id></mixed-citation>
    </ref>
    <ref id="bib39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abràmoff</surname><given-names>MD</given-names></name>, <name name-style="western"><surname>Magalhães</surname><given-names>PJ</given-names></name>, <name name-style="western"><surname>Ram</surname><given-names>SJ</given-names></name></person-group><article-title>Image processing with ImageJ</article-title>. <source>Biophotonics Int</source>. <year>2004</year>;<volume>11</volume>(<issue>7</issue>):<fpage>36</fpage>–<lpage>42</lpage>.</mixed-citation>
    </ref>
    <ref id="bib40">
      <label>40.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Hardt</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>Understanding deep learning requires rethinking generalization</article-title>. <source>arXiv</source>. <year>2016</year>:<fpage>1611.03530</fpage>.</mixed-citation>
    </ref>
    <ref id="bib41">
      <label>41.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hamidinekoo</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Suhail</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Qaiser</surname><given-names>T</given-names></name>, <etal>et al</etal>.</person-group><article-title>Investigating the effect of various augmentations on the input data fed to a convolutional neural network for the task of mammographic mass classification</article-title>. In: <source>Annual Conference on Medical Image Understanding and Analysis</source>. <publisher-name>Springer</publisher-name>; <year>2017</year>:<fpage>398</fpage>–<lpage>409</lpage>.</mixed-citation>
    </ref>
    <ref id="bib42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="bib43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Bottou</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name>, <etal>et al</etal>.</person-group><article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proc IEEE</source>. <year>1998</year>;<volume>86</volume>(<issue>11</issue>):<fpage>2278</fpage>–<lpage>324</lpage>.</mixed-citation>
    </ref>
    <ref id="bib44">
      <label>44.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Liu</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>van der Maaten</surname><given-names>L</given-names></name>, <etal>et al</etal>.</person-group><article-title>Densely connected convolutional networks</article-title>. In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI</source>. <year>2017</year>:<fpage>2261</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="bib45">
      <label>45.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jia</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Shelhamer</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Donahue</surname><given-names>J</given-names></name>, <etal>et al</etal>.</person-group><article-title>Caffe: Convolutional Architecture for Fast Feature Embedding</article-title>. In: <source>Proceedings of the 22nd ACM International Conference on Multimedia</source>. <publisher-name>ACM</publisher-name>; <year>2014</year>:<fpage>675</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="bib46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bac-Molenaar</surname><given-names>JA</given-names></name>, <name name-style="western"><surname>Fradin</surname><given-names>EF</given-names></name>, <name name-style="western"><surname>Becker</surname><given-names>FF</given-names></name>, <etal>et al</etal>.</person-group><article-title>Genome-wide association mapping of fertility reduction upon heat stress reveals developmental stage-specific QTLs in <italic>Arabidopsis thaliana</italic></article-title>. <source>Plant Cell</source>. <year>2015</year>;<volume>27</volume>(<issue>7</issue>):<fpage>1857</fpage>–<lpage>74</lpage>.<pub-id pub-id-type="pmid">26163573</pub-id></mixed-citation>
    </ref>
    <ref id="bib47">
      <label>47.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Goodfellow</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Pouget-Abadie</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Mirza</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>Generative adversarial nets</article-title>. In: <source>Advances in Neural Information Processing Systems</source>. <year>2014</year>:<fpage>2672</fpage>–<lpage>80</lpage>.</mixed-citation>
    </ref>
    <ref id="bib48">
      <label>48.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Isola</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Zhu</surname><given-names>JY</given-names></name>, <name name-style="western"><surname>Zhou</surname><given-names>T</given-names></name>, <etal>et al</etal>.</person-group><article-title>Image-to-image translation with conditional adversarial networks</article-title>. In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognitiion, Honolulu, HI</source>. <year>2017</year>:<fpage>1125</fpage>–<lpage>34</lpage>.</mixed-citation>
    </ref>
    <ref id="bib49">
      <label>49.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Luc</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Couprie</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Chintala</surname><given-names>S</given-names></name>, <etal>et al</etal>.</person-group><article-title>Semantic segmentation using adversarial networks</article-title>. <source>arXiv</source>. <year>2016</year>:<fpage>1611.08408</fpage>.</mixed-citation>
    </ref>
    <ref id="bib50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hamidinekoo</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Denton</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Rampun</surname><given-names>A</given-names></name>, <etal>et al</etal>.</person-group><article-title>Deep learning in mammography and breast histology, an overview and future trends</article-title>. <source>Med Image Anal</source>. <year>2018</year>;<volume>47</volume>:<fpage>45</fpage>–<lpage>67</lpage>.<pub-id pub-id-type="pmid">29679847</pub-id></mixed-citation>
    </ref>
    <ref id="bib51">
      <label>51.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Grall</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hamidinekoo</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Malcolm</surname><given-names>P</given-names></name>, <etal>et al</etal>.</person-group><article-title>Using a conditional generative adversarial network (cGAN) for prostate segmentation</article-title>. <person-group person-group-type="editor"><name name-style="western"><surname>Zheng</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Williams</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Chen</surname><given-names>K</given-names></name></person-group>, eds. <source>23rd Conference on Medical Image Understanding and Analysis</source>. <publisher-loc>Cham</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2019</year>, doi:<pub-id pub-id-type="doi">10.1007/978-3-030-39343-4_2</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hamidinekoo</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Garzón-Martínez</surname><given-names>GA</given-names></name>, <name name-style="western"><surname>Ghahremani</surname><given-names>M</given-names></name>, <etal>et al</etal>.</person-group><article-title>Supporting data for “DeepPod: a convolutional neural network based quantification of fruit number in <italic>Arabidopsis</italic></article-title>.” <source>GigaScience Database</source>. <year>2020</year>
<pub-id pub-id-type="doi">10.5524/100704</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib53_934_1582153552855">
      <label>53</label>
      <mixed-citation publication-type="journal"><comment>Hamidinekoo A, Garzón-Martínez GA, Ghahremani M, et al. Supporting data for “DeepPod: a convolutional neural network based quantification of fruit number in <italic>Arabidopsis</italic>.” Aberystwyth Research Portal Dataset 2020, DOI:10.20391/21154739-f718-457b-96ff-838408f2b696</comment>.</mixed-citation>
    </ref>
  </ref-list>
</back>
