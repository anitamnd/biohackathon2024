<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Gigascience</journal-id>
    <journal-id journal-id-type="iso-abbrev">Gigascience</journal-id>
    <journal-id journal-id-type="publisher-id">gigascience</journal-id>
    <journal-title-group>
      <journal-title>GigaScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2047-217X</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7786169</article-id>
    <article-id pub-id-type="pmid">33404053</article-id>
    <article-id pub-id-type="doi">10.1093/gigascience/giaa152</article-id>
    <article-id pub-id-type="publisher-id">giaa152</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Technical Note</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
        <subject>AcademicSubjects/SCI02254</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Tool recommender system in Galaxy using deep learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2068-4695</contrib-id>
        <name>
          <surname>Kumar</surname>
          <given-names>Anup</given-names>
        </name>
        <!--<email>kumara@informatik.uni-freiburg.de</email>-->
        <aff><institution>Bioinformatics Group, Department of Computer Science, University of Freiburg</institution>, Georges-Koehler-Allee 106, 79110 Freiburg, <country country="DE">Germany</country></aff>
        <xref ref-type="author-notes" rid="afn1"/>
        <xref ref-type="corresp" rid="cor1"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9760-8992</contrib-id>
        <name>
          <surname>Rasche</surname>
          <given-names>Helena</given-names>
        </name>
        <aff><institution>Bioinformatics Group, Department of Computer Science, University of Freiburg</institution>, Georges-Koehler-Allee 106, 79110 Freiburg, <country country="DE">Germany</country></aff>
        <xref ref-type="author-notes" rid="afn1"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-3079-6586</contrib-id>
        <name>
          <surname>Grüning</surname>
          <given-names>Björn</given-names>
        </name>
        <aff><institution>Bioinformatics Group, Department of Computer Science, University of Freiburg</institution>, Georges-Koehler-Allee 106, 79110 Freiburg, <country country="DE">Germany</country></aff>
        <xref ref-type="author-notes" rid="afn1"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8231-3323</contrib-id>
        <name>
          <surname>Backofen</surname>
          <given-names>Rolf</given-names>
        </name>
        <aff><institution>Bioinformatics Group, Department of Computer Science, University of Freiburg</institution>, Georges-Koehler-Allee 106, 79110 Freiburg, <country country="DE">Germany</country></aff>
        <aff><institution>Signalling Research Centres BIOSS and CIBSS, University of Freiburg</institution>, Schaenzlestr. 18, 79104 Freiburg, <country country="DE">Germany</country></aff>
        <xref ref-type="author-notes" rid="afn1"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">Correspondence address. Anup Kumar, Bioinformatics Group, Department of Computer Science, University of Freiburg, Georges-Koehler-Allee 106, 79110 Freiburg, Germany. E-mail: <email>kumara@informatik.uni-freiburg.de</email>.</corresp>
      <fn id="afn1">
        <p>Contributions follow author name order.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub" iso-8601-date="2021-01-06">
      <day>06</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>06</day>
      <month>1</month>
      <year>2021</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>10</volume>
    <issue>1</issue>
    <elocation-id>giaa152</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>2</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>02</day>
        <month>8</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>11</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press GigaScience.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="giaa152.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="abs1">
        <title>Background</title>
        <p>Galaxy is a web-based and open-source scientific data-processing platform. Researchers compose pipelines in Galaxy to analyse scientific data. These pipelines, also known as workflows, can be complex and difficult to create from thousands of tools, especially for researchers new to Galaxy. To help researchers with creating workflows, a system is developed to recommend tools that can facilitate further data analysis.</p>
      </sec>
      <sec id="abs2">
        <title>Findings</title>
        <p>A model is developed to recommend tools using a deep learning approach by analysing workflows composed by researchers on the European Galaxy server. The higher-order dependencies in workflows, represented as directed acyclic graphs, are learned by training a gated recurrent units neural network, a variant of a recurrent neural network. In the neural network training, the weights of tools used are derived from their usage frequencies over time and the sequences of tools are uniformly sampled from training data. Hyperparameters of the neural network are optimized using Bayesian optimization. Mean accuracy of 98% in recommending tools is achieved for the top-1 metric.</p>
      </sec>
      <sec id="abs3">
        <title>Conclusions</title>
        <p>The model is accessed by a Galaxy API to provide researchers with recommended tools in an interactive manner using multiple user interface integrations on the European Galaxy server. High-quality and highly used tools are shown at the top of the recommendations. The scripts and data to create the recommendation system are available under MIT license at <ext-link ext-link-type="uri" xlink:href="https://github.com/anuprulez/galaxy_tool_recommendation">https://github.com/anuprulez/galaxy_tool_recommendation</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="keywords">
      <kwd>recommender system</kwd>
      <kwd>Galaxy</kwd>
      <kwd>workflows</kwd>
      <kwd>deep learning</kwd>
      <kwd>neural networks</kwd>
      <kwd>gated recurrent units</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Deutsche Forschungsgemeinschaft</institution>
            <institution-id institution-id-type="DOI">10.13039/501100001659</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>390939984</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Albert-Ludwigs-Universität Freiburg</institution>
            <institution-id institution-id-type="DOI">10.13039/501100002714</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="12"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <title>Background</title>
    <p>Life sciences depend increasingly on high-throughput data, turning them into data science to a large extent. However, raw high-throughput data have little value on their own without proper analysis and interpretation. To simplify the data analysis process and to ensure a reproducible analysis, several workflow systems such as Bcbio-nextgen, Omics Pipe, Nextflow, Luigi, Toil, and many others have emerged [<xref rid="bib1" ref-type="bibr">1–3</xref>]. The main idea for workflow systems is based on the observation that any computational analysis of high-throughput data encompasses multiple steps such as quality control, preprocessing, quantification, and statistical analysis to transform raw data into scientific results. Collectively, these steps form a workflow where each step performs a definite transformation of the data, which can be performed using standardized tools. Using workflow for the analysis is simple and convenient and has several advantages. First, it is easy to replace individual tools by a newer version or to assess the influence of the associated step on the final result. Second, a workflow can be saved, shared, and reused, which ensures reproducible research. Therefore, workflows are becoming essential in the analysis of scientific data and there are multiple platforms where researchers can create workflows for their analyses. However, a critical question is how to assess whether a generated workflow is state-of-the-art or even valid at all. To give a concrete example, one can use several real-valued input vectors (such as fluorescence-based measurement stemming from arrays), transform them into integer-based values in the first step, and combine it with a tool that uses a count-based statistics (such as negative binomial distribution as used in DESeq2 [<xref rid="bib4" ref-type="bibr">4</xref>]) to determine values that show high differential behaviour. While this workflow would run on a workflow system without problems and even produce some results, the generated results are not valid because the wrong statistical model was applied. Therefore, it is important to use a tool for each step in a workflow that can bring desired results. To make this possible, a system is needed that can recommend useful tools at each step while creating a workflow.</p>
    <sec id="sec1-2">
      <title>Galaxy and workflows</title>
      <p>Galaxy is an open-source data-processing platform that enables researchers to create and store their workflows for multiple scientific analyses [<xref rid="bib5" ref-type="bibr">5</xref>]. A workflow in Galaxy is a directed acyclic graph and consists of 1 or many tool sequences to analyse scientific data such as DNA and RNA sequences. A tool consumes ≥1 data file as input, produces ≥1 data file as output, and supports a number of formats of these input and output files. In workflows, the tools are connected one after another following a constraint that the adjacent tools must have compatible data types. In other words, the data types of output files of a tool should match the data types of input files of the following tool. Galaxy has thousands of accessible tools, and acquiring familiarity and constructing workflows with these tools can be a complex and time-consuming task, especially for researchers new to Galaxy. To assist them in creating workflows and making them aware of the possible tools for further analyses, a recommender system is devised. The benefits of such a system are manifold. First, it will make researchers more efficient by saving the time they waste creating erroneous or less optimal workflows by choosing tools that may produce undesired results. Second, it will help researchers bypass the step of searching for tools separately, which will further reduce the time spent in creating workflows and at the same time increase the accessibility of tools. Third, it will promote high-quality tools that have been used more often in the past (last 1 year) to the top of the recommendations and downgrade those having lower usage frequencies. This is achieved by assigning each tool a weight derived from its usage frequency over a period of time. Finally, it can be extended to promote the newly added tools in Galaxy by showing them alongside the recommended tools predicted using the neural network approach.</p>
    </sec>
    <sec id="sec1-3">
      <title>Recommender systems</title>
      <p>The objective of having recommender systems in fields such as scientific literature search, online shopping, travel bookings, media-service providers, and many other fields is to help people discover suitable, interesting, and newly released products. These recommended products are recognized on the basis of the usage and purchasing patterns of people in the past. In the field of scientific literature search, the exponential increase in the number of published articles necessitates having a recommender system to help scientists explore relevant and recent papers quickly [<xref rid="bib6" ref-type="bibr">6–8</xref>]. Recommender systems are important in the world of commercial applications too. Companies such as Amazon and Netflix have appropriately used them to learn the preferences of their respective customers in selecting products such as their favourite books or movies and to propose a few products out of a large catalogue. By enabling users and customers to discover reasonable and customized products, recommender systems have helped them grow as organizations [<xref rid="bib9" ref-type="bibr">9</xref>, <xref rid="bib10" ref-type="bibr">10</xref>]. In short, recommender systems make it faster for users and customers to look through a few suggested products to find the most suitable ones. These successful implementations of recommender systems by organizations across the world working in diverse areas to assess the needs of their respective users in proposing relevant products motivated us to create a tool recommender system in Galaxy.</p>
    </sec>
    <sec id="sec1-4">
      <title>Related work</title>
      <p>To simplify creating workflows for scientific analyses, a few approaches have been proposed that suggest alternative tools and workflows. EDAM and semantic annotations of tools are used to compose workflows automatically for mass spectrometry–based proteomics [<xref rid="bib11" ref-type="bibr">11</xref>]. The annotations include the names, functionalities, and input and output data types of tools. The PROPHETS program generates suitable candidates of workflows that match the goal of the proposed workflow and its annotations [<xref rid="bib12" ref-type="bibr">12</xref>]. WINGS offers multiple variations of a workflow created using different tools. It makes use of the input parameters, types of datasets, and functions of tools to build the variations [<xref rid="bib13" ref-type="bibr">13</xref>, <xref rid="bib14" ref-type="bibr">14</xref>]. The approach used by DiBernardo et al. [<xref rid="bib15" ref-type="bibr">15</xref>] uses data types to facilitate the automatic creation of workflows. All these approaches depend on either annotations or matching input and output data types of adjacent tools in workflows, and they pose challenges such as the addition and maintenance of the meaningful annotations of tools and extracting input and output data types of adjacent tools. Moreover, these approaches have their workflow generation restricted to a few specific bioinformatics analyses such as proteomics or proteogenomics. In addition, they do not discuss the presence of higher-order relationships [<xref rid="bib16" ref-type="bibr">16</xref>] in tool sequences of workflows. Our approach to recommend tools in workflows aims to overcome these challenges in the following manner. First, it does not require collecting and storing information about tools. Second, it takes into account the higher-order relationships among tools (Fig. <xref ref-type="fig" rid="fig1">1</xref>) in tool sequences. Finally, it incorporates workflows from multiple scientific analyses to produce the recommender system.</p>
      <fig id="fig1" orientation="portrait" position="float">
        <label>Figure 1:</label>
        <caption>
          <p>An example workflow consisting of 5 different tools (a) is decomposed into multiple tool sequences (b–d). Each tool sequence shows higher-order dependencies where a tool is dependent on all of its prior tools. These dependencies are indicated by the dashed arrows.</p>
        </caption>
        <graphic xlink:href="giaa152fig1"/>
      </fig>
    </sec>
    <sec id="sec1-5">
      <title>Sequential learning on workflows</title>
      <p>Workflows, created by many researchers in Galaxy for different scientific analyses, are decomposed into numerous tool sequences (Fig. <xref ref-type="fig" rid="fig1">1</xref>). The sequential nature of these tool sequences where tools are connected one after another inspires us to apply similar learning techniques used for other sequential data such as text and speech. There are multiple studies in the fields of natural language processing, clinical research, and speech recognition that apply deep learning techniques on sequential data to obtain good accuracy in predicting future items. The approach used by Yin et al. [<xref rid="bib17" ref-type="bibr">17</xref>] finds context in long sequences of words for sentiment analysis and part-of-speech tagging using recurrent neural network (RNN) and achieves 85% and 93% accuracy, respectively. For clinical data, learning on long sequences of health states proves to be beneficial [<xref rid="bib18" ref-type="bibr">18</xref>]. The health states of patients recorded at different time points are analysed by accessing their electronic health records. The future health states of patients could be predicted by training RNN on the sequences of their past health states to achieve 85% accuracy. Moreover, variants of RNN are used to model speech and music signals [<xref rid="bib19" ref-type="bibr">19</xref>, <xref rid="bib20" ref-type="bibr">20</xref>]. These successful studies benefit from sequential learning techniques using different variants of RNN. Therefore, in our work, a variant of RNN—gated recurrent units (GRU)—is used to create the tool recommender system in Galaxy.</p>
      <p>A Bayesian network can also be used for modelling directed acyclic graphs (workflows) [<xref rid="bib21" ref-type="bibr">21</xref>, <xref rid="bib22" ref-type="bibr">22</xref>]. It requires the computation of joint and conditional probabilities of nodes in graphs, and an increase in the number of nodes can lead to a higher cost to compute these probabilities. In addition, making predictions by learning a probabilistic network is a hard problem [<xref rid="bib23" ref-type="bibr">23–25</xref>]. Because of these drawbacks of using a Bayesian network, it is not used in our approach to create the recommender system in Galaxy.</p>
    </sec>
  </sec>
  <sec id="h1content1609389354271">
    <title>Data Description</title>
    <p>More than 18,000 workflows from different scientific analyses such as RNA-seq, variant-calling, Hi-C, assembly, single-cell, proteomics, and so on in the European Galaxy server [<xref rid="bib26" ref-type="bibr">26</xref>] have been used to create the recommender system. A workflow consisting of 5 tools is shown in Fig. <xref ref-type="fig" rid="fig1">1a</xref>. It is divided into smaller tool sequences as shown in Fig. <xref ref-type="fig" rid="fig1">1b</xref>–<xref ref-type="fig" rid="fig1">d</xref>. The last tool, shown in green, of each tool sequence (of length n) is assigned as the label of the subsequence (of length n − 1) shown in blue in Fig. <xref ref-type="fig" rid="fig1">1</xref>. A label is an output that is learned and predicted by the recommender system. In the neural network learning, a tool is a label. For example, in Fig. <xref ref-type="fig" rid="fig1">1b</xref>, Tools D and E are the labels of the subsequence Tool A → Tool B → Tool C. They show higher-order dependencies in their connections, which implies that a tool is dependent not only on its immediate predecessor but also on all prior tools in the tool sequence. For example, in Fig. <xref ref-type="fig" rid="fig1">1c</xref>, the Tool C is dependent on Tools B and A. By analysing multiple workflow fragments in this way, the neural network learns that the label of a tool sequence Tool A → Tool B is Tool C. It is expected that dividing a tool sequence into fragments with a minimum length of 2 tools, as shown in Fig. <xref ref-type="fig" rid="fig1">1c</xref> and d, will improve the generalization performance of the neural network because it gets more tool sequences with a variety of lengths to learn from. The dependencies shown in Fig. <xref ref-type="fig" rid="fig1">1b</xref>–<xref ref-type="fig" rid="fig1">d</xref> present in tool sequences are learned using the GRU neural network by modelling the conditional probability given by Equation <xref ref-type="disp-formula" rid="equ1">1</xref> [<xref rid="bib27" ref-type="bibr">27</xref>]. Using the approach explained above, &gt;229,000 tool sequences are extracted.</p>
  </sec>
  <sec id="h1content1609389403316">
    <title>Usage pattern of tools</title>
    <p>Tools in Galaxy have different usage patterns. Some tools are used more often than other tools for multiple reasons such as differences in their functions and availability of similar but better tools. It is essential to analyse the usage patterns of tools because the recommender system proposes tools for researchers and these tools should have high relevance to their analyses. One of the key indicators of the relevance of tools can be their high usage frequencies. If a tool has been used often in the recent past, it implies that the tool is relevant. However, if a tool was used often a few years ago but has been used less often in the past 6 months, then the relevance of that tool has certainly declined. The usage frequencies of tools (shown as labels in Fig. <xref ref-type="fig" rid="fig1">1</xref>) over the past year are shown in Fig. <xref ref-type="fig" rid="fig2">2</xref>.</p>
    <fig id="fig2" orientation="portrait" position="float">
      <label>Figure 2:</label>
      <caption>
        <p>The usage frequencies of 4 tools collected over the past 1 year. Tools B and D have high usage frequencies almost every month, while Tools C and E have much lower usage frequencies. Tool A is absent from the plot because it is not the label of any tool for the workflow shown in Fig. <xref ref-type="fig" rid="fig1">1</xref>.</p>
      </caption>
      <graphic xlink:href="giaa152fig2"/>
    </fig>
    <sec id="h2content1608219671431">
      <title>Shared and non-shared workflows</title>
      <p>The set of workflows may have low-quality workflows that have not been published or are deleted or may have errors in their tool connections. To distinguish between high- and low-quality tool connections, the labels for each tool sequence are divided into 2 categories—shared and non-shared labels. The shared labels come from the published, non-deleted, and non-erroneous workflows while the non-published labels come from other workflows. While recommending tools for a tool or tool sequence, the shared labels are promoted to the top of the recommendations if available, followed by the non-shared labels. This enables high-quality tools to be shown as the top recommendations.</p>
    </sec>
    <sec id="h2content1608219681406">
      <title>Imbalance in workflows</title>
      <p>Tool sequences from these workflows may vary in number—some may occur more frequently and others may not. Therefore, the complete set of tool sequences may not be equally representative of all workflows coming from different scientific analyses. Learning on the imbalanced set of tool sequences can induce bias, which may have an undesired outcome—high accuracy in recommending tools coming from highly frequent tool sequences and low accuracy for tools coming from less frequent ones. To mitigate this bias, all tool sequences are chosen with uniform frequency while training the neural network, which allows it to attain comparable accuracy in recommending tools in different scientific analyses. This uniform sampling strategy is discussed in the “Implementation” section in detail.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="h1content1608219684942">
    <title>Results</title>
    <p>Three different neural network architectures—dense neural network (DNN), convolutional neural network (CNN), and gated recurrent units neural network (GRU)—are compared on their performances in predicting tools (Figs <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig4">4</xref>). The models obtained after training all the neural network architectures are used to predict tools for the tool sequences in the test data after every training iteration. Top-k precision (precision@k) is a popular metric for evaluating a recommender system [<xref rid="bib28" ref-type="bibr">28–30</xref>]. Precision@k implies how many of the <italic>k</italic> predicted tools are correct. The correctness here refers to the compatibility of the predicted tools with the tool for which predictions have been made. For example, <italic>k</italic> = 2 implies that there are 2 predicted tools with the highest predicted scores. If only 1 of them is correct, then the precision@2 is <inline-formula><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$1/2 = 0.5$\end{document}</tex-math></inline-formula>. In this way, precision@1 and precision@2 are computed for all the tool sequences in the test data and then averaged. Precision@1 and precision@2 metrics are used in this approach to evaluate the quality of the tool recommender system. The precisions of recommended tools are computed separately for the non-shared and shared recommendations and shown in different plots (Figs <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig4">4</xref>), but for usage frequencies, they are combined into 1 plot (Fig. <xref ref-type="fig" rid="fig5">5</xref>). The precision and usage frequencies of the predicted tools for the precision@1 and precision@2 (top-1 and top-2) metrics are computed over 10 training iterations for each experiment run. They are averaged and their respective standard deviations are computed over 10 experiment runs. Mean precision and usage frequencies are shown by the respective line plots, and the shaded regions span the area between 1 standard deviation above and below the mean (Figs <xref ref-type="fig" rid="fig3">3</xref>–<xref ref-type="fig" rid="fig5">5</xref>).</p>
    <fig id="fig3" orientation="portrait" position="float">
      <label>Figure 3:</label>
      <caption>
        <p>Top-k (precision@k) non-shared precision for DNN, CNN, and GRU neural networks with cross-entropy loss function in (a), (c), and (e) respectively . Top-k (precision@k) non-shared precision for DNN, CNN, and GRU neural networks with weighted cross-entropy loss function in (b), (d), and (f) respectively .</p>
      </caption>
      <graphic xlink:href="giaa152fig3"/>
    </fig>
    <fig id="fig4" orientation="portrait" position="float">
      <label>Figure 4:</label>
      <caption>
        <p>Top-k (precision@k) shared precision for DNN, CNN, and GRU neural networks with cross-entropy loss function in (a), (c), and (e) respectively. Top-k (precision@k) shared precision for DNN, CNN, and GRU neural networks with weighted cross-entropy loss function in (b), (d), and (f) respectively .</p>
      </caption>
      <graphic xlink:href="giaa152fig4"/>
    </fig>
    <fig id="fig5" orientation="portrait" position="float">
      <label>Figure 5:</label>
      <caption>
        <p>Usage frequencies of (top-k) predicted tools for DNN, CNN, and GRU neural networks with cross-entropy loss function in (a), (c), and (e) respectively. Usage frequencies of (top-k) predicted tools for DNN, CNN, and GRU neural networks with weighted cross-entropy loss function in (b), (d), and (f) respectively.</p>
      </caption>
      <graphic xlink:href="giaa152fig5"/>
    </fig>
    <sec id="h2content1608219768435">
      <title>Comparison of GRU neural network with other approaches</title>
      <p>The GRU neural network with the weighted cross-entropy loss function shows superior performance to CNN (Figs <xref ref-type="fig" rid="fig3">3d</xref>, and f, <xref ref-type="fig" rid="fig4">4d</xref> and <xref ref-type="fig" rid="fig4">f</xref>) by achieving 98% top-1 non-shared and shared precision, which proves that the GRU layers in a neural network are better for learning on tool sequences than the convolutional layer. Moreover, it shows a lower divergence in non-shared and shared precision and usage frequencies (Figs <xref ref-type="fig" rid="fig3">3d</xref> and <xref ref-type="fig" rid="fig3">f</xref>, <xref ref-type="fig" rid="fig4">4d</xref> and <xref ref-type="fig" rid="fig4">f</xref>, <xref ref-type="fig" rid="fig5">5d</xref> and <xref ref-type="fig" rid="fig5">f</xref>), establishing that its predictive strength is more stable than CNN over multiple experiment runs. Surprisingly, the weighted cross-entropy loss function does not have any beneficial effect on the CNN architecture as its non-shared and shared precision and usage frequencies show higher divergence over multiple experiment runs (Figs <xref ref-type="fig" rid="fig3">3c</xref> and <xref ref-type="fig" rid="fig3">d</xref>, <xref ref-type="fig" rid="fig4">4c</xref> and <xref ref-type="fig" rid="fig4">d</xref>, <xref ref-type="fig" rid="fig5">5c</xref> and <xref ref-type="fig" rid="fig5">d</xref>). Therefore, CNN is not used in our approach. In contrast to CNN, DNN achieves a similar non-shared and shared precision to the GRU neural network with a small divergence (Figs <xref ref-type="fig" rid="fig3">3a</xref>, <xref ref-type="fig" rid="fig3">b</xref>, and f and <xref ref-type="fig" rid="fig4">4a</xref>, <xref ref-type="fig" rid="fig4">b</xref>, and f). However, due to higher divergence in accumulated usage frequencies, it is not used in our approach (Fig. <xref ref-type="fig" rid="fig5">5a</xref>, <xref ref-type="fig" rid="fig5">b</xref>, and f). Weighted cross-entropy loss function in the GRU neural network (Fig. <xref ref-type="fig" rid="fig5">5f</xref>) drives it to classify tools more robustly with higher usage frequencies. In other words, it predicts tools with higher precision than CNN and lower divergence in usage frequencies than DNN. Therefore, it is used in our approach to learn on tool sequences and recommend tools.</p>
      <p>To compare the performance of the GRU neural network with approaches that do not use any neural network, 2 ideas are explored. The first approach simply stores all the sequences of tools [<xref rid="bib31" ref-type="bibr">31</xref>] formed using the technique shown in Fig. <xref ref-type="fig" rid="fig1">1</xref> to create a model. To recommend tools using this model, all the tool sequences are searched for a given tool or a sequence of tools. The second approach uses the ExtraTrees classifier [<xref rid="bib32" ref-type="bibr">32</xref>] to recommend tools. These two approaches are discussed in <xref ref-type="supplementary-material" rid="sup14">section S1</xref> of the supplementary document in detail.</p>
    </sec>
    <sec id="h2content1608219773875">
      <title>Benefit of regularization</title>
      <p>Using regularization minimizes overfitting by assisting the GRU neural network to make better recommendations by predicting tools that have low usage frequencies but are useful in addition to tools with high usage frequencies. For example, the recommendations of “UMI-tools count" [<xref rid="bib33" ref-type="bibr">33</xref>] tool with the regularized model include the “Seurat" [<xref rid="bib34" ref-type="bibr">34</xref>] tool, which is absent from recommendations by the non-regularized model. Another example is for “RaceID, Lineage computation using StemID" tool sequence, which gets “Lineage Branch Analysis using StemID" [<xref rid="bib35" ref-type="bibr">35</xref>] tool as one of the recommendations by the regularized model while the non-regularized model makes no recommendation at all. The recommendations for a popular mapper, RNA-STAR [<xref rid="bib36" ref-type="bibr">36</xref>], are featureCounts [<xref rid="bib37" ref-type="bibr">37</xref>], MultiQC [<xref rid="bib38" ref-type="bibr">38</xref>], Infer Experiment [<xref rid="bib39" ref-type="bibr">39</xref>], and a few others by both the models. But, in addition to these recommendations, the regularized model recommends the Read Distribution [<xref rid="bib39" ref-type="bibr">39</xref>] tool, which is not predicted by the non-regularized model. More details are provided in <xref ref-type="supplementary-material" rid="sup14">Supplementary Table 1</xref> in <xref ref-type="supplementary-material" rid="sup14">section S4</xref> of the supplementary document.</p>
    </sec>
    <sec id="h2content1608219777115">
      <title>Examples of tool recommendations</title>
      <p>To illustrate the real-time use of the recommender system in the European Galaxy server, 2 examples are provided. The first shows recommended tools for a tool sequence with 3 tools, Trimmomatic [<xref rid="bib40" ref-type="bibr">40</xref>] → BWA-MEM [<xref rid="bib41" ref-type="bibr">41</xref>] → FreeBayes [<xref rid="bib42" ref-type="bibr">42</xref>], in the workflow editor of the European Galaxy server (Fig. <xref ref-type="fig" rid="fig6">6</xref>). Trimmomatic is used to trim sequencing data such as DNA and RNA sequences. One of the useful analyses after trimming the sequences is to map them on a reference genome using a mapper. Several mappers such as BWA-MEM [<xref rid="bib41" ref-type="bibr">41</xref>], Bowtie2 [<xref rid="bib43" ref-type="bibr">43</xref>], and RNA-STAR [<xref rid="bib36" ref-type="bibr">36</xref>] are predicted. BWA-MEM is chosen from the predicted mappers and connected to Trimmomatic. After mapping, for further analysis of mapped sequences, many tools are predicted such as MultiQC for summarizing the quality of mapping, featureCounts for counting the reads mapped to different regions on the genome, or FreeBayes for detecting variants, and a few others. FreeBayes is chosen and a list of recommendations is shown as a dropdown containing tools such as bcftools norm [<xref rid="bib44" ref-type="bibr">44</xref>], VcfAllelicPrimitives [<xref rid="bib45" ref-type="bibr">45</xref>], and many others for the Trimmomatic → BWA-MEM → FreeBayes tool sequence. Another example of tool recommendations after using RNA-STAR is shown in Fig. <xref ref-type="fig" rid="fig7">7</xref>. It shows follow-up tools such as bamCoverage [<xref rid="bib46" ref-type="bibr">46</xref>] for calculating read coverage, MultiQC, featureCounts, and a few others. In summary, the tool recommendations provide useful knowledge about tools to Galaxy users and researchers to continue multiple scientific analyses.</p>
      <fig id="fig6" orientation="portrait" position="float">
        <label>Figure 6:</label>
        <caption>
          <p>Recommended tools, listed in the “Tool recommendations" dropdown, in the workflow editor of the European Galaxy server for the Trimmomatic → BWA-MEM → FreeBayes tool sequence. The recommended tools for the tool sequence can be seen in a dropdown while hovering on the right arrow button visible in the top right corner of the “FreeBayes" tool. Clicking on any recommended tool such as “bcftools norm" in the dropdown opens a new block for the chosen tool that can be connected to the tool sequence.</p>
        </caption>
        <graphic xlink:href="giaa152fig6"/>
      </fig>
      <fig id="fig7" orientation="portrait" position="float">
        <label>Figure 7:</label>
        <caption>
          <p>The figure shows recommended tools as leaves (on the right) of the tree after executing the RNA-STAR tool. Clicking on any recommended tool opens its definition in Galaxy and can be used for further analysis with the data files produced by the previous tool (RNA-STAR).</p>
        </caption>
        <graphic xlink:href="giaa152fig7"/>
      </fig>
      <p>Table <xref rid="tbl1" ref-type="table">1</xref> lists a few shared and non-shared recommended tools for multiple tool sequences in different scientific analyses such as computational chemistry, epigenetics, machine learning, proteomics, RNA sequencing, and a few others. The recommended tools shown in this table are frequently used for standard scientific analyses as highlighted in multiple Galaxy Training Network tutorials [<xref rid="bib60" ref-type="bibr">60</xref>].</p>
      <table-wrap id="tbl1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Shared and non-shared recommendations for tool sequences in different scientific analyses</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th rowspan="2" colspan="1">Scientific analysis</th>
              <th rowspan="2" colspan="1">Tool/Tool sequences</th>
              <th colspan="2" rowspan="1">Recommended tools</th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Shared</th>
              <th rowspan="1" colspan="1">Non-shared</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Computational chemistry</td>
              <td rowspan="1" colspan="1">Molecule to fingerprint [<xref rid="bib47" ref-type="bibr">47</xref>]</td>
              <td rowspan="1" colspan="1">Taylor-Butina clustering, NxN Clustering [<xref rid="bib48" ref-type="bibr">48</xref>], Similarity Search</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Epigenetics</td>
              <td rowspan="1" colspan="1">hicBuildMatrix [<xref rid="bib49" ref-type="bibr">49</xref>]</td>
              <td rowspan="1" colspan="1">hicSumMatrices</td>
              <td rowspan="1" colspan="1">hicMergeMatrixBins, hicPlotMatrix [<xref rid="bib50" ref-type="bibr">50</xref>], hicPCA, hicTransform</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">multiBamSummary [<xref rid="bib46" ref-type="bibr">46</xref>]</td>
              <td rowspan="1" colspan="1">plotCorrelation [<xref rid="bib51" ref-type="bibr">51</xref>]</td>
              <td rowspan="1" colspan="1">plotPCA</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Bowtie2</td>
              <td rowspan="1" colspan="1">MASC2 CallPeak [<xref rid="bib51" ref-type="bibr">51</xref>]</td>
              <td rowspan="1" colspan="1">bamCoverage, FreeBayes</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Machine learning</td>
              <td rowspan="1" colspan="1">Create a deep learning model architecture</td>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Create deep learning model [<xref rid="bib52" ref-type="bibr">52</xref>], Build Deep learning Batch Training Models</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Proteomics</td>
              <td rowspan="1" colspan="1">Msconvert [<xref rid="bib53" ref-type="bibr">53</xref>]</td>
              <td rowspan="1" colspan="1">Search GUI, FlashLFQ</td>
              <td rowspan="1" colspan="1">PeakPickerHiRes [<xref rid="bib54" ref-type="bibr">54</xref>]</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RNA Sequencing</td>
              <td rowspan="1" colspan="1">Cutadapt [<xref rid="bib55" ref-type="bibr">55</xref>]</td>
              <td rowspan="1" colspan="1">FastQC, RNA-STAR, MultiQC [<xref rid="bib56" ref-type="bibr">56</xref>]</td>
              <td rowspan="1" colspan="1">Bowtie2, Hisat2, BWA-MEM</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Cutadapt [<xref rid="bib55" ref-type="bibr">55</xref>], RNA-STAR</td>
              <td rowspan="1" colspan="1">featureCounts, MultiQC, Infer Experiment [<xref rid="bib56" ref-type="bibr">56</xref>]</td>
              <td rowspan="1" colspan="1">bamCoverage, RmDup</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Single-cell</td>
              <td rowspan="1" colspan="1">UMI-tools extract [<xref rid="bib33" ref-type="bibr">33</xref>]</td>
              <td rowspan="1" colspan="1">RNA-STAR [<xref rid="bib57" ref-type="bibr">57</xref>]</td>
              <td rowspan="1" colspan="1">Bowtie2, Hisat2, BWA-MEM, UMI-tools group</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Initial processing using RaceID [<xref rid="bib35" ref-type="bibr">35</xref>]</td>
              <td rowspan="1" colspan="1">Clustering using RaceID [<xref rid="bib58" ref-type="bibr">58</xref>]</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Initial processing using RaceID, Clustering using RaceID [<xref rid="bib35" ref-type="bibr">35</xref>]</td>
              <td rowspan="1" colspan="1">Cluster Inspection using RaceID [<xref rid="bib58" ref-type="bibr">58</xref>], Lineage computation using StemID</td>
              <td rowspan="1" colspan="1"/>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Variant-calling</td>
              <td rowspan="1" colspan="1">FreeBayes</td>
              <td rowspan="1" colspan="1">VcfAllelicPrimitives [<xref rid="bib59" ref-type="bibr">59</xref>]</td>
              <td rowspan="1" colspan="1">Gemini load</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">FreeBayes,VcfAllelicPrimitives</td>
              <td rowspan="1" colspan="1">SnpSift Filter, VT normalize</td>
              <td rowspan="1" colspan="1">SnpEff eff [<xref rid="bib59" ref-type="bibr">59</xref>]</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec id="h1content1608219803010">
    <title>Implementation</title>
    <p>To create a tool recommender system in Galaxy, workflows are collected from the European Galaxy server. A workflow may have 1 or many tool sequences where tools are connected one after another. Tool sequences are transformed into matrices and produced as input to a GRU neural network to learn patterns in the connections of tools.
<disp-formula id="equ1"><label>(1)</label><tex-math id="M2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}
p(x_T|x_1,x_2,....,x_{T-1})
\end{equation*}$$\end{document}</tex-math></disp-formula></p>
    <p>The probability of a tool (<italic>x<sub>T</sub></italic>) is estimated given all other prior tools (<italic>x</italic><sub>1</sub>, ..., <italic>x</italic><sub><italic>T</italic> − 1</sub>) for a tool sequence (<italic>x</italic><sub>1</sub>, ..., <italic>x</italic><sub><italic>T</italic> − 1</sub>, <italic>x<sub>T</sub></italic>). Neural network learning is classification because there are labels for tool sequences, which are learned and then predicted. Moreover, the classification is multi-class (multiple tools as labels) and multi-label (multiple tools as labels for a tool sequence) [<xref rid="bib61" ref-type="bibr">61</xref>]. To ensure unbiased learning and evaluation by the neural network, the set of tool sequences is divided into 2 parts—training and test. The training data are used for learning a model and the test data are used for evaluating the model.</p>
    <sec id="h2content1608219812202">
      <title>Uniform sampling</title>
      <p>Workflows in Galaxy come from different scientific analyses. It may happen that the numbers of workflows from these analyses are not comparable—some analyses may have a large number of workflows while some may have only a small number of workflows. This can cause some tools to be present very frequently in workflows while other tools are less frequent. Learning on these workflows and recommending tools may exhibit bias by showing better recommendations for the frequently occurring tools and poorer recommendations for the less frequent ones. To showcase this imbalance, the frequencies of the last tool in each tool sequence in training data are calculated and it is found that only a few tools have large frequencies and most of the tools are present in low frequencies (<xref ref-type="supplementary-material" rid="sup14">Supplementary Fig. 3</xref>). For example, the tools with very high frequencies (&gt;10,000) are “Concatenate datasets", “Cut", “Grouping", and “Join" while the tools with very low frequencies (&lt;5) are “Cluster inspection using RaceID", “rDock cavity definition" [<xref rid="bib62" ref-type="bibr">62</xref>], and “ChiRA collapse". Therefore, to overcome this drawback, the training data created after extracting tool sequences should be balanced to make the neural network learn on a similar number of tool sequences from different scientific analyses in each training iteration. To implement this strategy, a set of last tools in all tool sequences from the training data is collected. Furthermore, for each tool in this set, a list of indices of tool sequences in the training data are stored for which it is the last tool (<xref ref-type="supplementary-material" rid="sup14">Supplementary Table 3</xref>). Only the last tools are considered for implementing this strategy because of 2 reasons. First, the smallest tool sequences contain only 2 tools, and second, all tools become the last tool in ≥1 tool sequence and the computed frequencies of these last tools suggest their overall frequencies in the training data.</p>
      <p>In the neural network training, for each iteration (which consumes all tool sequences in the training data), small batches containing an equal number of tool sequences are created. For example, if the batch size is 100 and the size of training data is 2,000, then 20 (2,000/100 = 20) batches are created, each containing 100 tool sequences. In each batch, 100 tools from the set of last tools are uniformly selected (Column 2 in <xref ref-type="supplementary-material" rid="sup14">Supplementary Table 3</xref>) and for each selected tool, a tool sequence is chosen uniformly from its respective list of tool indices (Column 3 in <xref ref-type="supplementary-material" rid="sup14">Supplementary Table 3</xref>). After selecting tool sequences for many batches for each iteration of training (epoch), it is expected that all the tools from the set of last tools and their respective tool sequences are chosen. Performing this uniform selection of different tool sequences for each iteration, the training data become balanced. <xref ref-type="supplementary-material" rid="sup14">Supplementary Fig. 4</xref> shows that each last tool is present ∼1,670 times on average in each iteration (epoch). The order of tools in <xref ref-type="supplementary-material" rid="sup14">Supplementary Figs 3 and 4</xref> is the same.</p>
    </sec>
    <sec id="h2content1609471155245">
      <title>Data transformation</title>
      <p>Tool sequences extracted from workflows are transformed into vectors because neural networks require input data to be represented as vectors and matrices. Each tool sequence has 1 or more labels (Fig. <xref ref-type="fig" rid="fig1">1</xref>), and they are transformed into different vectors—a tool sequence vector (Fig. <xref ref-type="fig" rid="fig8">8b</xref>) and a label vector (Fig. <xref ref-type="fig" rid="fig8">8d</xref>). To form these vectors, a dictionary of tools is needed, which stores an index for each tool. Using the indices of tools, a tool sequence vector is created preserving the original order of tools as in the tool sequence. For example, Tool A has an index of “12" in the dictionary; therefore it is replaced by “12" in the vector (Fig. <xref ref-type="fig" rid="fig8">8b</xref>). The vector is padded with trailing zeros to keep the length of the vector the same across the varying lengths of tool sequences. The size of this vector is 25, which means that a tool sequence can have a maximum of 25 tools. The tool sequences larger than this size are discarded. The labels (Fig. <xref ref-type="fig" rid="fig8">8c</xref>) are transformed into a bit vector (Fig. <xref ref-type="fig" rid="fig8">8d</xref>) in which the positions, stored as indices in the dictionary of tools, of the labels (tools) are turned “on" (set to 1), specifying that these tools are the labels of the tool sequence and others are not (set to 0). The bit vector has the same size as the dictionary of tools. In the machine learning field, it is also known as a multi hot-encoded vector. Together, these 2 vectors form a training sample for the neural network. A pair of vectors are created in this manner for each tool sequence, and for all the tool sequences, they are combined to form 2 matrices, one for tool sequences and another for their respective labels. Internally, the label vectors have 2 subsets, one for shared labels (from the published, non-deleted and non-erroneous workflows) and another for non-shared labels (from the rest of the workflows). These matrices form input data to the neural network.</p>
      <fig id="fig8" orientation="portrait" position="float">
        <label>Figure 8:</label>
        <caption>
          <p>The figure shows how a tool sequence and its labels are transformed into vectors.</p>
        </caption>
        <graphic xlink:href="giaa152fig8"/>
      </fig>
    </sec>
    <sec id="h2content1608219823496">
      <title>Neural network architecture</title>
      <p>GRU neural network, a variant of RNN, is used for creating a model to recommend tools. The neural network architecture has multiple components such as different layers (Fig. <xref ref-type="fig" rid="fig9">9</xref>), activation functions, class weights, loss function, and hyperparameter tuning technique, which are discussed in detail in the following paragraphs.</p>
      <fig id="fig9" orientation="portrait" position="float">
        <label>Figure 9:</label>
        <caption>
          <p>The figure shows the architecture of the GRU neural network. It has 4 components as layers. The first layer is the input layer (yellow), followed by 2 stacked layers of GRU (cyan), and the last layer is the output layer (lavender). The dropout layers are added between the embedding and GRU layers, between the 2 GRU layers, and between the second GRU and dense layers.</p>
        </caption>
        <graphic xlink:href="giaa152fig9"/>
      </fig>
      <sec id="h3content1608219831569">
        <title>Embedding layer</title>
        <p>The first component of the neural network architecture is an input layer (Fig. <xref ref-type="fig" rid="fig9">9</xref>), which learns an embedding, a fixed-size vector, for each tool. This vector is used by the neural network as an internal representation of a tool. The embedding vector replaces the indices of tools in each tool sequence. The size of the embedding vector is fixed for all tools. For example, the vector of a tool sequence [12, 6, 75, 0, 0, ..., 0] is transformed into [[0.3, 0.01, 0.003, ..., 0.23], [0.5, 0.1, 0.005, ..., 0.9], [...], 0, 0, ...,0] by the embedding layer. The same embedding vector represents a tool in all tool sequences in which the tool is present.</p>
      </sec>
      <sec id="h3content1608219834697">
        <title>GRU layer</title>
        <p>The stacked layers of GRU learn deeper structures in the tool sequences by modelling the conditional probabilities of tools (labels) given all other prior tools (Fig. <xref ref-type="fig" rid="fig9">9</xref>). GRU has certain advantages that help it to learn on sequential data. First, it avoids the problems of vanishing and exploding gradients that commonly occur in traditional RNN [<xref rid="bib63" ref-type="bibr">63</xref>]. This is important because learning higher-order dependencies depends on the gradients of errors concerning the parameters (recurrent and input weight matrices) of GRU layers. Second, GRU has slightly fewer parameters than the long short-term memory network (LSTM), another variant of RNN, which makes using GRU simpler than LSTM. Finally, it achieves accuracy similar to that of LSTM [<xref rid="bib19" ref-type="bibr">19</xref>].</p>
      </sec>
      <sec id="h3content1608219838016">
        <title>Output layer</title>
        <p>The last component of the neural network architecture is a dense layer that computes predictions (Fig. <xref ref-type="fig" rid="fig9">9</xref>). The dimension of this layer is equal to the number of unique tools because it predicts a score for each tool (label). The predicted score of each tool is considered as its probability of being the label of an input tool sequence. The closer the predicted score of a tool is to 1, the more probable it is to be the recommended tool and the closer it is to 0, the less probable it is to be the recommended tool.</p>
      </sec>
      <sec id="h3content1608219843528">
        <title>Dropout layer</title>
        <p>Overfitting happens when a neural network performs exceptionally well on the training data but its performance on test (unseen) data remains poor. To minimize the effect of overfitting, a dropout layer is used between 2 layers of the neural network. It sets a few randomly chosen connections to 0 in the neural network to introduce some randomness to minimize overfitting [<xref rid="bib64" ref-type="bibr">64</xref>, <xref rid="bib65" ref-type="bibr">65</xref>]. Three dropout layers are used in our approach, one between the embedding and the first GRU layers, one between 2 GRU layers, and the last between the second GRU and dense layers.</p>
      </sec>
      <sec id="h3content1608219846616">
        <title>Activations</title>
        <p>These are mathematical functions that are used in neural networks to transform inputs to a layer into its outputs. Two activations are used in our approach—one is exponential linear units (ELU) [<xref rid="bib66" ref-type="bibr">66</xref>] and another is sigmoid (Equation <xref ref-type="disp-formula" rid="update1608855364794">2</xref>). ELU is used for both the GRU layers and has a special feature of being negative when the input is negative, which allows mean activation (output) to get closer to 0 compared to other activation functions such as ReLU [<xref rid="bib67" ref-type="bibr">67</xref>], which is always positive. As mean activations approach 0, the approximated and actual gradients get closer to each other. Therefore, using ELU in our neural network as an activation can be useful to achieve faster training, an improved reduction in loss, and better accuracy. Sigmoid is used in the output layer, which normalizes any real number to lie between 0 and 1, and this resulting quantity is considered the probability of each tool:
<disp-formula id="update1608855364794"><label>(2)</label><tex-math id="M3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}
f(x) = \frac{1}{1 + e^{-x}}.
\end{equation*}$$\end{document}</tex-math></disp-formula></p>
      </sec>
      <sec id="h3content1608219852560">
        <title>Usage frequencies of tools as weights</title>
        <p>To incorporate the usage frequency of tools in the recommender system, the usage frequencies of all the tools used in the past 1 year have been collected and are used in the neural network training as the weights of tools. A tool that has been used often (e.g., Tool B in Fig. <xref ref-type="fig" rid="fig2">2</xref>) in the past 1 year is assigned a higher weight than a tool (e.g., Tool C in Fig. <xref ref-type="fig" rid="fig2">2</xref>) that has been used less often in the past 1 year. When tools are recommended a score is assigned to each tool by the neural network. It is expected that a tool with a higher weight gets a higher score and a tool with a lower weight gets a lower score. To summarize, the relevance of a tool to be used in a workflow decays if its usage decreases in Galaxy over time. This weighting scheme filters out tools from the list of recommendations that have not been used in the past 1 year irrespective of their origin, either shared or non-shared workflows.</p>
        <p>Alternatively, the relevance of tools can also be ascertained by counting the occurrence of each tool in all workflows and these occurrences can be used as their weights in the neural network training. But, it may happen that some tools that were used often in the past to create workflows are not used anymore. Therefore, assigning weights to these tools in the neural network training based on their occurrences in workflows may not be a good indicator of their relevance and, overall, may not be optimal.</p>
        <p>A curve is fitted through the usage frequencies of each tool using support vector regression (SVR) to display a trend of the usage of the tool over time. Using this trend, the usage of the tool for the next month is predicted and its logarithm is used as the weight for this tool in the neural network training. The logarithm of usage frequencies is computed to normalize them because only a few tools have a significantly large magnitude of usage compared to that of the remaining tools, which may lead the neural network to learn and predict only tools with a very large magnitude of usage and ignore other tools. Learning a trend for each tool involves 5-fold cross-validation and optimizing 2 hyperparameters of SVR, kernel and degree, using grid search. The values used for the kernel are “rbf," “poly," and “linear" and the values of degree used are 2 and 3. By following the grid search, there are 3 (kernels) × 2 (degrees) = 6 different combinations of hyperparameters to be verified to find the best curve for each tool [<xref rid="bib68" ref-type="bibr">68</xref>].</p>
      </sec>
      <sec id="h3content1608219859536">
        <title>Loss function</title>
        <p>A neural network learns patterns from data by minimizing a loss function. Cross-entropy is a popular choice for a loss function in classification problems [<xref rid="bib69" ref-type="bibr">69</xref>]. In our approach, the cross-entropy function is used in the GRU neural network to compute the loss between the true and predicted label and is weighted by the label’s weight. The loss is summed up over all labels of a tool sequence and then averaged (Equation <xref ref-type="disp-formula" rid="update1608856337404">3</xref>). The term <italic>T</italic> is the total number of labels (size of the label bit vector). The term <italic>w<sub>i</sub></italic> is the weight of the <italic>i</italic><sup>th</sup> label. The terms <italic>p</italic><sup><italic>a</italic></sup> and <italic>p</italic><sup><italic>b</italic></sup> refer to the true and predicted label vectors for a tool sequence, respectively. In general, the loss is large when <italic>p</italic><sup><italic>a</italic></sup> and <italic>p</italic><sup><italic>b</italic></sup> are far away from each other, which means that the learning by the neural network is not good. If they are close, the loss is low and the predictions are better. When an unweighted cross-entropy is used as the loss function for any classification problem [<xref rid="bib70" ref-type="bibr">70</xref>], then it is assumed that all the predictions have the same weight and it does not differentiate between the more and less dominant labels. In our approach when it is used as a loss function in the neural network, then even though the predicted labels are correct they may not necessarily have large weights and thereby may be less relevant. Therefore, to reduce the possibility of less relevant labels appearing in recommendations, loss is weighted by the weights of labels. This means that if a label with a larger weight is misclassified, which means that the true and predicted values are different, then the overall loss is higher. In this way, the wrong classification of a label with a larger weight is penalized more than the wrong classification of a label with a smaller weight.
<disp-formula id="update1608856337404"><label>(3)</label><tex-math id="M4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym} 
\usepackage{amsfonts} 
\usepackage{amssymb} 
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}
\mathrm{loss} = - \frac{1}{T} \sum \nolimits_{i=1}^{T} \left[p_{i}^a \cdot \log (p^{b}_{i}) + (1 - p_{i}^a) \cdot \log (1 - p^{b}_{i})\right] * w_i
\end{equation*}$$\end{document}</tex-math></disp-formula></p>
        <p>The loss in Equation <xref ref-type="disp-formula" rid="update1608856337404">3</xref> is computed for all tool sequences in training data and is minimized using a root mean square propagation (RMSProp) optimizer. It follows an adaptive approach to estimate the learning rate by keeping knowledge of gradients in prior iterations. The learning rate is updated by dividing it with an average of the square of the prior gradients [<xref rid="bib71" ref-type="bibr">71</xref>].</p>
      </sec>
      <sec id="h3content1608219868375">
        <title>Hyperparameter tuning</title>
        <p>The hyperparameters in our approach are optimized using Bayesian (sequential model-based) optimization [<xref rid="bib72" ref-type="bibr">72</xref>]. It learns from the previously evaluated configurations, which ensures faster convergence. Reasonable ranges of all the hyperparameters to be optimized are given and the best configuration is found after 20 evaluations. More details are given in <xref ref-type="supplementary-material" rid="sup14">section S7</xref> of the supplementary document.</p>
      </sec>
    </sec>
    <sec id="h2content1608219871432">
      <title>Learning and predictions</title>
      <p>More than 229,000 tool sequences collected from &gt;18,000 workflows are divided into training and test data. A neural network learns patterns in the tool sequences from the training data and creates a model. The ability of the model to recommend tools is evaluated on the test data, which are unseen by the neural network during training. The training data form 80% (∼185,000) of all tool sequences and are iterated over 10 epochs of neural network training. The remaining 20% (∼45,000) is used as the test data. The running time of the training is ∼50 hours on a high-performance compute cluster provided by bwCloud [<xref rid="bib73" ref-type="bibr">73</xref>] with multiple cores. After learning on the training data, the model is used to predict tools. Each predicted tool gets a probability score of being the recommended tool of a tool or tool sequence. Two sets of predictions are made—shared and non-shared. Each set is sorted in descending order of their probabilities and the top ones in both sets are combined to show them as recommendations.</p>
    </sec>
  </sec>
  <sec id="sec2">
    <title>Summary and Future Work</title>
    <p>A system to recommend tools in Galaxy is built by analysing workflows using a variant of RNN (GRU) and a weighted cross-entropy loss function. The recommended tools are relevant for multiple scientific analyses with high accuracy as shown by the high similarities between the tools used in Galaxy Training Network tutorials and the recommended tools for similar analyses (Table <xref rid="tbl1" ref-type="table">1</xref>). Moreover, they are easily accessible through simple UI integrations in Galaxy (Figs <xref ref-type="fig" rid="fig6">6</xref> and <xref ref-type="fig" rid="fig7">7</xref>). Collectively, they improve user experience by helping researchers to easily create correct workflows. In addition, the approach does not store any information about tools and the recommendations are made by learning only the patterns of tool connections in workflows. The model [<xref rid="bib74" ref-type="bibr">74</xref>] created using this approach and an API [<xref rid="bib75" ref-type="bibr">75</xref>] are integrated into the European Galaxy server. The API resides with other Galaxy APIs and accesses a tool or a tool sequence specified by researchers to show its recommendations in real time using the model. The API is used at 2 different user interfaces in Galaxy—one shows recommendations in the workflow editor (Fig. <xref ref-type="fig" rid="fig6">6</xref>) and another shows them after each tool execution (Fig. <xref ref-type="fig" rid="fig7">7</xref>). The recommendation system should be potentially helpful for researchers who are new to the Galaxy platform. It shows them a few follow-up tools from a big collection of &gt;3,000 tools and enables them to perform multiple exploratory data analyses.</p>
    <p>Different Galaxy servers maintain different sets of tools and workflows. The present approach can be used to create different recommendation models for different Galaxy servers. Alternatively, all the workflows can be collected from multiple Galaxy servers and using the present approach, 1 recommendation model can be created by learning on the complete set of workflows and the model can be distributed to different Galaxy servers. To improve the quality of recommendations, the annotations of tools can be incorporated in the learning mechanism by assigning higher weights to the annotated tools in comparison to tools that are not annotated. Tools containing similar annotations may have similar functionalities, and using these similarities, tool recommendations can be further enhanced by showing similar tools for each recommended tool. In addition to learning tool connections to recommend tools, the knowledge of tools connecting to different tools based on their respective parameters can also be incorporated.</p>
  </sec>
  <sec sec-type="methods" id="sec3">
    <title>Methods</title>
    <sec id="sec3-1">
      <title>Library, model, and code repositories</title>
      <p>The Keras deep learning library is used for producing the neural network architectures [<xref rid="bib76" ref-type="bibr">76</xref>]. The trained model [<xref rid="bib74" ref-type="bibr">74</xref>] is saved as an H5 file to simplify its distribution to different Galaxy instances (Galaxy, <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/RRID:SCR_006281">RRID:SCR_006281</ext-link>). The file is an HDF5 store containing the weights of different layers of the neural network and their configurations, a dictionary of tools and their indices, and the weights of tools. The weights and configuration of the neural network are needed to recreate the trained model. The dictionary is used to replace IDs of the predicted tools by their indices in a tool sequence. All data and python scripts used in our approach are stored at GitHub for all approaches—GRU [<xref rid="bib77" ref-type="bibr">77</xref>], CNN [<xref rid="bib78" ref-type="bibr">78</xref>], and DNN [<xref rid="bib79" ref-type="bibr">79</xref>]. In each of these repositories, the process to create a tool recommendation model is explained. All these repositories are provided with a script (“extract_data.sh”) for collecting raw input datasets from a Galaxy instance. These datasets are workflows and usage frequencies of tools and are also provided in each repository. The values of multiple hyperparameters of neural networks, number of training iterations, and sizes of training and test data can be altered using a bash script (“train.sh"). To execute the scripts on a GPU-enabled machine, the “tensorflow-gpu" package should be installed instead of “tensorflow" as mentioned in the conda package dependencies file (“environment.yml"). To see recommended tools, an ipython script (“tool_recommendation_gru_wc.ipynb" for GRU repository) is also provided that loads and recreates a trained model to predict tools for a tool or a tool sequence. The result files storing precision, training, and validation losses and usage frequencies, which are used for generating line plots (Figs <xref ref-type="fig" rid="fig3">3</xref>–<xref ref-type="fig" rid="fig5">5</xref>), for all approaches are also available at GitHub [<xref rid="bib80" ref-type="bibr">80</xref>]. The code repositories of 2 other approaches that do not use neural networks are available at simple approach [<xref rid="bib31" ref-type="bibr">31</xref>] and ExtraTrees [<xref rid="bib32" ref-type="bibr">32</xref>].</p>
    </sec>
    <sec id="sec3-2">
      <title>New recommendation model</title>
      <p>On a usual Galaxy server, tools and workflows are dynamic as they are added and updated regularly. Therefore, it is important to train the GRU neural network on the complete set of workflows periodically to keep the tool recommendation model updated with the latest tools and workflows. Using a Galaxy tool [<xref rid="bib81" ref-type="bibr">81</xref>], a new recommendation model can be created after collecting workflows and tool usage data from a Galaxy server. The tool runs for several hours (&gt;24 hours) and creates a model, which is pushed to an online repository [<xref rid="bib74" ref-type="bibr">74</xref>]. From this repository, Galaxy downloads it using an API [<xref rid="bib75" ref-type="bibr">75</xref>] to recommend tools. The recommendation model is created periodically every 3–4 months to accommodate new workflows and tools. Galaxy administrators can decide upon the frequency of creating a new model. It can be created every month or every 6 months.</p>
    </sec>
    <sec id="sec3-3">
      <title>New tools as recommendations</title>
      <p>Galaxy administrators can overwrite the recommended tools predicted using the trained model by a different set of tools using the configuration option described by Kumar [<xref rid="bib82" ref-type="bibr">82</xref>]. In addition, to highlight the newly added tools, which are not part of the model, they can be appended to the recommendations using this additional configuration option.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>Availability of Supporting Source Code and Requirements</title>
    <p>Project name: Tool recommender in Galaxy using deep learning</p>
    <p>Project home page: <ext-link ext-link-type="uri" xlink:href="https://github.com/anuprulez/galaxy_tool_recommendation">https://github.com/anuprulez/galaxy_tool_recommendation</ext-link></p>
    <p>Operating system: Linux</p>
    <p>Programming languages: Python, XML, JavaScript</p>
    <p>Other requirements: Tensorflow, Keras, Scikit-learn, Numpy, H5py, Csvkit, Hyperopt</p>
    <p>License: MIT License</p>
    <p>
      <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/RRID:SCR_018491">RRID:SCR_018491</ext-link>
    </p>
    <p>Biotools ID: tool_recommender_system_in_galaxy</p>
  </sec>
  <sec sec-type="data-availability" id="sec5">
    <title>Data Availability</title>
    <p>A snapshot of the source code is available in the <italic>GigaScience</italic> GigaDB repository [<xref rid="bib83" ref-type="bibr">83</xref>].</p>
  </sec>
  <sec id="sec6">
    <title>Additional Files</title>
    <p>Supplementary Table 1. Comparison of recommendations between the GRU neural network, a simple model, and ExtraTrees classifier.</p>
    <p>Supplementary Table 2. Comparison of recommendations between the regularized and non-regularized GRU neural network.</p>
    <p>Supplementary Table 3. The strategy of uniform sampling of training data.</p>
    <p>Supplementary Figure 1. Architecture of convolutional neural network (CNN) used in the article.</p>
    <p>Supplementary Figure 2. Architecture of dense neural network (DNN) used in the article.</p>
    <p>Supplementary Figure 3. Original frequencies of last tools in training data.</p>
    <p>Supplementary Figure 4. The frequencies of last tools in training data after uniform sampling.</p>
    <p>Supplementary Figure 5. Top-1 non-shared precision for less frequent tools in test data.</p>
    <p>Supplementary Figure 6. Top-1 shared precision for less frequent tools in test data.</p>
    <p>Supplementary Figure 7. Top-1 and top-2 precision (non-shared and shared recommendations) of the ExtraTrees classifier for test data.</p>
  </sec>
  <sec id="sec7">
    <title>Abbreviations</title>
    <p>API: application programming interface; BWA: Burrows-Wheeler Aligner; CNN: convolutional neural network; DNN: dense neural network; EDAM: EMBRACE Data And Methods; ELU: exponential linear units; GPU: graphics processing unit; GRU: gated recurrent units; LSTM: long short-term memory network; PROPHETS: Process Realization and Optimization Platform Using Human-Readable Expression of Temporal-Logic Synthesis; RNN: recurrent neural network; SVR: support vector regression; UI: user interface; WINGS: workflow instance generation and specialization.</p>
  </sec>
  <sec id="sec7-4">
    <title>Competing Interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec id="sec7-5">
    <title>Funding</title>
    <p>This work was supported by the German Research Foundation (DFG) under Germany’s Excellence Strategy (CIBSS - EXC-2189 - Project ID 390939984) and German Federal Ministry of Education and Research (BMBF grant 031A538A de.NBI). The article processing charge was funded by the University of Freiburg in the funding programme Open Access Publishing.</p>
  </sec>
  <sec id="sec7-6">
    <title>Authors’ Contributions</title>
    <p>A.K. implemented the project and wrote the manuscript. H.R. wrote scripts for data collection, contributed to the manuscript, and deployed the project on the European Galaxy server. B.G. provided the idea of the project, validated results, and contributed to the manuscript. R.B. contributed to the manuscript. All authors approved the manuscript.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>giaa152_GIGA-D-20-00053_Original_Submission</label>
      <media xlink:href="giaa152_giga-d-20-00053_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup2">
      <label>giaa152_GIGA-D-20-00053_Revision_1</label>
      <media xlink:href="giaa152_giga-d-20-00053_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup3">
      <label>giaa152_GIGA-D-20-00053_Revision_2</label>
      <media xlink:href="giaa152_giga-d-20-00053_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup4">
      <label>giaa152_GIGA-D-20-00053_Revision_3</label>
      <media xlink:href="giaa152_giga-d-20-00053_revision_3.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup5">
      <label>giaa152_Response_to_Reviewer_Comments_Original_Submission</label>
      <media xlink:href="giaa152_response_to_reviewer_comments_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup6">
      <label>giaa152_Response_to_Reviewer_Comments_Revision_1</label>
      <media xlink:href="giaa152_response_to_reviewer_comments_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup7">
      <label>giaa152_Response_to_Reviewer_Comments_Revision_2</label>
      <media xlink:href="giaa152_response_to_reviewer_comments_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup8">
      <label>giaa152_Reviewer_1_Report_Original_Submission</label>
      <caption>
        <p>Bernie Pope, Ph.D. -- 3/9/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa152_reviewer_1_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup9">
      <label>giaa152_Reviewer_1_Report_Revision_1</label>
      <caption>
        <p>Bernie Pope, Ph.D. -- 8/19/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa152_reviewer_1_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup10">
      <label>giaa152_Reviewer_2_Report_Original_Submission</label>
      <caption>
        <p>Jeremy Leipzig -- 3/31/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa152_reviewer_2_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup11">
      <label>giaa152_Reviewer_3_Report_Original_Submission</label>
      <caption>
        <p>Katy Wolstencroft -- 4/2/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa152_reviewer_3_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup12">
      <label>giaa152_Reviewer_3_Report_Revision_1</label>
      <caption>
        <p>Katy Wolstencroft -- 9/7/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa152_reviewer_3_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup13">
      <label>giaa152_Reviewer_4_Report_Original_Submission</label>
      <caption>
        <p>MatÃoÅ¡ KalaÅ¡ -- 4/20/2020 Reviewed</p>
      </caption>
      <media xlink:href="giaa152_reviewer_4_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="sup14">
      <label>giaa152_Supplemental_Files</label>
      <media xlink:href="giaa152_supplemental_files.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>We thank Simon Bray and Joachim Wolff for proofreading the manuscript, Dr. Wolfgang Maier for providing feedback, and Gianmauro Cuccuru for deploying it on the European Galaxy server.</p>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="bib1">
      <label>1.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ewels</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Krueger</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Käller</surname><given-names>M</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Cluster Flow: A user-friendly bioinformatics workflow tool</article-title>. <source>F1000Res</source>. <year>2017</year>;<volume>5</volume>:<fpage>2824</fpage>.</mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Leipzig</surname><given-names>J</given-names></name></person-group><article-title>A review of bioinformatic pipeline frameworks</article-title>. <source>Brief Bioinform</source>. <year>2017</year>;<volume>18</volume>(<issue>3</issue>):<fpage>530</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">27013646</pub-id></mixed-citation>
    </ref>
    <ref id="bib3">
      <label>3.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Baichoo</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Souilmi</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Panji</surname><given-names>S</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Developing reproducible bioinformatics analysis workflows for heterogeneous computing environments to support African genomics</article-title>. <source>BMC Bioinformatics</source>. <year>2018</year>;<volume>19</volume>:<fpage>457</fpage>.<pub-id pub-id-type="pmid">30486782</pub-id></mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Love</surname><given-names>MI</given-names></name>, <name name-style="western"><surname>Huber</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Anders</surname><given-names>S</given-names></name></person-group><article-title>Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2</article-title>. <source>Genome Biol</source>. <year>2014</year>;<volume>550</volume>, doi:<pub-id pub-id-type="doi">10.1186/s13059-014-0550-8</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Afgan</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Baker</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Batut</surname><given-names>B</given-names></name>, <etal>et al.</etal></person-group>  <article-title>The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update</article-title>. <source>Nucleic Acids Res</source>. <year>2018</year>;<volume>46</volume>(<issue>W1</issue>):<fpage>W537</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">29790989</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <label>6.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Bela</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Beel</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Hentschel</surname><given-names>C</given-names></name></person-group><article-title>Scienstein: A research paper recommender system</article-title>. In: <source>Proceedings of the International Conference on Emerging Trends in Computing</source>; <year>2009</year>:<fpage>309</fpage>–<lpage>15</lpage>.</mixed-citation>
    </ref>
    <ref id="bib7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Achakulvisut</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Acuna</surname><given-names>DE</given-names></name>, <name name-style="western"><surname>Ruangrong</surname><given-names>T</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Science Concierge: A fast content-based recommendation system for scientific publications</article-title>. <source>PLoS One</source>. <year>2016</year>;<volume>11</volume>(<issue>7</issue>):<fpage>e0158423</fpage>.<pub-id pub-id-type="pmid">27383424</pub-id></mixed-citation>
    </ref>
    <ref id="bib8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Liang</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Xu</surname><given-names>D</given-names></name>, <etal>et al.</etal></person-group>  <article-title>A content-based recommender system for computer science publications</article-title>. <source>Knowl Based Syst</source>. <year>2018</year>;<volume>157</volume>, doi:<pub-id pub-id-type="doi">10.1016/j.knosys.2018.05.001</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib9">
      <label>9.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gomez-Uribe</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Hunt</surname><given-names>N</given-names></name></person-group><article-title>The Netflix recommender system: Algorithms, business value, and innovation</article-title>. <source>ACM Trans Manag Inf Syst</source>. <year>2016</year>;<volume>6</volume>(<issue>4</issue>), doi:<pub-id pub-id-type="doi">10.1145/2843948</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib10">
      <label>10.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Linden</surname><given-names>G</given-names></name></person-group><article-title>Two decades of recommender systems at Amazon.com</article-title>. <source>IEEE Internet Comput</source>. <year>2017</year>;<volume>21</volume>(<issue>3</issue>), doi:<pub-id pub-id-type="doi">10.1109/MIC.2017.72</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib11">
      <label>11.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Palmblad</surname><given-names>M</given-names></name>, <name name-style="western"><surname>L</surname><given-names>LA</given-names></name>, <name name-style="western"><surname>Ison</surname><given-names>J</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Automated workflow composition in mass spectrometry-based proteomics</article-title>. <source>Bioinformatics</source>. <year>2019</year>;<volume>35</volume>(<issue>4</issue>):<fpage>656</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">30060113</pub-id></mixed-citation>
    </ref>
    <ref id="bib12">
      <label>12.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Naujokat</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Lamprecht</surname><given-names>AL</given-names></name>, <name name-style="western"><surname>Steffen</surname><given-names>B</given-names></name></person-group><article-title>Loose programming with PROPHETS</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>de Lara</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Zisman</surname><given-names>A</given-names></name></person-group>, eds.: <source>Fundamental Approaches to Software Engineering</source>. <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>1996</year>, doi:<pub-id pub-id-type="doi">10.1007/978-3-642-28872-2_7</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib13">
      <label>13.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gil</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Ratnakar</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Kim</surname><given-names>J</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Wings intelligent workflow-based design of computational experiments</article-title>. <source>IEEE Intell Syst</source>. <year>2011</year>;<volume>26</volume>(<issue>1</issue>):<fpage>62</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="bib14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Srivastava</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Adusumilli</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Boyce</surname><given-names>H</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Semantic workflows for benchmark challenges: Enhancing comparability, reusability and reproducibility</article-title>. <source>Pac Symp Biocomput</source>. <year>2019</year>;<volume>24</volume>:<fpage>208</fpage>–<lpage>19</lpage>.<pub-id pub-id-type="pmid">30864323</pub-id></mixed-citation>
    </ref>
    <ref id="bib15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>DiBernardo</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Pottinger</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Wilkinson</surname><given-names>M</given-names></name></person-group><article-title>Semi-automatic web service composition for the life sciences using the biomoby semantic web framework</article-title>. <source>J Biomed Inform</source>. <year>2008</year>;<volume>41</volume>(<issue>5</issue>):<fpage>837</fpage>–<lpage>47</lpage>.<pub-id pub-id-type="pmid">18373957</pub-id></mixed-citation>
    </ref>
    <ref id="bib16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Michalski</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Memisevic</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Konda</surname><given-names>KR</given-names></name></person-group><article-title>Modeling sequential data using higher-order relational features and predictive training</article-title>. <year>2014</year>, <comment>arXiv:1402.2333</comment>.</mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Kann</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Yu</surname><given-names>M</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Comparative study of CNN and RNN for natural language processing</article-title>. <year>2017</year>, <comment>arXiv:1702.01923</comment>.</mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lipton</surname><given-names>ZC</given-names></name>, <name name-style="western"><surname>Kale</surname><given-names>DC</given-names></name>, <name name-style="western"><surname>Elkan</surname><given-names>C</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Learning to diagnose with LSTM recurrent neural networks</article-title>. <year>2015</year>, <comment>arXiv:1511.03677</comment>.</mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chung</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Gulcehre</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Cho</surname><given-names>K</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Empirical evaluation of gated recurrent neural networks on sequence modeling</article-title>. In: <source>NIPS 2014 Workshop on Deep Learning</source>. <year>2014</year>, <comment>arXiv:1412.3555</comment>.</mixed-citation>
    </ref>
    <ref id="bib20">
      <label>20.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Boulanger-Lewandowski</surname><given-names>N</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Vincent</surname><given-names>P</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</article-title>. In: <source>Proceedings of the 29th International Conference on Machine Learning, Edinburgh</source>. <publisher-loc>Madison, WI</publisher-loc>: <publisher-name>Omnipress</publisher-name>; <year>2012</year>:<fpage>1881</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="bib21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Karan</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Zola</surname><given-names>J</given-names></name></person-group><article-title>Exact structure learning of Bayesian networks by optimal path extension</article-title>. In: <source>IEEE International Conference on Big Data, Washington, DC</source>. <year>2016</year>:<fpage>48</fpage>–<lpage>55</lpage>.</mixed-citation>
    </ref>
    <ref id="bib22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Spirtes</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Glymour</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Scheines</surname><given-names>R</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Constructing Bayesian network models of gene expression networks from microarray data. Carnegie Mellon University</article-title>, <source>Journal contribution</source>. <year>2018</year>, <pub-id pub-id-type="doi">10.1184/R1/6491291.v1</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chickering</surname><given-names>DM</given-names></name></person-group><article-title>Learning Bayesian networks is NP-complete</article-title>. In: <person-group person-group-type="editor"><name name-style="western"><surname>Fisher</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Lenz</surname><given-names>HJ</given-names></name></person-group>, eds. <source>Learning from Data</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>1996</year>. doi:<pub-id pub-id-type="doi">10.1007/978-1-4612-2404-4_12</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chickering</surname><given-names>DM</given-names></name>, <name name-style="western"><surname>Heckerman</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Meek</surname><given-names>C</given-names></name></person-group><article-title>Large-sample learning of Bayesian networks is NP-hard</article-title>. <source>J Mach Learn Res</source>. <year>2004</year>;<volume>5</volume>:<fpage>1287</fpage>–<lpage>330</lpage>.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cooper</surname><given-names>GF</given-names></name></person-group><article-title>The computational complexity of probabilistic inference using Bayesian belief networks</article-title>. <source>Artif Intell</source>. <year>1990</year>;<volume>42</volume>:<fpage>393</fpage>–<lpage>405</lpage>.</mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>European Galaxy Server</collab>.</person-group>  <comment><ext-link ext-link-type="uri" xlink:href="https://usegalaxy.eu/">https://usegalaxy.eu/</ext-link></comment>
<year>2020</year>
<comment>Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jian</surname><given-names>X</given-names></name>, <name name-style="western"><surname>Wickramarathne</surname><given-names>TL</given-names></name>, <name name-style="western"><surname>Chawla</surname><given-names>NV</given-names></name></person-group><article-title>Representing higher-order dependencies in networks</article-title>. <source>Sci Adv</source>. <year>2016</year>;<volume>2</volume>(<issue>5</issue>), doi:<pub-id pub-id-type="doi">10.1126/sciadv.1600028</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib28">
      <label>28.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Said</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Bellogín</surname><given-names>A</given-names></name>, <name name-style="western"><surname>de Vries</surname><given-names>AP</given-names></name></person-group><article-title>A Top-N recommender system evaluation protocol inspired by deployed systems</article-title>. In: <source>Proceedings of the 2013 ACM RecSys Workshop on Large-Scale Recommender Systems, Hong Kong</source>. <year>2013</year>, <comment><ext-link ext-link-type="uri" xlink:href="https://ir.cwi.nl/pub/21489">https://ir.cwi.nl/pub/21489</ext-link></comment>.</mixed-citation>
    </ref>
    <ref id="bib29">
      <label>29.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kang</surname><given-names>Z</given-names></name>, <name name-style="western"><surname>Peng</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Cheng</surname><given-names>Q</given-names></name></person-group><article-title>Top-N recommender system via matrix completion</article-title>. In: <source>Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)</source>. <year>2016</year>:<fpage>179</fpage>–<lpage>85</lpage>.</mixed-citation>
    </ref>
    <ref id="bib30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Deshpande</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Karypis</surname><given-names>G</given-names></name></person-group><article-title>Item-based top-N recommender algorithms</article-title>. <source>ACM Trans Inf Syst</source>. <year>2004</year>;<volume>22</volume>(<issue>1</issue>):<fpage>143</fpage>–<lpage>77</lpage>.</mixed-citation>
    </ref>
    <ref id="bib31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name></person-group><article-title>Tool Recommender in Galaxy using stored tool sequences</article-title>. <year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/anuprulez/galaxy_tool_recommendation/tree/statistical_model">https://github.com/anuprulez/galaxy_tool_recommendation/tree/statistical_model</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name></person-group><article-title>Tool recommender system in Galaxy using extra trees classifier</article-title>. <year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/anuprulez/galaxy_tool_recommendation/tree/sklearn_rf">https://github.com/anuprulez/galaxy_tool_recommendation/tree/sklearn_rf</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Smith</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Heger</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Sudbery</surname><given-names>I</given-names></name></person-group><article-title>UMI-tools: modeling sequencing errors in Unique Molecular Identifiers to improve quantification accuracy</article-title>. <source>Genome Res</source>. <year>2017</year>;<volume>27</volume>(<issue>3</issue>):<fpage>491</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">28100584</pub-id></mixed-citation>
    </ref>
    <ref id="bib34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Butler</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Hoffman</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Smibert</surname><given-names>P</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Integrating single-cell transcriptomic data across different conditions, technologies, and species</article-title>. <source>Nat Biotechnol</source>. <year>2018</year>;<volume>36</volume>(<issue>5</issue>):<fpage>411</fpage>–<lpage>20</lpage>.<pub-id pub-id-type="pmid">29608179</pub-id></mixed-citation>
    </ref>
    <ref id="bib35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grn</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Lyubimova</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Kester</surname><given-names>L</given-names></name>,<etal>et al.</etal></person-group>  <article-title>Single-cell messenger RNA sequencing reveals rare intestinal cell types</article-title>. <source>Nature</source>. <year>2015</year>;<volume>525</volume>(<issue>7568</issue>):<fpage>251</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">26287467</pub-id></mixed-citation>
    </ref>
    <ref id="bib36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dobin</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Davis</surname><given-names>CA</given-names></name>, <name name-style="western"><surname>Schlesinger</surname><given-names>F</given-names></name>, <etal>et al.</etal></person-group>  <article-title>STAR: ultrafast universal RNA-seq aligner</article-title>. <source>Bioinformatics</source>. <year>2013</year>;<volume>29</volume>(<issue>1</issue>):<fpage>15</fpage>–<lpage>21</lpage>.<pub-id pub-id-type="pmid">23104886</pub-id></mixed-citation>
    </ref>
    <ref id="bib37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liao</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Smyth</surname><given-names>GK</given-names></name>, <name name-style="western"><surname>Shi</surname><given-names>W</given-names></name></person-group><article-title>featureCounts: an efficient general purpose program for assigning sequence reads to genomic features</article-title>. <source>Bioinformatics</source>. <year>2013</year>;<volume>30</volume>(<issue>7</issue>):<fpage>923</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">24227677</pub-id></mixed-citation>
    </ref>
    <ref id="bib38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ewels</surname><given-names>P</given-names></name>, <name name-style="western"><surname>Magnusson</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Lundin</surname><given-names>S</given-names></name>, <etal>et al.</etal></person-group>  <article-title>MultiQC: summarize analysis results for multiple tools and samples in a single report</article-title>. <source>Bioinformatics</source>. <year>2016</year>;<volume>32</volume>(<issue>19</issue>):<fpage>3047</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">27312411</pub-id></mixed-citation>
    </ref>
    <ref id="bib39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>L</given-names></name>, <name name-style="western"><surname>Wang</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Li</surname><given-names>W</given-names></name></person-group><article-title>RSeQC: quality control of RNA-seq experiments</article-title>. <source>Bioinformatics</source>. <year>2012</year>;<volume>28</volume>(<issue>16</issue>):<fpage>2184</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">22743226</pub-id></mixed-citation>
    </ref>
    <ref id="bib40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bolger</surname><given-names>AM</given-names></name>, <name name-style="western"><surname>Lohse</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Usadel</surname><given-names>B</given-names></name></person-group><article-title>Trimmomatic: a flexible trimmer for Illumina sequence data</article-title>. <source>Bioinformatics</source>. <year>2014</year>;<volume>30</volume>:<fpage>2114</fpage>–<lpage>20</lpage>.<pub-id pub-id-type="pmid">24695404</pub-id></mixed-citation>
    </ref>
    <ref id="bib41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H</given-names></name></person-group><article-title>Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM</article-title>. <year>2013</year>, <comment>arXiv:1303.3997</comment>.</mixed-citation>
    </ref>
    <ref id="bib42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garrison</surname><given-names>E</given-names></name>, <name name-style="western"><surname>Marth</surname><given-names>G</given-names></name></person-group><article-title>Haplotype-based variant detection from short-read sequencing</article-title>. <year>2012</year>, <comment>arXiv:1207.3907</comment>.</mixed-citation>
    </ref>
    <ref id="bib43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Langmead</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Trapnell</surname><given-names>C</given-names></name>, <name name-style="western"><surname>Pop</surname><given-names>M</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Ultrafast and memory-efficient alignment of short DNA sequences to the human genome</article-title>. <source>Genome Biol</source>. <year>2009</year>;<volume>10</volume>, doi:<pub-id pub-id-type="doi">10.1186/gb-2009-10-3-r25</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib44">
      <label>44.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Handsaker</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Wysoker</surname><given-names>A</given-names></name>, <etal>et al.</etal></person-group>  <article-title>The Sequence Alignment/Map format and SAMtools</article-title>. <source>Bioinformatics</source>. <year>2009</year>;<volume>25</volume>(<issue>16</issue>):<fpage>2078</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">19505943</pub-id></mixed-citation>
    </ref>
    <ref id="bib45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Garrison</surname><given-names>E</given-names></name></person-group><comment>A C++ library for parsing and manipulating VCF files</comment><year>2015</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/vcflib/vcflib">https://github.com/vcflib/vcflib</ext-link>. Accessed 2 August 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ramirez</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Ryan</surname><given-names>DP</given-names></name>, <name name-style="western"><surname>Gruening</surname><given-names>B</given-names></name>, <etal>et al.</etal></person-group>  <article-title>deepTools2: a next generation web server for deep-sequencing data analysis</article-title>. <source>Nucleic Acids Res</source>. <year>2016</year>;<volume>44</volume>(<issue>W1</issue>):<fpage>W160</fpage>–<lpage>5</lpage>.<pub-id pub-id-type="pmid">27079975</pub-id></mixed-citation>
    </ref>
    <ref id="bib47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>O’Boyle</surname><given-names>NM</given-names></name>, <name name-style="western"><surname>Banck</surname><given-names>M</given-names></name>, <name name-style="western"><surname>James</surname><given-names>CA</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Open Babel: an open chemical toolbox</article-title>. <source>J Cheminform</source>. <year>2011</year>;<volume>3</volume>(<issue>1</issue>), doi:<pub-id pub-id-type="doi">10.1186/1758-2946-3-33</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib48">
      <label>48.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bray</surname><given-names>S</given-names></name></person-group><comment>Protein-ligand docking (Galaxy Training Materials)</comment><year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://training.galaxyproject.org/training-material/topics/computational-chemistry/tutorials/cheminformatics/tutorial.html">https://training.galaxyproject.org/training-material/topics/computational-chemistry/tutorials/cheminformatics/tutorial.html</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib49">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ramirez</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Wolff</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Gruening</surname><given-names>B</given-names></name>  <etal>et al.</etal></person-group>, <comment>Deeptools/Hicexplorer: Winter Release.</comment>  <year>2017</year>
<comment><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/1133705">https://zenodo.org/record/1133705</ext-link></comment>. Accessed 2 August 2020.</mixed-citation>
    </ref>
    <ref id="bib50">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wolff</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Ramirez</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Bhardwaj</surname><given-names>V</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Hi-C analysis of <italic>Drosophila melanogaster</italic> cells using HiCExplorer (Galaxy Training Materials)</article-title>. <year>2020</year>
<comment><ext-link ext-link-type="uri" xlink:href="https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/hicexplorer/tutorial.html">https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/hicexplorer/tutorial.html</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib51">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dündar</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Erxleben</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Batut</surname><given-names>B</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Formation of the Super-Structures on the Inactive X (Galaxy Training Materials)</article-title>. <year>2019</year>
<comment><ext-link ext-link-type="uri" xlink:href="https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/formation_of_super-structures_on_xi/tutorial.html">https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/formation_of_super-structures_on_xi/tutorial.html</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib52">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Khanteymoori</surname><given-names>A</given-names></name></person-group><article-title>Introduction to deep learning (Galaxy Training Materials)</article-title>. <year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro_deep_learning/tutorial.html">https://training.galaxyproject.org/training-material/topics/statistics/tutorials/intro_deep_learning/tutorial.html</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib53">
      <label>53.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kessner</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Chambers</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Burke</surname><given-names>R</given-names></name>, <etal>et al.</etal></person-group>  <article-title>ProteoWizard: open source software for rapid proteomics tools development</article-title>. <source>Bioinformatics</source>. <year>2008</year>;<volume>24</volume>(<issue>21</issue>):<fpage>2534</fpage>–<lpage>6</lpage>.<pub-id pub-id-type="pmid">18606607</pub-id></mixed-citation>
    </ref>
    <ref id="bib54">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sigloch</surname><given-names>FC</given-names></name>, <name name-style="western"><surname>Grüning</surname><given-names>B</given-names></name></person-group><article-title>Peptide and Protein ID using OpenMS tools (Galaxy Training Materials)</article-title>. <year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/protein-id-oms/tutorial.html">https://training.galaxyproject.org/training-material/topics/proteomics/tutorials/protein-id-oms/tutorial.html</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib55">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Marcel</surname><given-names>M</given-names></name></person-group><article-title>Cutadapt removes adapter sequences from high-throughput sequencing reads</article-title>. <source>EMBnet J</source>. <year>2011</year>;<volume>17</volume>(<issue>1</issue>), doi:<pub-id pub-id-type="doi">10.14806/ej.17.1.200</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib56">
      <label>56.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Batut</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Freeberg</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Heydarian</surname><given-names>M</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Reference-based RNA-Seq data analysis (Galaxy Training Materials)</article-title>. <year>2020</year>
<comment><ext-link ext-link-type="uri" xlink:href="https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html">https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/ref-based/tutorial.html</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib57">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tekman</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Batut</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Erxleben</surname><given-names>A</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Pre-processing of Single-Cell RNA Data (Galaxy Training Materials)</article-title>. <year>2020</year>
<comment><ext-link ext-link-type="uri" xlink:href="https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/scrna-preprocessing/tutorial.html">https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/scrna-preprocessing/tutorial.html</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib58">
      <label>58.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tekman</surname><given-names>M</given-names></name>, <name name-style="western"><surname>Ostrovsky</surname><given-names>A</given-names></name></person-group><article-title>Downstream single-cell RNA analysis with RaceID (Galaxy Training Materials)</article-title>. <year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/scrna-raceid/tutorial.html">https://training.galaxyproject.org/training-material/topics/transcriptomics/tutorials/scrna-raceid/tutorial.html</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib59">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nekrutenko</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Soranzo</surname><given-names>N</given-names></name></person-group><article-title>Calling variants in diploid systems (Galaxy Training Materials)</article-title>. <year>2019</year><comment><ext-link ext-link-type="uri" xlink:href="https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/dip/tutorial.html">https://training.galaxyproject.org/training-material/topics/variant-analysis/tutorials/dip/tutorial.html</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib60">
      <label>60.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Batut</surname><given-names>B</given-names></name>, <name name-style="western"><surname>Hiltemann</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Bagnacani</surname><given-names>A</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Community-driven data analysis training for biology</article-title>. <source>Cell Syst</source>. <year>2018</year>;<volume>6</volume>:<fpage>752</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">29953864</pub-id></mixed-citation>
    </ref>
    <ref id="bib61">
      <label>61.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tsoumakas</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Katakis</surname><given-names>I</given-names></name></person-group><article-title>Multi-label classification: an overview</article-title>. <source>Int J Data Warehous Min</source>. <year>2009</year>;<volume>3</volume>, doi:<pub-id pub-id-type="doi">10.4018/jdwm.2007070101</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib62">
      <label>62.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ruiz-Carmona</surname><given-names>S</given-names></name>, <name name-style="western"><surname>Alvarez-Garcia</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Foloppe</surname><given-names>N</given-names></name>, <etal>et al.</etal></person-group>  <article-title>rDock: A fast, versatile and open source program for docking ligands to proteins and nucleic acids</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>(<issue>4</issue>), doi:<pub-id pub-id-type="doi">10.1371/journal.pcbi.1003571</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib63">
      <label>63.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pascanu</surname><given-names>R</given-names></name>, <name name-style="western"><surname>Mikolov</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Bengio</surname><given-names>Y</given-names></name></person-group><article-title>On the difficulty of training recurrent neural networks</article-title>. <year>2012</year>, <comment>arXiv:1211.5063</comment>.</mixed-citation>
    </ref>
    <ref id="bib64">
      <label>64.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zaremba</surname><given-names>W</given-names></name>, <name name-style="western"><surname>Sutskever</surname><given-names>I</given-names></name>, <name name-style="western"><surname>Vinyals</surname><given-names>O</given-names></name></person-group><article-title>Recurrent neural network regularization</article-title>. <year>2014</year>, <comment>arXiv:1409.2329</comment>.</mixed-citation>
    </ref>
    <ref id="bib65">
      <label>65.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Gal</surname><given-names>Y</given-names></name>, <name name-style="western"><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><article-title>A theoretically grounded application of dropout in recurrent neural networks</article-title>. In: <source>Proceedings of the 30th International Conference on Neural Information Processing Systems</source>. <year>2016</year>:<fpage>1027</fpage>–<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="bib66">
      <label>66.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Clevert</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Unterthiner</surname><given-names>T</given-names></name>, <name name-style="western"><surname>Hochreiter</surname><given-names>S</given-names></name></person-group><article-title>Fast and accurate deep network learning by exponential linear units (ELUs)</article-title>. <year>2015</year>, <comment>arXiv:1511.07289</comment>.</mixed-citation>
    </ref>
    <ref id="bib67">
      <label>67.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Nair</surname><given-names>V</given-names></name>, <name name-style="western"><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>Rectified linear units improve restricted Boltzmann machines</article-title>. In: <source>ICML’10: Proceedings of the 27th International Conference on International Conference on Machine Learning</source>. <year>2010</year>:<fpage>807</fpage>–<lpage>14</lpage>.</mixed-citation>
    </ref>
    <ref id="bib68">
      <label>68.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pedregosa</surname><given-names>F</given-names></name>, <name name-style="western"><surname>Varoquaux</surname><given-names>G</given-names></name>, <name name-style="western"><surname>Gramfort</surname><given-names>A</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Scikit-learn: Machine Learning in Python</article-title>. <source>J Mach Learn Res</source>. <year>2011</year>;<volume>12</volume>:<fpage>2825</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="bib69">
      <label>69.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Janocha</surname><given-names>K</given-names></name>, <name name-style="western"><surname>Czarnecki</surname><given-names>W</given-names></name></person-group><article-title>On loss functions for deep neural networks in classification</article-title>. <year>2017</year>, <comment>arXiv:1702.05659</comment>.</mixed-citation>
    </ref>
    <ref id="bib70">
      <label>70.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sadowski</surname><given-names>P</given-names></name></person-group><article-title>Notes on backpropagation</article-title>. <year>2016</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.ics.uci.edu/~pjsadows/notes.pdf">https://www.ics.uci.edu/~pjsadows/notes.pdf</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib71">
      <label>71.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ruder</surname><given-names>S</given-names></name></person-group><article-title>An overview of gradient descent optimization algorithms</article-title>. <year>2016</year>, <comment>arXiv:1609.04747</comment>.</mixed-citation>
    </ref>
    <ref id="bib72">
      <label>72.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bergstra</surname><given-names>J</given-names></name>, <name name-style="western"><surname>Yamins</surname><given-names>D</given-names></name>, <name name-style="western"><surname>Cox</surname><given-names>DD</given-names></name></person-group><article-title>Hyperopt: a Python library for optimizing the hyperparameters of machine learning algorithms</article-title>. <source>Comput Sci Discov</source>. <year>2015</year>;<volume>8</volume>:<fpage>014008</fpage>.</mixed-citation>
    </ref>
    <ref id="bib73">
      <label>73.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>BwCluster</collab>.</person-group>  <comment><ext-link ext-link-type="uri" xlink:href="https://portal.bw-cloud.org/project/">https://portal.bw-cloud.org/project/</ext-link></comment>
<year>2020</year>
<comment>Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib74">
      <label>74.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name></person-group><article-title>Tool Recommender model</article-title>. <year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/galaxyproject/galaxy-test-data/blob/master/tool_recommendation_model.hdf5">https://github.com/galaxyproject/galaxy-test-data/blob/master/tool_recommendation_model.hdf5</ext-link>. Accessed 29 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib75">
      <label>75.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name></person-group><article-title>Get tool predictions</article-title>. <year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/usegalaxy-eu/galaxy/blob/release_20.05_europe/lib/galaxy/webapps/galaxy/api/workflows.py#L638">https://github.com/usegalaxy-eu/galaxy/blob/release_20.05_europe/lib/galaxy/webapps/galaxy/api/workflows.py#L638</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib76">
      <label>76.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chollet</surname><given-names>F.</given-names></name>, <etal>et al.</etal></person-group>  <comment>Keras. <ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>.</comment>  <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="bib77">
      <label>77.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name></person-group><article-title>Tool Recommender in Galaxy using GRU neural network</article-title>. <year>2020</year>; <comment><ext-link ext-link-type="uri" xlink:href="https://github.com/anuprulez/galaxy_tool_recommendation">https://github.com/anuprulez/galaxy_tool_recommendation</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib78">
      <label>78.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name></person-group><article-title>Tool Recommender in Galaxy using CNN neural network</article-title>. <year>2020</year>; <comment><ext-link ext-link-type="uri" xlink:href="https://github.com/anuprulez/galaxy_tool_recommendation/tree/cnn_wc">https://github.com/anuprulez/galaxy_tool_recommendation/tree/cnn_wc</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib79">
      <label>79.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name></person-group><article-title>Tool Recommender in Galaxy using DNN neural network</article-title>. <year>2020</year>;. <comment><ext-link ext-link-type="uri" xlink:href="https://github.com/anuprulez/galaxy_tool_recommendation/tree/dnn_wc">https://github.com/anuprulez/galaxy_tool_recommendation/tree/dnn_wc</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib80">
      <label>80.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name></person-group><source>Output results files</source>. <year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/anuprulez/galaxy_tool_recommendation/tree/master/output_files/data_20_05">https://github.com/anuprulez/galaxy_tool_recommendation/tree/master/output_files/data_20_05</ext-link>. Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib81">
      <label>81.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name></person-group><article-title>Tool Recommender model creator</article-title>. <comment><ext-link ext-link-type="uri" xlink:href="https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/create_tool_recommendation_model/create_tool_recommendation_model/0.0.3">https://usegalaxy.eu/root?tool_id=toolshed.g2.bx.psu.edu/repos/bgruening/create_tool_recommendation_model/create_tool_recommendation_model/0.0.3</ext-link></comment><year>2020</year><comment>Accessed 22 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib82">
      <label>82.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name></person-group><article-title>Tool Recommender overwrite</article-title>. <comment><ext-link ext-link-type="uri" xlink:href="https://github.com/usegalaxy-eu/galaxy/blob/release_20.05_europe/config/tool_recommendations_overwrite.yml.sample">https://github.com/usegalaxy-eu/galaxy/blob/release_20.05_europe/config/tool_recommendations_overwrite.yml.sample</ext-link></comment><year>2020</year><comment>Accessed 30 July 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib83">
      <label>83.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kumar</surname><given-names>A</given-names></name>, <name name-style="western"><surname>Rasche</surname><given-names>H</given-names></name>, <name name-style="western"><surname>Gruening</surname><given-names>B</given-names></name>, <etal>et al.</etal></person-group>  <article-title>Supporting data for “Tool recommender system in Galaxy using deep learning.”</article-title>. <source>GigaScience Database</source>. <year>2020</year>
<pub-id pub-id-type="doi">10.5524/100838</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
