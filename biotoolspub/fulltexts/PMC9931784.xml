<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Neuroinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Neuroinformatics</journal-id>
    <journal-title-group>
      <journal-title>Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1539-2791</issn>
    <issn pub-type="epub">1559-0089</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9931784</article-id>
    <article-id pub-id-type="pmid">36178571</article-id>
    <article-id pub-id-type="publisher-id">9607</article-id>
    <article-id pub-id-type="doi">10.1007/s12021-022-09607-1</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Automatic Cerebral Hemisphere Segmentation in Rat MRI with Ischemic Lesions via Attention-based Convolutional Neural Networks</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2708-1547</contrib-id>
        <name>
          <surname>Valverde</surname>
          <given-names>Juan Miguel</given-names>
        </name>
        <address>
          <email>juanmiguel.valverde@uef.fi</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shatillo</surname>
          <given-names>Artem</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>De Feo</surname>
          <given-names>Riccardo</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tohka</surname>
          <given-names>Jussi</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.9668.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0726 2490</institution-id><institution>A.I. Virtanen Institute for Molecular Sciences, </institution><institution>University of Eastern Finland, </institution></institution-wrap>Kuopio, 70150 Finland </aff>
      <aff id="Aff2"><label>2</label>Charles River Discovery Services, Kuopio, 70210 Finland </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>30</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>30</day>
      <month>9</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2023</year>
    </pub-date>
    <volume>21</volume>
    <issue>1</issue>
    <fpage>57</fpage>
    <lpage>70</lpage>
    <history>
      <date date-type="accepted">
        <day>22</day>
        <month>9</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access </bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">We present MedicDeepLabv3+, a convolutional neural network that is the first completely automatic method to segment cerebral hemispheres in magnetic resonance (MR) volumes of rats with ischemic lesions. MedicDeepLabv3+ improves the state-of-the-art DeepLabv3+ with an advanced decoder, incorporating spatial attention layers and additional skip connections that, as we show in our experiments, lead to more precise segmentations. MedicDeepLabv3+ requires no MR image preprocessing, such as bias-field correction or registration to a template, produces segmentations in less than a second, and its GPU memory requirements can be adjusted based on the available resources. We optimized MedicDeepLabv3+ and six other state-of-the-art convolutional neural networks (DeepLabv3+, UNet, HighRes3DNet, V-Net, VoxResNet, Demon) on a heterogeneous training set comprised by MR volumes from 11 cohorts acquired at different lesion stages. Then, we evaluated the trained models and two approaches specifically designed for rodent MRI skull stripping (RATS and RBET) on a large dataset of 655 MR rat brain volumes. In our experiments, MedicDeepLabv3+ outperformed the other methods, yielding an average Dice coefficient of 0.952 and 0.944 in the brain and contralateral hemisphere regions. Additionally, we show that despite limiting the GPU memory and the training data, our MedicDeepLabv3+ also provided satisfactory segmentations. In conclusion, our method, publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jmlipman/MedicDeepLabv3Plus">https://github.com/jmlipman/MedicDeepLabv3Plus</ext-link>, yielded excellent results in multiple scenarios, demonstrating its capability to reduce human workload in rat neuroimaging studies.</p>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1007/s12021-022-09607-1.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Hemisphere segmentation</kwd>
      <kwd>MRI</kwd>
      <kwd>Convolutional Neural Networks</kwd>
      <kwd>Rodent imaging</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010665</institution-id>
            <institution>H2020 Marie Skłodowska-Curie Actions</institution>
          </institution-wrap>
        </funding-source>
        <award-id>740264</award-id>
        <principal-award-recipient>
          <name>
            <surname>Valverde</surname>
            <given-names>Juan Miguel</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004895</institution-id>
            <institution>European Social Fund</institution>
          </institution-wrap>
        </funding-source>
        <award-id>S21770</award-id>
        <principal-award-recipient>
          <name>
            <surname>De Feo</surname>
            <given-names>Riccardo</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002341</institution-id>
            <institution>Academy of Finland</institution>
          </institution-wrap>
        </funding-source>
        <award-id>316258</award-id>
        <principal-award-recipient>
          <name>
            <surname>Tohka</surname>
            <given-names>Jussi</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>University of Eastern Finland (UEF) including Kuopio University Hospital</institution>
        </funding-source>
      </award-group>
      <open-access>
        <p>Open access funding provided by University of Eastern Finland (UEF) including Kuopio University Hospital.</p>
      </open-access>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Science+Business Media, LLC, part of Springer Nature 2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Rodents are widely used in preclinical research to investigate brain diseases (Carbone, <xref ref-type="bibr" rid="CR6">2021</xref>). These studies often utilize in-vivo imaging technologies, such as magnetic resonance imaging (MRI), to visualize brain tissue at different time-points, which is necessary for studying disease progression. MRI permits the acquisition of brain images with different contrasts in a non-invasive manner, making MRI a particularly advantageous in-vivo imaging technology. However, these images typically need to be segmented before conducting quantitative analysis. As an example, the size of the hemispheric brain edema relative to the volume of the contralateral hemisphere is an important biomarker for acute stroke that requires accurate hemisphere segmentation (Swanson et al., <xref ref-type="bibr" rid="CR57">1990</xref>; Gerriets et al., <xref ref-type="bibr" rid="CR23">2004</xref>).</p>
    <p id="Par3">With brain edema biomarkers in mind, our work focuses on cerebral hemisphere segmentation in MRI volumes of rat brains with ischemic lesions. Segmenting these images is particularly challenging since lesions’ size, shape, location, and contrast can vary even within images from the same cohort, hampering, as we show in our experiments, traditional segmentation methods. Additionally, rodents’ small size makes image acquisition sensitive to misalignments, potentially producing slices with asymmetric hemispheres and particularly affecting anisotropic data. Furthermore, although neuroanatomical segmentation tools can be used to produce hemisphere masks (e.g., Schwarz et al. (<xref ref-type="bibr" rid="CR56">2006</xref>)), these tools only work on rodent brains without lesions, as lesions alter the appearance and location of the brain structures. These difficulties have led researchers and technicians to annotate rodent cerebral hemispheres manually (Freret et al., <xref ref-type="bibr" rid="CR21">2006</xref>; McBride et al., <xref ref-type="bibr" rid="CR43">2015</xref>), which is laborious and time-consuming, and motivates this work.</p>
    <p id="Par4">In recent years, convolutional neural networks (ConvNets) have been widely used to segment medical images due to their outstanding performance (Bakas et al., <xref ref-type="bibr" rid="CR3">2018</xref>; Bernard et al., <xref ref-type="bibr" rid="CR5">2018</xref>; Heller et al., <xref ref-type="bibr" rid="CR25">2021</xref>). ConvNets can be optimized end-to-end, require no preprocessing, such as bias-field correction and costly registration, and can produce segmentation masks in real time (De Feo et al., <xref ref-type="bibr" rid="CR15">2021</xref>). ConvNets can also be tailored to specific segmentation problems by incorporating domain constraints and shape priors (Kervadec et al., <xref ref-type="bibr" rid="CR31">2019</xref>). In particular, DeepLabv3+ architecture with its efficient computation of large image regions via dilated convolutions has demonstrated excellent results on various segmentation tasks in computer vision (Chen et al., <xref ref-type="bibr" rid="CR11">2018b</xref>). Xie et al. (<xref ref-type="bibr" rid="CR66">2019</xref>) utilized DeepLabv3+ for gland instance segmentation on histology images to estimate the segmentation maps and subsequently refined such estimation with a second ConvNet. Ma et al. (<xref ref-type="bibr" rid="CR41">2019</xref>) modified DeepLabv3+ for applying style transfer to homogenize MR images with different properties. Khan et al. (<xref ref-type="bibr" rid="CR32">2020</xref>) showed that DeepLabv3+ outperforms other ConvNets on prostate segmentation of T2-weighted MR scans.</p>
    <p id="Par5">We present and make publicly available MedicDeepLabv3+, the first method for segmenting cerebral hemispheres in MR images of rats with ischemic lesions. MedicDeepLabv3+ improves DeepLabv3+ architecture with a new decoder with spatial attention layers (Oktay et al., <xref ref-type="bibr" rid="CR49">2018</xref>; Wang et al., <xref ref-type="bibr" rid="CR62">2019</xref>) and an increased number of skip connections that facilitate the optimization. We optimized our method on a training set comprised by 51 MR rat brain volumes from 11 cohorts acquired at multiple lesion stages, and we evaluated it on a large and challenging dataset of 655 MR rat brain volumes. Our experiments show that MedicDeepLabv3+ outperformed the baseline state-of-the-art DeepLabv3+ (Chen et al., <xref ref-type="bibr" rid="CR11">2018b</xref>), UNet (Ronneberger et al., <xref ref-type="bibr" rid="CR53">2015</xref>), HighRes3DNet (Li et al., <xref ref-type="bibr" rid="CR39">2017</xref>), V-Net (Milletari et al., <xref ref-type="bibr" rid="CR44">2016</xref>), VoxResNet (Chen et al., <xref ref-type="bibr" rid="CR7">2018</xref>), and, particularly for skull stripping, it also outperformed Demon (Roy et al., <xref ref-type="bibr" rid="CR55">2018</xref>), RATS (Oguz et al., <xref ref-type="bibr" rid="CR48">2014</xref>), and RBET (Wood et al., <xref ref-type="bibr" rid="CR64">2013</xref>). Additionally, we evaluated MedicDeepLabv3+ with very limited GPU memory and training data, and our experiments demonstrate that, despite such restrictions, MedicDeepLabv3+ yields satisfactory segmentations, showcasing its usability in multiple real-life situations and environments.</p>
    <sec id="Sec2">
      <title>Related Work</title>
      <sec id="FPar1">
        <title>Anatomical segmentation of rodent brain MRI with lesions</title>
        <p id="Par6">Anatomical segmentation in MR images of rodents with lesions is an under-researched area; Roy et al. (<xref ref-type="bibr" rid="CR55">2018</xref>) and De Feo et al. (<xref ref-type="bibr" rid="CR14">2022</xref>) are the only studies that examined this problem. Roy et al. (<xref ref-type="bibr" rid="CR55">2018</xref>) showed that their Inception-based (Szegedy et al., <xref ref-type="bibr" rid="CR58">2015</xref>) skull-stripping ConvNet named ‘Demon’ outperformed other methods on MR images of mice and humans with traumatic brain injury. De Feo et al. (<xref ref-type="bibr" rid="CR14">2022</xref>) presented an ensemble of ConvNets named MU-Net-R for ipsi- and contralateral hippocampus segmentation on MR images of rats with traumatic brain injury. Mulder et al. (<xref ref-type="bibr" rid="CR45">2017</xref>) developed a lesion segmentation pipeline that includes an atlas-based contralateral hemisphere segmentation step. However, these hemisphere segmentations were not compared to a ground truth, and this approach is sensitive to the lesion appearance because it relies on registration.</p>
      </sec>
      <sec id="FPar2">
        <title>Anatomical segmentation of rodent brain MRI without lesions</title>
        <p id="Par7">The vast majority of anatomical segmentation methods for rodent MR brain images have been exclusively developed for brains without lesions. These methods can be classified into three categories. First, atlas-based segmentation approaches, which apply registration to one or more brain atlases (Pagani et al., <xref ref-type="bibr" rid="CR51">2016</xref>) and, afterwards, label candidates are refined or combined with, for instance, Markov random fields (Ma et al., <xref ref-type="bibr" rid="CR42">2014</xref>). As these approaches heavily rely on registration, they underperform in the presence of anatomical deformations. Second, methods that group nearby voxels with similar properties. These approaches typically start by proposing one or several candidate regions, and later adjust such regions with an energy function and, optionally, shape priors. Examples of these methods include surface deformation models (Wood et al., <xref ref-type="bibr" rid="CR64">2013</xref>), graph-based segmentation algorithms (Oguz et al., <xref ref-type="bibr" rid="CR48">2014</xref>), and a more recent approach that combines blobs into a single region (Liu et al., <xref ref-type="bibr" rid="CR40">2020</xref>). These approaches can handle different MRI contrasts and require no registration. However, they also rely on local features, such as nearby image gradients and intensities. Thus, these methods can be very sensitive to intensity inhomogeneities, and small brain deformities. Third, machine learning algorithms that classify brain features. These features can be handcrafted, such as in (Bae et al., <xref ref-type="bibr" rid="CR2">2009</xref>; Wu et al., <xref ref-type="bibr" rid="CR65">2012</xref>) where authors employed support vector machines to classify voxels into different neuroanatomical regions based on their intensity, location, neighbor labels, and probability maps. On the contrary, deep neural networks, a subclass of machine learning algorithms, can automatically find relevant features and learn meaningful non-linear relationships between such features. Methods based on neural networks, such as pulse-coupled neural networks (Chou et al., <xref ref-type="bibr" rid="CR13">2011</xref>; Murugavel and Sullivan, <xref ref-type="bibr" rid="CR46">2009</xref>) and ConvNets (Roy et al., <xref ref-type="bibr" rid="CR55">2018</xref>; Hsu et al., <xref ref-type="bibr" rid="CR26">2020</xref>; De Feo et al., <xref ref-type="bibr" rid="CR15">2021</xref>), have been used in the context of rodent MRI segmentation.</p>
      </sec>
      <sec id="FPar3">
        <title>
          <bold>Lesion segmentation of rodent brain MRI</bold>
        </title>
        <p id="Par8">The high contrast between lesion and non-lesion voxels in certain rodent brain MR images motivated the development of thresholding-based methods (Wang et al., <xref ref-type="bibr" rid="CR63">2007</xref>; Choi et al., <xref ref-type="bibr" rid="CR10">2018</xref>). However, these methods are not fully automatic, and they cannot be used in MR images with other contrasts, or lesions with different appearances. Mulder et al. (<xref ref-type="bibr" rid="CR45">2017</xref>) introduced a fully-automated pipeline to segment lesions via level sets (Dervieux &amp; Thomasset, <xref ref-type="bibr" rid="CR16">1980</xref>; Osher &amp; Sethian, <xref ref-type="bibr" rid="CR50">1988</xref>). Images were first registered to a template, then skull stripped, and their ventricles were segmented prior to the final lesion segmentation step. Arnaud et al. (<xref ref-type="bibr" rid="CR1">2018</xref>) framed lesion segmentation as an anomaly-detection problem and developed a pipeline that detects voxels with unusual intensity values with respect to healthy rodent brains. Valverde et al. (<xref ref-type="bibr" rid="CR61">2020</xref>) developed the first single-step method to segment rodent brain MRI lesions using ConvNets.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec3">
    <title>Materials and Methods</title>
    <sec id="Sec4">
      <title>MRI Data</title>
      <p id="Par9">
        <fig id="Fig1">
          <label>Fig. 1</label>
          <caption>
            <p><bold>A</bold>: Cohorts, acquisition time-points, and number of images. <bold>B</bold>: Example slice from each lesion stage in approximately the same brain area. The lesion border of the manual segmentation is outlined in green</p>
          </caption>
          <graphic xlink:href="12021_2022_9607_Fig1_HTML" id="MO1"/>
        </fig>
      </p>
      <p id="Par10">The image data, provided by Charles River Laboratories Discovery site (Kuopio, Finland)<xref ref-type="fn" rid="Fn1">1</xref>, consisted of 723 MR T2-weighted brain scans of 481 adult male Wistar rats weighting between 250-300 g derived from 11 different cohorts. Rats were induced focal cerebral ischemia by middle cerebral artery occlusion for 120 minutes in the right hemisphere of the brain (Koizumi et al., <xref ref-type="bibr" rid="CR34">1986</xref>). MR data was acquired at multiple time-points after the occlusion; for each of the 11 cohorts, time-points were different (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>A for details). In total, our dataset contained MR images from nine lesion stages: shams, 2h, 24h, D3, D7, D14, D21, D28, and D35. Figure <xref rid="Fig1" ref-type="fig">1</xref>B shows representative images of these lesion stages in approximately the same brain area. All animal experiments were conducted according to the National Institute of Health (NIH) guidelines for the care and use of laboratory animals, and approved by the National Animal Experiment Board, Finland. Multi-slice multi-echo sequence was used with the following parameters; TR = 2.5 s, 12 echo times (10-120 ms in 10 ms steps) and 4 averages in a horizontal 7T magnet. T2-weighted images were calculated as the sum of the all echoes. Eighteen coronal slices of 1 mm thickness were acquired using a field-of-view of 30x30 mm<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^2$$\end{document}</tex-math><mml:math id="M2"><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq1.gif"/></alternatives></inline-formula> producing 256x256 imaging matrices of resolution <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$117 \times 117 \mu m$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mn>117</mml:mn><mml:mo>×</mml:mo><mml:mn>117</mml:mn><mml:mi>μ</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq2.gif"/></alternatives></inline-formula> . Afterwards, these coronal slices were combined into a 3D volume.</p>
    </sec>
    <sec id="Sec5">
      <title>Data Preparation</title>
      <p id="Par11">The T2-weighted MRI volumes were not preprocessed (i.e., no registration, bias-field or artifact correction), and their intensity values were standardized to have zero mean and unit variance. Brain and contralateral hemisphere masks were annotated by several trained technicians employed by Charles River according to a standard operating procedure. These annotations did not include the cerebellum and the olfactory bulb. Finally, we computed the ipsilateral hemisphere mask by subtracting the contralateral hemisphere from the brain mask, yielding non-overlapping regions (i.e., the background, ipsilateral and contralateral hemispheres) for optimizing the ConvNets.</p>
    </sec>
    <sec id="Sec6">
      <title>Train, Validation and Test Sets</title>
      <p id="Par12">We divided the MR images into a training set of 51 volumes, validation set of 17 volumes, and test set of 655 volumes. Specifically, we grouped the MR images by their cohort and acquisition time-point (Fig. <xref rid="Fig1" ref-type="fig">1</xref>A). From the resulting 17 subgroups, our training and validation sets comprised 3 and 1 MR images, respectively, per subgroup. Images from sham-operated animals were not included to the training and validation sets since our work focused on rat brains with lesions. The remaining 655 MR images, including shams, formed the independent test set. This splitting strategy aimed to create a diverse training set, as brain lesions have notably different T2-weighted MRI intensities depending on the lesion stage, and annotations can differ slightly across cohorts due to the task subjectivity and the consequent low inter-rater agreement (Mulder et al., <xref ref-type="bibr" rid="CR45">2017</xref>; Valverde et al., <xref ref-type="bibr" rid="CR61">2020</xref>).<fig id="Fig2"><label>Fig. 2</label><caption><p>MedicDeepLabv3+ architecture</p></caption><graphic xlink:href="12021_2022_9607_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>MedicDeepLabv3+</title>
      <p id="Par13">MedicDeepLabv3+ is a 3D fully convolutional neural network (ConvNet) based on DeepLabv3+ (Chen et al., <xref ref-type="bibr" rid="CR11">2018b</xref>) and UNet (Ronneberger et al., <xref ref-type="bibr" rid="CR53">2015</xref>). We chose DeepLabv3+ because of its excellent performance in semantic segmentation tasks, and we modified its last layers to resemble more closely to UNet, which is an architecture widely used in medical image segmentation. DeepLabv3+ first employs Xception (Chollet, <xref ref-type="bibr" rid="CR12">2017</xref>) to transform the input and reduce its dimensionality, and then it upsamples the transformed data, twice, by a factor of four. MedicDeepLabv3+ replaces these last layers (i.e., the decoder) with three stages of skip connections and convolutional layers, and, as we describe below, it incorporates custom spatial attention layers, enabling deep supervision (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>).</p>
      <sec id="Sec8">
        <title>Encoder</title>
        <p id="Par14">MedicDeepLabv3+ stacks several <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$3 \times 3 \times 3$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq3.gif"/></alternatives></inline-formula> convolutional layers, normalizes the data with Batch Normalization (Ioffe and Szegedy, <xref ref-type="bibr" rid="CR27">2015</xref>), and incorporates residual connections (He et al., <xref ref-type="bibr" rid="CR24">2016</xref>). Both batch normalization and residual connections are well established architectural components that have been shown to facilitate the optimization in deep ConvNets (Drozdzal et al., <xref ref-type="bibr" rid="CR19">2016</xref>; Li et al., <xref ref-type="bibr" rid="CR38">2018</xref>). The first layers of MedicDeepLabv3+ correspond to Xception (Chollet, <xref ref-type="bibr" rid="CR12">2017</xref>), which uses depthwise-separable convolutions instead of regular convolutions. These depthwise-separable convolutions are advantageous over regular convolutions as they can decouple channel and spatial information. This is achieved by separating the operations of a regular convolution into a spatial feature learning and a channel combination step, increasing the efficiency and performance of the model (Chollet, <xref ref-type="bibr" rid="CR12">2017</xref>).</p>
        <p id="Par15">MedicDeepLabv3+ utilizes dilated convolutions in the last layer of Xception. Dilated convolutions sample padded input patches and multiply the non-padded values with the convolution kernel, thus, expanding the receptive field of the network (Chen et al., <xref ref-type="bibr" rid="CR8">2014</xref>). In other words, dilated convolutions permit to adjust the area that influences the classification of each voxel, and, as increasing this area has shown to improve model performance, we opted to employ dilated convolutions as in DeepLabv3+ (Chen et al., <xref ref-type="bibr" rid="CR9">2017</xref>). After the Xception backbone, DeepLabv3+’s Atrous Spatial Pyramid Pooling (ASPP) module concatenates parallel branches of dilated convolutional layers with different dilation rates and an average pooling followed by trilinear interpolation. Then, a pointwise convolution combines and reduces the number of channels. To this step, the described architecture reduces the data dimensionality by a factor of 16.</p>
      </sec>
      <sec id="Sec9">
        <title>Decoder</title>
        <p id="Par16">We developed a new decoder for MedicDeepLabv3+ with more stages of skip connections and convolution blocks than DeepLabv3+. In each stage, feature maps are upsampled via trilinear interpolation and concatenated to previous feature maps from the encoder. Subsequently, <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$3 \times 3 \times 3$$\end{document}</tex-math><mml:math id="M8"><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq4.gif"/></alternatives></inline-formula> convolutions halve the number of channels (Fig. <xref rid="Fig2" ref-type="fig">2</xref>, blue blocks), and a ResNet block (He et al., <xref ref-type="bibr" rid="CR24">2016</xref>) further transforms the data (Fig. <xref rid="Fig2" ref-type="fig">2</xref>, orange blocks). The consequent increase of skip-connections facilitates MedicDeepLabv3+ optimization (Drozdzal et al., <xref ref-type="bibr" rid="CR19">2016</xref>; Li et al., <xref ref-type="bibr" rid="CR38">2018</xref>). Importantly, DeepLabv3+ produces segmentations at <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times 4$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mo>×</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq5.gif"/></alternatives></inline-formula> less resolution than the original images that, to match their size, are upsampled via interpolation. In contrast, our MedicDeepLabv3+ incorporates more convolutional layers at the end of its architecture to perform a final data transformation at the same resolution as the input.<fig id="Fig3"><label>Fig. 3</label><caption><p>Spatial attention block (details in Sect. <xref rid="Sec9" ref-type="sec">2.4.2</xref>)</p></caption><graphic xlink:href="12021_2022_9607_Fig3_HTML" id="MO3"/></fig></p>
        <p id="Par17">Another key difference with respect to DeepLabv3+ is that MedicDeepLabv3+ utilizes spatial attention layers (Oktay et al., <xref ref-type="bibr" rid="CR49">2018</xref>; Wang et al., <xref ref-type="bibr" rid="CR62">2019</xref>). Attention layers behave as dynamic activation functions that first learn and then apply voxel-wise importance maps, transforming feature maps differently based on their values. Consequently, attention layers permit to learn more complex relations in the data. Furthermore, attention layers have been shown to aid the network to identify and focus on the most important features, improving performance (Fu et al., <xref ref-type="bibr" rid="CR22">2019</xref>). In our implementation, these layers (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) transform the inputs with a depthwise-separable convolution and, subsequently, average the resulting feature maps. Afterwards, a sigmoid activation function transforms the data non-linearly, producing spatial attention maps with values in the range [0, 1]. Then, these attention maps multiply the input feature maps voxel-wise. To encourage spatial attention maps that lead to the ground truth and to further facilitate the optimization, we added a branch in the first two attention layers for generating downsampled probability maps of the segmentation masks, enabling deep supervision (Fig. <xref rid="Fig3" ref-type="fig">3</xref>, red and pink arrows).</p>
      </sec>
      <sec id="Sec10">
        <title>Loss Function</title>
        <p id="Par18">We trained MedicDeepLabv3+ with deep supervision (Lee et al., <xref ref-type="bibr" rid="CR37">2015</xref>), i.e., we minimized the sum of cross entropy and Dice loss of all outputs of MedicDeepLabv3+ (Fig <xref rid="Fig2" ref-type="fig">2</xref>, pink arrow). Formally, we minimized <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L = \sum _{s \in S} L_{CE}^s + L_{Dice}^s$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">CE</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">Dice</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq6.gif"/></alternatives></inline-formula> with <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S = \{1,2,3\}$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq7.gif"/></alternatives></inline-formula> indicating each MedicDeepLabv3+ output (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>). Cross entropy treats the model predictions and the ground truth as distributions<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{CE}=-\frac{1}{NC} \sum _{i=1}^{N} \sum _{c=1}^{C} p_{i,c} \log (q_{i,c}), \end{aligned}$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">CE</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi mathvariant="italic">NC</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>log</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12021_2022_9607_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p_{i,c} \in \{0, 1\}$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq8.gif"/></alternatives></inline-formula> represents whether voxel <italic>i</italic> belongs to class <italic>c</italic>, and <inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$q_{i,c} \in [0, 1]$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq9.gif"/></alternatives></inline-formula> its predicted Softmax probability. <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C = 3$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq10.gif"/></alternatives></inline-formula> for background, ipsilateral and contralateral hemisphere classes, and <italic>N</italic> is the total number of voxels. Dice loss estimates the Dice coefficient between the predictions and the ground truth:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} L_{Dice}=1 - \frac{2}{C} \sum _{c=1}^C \frac{\sum _{i}^{N} p_{i,c} q_{i,c}}{\sum _{i}^{N} p_{i,c}^{2}+ q_{i,c}^{2}}. \end{aligned}$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">Dice</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>C</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>C</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12021_2022_9607_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par19">Minimizing cross entropy and Dice loss is a common practice in medical image segmentation (Myronenko &amp; Hatamizadeh, <xref ref-type="bibr" rid="CR47">2019</xref>; Isensee et al., <xref ref-type="bibr" rid="CR29">2021</xref>). Cross entropy optimization reduces the difference between the ground truth and prediction distributions. Dice loss optimization increases the Dice coefficient that we ultimately aim to maximize and it is particularly beneficial in class-imbalanced datasets (Milletari et al., <xref ref-type="bibr" rid="CR44">2016</xref>). Additionally, their optimization at different stages via deep supervision is equivalent to adding shortcut connections to propagate the gradients to various layers, facilitating the optimization of those layers.</p>
      </sec>
    </sec>
    <sec id="Sec11">
      <title>Experimental Design</title>
      <sec id="Sec12">
        <title>Metrics</title>
        <p id="Par20">We assessed the automatic segmentations with Dice coefficient (Dice, <xref ref-type="bibr" rid="CR17">1945</xref>), Hausdorff distance (Rote, <xref ref-type="bibr" rid="CR54">1991</xref>), precision, and recall. Dice coefficient measures the overlapping volume between the ground truth and the prediction<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Dice(A, B) = \frac{2|A \cap B|}{|A| + |B|}, \end{aligned}$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">|</mml:mo><mml:mi>A</mml:mi><mml:mo>∩</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12021_2022_9607_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> and <italic>B</italic> are the segmentation masks. Hausdorff distance (HD) is a quality metric that calculates the distance to the misclassification located the farthest from the boundary masks. Formally:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} d(A, B)=\max \left\{ \max _{a \in \partial A} \min _{b \in \partial B}|b-a|, \max _{b \in \partial B} \min _{a \in \partial A}|a-b|\right\} , \end{aligned}$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo movablelimits="true">max</mml:mo><mml:mfenced close="}" open="{"><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>∂</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo movablelimits="true">min</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mi>∂</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>b</mml:mi><mml:mo>-</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>,</mml:mo></mml:mrow><mml:munder><mml:mo movablelimits="true">max</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mi>∂</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo movablelimits="true">min</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>∂</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>a</mml:mi><mml:mo>-</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12021_2022_9607_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq11"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\partial A$$\end{document}</tex-math><mml:math id="M30"><mml:mrow><mml:mi>∂</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq11.gif"/></alternatives></inline-formula> and <inline-formula id="IEq12"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\partial B$$\end{document}</tex-math><mml:math id="M32"><mml:mrow><mml:mi>∂</mml:mi><mml:mi>B</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq12.gif"/></alternatives></inline-formula> are the boundary voxels of <italic>A</italic> and <italic>B</italic>, respectively. In other words, HD provides the distance to the largest segmentation error. We provided HD values in mm and we accounted for voxel anisotropy. Finally, precision is the percentage of voxels accurately classified as brain/hemisphere, and recall is the percentage of brain/hemisphere voxels that were correctly identified:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Prec = \frac{TP}{TP+FP} \; Recall = \frac{TP}{TP+FN}. \end{aligned}$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mspace width="0.277778em"/><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12021_2022_9607_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec13">
        <title>Benchmarked Methods</title>
        <p id="Par21">We compared our MedicDeepLabv3+ with DeepLabv3+ baseline (Chen et al., <xref ref-type="bibr" rid="CR11">2018b</xref>), UNet (Ronneberger et al., <xref ref-type="bibr" rid="CR53">2015</xref>), HighRes3DNet (Li et al., <xref ref-type="bibr" rid="CR39">2017</xref>), V-Net (Milletari et al., <xref ref-type="bibr" rid="CR44">2016</xref>), VoxResNet (Chen et al., <xref ref-type="bibr" rid="CR7">2018</xref>), Demon (Roy et al., <xref ref-type="bibr" rid="CR55">2018</xref>), RATS (Oguz et al., <xref ref-type="bibr" rid="CR48">2014</xref>), and RBET (Wood et al., <xref ref-type="bibr" rid="CR64">2013</xref>). Since Demon, RATS, and RBET were exclusively designed for rodent skull stripping, we computed contralateral hemisphere masks only with MedicDeepLabv3+, DeepLabv3+, UNet, HighRes3DNet, V-Net, and VoxResNet. MedicDeepLabv3+ and all the other ConvNets were optimized on our training set with Adam (Kingma &amp; Ba, <xref ref-type="bibr" rid="CR33">2014</xref>) (<inline-formula id="IEq13"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta _1 = 0.9$$\end{document}</tex-math><mml:math id="M36"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq13.gif"/></alternatives></inline-formula>,<inline-formula id="IEq14"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\beta _2 = 0.999$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq14.gif"/></alternatives></inline-formula>, <inline-formula id="IEq15"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\epsilon = 10^{-8}$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq15.gif"/></alternatives></inline-formula>), starting with a learning rate of <inline-formula id="IEq16"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^{-5}$$\end{document}</tex-math><mml:math id="M42"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq16.gif"/></alternatives></inline-formula>. ConvNets’ hyper-parameters (e.g., activation functions, number of filters, number of convolutional layers, normalization layers, type of downsampling/upsampling layers, dilation rate in atrous convolution layers, kernel size) were set as instructed in the articles proposing the method (i.e., DeepLabv3+ (Chen et al., <xref ref-type="bibr" rid="CR11">2018b</xref>), UNet (Ronneberger et al., <xref ref-type="bibr" rid="CR53">2015</xref>), HighRes3DNet (Li et al., <xref ref-type="bibr" rid="CR39">2017</xref>), V-Net (Milletari et al., <xref ref-type="bibr" rid="CR44">2016</xref>), VoxResNet (Chen et al., <xref ref-type="bibr" rid="CR7">2018a</xref>), Demon (Roy et al., <xref ref-type="bibr" rid="CR55">2018</xref>). As such, these hyper-parameters can be considered to be close to optimal. MedicDeepLabv3+, DeepLabv3+, HighRes3DNet, V-Net, VoxResNet and UNet were trained for 300 epochs, and Demon was trained for an equivalent amount of time. All ConvNets were either 2D or 3D; we disregarded 2.5D ConvNets (e.g., Kushibar et al. (<xref ref-type="bibr" rid="CR35">2018</xref>)) that concatenate three orthogonal 2D images to classify the voxel at their intersection since they are more inefficient at inference time than 2D and 3D ConvNets. We ensembled three models (Dietterich, <xref ref-type="bibr" rid="CR18">2000</xref>) since this strategy markedly improved segmentation performance in our previous work (Valverde et al., <xref ref-type="bibr" rid="CR61">2020</xref>). More specifically, we trained each ConvNet three times, separately, starting from different random initializations. Then, we formed the final segmentations based on the majority vote from the binarized outputs of the three trained models.</p>
        <p id="Par22">We conducted a grid-search for best hyperparameters for RATS and RBET. We performed the grid-search using merged training and validation sets as RATS and RBET do not involve supervised learning, thus making it possible to use also the training set for hyper-parameter tuning. Subsequently, we utilized the best-performing hyper-parameters on the test set. With RATS, computing the brain mask <inline-formula id="IEq17"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}$$\end{document}</tex-math><mml:math id="M44"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq17.gif"/></alternatives></inline-formula> of image <italic>x</italic> requires setting three hyper-parameters: intensity threshold <italic>t</italic>, <inline-formula id="IEq18"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M46"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq18.gif"/></alternatives></inline-formula>, and rodent brain volume <italic>s</italic>, i.e., <inline-formula id="IEq19"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y} = RATS(x, t, \alpha , s)$$\end{document}</tex-math><mml:math id="M48"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq19.gif"/></alternatives></inline-formula>. As rat brain volumes are highly similar in adult rats, we left this hyper-parameter with its default value, <inline-formula id="IEq20"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s=1650$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1650</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq20.gif"/></alternatives></inline-formula> mm<inline-formula id="IEq21"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^3$$\end{document}</tex-math><mml:math id="M52"><mml:msup><mml:mrow/><mml:mn>3</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq21.gif"/></alternatives></inline-formula>. Thus, we only optimized for the threshold <italic>t</italic> and <inline-formula id="IEq22"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><mml:math id="M54"><mml:mi>α</mml:mi></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq22.gif"/></alternatives></inline-formula> hyper-parameters. Since RATS assumes that all intensity values are positive integers, we employed unnormalized images with RATS. We optimized RATS hyper-parameters by maximizing the Dice coefficients in the training and validation sets:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \underset{i, \alpha }{\arg \max } \sum _{x \in X_{train+val}} Dice(y, RATS(x, t, \alpha , 1650)) : t = P_{\%i}, \end{aligned}$$\end{document}</tex-math><mml:math id="M56" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:munder><mml:mrow><mml:mo>arg</mml:mo><mml:mo movablelimits="true">max</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>R</mml:mi><mml:mi>A</mml:mi><mml:mi>T</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mn>1650</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>%</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12021_2022_9607_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>where <italic>Dice</italic> is the Dice coefficient (Eq. (<xref rid="Equ3" ref-type="disp-formula">3</xref>)) between the ground-truth brain mask <italic>y</italic> and RATS’ output, <inline-formula id="IEq23"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha = 0, 1, \ldots , 10$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq23.gif"/></alternatives></inline-formula> balances the importance between gradients and intensity values, and <inline-formula id="IEq24"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P_{\%i}$$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo>%</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq24.gif"/></alternatives></inline-formula> is the <italic>i</italic>th percentile of <italic>x</italic> with <inline-formula id="IEq25"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i = 0.01, 0.02, \ldots , 0.99$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn><mml:mo>,</mml:mo><mml:mn>0.02</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq25.gif"/></alternatives></inline-formula>. Since finding <italic>t</italic> is potentially suboptimal due to the distribution variability across images, we optimized for the <italic>i</italic>th percentile, yielding image-specific thresholds. In total, our hyper-parameter grid search in RATS comprised 1089 different parameter value combinations. For RBET, we optimized the Dice coefficient to find the optimal ellipse axes ratio <italic>w</italic>:<italic>h</italic>:<italic>d</italic> with <italic>w</italic>, <italic>h</italic>, <italic>d</italic> from 0.1 to 1 in steps of 0.05, accounting for <inline-formula id="IEq26"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$19^3$$\end{document}</tex-math><mml:math id="M64"><mml:msup><mml:mn>19</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq26.gif"/></alternatives></inline-formula> different configurations. Note that, despite optimizing over a large number of hyper-parameter choices may increase the risk to overfit, our train, validation and test sets were derived so that <inline-formula id="IEq27"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{train+val}$$\end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq27.gif"/></alternatives></inline-formula> is a good representation of <inline-formula id="IEq28"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{test}$$\end{document}</tex-math><mml:math id="M68"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi mathvariant="italic">test</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq28.gif"/></alternatives></inline-formula> (see Sec. <xref rid="Sec6" ref-type="sec">2.3</xref>).</p>
        <p id="Par23">Unlike ConvNets that can be optimized to segment specific brain regions, RATS and RBET perform skull stripping, segmenting also the cerebellum and olfactory bulb that were not annotated. As these brain areas were not part of our ground truth, RATS and RBET segmentations would be unnecessarily penalized in those areas. Thus, before computing the metrics, we discarded the slices containing cerebellum and olfactory bulb. This evaluation strategy ignores potential misclassifications in the excluded slices, slightly favoring RATS and RBET.</p>
        <p id="Par24">We tested whether the difference in Dice coefficient and HD between our MedicDeepLabv3+ and the other methods was significant. The hypothesis was tested with a paired two-sample permutation test using the mean-absolute difference as the test statistic. We considered <italic>p</italic>-values smaller than 0.05 as statistically significant.</p>
      </sec>
      <sec id="Sec14">
        <title>Brain Midline Evaluation</title>
        <p id="Par25">We calculated the average Dice coefficients of contra- and ipsilateral hemispheres around the brain midline—boundary between both hemispheres (see Fig. <xref rid="Fig4" ref-type="fig">4</xref>A). Specifically, we considered the volume after expanding brain midline voxels in the coronal plane via morphological dilation <italic>n</italic> times, with <inline-formula id="IEq29"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n = 1, 2, \ldots , 10$$\end{document}</tex-math><mml:math id="M70"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12021_2022_9607_Article_IEq29.gif"/></alternatives></inline-formula>. In contrast to brain vs. non-brain tissue boundaries, the brain midline volume is more ambiguous to annotate due to the lower intensity contrast between hemispheres, hence the importance to assess the performance in this area. This evaluation aims to supplement computing Dice coefficient and HD on the whole 3D volumes. Since most of the voxels lie within the hemisphere borders, Dice coefficients tend to be very high, and since HD might indicate the distance to a misclassification that can be easily corrected via postprocessing (e.g., a misclassification outside the brain), HD alone does not suffice to assess specific areas. Note that, similarly to RATS and RBET evaluation, this experiment computed the Dice coefficient only on the slices that were manually annotated, as finding the brain midline requires these manual annotations. Consequently, the evaluated 3D masks excluded non-annotated slices that could have false positives.</p>
      </sec>
      <sec id="Sec15">
        <title>Biomarkers Based on Hemisphere Segmentation</title>
        <p id="Par26">Since the ratio between contra- and ipsilateral hemispheres volume is an important biomarker for acute stroke (Swanson et al., <xref ref-type="bibr" rid="CR57">1990</xref>; Gerriets et al., <xref ref-type="bibr" rid="CR23">2004</xref>), we compared the hemisphere volume ratio of the ground truth with the hemisphere volume ratio of the automatic segmentations. For this, we computed the effect size via Cohen’s d (Lakens, <xref ref-type="bibr" rid="CR36">2013</xref>) and the bias-corrected and accelerated (BCa) bootstrap confidence intervals (Efron, <xref ref-type="bibr" rid="CR20">1987</xref>) with 100000 bootstrap resamples. An effect size close to zero with a narrow confidence interval indicates a high similarity between automated and manual segmentation based biomarkers.</p>
      </sec>
      <sec id="Sec16">
        <title>Performance with Limited GPU Memory and Data</title>
        <p id="Par27">Motivated by potential GPU memory limitations, we studied the performance and computational requirements of multiple versions of MedicDeepLabv3+ with lower capacity and, consequently, lower GPU memory usage. To investigate this, we varied the number of kernel filters in all convolutions of MedicDeepLabv3+ that determines the number of parameters. For instance, decreasing the number of kernel filters by half in the encoder also decreases the number of kernel filters in the decoder to half.</p>
        <p id="Par28">Separately, we evaluated the proposed MedicDeepLabv3+ on each cohort and time-point independently, simulating the typical scenario in rodent studies with extremely scarce annotated data. For each of the 17 groups containing no sham animals (Fig. <xref rid="Fig1" ref-type="fig">1</xref>A), we trained an ensemble of three MedicDeepLabv3+ on <italic>only three images</italic>, employed another image for validation during the optimization, and we evaluated this ensemble on the remaining holdout images from the same group.<fig id="Fig4"><label>Fig. 4</label><caption><p><bold>A</bold>: Example of ground truth and its brain midline area after four (red) and ten (green) iterations of morphological dilation. <bold>B</bold>-<bold>C</bold>: Dice coefficients for the ipsi- and contralateral hemisphere classes in the brain midline area with different morphological dilation iterations (brain midline area sizes)</p></caption><graphic xlink:href="12021_2022_9607_Fig4_HTML" id="MO10"/></fig></p>
      </sec>
      <sec id="Sec17">
        <title>Implementation</title>
        <p id="Par29">MedicDeepLabv3+, DeepLabv3+, UNet, HighRes3DNet, V-Net, VoxResNet, and Demon were implemented in Pytorch (Paszke et al., <xref ref-type="bibr" rid="CR52">2019</xref>) and were run on Ubuntu 16.04 with an Intel Xeon W-2125 CPU @ 4.00GHz processor, 64 GB of memory and an NVidia GeForce GTX 1080 Ti with 11 GB of memory. MedicDeepLabv3+ and the scripts for segmenting rat MR images and to optimize new models are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jmlipman/MedicDeepLabv3Plus">https://github.com/jmlipman/MedicDeepLabv3Plus</ext-link>. These scripts are ready for use via command line interface with a single command, and users can easily adjust the number of initial filters that controls the model size, capacity, and GPU memory requirements. Additionally, we provide the optimized parameters (i.e., the weights) of MedicDeepLabv3+ at <ext-link ext-link-type="uri" xlink:href="https://github.com/jmlipman/MedicDeepLabv3Plus">https://github.com/jmlipman/MedicDeepLabv3Plus</ext-link>.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec18">
    <title>Results</title>
    <sec id="Sec19">
      <title>Segmentation Metrics Comparison</title>
      <p id="Par30">Our MedicDeepLabv3+ produced brain and hemisphere masks with the highest Dice coefficients (0.952 and 0.944) and precision (0.94 and 0.94), and the lowest HD (1.856 and 2.064) (see Table <xref rid="Tab1" ref-type="table">1</xref>). MedicDeepLabv3+ also achieved the highest Dice coefficients in the brain and contralateral hemisphere most frequently, in 38% and 36% of the test images, respectively, followed by UNet (24% and 26%), VNet (13% and 13%), VoxResNet (13% and 12%), HighRes3DNet (11% and 12%) and the others (1% or less). In the majority of cases, MedicDeepLabv3+ produced segmentations with Dice and HD significantly better than the compared methods (see Table <xref rid="Tab1" ref-type="table">1</xref>). All ConvNets performed better than RATS and RBET and, particularly, 3D ConvNets (MedicDeepLabv3+, DeepLabv3+, HighRes3DNet, V-Net, and VoxResNet) consistently yielded lower HD than 2D ConvNets (UNet, Demon). Our MedicDeepLabv3+ produced finer segmentations that were more similar to the ground truth than the baseline DeepLabv3+ which generated masks with imprecise borders. UNet also produced segmentations with higher Dice and recall than DeepLabv3+, although UNet HD was considerably lower. HighRes3DNet, V-Net, and VoxResNet yielded slightly worse Dice coefficients and HDs than MedicDeepLabv3+. Figure <xref rid="Fig5" ref-type="fig">5</xref> illustrates these results on the MR image with the highest hemispheric volume imbalance. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows that RBET was incapable of finding the brain boundaries; RATS produced segmentations with several holes and non-smooth borders; 2D ConvNets misclassified the olfactory bulb and cerebellum; and, in agreement with Table <xref rid="Tab1" ref-type="table">1</xref>, MedicDeepLabv3+ produced the segmentation mask most similar to the ground truth. We included 17 images (one per cohort and lesion time-point) in <xref rid="MOESM2" ref-type="media">Online Resource 1</xref> that also corroborate the higher performance of MedicDeepLabv3+. The computation time to optimize these methods also varied notably: on average, ConvNets required 16 hours, and RATS and RBET needed six days. Furthermore, MedicDeepLabv3+ segmented the images in real time, requiring approximately 0.4 seconds per image.</p>
    </sec>
    <sec id="Sec20">
      <title>Brain Midline Experiment</title>
      <p id="Par31">Regarding the brain midline area experiment (Sect. <xref rid="Sec14" ref-type="sec">2.5.3</xref>, Fig. <xref rid="Fig4" ref-type="fig">4</xref>B,C), MedicDeepLabv3+ outperformed the baseline DeepLabv3+ across different area sizes (average Dice coefficient difference of 0.07). VoxResNet, HighRes3DNet, V-Net, and MedicDeepLabv3+ yielded very similar Dice coefficients, and UNet produced the highest Dice coefficients by a small margin (average difference between UNet and MedicDeepLabv3+ of only 0.02). Additionally, Dice coefficients were similar across hemispheres regardless of the segmentation method.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Dice coefficients, Hausdorff distances (HD), precision, and recall of the brain and contralateral hemisphere (CH) masks derived from the evaluated methods (mean ± std). Bold: best scores. Scores that were significantly different from our MedicDeepLabv3+ were marked with * (<italic>p</italic>-value &lt; 0.05)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Approach</th><th align="left">Dice</th><th align="left">HD</th><th align="left">Prec</th><th align="left">Recall</th></tr></thead><tbody><tr><td align="left">Brain</td><td align="left">MedicDeepLabv3+</td><td char="±" align="char"><bold>0.952 ± 0.04</bold></td><td char="±" align="char"><bold>1.856 ± 0.91</bold></td><td char="±" align="char">0.94 ± 0.07</td><td char="±" align="char">0.97 ± 0.03</td></tr><tr><td align="left"/><td align="left">VoxResNet</td><td char="±" align="char">0.951 ± 0.04</td><td char="±" align="char">2.042 ± 1.02*</td><td char="±" align="char">0.94 ± 0.07</td><td char="±" align="char">0.97 ± 0.02</td></tr><tr><td align="left"/><td align="left">HighRes3DNet</td><td char="±" align="char">0.949 ± 0.04*</td><td char="±" align="char">1.858 ± 1.04</td><td char="±" align="char">0.93 ± 0.07</td><td char="±" align="char">0.97 ± 0.02</td></tr><tr><td align="left"/><td align="left">V-Net</td><td char="±" align="char">0.948 ± 0.04*</td><td char="±" align="char">1.920 ± 1.05*</td><td char="±" align="char">0.94 ± 0.07</td><td char="±" align="char">0.97 ± 0.02</td></tr><tr><td align="left"/><td align="left">UNet</td><td char="±" align="char">0.947 ± 0.05*</td><td char="±" align="char">3.477 ± 1.20*</td><td char="±" align="char">0.93 ± 0.07</td><td char="±" align="char">0.97 ± 0.02</td></tr><tr><td align="left"/><td align="left">DeepLabv3+</td><td char="±" align="char">0.936 ± 0.04*</td><td char="±" align="char">2.149 ± 1.02*</td><td char="±" align="char">0.93 ± 0.07</td><td char="±" align="char">0.95 ± 0.03</td></tr><tr><td align="left"/><td align="left">Demon</td><td char="±" align="char">0.934 ± 0.04*</td><td char="±" align="char">3.621 ± 1.17*</td><td char="±" align="char">0.92 ± 0.07</td><td char="±" align="char">0.96 ± 0.02</td></tr><tr><td align="left"/><td align="left">RATS</td><td char="±" align="char">0.913 ± 0.01*</td><td char="±" align="char">2.221 ± 0.51*</td><td char="±" align="char">0.91 ± 0.03</td><td char="±" align="char">0.92 ± 0.02</td></tr><tr><td align="left"/><td align="left">RBET</td><td char="±" align="char">0.781 ± 0.10*</td><td char="±" align="char">3.628 ± 0.46*</td><td char="±" align="char">0.89 ± 0.05</td><td char="±" align="char">0.70 ± 0.10</td></tr><tr><td align="left">CH</td><td align="left">MedicDeepLabv3+</td><td char="±" align="char">0.944 ± 0.04</td><td char="±" align="char"><bold>2.064 ± 1.85</bold></td><td char="±" align="char"><bold>0.94 ± 0.08</bold></td><td char="±" align="char">0.96 ± 0.03</td></tr><tr><td align="left"/><td align="left">VoxResNet</td><td char="±" align="char">0.944 ± 0.04</td><td char="±" align="char">2.265 ± 1.86*</td><td char="±" align="char">0.93 ± 0.07</td><td char="±" align="char">0.96 ± 0.02</td></tr><tr><td align="left"/><td align="left">HighRes3DNet</td><td char="±" align="char">0.942 ± 0.04*</td><td char="±" align="char">2.205 ± 1.86*</td><td char="±" align="char">0.93 ± 0.07</td><td char="±" align="char">0.96 ± 0.03</td></tr><tr><td align="left"/><td align="left">V-Net</td><td char="±" align="char">0.940 ± 0.04*</td><td char="±" align="char">2.218 ± 1.86*</td><td char="±" align="char">0.93 ± 0.07</td><td char="±" align="char">0.96 ± 0.03</td></tr><tr><td align="left"/><td align="left">UNet</td><td char="±" align="char">0.941 ± 0.05*</td><td char="±" align="char">3.689 ± 1.64*</td><td char="±" align="char">0.92 ± 0.07</td><td char="±" align="char"><bold>0.97 ± 0.02</bold></td></tr><tr><td align="left"/><td align="left">DeepLabv3+</td><td char="±" align="char">0.921 ± 0.04*</td><td char="±" align="char">2.411 ± 1.80*</td><td char="±" align="char">0.91 ± 0.07</td><td char="±" align="char">0.94 ± 0.03</td></tr></tbody></table></table-wrap></p>
      <p id="Par32">
        <fig id="Fig5">
          <label>Fig. 5</label>
          <caption>
            <p>T2-weighted image, ground truth and automatic segmentations of the rat with the most imbalanced hemispheric volumes</p>
          </caption>
          <graphic xlink:href="12021_2022_9607_Fig5_HTML" id="MO11"/>
        </fig>
      </p>
    </sec>
    <sec id="Sec21">
      <title>Hemispheric Ratio Experiment</title>
      <p id="Par33">The computed Cohen’s d shows that, in terms of magnitude, all methods produced hemispheric ratio distributions not too different from the ground truth (Table <xref rid="Tab2" ref-type="table">2</xref>). Among these methods, MedicDeepLabv3+ and V-Net provided the smallest effect size and the most zero-centered confidence interval, with MedicDeepLabv3+’s confidence interval being narrower than V-Net’s. DeepLabv3+’s confidence interval was the largest and contained zero whereas UNet’s confidence interval was the narrowest—slightly narrower than MedicDeepLabv3+’s—and did not contain zero.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Cohen’s d that measured the effect size and its confidence intervals</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Approach</th><th align="left">Cohen’s d</th><th align="left">Confidence Interval</th></tr></thead><tbody><tr><td align="left">MedicDeepLabv3+</td><td char="." align="char">0.008</td><td align="left">[-0.013, 0.035]</td></tr><tr><td align="left">VoxResNet</td><td char="." align="char">-0.042</td><td align="left">[-0.060, -0.025]</td></tr><tr><td align="left">HighRes3DNet</td><td char="." align="char">-0.102</td><td align="left">[-0.125, -0.080]</td></tr><tr><td align="left">V-Net</td><td char="." align="char">0.003</td><td align="left">[-0.042, 0.022]</td></tr><tr><td align="left">UNet</td><td char="." align="char">-0.038</td><td align="left">[-0.054, -0.021]</td></tr><tr><td align="left">DeepLabv3+</td><td char="." align="char">0.050</td><td align="left">[-0.008, 0.099]</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec22">
      <title>Limited Resources</title>
      <p id="Par34">Table <xref rid="Tab3" ref-type="table">3</xref> lists the characteristics, computational requirements, and performance of different versions of MedicDeepLabv3+ on the contralateral hemisphere segmentation (performance on the brain can be found in <xref rid="MOESM3" ref-type="media">Online Resource 2</xref>). Reducing the number of parameters by decreasing the number of initial filters reduced notably the required GPU memory and training time while it barely affected MedicDeepLabv3+’s performance. For instance, reducing the number of parameters by 93.5% (from 79.1M to 5.1M) decreased the required GPU memory and training time by 72% while it decreased the Dice coefficient in the contralateral hemisphere by only 1%.</p>
      <p id="Par35">Tables <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref> show the performance of MedicDeepLabv3+ optimized and evaluated on each cohort and acquisition time-point separately. In other words, for each cohort and acquisition time-point, the training set was comprised by only three images and test set size (Tables <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>, “Volumes” column) varied across the 17 groups. MedicDeepLabv3+, on average, performed slightly worse than in our first experiment that utilized 17 times more annotated data. Performance measures across these groups varied notably: in the contralateral hemisphere segmentations (Table <xref rid="Tab5" ref-type="table">5</xref>) Dice coefficients ranged from 0.876 to 0.951, HD from 1.200 to 3.745, precision from 0.871 to 0.962, and recall from 0.859 to 0.967. Additionally, in agreement with our previous experiment, performance on the contralateral hemisphere was slightly lower than on the brain.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison between multiple versions of MedicDeepLabv3+ with different capacity. Columns: proportion of kernel filters with respect to the default configuration, trainable ConvNet parameters (in millions), optimization time for 300 epochs in our workstation (see Sect. <xref rid="Sec17" ref-type="sec">2.5.6</xref> for details) in hours, maximum GPU memory required during training and evaluation, Dice and HD in the contralateral hemisphere (mean ± std). Bold: default configuration, highest performance</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Rate</th><th align="left">Parameters</th><th align="left">Time (h)</th><th align="left">Mem. (train)</th><th align="left">Mem. (eval)</th><th align="left">Dice</th><th align="left">HD</th></tr></thead><tbody><tr><td align="left"><bold>1</bold></td><td char="." align="char"><bold>79.1M</bold></td><td char="." align="char"><bold>16.2</bold></td><td align="left"><bold>8857 MiB</bold></td><td align="left"><bold>2935 MiB</bold></td><td char="±" align="char"><bold>0.944 ± 0.04</bold></td><td char="±" align="char"><bold>2.064 ± 1.85</bold></td></tr><tr><td align="left">0.875</td><td char="." align="char">60.7M</td><td char="." align="char">14.4</td><td align="left">7571 MiB</td><td align="left">2617 MiB</td><td char="±" align="char">0.941 ± 0.04</td><td char="±" align="char">2.103 ± 1.86</td></tr><tr><td align="left">0.750</td><td char="." align="char">44.7M</td><td char="." align="char">12.1</td><td align="left">6545 MiB</td><td align="left">2319 MiB</td><td char="±" align="char">0.941 ± 0.04</td><td char="±" align="char">2.118 ± 1.86</td></tr><tr><td align="left">0.625</td><td char="." align="char">31.1M</td><td char="." align="char">10.3</td><td align="left">5619 MiB</td><td align="left">2007 MiB</td><td char="±" align="char">0.941 ± 0.04</td><td char="±" align="char">2.099 ± 1.85</td></tr><tr><td align="left">0.500</td><td char="." align="char">20.0M</td><td char="." align="char">7.8</td><td align="left">4577 MiB</td><td align="left">1717 MiB</td><td char="±" align="char">0.939 ± 0.05</td><td char="±" align="char">2.099 ± 1.83</td></tr><tr><td align="left">0.375</td><td char="." align="char">11.3M</td><td char="." align="char">6.2</td><td align="left">3531 MiB</td><td align="left">1421 MiB</td><td char="±" align="char">0.937 ± 0.05</td><td char="±" align="char">2.138 ± 1.85</td></tr><tr><td align="left">0.250</td><td char="." align="char">5.1M</td><td char="." align="char">4.5</td><td align="left">2503 MiB</td><td align="left">1121 MiB</td><td char="±" align="char">0.933 ± 0.05</td><td char="±" align="char">2.078 ± 1.82</td></tr></tbody></table></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Dice, Hausdorff distance (HD), precision, and recall on the brain masks derived with MedicDeepLabv3+ in each cohort and time-point (TP) separately. Volumes indicate the number of volumes in the test set. Mean ± std</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Cohort</th><th align="left">TP</th><th align="left">Volumes</th><th align="left">Dice</th><th align="left">HD</th><th align="left">Prec</th><th align="left">Recall</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">2h</td><td align="left">8</td><td char="±" align="char">0.916 ± 0.03</td><td char="±" align="char">2.018 ± 0.47</td><td char="±" align="char">0.929 ± 0.08</td><td char="±" align="char">0.913 ± 0.08</td></tr><tr><td align="left">1</td><td align="left">24h</td><td align="left">8</td><td char="±" align="char">0.912 ± 0.10</td><td char="±" align="char">2.581 ± 1.82</td><td char="±" align="char">0.894 ± 0.18</td><td char="±" align="char">0.952 ± 0.02</td></tr><tr><td align="left">2</td><td align="left">24h</td><td align="left">13</td><td char="±" align="char">0.915 ± 0.02</td><td char="±" align="char">3.745 ± 1.12</td><td char="±" align="char">0.929 ± 0.07</td><td char="±" align="char">0.909 ± 0.05</td></tr><tr><td align="left">3</td><td align="left">D35</td><td align="left">16</td><td char="±" align="char">0.949 ± 0.02</td><td char="±" align="char">2.142 ± 0.44</td><td char="±" align="char">0.956 ± 0.03</td><td char="±" align="char">0.942 ± 0.02</td></tr><tr><td align="left">4</td><td align="left">24h</td><td align="left">40</td><td char="±" align="char">0.957 ± 0.01</td><td char="±" align="char">1.981 ± 0.87</td><td char="±" align="char">0.985 ± 0.01</td><td char="±" align="char">0.930 ± 0.02</td></tr><tr><td align="left">5</td><td align="left">24h</td><td align="left">23</td><td char="±" align="char">0.947 ± 0.01</td><td char="±" align="char">1.557 ± 0.43</td><td char="±" align="char">0.935 ± 0.03</td><td char="±" align="char">0.960 ± 0.02</td></tr><tr><td align="left">6</td><td align="left">D3</td><td align="left">60</td><td char="±" align="char">0.973 ± 0.01</td><td char="±" align="char">1.200 ± 0.32</td><td char="±" align="char">0.968 ± 0.02</td><td char="±" align="char">0.978 ± 0.01</td></tr><tr><td align="left">6</td><td align="left">D28</td><td align="left">58</td><td char="±" align="char">0.946 ± 0.02</td><td char="±" align="char">1.688 ± 0.60</td><td char="±" align="char">0.937 ± 0.04</td><td char="±" align="char">0.957 ± 0.02</td></tr><tr><td align="left">7</td><td align="left">D3</td><td align="left">35</td><td char="±" align="char">0.956 ± 0.01</td><td char="±" align="char">1.723 ± 0.92</td><td char="±" align="char">0.968 ± 0.02</td><td char="±" align="char">0.945 ± 0.03</td></tr><tr><td align="left">7</td><td align="left">D21</td><td align="left">35</td><td char="±" align="char">0.950 ± 0.01</td><td char="±" align="char">1.548 ± 0.75</td><td char="±" align="char">0.937 ± 0.02</td><td char="±" align="char">0.964 ± 0.01</td></tr><tr><td align="left">8</td><td align="left">24h</td><td align="left">29</td><td char="±" align="char">0.956 ± 0.01</td><td char="±" align="char">1.742 ± 0.92</td><td char="±" align="char">0.956 ± 0.03</td><td char="±" align="char">0.956 ± 0.02</td></tr><tr><td align="left">8</td><td align="left">D3</td><td align="left">26</td><td char="±" align="char">0.953 ± 0.01</td><td char="±" align="char">1.599 ± 0.35</td><td char="±" align="char">0.949 ± 0.02</td><td char="±" align="char">0.957 ± 0.01</td></tr><tr><td align="left">8</td><td align="left">D14</td><td align="left">26</td><td char="±" align="char">0.948 ± 0.01</td><td char="±" align="char">1.678 ± 0.43</td><td char="±" align="char">0.929 ± 0.03</td><td char="±" align="char">0.967 ± 0.01</td></tr><tr><td align="left">8</td><td align="left">D28</td><td align="left">23</td><td char="±" align="char">0.943 ± 0.02</td><td char="±" align="char">1.713 ± 0.36</td><td char="±" align="char">0.940 ± 0.03</td><td char="±" align="char">0.946 ± 0.03</td></tr><tr><td align="left">9</td><td align="left">24h</td><td align="left">77</td><td char="±" align="char">0.951 ± 0.03</td><td char="±" align="char">1.838 ± 1.02</td><td char="±" align="char">0.970 ± 0.04</td><td char="±" align="char">0.935 ± 0.04</td></tr><tr><td align="left">10</td><td align="left">D7</td><td align="left">36</td><td char="±" align="char">0.919 ± 0.05</td><td char="±" align="char">2.226 ± 0.81</td><td char="±" align="char">0.880 ± 0.10</td><td char="±" align="char">0.970 ± 0.02</td></tr><tr><td align="left">11</td><td align="left">24h</td><td align="left">28</td><td char="±" align="char">0.937 ± 0.02</td><td char="±" align="char">2.696 ± 0.79</td><td char="±" align="char">0.954 ± 0.04</td><td char="±" align="char">0.923 ± 0.04</td></tr><tr><td align="left" colspan="2">Average</td><td align="left">541</td><td char="±" align="char">0.948 ± 0.03</td><td char="±" align="char">1.833 ± 0.89</td><td char="±" align="char">0.949 ± 0.05</td><td char="±" align="char">0.951 ± 0.03</td></tr></tbody></table></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Dice, Hausdorff distance (HD), precision, and recall on the contralateral hemisphere masks derived with MedicDeepLabv3+ in each cohort and time-point (TP) separately. Volumes indicate the number of volumes in the test set. Mean ± std</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Cohort</th><th align="left">TP</th><th align="left">Volumes</th><th align="left">Dice</th><th align="left">HD</th><th align="left">Prec</th><th align="left">Recall</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">2h</td><td align="left">8</td><td char="±" align="char">0.883 ± 0.03</td><td char="±" align="char">3.593 ± 0.83</td><td char="±" align="char">0.871 ± 0.06</td><td char="±" align="char">0.904 ± 0.07</td></tr><tr><td align="left">1</td><td align="left">24h</td><td align="left">8</td><td char="±" align="char">0.886 ± 0.11</td><td char="±" align="char">3.181 ± 1.76</td><td char="±" align="char">0.874 ± 0.18</td><td char="±" align="char">0.915 ± 0.03</td></tr><tr><td align="left">2</td><td align="left">24h</td><td align="left">13</td><td char="±" align="char">0.876 ± 0.03</td><td char="±" align="char">3.792 ± 1.60</td><td char="±" align="char">0.902 ± 0.08</td><td char="±" align="char">0.859 ± 0.05</td></tr><tr><td align="left">3</td><td align="left">D35</td><td align="left">16</td><td char="±" align="char">0.927 ± 0.02</td><td char="±" align="char">1.977 ± 0.94</td><td char="±" align="char">0.948 ± 0.03</td><td char="±" align="char">0.907 ± 0.03</td></tr><tr><td align="left">4</td><td align="left">24h</td><td align="left">40</td><td char="±" align="char">0.928 ± 0.02</td><td char="±" align="char">1.889 ± 0.76</td><td char="±" align="char">0.962 ± 0.03</td><td char="±" align="char">0.898 ± 0.03</td></tr><tr><td align="left">5</td><td align="left">24h</td><td align="left">23</td><td char="±" align="char">0.899 ± 0.04</td><td char="±" align="char">1.630 ± 0.48</td><td char="±" align="char">0.912 ± 0.04</td><td char="±" align="char">0.888 ± 0.06</td></tr><tr><td align="left">6</td><td align="left">D3</td><td align="left">60</td><td char="±" align="char">0.951 ± 0.02</td><td char="±" align="char">2.766 ± 2.27</td><td char="±" align="char">0.936 ± 0.04</td><td char="±" align="char">0.967 ± 0.01</td></tr><tr><td align="left">6</td><td align="left">D28</td><td align="left">58</td><td char="±" align="char">0.935 ± 0.02</td><td char="±" align="char">1.387 ± 0.51</td><td char="±" align="char">0.932 ± 0.04</td><td char="±" align="char">0.939 ± 0.02</td></tr><tr><td align="left">7</td><td align="left">D3</td><td align="left">35</td><td char="±" align="char">0.930 ± 0.02</td><td char="±" align="char">2.038 ± 1.40</td><td char="±" align="char">0.959 ± 0.02</td><td char="±" align="char">0.904 ± 0.04</td></tr><tr><td align="left">7</td><td align="left">D21</td><td align="left">35</td><td char="±" align="char">0.939 ± 0.01</td><td char="±" align="char">1.487 ± 0.90</td><td char="±" align="char">0.927 ± 0.03</td><td char="±" align="char">0.952 ± 0.02</td></tr><tr><td align="left">8</td><td align="left">24h</td><td align="left">29</td><td char="±" align="char">0.935 ± 0.02</td><td char="±" align="char">2.574 ± 2.32</td><td char="±" align="char">0.936 ± 0.03</td><td char="±" align="char">0.936 ± 0.03</td></tr><tr><td align="left">8</td><td align="left">D3</td><td align="left">26</td><td char="±" align="char">0.903 ± 0.07</td><td char="±" align="char">5.874 ± 0.82</td><td char="±" align="char">0.902 ± 0.03</td><td char="±" align="char">0.912 ± 0.11</td></tr><tr><td align="left">8</td><td align="left">D14</td><td align="left">26</td><td char="±" align="char">0.933 ± 0.02</td><td char="±" align="char">1.911 ± 1.34</td><td char="±" align="char">0.901 ± 0.04</td><td char="±" align="char">0.967 ± 0.01</td></tr><tr><td align="left">8</td><td align="left">D28</td><td align="left">23</td><td char="±" align="char">0.932 ± 0.02</td><td char="±" align="char">1.830 ± 1.23</td><td char="±" align="char">0.929 ± 0.03</td><td char="±" align="char">0.935 ± 0.03</td></tr><tr><td align="left">9</td><td align="left">24h</td><td align="left">77</td><td char="±" align="char">0.917 ± 0.03</td><td char="±" align="char">2.234 ± 1.67</td><td char="±" align="char">0.926 ± 0.05</td><td char="±" align="char">0.911 ± 0.04</td></tr><tr><td align="left">10</td><td align="left">D7</td><td align="left">36</td><td char="±" align="char">0.908 ± 0.05</td><td char="±" align="char">3.195 ± 2.20</td><td char="±" align="char">0.871 ± 0.10</td><td char="±" align="char">0.958 ± 0.03</td></tr><tr><td align="left">11</td><td align="left">24h</td><td align="left">28</td><td char="±" align="char">0.894 ± 0.03</td><td char="±" align="char">4.200 ± 2.27</td><td char="±" align="char">0.900 ± 0.04</td><td char="±" align="char">0.892 ± 0.04</td></tr><tr><td align="left" colspan="2">Average</td><td align="left">541</td><td char="±" align="char">0.923 ± 0.04</td><td char="±" align="char">2.480 ± 1.89</td><td char="±" align="char">0.924 ± 0.05</td><td char="±" align="char">0.926 ± 0.05</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec23">
    <title>Discussion</title>
    <p id="Par36">We presented MedicDeepLabv3+, the first method for hemisphere segmentation in rat MR images with ischemic lesions. We compared MedicDeepLabv3+ performance with state-of-the-art DeepLabv3+, UNet, HighRes3DNet, V-Net, VoxResNet, and three brain extraction algorithms (Demon, RATS, and RBET) combining several preclinical neuroimaging studies to a large dataset of 723 rat MR volumes.</p>
    <p id="Par37">ConvNets performed markedly better and their training time was about 10 times shorter than RATS (Oguz et al., <xref ref-type="bibr" rid="CR48">2014</xref>) and RBET (Wood et al., <xref ref-type="bibr" rid="CR64">2013</xref>). The superior performance of ConvNets was not surprising, as RATS and RBET were not designed to segment brains with widely varying intensity values, such as those found in brains with lesions. This outperformance of ConvNets over more traditional segmentation algorithms on rodent MRI aligns with recent research (Roy et al., <xref ref-type="bibr" rid="CR55">2018</xref>; Liu et al., <xref ref-type="bibr" rid="CR40">2020</xref>; De Feo et al., <xref ref-type="bibr" rid="CR15">2021</xref>).</p>
    <p id="Par38">MedicDeepLabv3+ yielded the highest Dice coefficients, precision and recall, and the lowest HD (Table <xref rid="Tab1" ref-type="table">1</xref>). Particularly, the outperformance of MedicDeepLabv3+ over the baseline DeepLabv3+ (Chen et al., <xref ref-type="bibr" rid="CR11">2018b</xref>) indicates that the proposed modifications (i.e., the incorporation of spatial attention layers and additional skip-connections), altogether, led to improvements. Similar improvements after incorporating attention layers, such as the proposed spatial attention layers, have also been reported in the literature (Oktay et al., <xref ref-type="bibr" rid="CR49">2018</xref>; Wang et al., <xref ref-type="bibr" rid="CR62">2019</xref>; Tao et al., <xref ref-type="bibr" rid="CR60">2019</xref>; Xu e tal., <xref ref-type="bibr" rid="CR67">2020</xref>). The same applies for adding skip connections (Drozdzal et al., <xref ref-type="bibr" rid="CR19">2016</xref>; Li et al., <xref ref-type="bibr" rid="CR38">2018</xref>). In the brain midline area experiment, UNet achieved slightly higher Dice coefficients than the other 3D ConvNets. However, these Dice coefficients were computed only in the annotated slices, as finding the brain midline requires the manual annotations. As we showed in Table <xref rid="Tab1" ref-type="table">1</xref>, Fig. <xref rid="Fig5" ref-type="fig">5</xref>, and the 17 Figures in <xref rid="MOESM2" ref-type="media">Online Resource 1</xref>, 2D ConvNets, including UNet, produced misclassifications in the cerebellum and the olfactory bulb that were not annotated, leading to notably higher HD. Therefore, the small difference between UNet and the 3D ConvNets (Fig. <xref rid="Fig4" ref-type="fig">4</xref>) comes at the expense of those misclassifications that were disregarded during the evaluation. The differences among HighRes3DNet, V-Net, VoxResNet and MedicDeepLabv3+ were also very small. In contrast, the difference between MedicDeepLabv3+ and the baseline DeepLabv3+ was three times larger than between MedicDeepLabv3+ and UNet.</p>
    <p id="Par39">Our benchmark (Table <xref rid="Tab1" ref-type="table">1</xref>) provides a valuable insight into whether 2D ConvNets produce better segmentations than 3D ConvNets on highly anisotropic data. In recent literature, 2D ConvNets appeared to be better (Jang et al., <xref ref-type="bibr" rid="CR30">2017</xref>; Isensee et al., <xref ref-type="bibr" rid="CR28">2017</xref>; Baumgartner et al., <xref ref-type="bibr" rid="CR4">2017</xref>), including in rodent images similar to our dataset (De Feo et al., <xref ref-type="bibr" rid="CR15">2021</xref>). 2D ConvNets outperformance may arise because contiguous slices can differ significantly in anisotropic data, thus, three-dimensional information might be unnecessary, and slice appearance might suffice to segment the regions of interest. Our data and, particularly, our manual annotations, were specially challenging since our regions of interest had similar intensity values to the cerebellum and olfactory bulb that were not annotated. Therefore, three-dimensional information can be critical to learn the location in the rostro-caudal axis of certain areas to avoid them. Indeed, our results support this intuition. Although Dice coefficient, precision and recall varied across architectures (Table <xref rid="Tab1" ref-type="table">1</xref>), HD was consistently lower with 3D ConvNets. In other words, 2D ConvNets produced more critical misclassifications. Thus, our data showcased a scenario in which, despite the anisotropy, 3D ConvNets were superior to 2D ConvNets, showing that the architectural choices need to consider more specific information and not just whether the data is anisotropic.</p>
    <p id="Par40">We measured the discrepancy magnitude between the hemispheric ratio distributions from the segmentations and from the ground truth (Table <xref rid="Tab2" ref-type="table">2</xref>), and V-Net and our MedicDeepLabv3+ yielded the smallest effect size, indicating that the hemispheric ratios of their corresponding segmentations were more similar to the ground truth than the other ConvNets. We want to emphasize the importance of accurate hemispheric ratios as they are biomarkers for predicting acute stroke (Swanson et al., <xref ref-type="bibr" rid="CR57">1990</xref>; Gerriets et al., <xref ref-type="bibr" rid="CR23">2004</xref>). Both V-Net and MedicDeepLabv3+’s confidence intervals were zero-centered, and between these two, MedicDeepLabv3+’s was one third smaller than V-Net’s. The effect size of VoxResNet and HighRes3DNet (the second and third best performing ConvNets after MedicDeepLabv3+) was much higher, and their confidence intervals did not include zero, which indicates that their hemispheric ratios were biased, being considerably larger than the ground truth. UNet’s and DeepLabv3+’s effect size were also high, and DeepLabv3+’s confidence interval was the largest across all compared ConvNets. Thus, overall, and in agreement with the other experiments, MedicDeepLabv3+ compared favorably with the baseline DeepLabv3+ and the other competing methods.</p>
    <p id="Par41">ConvNets, and especially our MedicDeepLabv3+, produced segmentations more similar to the ground truth than the other methods (Table <xref rid="Tab1" ref-type="table">1</xref>). Since these ConvNets were high capacity—requiring large GPU memory—and they were optimized with several images, their outperformance is in line with recent research (Tan &amp; Le, <xref ref-type="bibr" rid="CR59">2019</xref>). However, annotated data are often scarce, and large GPU memory to optimize ConvNets is not necessarily available. Motivated by these constraints, we showed in two separate experiments that MedicDeepLabv3+ performed remarkably well with few annotated data and very limited GPU memory (see Tables <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref>, and <xref rid="Tab5" ref-type="table">5</xref>). In other words, our method can handle different scenarios without excessively sacrificing performance, which showcases MedicDeepLabv3+ generalization capabilities.</p>
    <p id="Par42">MedicDeepLabv3+ is publicly available, and it can be easily incorporated into existing pipelines, reducing human workload and accelerating rodent neuroimaging analyses. Furthermore, MedicDeepLabv3+ is fast, requires no preprocessing and postprocessing, and it can be optimized on MR images with different contrast, voxel resolution, field of view, lesion appearance, and limited GPU memory and annotated data. As hemisphere segmentation masks can be utilized in diverse studies, our work is relevant for multiple applications involving brain lesions in rat images.</p>
  </sec>
  <sec id="Sec24">
    <title>Information Sharing Statement</title>
    <p id="Par43">The source code of MedicDeepLabv3+ and the trained models are publicly available under MIT License at <ext-link ext-link-type="uri" xlink:href="https://github.com/jmlipman/MedicDeepLabv3Plus">https://github.com/jmlipman/MedicDeepLabv3Plus</ext-link>. All quantitative evaluation measures, to which conclusion in the work are based on, are presented in the <xref rid="MOESM1" ref-type="media">Supplementary Materials</xref>. Training data may be available from Charles River Discovery Services upon the request of qualified parties but restrictions relating to the client privacy and intellectual property apply to the availability to these data, which were used under license for the current study. Typically, training data access will occur through collaboration and require interested parties to sign a material transfer agreement with Charles River Discovery Services prior to access to the training data.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec25">
      <p>Below is the link to the electronic supplementary material.<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="12021_2022_9607_MOESM1_ESM.zip"><caption><p>Supplementary file1 (RAR 1.26 MB)</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="12021_2022_9607_MOESM2_ESM.pdf"><caption><p>Supplementary file2 (PDF 6.51 MB)</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM3"><media xlink:href="12021_2022_9607_MOESM3_ESM.pdf"><caption><p>Supplementary file3 (PDF 90.7 KB)</p></caption></media></supplementary-material></p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par49">
        <ext-link ext-link-type="uri" xlink:href="https://www.criver.com/products-services/discovery-services">https://www.criver.com/products-services/discovery-services</ext-link>
      </p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The work of J.M. Valverde was funded from the European Union’s Horizon 2020 Framework Programme (Marie Skodowska Curie grant agreement #740264 (GENOMMED)). This work has also been supported by the grant #316258 from Academy of Finland (J. Tohka) and grant S21770 from the European Social Fund (R. De Feo). Part of the computational analysis was run on the servers provided by Bioinformatics Center, University of Eastern Finland, Finland.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author Contributions</title>
    <p>Conceptualization: Juan Miguel Valverde, Artem Shatillo, Riccardo De Feo, Jussi Tohka; Data curation: Artem Shatillo; Formal Analysis: Juan Miguel Valverde, Jussi Tohka; Investigation: Juan Miguel Valverde; Funding acquisition: Jussi Tohka; Methodology: Juan Miguel Valverde, Riccardo De Feo, Jussi Tohka; Project administration: Jussi Tohka; Resources: Artem Shatillo; Software: Juan Miguel Valverde; Supervision: Jussi Tohka; Validation: Juan Miguel Valverde, Jussi Tohka; Visualization: Juan Miguel Valverde; Writing—original draft: Juan Miguel Valverde; Writing—review &amp; editing: Juan Miguel Valverde, Artem Shatillo, Riccardo De Feo, Jussi Tohka.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Open access funding provided by University of Eastern Finland (UEF) including Kuopio University Hospital. The work of J.M. Valverde was funded from the European Union’s Horizon 2020 Framework Programme (Marie Skodowska Curie grant agreement #740264 (GENOMMED)). This work has also been supported by the grant #316258 from Academy of Finland (J. Tohka) and grant S21770 from the European Social Fund (R. De Feo).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data Availability</title>
    <p>The trained models are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jmlipman/MedicDeepLabv3Plus">https://github.com/jmlipman/MedicDeepLabv3Plus</ext-link>. Training data may be available from Charles River Discovery Services upon the request of qualified parties but restrictions relating to the client privacy and intellectual property apply to the availability to these data, which were used under license for the current study. Typically, training data access will occur through collaboration and require interested parties to sign a material transfer agreement with Charles River Discovery Services prior to access to the training data.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code Availability</title>
    <p>The source code of MedicDeepLabv3+ and the trained models are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jmlipman/MedicDeepLabv3Plus">https://github.com/jmlipman/MedicDeepLabv3Plus</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar4">
      <title>Ethics Approval</title>
      <p id="Par44">All animal experiments were conducted according to the National Institute of Health (NIH) guidelines for the care and use of laboratory animals, and approved by the National Animal Experiment Board, Finland.</p>
    </notes>
    <notes id="FPar5">
      <title>Consent to Participate</title>
      <p id="Par45">Not applicable.</p>
    </notes>
    <notes id="FPar6">
      <title>Consent for Publication</title>
      <p id="Par46">Not applicable.</p>
    </notes>
    <notes id="FPar7" notes-type="COI-statement">
      <title>Conflicts of Interest/Competing Interests</title>
      <p id="Par47">As disclosed in the affiliation section, Artem Shatillo, MD is a full-time payroll employee of the Charles River Discovery Services, Finland—a commercial preclinical contract research organization (CRO), which participated in the project and provided raw data as a part of companys R &amp;D initiative.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Arnaud</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Forbes</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Coquery</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Collomb</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Lemasson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Barbier</surname>
            <given-names>EL</given-names>
          </name>
        </person-group>
        <article-title>Fully automatic lesion localization and characterization: Application to brain tumors using multiparametric quantitative mri data</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <fpage>1678</fpage>
        <lpage>1689</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2018.2794918</pub-id>
        <?supplied-pmid 29969418?>
        <pub-id pub-id-type="pmid">29969418</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bae</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Badea</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Automated segmentation of mouse brain images using extended mrf</article-title>
        <source>Neuroimage</source>
        <year>2009</year>
        <volume>46</volume>
        <fpage>717</fpage>
        <lpage>725</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.02.012</pub-id>
        <?supplied-pmid 19236923?>
        <pub-id pub-id-type="pmid">19236923</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <mixed-citation publication-type="other">Bakas, S., Reyes, M., Jakab, A., Bauer, S., Rempfler, M., Crimi, A., Shinohara, R. T., Berger, C., Ha, S. M., Rozycki, M. et al. (2018). Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1811.02629">arXiv:1811.02629</ext-link></mixed-citation>
    </ref>
    <ref id="CR4">
      <mixed-citation publication-type="other">Baumgartner, C. F., Koch, L. M., Pollefeys, M., &amp; Konukoglu, E. (2017). An exploration of 2d and 3d deep learning techniques for cardiac mr image segmentation. In <italic>International Workshop on Statistical Atlases and Computational Models of the Heart</italic> (pp. 111–119). Springer.</mixed-citation>
    </ref>
    <ref id="CR5">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bernard</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Lalande</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zotti</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Cervenansky</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>P-A</given-names>
          </name>
          <name>
            <surname>Cetin</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Lekadir</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Camara</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Ballester</surname>
            <given-names>MAG</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved?</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <fpage>2514</fpage>
        <lpage>2525</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2018.2837502</pub-id>
        <?supplied-pmid 29994302?>
        <pub-id pub-id-type="pmid">29994302</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carbone</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Estimating mouse and rat use in american laboratories by extrapolation from animal welfare act-regulated species</article-title>
        <source>Scientific Reports</source>
        <year>2021</year>
        <volume>11</volume>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-020-79961-0</pub-id>
        <pub-id pub-id-type="pmid">33414495</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>P-A</given-names>
          </name>
        </person-group>
        <article-title>Voxresnet: Deep voxelwise residual networks for brain segmentation from 3d mr images</article-title>
        <source>NeuroImage</source>
        <year>2018</year>
        <volume>170</volume>
        <fpage>446</fpage>
        <lpage>455</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.04.041</pub-id>
        <?supplied-pmid 28445774?>
        <pub-id pub-id-type="pmid">28445774</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <mixed-citation publication-type="other">Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., &amp; Yuille, A. L. (2014). Semantic image segmentation with deep convolutional nets and fully connected crfs. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.7062">arXiv:1412.7062</ext-link></mixed-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>L-C</given-names>
          </name>
          <name>
            <surname>Papandreou</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Kokkinos</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Yuille</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title>
        <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        <year>2017</year>
        <volume>40</volume>
        <fpage>834</fpage>
        <lpage>848</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id>
        <?supplied-pmid 28463186?>
        <pub-id pub-id-type="pmid">28463186</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choi</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Yi</surname>
            <given-names>KS</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S-R</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Jeon</surname>
            <given-names>C-Y</given-names>
          </name>
          <name>
            <surname>Hwang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Cha</surname>
            <given-names>S-H</given-names>
          </name>
        </person-group>
        <article-title>A novel voxel-wise lesion segmentation technique on 3.0-t diffusion mri of hyperacute focal cerebral ischemia at 1 h after permanent mcao in rats</article-title>
        <source>Journal of Cerebral Blood Flow &amp; Metabolism</source>
        <year>2018</year>
        <volume>38</volume>
        <fpage>1371</fpage>
        <lpage>1383</lpage>
        <pub-id pub-id-type="doi">10.1177/0271678X17714179</pub-id>
        <pub-id pub-id-type="pmid">28598225</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <mixed-citation publication-type="other">Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., &amp; Adam, H. (2018b). Encoder-decoder with atrous separable convolution for semantic image segmentation. In <italic>Proceedings of the European conference on Computer Vision (ECCV)</italic> (pp. 801–818).</mixed-citation>
    </ref>
    <ref id="CR12">
      <mixed-citation publication-type="other">Chollet, F. (2017). Xception: Deep learning with depthwise separable convolutions. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic> (pp. 1251–1258).</mixed-citation>
    </ref>
    <ref id="CR13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chou</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bingren</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chuang</surname>
            <given-names>K-H</given-names>
          </name>
        </person-group>
        <article-title>Robust automatic rodent brain extraction using 3-d pulse-coupled neural networks (pcnn)</article-title>
        <source>IEEE Transactions on Image Processing</source>
        <year>2011</year>
        <volume>20</volume>
        <fpage>2554</fpage>
        <lpage>2564</lpage>
        <pub-id pub-id-type="doi">10.1109/TIP.2011.2126587</pub-id>
        <?supplied-pmid 21411404?>
        <pub-id pub-id-type="pmid">21411404</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <mixed-citation publication-type="other">De Feo, R., Hmlinen, E., Manninen, E., Immonen, R., Valverde, J. M., Ndode-Ekane, X. E., Grhn, O., Pitknen, A., &amp; Tohka, J. (2022). Convolutional neural networks enable robust automatic segmentation of the rat hippocampus in mri after traumatic brain injury. <italic>Frontiers in Neurology</italic>, <italic>13</italic>.</mixed-citation>
    </ref>
    <ref id="CR15">
      <mixed-citation publication-type="other">De Feo, R., Shatillo, A., Sierra, A., Valverde, J. M., Gröhn, O., Giove, F., &amp; Tohka, J. (2021). Automated joint skull-stripping and segmentation with multi-task u-net in large mouse brain mri databases. <italic>NeuroImage</italic>, (p. 117734).</mixed-citation>
    </ref>
    <ref id="CR16">
      <mixed-citation publication-type="other">Dervieux, A., &amp; Thomasset, F. (1980). A finite element method for the simulation of a rayleigh-taylor instability. In <italic>Approximation methods for Navier-Stokes problems</italic> (pp. 145–158). Springer.</mixed-citation>
    </ref>
    <ref id="CR17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dice</surname>
            <given-names>LR</given-names>
          </name>
        </person-group>
        <article-title>Measures of the amount of ecologic association between species</article-title>
        <source>Ecology</source>
        <year>1945</year>
        <volume>26</volume>
        <fpage>297</fpage>
        <lpage>302</lpage>
        <pub-id pub-id-type="doi">10.2307/1932409</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <mixed-citation publication-type="other">Dietterich, T. G. (2000). Ensemble methods in machine learning. In <italic>International workshop on multiple classifier systems</italic> (pp. 1–15). Springer.</mixed-citation>
    </ref>
    <ref id="CR19">
      <mixed-citation publication-type="other">Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S., &amp; Pal, C. (2016). The importance of skip connections in biomedical image segmentation. In <italic>Deep Learning and Data Labeling for Medical Applications</italic> (pp. 179–187). Springer.</mixed-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Efron</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Better bootstrap confidence intervals</article-title>
        <source>Journal of the American Statistical Association</source>
        <year>1987</year>
        <volume>82</volume>
        <fpage>171</fpage>
        <lpage>185</lpage>
        <pub-id pub-id-type="doi">10.1080/01621459.1987.10478410</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Freret</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Chazalviel</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Roussel</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bernaudin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Schumann-Bard</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Boulouard</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Long-term functional outcome following transient middle cerebral artery occlusion in the rat: correlation between brain damage and behavioral impairment</article-title>
        <source>Behavioral Neuroscience</source>
        <year>2006</year>
        <volume>120</volume>
        <fpage>1285</fpage>
        <pub-id pub-id-type="doi">10.1037/0735-7044.120.6.1285</pub-id>
        <?supplied-pmid 17201474?>
        <pub-id pub-id-type="pmid">17201474</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <mixed-citation publication-type="other">Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., &amp; Lu, H. (2019). Dual attention network for scene segmentation. In <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> (pp. 3146–3154).</mixed-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerriets</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Stolz</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Walberer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Muller</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kluge</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bachmann</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Fisher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kaps</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bachmann</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Noninvasive quantification of brain edema and the space-occupying effect in rat stroke models using magnetic resonance imaging</article-title>
        <source>Stroke</source>
        <year>2004</year>
        <volume>35</volume>
        <fpage>566</fpage>
        <lpage>571</lpage>
        <pub-id pub-id-type="doi">10.1161/01.STR.0000113692.38574.57</pub-id>
        <?supplied-pmid 14739415?>
        <pub-id pub-id-type="pmid">14739415</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <mixed-citation publication-type="other">He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic> (pp. 770–778).</mixed-citation>
    </ref>
    <ref id="CR25">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Heller</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Isensee</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Maier-Hein</surname>
            <given-names>KH</given-names>
          </name>
          <name>
            <surname>Hou</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Nan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Mu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct imaging: Results of the kits19 challenge</article-title>
        <source>Medical Image Analysis</source>
        <year>2021</year>
        <volume>67</volume>
        <fpage>101821</fpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2020.101821</pub-id>
        <?supplied-pmid 33049579?>
        <pub-id pub-id-type="pmid">33049579</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <mixed-citation publication-type="other">Hsu, L.-M., Wang, S., Ranadive, P., Ban, W., Chao, T.-H. H., Song, S., Cerri, D. H., Walton, L. R., Broadwater, M. A., Lee, S.-H. et al. (2020). Automatic skull stripping of rat and mouse brain mri data using u-net. <italic>Frontiers in Neuroscience</italic>.</mixed-citation>
    </ref>
    <ref id="CR27">
      <mixed-citation publication-type="other">Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1502.03167">arXiv:1502.03167</ext-link></mixed-citation>
    </ref>
    <ref id="CR28">
      <mixed-citation publication-type="other">Isensee, F., Jaeger, P. F., Full, P. M., Wolf, I., Engelhardt, S., &amp; Maier-Hein, K. H. (2017). Automatic cardiac disease assessment on cine-mri via time-series segmentation and domain specific features. In <italic>International Workshop on Statistical Atlases and Computational Models of the Heart</italic> (pp. 120–129). Springer.</mixed-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Isensee</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Jaeger</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Kohl</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Petersen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Maier-Hein</surname>
            <given-names>KH</given-names>
          </name>
        </person-group>
        <article-title>nnu-net: a self-configuring method for deep learning-based biomedical image segmentation</article-title>
        <source>Nature Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>203</fpage>
        <lpage>211</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01008-z</pub-id>
        <?supplied-pmid 33288961?>
        <pub-id pub-id-type="pmid">33288961</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <mixed-citation publication-type="other">Jang, Y., Hong, Y., Ha, S., Kim, S., &amp; Chang, H.-J. (2017). Automatic segmentation of lv and rv in cardiac mri. In <italic>International Workshop on Statistical Atlases and Computational Models of the Heart</italic> (pp. 161–169). Springer.</mixed-citation>
    </ref>
    <ref id="CR31">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kervadec</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Dolz</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Granger</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Boykov</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ayed</surname>
            <given-names>IB</given-names>
          </name>
        </person-group>
        <article-title>Constrained-cnn losses for weakly supervised segmentation</article-title>
        <source>Medical Image Analysis</source>
        <year>2019</year>
        <volume>54</volume>
        <fpage>88</fpage>
        <lpage>99</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2019.02.009</pub-id>
        <?supplied-pmid 30851541?>
        <pub-id pub-id-type="pmid">30851541</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Khan</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Yahya</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Alsaih</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ali</surname>
            <given-names>SSA</given-names>
          </name>
          <name>
            <surname>Meriaudeau</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of deep neural networks for semantic segmentation of prostate in t2w mri</article-title>
        <source>Sensors</source>
        <year>2020</year>
        <volume>20</volume>
        <fpage>3183</fpage>
        <pub-id pub-id-type="doi">10.3390/s20113183</pub-id>
        <?supplied-pmid 32503330?>
        <pub-id pub-id-type="pmid">32503330</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <mixed-citation publication-type="other">Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization. <italic>CoRR</italic>, <italic>abs/1412.6980</italic>.</mixed-citation>
    </ref>
    <ref id="CR34">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Koizumi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yoshida</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Nakazawa</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ooneda</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Experimental studies of ischemic brain edema. 1. a new experimental model of cerebral embolism in rats in which recirculation can be introduced in the ischemic area</article-title>
        <source>Japanese Journal of Stroke</source>
        <year>1986</year>
        <volume>8</volume>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kushibar</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Valverde</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gonzalez-Villa</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bernal</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cabezas</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Oliver</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lladó</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Automated sub-cortical brain structure segmentation combining spatial and deep convolutional features</article-title>
        <source>Medical Image Analysis</source>
        <year>2018</year>
        <volume>48</volume>
        <fpage>177</fpage>
        <lpage>186</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2018.06.006</pub-id>
        <?supplied-pmid 29935442?>
        <pub-id pub-id-type="pmid">29935442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lakens</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and anovas</article-title>
        <source>Frontiers in Psychology</source>
        <year>2013</year>
        <volume>4</volume>
        <fpage>863</fpage>
        <pub-id pub-id-type="doi">10.3389/fpsyg.2013.00863</pub-id>
        <?supplied-pmid 24324449?>
        <pub-id pub-id-type="pmid">24324449</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <mixed-citation publication-type="other">Lee, C.-Y., Xie, S., Gallagher, P., Zhang, Z., &amp; Tu, Z. (2015). Deeply-supervised nets. In <italic>Artificial Intelligence and Statistics</italic> (pp. 562–570). PMLR.</mixed-citation>
    </ref>
    <ref id="CR38">
      <mixed-citation publication-type="other">Li, H., Xu, Z., Taylor, G., Studer, C., &amp; Goldstein, T. (2018). Visualizing the loss landscape of neural nets. In <italic>Advances in Neural Information Processing Systems</italic> (pp. 6389–6399).</mixed-citation>
    </ref>
    <ref id="CR39">
      <mixed-citation publication-type="other">Li, W., Wang, G., Fidon, L., Ourselin, S., Cardoso, M. J., &amp; Vercauteren, T. (2017). On the compactness, efficiency, and representation of 3d convolutional networks: brain parcellation as a pretext task. In <italic>International Conference on Information Processing in Medical Imaging</italic> (pp. 348–360). Springer.</mixed-citation>
    </ref>
    <ref id="CR40">
      <mixed-citation publication-type="other">Liu, Y., Unsal, H. S., Tao, Y., &amp; Zhang, N. (2020). Automatic brain extraction for rodent mri images. <italic>Neuroinformatics</italic>, (pp. 1–12).</mixed-citation>
    </ref>
    <ref id="CR41">
      <mixed-citation publication-type="other">Ma, C., Ji, Z., &amp; Gao, M. (2019). Neural style transfer improves 3d cardiovascular mr image segmentation on inconsistent data. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic> (pp. 128–136). Springer.</mixed-citation>
    </ref>
    <ref id="CR42">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Cardoso</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Modat</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Powell</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wells</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Holmes</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wiseman</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Tybulewicz</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Fisher</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Lythgoe</surname>
            <given-names>MF</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automatic structural parcellation of mouse brain mri using multi-atlas label fusion</article-title>
        <source>PloS one</source>
        <year>2014</year>
        <volume>9</volume>
        <fpage>e86576</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0086576</pub-id>
        <?supplied-pmid 24475148?>
        <pub-id pub-id-type="pmid">24475148</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McBride</surname>
            <given-names>DW</given-names>
          </name>
          <name>
            <surname>Klebe</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <article-title>Correcting for brain swellings effects on infarct volume calculation after middle cerebral artery occlusion in rats</article-title>
        <source>Translational stroke research</source>
        <year>2015</year>
        <volume>6</volume>
        <fpage>323</fpage>
        <lpage>338</lpage>
        <pub-id pub-id-type="doi">10.1007/s12975-015-0400-3</pub-id>
        <?supplied-pmid 25933988?>
        <pub-id pub-id-type="pmid">25933988</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <mixed-citation publication-type="other">Milletari, F., Navab, N., &amp; Ahmadi, S.-A. (2016). V-net: Fully convolutional neural networks for volumetric medical image segmentation. In <italic>2016 Fourth International Conference on 3D Vision (3DV)</italic> (pp. 565–571). IEEE.</mixed-citation>
    </ref>
    <ref id="CR45">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mulder</surname>
            <given-names>IA</given-names>
          </name>
          <name>
            <surname>Khmelinskii</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dzyubachyk</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>de Jong</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Rieff</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wermer</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Hoehn</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lelieveldt</surname>
            <given-names>BP</given-names>
          </name>
          <name>
            <surname>van den Maagdenberg</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>Automated ischemic lesion segmentation in mri mouse brain data after transient middle cerebral artery occlusion</article-title>
        <source>Frontiers in neuroinformatics</source>
        <year>2017</year>
        <volume>11</volume>
        <fpage>3</fpage>
        <?supplied-pmid 28197090?>
        <pub-id pub-id-type="pmid">28197090</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Murugavel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sullivan</surname>
            <given-names>JM</given-names>
            <suffix>Jr</suffix>
          </name>
        </person-group>
        <article-title>Automatic cropping of mri rat brain volumes using pulse coupled neural networks</article-title>
        <source>Neuroimage</source>
        <year>2009</year>
        <volume>45</volume>
        <fpage>845</fpage>
        <lpage>854</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.12.021</pub-id>
        <?supplied-pmid 19167504?>
        <pub-id pub-id-type="pmid">19167504</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <mixed-citation publication-type="other">Myronenko, A., &amp; Hatamizadeh, A. (2019). Robust semantic segmentation of brain tumor regions from 3d mris. In <italic>International MICCAI Brainlesion Workshop</italic> (pp. 82–89). Springer.</mixed-citation>
    </ref>
    <ref id="CR48">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Oguz</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Rumple</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sonka</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Rats: rapid automatic tissue segmentation in rodent brain mri</article-title>
        <source>Journal of neuroscience methods</source>
        <year>2014</year>
        <volume>221</volume>
        <fpage>175</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.09.021</pub-id>
        <?supplied-pmid 24140478?>
        <pub-id pub-id-type="pmid">24140478</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <mixed-citation publication-type="other">Oktay, O., Schlemper, J., Folgoc, L. L., Lee, M., Heinrich, M., Misawa, K., Mori, K., McDonagh, S., Hammerla, N. Y., Kainz, B. et al. (2018). Attention u-net: Learning where to look for the pancreas. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1804.03999">arXiv:1804.03999</ext-link></mixed-citation>
    </ref>
    <ref id="CR50">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Osher</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sethian</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>Fronts propagating with curvature-dependent speed: Algorithms based on hamilton-jacobi formulations</article-title>
        <source>Journal of computational physics</source>
        <year>1988</year>
        <volume>79</volume>
        <fpage>12</fpage>
        <lpage>49</lpage>
        <pub-id pub-id-type="doi">10.1016/0021-9991(88)90002-2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pagani</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Damiano</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Galbusera</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tsaftaris</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Gozzi</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Semi-automated registration-based anatomical labelling, voxel based morphometry and cortical thickness mapping of the mouse brain</article-title>
        <source>Journal of neuroscience methods</source>
        <year>2016</year>
        <volume>267</volume>
        <fpage>62</fpage>
        <lpage>73</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2016.04.007</pub-id>
        <?supplied-pmid 27079699?>
        <pub-id pub-id-type="pmid">27079699</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <mixed-citation publication-type="other">Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L. et al. (2019). Pytorch: An imperative style, high-performance deep learning library. In <italic>Advances in Neural Information Processing Systems</italic> (pp. 8024–8035).</mixed-citation>
    </ref>
    <ref id="CR53">
      <mixed-citation publication-type="other">Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In <italic>International Conference on Medical Image Computing and Computer-assisted Intervention</italic> (pp. 234–241). Springer.</mixed-citation>
    </ref>
    <ref id="CR54">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rote</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Computing the minimum hausdorff distance between two point sets on a line under translation</article-title>
        <source>Information Processing Letters</source>
        <year>1991</year>
        <volume>38</volume>
        <fpage>123</fpage>
        <lpage>127</lpage>
        <pub-id pub-id-type="doi">10.1016/0020-0190(91)90233-8</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <mixed-citation publication-type="other">Roy, S., Knutsen, A., Korotcov, A., Bosomtwi, A., Dardzinski, B., Butman, J. A., &amp; Pham, D. L. (2018). A deep learning framework for brain extraction in humans and animals with traumatic brain injury. In <italic>2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</italic> (pp. 687–691). IEEE.</mixed-citation>
    </ref>
    <ref id="CR56">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schwarz</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Danckaert</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Reese</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Gozzi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Paxinos</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Watson</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Merlo-Pich</surname>
            <given-names>EV</given-names>
          </name>
          <name>
            <surname>Bifone</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A stereotaxic mri template set for the rat brain with tissue class distribution maps and co-registered anatomical atlas: application to pharmacological mri</article-title>
        <source>Neuroimage</source>
        <year>2006</year>
        <volume>32</volume>
        <fpage>538</fpage>
        <lpage>550</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.04.214</pub-id>
        <?supplied-pmid 16784876?>
        <pub-id pub-id-type="pmid">16784876</pub-id>
      </element-citation>
    </ref>
    <ref id="CR57">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Swanson</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Morton</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Tsao-Wu</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Savalos</surname>
            <given-names>RA</given-names>
          </name>
          <name>
            <surname>Davidson</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sharp</surname>
            <given-names>FR</given-names>
          </name>
        </person-group>
        <article-title>A semiautomated method for measuring brain infarct volume</article-title>
        <source>Journal of Cerebral Blood Flow &amp; Metabolism</source>
        <year>1990</year>
        <volume>10</volume>
        <fpage>290</fpage>
        <lpage>293</lpage>
        <pub-id pub-id-type="doi">10.1038/jcbfm.1990.47</pub-id>
        <pub-id pub-id-type="pmid">1689322</pub-id>
      </element-citation>
    </ref>
    <ref id="CR58">
      <mixed-citation publication-type="other">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., &amp; Rabinovich, A. (2015). Going deeper with convolutions. In <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</italic> (pp. 1–9).</mixed-citation>
    </ref>
    <ref id="CR59">
      <mixed-citation publication-type="other">Tan, M., &amp; Le, Q. (2019). Efficientnet: Rethinking model scaling for convolutional neural networks. In <italic>International Conference on Machine Learning</italic> (pp. 6105–6114). PMLR.</mixed-citation>
    </ref>
    <ref id="CR60">
      <mixed-citation publication-type="other">Tao, Q., Ge, Z., Cai, J., Yin, J., &amp; See, S. (2019). Improving deep lesion detection using 3d contextual and spatial attention. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic> (pp. 185–193). Springer.</mixed-citation>
    </ref>
    <ref id="CR61">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Valverde</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Shatillo</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>De Feo</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gröhn</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Sierra</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Tohka</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Ratlesnetv2: A fully convolutional network for rodent brain lesion segmentation</article-title>
        <source>Frontiers in neuroscience</source>
        <year>2020</year>
        <volume>14</volume>
        <fpage>1333</fpage>
        <pub-id pub-id-type="doi">10.3389/fnins.2020.610239</pub-id>
      </element-citation>
    </ref>
    <ref id="CR62">
      <mixed-citation publication-type="other">Wang, G., Shapey, J., Li, W., Dorent, R., Demitriadis, A., Bisdas, S., Paddick, I., Bradford, R., Zhang, S., Ourselin, S. et al. (2019). Automatic segmentation of vestibular schwannoma from t2-weighted mri by deep spatial attention with hardness-weighted loss. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic> (pp. 264–272). Springer.</mixed-citation>
    </ref>
    <ref id="CR63">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Cheung</surname>
            <given-names>P-T</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>GX</given-names>
          </name>
          <name>
            <surname>Bhatia</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>EX</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Khong</surname>
            <given-names>P-L</given-names>
          </name>
        </person-group>
        <article-title>Comparing diffusion-weighted and t2-weighted mr imaging for the quantification of infarct size in a neonatal rat hypoxic-ischemic model at 24 h post-injury</article-title>
        <source>International journal of developmental neuroscience</source>
        <year>2007</year>
        <volume>25</volume>
        <fpage>1</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ijdevneu.2006.12.003</pub-id>
        <?supplied-pmid 17229540?>
        <pub-id pub-id-type="pmid">17229540</pub-id>
      </element-citation>
    </ref>
    <ref id="CR64">
      <mixed-citation publication-type="other">Wood, T. C., Lythgoe, D. J., &amp; Williams, S. C. (2013). rbet: making bet work for rodent brains. In <italic>Proceeding of the</italic> <italic>International Society for Magnetic Resonance in Medicine</italic> (p. 2706). volume 21.</mixed-citation>
    </ref>
    <ref id="CR65">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bae</surname>
            <given-names>MH</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Badea</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A prior feature svm-mrf based method for mouse brain segmentation</article-title>
        <source>NeuroImage</source>
        <year>2012</year>
        <volume>59</volume>
        <fpage>2298</fpage>
        <lpage>2306</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.09.053</pub-id>
        <?supplied-pmid 21988893?>
        <pub-id pub-id-type="pmid">21988893</pub-id>
      </element-citation>
    </ref>
    <ref id="CR66">
      <mixed-citation publication-type="other">Xie, Y., Lu, H., Zhang, J., Shen, C., &amp; Xia, Y. (2019). Deep segmentation-emendation model for gland instance segmentation. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic> (pp. 469–477). Springer.</mixed-citation>
    </ref>
    <ref id="CR67">
      <mixed-citation publication-type="other">Xu, X., Lian, C., Wang, S., Wang, A., Royce, T., Chen, R., Lian, J., &amp; Shen, D. (2020). Asymmetrical multi-task attention u-net for the segmentation of prostate bed in ct image. In <italic>International Conference on Medical Image Computing and Computer-Assisted Intervention</italic> (pp. 470–479). Springer.</mixed-citation>
    </ref>
  </ref-list>
</back>
