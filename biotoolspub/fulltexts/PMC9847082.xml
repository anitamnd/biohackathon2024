<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats-oasis2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Protein Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Protein Sci</journal-id>
    <journal-id journal-id-type="doi">10.1002/(ISSN)1469-896X</journal-id>
    <journal-id journal-id-type="publisher-id">PRO</journal-id>
    <journal-title-group>
      <journal-title>Protein Science : A Publication of the Protein Society</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0961-8368</issn>
    <issn pub-type="epub">1469-896X</issn>
    <publisher>
      <publisher-name>John Wiley &amp; Sons, Inc.</publisher-name>
      <publisher-loc>Hoboken, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9847082</article-id>
    <article-id pub-id-type="pmid">36519247</article-id>
    <article-id pub-id-type="doi">10.1002/pro.4541</article-id>
    <article-id pub-id-type="publisher-id">PRO4541</article-id>
    <article-categories>
      <subj-group subj-group-type="overline">
        <subject>Tools for Protein Science</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Tools for Protein Science</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title><styled-content style="fixed-case" toggle="no">COLLAPSE</styled-content>: A representation learning framework for identification and characterization of protein structural sites</article-title>
      <alt-title alt-title-type="left-running-head">Derry and Altman</alt-title>
    </title-group>
    <contrib-group>
      <contrib id="pro4541-cr-0001" contrib-type="author">
        <name>
          <surname>Derry</surname>
          <given-names>Alexander</given-names>
        </name>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2076-1184</contrib-id>
        <xref rid="pro4541-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib id="pro4541-cr-0002" contrib-type="author" corresp="yes">
        <name>
          <surname>Altman</surname>
          <given-names>Russ B.</given-names>
        </name>
        <xref rid="pro4541-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="pro4541-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <address>
          <email>russ.altman@stanford.edu</email>
        </address>
      </contrib>
    </contrib-group>
    <aff id="pro4541-aff-0001">
      <label>
        <sup>1</sup>
      </label>
      <named-content content-type="organisation-division">Department of Biomedical Data Science</named-content>
      <institution>Stanford University</institution>
      <city>Stanford</city>
      <named-content content-type="country-part">California</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="pro4541-aff-0002">
      <label>
        <sup>2</sup>
      </label>
      <named-content content-type="organisation-division">Departments of Bioengineering, Genetics, and Medicine</named-content>
      <institution>Stanford University</institution>
      <city>Stanford</city>
      <named-content content-type="country-part">California</named-content>
      <country country="US">USA</country>
    </aff>
    <author-notes>
      <corresp id="correspondenceTo"><label>*</label><bold>Correspondence</bold><break/>
Russ B. Altman, 443 Via Ortega Room 209, Stanford, CA 94305, USA.<break/>
Email: <email>russ.altman@stanford.edu</email>
<break/>
</corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>01</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>01</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <volume>32</volume>
    <issue seq="40">2</issue>
    <issue-id pub-id-type="doi">10.1002/pro.v32.2</issue-id>
    <elocation-id>e4541</elocation-id>
    <history>
      <date date-type="rev-recd">
        <day>02</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="received">
        <day>30</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>08</day>
        <month>12</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <!--&#x000a9; 2023 The Protein Society-->
      <copyright-statement content-type="article-copyright">© 2022 The Authors. <italic toggle="yes">Protein Science</italic> published by Wiley Periodicals LLC on behalf of The Protein Society.</copyright-statement>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link> License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non‐commercial and no modifications or adaptations are made.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="file:PRO-32-e4541.pdf"/>
    <abstract>
      <title>Abstract</title>
      <p>The identification and characterization of the structural sites which contribute to protein function are crucial for understanding biological mechanisms, evaluating disease risk, and developing targeted therapies. However, the quantity of known protein structures is rapidly outpacing our ability to functionally annotate them. Existing methods for function prediction either do not operate on local sites, suffer from high false positive or false negative rates, or require large site‐specific training datasets, necessitating the development of new computational methods for annotating functional sites at scale. We present COLLAPSE (Compressed Latents Learned from Aligned Protein Structural Environments), a framework for learning deep representations of protein sites. COLLAPSE operates directly on the 3D positions of atoms surrounding a site and uses evolutionary relationships between homologous proteins as a self‐supervision signal, enabling learned embeddings to implicitly capture structure–function relationships within each site. Our representations generalize across disparate tasks in a transfer learning context, achieving state‐of‐the‐art performance on standardized benchmarks (protein–protein interactions and mutation stability) and on the prediction of functional sites from the <sc>prosite</sc> database. We use COLLAPSE to search for similar sites across large protein datasets and to annotate proteins based on a database of known functional sites. These methods demonstrate that COLLAPSE is computationally efficient, tunable, and interpretable, providing a general‐purpose platform for computational protein analysis.</p>
    </abstract>
    <kwd-group kwd-group-type="author-generated">
      <kwd id="pro4541-kwd-0001">deep learning</kwd>
      <kwd id="pro4541-kwd-0002">functional site annotation</kwd>
      <kwd id="pro4541-kwd-0003">protein structure analysis</kwd>
      <kwd id="pro4541-kwd-0004">representation learning</kwd>
      <kwd id="pro4541-kwd-0005">structural informatics</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="funding-0001">
        <funding-source>
          <institution-wrap>
            <institution>Chan Zuckerberg Initiative
</institution>
            <institution-id institution-id-type="doi">10.13039/100014989</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group id="funding-0002">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health
</institution>
            <institution-id institution-id-type="doi">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>GM102365</award-id>
      </award-group>
      <award-group id="funding-0003">
        <funding-source>
          <institution-wrap>
            <institution>U.S. National Library of Medicine
</institution>
            <institution-id institution-id-type="doi">10.13039/100000092</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>LM012409</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="5"/>
      <table-count count="2"/>
      <page-count count="15"/>
      <word-count count="9716"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>source-schema-version-number</meta-name>
        <meta-value>2.0</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>February 2023</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>details-of-publishers-convertor</meta-name>
        <meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.2.3 mode:remove_FC converted:18.01.2023</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <p content-type="self-citation">
      <mixed-citation publication-type="journal" id="pro4541-cit-9001"><string-name><surname>Derry</surname><given-names>A</given-names></string-name>, <string-name><surname>Altman</surname><given-names>RB</given-names></string-name>. <article-title><styled-content style="fixed-case" toggle="no">COLLAPSE</styled-content>: A representation learning framework for identification and characterization of protein structural sites</article-title>. <source>Protein Science</source>. <year>2023</year>;<volume>32</volume>(<issue>2</issue>):<elocation-id>e4541</elocation-id>. <pub-id pub-id-type="doi">10.1002/pro.4541</pub-id></mixed-citation>
    </p>
    <fn-group id="pro4541-ntgp-0001">
      <fn id="pro4541-note-0001">
        <p><bold>Review Editor:</bold> Nir Ben‐Tal</p>
      </fn>
      <fn id="pro4541-note-1002">
        <p><bold>Funding information</bold> Chan Zuckerberg Initiative; National Institutes of Health, Grant/Award Number: GM102365; U.S. National Library of Medicine, Grant/Award Number: LM012409</p>
      </fn>
    </fn-group>
  </notes>
</front>
<body id="pro4541-body-0001">
  <sec id="pro4541-sec-0001">
    <label>1</label>
    <title>INTRODUCTION</title>
    <p>The three‐dimensional structure of a protein determines its functional characteristics and ability to interact with other molecules, including other proteins, endogenous small molecules, and therapeutic drugs. Biochemical interactions occur at specific regions of the protein known as functional sites. We consider functional sites that range from a few atoms which coordinate an ion or catalyze a reaction to larger regions which bind a cofactor or form a protein–protein interaction surface. The identification of such sites—and accurate modeling of the local structure–function relationship—is critical for determining a protein's biological role, including our understanding of disease pathogenesis and ability to develop targeted therapies or protein engineering technologies. Significant effort has gone into curating databases to catalog these structure–function relationships, (Akiva et al., <xref rid="pro4541-bib-0002" ref-type="bibr">2014</xref>; Furnham et al., <xref rid="pro4541-bib-0019" ref-type="bibr">2014</xref>; Ribeiro et al., <xref rid="pro4541-bib-0047" ref-type="bibr">2018</xref>) but this cannot keep up with the rapid increase in proteins in need of annotation. The number of proteins of the Protein Data Bank (PDB) (Berman et al., <xref rid="pro4541-bib-0008" ref-type="bibr">2002</xref>) increases each year, and AlphaFold (Jumper et al., <xref rid="pro4541-bib-0033" ref-type="bibr">2021</xref>) has added high‐quality predicted structures for hundreds of thousands more. This explosion of protein structure data necessitates the development of computational methods for identifying, characterizing, and comparing functional sites at proteome scale.</p>
    <p>Many widely used methods for protein function identification are based on sequence. Sequence profiles and hidden Markov models built using homologous proteins (Bernhofer et al., <xref rid="pro4541-bib-0009" ref-type="bibr">2021</xref>; El‐Gebali et al., <xref rid="pro4541-bib-0017" ref-type="bibr">2019</xref>; Haft et al., <xref rid="pro4541-bib-0024" ref-type="bibr">2013</xref>; Mi et al., <xref rid="pro4541-bib-0041" ref-type="bibr">2005</xref>; Mitchell et al., <xref rid="pro4541-bib-0042" ref-type="bibr">2019</xref>) are often used to infer function by membership in a particular family, but these methods do not always identify specific functional residues and can misannotate proteins in mechanistically diverse families (Schnoes et al., <xref rid="pro4541-bib-0052" ref-type="bibr">2009</xref>). Additionally, structure and function are often conserved even when sequence similarity is very low, resulting in large numbers of false negatives for methods based on sequence alignment (Rost, <xref rid="pro4541-bib-0050" ref-type="bibr">1999</xref>; Tian &amp; Skolnick, <xref rid="pro4541-bib-0057" ref-type="bibr">2003</xref>). Approaches based on identifying conserved sequence motifs within families can help to address these issues (Attwood, <xref rid="pro4541-bib-0006" ref-type="bibr">2002</xref>; Sigrist et al., <xref rid="pro4541-bib-0053" ref-type="bibr">2013</xref>). However, these methods suffer from similar limitations as sequences diverge, resulting in high false positive and false negative rates, especially when the functional residues are far apart in sequence (Fetrow &amp; Skolnick, <xref rid="pro4541-bib-0018" ref-type="bibr">1998</xref>). More generally, sequence‐based methods cannot capture the complex 3D conformations and physicochemical interactions required to accurately define a functional site or inform opportunities to engineer or mutate specific residues.</p>
    <p>Recently, methods have applied machine learning to predict function from sequence (Kulmanov &amp; Hoehndorf, <xref rid="pro4541-bib-0036" ref-type="bibr">2020</xref>; Sanderson et al., <xref rid="pro4541-bib-0051" ref-type="bibr">2021</xref>) or structure (Gligorijević et al., <xref rid="pro4541-bib-0022" ref-type="bibr">2021</xref>). However, like profile‐based methods, these lack the local resolution necessary to identify specific functional sites, and their reliance on nonspecific functional labels such as those provided by Gene Ontology terms (Ashburner et al., <xref rid="pro4541-bib-0005" ref-type="bibr">2000</xref>) often limits practical utility (Ramola et al., <xref rid="pro4541-bib-0046" ref-type="bibr">2022</xref>). Machine learning approaches that focus on local functional sites are either specific to a particular type of site (e.g., ligand binding, [Tubiana et al., <xref rid="pro4541-bib-0062" ref-type="bibr">2022</xref>]; [Zhao et al., <xref rid="pro4541-bib-0070" ref-type="bibr">2020</xref>] enzyme active sites [Moraes et al., <xref rid="pro4541-bib-0043" ref-type="bibr">2017</xref>]) or require building specific models for each functional site of interest, (Buturovic et al., <xref rid="pro4541-bib-0011" ref-type="bibr">2014</xref>; Torng &amp; Altman, <xref rid="pro4541-bib-0059" ref-type="bibr">2019a</xref>) which can be computationally expensive and demands sufficient data to train an accurate model.</p>
    <p>A major consideration for building generalizable machine learning models for protein sites is the choice of local structure representation. FEATURE, (Bagley &amp; Altman, <xref rid="pro4541-bib-0007" ref-type="bibr">2008</xref>) a hand‐crafted property‐based representation, has shown utility for many functionally‐relevant tasks (Buturovic et al., <xref rid="pro4541-bib-0011" ref-type="bibr">2014</xref>; Liu &amp; Altman, <xref rid="pro4541-bib-0038" ref-type="bibr">2011</xref>; Tang &amp; Altman, <xref rid="pro4541-bib-0056" ref-type="bibr">2014</xref>). However, FEATURE uses heterogeneous features (a mix of counts, binary, and continuous) which are more difficult to train on and meaningfully compare in high dimensions. Additionally, FEATURE consists of radial features without considering orientation and does not account for interactions between atoms in 3D, leading to loss of information (Torng &amp; Altman, <xref rid="pro4541-bib-0059" ref-type="bibr">2019a</xref>). Deep learning presents an attractive alternative by enabling the extraction of features directly from raw data, (LeCun et al., <xref rid="pro4541-bib-0037" ref-type="bibr">2015</xref>) but the high complexity of deep learning models means that they require large amounts of labeled data. To address this, a paradigm has emerged in which models are pre‐trained on very large unlabeled datasets to extract robust and generalizable features which can then be “transferred” to downstream tasks (Hu et al., <xref rid="pro4541-bib-0029" ref-type="bibr">2019</xref>; Oquab et al., <xref rid="pro4541-bib-0044" ref-type="bibr">2014</xref>). This approach has been successfully applied to learn representations of small molecules (Duvenaud et al., <xref rid="pro4541-bib-0016" ref-type="bibr">2015</xref>; Gilmer et al., <xref rid="pro4541-bib-0021" ref-type="bibr">2017</xref>) and protein sequences, (Alley et al., <xref rid="pro4541-bib-0003" ref-type="bibr">2019</xref>; Rives et al., 2019; Sanderson et al., <xref rid="pro4541-bib-0051" ref-type="bibr">2021</xref>) but there are few examples of representations learned directly from 3D structure. Initial efforts focus on entire proteins rather than sites and operate only at residue‐level resolution (Hermosilla &amp; Ropinski, <xref rid="pro4541-bib-0025" ref-type="bibr">2021</xref>; Zhang et al., <xref rid="pro4541-bib-0069" ref-type="bibr">2022</xref>).</p>
    <p>We address these issues by developing compressed latents learned from aligned protein structural environments (COLLAPSE), a framework for functional site characterization, identification, and comparison which (a) focuses on local structural sites, defined as all atoms within a 10 Å radius of a specific residue; (b) captures complex 3D interactions at atom resolution; (c) works with arbitrary sites, regardless of the number of known examples; and (d) enables comparison between sites across proteins. COLLAPSE combines self‐supervised methods from computer vision, (Grill et al., <xref rid="pro4541-bib-0023" ref-type="bibr">2020</xref>) graph neural networks designed for protein structure, (Jing et al., <xref rid="pro4541-bib-0031" ref-type="bibr">2020</xref>; Jing et al., <xref rid="pro4541-bib-0030" ref-type="bibr">2021</xref>) and multiple sequence alignments of homologous proteins to learn 512‐dimensional protein site embeddings that capture structure–function relationships both within and between proteins.</p>
    <p>Self‐supervised representation learning refers to the procedure of training a model to extract high‐level features from raw data using one or more “pretext tasks” defined using intrinsic characteristics of the input data. The choice of pretext task is critical to the utility of the learned representations. A popular class of methods involves minimizing the distance between the embeddings of two augmented versions of the same data point (for example, cropped and rotated views of the same image), thereby learning a representation that is robust to noise which is independent of the fundamental features of the original data (Che et al., <xref rid="pro4541-bib-0012" ref-type="bibr">2020</xref>; Chen &amp; He, <xref rid="pro4541-bib-0013" ref-type="bibr">2020</xref>; Grill et al., <xref rid="pro4541-bib-0023" ref-type="bibr">2020</xref>). Since function is largely conserved within a protein family, we draw an analogy between homologous proteins and augmented views of the same image. Specifically, we hypothesized that by pulling together the embeddings of corresponding sites in homologous proteins, we could train the model to learn features which capture the site's structural and functional role. In this scheme, sequence alignments are used to identify correspondences between amino acids, which are then mapped to 3D structures to define the structural site surrounding each residue (Figure <xref rid="pro4541-fig-0001" ref-type="fig">1</xref>, Section <xref rid="pro4541-sec-0004" ref-type="sec">2.2</xref>).</p>
    <fig position="float" fig-type="FIGURE" id="pro4541-fig-0001">
      <label>FIGURE 1</label>
      <caption>
        <p>Schematic of a single iteration of COLLAPSE algorithm. Clockwise from the top left, we show (1 a,b) the process of sampling a pair of sites from the MSA, (2) extracting the corresponding structural environments, and (3) converting into a spatial graph. The inset shows the node and edge featurization scheme. Finally, we show (4) a schematic of the network architecture, consisting of paired graph neural networks followed by mean pooling over all nodes to produce site embeddings. (5) these embeddings are then compared using a loss function weighted by the conservation of the position in the MSA, as shown by the sequence logo in center left</p>
      </caption>
      <graphic xlink:href="PRO-32-e4541-g005" position="anchor" id="jats-graphic-1"/>
    </fig>
    <p>Pre‐trained representations are typically used in one of two settings: (a) transfer learning, which leverages general representations to improve performance on problem‐specific supervised tasks where access to labeled data is limited; and (b) extracting insights about the underlying data from the learned embedding space directly (e.g., via visualization or embedding comparisons) (Detlefsen et al., <xref rid="pro4541-bib-0015" ref-type="bibr">2022</xref>). In this paper, we illustrate the utility of COLLAPSE protein site in both settings. First, we demonstrate that COLLAPSE generalizes in a transfer learning setting, achieving competitive or best‐in‐class results across a range of downstream tasks. Second, we describe two applications that demonstrate the power of our embeddings for protein function analysis without the need to train any downstream models: an iterated search procedure for identifying similar functional sites across large protein databases, and a method for efficiently annotating putative functional sites in an unlabeled protein. All datasets, models, functionality, and source code can be found in our Github repository (<ext-link xlink:href="https://github.com/awfderry/COLLAPSE" ext-link-type="uri">https://github.com/awfderry/COLLAPSE</ext-link>).</p>
  </sec>
  <sec sec-type="results" id="pro4541-sec-0002">
    <label>2</label>
    <title>RESULTS</title>
    <sec id="pro4541-sec-0003">
      <label>2.1</label>
      <title>Intrinsic evaluation of <styled-content style="fixed-case" toggle="no">COLLAPSE</styled-content> embeddings</title>
      <p>To evaluate the extent to which COLLAPSE embeddings capture relevant structural and functional features, we embedded the environments of all residues in a held‐out set consisting of proteins with varying levels of sequence similarity to proteins in the training set. First, we find that the degree of similarity between embeddings of aligned sites is correlated with the level of conservation of that site in the multiple sequence alignment (MSA) (Figure <xref rid="pro4541-fig-0002" ref-type="fig">2a</xref>). Even at less than 30% conservation, aligned sites are significantly more similar on average than a randomly sampled background of nonaligned sites <mml:math id="jats-math-1" display="inline" overflow="scroll"><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>15</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math>
</p>
      <fig position="float" fig-type="FIGURE" id="pro4541-fig-0002">
        <label>FIGURE 2</label>
        <caption>
          <p>Analysis of learned embeddings. (a) Raw cosine similarity distributions (i.e., before quantile transformation) of aligned sites, binned by the sequence conservation of the corresponding column of the MSA. Highly conserved positions also have highly similar embeddings, but even less conserved positions have more similar embeddings than randomly sampled nonaligned sites (in pink). (b) Spatial sensitivity of embedding similarity, as measured by the sequence distance between two sites. Results are stratified by the average distance to the closest training protein, demonstrating that neighboring embeddings can be readily distinguished even for proteins with very low similarity to the training set. (c) tSNE projection of average protein‐level embeddings for single‐domain chains, colored by the highest‐level CATH class, showing that embeddings effectively capture secondary structure patterns</p>
        </caption>
        <graphic xlink:href="PRO-32-e4541-g001" position="anchor" id="jats-graphic-3"/>
      </fig>
      <p>We also confirmed that our embeddings capture local information at a residue‐level resolution, meaning that neighboring environments can be effectively distinguished from each other. Indeed, the normalized cosine similarity between residue embeddings decreases between the residues in sequence increases (Figure <xref rid="pro4541-fig-0002" ref-type="fig">2b</xref>). This effect generalizes even to proteins far away from the training set in sequence identity. Finally, among chains with a single fold according to CATH 4.2 (Orengo et al., <xref rid="pro4541-bib-0045" ref-type="bibr">1997</xref>) (<mml:math id="jats-math-2" display="inline" overflow="scroll"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>11,270</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:math>, the top‐level structural class can be distinguished clearly in protein‐level embeddings, suggesting that secondary structure is a major feature captured by COLLAPSE (Figure <xref rid="pro4541-fig-0002" ref-type="fig">2c</xref>). Lower levels of the CATH hierarchy also cluster clearly in low‐dimensional space (Figure <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S1</xref>).</p>
    </sec>
    <sec id="pro4541-sec-0004">
      <label>2.2</label>
      <title>Transfer learning and fine‐tuning to improve performance on supervised tasks</title>
      <p>To assess COLLAPSE in a transfer learning context, we use ATOM3D, a suite of benchmarking tasks and datasets for machine learning in structural biology (Townshend et al., <xref rid="pro4541-bib-0061" ref-type="bibr">2021</xref>). We selected two tasks from ATOM3D which focus on protein sites: protein interface prediction (PIP) and mutation stability prediction (MSP). We compare performance to the ATOM3D reference models and to the task‐specific GVP‐GNN reported in (Jing et al., <xref rid="pro4541-bib-0030" ref-type="bibr">2021</xref>), which is state‐of‐the‐art for all tasks. Table <xref rid="pro4541-tbl-0001" ref-type="table">1</xref> reports the results both with and without fine‐tuning the embedding model parameters. Without fine‐tuning, COLLAPSE embeddings and a simple classifier achieve results comparable or better than the ATOM3D reference models trained specifically for each task. Fine‐tuning improves performance further, achieving state‐of‐the‐art on PIP and comparable performance to the GVP‐GNN on MSP, outperforming FEATURE as well as the ATOM3D baselines (Table <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S6</xref>). As an external evaluation, we also evaluated COLLAPSE on the prediction of protein–protein interaction sites compared to MaSIF, a deep learning model designed for protein surfaces (Gainza et al., <xref rid="pro4541-bib-0020" ref-type="bibr">2020</xref>). Our models achieve close to the performance of MaSIF and better than baseline methods despite the fact that the randomly‐sampled surface points of the MaSIF dataset are out‐of‐distribution for our model pre‐trained on environments centered around individual residues (Figure <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S8</xref>).</p>
      <table-wrap position="float" id="pro4541-tbl-0001" content-type="TABLE">
        <label>TABLE 1</label>
        <caption>
          <p>Performance of models trained on ATOM3D benchmark tasks</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">Task (metric)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">COLLAPSE (fixed)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">COLLAPSE (fine‐tuned)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">ATOM3D 3DCNN</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">ATOM3D GNN</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">ATOM3D ENN</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">GVP‐GNN</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">PIP (AUROC)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.848 ± 0.018</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>0.881 ± 0.004</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.844 ± 0.002</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.669 ± 0.001</td>
              <td align="left" valign="top" rowspan="1" colspan="1">N/A</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.866 ± 0.004</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">MSP (AUROC)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.616 ± 0.006</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>0.668 ± 0.018</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.574 ± 0.005</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.621 ± 0.009</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0.574 ± 0.040</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>0.680 ± 0.015</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="pro4541-ntgp-0002">
          <fn id="pro4541-note-0002">
            <p><italic toggle="yes">Note</italic>: Comparisons are made with ATOM3D reference architectures—3D convolutional neural network (3DCNN), graph neural network (GNN), and equivariant neural network (ENN)—as well as the geometric vector perceptron (GVP‐GNN) results reported in Jing et al. (<xref rid="pro4541-bib-0030" ref-type="bibr">2021</xref>), which is state‐of‐the‐art for these datasets. The metric is area under the receiver operator characteristic curve (AUROC), and we report mean and standard deviation across three training runs. Numbers in bold indicate best performance on each task (within one standard deviation).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="pro4541-sec-0005">
      <label>2.3</label>
      <title>Building functional site prediction models using <styled-content style="fixed-case" toggle="no">COLLAPSE</styled-content> embeddings</title>
      <p>We train prediction models for 10 functional sites defined by the <sc>prosite</sc> database, (Sigrist et al., <xref rid="pro4541-bib-0053" ref-type="bibr">2013</xref>) which identifies local sites using curated sequence motifs. On sites labeled true positive (TP) by <sc>prosite</sc>, COLLAPSE outperforms the analogous FEATURE models and perform comparably or better than task‐specific 3DCNN models trained end‐to‐end, achieving greater than 86% recall on all sites at a threshold of 99% precision (Figure <xref rid="pro4541-fig-0003" ref-type="fig">3</xref>). <sc>prosite</sc> also provides false negatives (FNs; true proteins which are not recognized by the <sc>prosite</sc> pattern) and false positives (FPs; proteins which match the <sc>prosite</sc> pattern but are not members of the functional family). Table <xref rid="pro4541-tbl-0002" ref-type="table">2</xref> shows the number of proteins correctly reclassified by the models trained on TP sites. For all families, COLLAPSE correctly identifies a greater or equal number of FN proteins compared to FEATURE and 3DCNN classifiers. The improvement is notable in some cases, such as a 162.5% increase in proteins detected for IG_MHC, a 37.5% increase for ADH_SHORT, and a 17.6% increase for EF_HAND_1. For four of the seven proteins with FP data, we correctly rule out all FPs. For ADH_SHORT and EF_HAND_1, we perform 9.1% and 4.0% worse relative to 3DCNN, respectively, but this slight increase in FPs is not substantial relative to the improvement in FNs recovered for these families. To evaluate our embeddings beyond <sc>prosite</sc>, we also trained functional residue prediction models on four benchmark datasets from (Xin &amp; Radivojac, <xref rid="pro4541-bib-0067" ref-type="bibr">2011</xref>). Models based on COLLAPSE embeddings perform significantly better than the best‐performing method on the prediction of zinc binding sites and enzyme catalytic sites (15.3% and 9.3% improvement, respectively), and within 10% of the best‐performing method on DNA binding and phosphorylation sites (Table <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S5</xref>).</p>
      <table-wrap position="float" id="pro4541-tbl-0002" content-type="TABLE">
        <label>TABLE 2</label>
        <caption>
          <p>Performance of models trained on <sc>prosite</sc> TP/TN on held‐out <sc>prosite</sc> FP/FN annotations</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">Site</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1"><sc>prosite</sc> label</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">COLLAPSE</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">FEATURE</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">3DCNN</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1"><sc>prosite</sc> total</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td rowspan="2" align="left" valign="top" colspan="1">ADH_SHORT</td>
              <td align="left" valign="top" rowspan="1" colspan="1">FN</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>11</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">8</td>
              <td align="left" valign="top" rowspan="1" colspan="1">7</td>
              <td align="left" valign="top" rowspan="1" colspan="1">14</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">FP</td>
              <td align="left" valign="top" rowspan="1" colspan="1">30</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>33</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>33</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">33</td>
            </tr>
            <tr>
              <td rowspan="2" align="left" valign="top" colspan="1">EF_HAND_1</td>
              <td align="left" valign="top" rowspan="1" colspan="1">FN</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>40</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">28</td>
              <td align="left" valign="top" rowspan="1" colspan="1">34</td>
              <td align="left" valign="top" rowspan="1" colspan="1">48</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">FP</td>
              <td align="left" valign="top" rowspan="1" colspan="1">120</td>
              <td align="left" valign="top" rowspan="1" colspan="1">106</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>125</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">128</td>
            </tr>
            <tr>
              <td rowspan="2" align="left" valign="top" colspan="1">EGF_1</td>
              <td align="left" valign="top" rowspan="1" colspan="1">FN</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>60</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">34</td>
              <td align="left" valign="top" rowspan="1" colspan="1">58</td>
              <td align="left" valign="top" rowspan="1" colspan="1">90</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">FP</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>19</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>19</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>19</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">19</td>
            </tr>
            <tr>
              <td rowspan="2" align="left" valign="top" colspan="1">IG_MHC</td>
              <td align="left" valign="top" rowspan="1" colspan="1">FN</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>21</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">8</td>
              <td align="left" valign="top" rowspan="1" colspan="1">8</td>
              <td align="left" valign="top" rowspan="1" colspan="1">47</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">FP</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>31</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>31</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>31</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">31</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">PROTEIN_KINASE_ST</td>
              <td align="left" valign="top" rowspan="1" colspan="1">FN</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>269</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">264</td>
              <td align="left" valign="top" rowspan="1" colspan="1">268</td>
              <td align="left" valign="top" rowspan="1" colspan="1">271</td>
            </tr>
            <tr>
              <td rowspan="2" align="left" valign="top" colspan="1">PROTEIN_KINASE_TYR</td>
              <td align="left" valign="top" rowspan="1" colspan="1">FN</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>3</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>3</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>3</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">3</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">FP</td>
              <td align="left" valign="top" rowspan="1" colspan="1">14</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>20</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>20</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">20</td>
            </tr>
            <tr>
              <td rowspan="2" align="left" valign="top" colspan="1">TRYPSIN_HIS</td>
              <td align="left" valign="top" rowspan="1" colspan="1">FN</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>10</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">3</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>10</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">16</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">FP</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>4</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>4</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>4</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">4</td>
            </tr>
            <tr>
              <td rowspan="2" align="left" valign="top" colspan="1">TRYPSIN_SER</td>
              <td align="left" valign="top" rowspan="1" colspan="1">FN</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>9</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>9</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>9</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">12</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">FP</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>1</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>1</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <bold>1</bold>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">1</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="pro4541-ntgp-0003">
          <fn id="pro4541-note-0003">
            <p><italic toggle="yes">Note</italic>: Comparisons are made with FEATURE and 3DCNN numbers as reported in Torng et al. (<xref rid="pro4541-bib-0060" ref-type="bibr">2019</xref>). The number of proteins which are correctly reclassified (i.e., FPs predicted as negative, FNs predicted as positive) is reported for each method (higher is better). Numbers in bold indicate best performance on each site.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <fig position="float" fig-type="FIGURE" id="pro4541-fig-0003">
        <label>FIGURE 3</label>
        <caption>
          <p>Performance of models trained on true positives from 10 <sc>prosite</sc> functional sites in 5‐fold cross‐validation: COLLAPSE embeddings + SVM (blue), 3DCNN trained end‐to‐end (dark gray), and FEATURE vectors + SVM (light gray). Metric is the recall for all TP annotations at a threshold, which produces 99% precision. COLLAPSE achieves better recall than FEATURE and better or comparable recall to the 3DCNN</p>
        </caption>
        <graphic xlink:href="PRO-32-e4541-g004" position="anchor" id="jats-graphic-5"/>
      </fig>
    </sec>
    <sec id="pro4541-sec-0006">
      <label>2.4</label>
      <title>Iterative search for functional sites across protein databases</title>
      <p>While COLLAPSE embeddings can be used to train highly accurate models for functional site detection, we can only train such models for those functional sites for which we have sufficient training examples. Another way to understand the possible function of a site is to analyze similar sites retrieved from a structure database. The set of hits retrieved by this search may contain known functional annotations or other information which sheds light on the query site. We use iterative COLLAPSE embedding comparisons to perform such a search across the PDB. We investigate the performance of this method on the <sc>prosite</sc> dataset while varying two parameters: the number of iterations and the p‐value cutoff for inclusion at each iteration. The method generally achieves high recall and precision after 2–5 iterations at a p‐value cutoff of <mml:math id="jats-math-3" display="inline" overflow="scroll"><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>–</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math> (Figure <xref rid="pro4541-fig-0004" ref-type="fig">4</xref>; Figure <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S4</xref>). Notably, when evaluating on the FN and FP subsets, our search method even outperforms the cross‐validated models on some sites (e.g., IG_MHC, Figure <xref rid="pro4541-fig-0004" ref-type="fig">4a</xref>). However, the precision and recall characteristics vary widely across families; in some cases it predicts the same set of proteins as the trained model (e.g., TRYPSIN_HIS; Figure <xref rid="pro4541-fig-0004" ref-type="fig">4b</xref>), while in others it performs worse (e.g., EF_HAND_1; Figure <xref rid="pro4541-fig-0004" ref-type="fig">4c</xref>). Importantly, the method requires no training and is very efficient: runtime per iteration scales linearly with the size of the query set and with database size (Figure <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S5</xref>).</p>
      <fig position="float" fig-type="FIGURE" id="pro4541-fig-0004">
        <label>FIGURE 4</label>
        <caption>
          <p>Iterated functional site search performance per iteration for three <sc>prosite</sc> families. Colors denote different user‐specified empirical p‐value cutoffs and error bars represent variance over three randomly sampled queries. From left to right, metrics shown are: Precision across all results (including TP, FP, and FN), recall across all results, proportion of TP sites predicted correctly, proportion of FN sites predicted correctly, and proportion of FP sites predicted correctly. For FN and FP, the performance of our CV‐trained models is shown as a red dashed line. Sample error is shown for three random starting queries. The three families shown are (a) IG_MHC, (b) TRYPSIN_HIS, and (c) EF_HAND_1, in order of relative performance compared to CV‐trained models. While the performance characteristics vary across sites, the number of iterations and p‐value cutoff can be tuned to achieve good performance</p>
        </caption>
        <graphic xlink:href="PRO-32-e4541-g002" position="anchor" id="jats-graphic-7"/>
      </fig>
    </sec>
    <sec id="pro4541-sec-0007">
      <label>2.5</label>
      <title>Annotation of functional sites in protein structures</title>
      <p>Our iterative search method assumes that a site of interest has already been identified. However, when a new protein is discovered and its structure is solved, the locations of functional sites are often unknown. By comparing local environments in the protein's structure to those contained in databases of known functional sites, we can predict which sites are likely to be functional. Figure <xref rid="pro4541-fig-0005" ref-type="fig">5</xref> shows two example annotations using a modified mutual best hit criterion against a reference database consisting of embeddings from <sc>prosite</sc> and the catalytic site atlas (CSA). First, we show the structure of meizothrombin, a precursor to thrombin and a trypsin‐like serine protease with a canonical His‐Asp‐Ser catalytic triad. Our method correctly identifies all three residues as belonging to the trypsin‐like serine protease family in <sc>prosite</sc> (Figure <xref rid="pro4541-fig-0005" ref-type="fig">5a</xref>). Hits against the CSA, which are more specific, also include closely homologous proteins such as C3/C5 convertase. The associated kringle domain is also identified by its characteristic disulfide bond. Second, we show the structure of beta‐glucuronidase (Figure <xref rid="pro4541-fig-0005" ref-type="fig">5b</xref>), a validation set protein which has no homologs in the training set. We correctly identify all four catalytic residues defined by the CSA (in yellow), as well as <sc>prosite</sc> signatures corresponding to the glycosyl hydrolases family 2, the family which contains beta‐glucuronidase.</p>
      <fig position="float" fig-type="FIGURE" id="pro4541-fig-0005">
        <label>FIGURE 5</label>
        <caption>
          <p>Results of functional annotation tool applied to (a) meizothrombin (PDB ID 1A0H) and (b) beta‐glucuronidase (PDB ID 3HN3), both at <italic toggle="yes">p</italic> &lt; <mml:math id="jats-math-4" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>. No member of the beta‐glucuronidase family is in the training set (maximum sequence identity 2.8%). Functional residues identified by our method are shown as spheres, with colors corresponding to the functional site. Hits labeled in bold are also significant at a more stringent cutoff (<italic toggle="yes">p</italic> &lt; <mml:math id="jats-math-5" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math>). All hits represent either the correct function or those of very closely related proteins, showing that COLLAPSE is effective for annotation of proteins whether or not similar proteins are present in the training set</p>
        </caption>
        <graphic xlink:href="PRO-32-e4541-g003" position="anchor" id="jats-graphic-9"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="pro4541-sec-0008">
    <label>3</label>
    <title>DISCUSSION</title>
    <sec id="pro4541-sec-0009">
      <label>3.1</label>
      <title>The <styled-content style="fixed-case" toggle="no">COLLAPSE</styled-content> self‐supervised training framework enables the learning of rich, informative embeddings of functional sites</title>
      <p>The utility of COLLAPSE embeddings for functional analysis derives from several key features of the training algorithm. First, the use of homology as a source of self‐supervision signal allows the model to learn patterns of structural conservation across proteins, imbuing the model with a biological inductive bias towards features that may be important to the protein's function. Such patterns could in theory be learned by a model which sees each protein independently, but it would require much more data and training time to identify subtle signals across disparate proteins. The bootstrap training objective not only forces the model to learn a distance function for comparing embeddings that meaningfully captures the functional relationship between sites, but by sampling pairs of proteins and residues each iteration it also greatly increases the effective size of the training set relative to models that take in a single protein (or even residue) at a time. Indeed, we have found empirically that nonbootstrap objectives (e.g., those based on masked residue modeling or autoencoders) produce representations that are much less informative for functional tasks. While MSAs have proved crucial to the success of sequence‐based models, to our knowledge this is the first time they have been used to provide a supervision signal for a structure‐based model.</p>
      <p>Second, by focusing on local protein sites, our embeddings are more precise and flexible than models which produce a single representation of an entire protein. COLLAPSE embeddings can be used for arbitrary tasks on the level of single residues or even individual functional atoms, to detect important regions in proteins, and to identify functional relationships between proteins even if they are divergent in sequence or global fold. Moreover, by aggregating over multiple residues or entire proteins, site‐specific embeddings can also be applied to domain‐level or full‐protein tasks. For simple tasks such as distinguishing high‐level CATH class (Figure <xref rid="pro4541-fig-0002" ref-type="fig">2c</xref>), a naive approach of averaging over all residues is sufficient, but more complex aggregation methods will be necessary for many problems. As a proof of concept, we trained a simple recurrent neural network (RNN) model on sequences of COLLAPSE embeddings to make protein‐level predictions of fold and enzyme class (Supplementary Note 9). Compared to 16 other sequence and structure based methods, (Hermosilla et al., <xref rid="pro4541-bib-0025" ref-type="bibr">2020</xref>) our RNNs trained on COLLAPSE embeddings rank in the top five across all tasks and testing sets (Figure <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S7</xref>).</p>
      <p>Finally, by using an atomic graph representation and a GVP‐GNN encoder, COLLAPSE captures all inter‐atomic interactions (in contrast to methods which operate at a residue level) and produces representations that are fully equivariant to 3D rotation and translation. The importance of capturing local structural features in functional site analysis is demonstrated by the improved performance of COLLAPSE relative to sequence‐based methods such as MMSeqs2 (Steinegger &amp; Söding, <xref rid="pro4541-bib-0501" ref-type="bibr">2017</xref>) (Table <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S4</xref>), especially on the more difficult FN and FP proteins. While sequence embeddings from ESM‐1b (Rives et al., <xref rid="pro4541-bib-0049" ref-type="bibr">2021</xref>) do produce very good predictive models for certain sites, they dramatically overfit to others (notably IG_MHC) and generally underperform across all sites in search and annotation applications (Figures <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S9–S10</xref>, Supplementary Note 5). This suggests that sequence and structure representations are complementary, and many tasks may benefit from a combination of the two approaches.</p>
    </sec>
    <sec id="pro4541-sec-0010">
      <label>3.2</label>
      <title><styled-content style="fixed-case" toggle="no">COLLAPSE</styled-content> produces general‐purpose representations which facilitate the improvement of computational methods for diverse applications in protein function analysis</title>
      <p>COLLAPSE is effective in transfer learning and as fixed embeddings, and generalizes across tasks that require the model to learn different aspects of the protein structure–function relationship. Although it may not be the state‐of‐the‐art on every task, its competitive performance in every context we tested demonstrates its utility as a general‐purpose representation. This flexibility makes COLLAPSE embeddings ideal for not only building new predictive models and performing comparative analyses, but also for easily incorporating structural information into existing computational methods. Given the improved performance over FEATURE across our benchmarks, we also expect that substituting COLLAPSE embeddings will lead to improved performance for most applications addressed by the FEATURE suite of methods (Liu &amp; Altman, <xref rid="pro4541-bib-0038" ref-type="bibr">2011</xref>; Tang &amp; Altman, <xref rid="pro4541-bib-0056" ref-type="bibr">2014</xref>; Torng &amp; Altman, <xref rid="pro4541-bib-0060" ref-type="bibr">2019b</xref>).</p>
      <p>Another important aspect which sets COLLAPSE apart from task‐specific machine learning methods is the ability to perform meaningful comparisons between functional sites directly in the embedding space. Due to the bootstrap pre‐training objective, the embedding distance provides a functionally relevant distance measure for comparing sites. Note that for all comparisons, our method of standardizing embedding comparisons is critical for determining their statistical significance as well as increasing their effective range (Supplementary Note 1). We demonstrate the benefits of using comparisons directly in the embedding space by developing methods for functional site search and annotation, both of which are efficient, generalizable, and allow a user to tune the sensitivity and specificity of the results. For example, for discovery applications it may be desirable to optimize for sensitivity at the cost of more false positives, while prioritizing drug targets for experimental validation may require greater specificity.</p>
      <p>The ability of iterative nearest‐neighbor searches in the embedding space to identify known sites in <sc>prosite</sc> demonstrates that functional sites cluster meaningfully in the embedding space. The effect of changing input parameters (number of iterations and p‐value cutoff) on the sensitivity and specificity of the results varies somewhat across functional families. In some cases (notably IG_MHC), this method achieves better sensitivity for FNs than even machine learning models trained using CV, while in others (EF_HAND_1, PROTEIN_KINASE_TYR) it cannot achieve this without a significant drop in precision. This is likely due to differences in structural conservation between sites, whereby sites which are more structurally heterogeneous are more difficult to fully capture using a nearest‐neighbor approach than a trained model which can learn to recognize diverse structural patterns. However, since training an accurate model requires access to a representative training dataset which is not always available, we consider our search method to be a powerful complement to site‐specific models in cases where labeled data is scarce or where the similarity to a specific query is important. We also note that while structural search methods exist for full proteins (Holm &amp; Rosenström, <xref rid="pro4541-bib-0027" ref-type="bibr">2010</xref>; van Kempen et al., <xref rid="pro4541-bib-0064" ref-type="bibr">2022</xref>) or binding sites, (Liu &amp; Altman, <xref rid="pro4541-bib-0038" ref-type="bibr">2011</xref>; Valasatava et al., <xref rid="pro4541-bib-0063" ref-type="bibr">2014</xref>; Zemla et al., <xref rid="pro4541-bib-0068" ref-type="bibr">2022</xref>) ours is the first search tool specifically designed for arbitrary local structural sites.</p>
      <p>Functional annotation of novel protein structures is of great value to the structural biology and biochemistry communities, but there are few tools for doing so at the residue level. COLLAPSE provides a method for residue‐level annotation which is efficient and tunable, making it suitable for both screening and discovery purposes. As shown by the examples in Figure <xref rid="pro4541-fig-0005" ref-type="fig">5</xref>, the method identifies known functional annotations while limiting false positives to closely related homologs, even when the input is not related to any protein in the training set (&lt;5% sequence identity for beta‐glucuronidase). Confidence in a new prediction's accuracy can be assessed by its significance level or more sophisticated evaluations for example, multiple neighboring residues being annotated with the same function may imply a more likely correct prediction. Additionally, because we can identify many sites on a single protein, it is also possible to identify previously unknown alternative functional sites or allosteric sites. Importantly, all predictions can be explained and cross‐referenced by rich metadata from the reference data sources, enhancing trust and usability. Of the PDBs returned for true positive sites in meizothrombin and beta‐glucuronidase, 45.5% (20/44) and 87.5% (14/16), respectively, were not hits in a protein PSI‐BLAST search with standard parameters, demonstrating the value of local structural comparisons for functional annotation. Additionally, the method is easy to update and extend over time via the addition of new sources of functional data, and reference databases can even be added or removed on a case‐by‐case basis.</p>
    </sec>
    <sec id="pro4541-sec-0011">
      <label>3.3</label>
      <title>Advances in protein structure prediction provide great opportunities for expanding functional analysis and discovery</title>
      <p>COLLAPSE depends on the availability of solved 3D protein structures in the PDB. This restricts not only the number of homologous proteins that can be compared at each training step, but also the set of protein families which can even be considered—less than one third of alignments in the CDD contained at least two proteins with structures in the PDB. Including structures from AlphaFold Structure Database (Varadi et al., <xref rid="pro4541-bib-0065" ref-type="bibr">2022</xref>) would dramatically increase the coverage of our training dataset, but the utility of including predicted structures alongside experimentally solved structures in training or evaluation of machine learning models still needs to be evaluated (Derry et al., <xref rid="pro4541-bib-0014" ref-type="bibr">2022</xref>). A preliminary evaluation of our annotation method on the predicted structure for meizothrombin reveals high agreement with the corresponding PDB structure (Figure <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S6</xref>) despite a root‐mean‐square deviation of 3.67 Å between the two structures, suggesting that COLLAPSE may already generalize to AlphaFold predictions for some proteins. Given this finding, we anticipate that COLLAPSE will be a powerful tool for functional discovery within the AlphaFold database, which has already yielded several novel insights (Akdel et al., <xref rid="pro4541-bib-0001" ref-type="bibr">2021</xref>; Bordin et al., <xref rid="pro4541-bib-0010" ref-type="bibr">2022</xref>).</p>
      <p>In summary, COLLAPSE is a general‐purpose protein structure embedding method for functional site analysis. We provide a Python package and command‐line tools for generating embeddings for any protein site, conducting functional site searches, and annotating input protein structures. We also provide downloadable databases of embeddings for a nonredundant subset of the PDB and for known functional sites. We anticipate that as more data becomes available, these tools will serve as a catalyst for data‐driven biological discovery and become a critical component of the protein research toolkit.</p>
    </sec>
  </sec>
  <sec sec-type="materials-and-methods" id="pro4541-sec-0012">
    <label>4</label>
    <title>MATERIALS AND METHODS</title>
    <sec id="pro4541-sec-0013">
      <label>4.1</label>
      <title>Training dataset and data processing</title>
      <p>COLLAPSE pre‐training relies on a source of high‐quality protein families associated with known structures and functions, as well as multiple sequence alignments (MSAs) in order to define site correspondences. We use the NCBI‐curated subset of the Conserved Domain Database (CDD), (Lu et al., <xref rid="pro4541-bib-0508" ref-type="bibr">2020</xref>; Marchler‐Bauer et al., <xref rid="pro4541-bib-0040" ref-type="bibr">2003</xref>) which explicitly validates domain boundaries using 3D structural information. We downloaded all curated MSAs from the CDD (n = 17,906 as of Sep. 2021) and filtered out those that contained less than two proteins with structures deposited in the PDB. After removing chains with incomplete data or which could not be processed properly, this resulted in 5643 alignments for training, corresponding to 16,931 PDB chains (Figure <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S2</xref>). We then aligned the sequences extracted from the ATOM records in each PDB chain to its MSA, without altering the original alignment, thus establishing the correct mapping from alignment position to PDB residue number. As a held‐out set for validation, we select 1370 families defined by <sc>pfam</sc> (El‐Gebali et al., <xref rid="pro4541-bib-0017" ref-type="bibr">2019</xref>) which do not share a common superfamily cluster (as defined by the CDD) with any training family. We then bin these families based on the average sequence identity to the nearest protein in the training dataset and sample five families from each bin, resulting in 50 validation families with varying levels of similarity to the training data (Table <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S1</xref>).</p>
      <sec id="pro4541-sec-0014">
        <label>4.1.1</label>
        <title>
Definition of sites and environments
</title>
        <p>In general, we define protein sites relative to the location of the relevant residues. Specifically, we define the environment surrounding a protein site as all atoms within 10 Å radius of the functional center of the central residue. The functional center is defined as the centroid of the functional atoms of the side chain as defined by previous work (Bagley &amp; Altman, <xref rid="pro4541-bib-0007" ref-type="bibr">2008</xref>; Torng &amp; Altman, <xref rid="pro4541-bib-0059" ref-type="bibr">2019a</xref>). For residues with two functional centers (Trp and Tyr), during training one is randomly chosen at each iteration, and at inference time the choice depends on the specific application (i.e., if the function being evaluated depends on the aromatic or polar group; see Table <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S2</xref>). If the functional atom is not known (e.g., for annotating unlabeled proteins), we take the average over all heavy side‐chain atoms.</p>
      </sec>
      <sec id="pro4541-sec-0015">
        <label>4.1.2</label>
        <title>
Empirical background calculation
</title>
        <p>To make comparisons more meaningful and to provide a mechanism for calculating statistical significance, we quantile‐transform all cosine similarities relative to an empirical cosine similarity distribution. To compute background distributions, we use a high‐resolution (&lt;2.0 Å), nonredundant subset of the PDB at 30% sequence identity provided by the PISCES server (Wang &amp; Dunbrack, <xref rid="pro4541-bib-0066" ref-type="bibr">2003</xref>) (5833 proteins). We compute the embeddings of 100 sites from each structure, corresponding to five for each amino acid type, sampled with replacement. Exhaustively computing all pairwise similarities is computationally infeasible, so we sample <mml:math id="jats-math-6" display="inline" overflow="scroll"><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>50,000</mml:mn></mml:math> pairs of environments and compute the cosine similarity of each. We performed this procedure to generate empirical similarity distributions <mml:math id="jats-math-7" display="inline" overflow="scroll"><mml:mfenced open="(" close=")" separators=",,"><mml:msub><mml:mi>S</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>…</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mfenced></mml:math> for the entire dataset and for each amino acid individually (Figure <xref rid="pro4541-supitem-0001" ref-type="supplementary-material">S3</xref>). Cosine similarities <mml:math id="jats-math-8" display="inline" overflow="scroll"><mml:mfenced open="(" close=")"><mml:mi>s</mml:mi></mml:mfenced></mml:math> are then quantile‐transformed relative to the relevant empirical cumulative distribution function:<disp-formula id="pro4541-disp-0001"><mml:math id="jats-math-9" display="block" overflow="scroll"><mml:mi>F</mml:mi><mml:mfenced open="(" close=")"><mml:mi>s</mml:mi></mml:mfenced><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mn mathvariant="bold">1</mml:mn><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi></mml:msub><mml:mo mathvariant="bold-italic">&lt;</mml:mo><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula>The <italic toggle="yes">p</italic>‐value for any embedding comparison is then defined as <mml:math id="jats-math-10" display="inline" overflow="scroll"><mml:mn>1</mml:mn><mml:mo>–</mml:mo><mml:mi>F</mml:mi><mml:mfenced open="(" close=")"><mml:mi>s</mml:mi></mml:mfenced></mml:math>, or the probability that a randomly sampled pair of embeddings is at least as similar as the pair in question. Amino acid–specific empirical backgrounds are used for functional site search and are aggregated into a single combined distribution for annotation. For the functional site–specific background used to filter hits during annotation, we use an empirical background computed by comparing each functional site embedding to the embeddings of the corresponding amino acid in the 30% nonredundant PDB subset.</p>
      </sec>
    </sec>
    <sec id="pro4541-sec-0016">
      <label>4.2</label>
      <title><styled-content style="fixed-case" toggle="no">COLLAPSE</styled-content> training algorithm</title>
      <p>Each iteration of the COLLAPSE pre‐training algorithm consists of the following steps, as shown in Figure <xref rid="pro4541-fig-0001" ref-type="fig">1</xref>. We trained our final model using the Adam optimizer (Kingma &amp; Ba, <xref rid="pro4541-bib-0035" ref-type="bibr">2015</xref>) with a learning rate of 1 e‐4 and a batch size of 48 pairs for 1200 epochs on a single TESLA V100 GPU. Model selection and hyperparameter tuning was evaluated using intrinsic embedding characteristics (Section <xref rid="pro4541-sec-0003" ref-type="sec">2.1</xref>) and ATOM3D validation set performance (Section <xref rid="pro4541-sec-0017" ref-type="sec">4.3</xref>). See Supplementary Note 2 for further discussion of hyperparameter selection and modeling choices.<list list-type="bullet" id="pro4541-list-0001"><list-item id="pro4541-li-0001"><p>Step 1. Randomly sample one pair of proteins from the MSA and one aligned position from each protein (i.e., there is not a gap in either protein). Map MSA column position to PDB residue number using the pre‐computed alignment described in Section <xref rid="pro4541-sec-0004" ref-type="sec">2.2</xref>. Note that this step ensures that each epoch, a different pair of residues is sampled from each CDD family, effectively increasing the size of the training dataset by many orders of magnitude relative to a strategy which trains on individual proteins or MSAs.</p></list-item><list-item id="pro4541-li-0002"><p>Step 2. Extract 3D environment around each selected residue (Section <xref rid="pro4541-sec-0014" ref-type="sec">4.1.1</xref>). Only atoms from the same chain are considered. Waters and hydrogens are excluded but ligands, metal ions, and cofactors are included.</p></list-item><list-item id="pro4541-li-0003"><p>Step 3. Convert each environment into a spatial graph <mml:math id="jats-math-11" display="inline" overflow="scroll"><mml:mi mathvariant="script">G</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="script">V</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:mfenced></mml:math>. Each node in the graph represents an atom and is featurized by a one‐hot encoding of the atom type <mml:math id="jats-math-12" display="inline" overflow="scroll"><mml:mi mathvariant="script">V</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="true">{</mml:mo></mml:math> carbon (C), nitrogen (N), oxygen (O), fluorine (F), sulfur (S), chlorine (Cl), phosphorus (P), selenium (Se), iron (Fe), zinc (Zn), calcium (Ca), magnesium (Mg), and “other” <mml:math id="jats-math-13" display="inline" overflow="scroll"><mml:mo stretchy="true">}</mml:mo></mml:math>, representing the most common elements found in the PDB. Edges in the graph are defined between any pair of atoms separated by than 4.5 Å. Following (Jing et al., <xref rid="pro4541-bib-0030" ref-type="bibr">2021</xref>) edges between atoms <mml:math id="jats-math-14" display="inline" overflow="scroll"><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced></mml:math> with coordinates <mml:math id="jats-math-15" display="inline" overflow="scroll"><mml:mfenced open="(" close=")" separators=","><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfenced></mml:math> are featurized using (1) a 16‐dimensional Gaussian radial basis function encoding of distance <mml:math id="jats-math-16" display="inline" overflow="scroll"><mml:mi>r</mml:mi><mml:mfenced open="(" close=")"><mml:mrow><mml:mo stretchy="true">‖</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>–</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true">‖</mml:mo></mml:mrow></mml:mfenced></mml:math> and (2) a unit vector <mml:math id="jats-math-17" display="inline" overflow="scroll"><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>–</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo></mml:math> encoding orientation.</p></list-item><list-item id="pro4541-li-0004"><p>Step 4. Compute embeddings of each site. We embed each pair of structural graphs <mml:math id="jats-math-18" display="inline" overflow="scroll"><mml:mfenced open="(" close=")" separators=","><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mfenced></mml:math> using a pair of graph neural networks, each composed of three layers of Geometric Vector Perceptrons (GVPs), (Jing et al., <xref rid="pro4541-bib-0031" ref-type="bibr">2020</xref>; Jing et al., <xref rid="pro4541-bib-0030" ref-type="bibr">2021</xref>) which learn rotationally‐equivariant representations of each atom and have proved to be state‐of‐the‐art in a variety of tasks involving protein structure (Hsu et al., <xref rid="pro4541-bib-0028" ref-type="bibr">2022</xref>; Jing et al., <xref rid="pro4541-bib-0030" ref-type="bibr">2021</xref>). We adopt all network hyperparameters (e.g., number of hidden dimensions) from (Jing et al., <xref rid="pro4541-bib-0030" ref-type="bibr">2021</xref>). Formally, each GVP learns a transformation of the input graph into 512‐dimensional embeddings of each node:</p></list-item></list>
<disp-formula id="pro4541-disp-0002"><mml:math id="jats-math-19" display="block" overflow="scroll"><mml:mi>f</mml:mi><mml:mfenced open="(" close=")" separators=";"><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>θ</mml:mi></mml:mfenced><mml:mo>→</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="script">V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∣</mml:mo><mml:mo>×</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula>
<disp-formula id="pro4541-disp-0003"><mml:math id="jats-math-20" display="block" overflow="scroll"><mml:mi>f</mml:mi><mml:mfenced open="(" close=")" separators=";"><mml:msub><mml:mi mathvariant="script">G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>ϕ</mml:mi></mml:mfenced><mml:mo>→</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="script">V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>∣</mml:mo><mml:mo>×</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:msup></mml:math></disp-formula>The final embedding of the entire graph is then computed by global mean pooling over the embeddings of each atom. While in principle, the two networks could be direct copies of each other (i.e., have tied parameters <mml:math id="jats-math-21" display="inline" overflow="scroll"><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mi>ϕ</mml:mi></mml:math>), we adopt the approach proposed by Grill et al. (<xref rid="pro4541-bib-0023" ref-type="bibr">2020</xref>) which refers to the two networks as the <italic toggle="yes">online encoder</italic> and the <italic toggle="yes">target encoder</italic>, respectively. Only the online network parameters <mml:math id="jats-math-22" display="inline" overflow="scroll"><mml:mi>θ</mml:mi></mml:math> are updated by gradient descent, while the target network parameters <mml:math id="jats-math-23" display="inline" overflow="scroll"><mml:mi>ϕ</mml:mi></mml:math> are updated as an exponential moving average of <mml:math id="jats-math-24" display="inline" overflow="scroll"><mml:mi>θ</mml:mi></mml:math>:<disp-formula id="pro4541-disp-0004"><mml:math id="jats-math-25" display="block" overflow="scroll"><mml:mi>ϕ</mml:mi><mml:mo>←</mml:mo><mml:mi mathvariant="italic">μϕ</mml:mi><mml:mo linebreak="goodbreak">+</mml:mo><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo>–</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:mfenced><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula>where <mml:math id="jats-math-26" display="inline" overflow="scroll"><mml:mi>μ</mml:mi></mml:math> is a momentum parameter which we set equal to 0.99. No gradients are propagated back through the target network, so only <mml:math id="jats-math-27" display="inline" overflow="scroll"><mml:mi>θ</mml:mi></mml:math> is updated based on the data during training. Intuitively, the target network produces a regression target based on a “decayed” representation, while the online network is trained to continually improve this representation. Only the online network is used to generate embeddings for all downstream applications.<list list-type="bullet" id="pro4541-list-0002"><list-item id="pro4541-li-0005"><p>Step 5. Compute loss and update parameters. The loss function is defined directly in the embedding space using the cosine similarity between the target network embedding <mml:math id="jats-math-28" display="inline" overflow="scroll"><mml:msub><mml:mi>z</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>512</mml:mn></mml:msup></mml:math> and the online network embedding <mml:math id="jats-math-29" display="inline" overflow="scroll"><mml:msub><mml:mi>z</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>512</mml:mn></mml:msup></mml:math> projected through a simple predictor network <mml:math id="jats-math-30" display="inline" overflow="scroll"><mml:mtext mathvariant="italic">pred</mml:mtext><mml:mfenced open="(" close=")"><mml:msub><mml:mi>z</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mfenced><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>512</mml:mn></mml:msup></mml:math>. This predictor network learns to optimally match the outputs of the online and target networks and is crucial to avoiding collapsed representations. To increase the signal‐to‐noise ratio and encourage the model to learn functionally relevant information, we weight the loss at each iteration by the sequence conservation <mml:math id="jats-math-31" display="inline" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mtext mathvariant="italic">cons</mml:mtext></mml:msub></mml:math> of that column in the original MSA (defined by the inverse of the Shannon's entropy of amino acids at that position, ignoring gaps). To reduce bias, we include all proteins in the alignment curated by CDD for computing conservation, even those without corresponding structures. As a result of this, the loss function is expressed as:</p></list-item></list>
<disp-formula id="pro4541-disp-0005"><mml:math id="jats-math-32" display="block" overflow="scroll"><mml:mi mathvariant="script">L</mml:mi><mml:mo linebreak="goodbreak">=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext mathvariant="italic">cons</mml:mtext></mml:msub><mml:mo>∙</mml:mo><mml:mfenced open="[" close="]"><mml:mrow><mml:mn>2</mml:mn><mml:mo>–</mml:mo><mml:mn>2</mml:mn><mml:mo>∙</mml:mo><mml:mfrac><mml:mrow><mml:mspace width="0.25em"/><mml:mfenced open="⟨" close="⟩" separators=","><mml:mrow><mml:mtext mathvariant="italic">pred</mml:mtext><mml:mfenced open="(" close=")"><mml:msub><mml:mi>z</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mfenced open="‖" close="‖"><mml:mrow><mml:mtext mathvariant="italic">pred</mml:mtext><mml:mfenced open="(" close=")"><mml:msub><mml:mi>z</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfenced><mml:mn>2</mml:mn></mml:msub><mml:mo>∙</mml:mo><mml:msub><mml:mfenced open="|" close="|"><mml:mfenced open="|" close="|"><mml:msub><mml:mi>z</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mspace width="0.25em"/></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula>
</p>
      <p>where<disp-formula id="pro4541-disp-0006"><mml:math id="jats-math-33" display="block" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mtext mathvariant="italic">cons</mml:mtext></mml:msub><mml:mo linebreak="goodbreak">=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>–</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="italic">AA</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi mathvariant="italic">log</mml:mi><mml:mfenced open="(" close=")"><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac></mml:math></disp-formula>Finally, we symmetrize the loss by passing each site in the input pair through both online and target networks and summing the loss from each. This symmetrized loss is then used to optimize the parameters of the online network using gradient descent.</p>
    </sec>
    <sec id="pro4541-sec-0017">
      <label>4.3</label>
      <title>Transfer learning and fine‐tuning</title>
      <p>We retrieved the pre‐processed PIP and MSP datasets from (Townshend et al., <xref rid="pro4541-bib-0061" ref-type="bibr">2021</xref>) as well as the performance metrics for baseline models. Both datasets consist of paired residue environments, so we embed the environments surrounding each residue in the pair, concatenate the embeddings, and train a two‐layer feed‐forward neural network to predict the binary outcome (see Supplementary Note 3 for details). For the comparison with MaSIF, we obtained the MaSIF‐site training and testing datasets from (Gainza et al., <xref rid="pro4541-bib-0020" ref-type="bibr">2020</xref>). Since MaSIF operates on a dense mesh of points along the protein surface, we computed embeddings for environments centered around each surface coordinate. We then added a two‐layer feed‐forward network to predict whether or not the residue is part of a protein–protein interaction site (Supplementary Note 4). For all tasks, we trained models with and without allowing the parameters of the embedding model to change (fine‐tuned and fixed, respectively). Hyperparameters were selected by monitoring the relevant metric on the validation set.</p>
    </sec>
    <sec id="pro4541-sec-0018">
      <label>4.4</label>
      <title>Training site‐specific models on <sc>prosite</sc> data</title>
      <p>We choose 10 sites presented in (Torng &amp; Altman, <xref rid="pro4541-bib-0059" ref-type="bibr">2019a</xref>) selected because they are the most challenging to predict using FEATURE‐based approaches (Buturovic et al., <xref rid="pro4541-bib-0011" ref-type="bibr">2014</xref>; Torng &amp; Altman, <xref rid="pro4541-bib-0059" ref-type="bibr">2019a</xref>). For each functional site, we train a binary classifier on fixed COLLAPSE embeddings in five‐fold nested cross‐validation (CV). The classifiers are support vector machines (SVMs) with radial basis function kernels and weighted by class frequency. Within each training fold, the inner CV is used to select the regularization hyperparameter <mml:math id="jats-math-34" display="inline" overflow="scroll"><mml:mi>C</mml:mi><mml:mo>∈</mml:mo><mml:mfenced open="{" close="}"><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn><mml:mo>,</mml:mo><mml:mn>5000</mml:mn></mml:mrow></mml:mfenced></mml:math> and the outer CV is used for model evaluation. To enable more accurate comparisons, we use the same dataset, evaluation procedures as (Torng &amp; Altman, <xref rid="pro4541-bib-0059" ref-type="bibr">2019a</xref>). We benchmark against reported results for SVMs trained on FEATURE vectors (a direct comparison to our procedure) and 3D convolutional neural networks (3DCNNs) trained end‐to‐end on the functional site structures (the current state of the art for this task). We use <sc>prosite</sc> FN/FP sites as an independent validation of our trained models, using an ensemble of the models trained on each CV fold and the classification threshold determined above. A site is considered positive if the probability estimate from any of the five‐fold models is greater than the threshold. Some proteins contain more than one site; in these cases, the protein is considered to be positive if any sites are predicted to be positive.</p>
    </sec>
    <sec id="pro4541-sec-0019">
      <label>4.5</label>
      <title>Iterated functional site search</title>
      <p>First, we embed the database to be searched against using the pre‐trained COLLAPSE model. For the results presented in Section <xref rid="pro4541-sec-0006" ref-type="sec">2.4</xref>, we use the same <sc>prosite</sc> dataset used to train our cross‐validated models to enable accurate comparisons. However, we also provide an embedding dataset for the entire PDB and scripts for generating databases for any set of protein structures. Then, we index the embedding database using FAISS, (Johnson et al., <xref rid="pro4541-bib-0032" ref-type="bibr">2021</xref>) which enables efficient similarity searches for high‐dimensional data. For each site, we then perform the following procedure five times with different random seeds in order to assess the variability of results under different query sites. The input parameters are the number of iterations <mml:math id="jats-math-35" display="inline" overflow="scroll"><mml:msub><mml:mi>n</mml:mi><mml:mtext mathvariant="italic">iter</mml:mtext></mml:msub></mml:math> and the p‐value cutoff for selecting sites at each iteration <mml:math id="jats-math-36" display="inline" overflow="scroll"><mml:msub><mml:mi>p</mml:mi><mml:mtext mathvariant="italic">cutoff</mml:mtext></mml:msub></mml:math>.<list list-type="order" id="pro4541-list-0003"><list-item id="pro4541-li-0006"><p>Sample a single site from the <sc>prosite</sc> TP dataset (to simulate querying a known functional site), generate COLLAPSE embedding, and add to query set.</p></list-item><list-item id="pro4541-li-0007"><p>Compute effective cosine similarity cutoff <mml:math id="jats-math-37" display="inline" overflow="scroll"><mml:msub><mml:mi>s</mml:mi><mml:mtext mathvariant="italic">cutoff</mml:mtext></mml:msub></mml:math> using the <mml:math id="jats-math-38" display="inline" overflow="scroll"><mml:mfenced open="(" close=")"><mml:mrow><mml:mn>1</mml:mn><mml:mo>–</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mtext mathvariant="italic">cutoff</mml:mtext></mml:msub></mml:mrow></mml:mfenced></mml:math> quantile of the empirical background for the functional amino acid of the query site (e.g., cysteine for an EGF_1 site).</p></list-item><list-item id="pro4541-li-0008"><p>Compare embedding(s) of query to database and retrieve all neighbors within <mml:math id="jats-math-39" display="inline" overflow="scroll"><mml:msub><mml:mi>s</mml:mi><mml:mtext mathvariant="italic">cutoff</mml:mtext></mml:msub></mml:math> of the query.</p></list-item><list-item id="pro4541-li-0009"><p>Add all neighbors to query set and repeat Step 3 <mml:math id="jats-math-40" display="inline" overflow="scroll"><mml:msub><mml:mi>n</mml:mi><mml:mtext mathvariant="italic">iter</mml:mtext></mml:msub></mml:math> times. Note that when there is more than one query point, neighbors to <italic toggle="yes">any</italic> point in the query are returned.</p></list-item><list-item id="pro4541-li-0010"><p>Compute precision and recall of final query set, using <sc>prosite</sc> data as ground truth.</p></list-item></list>
</p>
    </sec>
    <sec id="pro4541-sec-0020">
      <label>4.6</label>
      <title>Protein site annotation</title>
      <p>Instead of a database of all protein sites, the annotation method requires a database of known functional sites. We use all true positive sites defined in <sc>prosite</sc>. For each pattern, we identify all matching PDBs using the ScanProsite tool (Sigrist et al., <xref rid="pro4541-bib-0053" ref-type="bibr">2013</xref>) and extract the residues corresponding to all fully conserved positions in the pattern (i.e., where only one residue is allowed). The environment around each residue is embedded using COLLAPSE. We also embed all residues in the CSA, a curated dataset of catalytic residues responsible for an enzyme's function. All data processing matches the pre‐training procedure. The final dataset consists of 25,407 embeddings representing 1870 unique functional sites.</p>
      <p>The annotation method operates in a similar fashion to the search method, where each residue in the input protein is embedded and compared to the functional site database. Any residue that has a hit with a p‐value below the pre‐specified cutoff is returned as a potential functional site. To filter out false positives due to common or nonspecific features (e.g., small polar residues in alpha‐helices), we also remove hits which are not significant against the empirical distribution specific to that functional site (Section <xref rid="pro4541-sec-0015" ref-type="sec">4.1.2</xref>). This results in a modified mutual best hit criterion with two user‐specified parameters: the residue‐level and site‐level significance thresholds. Along with each hit is the metadata associated with the corresponding database entry (PDB ID, functional site description, etc.) so each result can be examined in more detail. For the examples presented we remove all ligand atoms from the input structure to reduce the influence of nonprotein atoms on the embeddings.</p>
    </sec>
  </sec>
  <sec id="pro4541-sec-0024">
    <title>AUTHOR CONTRIBUTIONS</title>
    <p><bold>Alexander Derry:</bold> Conceptualization (lead); data curation (lead); formal analysis (lead); funding acquisition (equal); investigation (lead); methodology (lead); resources (equal); software (lead); validation (lead); visualization (lead); writing – original draft (lead); writing – review and editing (equal). <bold>Russ B. Altman:</bold> Conceptualization (supporting); funding acquisition (equal); investigation (supporting); methodology (supporting); project administration (lead); resources (equal); supervision (lead); writing – review and editing (equal).</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supporting information</title>
    <supplementary-material id="pro4541-supitem-0001" position="float" content-type="local-data">
      <caption>
        <p><bold>Data S1:</bold> Supporting Information</p>
      </caption>
      <media xlink:href="PRO-32-e4541-s001.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="pro4541-sec-0021">
    <title>ACKNOWLEDGMENTS</title>
    <p>We thank Kristy Carpenter, Delaney Smith, Adam Lavertu, and Wen Torng for useful discussions. Computing for this project was performed on the Sherlock cluster; we would like to thank Stanford University and the Stanford Research Computing Center for providing computational resources and support. A.D. is supported by LM012409 and R.B.A. is supported by NIH GM102365 and Chan Zuckerberg Biohub.</p>
  </ack>
  <sec sec-type="data-availability" id="pro4541-sec-0023">
    <title>DATA AVAILABILITY STATEMENT</title>
    <p>The data that support the findings of this study are openly available in Zenodo at <ext-link xlink:href="http://doi.org/10.5281/zenodo.6903423" ext-link-type="uri" specific-use="dataset is-supplemented-by">http://doi.org/10.5281/zenodo.6903423</ext-link>, reference number 6903423.</p>
  </sec>
  <ref-list id="pro4541-bibl-0001" content-type="cited-references">
    <title>REFERENCES</title>
    <ref id="pro4541-bib-0001">
      <mixed-citation publication-type="journal" id="pro4541-cit-0001"><string-name><surname>Akdel</surname><given-names>M, Pires DE, Pardo EP,</given-names></string-name> et al. <article-title>A structural biology community assessment of AlphaFold 2 applications</article-title>. <source>bioRxiv</source>. <year>2021</year>:<elocation-id>461876</elocation-id>. <pub-id pub-id-type="doi">10.1101/2021.09.26.461876</pub-id>
</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0002">
      <mixed-citation publication-type="journal" id="pro4541-cit-0002"><string-name><surname>Akiva</surname><given-names>E</given-names></string-name>, <string-name><surname>Brown</surname><given-names>S</given-names></string-name>, <string-name><surname>Almonacid</surname><given-names>DE</given-names></string-name>, <string-name><surname>Barber</surname><given-names>AE</given-names><suffix>2nd</suffix></string-name>, <string-name><surname>Custer</surname><given-names>AF</given-names></string-name>, <string-name><surname>Hicks</surname><given-names>MA</given-names></string-name>, et al. <article-title>The structure‐function linkage database</article-title>. <source>Nucleic Acids Res</source>. <year>2014</year>;<volume>42</volume>:<fpage>D521</fpage>–<lpage>30</lpage>.<pub-id pub-id-type="pmid">24271399</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0003">
      <mixed-citation publication-type="journal" id="pro4541-cit-0003"><string-name><surname>Alley</surname><given-names>EC</given-names></string-name>, <string-name><surname>Khimulya</surname><given-names>G</given-names></string-name>, <string-name><surname>Biswas</surname><given-names>S</given-names></string-name>, <string-name><surname>AlQuraishi</surname><given-names>M</given-names></string-name>, <string-name><surname>Church</surname><given-names>GM</given-names></string-name>. <article-title>Unified rational protein engineering with sequence‐based deep representation learning</article-title>. <source>Nat Methods</source>. <year>2019</year>;<volume>16</volume>:<fpage>1315</fpage>–<lpage>22</lpage>.<pub-id pub-id-type="pmid">31636460</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0005">
      <mixed-citation publication-type="journal" id="pro4541-cit-0005"><string-name><surname>Ashburner</surname><given-names>M</given-names></string-name>, <string-name><surname>Ball</surname><given-names>CA</given-names></string-name>, <string-name><surname>Blake</surname><given-names>JA</given-names></string-name>, <string-name><surname>Botstein</surname><given-names>D</given-names></string-name>, <string-name><surname>Butler</surname><given-names>H</given-names></string-name>, <string-name><surname>Cherry</surname><given-names>JM</given-names></string-name>, et al. <article-title>Gene ontology: tool for the unification of biology</article-title>. <source>Nat Genet</source>. <year>2000</year>;<volume>25</volume>:<fpage>25</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">10802651</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0006">
      <mixed-citation publication-type="journal" id="pro4541-cit-0006"><string-name><surname>Attwood</surname><given-names>TK</given-names></string-name>. <article-title>The PRINTS database: a resource for identification of protein families</article-title>. <source>Brief Bioinform</source>. <year>2002</year>;<volume>3</volume>:<fpage>252</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">12230034</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0007">
      <mixed-citation publication-type="journal" id="pro4541-cit-0007"><string-name><surname>Bagley</surname><given-names>SC</given-names></string-name>, <string-name><surname>Altman</surname><given-names>RB</given-names></string-name>. <article-title>Characterizing the microenvironment surrounding protein sites</article-title>. <source>Protein Sci</source>. <year>2008</year>;<volume>4</volume>:<fpage>622</fpage>–<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0008">
      <mixed-citation publication-type="journal" id="pro4541-cit-0008"><string-name><surname>Berman</surname><given-names>HM</given-names></string-name>, <string-name><surname>Battistuz</surname><given-names>T</given-names></string-name>, <string-name><surname>Bhat</surname><given-names>TN</given-names></string-name>, <string-name><surname>Bluhm</surname><given-names>WF</given-names></string-name>, <string-name><surname>Bourne</surname><given-names>PE</given-names></string-name>, <string-name><surname>Burkhardt</surname><given-names>K</given-names></string-name>, et al. <article-title>The Protein Data Bank</article-title>. <source>Acta Crystallogr D Biol Crystallogr</source>. <year>2002</year>;<volume>58</volume>:<fpage>899</fpage>–<lpage>907</lpage>.<pub-id pub-id-type="pmid">12037327</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0009">
      <mixed-citation publication-type="journal" id="pro4541-cit-0009"><string-name><surname>Bernhofer</surname><given-names>M</given-names></string-name>, <string-name><surname>Dallago</surname><given-names>C</given-names></string-name>, <string-name><surname>Karl</surname><given-names>T</given-names></string-name>, <string-name><surname>Satagopam</surname><given-names>V</given-names></string-name>, <string-name><surname>Heinzinger</surname><given-names>M</given-names></string-name>, <string-name><surname>Littmann</surname><given-names>M</given-names></string-name>, et al. <article-title>PredictProtein ‐ predicting protein structure and function for 29 years</article-title>. <source>Nucleic Acids Res</source>. <year>2021</year>;<volume>49</volume>:<fpage>W535</fpage>–<lpage>40</lpage>.<pub-id pub-id-type="pmid">33999203</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0010">
      <mixed-citation publication-type="journal" id="pro4541-cit-0010"><string-name><surname>Bordin</surname><given-names>N, Sillitoe I, Nallapareddy V,</given-names></string-name> et al. <article-title>AlphaFold2 reveals commonalities and novelties in protein structure space for 21 model organisms</article-title>. <source>bioRxiv</source>. <year>2022</year>:<elocation-id>494367</elocation-id>. <pub-id pub-id-type="doi">10.1101/2022.06.02.494367</pub-id>
</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0011">
      <mixed-citation publication-type="journal" id="pro4541-cit-0011"><string-name><surname>Buturovic</surname><given-names>L</given-names></string-name>, <string-name><surname>Wong</surname><given-names>M</given-names></string-name>, <string-name><surname>Tang</surname><given-names>GW</given-names></string-name>, <string-name><surname>Altman</surname><given-names>RB</given-names></string-name>, <string-name><surname>Petkovic</surname><given-names>D</given-names></string-name>. <article-title>High precision prediction of functional sites in protein structures</article-title>. <source>PLoS One</source>. <year>2014</year>;<volume>9</volume>:<elocation-id>e91240</elocation-id>.<pub-id pub-id-type="pmid">24632601</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0012">
      <mixed-citation publication-type="journal" id="pro4541-cit-0012"><string-name><surname>Che</surname><given-names>F, Yang G, Zhang D, Tao J, Shao P, Liu T</given-names></string-name>. <article-title>Self‐supervised graph representation learning via bootstrapping</article-title>. <source>arXiv</source>. <year>2020</year>;2011:05126. <pub-id pub-id-type="doi">10.48550/arXiv.2011.05126</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0013">
      <mixed-citation publication-type="journal" id="pro4541-cit-0013"><string-name><surname>Chen</surname><given-names>X</given-names></string-name>, <string-name><surname>He</surname><given-names>K</given-names></string-name>. <article-title>Exploring simple Siamese representation learning</article-title>. <source>arXiv</source>. <year>2020</year>. 2011.10566. <pub-id pub-id-type="doi">10.48550/arXiv.2011.10566</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0014">
      <mixed-citation publication-type="journal" id="pro4541-cit-0014"><string-name><surname>Derry</surname><given-names>A</given-names></string-name>, <string-name><surname>Carpenter</surname><given-names>KA</given-names></string-name>, <string-name><surname>Altman</surname><given-names>RB</given-names></string-name>. <article-title>Training data composition affects performance of protein structure analysis algorithms</article-title>. <source>Pac Symp Biocomput</source>. <year>2022</year>;<volume>27</volume>:<fpage>10</fpage>–<lpage>21</lpage>.<pub-id pub-id-type="pmid">34890132</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0015">
      <mixed-citation publication-type="journal" id="pro4541-cit-0015"><string-name><surname>Detlefsen</surname><given-names>NS</given-names></string-name>, <string-name><surname>Hauberg</surname><given-names>S</given-names></string-name>, <string-name><surname>Boomsma</surname><given-names>W</given-names></string-name>. <article-title>Learning meaningful representations of protein sequences</article-title>. <source>Nat Commun</source>. <year>2022</year>;<volume>13</volume>:<fpage>1914</fpage>.<pub-id pub-id-type="pmid">35395843</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0016">
      <mixed-citation publication-type="journal" id="pro4541-cit-0016"><string-name><surname>Duvenaud</surname><given-names>D, Maclaurin D, Aguilera‐Iparraguirre J,</given-names></string-name> et al. <article-title>Convolutional networks on graphs for learning molecular fingerprints</article-title>. <source>Advances in Neural Information Processing Systems</source>. <year>2015</year>;28.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0017">
      <mixed-citation publication-type="journal" id="pro4541-cit-0017"><string-name><surname>El‐Gebali</surname><given-names>S, Mistry J, Bateman A, et al</given-names></string-name>. <article-title>The Pfam protein families database in 2019</article-title>. <source>Nucleic Acids Res</source>. <year>2019</year>;<volume>47</volume>:<fpage>D427</fpage>–<lpage>32</lpage>.<pub-id pub-id-type="pmid">30357350</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0018">
      <mixed-citation publication-type="journal" id="pro4541-cit-0018"><string-name><surname>Fetrow</surname><given-names>JS</given-names></string-name>, <string-name><surname>Skolnick</surname><given-names>J</given-names></string-name>. <article-title>Method for prediction of protein function from sequence using the sequence‐to‐structure‐to‐function paradigm with application to glutaredoxins/thioredoxins and T1 ribonucleases</article-title>. <source>J Mol Biol</source>. <year>1998</year>;<volume>281</volume>:<fpage>949</fpage>–<lpage>68</lpage>.<pub-id pub-id-type="pmid">9719646</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0019">
      <mixed-citation publication-type="journal" id="pro4541-cit-0019"><string-name><surname>Furnham</surname><given-names>N, Holliday GL, de Beer TA, Jacobsen JO, Pearson WR, Thornton JM</given-names></string-name>. <article-title>The catalytic site atlas 2.0: cataloging catalytic sites and residues identified in enzymes</article-title>. <source>Nucleic Acids Res</source>. <year>2014</year>;<volume>42</volume>:<fpage>D485</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">24319146</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0020">
      <mixed-citation publication-type="journal" id="pro4541-cit-0020"><string-name><surname>Gainza</surname><given-names>P</given-names></string-name>, <string-name><surname>Sverrisson</surname><given-names>F</given-names></string-name>, <string-name><surname>Monti</surname><given-names>F</given-names></string-name>, <string-name><surname>Rodolà</surname><given-names>E</given-names></string-name>, <string-name><surname>Boscaini</surname><given-names>D</given-names></string-name>, <string-name><surname>Bronstein</surname><given-names>MM</given-names></string-name>, et al. <article-title>Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</article-title>. <source>Nat Methods</source>. <year>2020</year>;<volume>17</volume>:<fpage>184</fpage>–<lpage>92</lpage>.<pub-id pub-id-type="pmid">31819266</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0023">
      <mixed-citation publication-type="journal" id="pro4541-cit-0023"><string-name><surname>Grill</surname><given-names>J‐B, Strub F, Altché F,</given-names></string-name> et al. <article-title>Bootstrap your own latent ‐ a new approach to self‐supervised learning</article-title>. <source>Adv Neural Inf Process Syst</source>. <year>2020</year>;<volume>33</volume>:<fpage>21271</fpage>–<lpage>84</lpage>.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0021">
      <mixed-citation publication-type="journal" id="pro4541-cit-0021"><string-name><surname>Gilmer</surname><given-names>J</given-names></string-name>, <string-name><surname>Schoenholz</surname><given-names>SS</given-names></string-name>, <string-name><surname>Riley</surname><given-names>PF</given-names></string-name>, <string-name><surname>Vinyals</surname><given-names>O</given-names></string-name>, <string-name><surname>Dahl</surname><given-names>GE</given-names></string-name>. <article-title>Neural message passing for quantum chemistry</article-title>. <source>Proceedings of the 34th International Conference on Machine Learning</source>. <year>2017</year>;<volume>70</volume>:1263–72.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0022">
      <mixed-citation publication-type="journal" id="pro4541-cit-0022"><string-name><surname>Gligorijević</surname><given-names>V</given-names></string-name>, <string-name><surname>Renfrew</surname><given-names>PD</given-names></string-name>, <string-name><surname>Kosciolek</surname><given-names>T</given-names></string-name>, <string-name><surname>Leman</surname><given-names>JK</given-names></string-name>, <string-name><surname>Berenberg</surname><given-names>D</given-names></string-name>, <string-name><surname>Vatanen</surname><given-names>T</given-names></string-name>, et al. <article-title>Structure‐based protein function prediction using graph convolutional networks</article-title>. <source>Nat Commun</source>. <year>2021</year>;<volume>12</volume>:<fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">33397941</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0024">
      <mixed-citation publication-type="journal" id="pro4541-cit-0024"><string-name><surname>Haft</surname><given-names>DH</given-names></string-name>, <string-name><surname>Selengut</surname><given-names>JD</given-names></string-name>, <string-name><surname>Richter</surname><given-names>RA</given-names></string-name>, <string-name><surname>Harkins</surname><given-names>D</given-names></string-name>, <string-name><surname>Basu</surname><given-names>MK</given-names></string-name>, <string-name><surname>Beck</surname><given-names>E</given-names></string-name>. <article-title>TIGRFAMs and genome properties in 2013</article-title>. <source>Nucleic Acids Res</source>. <year>2013</year>;<volume>41</volume>:<fpage>D387</fpage>–<lpage>95</lpage>.<pub-id pub-id-type="pmid">23197656</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0025">
      <mixed-citation publication-type="journal" id="pro4541-cit-0025"><string-name><surname>Hermosilla</surname><given-names>P, Schäfer M, Lang M,</given-names></string-name> et al. <article-title>Intrinsic‐extrinsic convolution and pooling for learning on 3D protein structures</article-title>. <source>arXiv</source>. <year>2020</year>;2007:06252. <pub-id pub-id-type="doi">10.48550/arXiv.2007.06252</pub-id>
</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0026">
      <mixed-citation publication-type="miscellaneous" id="pro4541-cit-0026"><string-name><surname>Hermosilla</surname><given-names>P</given-names></string-name>, <string-name><surname>Ropinski</surname><given-names>T. Contrastive representation learning for 3D protein structures.</given-names></string-name><italic toggle="yes">arXiv</italic>. <year>2021</year>;2205:15675. <pub-id pub-id-type="doi">10.48550/arXiv.2205.15675</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0027">
      <mixed-citation publication-type="journal" id="pro4541-cit-0027"><string-name><surname>Holm</surname><given-names>L</given-names></string-name>, <string-name><surname>Rosenström</surname><given-names>P</given-names></string-name>. <article-title>Dali server: conservation mapping in 3D</article-title>. <source>Nucleic Acids Res</source>. <year>2010</year>;<volume>38</volume>:<fpage>W545</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">20457744</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0028">
      <mixed-citation publication-type="journal" id="pro4541-cit-0028"><string-name><surname>Hsu</surname><given-names>C, Verkuil R, Liu J,</given-names></string-name> et al. <article-title>Learning inverse folding from millions of predicted structures</article-title>. <source>bioRxiv</source>. <year>2022</year>:<elocation-id>487779</elocation-id>. <pub-id pub-id-type="doi">10.1101/2022.04.10.487779</pub-id>
</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0029">
      <mixed-citation publication-type="journal" id="pro4541-cit-0029"><string-name><surname>Hu</surname><given-names>W, Liu B, Gomes J, et al</given-names></string-name>. <article-title>Strategies for pre‐training graph neural networks</article-title>. <source>International conference on learning representations</source>; <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0030">
      <mixed-citation publication-type="journal" id="pro4541-cit-0030"><string-name><surname>Jing</surname><given-names>B</given-names></string-name>, <string-name><surname>Eismann</surname><given-names>S</given-names></string-name>, <string-name><surname>Soni</surname><given-names>PN</given-names></string-name>, <string-name><surname>Dror</surname><given-names>RO</given-names></string-name>. <article-title>Equivariant graph neural networks for 3D macromolecular structure</article-title>. <source>arXiv</source>. <year>2021</year>;2106:03843. <pub-id pub-id-type="doi">10.48550/arXiv.2106.03843</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0031">
      <mixed-citation publication-type="journal" id="pro4541-cit-0031"><string-name><surname>Jing</surname><given-names>B</given-names></string-name>, <string-name><surname>Eismann</surname><given-names>S</given-names></string-name>, <string-name><surname>Suriana</surname><given-names>P</given-names></string-name>, <string-name><surname>Townshend</surname><given-names>RJL</given-names></string-name>, <string-name><surname>Dror</surname><given-names>R</given-names></string-name>. <article-title>Learning from protein structure with geometric vector Perceptrons</article-title>. <source>arXiv</source>. <year>2020</year>;<volume>2009</volume>:<fpage>01411</fpage>. <pub-id pub-id-type="doi">10.48550/arXiv.2009.01411</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0032">
      <mixed-citation publication-type="journal" id="pro4541-cit-0032"><string-name><surname>Johnson</surname><given-names>J</given-names></string-name>, <string-name><surname>Douze</surname><given-names>M</given-names></string-name>, <string-name><surname>Jegou</surname><given-names>H</given-names></string-name>. <article-title>Billion‐scale similarity search with GPUs</article-title>. <source>IEEE Trans Big Data</source>. <year>2021</year>;<volume>7</volume>:<fpage>535</fpage>–<lpage>47</lpage>.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0033">
      <mixed-citation publication-type="journal" id="pro4541-cit-0033"><string-name><surname>Jumper</surname><given-names>J</given-names></string-name>, <string-name><surname>Evans</surname><given-names>R</given-names></string-name>, <string-name><surname>Pritzel</surname><given-names>A</given-names></string-name>, <string-name><surname>Green</surname><given-names>T</given-names></string-name>, <string-name><surname>Figurnov</surname><given-names>M</given-names></string-name>, <string-name><surname>Ronneberger</surname><given-names>O</given-names></string-name>, et al. <article-title>Highly accurate protein structure prediction with AlphaFold</article-title>. <source>Nature</source>. <year>2021</year>;<volume>596</volume>:<fpage>583</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">34265844</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0035">
      <mixed-citation publication-type="miscellaneous" id="pro4541-cit-0035"><string-name><surname>Kingma</surname><given-names>DP</given-names></string-name>, <string-name><surname>Ba</surname><given-names>JL.</given-names></string-name><article-title>Adam: a method for stochastic optimization. in 3rd international conference on learning representations, ICLR 2015 ‐ conference track proceedings</article-title> (<year>2015</year>).</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0036">
      <mixed-citation publication-type="journal" id="pro4541-cit-0036"><string-name><surname>Kulmanov</surname><given-names>M</given-names></string-name>, <string-name><surname>Hoehndorf</surname><given-names>R</given-names></string-name>. <article-title>DeepGOPlus: improved protein function prediction from sequence</article-title>. <source>Bioinformatics</source>. <year>2020</year>;<volume>36</volume>:<fpage>422</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">31350877</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0037">
      <mixed-citation publication-type="journal" id="pro4541-cit-0037"><string-name><surname>LeCun</surname><given-names>Y</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>G</given-names></string-name>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>:<fpage>436</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0038">
      <mixed-citation publication-type="journal" id="pro4541-cit-0038"><string-name><surname>Liu</surname><given-names>T</given-names></string-name>, <string-name><surname>Altman</surname><given-names>RB</given-names></string-name>. <article-title>Using multiple microenvironments to find similar ligand‐binding sites: application to kinase inhibitor binding</article-title>. <source>PLoS Comput Biol</source>. <year>2011</year>;<volume>7</volume>:<elocation-id>e1002326</elocation-id>.<pub-id pub-id-type="pmid">22219723</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0508">
      <mixed-citation publication-type="journal" id="pro4541-cit-0508"><string-name><surname>Lu</surname><given-names>S</given-names></string-name>, <string-name><surname>Wang</surname><given-names>J</given-names></string-name>, <string-name><surname>Chitsaz</surname><given-names>F</given-names></string-name>, <string-name><surname>Derbyshire</surname><given-names>MK</given-names></string-name>, <string-name><surname>Geer</surname><given-names>RC</given-names></string-name>, <string-name><surname>Gonzales</surname><given-names>NR</given-names></string-name>, et al. <article-title>CDD/SPARCLE: the conserved domain database in 2020</article-title>. <source>Nucleic Acids Res</source>. <year>2020</year>;<volume>48</volume>:<fpage>D265</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">31777944</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0040">
      <mixed-citation publication-type="journal" id="pro4541-cit-0040"><string-name><surname>Marchler‐Bauer</surname><given-names>A</given-names></string-name>, <string-name><surname>Anderson</surname><given-names>JB</given-names></string-name>, <string-name><surname>DeWeese‐Scott</surname><given-names>C</given-names></string-name>, <string-name><surname>Fedorova</surname><given-names>ND</given-names></string-name>, <string-name><surname>Geer</surname><given-names>LY</given-names></string-name>, <string-name><surname>He</surname><given-names>S</given-names></string-name>, et al. <article-title>CDD: a curated Entrez database of conserved domain alignments</article-title>. <source>Nucleic Acids Res</source>. <year>2003</year>;<volume>31</volume>:<fpage>383</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">12520028</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0043">
      <mixed-citation publication-type="journal" id="pro4541-cit-0043"><string-name><surname>Moraes</surname><given-names>JPA</given-names></string-name>, <string-name><surname>Pappa</surname><given-names>GL</given-names></string-name>, <string-name><surname>Pires</surname><given-names>DEV</given-names></string-name>, <string-name><surname>Izidoro</surname><given-names>SC</given-names></string-name>. <article-title>GASS‐WEB: a web server for identifying enzyme active sites based on genetic algorithms</article-title>. <source>Nucleic Acids Res</source>. <year>2017</year>;<volume>45</volume>:<fpage>W315</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">28459991</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0041">
      <mixed-citation publication-type="journal" id="pro4541-cit-0041"><string-name><surname>Mi</surname><given-names>H</given-names></string-name>, <string-name><surname>Lazareva‐Ulitsky</surname><given-names>B</given-names></string-name>, <string-name><surname>Loo</surname><given-names>R</given-names></string-name>, <string-name><surname>Kejariwal</surname><given-names>A</given-names></string-name>, <string-name><surname>Vandergriff</surname><given-names>J</given-names></string-name>, <string-name><surname>Rabkin</surname><given-names>S</given-names></string-name>, et al. <article-title>The PANTHER database of protein families, subfamilies, functions and pathways</article-title>. <source>Nucleic Acids Res</source>. <year>2005</year>;<volume>33</volume>:<fpage>D284</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">15608197</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0042">
      <mixed-citation publication-type="journal" id="pro4541-cit-0042"><string-name><surname>Mitchell</surname><given-names>AL</given-names></string-name>, <string-name><surname>Attwood</surname><given-names>TK</given-names></string-name>, <string-name><surname>Babbitt</surname><given-names>PC</given-names></string-name>, <string-name><surname>Blum</surname><given-names>M</given-names></string-name>, <string-name><surname>Bork</surname><given-names>P</given-names></string-name>, <string-name><surname>Bridge</surname><given-names>A</given-names></string-name>, et al. <article-title>InterPro in 2019: improving coverage, classification and access to protein sequence annotations</article-title>. <source>Nucleic Acids Res</source>. <year>2019</year>;<volume>47</volume>:<fpage>D351</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">30398656</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0044">
      <mixed-citation publication-type="book" id="pro4541-cit-0044"><string-name><surname>Oquab</surname><given-names>M</given-names></string-name>, <string-name><surname>Bottou</surname><given-names>L</given-names></string-name>, <string-name><surname>Laptev</surname><given-names>I</given-names></string-name>, <string-name><surname>Sivic</surname><given-names>J</given-names></string-name>. <part-title>Learning and transferring mid‐level image representations using convolutional neural networks</part-title>. <source>2014 IEEE conference on computer vision and pattern recognition</source>; <year>2014</year>. New York (NY): IEEE. p. 1717–1724.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0045">
      <mixed-citation publication-type="journal" id="pro4541-cit-0045"><string-name><surname>Orengo</surname><given-names>CA</given-names></string-name>, <string-name><surname>Michie</surname><given-names>AD</given-names></string-name>, <string-name><surname>Jones</surname><given-names>S</given-names></string-name>, <string-name><surname>Jones</surname><given-names>DT</given-names></string-name>, <string-name><surname>Swindells</surname><given-names>MB</given-names></string-name>, <string-name><surname>Thornton</surname><given-names>JM</given-names></string-name>. <article-title>CATH–a hierarchic classification of protein domain structures</article-title>. <source>Structure</source>. <year>1997</year>;<volume>5</volume>:<fpage>1093</fpage>–<lpage>108</lpage>.<pub-id pub-id-type="pmid">9309224</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0046">
      <mixed-citation publication-type="journal" id="pro4541-cit-0046"><string-name><surname>Ramola</surname><given-names>R</given-names></string-name>, <string-name><surname>Friedberg</surname><given-names>I</given-names></string-name>, <string-name><surname>Radivojac</surname><given-names>P</given-names></string-name>. <article-title>The field of protein function prediction as viewed by different domain scientists</article-title>. <source>bioRxiv</source>. <year>2022</year>:<elocation-id>488641</elocation-id>. <pub-id pub-id-type="doi">10.1101/2022.04.18.488641</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0047">
      <mixed-citation publication-type="journal" id="pro4541-cit-0047"><string-name><surname>Ribeiro</surname><given-names>AJM</given-names></string-name>, <string-name><surname>Holliday</surname><given-names>GL</given-names></string-name>, <string-name><surname>Furnham</surname><given-names>N</given-names></string-name>, <string-name><surname>Tyzack</surname><given-names>JD</given-names></string-name>, <string-name><surname>Ferris</surname><given-names>K</given-names></string-name>, <string-name><surname>Thornton</surname><given-names>JM</given-names></string-name>. <article-title>Mechanism and catalytic site atlas (M‐CSA): a database of enzyme reaction mechanisms and active sites</article-title>. <source>Nucleic Acids Res</source>. <year>2018</year>;<volume>46</volume>:<fpage>D618</fpage>–<lpage>23</lpage>.<pub-id pub-id-type="pmid">29106569</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0049">
      <mixed-citation publication-type="journal" id="pro4541-cit-0049"><string-name><surname>Rives</surname><given-names>A</given-names></string-name>, <string-name><surname>Meier</surname><given-names>J</given-names></string-name>, <string-name><surname>Sercu</surname><given-names>T</given-names></string-name>, et al. <article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title>. <source>Proc Natl Acad Sci U S A</source>. <year>2021</year>;<volume>118</volume>:e2016239118.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0050">
      <mixed-citation publication-type="journal" id="pro4541-cit-0050"><string-name><surname>Rost</surname><given-names>B</given-names></string-name>. <article-title>Twilight zone of protein sequence alignments</article-title>. <source>Protein Eng</source>. <year>1999</year>;<volume>12</volume>:<fpage>85</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">10195279</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0051">
      <mixed-citation publication-type="journal" id="pro4541-cit-0051"><string-name><surname>Sanderson</surname><given-names>T</given-names></string-name>, <string-name><surname>Bileschi</surname><given-names>ML</given-names></string-name>, <string-name><surname>Belanger</surname><given-names>D</given-names></string-name>, <string-name><surname>Colwell</surname><given-names>LJ</given-names></string-name>. <article-title>ProteInfer: deep networks for protein functional inference</article-title>. <source>bioRxiv</source>. <year>2021</year>:<elocation-id>461077</elocation-id>. <pub-id pub-id-type="doi">10.1101/2021.09.20.461077</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0052">
      <mixed-citation publication-type="journal" id="pro4541-cit-0052"><string-name><surname>Schnoes</surname><given-names>AM</given-names></string-name>, <string-name><surname>Brown</surname><given-names>SD</given-names></string-name>, <string-name><surname>Dodevski</surname><given-names>I</given-names></string-name>, <string-name><surname>Babbitt</surname><given-names>PC</given-names></string-name>. <article-title>Annotation error in public databases: misannotation of molecular function in enzyme superfamilies</article-title>. <source>PLoS Comput Biol</source>. <year>2009</year>;<volume>5</volume>:<elocation-id>e1000605</elocation-id>.<pub-id pub-id-type="pmid">20011109</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0501">
      <mixed-citation publication-type="journal" id="pro4541-cit-0501"><string-name><surname>Steinegger</surname><given-names>M</given-names></string-name>, <string-name><surname>Söding</surname><given-names>J</given-names></string-name>. <article-title>MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</article-title>. <source>Nat Biotechnol</source>. <year>2017</year>;<volume>35</volume>:<fpage>1026</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">29035372</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0053">
      <mixed-citation publication-type="journal" id="pro4541-cit-0053"><string-name><surname>Sigrist</surname><given-names>CJA, de Castro E, Cerutti L,</given-names></string-name> et al. <article-title>New and continuing developments at PROSITE</article-title>. <source>Nucleic Acids Res</source>. <year>2013</year>;<volume>41</volume>:<fpage>D344</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">23161676</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0057">
      <mixed-citation publication-type="journal" id="pro4541-cit-0057"><string-name><surname>Tian</surname><given-names>W</given-names></string-name>, <string-name><surname>Skolnick</surname><given-names>J</given-names></string-name>. <article-title>How well is enzyme function conserved as a function of pairwise sequence identity?</article-title><source>J Mol Biol</source>. <year>2003</year>;<volume>333</volume>:<fpage>863</fpage>–<lpage>82</lpage>.<pub-id pub-id-type="pmid">14568541</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0056">
      <mixed-citation publication-type="journal" id="pro4541-cit-0056"><string-name><surname>Tang</surname><given-names>GW</given-names></string-name>, <string-name><surname>Altman</surname><given-names>RB</given-names></string-name>. <article-title>Knowledge‐based fragment binding prediction</article-title>. <source>PLoS Comput Biol</source>. <year>2014</year>;<volume>10</volume>:<elocation-id>e1003589</elocation-id>.<pub-id pub-id-type="pmid">24762971</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0062">
      <mixed-citation publication-type="journal" id="pro4541-cit-0062"><string-name><surname>Tubiana</surname><given-names>J</given-names></string-name>, <string-name><surname>Schneidman‐Duhovny</surname><given-names>D</given-names></string-name>, <string-name><surname>Wolfson</surname><given-names>HJ</given-names></string-name>. <article-title>ScanNet: an interpretable geometric deep learning model for structure‐based protein binding site prediction</article-title>. <source>Nat Methods</source>. <year>2022</year>;<volume>19</volume>:<fpage>730</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">35637310</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0059">
      <mixed-citation publication-type="journal" id="pro4541-cit-0059"><string-name><surname>Torng</surname><given-names>W</given-names></string-name>, <string-name><surname>Altman</surname><given-names>RB</given-names></string-name>. <article-title>Graph convolutional neural networks for predicting drug‐target interactions</article-title>. <source>J Chem Inf Model</source>. <year>2019</year>;<volume>473074</volume>:<fpage>4131</fpage>–<lpage>49</lpage>.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0060">
      <mixed-citation publication-type="journal" id="pro4541-cit-0060"><string-name><surname>Torng</surname><given-names>W</given-names></string-name>, <string-name><surname>Altman</surname><given-names>RB</given-names></string-name>. <article-title>High precision protein functional site detection using 3D convolutional neural networks</article-title>. <source>Bioinformatics</source>. <year>2019</year>;<volume>35</volume>:<fpage>1503</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">31051039</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0061">
      <mixed-citation publication-type="book" id="pro4541-cit-0061"><string-name><surname>Townshend</surname><given-names>RJL, Vögele M, Suriana PA,</given-names></string-name> et al. <part-title>ATOM3D: tasks on molecules in three dimensions</part-title>. Proceedings of the <source>Neural Information Processing Systems Track on Datasets and Benchmarks</source>; <year>2021</year>. Red Hook (NY): Curran Associates, Inc.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0064">
      <mixed-citation publication-type="journal" id="pro4541-cit-0064"><string-name><surname>van Kempen</surname><given-names>M, Kim SS, Tumescheit C, Mirdita M, Söding J, Steinegger M</given-names></string-name>. <article-title>Foldseek: fast and accurate protein structure search</article-title>. <source>bioRxiv</source>. <year>2022</year>:<elocation-id>479398</elocation-id>. <pub-id pub-id-type="doi">10.1101/2022.02.07.479398</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0063">
      <mixed-citation publication-type="journal" id="pro4541-cit-0063"><string-name><surname>Valasatava</surname><given-names>Y</given-names></string-name>, <string-name><surname>Rosato</surname><given-names>A</given-names></string-name>, <string-name><surname>Cavallaro</surname><given-names>G</given-names></string-name>, <string-name><surname>Andreini</surname><given-names>C</given-names></string-name>. <article-title>MetalS3, a database‐mining tool for the identification of structurally similar metal sites</article-title>. <source>J Biol Inorg Chem</source>. <year>2014</year>;<volume>19</volume>:<fpage>937</fpage>–<lpage>45</lpage>.<pub-id pub-id-type="pmid">24699831</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0065">
      <mixed-citation publication-type="journal" id="pro4541-cit-0065"><string-name><surname>Varadi</surname><given-names>M</given-names></string-name>, <string-name><surname>Anyango</surname><given-names>S</given-names></string-name>, <string-name><surname>Deshpande</surname><given-names>M</given-names></string-name>, <string-name><surname>Nair</surname><given-names>S</given-names></string-name>, <string-name><surname>Natassia</surname><given-names>C</given-names></string-name>, <string-name><surname>Yordanova</surname><given-names>G</given-names></string-name>, et al. <article-title>AlphaFold protein structure database: massively expanding the structural coverage of protein‐sequence space with high‐accuracy models</article-title>. <source>Nucleic Acids Res</source>. <year>2022</year>;<volume>50</volume>:<fpage>D439</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">34791371</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0066">
      <mixed-citation publication-type="journal" id="pro4541-cit-0066"><string-name><surname>Wang</surname><given-names>G</given-names></string-name>, <string-name><surname>Dunbrack</surname><given-names>RL</given-names></string-name>. <article-title>PISCES: a protein sequence culling server</article-title>. <source>Bioinformatics</source>. <year>2003</year>;<volume>19</volume>:<fpage>1589</fpage>–<lpage>91</lpage>.<pub-id pub-id-type="pmid">12912846</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0067">
      <mixed-citation publication-type="journal" id="pro4541-cit-0067"><collab collab-type="authors">Xin F, Radivojac P</collab>
. <article-title>Computational methods for identification of functional residues in protein structures</article-title>. <source>Curr Protein Pept Sci</source>. <year>2011</year>;<volume>12</volume>:456–69.</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0068">
      <mixed-citation publication-type="journal" id="pro4541-cit-0068"><string-name><surname>Zemla</surname><given-names>A</given-names></string-name>, <string-name><surname>Allen</surname><given-names>JE</given-names></string-name>, <string-name><surname>Kirshner</surname><given-names>D</given-names></string-name>, <string-name><surname>Lightstone</surname><given-names>FC</given-names></string-name>. <article-title>PDBspheres ‐ a method for finding 3D similarities in local regions in proteins</article-title>. <source>bioRxiv</source>. <year>2022</year>:<elocation-id>474934</elocation-id>. <pub-id pub-id-type="doi">10.1101/2022.01.04.474934</pub-id></mixed-citation>
    </ref>
    <ref id="pro4541-bib-0069">
      <mixed-citation publication-type="journal" id="pro4541-cit-0069"><string-name><surname>Zhang</surname><given-names>Z, Xu M, Jamasb A,</given-names></string-name> et al. <article-title>Protein representation learning by geometric structure Pretraining</article-title>. <source>arXiva</source>. <year>2022</year>;2203:06125. <pub-id pub-id-type="doi">10.48550/arXiv.2203.06125</pub-id>
</mixed-citation>
    </ref>
    <ref id="pro4541-bib-0070">
      <mixed-citation publication-type="journal" id="pro4541-cit-0070"><string-name><surname>Zhao</surname><given-names>J</given-names></string-name>, <string-name><surname>Cao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>L</given-names></string-name>. <article-title>Exploring the computational methods for protein‐ligand binding site prediction</article-title>. <source>Comput Struct Biotechnol J</source>. <year>2020</year>;<volume>18</volume>:<fpage>417</fpage>–<lpage>26</lpage>.<pub-id pub-id-type="pmid">32140203</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
