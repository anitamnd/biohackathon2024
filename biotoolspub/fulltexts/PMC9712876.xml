<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Digit Imaging</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Digit Imaging</journal-id>
    <journal-title-group>
      <journal-title>Journal of Digital Imaging</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0897-1889</issn>
    <issn pub-type="epub">1618-727X</issn>
    <publisher>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9712876</article-id>
    <article-id pub-id-type="pmid">35768752</article-id>
    <article-id pub-id-type="publisher-id">660</article-id>
    <article-id pub-id-type="doi">10.1007/s10278-022-00660-5</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>AnatomySketch: An Extensible Open-Source Software Platform for Medical Image Analysis Algorithm Development</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Zhuang</surname>
          <given-names>Mingrui</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Chen</surname>
          <given-names>Zhonghua</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1813-2162</contrib-id>
        <name>
          <surname>Wang</surname>
          <given-names>Hongkai</given-names>
        </name>
        <address>
          <email>wang.hongkai@dlut.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Tang</surname>
          <given-names>Hong</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>He</surname>
          <given-names>Jiang</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Qin</surname>
          <given-names>Bobo</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yang</surname>
          <given-names>Yuxin</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jin</surname>
          <given-names>Xiaoxian</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yu</surname>
          <given-names>Mengzhu</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Jin</surname>
          <given-names>Baitao</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Taijing</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kettunen</surname>
          <given-names>Lauri</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.30055.33</institution-id><institution-id institution-id-type="ISNI">0000 0000 9247 7930</institution-id><institution>School of Biomedical Engineering, Faculty of Electronic Information and Electrical Engineering, </institution><institution>Dalian University of Technology, </institution></institution-wrap>Dalian, 116024 China </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.9681.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 1013 7965</institution-id><institution>Faculty of Information Technology, </institution><institution>University of Jyväskylä, </institution></institution-wrap>40100 Jyväskylä, Finland </aff>
      <aff id="Aff3"><label>3</label>Liaoning Key Laboratory of Integrated Circuit and Biomedical Electronic System, Dalian, 116024 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>29</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <volume>35</volume>
    <issue>6</issue>
    <fpage>1623</fpage>
    <lpage>1633</lpage>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>10</month>
        <year>2021</year>
      </date>
      <date date-type="rev-recd">
        <day>7</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>5</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">The development of medical image analysis algorithm is a complex process including the multiple sub-steps of model training, data visualization, human–computer interaction and graphical user interface (GUI) construction. To accelerate the development process, algorithm developers need a software tool to assist with all the sub-steps so that they can focus on the core function implementation. Especially, for the development of deep learning (DL) algorithms, a software tool supporting training data annotation and GUI construction is highly desired. In this work, we constructed AnatomySketch, an extensible open-source software platform with a friendly GUI and a flexible plugin interface for integrating user-developed algorithm modules. Through the plugin interface, algorithm developers can quickly create a GUI-based software prototype for clinical validation. AnatomySketch supports image annotation using the stylus and multi-touch screen. It also provides efficient tools to facilitate the collaboration between human experts and artificial intelligent (AI) algorithms. We demonstrate four exemplar applications including customized MRI image diagnosis, interactive lung lobe segmentation, human-AI collaborated spine disc segmentation and Annotation-by-iterative-Deep-Learning (AID) for DL model training. Using AnatomySketch, the gap between laboratory prototyping and clinical testing is bridged and the development of MIA algorithms is accelerated. The software is opened at <ext-link ext-link-type="uri" xlink:href="https://github.com/DlutMedimgGroup/AnatomySketch-Software">https://github.com/DlutMedimgGroup/AnatomySketch-Software</ext-link>.</p>
      <sec>
        <title>Supplementary Information</title>
        <p>The online version contains supplementary material available at 10.1007/s10278-022-00660-5.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Medical image analysis</kwd>
      <kwd>Image annotation</kwd>
      <kwd>User interaction</kwd>
      <kwd>Algorithm development</kwd>
      <kwd>Deep learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Key Research and Development Program (CN)</institution>
        </funding-source>
        <award-id>2020YFB1711500</award-id>
        <award-id>2020YFB1711501</award-id>
        <award-id>2020YFB1711503</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wang</surname>
            <given-names>Hongkai</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>general program of National Natural Science Fund of China</institution>
        </funding-source>
        <award-id>81971693</award-id>
        <award-id>61971445</award-id>
        <award-id>61971089</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wang</surname>
            <given-names>Hongkai</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>Hong</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Dalian City Science and Technology Innovation Funding (CN)</institution>
        </funding-source>
        <award-id>2018J12GX042</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wang</surname>
            <given-names>Hongkai</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Fundamental Research Funds for the Central Universities (CN)</institution>
        </funding-source>
        <award-id>DUT19JC01</award-id>
        <award-id>DUT20YG122</award-id>
        <principal-award-recipient>
          <name>
            <surname>Wang</surname>
            <given-names>Hongkai</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>Hong</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Funding of Liaoning Key Lab of IC &amp; BME System</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Funding of Dalian Engineering Research Center for Artificial Intelligence in Medical Imaging</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par2">Nowadays, computer-assisted medical image analysis (MIA) algorithms are increasingly used in disease diagnosis and treatment. The development of MIA algorithms is a complex process involving algorithm design, model training, software implementation and performance testing. To speed up the development, researchers need a convenient software platform to assist with different sub-step of the process. Ideally, the platform should include a graphical user interface (GUI) for user interaction and data visualization, as well as a plugin interface for user-developed algorithm integration. Many software tools have been established to meet these needs. Several code libraries were developed to help with algorithm implementation, such as the classical itk [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>], vtk [<xref ref-type="bibr" rid="CR3">3</xref>], elastix [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>], ANTS [<xref ref-type="bibr" rid="CR6">6</xref>] and the recently published libraries for radiomics (e.g. pyradiaomics [<xref ref-type="bibr" rid="CR7">7</xref>]) and deep learning (e.g. monai<xref ref-type="fn" rid="Fn1">1</xref>). GUI-based software tools were also developed for image segmentation (e.g. ITK-SNAP [<xref ref-type="bibr" rid="CR8">8</xref>], MITK [<xref ref-type="bibr" rid="CR9">9</xref>], TurgleSeg [<xref ref-type="bibr" rid="CR10">10</xref>], Seg3D), data annotation (DicomAnnotator [<xref ref-type="bibr" rid="CR11">11</xref>] and Pair<xref ref-type="fn" rid="Fn2">2</xref>) and the analysis of specific imaging modalities (e.g. SpheroidJ [<xref ref-type="bibr" rid="CR12">12</xref>], MNI SISCOM [<xref ref-type="bibr" rid="CR13">13</xref>] and OIPAV [<xref ref-type="bibr" rid="CR14">14</xref>]). Few of these tools are extensible for user-developed algorithms including deep neural networks. The 3D Slicer [<xref ref-type="bibr" rid="CR15">15</xref>] software has powerful extension capabilities and a rich library of extension modules [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>], but its programming mechanism and interaction workflow are relatively complex for junior programmers.</p>
    <p id="Par3">In recent years, deep learning (DL) algorithms are increasingly used in clinical applications. To alleviate the heavy burden of data annotation for DL model training, the procedure of Annotation-by-iterative-Deep-Learning (AID) becomes popular. In the AID workflow, the human experts first annotate a small set of training data which are used for training a preliminary annotation network. The preliminary network is then used to automatically annotate more training data with imperfect accuracy. Next, the human expert proofread the network-annotated data to ensure the annotation accuracy, and the proofread data are supplemented to the training set to retrain a more accurate annotation network. As this process is repeated, the network becomes more and more accurate; thus, fewer and fewer human effort is needed for data annotation. To support the AID workflow, a software platform with convenient annotation/proofreading tools and a plugin interface for the annotation network model is necessary. Philbrick et al. developed the RIL-Contour [<xref ref-type="bibr" rid="CR18">18</xref>] software with AID function which supports multiuser collaboration and network model version management. However, this software only focuses on data annotation; it is not a general assistance tool for the entire algorithm development workflow.</p>
    <p id="Par4">Besides the needs of a software assisting the classical MIA algorithm development, the recently trend of DL algorithm research also requires a software platform to integrate neural network models and the AID workflow. Due to the lack of such a platform, researchers tediously switch between different assistance tools, making the development process complicated and slow. Moreover, without a GUI-based platform, many algorithms are published as command-line tools or even source codes that are unfriendly to clinical users.</p>
    <p id="Par5">In response to the existing needs, we developed a software platform named AnatomySketch for fast MIA algorithm integration and GUI-based software prototyping. AnatomySketch (AS) offers convenient tools for data annotation, image visualization and algorithm integration, so that the algorithm developers can focus on core function development and rapidly produce a software prototype for algorithm demonstration and testing. Compared to the existing software tools, AnatomySketch has an easier interface for DL model integration and more convenient supports for multi-touch and stylus-based image annotation. The software is developed with the following key features:<list list-type="simple"><list-item><label>(i)</label><p id="Par6"><italic>A convenient GUI for data visualization and human–computer interaction.</italic> AnatomySketch has a concise interface for multi-modality and multi-dimensional image visualization. It also incorporates a library of basic processing tools for medical images and graphical models to save the time of basic processing function implementation. To support the development of semi-automated algorithms and the annotation of training data, AnatomySketch provides simple workflow for data annotation mouse, shortcut keys, stylus and multi-touch screen. It also facilitates a simple correction of the automatic segmentation with an inbuilt contour editing method.</p></list-item><list-item><label>(ii)</label><p id="Par7"><italic>Flexible plugin interface for user-developed algorithms.</italic> With a flexible plugin interface, the software allows the users to integrate their algorithms (including DL models) as extension function modules. This feature facilitates rapid prototyping of GUI-based software for specific clinical applications, making the evaluation and demonstration of novel algorithms easier and faster. By combining the DL model plugins with the annotation tools, the AID workflow can be realized to speed up DL model training.</p></list-item></list></p>
    <p id="Par8">The following sections will introduce the detailed software feature and demonstrate exemplar applications of fast software prototyping and AID workflow realization.</p>
  </sec>
  <sec id="Sec2">
    <title>Method</title>
    <sec id="Sec3">
      <title>Software Design and Architecture</title>
      <p id="Par9">AnatomySketch is designed with a concise GUI consisting of a menu bar (on the top), a data list for data property management (on the top-left), a customizable widget panel for user-developed function modules (on the bottom-left) and a display area (on the right) with three orthogonal section windows and one three-dimensional (3D) view window. The design philosophy of the GUI is being simple, intuitive and familiar to the MIA researchers. The GUI has two operation modes, namely the desktop mode for mouse and keyboard interaction and the tablet mode for stylus and touch screen interaction.</p>
      <p id="Par10">The desktop mode has a classical layout (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a) similar to the well-known MITK, ITK-SNAP and 3D Slicer. Distinctively, the layout has a customizable widget panel for user-developed algorithms on the bottom-left (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b). A drop list (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c) on top of the panel allows the selection of algorithm modules, and the panel layout changes according to the selected algorithm. The algorithm-specific panel layout is defined via a configuration file specifying the positions and appearances of the control widgets (including drop menu, text box, press button and explanatory text). Details of the layout definition will be introduced in the “<xref rid="Sec6" ref-type="sec">Extension Module</xref>” section.<fig id="Fig1"><label>Fig. 1</label><caption><p><bold>a</bold> AnatomySketch interface. <bold>b</bold> The function module panel. <bold>c</bold> An example of the drop list of user-defined function modules, mostly segmentation methods in this case, including deep network models. <bold>d </bold>An example of calling the software GUI (the highlighted line of code) to visualize intermediate variables.</p></caption><graphic xlink:href="10278_2022_660_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par11">To assist with data visualization during algorithm debugging, we also allow the desktop mode GUI to be called as an inline function of the user programs (e.g. Python, MATLAB or C +  + code). This feature is especially useful for accepting user interaction during the algorithm workflow or for inspecting the intermediate variables of the data flow. The intermediate variables can be images, graphical shape models and user-annotations. It is possible to overlay multiple variables in the display window to check the accuracy of image segmentation and/or registration. Figure <xref rid="Fig1" ref-type="fig">1</xref>d shows a line of Python code (namely the “Open_In_AS” function) calling the GUI for intermediate image inspection. This Python code is available on the website of AnatomySketch.</p>
      <p id="Par12">The tablet mode is developed to take advantage of the multi-touch screen and the stylus (if available) for efficient annotation of region boundaries. This mode can be activated by clicking expansion button on the top-right of the display windows. The clicked window is enlarged as a palette for stylus sketching. As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, the operator can use the stylus with one hand to drawn contours and scribbles, and meanwhile use the other hand to zoom, pan or rotate the image via the multi-touch screen.<fig id="Fig2"><label>Fig. 2</label><caption><p>The architecture diagram of the software platform</p></caption><graphic xlink:href="10278_2022_660_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par13">The architecture of the AS software is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. The software architecture is composed of three layers including the interaction layer, the function layer and the data layer. The blue and green arrows denote the data and command paths between the modules, respectively. As the core of the software, the function modules coordinate all the other modules to handle data processing and user interactions.</p>
    </sec>
    <sec id="Sec4">
      <title>Interactive Annotation and Proofreading</title>
      <p id="Par14">As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b, AnatomySketch supports the annotation of anatomical landmarks, bounding boxes, edge contours, curves, scribbles and object regions using the mouse or the stylus. All the annotations can be accessed by the user-developed plugin modules as inputs. The annotations can also be exported into computer discs as separate files for offline algorithm training.<fig id="Fig3"><label>Fig. 3</label><caption><p>Interactive data annotation. <bold>a</bold> The tablet mode layout supporting stylus sketching and multi-touch gestures. <bold>b</bold> Multiple annotation tools are provided by the software</p></caption><graphic xlink:href="10278_2022_660_Fig3_HTML" id="MO3"/></fig></p>
      <p id="Par15">AnatomySketch also provides a convenient boundary correction tool for proofreading the segmentation results of automatic algorithms. This tool is implemented based on the free-form deformation (FFD) method [<xref ref-type="bibr" rid="CR19">19</xref>]. It allows the user to drag the inaccurate boundary towards the correct position (as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>). We applied both 2D and 3D versions of the FFD method for adjusting 2D contours and 3D surfaces, respectively.<fig id="Fig4"><label>Fig. 4</label><caption><p>Proofreading of the automatic segmentation result. The inaccurate boundary can be dragged towards the correct position (the dashed curve) using the stylus or the mouse</p></caption><graphic xlink:href="10278_2022_660_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par16">FFD is a point-controlled contour deformation method. After the user dragging operation, a <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$6\times 6$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mn>6</mml:mn><mml:mo>×</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="10278_2022_660_Article_IEq1.gif"/></alternatives></inline-formula> grid is constructed around the starting point of the dragging. The mouse/stylus motion vector (yellow arrow in Fig. <xref rid="Fig4" ref-type="fig">4</xref>) is first extrapolated to the <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$6\times 6$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mn>6</mml:mn><mml:mo>×</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="10278_2022_660_Article_IEq2.gif"/></alternatives></inline-formula> grid vertices by solving an inverse interpolation function of the cubic B-spline. Then, the deformation vectors of contour points are interpolated by solving the cubic B-sample interpolation function,<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{p}}=\sum\limits_{i=0}^{3}\left(\begin{array}{c}3\\ i\end{array}\right){\left(1-m\right)}^{3-i}{m}^{i}\left(\sum\limits_{j=0}^{3}\left(\begin{array}{c}3\\ j\end{array}\right) {\left(1-n\right)}^{3-j}{n}^{j}{\text{p}}_{i,j}\right)\end{array}$$\end{document}</tex-math><mml:math id="M6" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>p</mml:mtext><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:munderover><mml:mfenced close=")" open="("><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>3</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mi>i</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>m</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msup><mml:mfenced close=")" open="("><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:munderover><mml:mfenced close=")" open="("><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>3</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow/><mml:mi>j</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>n</mml:mi></mml:mfenced></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msup><mml:msub><mml:mtext>p</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="10278_2022_660_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par17">where <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{p}}$$\end{document}</tex-math><mml:math id="M8"><mml:mtext>p</mml:mtext></mml:math><inline-graphic xlink:href="10278_2022_660_Article_IEq3.gif"/></alternatives></inline-formula> is the interpolated deformation vector of a contour vertex, <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(m,n)$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="10278_2022_660_Article_IEq4.gif"/></alternatives></inline-formula> is the normalized local coordinate of the dragging start point, and <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{p}}_{i,j}$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mtext>p</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="10278_2022_660_Article_IEq5.gif"/></alternatives></inline-formula> is the deformation vector of grid node <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(i,j)$$\end{document}</tex-math><mml:math id="M14"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="10278_2022_660_Article_IEq6.gif"/></alternatives></inline-formula>. The readers are referred to [<xref ref-type="bibr" rid="CR19">19</xref>] for more details of the method.</p>
    </sec>
    <sec id="Sec5">
      <title>Extension Module</title>
      <p id="Par18">AnatomySketch provides a flexible plugin interface for the integration of user-developed algorithms. Figure <xref rid="Fig5" ref-type="fig">5</xref>a shows the workflow of the extension modules. A configuration file and a program file simply form the plugin interface of the software. The configuration file is a text-format definition file specifying the extension module type, input and output parameters and the GUI design. The software will read this definition file and realize the extension interface accordingly. The program file is the user-programmed executable (exe) file or a dynamic link library (dll) of the core algorithm function. Figure <xref rid="Fig5" ref-type="fig">5</xref>b shows an example of the configuration file for thresholding segmentation. The configuration file is imported into the software to create the customized widget panel (Fig. <xref rid="Fig5" ref-type="fig">5</xref>c). The program file is invoked by clicking the “Calculate” button on the widget panel. To transfer input and output data between the software and the program file, a loose coupling mechanism is adopted. The software first writes the input data (i.e. image arrays, polygonal meshes or annotations) into the computer disc, then the user program imports them for computation and writes the outputs to the computer disc. Finally, the software gets the output results from the disc and updates the GUI display.<fig id="Fig5"><label>Fig. 5</label><caption><p>The extension module. <bold>a</bold> The workflow of the extension module. <bold>b</bold> An example of the configuration file. <bold>c</bold> The widget panel generated by AnatomySketch according to the configuration file of (<bold>b</bold>)</p></caption><graphic xlink:href="10278_2022_660_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par19">On the AnatomySketch website, several quick-start templates of the configuration files and program files are provided. The developers can publish their algorithm modules on the website to promote the usage of their works.</p>
    </sec>
    <sec id="Sec6">
      <title>Deep Learning Support</title>
      <p id="Par20">Thanks to the flexible plugin interface, deep neural networks can be integrated into the software as plugin modules. The developer needs to create a Python-based program file to get input data from the software and calls the network model to process the data. The network can be developed using any DL platform (e.g. PyTorch, TensorFlow or Keras) and compiled as an executable file to be called by the user-created Python program. The outputs of the network are written into the hard drive and then imported into the software via the plugin interface.</p>
      <p id="Par21">In AnatomySketch, the AID workflow is realized by combining the annotation tools, the proofreading tools and the plugin modules. Rich annotation tools of AnatomySketch are used for labelling the primary training data, then a user-defined Python program is called to train the network and use the trained model to label more images. During the proofreading, the 2D and 3D FFD tools are used to correct the segmentation errors. The entire AID workflow is customized as a plugin module with a widget panel supporting iterative training and proofreading. Video 1 attached to this paper demonstrates the AID support features of AnatomySketch.</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Results</title>
    <p id="Par22">In this section, we will demonstrate the examples of using AnatomySketch for fast plugin module development and software prototyping. We will also show examples of human-AI collaborated image segmentation and AID-based data annotation. These examples demonstrate the convenience of integrating DL models or interactive algorithms into our software, while such integration can be time consuming or even infeasible for the existing tools.</p>
    <sec id="Sec8">
      <title>Software Prototyping</title>
      <p id="Par23">In the first example, AnatomySketch was used in a medical research project for analyzing intratumoral susceptibility signal intensities (ITSS) in enhanced T2 angiography of hepatocellular carcinoma. Because this study uses special MRI pulse sequences for ITSS imaging, customized software needs to be developed for the data analysis. As a collaborator of this study, our group took only 1 day to develop a plugin module and created a GUI-based software prototype for the ITSS MR image analysis. With our software prototype, the doctors contoured the region of interest (ROI) in a few interleaved axial slices and interpolated the 3D surface from the 2D contours. The proofreading tool was used to adjust the 3D surface to precisely fit the tumour boundary. The software automatically removed the MR imaging artefact and extracted the high-ITSS voxels via thresholding. The algorithm parameters such as ITSS threshold can be adjusted in the customized widget panel. Figure <xref rid="Fig6" ref-type="fig">6</xref> shows the software GUI. The blue contour represents the ROI boundary and the green pixels represent extracted high-ITSS voxels. This tool has been used in a series of published studies [<xref ref-type="bibr" rid="CR20">20</xref>–<xref ref-type="bibr" rid="CR23">23</xref>]. Although the development of this module is simple and straightforward in the AS platform, similar extension may require time-consuming software recompilation in some existing tools (e.g. for MITK and ITKsnap).<fig id="Fig6"><label>Fig. 6</label><caption><p>An example of user-defined extension module for ITSS analysis in MR images</p></caption><graphic xlink:href="10278_2022_660_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par24">In another example, AnatomySketch was used to annotate lung lobes in CT images for training a lobe segmentation network of commercial software. Engineers from a local company created an interactive lung lobe annotation plugin for AnatomySketch. The contours of lung fissures are manually sketched in a few coronal slices and a complete fissure surface was interpolated from the contours using radial basis function (RBF) interpolation. The proofreading tools of AnatomySketch were used to adjust the interpolated fissure surfaces. Figure <xref rid="Fig7" ref-type="fig">7</xref> shows the GUI of the plugin module; VIDEO 2 attached to this paper exhibits the working process of this plugin module. Using this module, the annotation of five lung lobes took less than 20 min per image. Two engineers from the company annotated 100 CT images in 3 days, thanks to the stylus support of our software. In contrast, stylus interaction is not specially optimized in any other existing tool.<fig id="Fig7"><label>Fig.7</label><caption><p>An example of user-developed plugin module for lung lobe annotation in CT images</p></caption><graphic xlink:href="10278_2022_660_Fig7_HTML" id="MO7"/></fig></p>
    </sec>
    <sec id="Sec9">
      <title>Deep Learning Supports</title>
      <p id="Par25">Through the plugin interface, user-developed deep network models are integrated into AnatomySketch as extension modules. As shown in the example of Fig. <xref rid="Fig8" ref-type="fig">8</xref>, a dense V-Net model [<xref ref-type="bibr" rid="CR24">24</xref>] was trained to segment the intervertebral disc and the surrounding nerves and vessels from lumbar CT images. Due to the lack of enough training data, the network occasionally produced inaccurate segmentation at the fuzzy boundary of the herniated discs (Fig. <xref rid="Fig8" ref-type="fig">8</xref>a). Thanks to the 3D FFD proofreading tool, a human expert was able to correct the inaccurate segmentation within 5 min per image (Fig. <xref rid="Fig8" ref-type="fig">8</xref>b). In this way, the AI model and human expert collaborate with each other to achieve efficient and accurate segmentation of anatomical objects with weak boundaries. As a comparison, none of the existing medical image processing tools facilitate such efficient and direct proofreading of the DL segmentation results. Both MITK and 3D Slicer provide the AI-assisted annotation plugins, but they require Internet connection to the NVIDIA AI-Assisted Annotation Server for data transfer, which is inconvenient for the applications without Internet connections or with data privacy concerns.<fig id="Fig8"><label>Fig. 8</label><caption><p>An example of 3D FFD proofreading. <bold>a</bold> The pink area in white contour is the automatic segmentation result of V-Net model. The red area and the white contour depict the under-segmented part. <bold>b</bold> Human expert proofreading result (the adjusted white contour) using the FFD tool</p></caption><graphic xlink:href="10278_2022_660_Fig8_HTML" id="MO8"/></fig></p>
      <p id="Par26">Another example of human-AI interaction is the realization of AID workflow. We trained a DeepSnake network [<xref ref-type="bibr" rid="CR25">25</xref>] to segment abdominal organs from CT images. The network generates 2D contours surrounding the target organs and deforms the contours to fit the organ boundaries. We first used a small set of expert-labelled training images to train a preliminary network, then used the preliminary network to generate the organ contours of more images. The automatically generated contours are proofread by human experts using the 2D FFD tool of AnatomySketch, and the images with proofread contours are supplemented to the training set to finetune the network. Figure <xref rid="Fig9" ref-type="fig">9</xref> displays the predicted contours of the preliminary and retrained models for two representative slices, respectively. It is obvious that the retrained network (trained with about 1800 slices of 15 CT series) yields more accurate contour prediction than the preliminary network (trained using about 600 slices of five CT series).<fig id="Fig9"><label>Fig. 9</label><caption><p>AID annotation results of two exemplar CT slices, showing that the retrained network yield more accurate results than the preliminary network. The ground truth comes from human expert manual labelling</p></caption><graphic xlink:href="10278_2022_660_Fig9_HTML" id="MO9"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Discussions</title>
    <p id="Par27">AnatomySketch is developed with the goal of accelerating MIA algorithm development and bridging the gap between laboratory research and clinical application. The software is designed to meet many specific needs of MIA algorithm development:<list list-type="simple"><list-item><label>(i)</label><p id="Par28"><italic>Software prototyping.</italic> During the past decade, many MIA algorithms were developed as command-line tools due to the difficulty of GUI construction. This problem also hampers the promotion of DL-based algorithms. Many DL models are published as source codes that cannot be tested by clinical users. AnatomySketch tackles this problem with a flexible plugin interface. As demonstrated in the examples of Figs. <xref rid="Fig6" ref-type="fig">6</xref> and <xref rid="Fig8" ref-type="fig">8</xref>, the prototyping of a new software tool for special image modality (e.g. the ITSS MR) or specific application task (e.g. the segmentation of lumbar disc herniation) took less than 1 day. With a friendly GUI, the promotion of new MIA algorithms in the clinical environment becomes faster and more convenient.</p></list-item><list-item><label>(ii)</label><p id="Par29"><italic>Data annotation.</italic> The fast popularization of deep learning techniques proposes strong needs for data annotation. AnatomySketch provides annotation tools and the tablet mode for efficient image annotation. It also realizes the AID workflow for iterative annotation and training. An AID workflow is developed for abdominal organ segmentation based on the DeepSnake network. Thanks to the flexible plugin interface, AnatomySketch is versatile enough for realizing task-specific AID workflow based on different network models.</p></list-item><list-item><label>(iii)</label><p id="Par30"><italic>Human–computer interaction.</italic> Because fully automatic algorithms may not guarantee robustness in complex clinical scenario, semi-automatic algorithms with human guidance and corrections become practical choices. As shown in the example of Fig. <xref rid="Fig7" ref-type="fig">7</xref>, the plugin module interpolates the lung fissure surfaces using human-annotated curves as the guidance. Figure <xref rid="Fig9" ref-type="fig">9</xref> demonstrates another example in which human interaction is introduced to correct the DL model output. AnatomySketch supports both input guidance and posterior correction to the DL models, facilitating flexible human-AI collaboration for clinical image analysis.</p></list-item></list></p>
    <p id="Par31">When comparing AnatomySketch with existing MIA software tools, we find that some key features of AnatomySketch have been integrated into the existing tools. The extension module is also available in the Slicer software [<xref ref-type="bibr" rid="CR15">15</xref>, <xref ref-type="bibr" rid="CR16">16</xref>]. The integration of DL models has been realized by the MITK NVIDIA clara plugin<xref ref-type="fn" rid="Fn3">3</xref> and the RIL-contour software [<xref ref-type="bibr" rid="CR18">18</xref>]. The AID workflow is also supported by the RIL-contour software. However, these software tools were only designed to assist with a certain step of the entire algorithm development workflow. The advantage of AnatomySketch is the supporting of the complete workflow, including data visualization, image annotation, algorithm integration and software prototyping.</p>
    <p id="Par32">As a newly established software, the number of user-developed plugin modules for AnatomySketch is still growing, especially for specific clinical applications. We will keep maintaining the web community to help the developers sharing their plugin modules and gain potential users from universities and hospitals. We also plan to add online crowdsourcing tools for multi-rater annotation and proofreading. Moreover, because AnatomySketch is increasingly used by the doctors who do not share the medical images, we will incorporate federated learning [<xref ref-type="bibr" rid="CR26">26</xref>] ability for multi-centre model training without sharing confidential medical data. A plugin module will be developed to allow AS software to communicate with multi-centre client databases and to invoke models for inference. In this scenario, physicians at each centre can use the tools provided in AS to annotate images and invoke network models with AS interface.</p>
  </sec>
  <sec id="Sec11">
    <title>Conclusion</title>
    <p id="Par33">We developed a medical image analysis software platform named AnatomySketch to assist with MIA algorithm development. The software is specially designed for efficient image annotation and convenient integration of user-developed algorithm modules including deep neural networks. The AID workflow can also be realized to accelerate the training of DL models. For the next step, we will construct a web community for sharing user-developed extension modules and incorporate federated learning to facilitate mutual learning between DL models from multi-centres.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec12">
      <p>Below is the link to the electronic supplementary material.<supplementary-material content-type="local-data" id="MOESM1"><media xlink:href="10278_2022_660_MOESM1_ESM.mp4"><caption><p>Supplementary file1 (MP4 67907 KB)</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM2"><media xlink:href="10278_2022_660_MOESM2_ESM.mp4"><caption><p>Supplementary file2 (MP4 1013 KB)</p></caption></media></supplementary-material></p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par36">Monai: <ext-link ext-link-type="uri" xlink:href="https://monai.io/">https://monai.io/</ext-link></p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par37">Pair: <ext-link ext-link-type="uri" xlink:href="http://www.aipair.com.cn/">http://www.aipair.com.cn/</ext-link></p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p id="Par38">MITK NVIDIA clara plugin: <ext-link ext-link-type="uri" xlink:href="https://www.mitk.org/wiki/MITK_ReleaseNotes_2018.04.2">https://www.mitk.org/wiki/MITK_ReleaseNotes_2018.04.2</ext-link></p>
    </fn>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Mingrui Zhuang and Zhonghua Chen contributed equally to this paper.</p>
    </fn>
  </fn-group>
  <notes notes-type="author-contribution">
    <title>Author Contribution</title>
    <p>Mingrui Zhuang and Zhonghua Chen contributed equally to this paper.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported in part by the National Key Research and Development Program No. 2020YFB1711500, 2020YFB1711501 and 2020YFB1711503; the general program of National Natural Science Fund of China (No. 81971693, 61971445 and 61971089); Dalian City Science and Technology Innovation Funding (No. 2018J12GX042); the Fundamental Research Funds for the Central Universities (No. DUT19JC01 and DUT20YG122); the funding of Liaoning Key Lab of IC &amp; BME System and Dalian Engineering Research Center for Artificial </p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of Data and Material</title>
    <p>The Declarations</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Code Availability</title>
    <p>The Anatomy Sketch software is opened at <ext-link ext-link-type="uri" xlink:href="https://github.com/DlutMedimgGroup/AnatomySketch-Software">https://github.com/DlutMedimgGroup/AnatomySketch-Software</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar1" notes-type="COI-statement">
      <title>Conflict of Interest</title>
      <p id="Par34">The authors declare no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yoo</surname>
            <given-names>TS</given-names>
          </name>
          <name>
            <surname>Ackerman</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Lorensen</surname>
            <given-names>WE</given-names>
          </name>
          <name>
            <surname>Schroeder</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Chalana</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Aylward</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Metaxas</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Whitaker</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Engineering and algorithm design for an image processing Api: a technical report on ITK–the Insight Toolkit</article-title>
        <source>Stud Health Technol Inform</source>
        <year>2002</year>
        <volume>85</volume>
        <fpage>586</fpage>
        <lpage>592</lpage>
        <?supplied-pmid 15458157?>
        <pub-id pub-id-type="pmid">15458157</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>McCormick</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Jomier</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Marion</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ibanez</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>ITK: enabling reproducible research and open science</article-title>
        <source>Frontiers in neuroinformatics</source>
        <year>2014</year>
        <volume>8</volume>
        <fpage>13</fpage>
        <pub-id pub-id-type="doi">10.3389/fninf.2014.00013</pub-id>
        <?supplied-pmid 24600387?>
        <pub-id pub-id-type="pmid">24600387</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Schroeder, W.; Martin, K.; Lorensen, B. <italic>The Visualization Toolkit (4th ed.)</italic>; Kitware: 2006.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Klein</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Staring</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Viergever</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Pluim</surname>
            <given-names>JPW</given-names>
          </name>
        </person-group>
        <article-title>elastix: A Toolbox for Intensity-Based Medical Image Registration</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2010</year>
        <volume>29</volume>
        <fpage>196</fpage>
        <lpage>205</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2009.2035616</pub-id>
        <?supplied-pmid 19923044?>
        <pub-id pub-id-type="pmid">19923044</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shamonin</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bron</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Lelieveldt</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Smits</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Klein</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Staring</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Fast Parallel Image Registration on CPU and GPU for Diagnostic Classification of Alzheimer's Disease</article-title>
        <source>Frontiers in Neuroinformatics</source>
        <year>2014</year>
        <pub-id pub-id-type="doi">10.3389/fninf.2013.00050</pub-id>
        <?supplied-pmid 24474917?>
        <pub-id pub-id-type="pmid">24474917</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Avants, B.; Tustison, N.; Song, G. Advanced normalization tools (ANTS). <italic>Insight J</italic><bold>2008</bold>, <italic>1–35</italic>.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van Griethuysen</surname>
            <given-names>JJM</given-names>
          </name>
          <name>
            <surname>Fedorov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Parmar</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hosny</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Aucoin</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Narayan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Beets-Tan</surname>
            <given-names>RGH</given-names>
          </name>
          <name>
            <surname>Fillion-Robin</surname>
            <given-names>J-C</given-names>
          </name>
          <name>
            <surname>Pieper</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Aerts</surname>
            <given-names>HJWL</given-names>
          </name>
        </person-group>
        <article-title>Computational Radiomics System to Decode the Radiographic Phenotype</article-title>
        <source>Cancer Research</source>
        <year>2017</year>
        <volume>77</volume>
        <fpage>e104</fpage>
        <lpage>e107</lpage>
        <pub-id pub-id-type="doi">10.1158/0008-5472.Can-17-0339</pub-id>
        <?supplied-pmid 29092951?>
        <pub-id pub-id-type="pmid">29092951</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <mixed-citation publication-type="other">Yushkevich, P.A.; Gao, Y.; Gerig, G. ITK-SNAP: An interactive tool for semi-automatic segmentation of multi-modality biomedical images. In Proceedings of the 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 16–20 Aug. 2016, 2016; pp. 3342–3345.</mixed-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wolf</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Vetter</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wegner</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Böttger</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Nolden</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Schöbinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hastenteufel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kunert</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Meinzer</surname>
            <given-names>H-P</given-names>
          </name>
        </person-group>
        <article-title>The Medical Imaging Interaction Toolkit</article-title>
        <source>Medical Image Analysis</source>
        <year>2005</year>
        <volume>9</volume>
        <fpage>594</fpage>
        <lpage>604</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2005.04.005</pub-id>
        <?supplied-pmid 15896995?>
        <pub-id pub-id-type="pmid">15896995</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Poon, K.; Hamarneh, G.; Abugharbieh, R. <italic>Segmentation of complex objects with non-spherical topologies from volumetric medical images using 3D livewire</italic>; SPIE: 2007; Volume 6512.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dong</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Haynor</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>O'Reilly</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Linnau</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Yaniv</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Jarvik</surname>
            <given-names>JG</given-names>
          </name>
          <name>
            <surname>Cross</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>DicomAnnotator: a Configurable Open-Source Software Program for Efficient DICOM Image Annotation</article-title>
        <source>J Digit Imaging</source>
        <year>2020</year>
        <volume>33</volume>
        <fpage>1514</fpage>
        <lpage>1526</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-020-00370-w</pub-id>
        <?supplied-pmid 32666365?>
        <pub-id pub-id-type="pmid">32666365</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lacalle</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Castro-Abril</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Randelovic</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Dominguez</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Heras</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mata</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Mata</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Mendez</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Pascual</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Ochoa</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>SpheroidJ: An Open-Source Set of Tools for Spheroid Segmentation</article-title>
        <source>Comput Methods Programs Biomed</source>
        <year>2021</year>
        <volume>200</volume>
        <fpage>105837</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105837</pub-id>
        <?supplied-pmid 33221056?>
        <pub-id pub-id-type="pmid">33221056</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moreau</surname>
            <given-names>JT</given-names>
          </name>
          <name>
            <surname>Saint-Martin</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Baillet</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Dudley</surname>
            <given-names>RWR</given-names>
          </name>
        </person-group>
        <article-title>MNI SISCOM: an Open-Source Tool for Computing Subtraction Ictal Single-Photon Emission CT Coregistered to MRI</article-title>
        <source>J Digit Imaging</source>
        <year>2021</year>
        <volume>34</volume>
        <fpage>357</fpage>
        <lpage>361</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-021-00422-9</pub-id>
        <?supplied-pmid 33604806?>
        <pub-id pub-id-type="pmid">33604806</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Xiang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>OIPAV: an Integrated Software System for Ophthalmic Image Processing, Analysis, and Visualization</article-title>
        <source>J Digit Imaging</source>
        <year>2019</year>
        <volume>32</volume>
        <fpage>183</fpage>
        <lpage>197</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-017-0047-6</pub-id>
        <?supplied-pmid 30187316?>
        <pub-id pub-id-type="pmid">30187316</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Pieper, S.; Halle, M.; Kikinis, R. 3D Slicer. In Proceedings of the 2004 2nd IEEE International Symposium on Biomedical Imaging: Nano to Macro (IEEE Cat No. 04EX821), 18–18 April 2004, 2004; pp. 632–635 Vol. 631.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fedorov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Beichel</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Kalpathy-Cramer</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Finet</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Fillion-Robin</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Pujol</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bauer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Jennings</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Fennessy</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sonka</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>3D Slicer as an image computing platform for the Quantitative Imaging Network</article-title>
        <source>Magn Reson Imaging</source>
        <year>2012</year>
        <volume>30</volume>
        <fpage>1323</fpage>
        <lpage>1341</lpage>
        <pub-id pub-id-type="doi">10.1016/j.mri.2012.05.001</pub-id>
        <?supplied-pmid 22770690?>
        <pub-id pub-id-type="pmid">22770690</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <mixed-citation publication-type="other">Pinter, C.; Lasso, A.; Wang, A.; Sharp, G.C.; Alexander, K.; Jaffray, D.; Fichtinger, G. Performing radiation therapy research using the open-source SlicerRT toolkit. In <italic>World Congress on Medical Physics and Biomedical Engineering, June 7–12, 2015, Toronto, Canada</italic>; IFMBE Proceedings; 2015; pp. 622–625.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Philbrick</surname>
            <given-names>KA</given-names>
          </name>
          <name>
            <surname>Weston</surname>
            <given-names>AD</given-names>
          </name>
          <name>
            <surname>Akkus</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Kline</surname>
            <given-names>TL</given-names>
          </name>
          <name>
            <surname>Korfiatis</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sakinis</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Kostandy</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Boonrod</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zeinoddini</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Takahashi</surname>
            <given-names>N</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>RIL-Contour: a Medical Imaging Dataset Annotation Tool for and with Deep Learning</article-title>
        <source>J Digit Imaging</source>
        <year>2019</year>
        <volume>32</volume>
        <fpage>571</fpage>
        <lpage>581</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-019-00232-0</pub-id>
        <?supplied-pmid 31089974?>
        <pub-id pub-id-type="pmid">31089974</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <mixed-citation publication-type="other">Sederberg, T.W.; Parry, S.R. Free-form deformation of solid geometric models. In Proceedings of the Proceedings of the 13th annual conference on Computer graphics and interactive techniques, 1986; pp. 151–160.</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Cui, D.; Liu, A.; Wang, H.; Zhuang, M.; Zhao, Y.; Song, Q. Value of intratumoral susceptibility signal intensities in quantitatively and automatically evaluating histological grading of hepatocellular carcinoma using enhanced T2 star-weighted angiography. In Proceedings of the ISMRM 2021, Vancouver, Canada, 2021; p. 4364.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Cui, D.; Liu, A.; Wang, H.; Zhuang, M.; Song, Q. The combination of ITSS and R2* in quantitatively and automatically evaluating histological grade of HCC using ESWAN: A feasibility study. In Proceedings of the ISMRM 2021, Vancouver, Canada, 2021; p. 4645.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Hu, W.; Liu, A.; Li, Y.; Wang, H.; Zhuang, M.; Song, Q. Evaluation of R2* and automatically quantitative ITSS in diagnosis of malignant ovarian tumor. In Proceedings of the ISMRM 2021, Vancouver, Canada, 2021; p. 3758.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Liu, Y.; Wang, H.; Zhuang, M.; Chen, L.; Song, Q.; Meng, S.; Liu, A. Differential diagnosis of PCa and BPH using intratumoral susceptibility signal intensities based on ESWAN. In Proceedings of the ISMRM 2021, Vancouver, Canada, 2021; p. 3723.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gibson</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Giganti</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bonmati</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Bandula</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gurusamy</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Davidson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Pereira</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Clarkson</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Barratt</surname>
            <given-names>DC</given-names>
          </name>
        </person-group>
        <article-title>Automatic Multi-Organ Segmentation on Abdominal CT With Dense V-Networks</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <fpage>1822</fpage>
        <lpage>1834</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2018.2806309</pub-id>
        <?supplied-pmid 29994628?>
        <pub-id pub-id-type="pmid">29994628</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Peng, S.; Jiang, W.; Pi, H.; Li, X.; Bao, H.; Zhou, X. Deep Snake for Real-Time Instance Segmentation. 2020, pp 8530–8539.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pfitzner</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Steckhan</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Arnrich</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Federated Learning in a Medical Context: A Systematic Literature Review</article-title>
        <source>ACM Transactions on Internet Technology</source>
        <year>2021</year>
        <volume>21</volume>
        <fpage>1</fpage>
        <lpage>31</lpage>
        <pub-id pub-id-type="doi">10.1145/3412357</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
