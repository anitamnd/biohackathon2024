<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neurorobot</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neurorobot</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neurorobot.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neurorobotics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5218</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9383414</article-id>
    <article-id pub-id-type="doi">10.3389/fnbot.2022.958052</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Hypothesis and Theory</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>SPD-CNN: A plain CNN-based model using the symmetric positive definite matrices for cross-subject EEG classification with meta-transfer-learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Lezhi</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1720533/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Yu</surname>
          <given-names>Zhuliang</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/524109/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Yang</surname>
          <given-names>Jian</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c002" ref-type="corresp">
          <sup>*</sup>
        </xref>
        <uri xlink:href="http://loop.frontiersin.org/people/1844476/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>College of Automation Science and Engineering, South China University of Technology</institution>, <addr-line>Guangzhou</addr-line>, <country>China</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Pazhou Laboratory</institution>, <addr-line>Guangzhou</addr-line>, <country>China</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Duo Chen, Nanjing University of Chinese Medicine, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Minpeng Xu, Tianjin University, China; Peng Zhang, Huazhong University of Science and Technology, China</p>
      </fn>
      <corresp id="c001">*Correspondence: Zhuliang Yu <email>zlyu@scut.edu.cn</email></corresp>
      <corresp id="c002">Jian Yang <email>yangjianxin@scut.edu.cn</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>03</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>16</volume>
    <elocation-id>958052</elocation-id>
    <history>
      <date date-type="received">
        <day>31</day>
        <month>5</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>04</day>
        <month>7</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Chen, Yu and Yang.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Chen, Yu and Yang</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>The electroencephalography (EEG) signals are easily contaminated by various artifacts and noise, which induces a domain shift in each subject and significant pattern variability among different subjects. Therefore, it hinders the improvement of EEG classification accuracy in the cross-subject learning scenario. Convolutional neural networks (CNNs) have been extensively applied to EEG-based Brain-Computer Interfaces (BCIs) by virtue of the capability of performing automatic feature extraction and classification. However, they have been mainly applied to the within-subject classification which would consume lots of time for training and calibration. Thus, it limits the further applications of CNNs in BCIs. In order to build a robust classification algorithm for a calibration-less BCI system, we propose an end-to-end model that transforms the EEG signals into symmetric positive definite (SPD) matrices and captures the features of SPD matrices by using a CNN. To avoid the time-consuming calibration and ensure the application of the proposed model, we use the meta-transfer-learning (MTL) method to learn the essential features from different subjects. We validate our model by making extensive experiments on three public motor-imagery datasets. The experimental results demonstrate the effectiveness of our proposed method in the cross-subject learning scenario.</p>
    </abstract>
    <kwd-group>
      <kwd>EEG</kwd>
      <kwd>Motor imagery</kwd>
      <kwd>SPD matrices</kwd>
      <kwd>CNN</kwd>
      <kwd>Meta-transfer-learning</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>National Natural Science Foundation of China</institution>
            <institution-id institution-id-type="doi">10.13039/501100001809</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="6"/>
      <table-count count="6"/>
      <equation-count count="7"/>
      <ref-count count="42"/>
      <page-count count="0"/>
      <word-count count="7087"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>An EEG-based Brain-Computer Interface (BCI) is a system to measure and analyze the electroencephalography (EEG) brain signal (Rao, <xref rid="B29" ref-type="bibr">2013</xref>), thus enabling the communication or interaction between the brain and external environment (Kothe and Makeig, <xref rid="B16" ref-type="bibr">2013</xref>). Recent research has opened up the possibility for EEG signals to apply in rehabilitation (Tariq et al., <xref rid="B37" ref-type="bibr">2018</xref>), entertainment (Nijholt et al., <xref rid="B27" ref-type="bibr">2008</xref>), and transportation (Göhring et al., <xref rid="B7" ref-type="bibr">2013</xref>) because of the harmless, non-invasive, and inexpensive features of the EEG-BCI. Motor imagery (MI), which refers to the mental simulation of body movements, is a famous paradigm of the EEG-BCI system (Lotze and Halsband, <xref rid="B24" ref-type="bibr">2006</xref>). MI signals are widely used in the BCI system (Alamgir et al., <xref rid="B1" ref-type="bibr">2010</xref>; Arvaneh et al., <xref rid="B2" ref-type="bibr">2013</xref>; Jayaram et al., <xref rid="B13" ref-type="bibr">2016</xref>) because of their flexibility in reflecting the bioelectrical activity of the brain. These signals attract increasing attention in rehabilitation therapy (Naseer and Hong, <xref rid="B25" ref-type="bibr">2013</xref>, <xref rid="B26" ref-type="bibr">2015</xref>; Hong et al., <xref rid="B11" ref-type="bibr">2015</xref>).</p>
    <p>However, due to the separation between the signal source (inside the human brain) and the detector, the EEG signals would be easily contaminated by various artifacts and noise, including muscle movements, eye blinks, heartbeats, and environmental electro-magnetic field in the applications of the BCI-system. This phenomenon induces a domain shift in each subject, even in different sessions of the same subject (Reuderink et al., <xref rid="B30" ref-type="bibr">2011</xref>), and exhibits significant pattern variability between different subjects. Consequently, it hinders people from using the data generated from different subjects to improve the performance of the BCI system (Lotte and Guan, <xref rid="B23" ref-type="bibr">2010</xref>) and increasingly reduces the accuracy and stability of EEG cross-subject classification. Currently, the users of the BCI-system have to provide tons of EEG-data to build a user-specific classifier so that the system can work properly. Accordingly, it greatly lengthens the time of calibration of the BCI system and heavily inhibits BCI-system development.</p>
    <p>To overcome this problem, lots of methods are proposed to eliminate the shifting problem of data distribution among different subjects. Rodrigues et al. (<xref rid="B31" ref-type="bibr">2018</xref>) present a transfer Learning approach to match the statistical distributions of different sessions/subjects. This method allows the BCI systems to reuse the data from different users and reduce the calibration time. He and Wu (<xref rid="B9" ref-type="bibr">2019</xref>) propose a Euclidean Space Data Alignment Approach to align the time-domain EEG trials in the Euclidean Space and alleviate the domain shift between different sessions and subjects successfully. However, this kind of <bold>data-augmentation</bold> method normally classifies the data by the traditional geometry-aware classifiers (such as support vector machine and the minimum distance to mean classifier) (Barachant et al., <xref rid="B3" ref-type="bibr">2013</xref>), which are insufficient for feature extraction. Also, it requires people to use certain prior knowledge of brain science.</p>
    <p>With the development of machine learning, deep learning technology has been applied to extract discriminative features from EEG (Lotte et al., <xref rid="B22" ref-type="bibr">2018</xref>) and many <bold>model-based</bold> learning algorithms have been proposed for MI-EEG cross-session/subject classification (Wu et al., <xref rid="B39" ref-type="bibr">2020</xref>). Schirrmeister et al. (<xref rid="B33" ref-type="bibr">2017</xref>) focus on the application of different CNN architectures in EEG-MI classification and design an efficient network architecture to decode information from the EEG-MI signal. This method shows the powerful feature extraction ability of CNN and draws a great deal of attention to the applications of CNN in the BCI system. Lawhern et al. (<xref rid="B18" ref-type="bibr">2018</xref>) propose a brand-new compact CNN-based model called EEGNet, which contains depth-wise and separable convolutions to extract the descriptive information from EEG signal directly. This network structure is robust enough to learn a wide variety of interpretable features over a range of BCI tasks in cross-session/subject learning and gain outstanding classification performance. Fahimi et al. (<xref rid="B4" ref-type="bibr">2019</xref>) propose an inter-subject transfer learning framework built on top of the CNN model which is fed into three different EEG representations and transfers knowledge between different subjects thus avoiding time-consuming re-training. However, this kind of network focus on the feature extraction of EEG signal and their performances would deteriorate when the data of the user are insufficient, especially in the few-shot scenario of cross-subject learning.</p>
    <p>In the most recent studies, meta-learning, which is a task-level learning method, has seen substantial advancements in computer vision and speech recognition recently (Vanschoren, <xref rid="B38" ref-type="bibr">2018</xref>). This kind of learning method helps the neural network to extract usable features from related tasks and largely increases the generalization ability of the neural network. Li et al. (<xref rid="B20" ref-type="bibr">2021</xref>) use the training method called Model-Agnostic Meta-Learning (MAML) (Finn et al., <xref rid="B6" ref-type="bibr">2017</xref>) and build the CNN-based classifier which combines one and two dimensional-CNN layers to improve the accuracy of the MI-EEG classification. However, these kinds of meta-learning structure are very sensitive to neural network architectures (usually shallow neural networks), which often leads to instability during training and easily induces overfitting problems. Therefore, it limits the effectiveness of meta-learning.</p>
    <p>In consequence, given the above, an effective model that is capable of capturing essential features and a robust meta-learning method are both essential to cross-subject learning in EEG classification. The symmetric positive definite (SPD) matrices have been widely used in motor imagery EEG-based classification over the past few decades (Barachant et al., <xref rid="B3" ref-type="bibr">2013</xref>; Xu et al., <xref rid="B40" ref-type="bibr">2021</xref>), because of their capacity to capture informative structure from the data (Huang and Van Gool, <xref rid="B12" ref-type="bibr">2017</xref>). In terms of the ability to capture input data structure, the CNN has the powerful capability of extracting features of two-dimensional matrix-shape data (LeCun et al., <xref rid="B19" ref-type="bibr">1998</xref>; Krizhevsky et al., <xref rid="B17" ref-type="bibr">2012</xref>) and the SPD matrices are one of the two-dimensional matrix-shape data. Therefore, Hajinoroozi et al. (<xref rid="B8" ref-type="bibr">2017</xref>) combine the SPD matrices of EEG data and the deep learning method and present a series of deep covariance learning models for drivers' fatigue prediction, which explore the potential of this kind of method for the application of BCI system. Inspired by this, we propose a plain CNN-based model called SPD-CNN, which transforms the EEG signal into the SPD matrices and uses a CNN with five convolutional layers to capture the features of SPD matrices. Also, we apply a cutting edge meta-updating strategy called the meta-transfer-learning (MTL) (Sun et al., <xref rid="B35" ref-type="bibr">2019</xref>) which combines the advantage of transfer learning and meta-learning to extract the subject invariant features and alleviates the shifting problem between the source domain (training subjects) and the target domain (test subjects). The major contributions of this article can be summarized as follows.</p>
    <list list-type="bullet">
      <list-item>
        <p>The SPD-CNN model we proposed uses the SPD matrices of the EEG signal as descriptors to highlight the spatial information of the EEG-MI signal and reduces the diversity of EEG data characteristics of different subjects. Additionally, the proposed descriptor tremendously decreases the size of data and effectively reduces the difficulty of feature extraction.</p>
      </list-item>
      <list-item>
        <p>Using the MTL as our learning strategy helps the network extract the crucial features. In other words, it can transfer the domain knowledge between different subjects during the training process and enhance the robustness of the network in the BCI system.</p>
      </list-item>
      <list-item>
        <p>To the best of our knowledge, the network we proposed is simple to design and has fewer parameters than most networks for EEG classification currently. Therefore, it could simplify the training process tremendously and shortens the training time extremely.</p>
      </list-item>
    </list>
    <p>The remainder of the article is organized as follows. Section 2 presents the framework of the proposed approach. Section 3 describes the experimental settings, then shows the results, and provides a comprehensive analysis. The effectiveness of the proposed SPD descriptor is discussed in Section 4 and the conclusion is summarized in Section 5.</p>
  </sec>
  <sec id="s2">
    <title>2. Materials and methodology</title>
    <sec>
      <title>2.1. Data description</title>
      <p>We present examples with three public EEG-MI datasets which are BNCI2014001 (Tangermann et al., <xref rid="B36" ref-type="bibr">2012</xref>), BNCI2015004 (Scherer et al., <xref rid="B32" ref-type="bibr">2015</xref>), and Sch2017 (Schirrmeister et al., <xref rid="B33" ref-type="bibr">2017</xref>).</p>
      <p>BNCI2014001 consists of the EEG data from 9 subjects and this MI-paradigm consists of four different motor imagery tasks that the subjects are required to make the imagination of movement of the left hand, right hand, both feet and tongue. The EEG Signals are recorded with 22 electrodes at a 250 Hz sampling rate and two sessions were recorded for each subject. Each session is composed of 6 runs separated by short breaks. One run consists of 48 trials (12 for each of the four possible classes), yielding a total of 288 trials per session.</p>
      <p>BNCI2015004 is a 30-electrode dataset obtained from 14 subjects with disability (spinal cord injury and stroke). The dataset consists of five classes of imagined movements of right-hand and feet, mental word association, mental subtraction, and spatial navigation. The EEG signals are recorded at a 250 Hz sampling rate, and two sessions were recorded for each subject. Each session consists of 8 runs, resulting in 40 trials of each class. The EEG signals were bandpass filtered 0.5–100 Hz and sampled at a rate of 256 Hz.</p>
      <p>Sch2017 is a 128-electrode dataset obtained from 14 healthy subjects [6 women, 2 left-handed, age 27.2 ± 3.6 (mean ± std)] and this MI-paradigm consists of four different motor imagery tasks which ask subjects to make the imagination of movement of the left hand, right hand, both feet, and rest (no movement), with roughly 1,000 four-second trials of executed movements divided into 13 runs (each run consist of the approximately 1,000 trails per subject.</p>
      <p>Three datasets mentioned above are publicly available on the "Mother of all BCI Benchmarks"(MOABB) framework (Jayaram and Barachant, <xref rid="B14" ref-type="bibr">2018</xref>). In the experiment section, the subjects in the same dataset will be divided into training subjects, validation subjects, and test subjects who provide data for the training set, validation set, and test set for the cross-subject learning experiments, respectively. More details can be seen in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
      <table-wrap position="float" id="T1">
        <label>Table 1</label>
        <caption>
          <p>Key information of the three MI-EEG datasets used for experiences.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Dataset</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Number</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Trails</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Class</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Band pass</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Number of</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>of subjects</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>per subject</bold>
              </th>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>filter (Hz)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>electrodes channnels</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">BNCI2014001</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">576</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4–250</td>
              <td valign="top" align="center" rowspan="1" colspan="1">22</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">BNCI2015004</td>
              <td valign="top" align="center" rowspan="1" colspan="1">9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">400</td>
              <td valign="top" align="center" rowspan="1" colspan="1">5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">0.5–100</td>
              <td valign="top" align="center" rowspan="1" colspan="1">30</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sch2017</td>
              <td valign="top" align="center" rowspan="1" colspan="1">14</td>
              <td valign="top" align="center" rowspan="1" colspan="1">1,000</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4–250</td>
              <td valign="top" align="center" rowspan="1" colspan="1">128</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>2.2. SPD-CNN model</title>
      <p><xref rid="T2" ref-type="table">Table 2</xref> gives a brief description of the mathematical symbols that will be used in the rest of the article.</p>
      <table-wrap position="float" id="T2">
        <label>Table 2</label>
        <caption>
          <p>Table of symbols used in the article.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Symbols</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Meaning</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>F</italic>(Θ, θ)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">The classification network with parameter Θ and θ</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Θ, θ</td>
              <td valign="top" align="left" rowspan="1" colspan="1">The parameter of the feature extractor block and the classifier block</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Θ<sup><italic>pre</italic></sup>, Θ<sup><italic>meta</italic></sup></td>
              <td valign="top" align="left" rowspan="1" colspan="1">The parameter after the pre-train phase and the meta-adaption phase</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>f</italic>(Θ, θ)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">The network after the <italic>F</italic>(Θ, θ) is upgraded by specific task</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>X</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">The multiple time-series of a EEG matrix</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>C</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">The covariance matrix estimated by <italic>X</italic></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>D</italic><sub><italic>tr</italic></sub>, <italic>D</italic><sub><italic>val</italic></sub>, <italic>D</italic><sub><italic>te</italic></sub></td>
              <td valign="top" align="left" rowspan="1" colspan="1">Dataset <italic>D</italic> for Training,Validation and Testing phase</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>T</italic>
                <sub>
                  <italic>i</italic>
                </sub>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">The specific task which is sample from the <italic>D</italic></td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>l</italic>
                <sub>
                  <italic>i</italic>
                </sub>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">The loss function in task i during the inner-loop</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>L</italic>
                <sub>ϕ</sub>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">The meta loss function in meta training</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>As mentioned above, we are particularly interested in the case where the SPD matrices are spatial covariance matrices, which describe the second-order statistics of zero-mean multivariate time series. We assume that the information on the power and spatial distribution of EEG sources can be coded by a covariance matrix. Therefore, the spatial covariance matrix <italic>C</italic> of a T-sample realization of a zero-mean <italic>d</italic>-dimensional time series (<italic>d</italic> being the number of electrode channels) <italic>X</italic> ∈ <italic>R</italic><sup><sup><italic>d</italic></sup><sup>×</sup><italic>T</italic></sup>, is estimated as</p>
      <disp-formula id="E1">
        <label>(1)</label>
        <mml:math id="M1" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>C</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
                <mml:msubsup>
                  <mml:mrow>
                    <mml:mi>X</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                  </mml:mrow>
                </mml:msubsup>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>X</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>,</mml:mo>
                <mml:mi>i</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
                <mml:mo>,</mml:mo>
                <mml:mn>2</mml:mn>
                <mml:mo>,</mml:mo>
                <mml:mo>…</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mi>n</mml:mi>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>X</italic><sub><italic>i</italic></sub> is the sample from the EEG dataset <italic>D</italic> = {<italic>X</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, 2, ⋯ , <italic>n</italic>} and n is the total amount of samples in dataset <italic>D</italic>.</p>
      <p>Based on the analysis above, we develop a covariance matrix estimator called the SPD descriptor that captures not only the diversity among different electrode channels but also the statistical properties of EEG image regions. The descriptor is capable of estimating the <italic>d</italic>×<italic>d</italic> covariance matrix of the EEG features mentioned in Equation (1). Then, these matrices are normalized with the whole sample set mentioned in Equation (2) to improve the numerical stability of the model. Consequently, the network is able to focus on the critical features and accelerate the learning process (Shanker et al., <xref rid="B34" ref-type="bibr">1996</xref>).</p>
      <disp-formula id="E2">
        <label>(2)</label>
        <mml:math id="M2" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:msubsup>
                        <mml:mi>C</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mo>*</mml:mo>
                      </mml:msubsup>
                      <mml:mo>=</mml:mo>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:msub>
                        <mml:mi>C</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mo>−</mml:mo>
                      <mml:msub>
                        <mml:mi>C</mml:mi>
                        <mml:mrow>
                          <mml:mi>m</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>a</mml:mi>
                          <mml:mi>n</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>/</mml:mo>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mi>C</mml:mi>
                        <mml:mrow>
                          <mml:mi>s</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>d</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                      <mml:mo>,</mml:mo>
                      <mml:mn>2</mml:mn>
                      <mml:mo>,</mml:mo>
                      <mml:mo>⋯</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where <italic>C</italic><sub><italic>mean</italic></sub>, <italic>C</italic><sub><italic>std</italic></sub> is the mean and SD of covariance matrix set <italic>C</italic> = {<italic>C</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, 2, ⋯ , <italic>n</italic>} and <inline-formula><mml:math id="M3" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the output sample of the descriptor.</p>
      <p>After being processed by the SPD descriptor, the <italic>d</italic>×<italic>d</italic> matrices are taken into a Feature Extractor block. This block contains five convolutional layers (<italic>Conv</italic>1, <italic>Conv</italic>2, <italic>Conv</italic>3,<italic>Conv</italic>4, <italic>Conv</italic>5) with minimum convolution kernel (2 × 2). Then the output data from Feature Extractor were flattened and taken through a classifier block with a two-layer fully-connected network (<italic>FC</italic>) onto the BCI outputs. A whole visualization and full description of the SPD-CNN model can be found in <xref rid="F1" ref-type="fig">Figure 1</xref> and <xref rid="T3" ref-type="table">Table 3</xref>.</p>
      <fig position="float" id="F1">
        <label>Figure 1</label>
        <caption>
          <p>Overall visualization of the SPD-CNN architecture. It starts with an SPD descriptor to transform EEG into SPD matrices, then the matrices are encoded by Feature Extractor Block and flattened as the input data of the classification block. In the classification block, the features are passed to a two-fully connected layer and put into a soft-max classification with K units, K is the number of classes in the data.</p>
        </caption>
        <graphic xlink:href="fnbot-16-958052-g0001" position="float"/>
      </fig>
      <table-wrap position="float" id="T3">
        <label>Table 3</label>
        <caption>
          <p>Basic parameter of SPD-CNN model.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Block</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Layers</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Size and Kernel</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Activation</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Feature extractor</td>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>Conv</italic>1</td>
              <td valign="top" align="center" rowspan="1" colspan="1">4 × (2 ×2)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Relu</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>Conv</italic>2</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8 × (2 ×2)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Relu</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>Conv</italic>3</td>
              <td valign="top" align="center" rowspan="1" colspan="1">16 × (2 ×2)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Relu</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>Max</italic>−<italic>pool</italic></td>
              <td valign="top" align="center" rowspan="1" colspan="1">(2 ×2)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">-</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>Conv</italic>4</td>
              <td valign="top" align="center" rowspan="1" colspan="1">32 × (2 ×2)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Relu</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>Conv</italic>5</td>
              <td valign="top" align="center" rowspan="1" colspan="1">64 × (2 ×2)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Relu</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Clasiifier</td>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>FC</italic>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">2 ×32</td>
              <td valign="top" align="left" rowspan="1" colspan="1">softmax</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>In the Feature Extractor block,Conv means a convolutional layer and FC represents a two-layer fully-connected network, with 32 neurons inside the hidden layer as shown in <xref rid="F1" ref-type="fig">Figure 1</xref>.</p>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>2.3. Training structure and learning strategy</title>
      <p>Our training structure is to help the model extract the key features through learning a better initial set of parameters from various tasks of different subjects. Hence, the network gains a fast adaption to new user tasks using only a few data. This learning strategy is based on the assumption that the EEG data from different subjects share the same representative features. These features are just masked by the effect of individual variation and wide discrepancy in the experiment environment. In this section, we illustrate the main idea of MTL and describe its application in the EEG cross-subject learning scenario.</p>
      <p>The MTL combines the advantage of transfer learning and meta-learning structure. This training method uses the fine-tune skill and model-agnostic meta-learning (MAML) algorithm (Finn et al., <xref rid="B6" ref-type="bibr">2017</xref>) with a novel constrained setting on network parameters called scaling and shifting (SS) operation to solve the overfitting problem. Hence, our training framework consists of three parts: Pre-train, Meta-updating, and Fast adaption. The whole workflow in this framework is shown in <xref rid="F2" ref-type="fig">Figure 2</xref>.</p>
      <fig position="float" id="F2">
        <label>Figure 2</label>
        <caption>
          <p>Workflow of our training framework. The dataset for training, validation, and test process is displayed on different rectangular regions with the colors <inline-formula><mml:math id="M112" overflow="scroll"><mml:mstyle mathcolor="blue"><mml:mtext>blue</mml:mtext></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="M111" overflow="scroll"><mml:mstyle mathcolor="yellow"><mml:mtext>yellow</mml:mtext></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="M123" overflow="scroll"><mml:mstyle mathcolor="red"><mml:mtext>red</mml:mtext></mml:mstyle></mml:math></inline-formula>, respectfully. The picture of human heads in different colors (such as <inline-formula><mml:math id="M124" overflow="scroll"><mml:mstyle mathcolor="purple"><mml:mtext>purple</mml:mtext></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="M251" overflow="scroll"><mml:mstyle mathcolor="blue"><mml:mtext>blue</mml:mtext></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="M126" overflow="scroll"><mml:mstyle mathcolor="yellow"><mml:mtext>yellow</mml:mtext></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="M127" overflow="scroll"><mml:mstyle mathcolor="brown"><mml:mtext>brown</mml:mtext></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="M128" overflow="scroll"><mml:mstyle mathcolor="red"><mml:mtext>red</mml:mtext></mml:mstyle></mml:math></inline-formula>) with a hand or feet inside represent the data from different subjects doing motor-imagery tasks. The black horizontal lines with a <inline-formula><mml:math id="M129" overflow="scroll"><mml:mstyle mathcolor="black"><mml:mtext>black arrow</mml:mtext></mml:mstyle></mml:math></inline-formula> represent the change of the parameter of the neural network and the colorful vertical and horizontal lines (such as <inline-formula><mml:math id="M301" overflow="scroll"><mml:mstyle mathcolor="purple"><mml:mtext>purple</mml:mtext></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="M131" overflow="scroll"><mml:mstyle mathcolor="blue"><mml:mtext>blue</mml:mtext></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="M133" overflow="scroll"><mml:mstyle mathcolor="gray"><mml:mtext>gray</mml:mtext></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="M134" overflow="scroll"><mml:mstyle mathcolor="green"><mml:mtext>green</mml:mtext></mml:mstyle></mml:math></inline-formula>) indicate the direction of data flow. In addition, the black gears in the circle represent the update process of parameters. <bold>(A)</bold> Pre-train phase, <bold>(B)</bold> Meta-update phase, and <bold>(C)</bold> Domain-adaption phase.</p>
        </caption>
        <graphic xlink:href="fnbot-16-958052-g0002" position="float"/>
      </fig>
      <p>As shown in <xref rid="F2" ref-type="fig">Figures 2</xref>, <xref rid="F3" ref-type="fig">3</xref>, in the Pre-train phase, data of training subjects are merged randomly into a training dataset <italic>D</italic><sub><italic>tr</italic></sub> for classifier <italic>F</italic>(Θ<sup>*</sup>, θ<sup>*</sup>). The network <italic>F</italic> with initialized parameter (Θ<sup>*</sup>, θ<sup>*</sup>) is optimized by the traditional gradient descent method (refer to Equation 3) and gains the better initialized parameter(Θ<sup><italic>pre</italic></sup>, θ<sup><italic>pre</italic></sup>).</p>
      <disp-formula id="E3">
        <label>(3)</label>
        <mml:math id="M4" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>Θ</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>*</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>;</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>θ</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>*</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                  <mml:mo>]</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mo>:</mml:mo>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>Θ</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>*</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>;</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>θ</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>*</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                  <mml:mo>]</mml:mo>
                </mml:mrow>
                <mml:mo>-</mml:mo>
                <mml:mi>α</mml:mi>
                <mml:mo>∇</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>L</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>D</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mi>r</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mo>[</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mo>Θ</mml:mo>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo>*</mml:mo>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo>;</mml:mo>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>θ</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo>*</mml:mo>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mo>]</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where α is the learning rate of and <italic>L</italic><sub><italic>D</italic><sub><italic>tr</italic></sub></sub> denotes the most frequently used empirical loss in machine learning like cross-entropy (Zhang and Sabuncu, <xref rid="B42" ref-type="bibr">2018</xref>). This process neglects the domain shift from different subjects and provides a rough direction for the network to upgrade the parameter.</p>
      <fig position="float" id="F3">
        <label>Figure 3</label>
        <caption>
          <p>Diagram of parameters variation through the learning process in different phases. <bold>(A)</bold> Pre-train phase, <bold>(B)</bold> Meta-update phase, and <bold>(C)</bold> Domain-adaption phase.</p>
        </caption>
        <graphic xlink:href="fnbot-16-958052-g0003" position="float"/>
      </fig>
      <p>In the meta-update phase(b), we randomly re-initialize the parameter θ<sup>*</sup> first and use the MAML structure (Finn et al., <xref rid="B6" ref-type="bibr">2017</xref>) as a training structure with constraining parameter Φ<sub><italic>ss</italic></sub>. The Φ<sub><italic>ss</italic></sub> is updated by Equation (4) as follows,</p>
      <disp-formula id="E4">
        <label>(4)</label>
        <mml:math id="M5" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msub>
                  <mml:mrow>
                    <mml:mo>Φ</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>s</mml:mi>
                    <mml:mi>s</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>:</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mo>Φ</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>s</mml:mi>
                    <mml:mi>s</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>-</mml:mo>
                <mml:mi>λ</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mo>∇</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>Φ</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>s</mml:mi>
                        <mml:mi>s</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>L</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:mo>⋯</mml:mo>
                    <mml:mspace width="0.3em" class="thinspace"/>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>n</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mo>[</mml:mo>
                      <mml:mrow>
                        <mml:mo>Θ</mml:mo>
                        <mml:mo>;</mml:mo>
                        <mml:mi>θ</mml:mi>
                      </mml:mrow>
                      <mml:mo>]</mml:mo>
                    </mml:mrow>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>Φ</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>s</mml:mi>
                        <mml:mi>s</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where λ is the learning rate during the update process of Φ<sub><italic>ss</italic></sub>. The main idea of constraining parameter Φ<sub><italic>ss</italic></sub> is to restrict the learning process of weight and bias in each convolutional layer, which means the weights and the biases of the same CNN layer are scaled and shifted as a whole, respectively.</p>
      <p>To be specific, the weights <italic>W</italic> in the same CNN layer will time a scaling factor Φ<sub><italic>s</italic><sub>1</sub></sub> and the biases <italic>b</italic> in the same CNN layer will add a shifting factor Φ<sub><italic>s</italic><sub>2</sub></sub> through an update process. Assuming X is the input data, the SS operation could be expressed by Equation (5).</p>
      <disp-formula id="E5">
        <label>(5)</label>
        <mml:math id="M6" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>S</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>X</mml:mi>
                    <mml:mo>;</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mi>W</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mi>b</mml:mi>
                    <mml:mo>;</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>Φ</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>s</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>Φ</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>s</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>W</mml:mi>
                    <mml:mo>⊙</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>Φ</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>s</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
                <mml:mi>X</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mi>b</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>Φ</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>s</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where ⊙ denotes the element-wise multiplication (For details, refer to the article by Sun et al., <xref rid="B35" ref-type="bibr">2019</xref>).</p>
      <p>Inside the MAML learning framework, we sample the data of <italic>j</italic> classes (where <italic>j</italic> is the number of ways in few-shot learning) from the same training subject for a task. Therefore, each subject-specific task is seen as an independent sample of the same classification problem.</p>
      <p>More specifically, the train set <italic>D</italic><sub><italic>tr</italic></sub> was segmented into different training tasks <italic>T</italic><sub><italic>i</italic><sub><italic>k</italic></sub></sub> and test tasks <inline-formula><mml:math id="M7" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>,where <italic>T</italic><sub><italic>i</italic><sub><italic>k</italic></sub></sub>⊂<italic>T</italic> = {<italic>T</italic><sub><italic>i</italic><sub>1</sub></sub>, <italic>T</italic><sub><italic>i</italic><sub>2</sub></sub>, ...<italic>Ti</italic><sub><italic>n</italic></sub>} and <inline-formula><mml:math id="M8" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo>⊂</mml:mo><mml:msup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>T</mml:mi><mml:msubsup><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, n being the number of tasks in meta-learning. Significantly, the data of <italic>T</italic><sub><italic>i</italic><sub><italic>k</italic></sub></sub> and <inline-formula><mml:math id="M9" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are sampled from the same training subject <italic>x</italic><sub><italic>i</italic></sub> and the data of the subject-specified task (<italic>T</italic><sub><italic>i</italic><sub><italic>k</italic></sub></sub> or <inline-formula><mml:math id="M10" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>) are divided into training data and test data for the training process. As a result, the generalized model <italic>F</italic>(Θ<sup><italic>pre</italic></sup>, θ<sup>*</sup>).</p>
      <p>will be trained into different subject-specified networks <inline-formula><mml:math id="M11" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by gradient descent method. Also, after training the <inline-formula><mml:math id="M12" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with the training data of the test task <inline-formula><mml:math id="M13" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> again and calculating the loss function based on the test data of the <inline-formula><mml:math id="M14" overflow="scroll"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, each network <inline-formula><mml:math id="M15" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> would generate subject-specified loss <italic>l</italic><sub><italic>i</italic><sub><italic>k</italic></sub></sub>. After updating the parameter Θ<sup><italic>pre</italic></sup> several learning epochs, which is guided by the meta-loss <italic>L</italic>(Θ) based on different subject-specified loss <italic>l</italic><sub><italic>i</italic><sub><italic>k</italic></sub></sub>(refer to Equation 6), the parameters Θ<sup><italic>meta</italic></sup> with better generalization ability are selected by validate set <italic>D</italic><sub><italic>val</italic></sub> through the meta-validation process.</p>
      <disp-formula id="E6">
        <label>(6)</label>
        <mml:math id="M16" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:mi>L</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mrow>
                        <mml:mo>Θ</mml:mo>
                      </mml:mrow>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>∑</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>l</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>k</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>,</mml:mo>
                <mml:mi>k</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
                <mml:mo>,</mml:mo>
                <mml:mn>2</mml:mn>
                <mml:mo>,</mml:mo>
                <mml:mo>⋯</mml:mo>
                <mml:mspace width="0.3em" class="thinspace"/>
                <mml:mo>,</mml:mo>
                <mml:mi>n</mml:mi>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>In the domain-adaption phase(c), we fix the parameter of Feature Extractor Θ<sup><italic>meta</italic></sup> learned from the Meta-update phase and use the Fine-tune skill to train a user-specify network <inline-formula><mml:math id="M17" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which is greatly adapted to the user <italic>u</italic><sub><italic>j</italic></sub> pattern. In this process, a few data of the user from the test set are used to train the <italic>F</italic>(Θ<sup><italic>meta</italic></sup>, θ<sup>*</sup>) into <inline-formula><mml:math id="M18" overflow="scroll"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>Θ</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the parameter of the classifier block is updated by the Equation (7).</p>
      <disp-formula id="E7">
        <label>(7)</label>
        <mml:math id="M19" overflow="scroll">
          <mml:mtable class="eqnarray" columnalign="left">
            <mml:mtr>
              <mml:mtd>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>θ</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>*</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>′</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>←</mml:mo>
                <mml:mi>θ</mml:mi>
                <mml:mo>-</mml:mo>
                <mml:mi>β</mml:mi>
                <mml:msub>
                  <mml:mrow>
                    <mml:mo>∇</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>θ</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>L</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>u</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mo>[</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mo>Θ</mml:mo>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>m</mml:mi>
                            <mml:mi>e</mml:mi>
                            <mml:mi>t</mml:mi>
                            <mml:mi>a</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo>;</mml:mo>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>θ</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo>*</mml:mo>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mo>]</mml:mo>
                    </mml:mrow>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mo>Φ</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>s</mml:mi>
                        <mml:mi>s</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo stretchy="false">)</mml:mo>
                </mml:mrow>
              </mml:mtd>
            </mml:mtr>
          </mml:mtable>
        </mml:math>
      </disp-formula>
      <p>where β is the learning rate during the update process. After this phase, the network is greatly adapted to the situation of user <italic>u</italic><sub><italic>j</italic></sub> and gains better prediction in the BCI system.</p>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Experiments and results</title>
    <p>Our experiments aim to assess whether SPD-CNN is capable of extracting the discriminative information of EEG data recorded from different subjects and evaluate the transfer capacity of our proposed learning structure in the cross-subject scenario based on the recognition accuracy in the few-shot learning framework.</p>
    <sec>
      <title>3.1. Implementions details</title>
      <p>We conduct normal machine learning and few-shot learning experiments on the cross-subject scenario. In these experiments, we compare SPD-CNN with two wildly used models, DeepConvNet (Lawhern et al., <xref rid="B18" ref-type="bibr">2018</xref>) and EEGnet (Schirrmeister et al., <xref rid="B33" ref-type="bibr">2017</xref>), which perform well on EEG classification with code publicly available. The experiments show the different performance of classification between our training strategy and the benchmark of transfer learning methods in EEG classification.</p>
      <p>In the experiences of datasets BNCI2014001 and BNCI20150004, we choose three subjects for the validation set, two subjects as the user for the test set and all the remaining subjects for the training set randomly. This choosing process repeats 18 times, thus, producing 18 different folds. We follow the same procedure for the experiences of dataset Sch2017 except we increase the number of validate subjects to 5 and generate 28 folds.</p>
      <p>In the few-shot scenario, we consider the 4-class classification (5-class classification for BNCI2015004), so we sample 4-class(5-class classification for BNCI2015004), 5-shot/10 shot episodes to contain 5 or 10 samples for a train episode and 10 samples (each class) for episode test.</p>
      <p>The parameter of the network in our experiments are initialized by the normalization method from He et al. (<xref rid="B10" ref-type="bibr">2015</xref>) and the whole model is trained by Adam optimizer (Kingma and Ba, <xref rid="B15" ref-type="bibr">2014</xref>). The learning rates α, λ, and β of all learning phases are initialized as 0.001 and dropped by 1% every 10 epochs. All the loss functions are the normal form of cross-entropy cause there is no sample imbalance problem in all datasets (Fatourechi et al., <xref rid="B5" ref-type="bibr">2008</xref>). In the Pre-train phase, the batch size is set to 64 and the network will be trained 50 epochs in each fold. In the experiments of MTL, each task is sampled from the same subjects of all classes evenly in the meta-update phase. Furthermore, we use 60 tasks that form 12 meta-batch(5 tasks for each meta-batch) in one training update loop and choose 30 random tasks for meta-validation and meta-test. In the meta-update phase, the network will be trained 40 epochs in each fold.</p>
      <p>All the models were implemented based on PyTorch (Paszke et al., <xref rid="B28" ref-type="bibr">2019</xref>) and trained on a single GPU of 12 GB TITAN-Xp with Intel Xeon CPU E5-2620 v4 as CPU. More details can be found in the GitHub repository <ext-link xlink:href="https://github.com/sabinechen/SPD-CNN-Using-Meta-Transfer-Learing-EEG-Cross-Subject-learning" ext-link-type="uri">https://github.com/sabinechen/SPD-CNN-Using-Meta-Transfer-Learing-EEG-Cross-Subject-learning</ext-link>.</p>
    </sec>
    <sec>
      <title>3.2. Experimental evaluation</title>
      <p>To show the effectiveness of our model and learning strategy, we design some comparative experiments and ablative settings: Three networks are trained on the chosen dataset using Normal Machine Learning (ML), Transfer Learning (TL), and MTL method. In the experiments of ML, we train the networks from scratch only using the source-domain data, which is also called zero-shot.In the experiments of TL, we pre-train the networks on the source domain and fine-tune the classifier block of the networks on the target domain (5-shot and 10-shot). <xref rid="F4" ref-type="fig">Figures 4</xref>, <xref rid="F5" ref-type="fig">5</xref> provide a qualitative summary of the results for the cross-subject classification accuracy.</p>
      <fig position="float" id="F4">
        <label>Figure 4</label>
        <caption>
          <p>In each radar picture, every axis is assigned a variable that represents the classification accuracy of the specific subject (such as 1:subject1 and 2:subject2) and the different colors represent different network architectures (<inline-formula><mml:math id="M140" overflow="scroll"><mml:mstyle mathcolor="#8ea6c6"><mml:mtext>Blue:EEGNet</mml:mtext></mml:mstyle></mml:math></inline-formula>,<inline-formula><mml:math id="M141" overflow="scroll"><mml:mstyle mathcolor="#f2ddd9"><mml:mtext>Red:DeepConvNet</mml:mtext></mml:mstyle></mml:math></inline-formula>,<inline-formula><mml:math id="M142" overflow="scroll"><mml:mstyle mathcolor="#6fb9af"><mml:mtext>Green:SPD-CNN</mml:mtext></mml:mstyle></mml:math></inline-formula>).Also, the radar pictures arranged in the same column are shown the performance of experiences in the same dataset. The subgraph <bold>(A)</bold> represents the experiments that train the network with the ML method using zero-shot in the target domain. The subgraphs <bold>(B,C)</bold> represent the experiments that train the network with TL and MTL methods, respectively and fine-tune the network using 10shot on the target domain.</p>
        </caption>
        <graphic xlink:href="fnbot-16-958052-g0004" position="float"/>
      </fig>
      <fig position="float" id="F5">
        <label>Figure 5</label>
        <caption>
          <p>The aim of this radar picture is to show the different performances using different training methods and different colors represent different training strategies (<inline-formula><mml:math id="M150" overflow="scroll"><mml:mstyle mathcolor="#8ea6c6"><mml:mtext>Blue:ML</mml:mtext></mml:mstyle></mml:math></inline-formula>,<inline-formula><mml:math id="M151" overflow="scroll"><mml:mstyle mathcolor="#f2ddd9"><mml:mtext>Red:TL</mml:mtext></mml:mstyle></mml:math></inline-formula>,<inline-formula><mml:math id="M152" overflow="scroll"><mml:mstyle mathcolor="#6fb9af"><mml:mtext>Green:MTL</mml:mtext></mml:mstyle></mml:math></inline-formula>).The three subgraphs <bold>(A–C)</bold> represent the classification performance of the three models, respectively.</p>
        </caption>
        <graphic xlink:href="fnbot-16-958052-g0005" position="float"/>
      </fig>
      <p><xref rid="F4" ref-type="fig">Figure 4</xref> gives an overall picture of the performances obtained by training EEGNet, DeepConvNet, and SPD-CNN net on the target domains (10shot) with three learning strategies: ML, TL, and MTL. It shows that the three networks show noticeably varying patterns in the accuracy of different subjects in cross-subject learning. The green area, which represents the performance of SPD-CNN, almost covers other different color areas. It reveals that SPD-CNN has the remarkable ability to transfer source domains (train set) to the target domain (test set) in three datasets.</p>
      <p><xref rid="F5" ref-type="fig">Figure 5</xref> gives an overall picture of the performances of using different learning strategies on different networks. It shows that the coverage area of MTL is more evenly distributed in all dimensions than other learning strategies in most cases, indicating that the MTL strategy performs better than the other two learning strategies in the experiments. Therefore, we can conclude that the MTL learning strategy strengthens the generalization ability and robustness of the networks.</p>
      <p>Furthermore, we present the accuracy of different experiments and give a quantitative summary of the results in <xref rid="T4" ref-type="table">Table 4</xref> below.</p>
      <table-wrap position="float" id="T4">
        <label>Table 4</label>
        <caption>
          <p>The 4-way, 10-shot, and 5-shot classification accuracy (%) on three datasets (5-way for BNCI2015004).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr style="border-bottom: thin solid #000000;">
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Dataset</bold>
              </th>
              <th valign="top" align="center" colspan="5" rowspan="1">
                <bold>BNCI2014001</bold>
              </th>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <inline-graphic xlink:href="fnbot-16-958052-i0001.jpg"/>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>ML</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>TL-10</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>MTL-10</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>TL-5</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>MTL-5</bold>
              </th>
            </tr>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1"/>
              <th valign="top" align="center" colspan="5" rowspan="1"/>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EEGNet</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>37.65 ± 2.847</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">35.25 ± 2.7</td>
              <td valign="top" align="center" rowspan="1" colspan="1">35.95 ± 3.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">28.68 ± 3.17</td>
              <td valign="top" align="center" rowspan="1" colspan="1">29.84 ± 3.15</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DeepConv</td>
              <td valign="top" align="center" rowspan="1" colspan="1">33.96 ± 3.14</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38.75 ± 4.32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">38.8 ± 3.9</td>
              <td valign="top" align="center" rowspan="1" colspan="1">34.52 ± 3.52</td>
              <td valign="top" align="center" rowspan="1" colspan="1">35.64 ± 3.4</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>SPD-CNN</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">36.88 ± 3.56</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>46.78 ± 2.78</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>47.44 ± 4.1</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>42.99 ± 2.78</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>43.39 ± 2.63</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" colspan="5" rowspan="1">
                <bold>BNCI2015004</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EEGNet</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20.88 ± 2.72</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.22 ± 3.32</td>
              <td valign="top" align="center" rowspan="1" colspan="1">22.76 ± 2.36</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.3 ± 4.28</td>
              <td valign="top" align="center" rowspan="1" colspan="1">22.79 ± 3.4</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DeepConv</td>
              <td valign="top" align="center" rowspan="1" colspan="1">20.46 ± 2.23</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.37 ± 2.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">23.29 ± 3.02</td>
              <td valign="top" align="center" rowspan="1" colspan="1">21.74 ± 3.76</td>
              <td valign="top" align="center" rowspan="1" colspan="1">22.47 ± 4.31</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>SPD-CNN</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>22.74 ± 2.45</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>27.92 ± 3.3</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>28.57 ± 2.58</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>25.62 ± 3.63</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>26.82 ± 3.77</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" colspan="5" rowspan="1">
                <bold>Sch2017</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">EEGNet</td>
              <td valign="top" align="center" rowspan="1" colspan="1">50.2 ± 4.23</td>
              <td valign="top" align="center" rowspan="1" colspan="1">48.07 ± 3.91</td>
              <td valign="top" align="center" rowspan="1" colspan="1">49.27 ± 3.12</td>
              <td valign="top" align="center" rowspan="1" colspan="1">43.82 ± 4.21</td>
              <td valign="top" align="center" rowspan="1" colspan="1">45.13 ± 4.26</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">DeepConv</td>
              <td valign="top" align="center" rowspan="1" colspan="1">44.25 ± 3.06</td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.02 ± 3.49</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>59.22 ± 4.39</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">51 ± 2.67</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>56.4 ± 3.53</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <bold>SPD-CNN</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>50.44 ± 3.04</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>56.31 ± 4.53</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">56.92 ± 3.79</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>51.13 ± 3.44</bold>
              </td>
              <td valign="top" align="center" rowspan="1" colspan="1">52.94 ± 3.67</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p>Each accuracy is averaged over all subjects and folds. (in 95% confidence level). Bold values represents best results in this set of experiments.</p>
        </table-wrap-foot>
      </table-wrap>
      <p>As can be seen in <xref rid="T4" ref-type="table">Table 4</xref>, there was a statistically significant difference in the performance of different models [Friedman Test <inline-formula><mml:math id="M20" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>15</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>16</mml:mn><mml:mo>.</mml:mo><mml:mn>53</mml:mn></mml:math></inline-formula>, <italic>p</italic> = 0.0002 &lt; 0.05, <italic>Post-hoc</italic> analysis with Wilcoxon signed rank tests was conducted] and our model outperforms EEGNet (<italic>p</italic> = 0.0003 &lt; 0.05) and DeepConv (<italic>p</italic> = 0.005 &lt; 0.05) in most cases through vertical comparison in the table.</p>
      <p>Also, there was a statistically significant difference in the performance of different learning structures [<inline-formula><mml:math id="M21" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>9</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>.</mml:mo><mml:mn>67</mml:mn></mml:math></inline-formula>, <italic>p</italic> = 0.013 &lt; 0.05] and our learning structure (MTL-10) outperforms the traditional learning structures (TL-10: <italic>p</italic> = 0.0039 &lt; 0.05, ML: <italic>p</italic> = 0.019 &lt; 0.05) by a margin of 0.4–5.4% on accuracy through horizontal comparison and the improvement is much more evident when the data provided by the user for fast adaption (number of shots) is fewer in most cases. Furthermore, DeepConvNet gains much more improvement (about 3–5% in Sch2017) through MTL learning strategy than EEGNet and SPD-CNN net with shallow layers and little parameters. It suggests that the SS operation of MTL can effectively avoid the problem of “catastrophic forgetting” (Lopez-Paz and Ranzato, <xref rid="B21" ref-type="bibr">2017</xref>) (It means forgetting general patterns when adapting to a specific task) and as a result, the performance advantage of large-scale CNN is unleashed thoroughly, especially facing with large-scale data.</p>
      <p>Nevertheless, there is no free lunch, DeepConvNet required complex network design, and this kind of large-scale network architecture needs a high level of hardware, which consumes lots of time on the design and calibration in the BCI system. To be specific, the comparison of time complexity and the scale of data of neural networks are shown in <xref rid="T5" ref-type="table">Table 5</xref>. <xref rid="T5" ref-type="table">Table 5</xref> shows that SPD-CNN has a high speed of convergence and shorter training time, which are attributed to the small-scale input data and the plain network structure with little parameter.</p>
      <table-wrap position="float" id="T5">
        <label>Table 5</label>
        <caption>
          <p>The time complexity and scale of the dataset for different networks are compared in this table.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Method</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>EEGNet</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>DeepConvNet</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>SPD-CNN</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Dataset</td>
              <td valign="top" align="center" colspan="3" rowspan="1">Training Time (minute)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">BNCI2014001</td>
              <td valign="top" align="center" rowspan="1" colspan="1">48 m</td>
              <td valign="top" align="center" rowspan="1" colspan="1">156 m</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>23m</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">BNCI2015004</td>
              <td valign="top" align="center" rowspan="1" colspan="1">52 m</td>
              <td valign="top" align="center" rowspan="1" colspan="1">179 m</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>36m</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sch2017</td>
              <td valign="top" align="center" rowspan="1" colspan="1">218 m</td>
              <td valign="top" align="center" rowspan="1" colspan="1">418 m</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>175m</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="center" colspan="3" rowspan="1">Data Size(Gigabyte)</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">BNCI2014001</td>
              <td valign="top" align="center" colspan="2" rowspan="1">0.89G</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.016G</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">BNCI2015004</td>
              <td valign="top" align="center" colspan="2" rowspan="1">1.5G</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>0.022G</bold>
              </td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Sch2017</td>
              <td valign="top" align="center" colspan="2" rowspan="1">30.8G</td>
              <td valign="top" align="center" rowspan="1" colspan="1">
                <bold>1.7G</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="TN1">
            <p><italic>In this table, we train different network architectures using MTL, then calculate the scale of input data and the average consuming time of all folds. Bold values represents best results in this set of experiments</italic>.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>As described above, it can be concluded that the proposed SPD-CNN with few learnable parameters has a stronger feature extraction ability to find an approximate boundary to separate different samples from different labels, when the datasets are well described in the SPD manifold. Moreover, with the improvement coming from the MTL learning structure, the CNN-based model would rapidly adapt to the target domain with efficient usage of target data without forgetting key features learned from the source domain.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <sec>
      <title>4.1. Analysis of the SPD descriptor</title>
      <p>Extensive experiments above show that the SPD matrices are capable of retaining the discriminative information of brain activity and the information can be effectively extracted by the proposed network.</p>
      <p>To study the impact of different data descriptions in the cross-subject learning scenario, the raw EEG data and the SPD matrices of different subjects in BNCI2014001 were reduced to two dimensions by Principal Component Analysis (PCA) and all the samples from the same class were projected to this 2D feature space (Zhang et al., <xref rid="B41" ref-type="bibr">2018</xref>). Consequently, the sample distributions of the different subjects could be visualized in <xref rid="F6" ref-type="fig">Figure 6</xref>. Then we use averaged Euclidean distance to quantitatively measure the distance among different subjects in the feature space, and the result are shown in <xref rid="T6" ref-type="table">Table 6</xref>.</p>
      <fig position="float" id="F6">
        <label>Figure 6</label>
        <caption>
          <p>The disparity among five subjects of two classes, right hand and feet, which are on the left and right of the figure, respectively. <bold>(A)</bold> The visualization of sample distributions of raw EEG data. <bold>(B)</bold> The visualization of sample distributions of the SPD matrices.</p>
        </caption>
        <graphic xlink:href="fnbot-16-958052-g0006" position="float"/>
      </fig>
      <table-wrap position="float" id="T6">
        <label>Table 6</label>
        <caption>
          <p>The averaged Euclidean distance among different subjects of SPD matrices and EEG data in the feature space.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Class</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Euclidean</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>Euclidean</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>distance</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>distance</bold>
              </th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>(SPD matrices)</bold>
              </th>
              <th valign="top" align="center" rowspan="1" colspan="1">
                <bold>(EEG data)</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Right hand</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8.85</td>
              <td valign="top" align="center" rowspan="1" colspan="1">13.85</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Feet</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8.71</td>
              <td valign="top" align="center" rowspan="1" colspan="1">13.72</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Left hand</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8.90</td>
              <td valign="top" align="center" rowspan="1" colspan="1">13.86</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Tongue</td>
              <td valign="top" align="center" rowspan="1" colspan="1">8.33</td>
              <td valign="top" align="center" rowspan="1" colspan="1">13.96</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The results of <xref rid="F6" ref-type="fig">Figure 6</xref> and <xref rid="T6" ref-type="table">Table 6</xref> revealed that the gaps in the sample distributions among different subjects were closed by transforming the EEG data into SPD matrices.</p>
    </sec>
    <sec>
      <title>4.2. Limitations and future directions</title>
      <p>Though the proposed network and learning strategy have achieved great performance in the cross-subject scenario, the limitation is still involved in the current study. For the experiments, we only validate our method on the paradigm of motor imagery and the effectiveness of our method on the other paradigm in the EEG-BCI system is still unclear. Therefore, in future studies, we will focus on the other paradigm such as Steadystate Visually Evoked Potential (SSVEP) datasets and further explore the potential of the proposed approach.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="s5">
    <title>5. Conclusion</title>
    <p>In this study, we represent a brand-new model to extract cognitive information from EEG data. Compared with the two famous EEG networks, which utilize different convolutional layers to learn specific filters, we transform EEG signals into SPD matrices and design a plain CNN to learn the essential features from it. Considering the shifting problem between different subjects, we use the MTL training strategies to train our model and related experiments show that our training strategy is capable of keeping the adaptation ability of the networks while significantly reducing the number of parameters to transfer. It can be concluded that our proposed model performs well in the cross-subject learning scenario.</p>
    <p>Our contribution is part of a larger effort in the BCI learning research, intending to design robust algorithms which use the experience of deep learning in image recognition to mitigate inter-subject variability (Xu et al., <xref rid="B40" ref-type="bibr">2021</xref>) and extract shared information between different subjects. Besides, it is easy to notice that we could use more complex CNN-based models, which have the powerful feature extraction ability for SPD data. Given that, the topic considered here also opens several important questions to be investigated in the future. For instance, considering the feasibility of the network to extract the characteristics of the SPD data, to determine how to design the specific network architecture for this kind of data is promising research. Furthermore, with the feature expression based on the SPD form, data formats of different experiments in the same paradigm can be unified, and it allows us to gather information from several databases and use the CNN-based model to form a more robust classifier in the future.</p>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data availability statement</title>
    <p>The data that support the findings of this study are openly available in <ext-link xlink:href="https://github.com/NeuroTechX/moabb" ext-link-type="uri">https://github.com/NeuroTechX/moabb</ext-link>.</p>
  </sec>
  <sec id="s7">
    <title>Ethics statement</title>
    <p>Written informed consent was obtained from the individual(s) for the publication of any potentially identifiable images or data included in this article.</p>
  </sec>
  <sec id="s8">
    <title>Author contributions</title>
    <p>LC developed the theoretical formalism, performed the analytic calculations, and performed the numerical simulations. ZY and JY contributed to the final version of the manuscript. ZY supervised the project. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="funding-information" id="s9">
    <title>Funding</title>
    <p>This research was supported in part by the National Natural Science Foundation of China under Grants 61836003 and 61906211 and the Major Program of - Brain Science and Brain-Like Research of the National Science and Technology Innovation 2030 under Grant 2022ZD0211700.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s10">
    <title>Publisher's note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We thank the associate editor and the reviewers for their useful feedback that improved this paper. We are grateful to Professor Zhenghui Gu for her supervising the project and MinLing Feng for her help with the preparation of figures in this paper.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Alamgir</surname><given-names>M.</given-names></name><name><surname>Grosse-Wentrup</surname><given-names>M.</given-names></name><name><surname>Altun</surname><given-names>Y.</given-names></name></person-group> (<year>2010</year>). <article-title>“Multitask learning for brain-computer interfaces,”</article-title> in <source>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</source> (<publisher-loc>Sardinia</publisher-loc>: <publisher-name>JMLR Workshop and Conference Proceedings</publisher-name>), <fpage>17</fpage>–<lpage>24</lpage>.</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arvaneh</surname><given-names>M.</given-names></name><name><surname>Guan</surname><given-names>C.</given-names></name><name><surname>Ang</surname><given-names>K. K.</given-names></name><name><surname>Quek</surname><given-names>C.</given-names></name></person-group> (<year>2013</year>). <article-title>Eeg data space adaptation to reduce intersession nonstationarity in brain-computer interface</article-title>. <source>Neural Comput</source>. <volume>25</volume>, <fpage>2146</fpage>–<lpage>2171</lpage>. <pub-id pub-id-type="doi">10.1162/NECO_a_00474</pub-id><?supplied-pmid 23663147?><pub-id pub-id-type="pmid">23663147</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barachant</surname><given-names>A.</given-names></name><name><surname>Bonnet</surname><given-names>S.</given-names></name><name><surname>Congedo</surname><given-names>M.</given-names></name><name><surname>Jutten</surname><given-names>C.</given-names></name></person-group> (<year>2013</year>). <article-title>Classification of covariance matrices using a riemannian-based kernel for bci applications</article-title>. <source>Neurocomputing</source>
<volume>112</volume>, <fpage>172</fpage>–<lpage>178</lpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2012.12.039</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fahimi</surname><given-names>F.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Goh</surname><given-names>W. B.</given-names></name><name><surname>Lee</surname><given-names>T.-S.</given-names></name><name><surname>Ang</surname><given-names>K. K.</given-names></name><name><surname>Guan</surname><given-names>C.</given-names></name></person-group> (<year>2019</year>). <article-title>Inter-subject transfer learning with an end-to-end deep convolutional neural network for eeg-based bci</article-title>. <source>J. Neural Eng</source>. 16, 026007. <pub-id pub-id-type="doi">10.1088/1741-2552/aaf3f6</pub-id><?supplied-pmid 30524056?></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fatourechi</surname><given-names>M.</given-names></name><name><surname>Ward</surname><given-names>R. K.</given-names></name><name><surname>Mason</surname><given-names>S. G.</given-names></name><name><surname>Huggins</surname><given-names>J.</given-names></name><name><surname>Schloegl</surname><given-names>A.</given-names></name><name><surname>Birch</surname><given-names>G. E.</given-names></name></person-group> (<year>2008</year>). <article-title>“Comparison of evaluation metrics in classification applications with imbalanced datasets,”</article-title> in <source>2008 Seventh International Conference on Machine Learning and Applications</source> (<publisher-loc>San Diego, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>). <fpage>777</fpage>–<lpage>782</lpage>.</mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>C.</given-names></name><name><surname>Abbeel</surname><given-names>P.</given-names></name><name><surname>Levine</surname><given-names>S.</given-names></name></person-group> (<year>2017</year>). <article-title>“Model-agnostic meta-learning for fast adaptation of deep networks,”</article-title> in <source>International Conference on Machine Learning</source> (<publisher-loc>Sydney, NSW</publisher-loc>: <publisher-name>PMLR</publisher-name>), <fpage>1126</fpage>–<lpage>1135</lpage>.<?supplied-pmid 35653901?></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Göhring</surname><given-names>D.</given-names></name><name><surname>Latotzky</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>M.</given-names></name><name><surname>Rojas</surname><given-names>R.</given-names></name></person-group> (<year>2013</year>). <article-title>Semi-autonomous car control using brain computer interfaces</article-title>. <source>Intell. Auton. Syst</source>. <volume>12</volume>, <fpage>393</fpage>–<lpage>408</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-642-33932-5_37</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hajinoroozi</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>J. M.</given-names></name><name><surname>Huang</surname><given-names>Y.</given-names></name></person-group> (<year>2017</year>). <article-title>“Driver's fatigue prediction by deep covariance learning from eeg,”</article-title> in <source>2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</source> (<publisher-loc>Banff, AB</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>240</fpage>–<lpage>245</lpage>.</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>H.</given-names></name><name><surname>Wu</surname><given-names>D.</given-names></name></person-group> (<year>2019</year>). <article-title>Transfer learning for brain-computer interfaces: a euclidean space data alignment approach</article-title>. <source>IEEE Trans. Biomed. Eng</source>. <volume>67</volume>, <fpage>399</fpage>–<lpage>410</lpage>. <pub-id pub-id-type="doi">10.1109/TBME.2019.2913914</pub-id><?supplied-pmid 31034407?><pub-id pub-id-type="pmid">31034407</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>“Delving deep into rectifiers: surpassing human-level performance on imagenet classification,”</article-title> in <source>Proceedings of the IEEE International Conference on Computer Vision</source> (<publisher-loc>Santiago</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1026</fpage>–<lpage>1034</lpage>.</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>K.-S.</given-names></name><name><surname>Naseer</surname><given-names>N.</given-names></name><name><surname>Kim</surname><given-names>Y.-H.</given-names></name></person-group> (<year>2015</year>). <article-title>Classification of prefrontal and motor cortex signals for three-class fnirs-bci</article-title>. <source>Neurosci. Lett</source>. <volume>587</volume>, <fpage>87</fpage>–<lpage>92</lpage>. <pub-id pub-id-type="doi">10.1016/j.neulet.2014.12.029</pub-id><?supplied-pmid 25529197?><pub-id pub-id-type="pmid">25529197</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Z.</given-names></name><name><surname>Van Gool</surname><given-names>L.</given-names></name></person-group> (<year>2017</year>). <article-title>“A riemannian network for spd matrix learning,”</article-title> in <source>Thirty-First AAAI Conference on Artificial Intelligence</source> San Francisco, FL.</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jayaram</surname><given-names>V.</given-names></name><name><surname>Alamgir</surname><given-names>M.</given-names></name><name><surname>Altun</surname><given-names>Y.</given-names></name><name><surname>Scholkopf</surname><given-names>B.</given-names></name><name><surname>Grosse-Wentrup</surname><given-names>M.</given-names></name></person-group> (<year>2016</year>). <article-title>Transfer learning in brain-computer interfaces</article-title>. <source>IEEE Comput. Intell. Mag</source>. <volume>11</volume>, <fpage>20</fpage>–<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1109/MCI.2015.2501545</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jayaram</surname><given-names>V.</given-names></name><name><surname>Barachant</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>). <article-title>Moabb: trustworthy algorithm benchmarking for bcis</article-title>. <source>J. Neural Eng</source>. 15, 066011. <pub-id pub-id-type="doi">10.1088/1741-2552/aadea0</pub-id><?supplied-pmid 30177583?></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>D. P.</given-names></name><name><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Adam: a method for stochastic optimization</article-title>. <source>arXiv [Preprint] arXiv:</source>1412.6980. <pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kothe</surname><given-names>C. A.</given-names></name><name><surname>Makeig</surname><given-names>S.</given-names></name></person-group> (<year>2013</year>). <article-title>Bcilab: a platform for brain-computer interface development</article-title>. <source>J. Neural Eng</source>. 10, 056014. <pub-id pub-id-type="doi">10.1088/1741-2560/10/5/056014</pub-id><?supplied-pmid 23985960?></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A.</given-names></name><name><surname>Sutskever</surname><given-names>I.</given-names></name><name><surname>Hinton</surname><given-names>G. E.</given-names></name></person-group> (<year>2012</year>). <article-title>“Imagenet classification with deep convolutional neural networks.,”</article-title> in <source>Advances in Neural Information Processing Systems 25 (NIPS 2012)</source> Lake Tahoe.</mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lawhern</surname><given-names>V. J.</given-names></name><name><surname>Solon</surname><given-names>A. J.</given-names></name><name><surname>Waytowich</surname><given-names>N. R.</given-names></name><name><surname>Gordon</surname><given-names>S. M.</given-names></name><name><surname>Hung</surname><given-names>C. P.</given-names></name><name><surname>Lance</surname><given-names>B. J.</given-names></name></person-group> (<year>2018</year>). <article-title>Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces</article-title>. <source>J. Neural Eng</source>. 15, 056013. <pub-id pub-id-type="doi">10.1088/1741-2552/aace8c</pub-id><?supplied-pmid 29932424?></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Haffner</surname><given-names>P.</given-names></name></person-group> (<year>1998</year>). <article-title>Gradient-based learning applied to document recognition</article-title>. <source>Proc. IEEE</source>
<volume>86</volume>, <fpage>2278</fpage>–<lpage>2324</lpage>. <pub-id pub-id-type="doi">10.1109/5.726791</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>D.</given-names></name><name><surname>Ortega</surname><given-names>P.</given-names></name><name><surname>Wei</surname><given-names>X.</given-names></name><name><surname>Faisal</surname><given-names>A.</given-names></name></person-group> (<year>2021</year>). <article-title>“Model-agnostic meta-learning for eeg motor imagery decoding in brain-computer-interfacing,”</article-title> in <source>2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)</source> (<publisher-loc>IEEE</publisher-loc>), <fpage>527</fpage>–<lpage>530</lpage>.</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lopez-Paz</surname><given-names>D.</given-names></name><name><surname>Ranzato</surname><given-names>M.</given-names></name></person-group> (<year>2017</year>). <article-title>“Gradient episodic memory for continual learning,”</article-title> in <source>Advances in Neural Information Processing Systems 30 (NIPS 2017)</source>
<publisher-loc>Long Beach, CA</publisher-loc>: <publisher-name>CAES</publisher-name>.<?supplied-pmid 34564106?></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lotte</surname><given-names>F.</given-names></name><name><surname>Bougrain</surname><given-names>L.</given-names></name><name><surname>Cichocki</surname><given-names>A.</given-names></name><name><surname>Clerc</surname><given-names>M.</given-names></name><name><surname>Congedo</surname><given-names>M.</given-names></name><name><surname>Rakotomamonjy</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>A review of classification algorithms for eeg-based brain-computer interfaces: a 10 year update</article-title>. <source>J. Neural Eng</source>. 15, 031005. <pub-id pub-id-type="doi">10.1088/1741-2552/aab2f2</pub-id><?supplied-pmid 29488902?></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lotte</surname><given-names>F.</given-names></name><name><surname>Guan</surname><given-names>C.</given-names></name></person-group> (<year>2010</year>). <article-title>“Learning from other subjects helps reducing brain-computer interface calibration time,”</article-title> in <source>2010 IEEE International Conference on Acoustics, Speech and Signal Processing</source> (<publisher-loc>Dallas, TX</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>614</fpage>–<lpage>617</lpage>.</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lotze</surname><given-names>M.</given-names></name><name><surname>Halsband</surname><given-names>U.</given-names></name></person-group> (<year>2006</year>). <article-title>Motor imagery</article-title>. <source>J. Physiol. Paris</source>
<volume>99</volume>, <fpage>386</fpage>–<lpage>395</lpage>. <pub-id pub-id-type="doi">10.1016/j.jphysparis.2006.03.012</pub-id><?supplied-pmid 16716573?><pub-id pub-id-type="pmid">16716573</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naseer</surname><given-names>N.</given-names></name><name><surname>Hong</surname><given-names>K.-S.</given-names></name></person-group> (<year>2013</year>). <article-title>Classification of functional near-infrared spectroscopy signals corresponding to the right-and left-wrist motor imagery for development of a brain-computer interface</article-title>. <source>Neurosci. Lett</source>. <volume>553</volume>, <fpage>84</fpage>–<lpage>89</lpage>. <pub-id pub-id-type="doi">10.1016/j.neulet.2013.08.021</pub-id><?supplied-pmid 23973334?><pub-id pub-id-type="pmid">23973334</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naseer</surname><given-names>N.</given-names></name><name><surname>Hong</surname><given-names>K.-S.</given-names></name></person-group> (<year>2015</year>). <article-title>Decoding answers to four-choice questions using functional near infrared spectroscopy</article-title>. <source>J. Near Infrared Spectrosc</source>. <volume>23</volume>, <fpage>23</fpage>–<lpage>31</lpage>. <pub-id pub-id-type="doi">10.1255/jnirs.1145</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nijholt</surname><given-names>A.</given-names></name><name><surname>Tan</surname><given-names>D.</given-names></name><name><surname>Allison</surname><given-names>B.</given-names></name><name><surname>del</surname><given-names>R.</given-names></name><name><surname>Milan</surname><given-names>J.</given-names></name><name><surname>Graimann</surname><given-names>B.</given-names></name></person-group> (<year>2008</year>). <article-title>“Brain-computer interfaces for hci and games,”</article-title> in <source>CHI'08 Extended Abstracts on Human Factors in Computing Systems</source> Florence (3925–3928).</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A.</given-names></name><name><surname>Gross</surname><given-names>S.</given-names></name><name><surname>Massa</surname><given-names>F.</given-names></name><name><surname>Lerer</surname><given-names>A.</given-names></name><name><surname>Bradbury</surname><given-names>J.</given-names></name><name><surname>Chanan</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>“Pytorch: an imperative style, high-performance deep learning library,”</article-title> in <source>Advances in Neural Information Processing Systems 32</source> (<publisher-loc>Moscow</publisher-loc>: <publisher-name>Curran Associates, Inc.</publisher-name>), <fpage>8024</fpage>–<lpage>8035</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>R. P..</given-names></name></person-group> (<year>2013</year>). <source>Brain-Computer Interfacing: An Introduction</source>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Reuderink</surname><given-names>B.</given-names></name><name><surname>Farquhar</surname><given-names>J.</given-names></name><name><surname>Poel</surname><given-names>M.</given-names></name><name><surname>Nijholt</surname><given-names>A.</given-names></name></person-group> (<year>2011</year>). <article-title>“A subject-independent brain-computer interface based on smoothed, second-order baselining,”</article-title> in <source>2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society</source> (<publisher-loc>Boston, MA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>4600</fpage>–<lpage>4604</lpage>.<?supplied-pmid 22255362?></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodrigues</surname><given-names>P. L. C.</given-names></name><name><surname>Jutten</surname><given-names>C.</given-names></name><name><surname>Congedo</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>). <article-title>Riemannian procrustes analysis: transfer learning for brain-computer interfaces</article-title>. <source>IEEE Trans. Biomed. Eng</source>. <volume>66</volume>, <fpage>2390</fpage>–<lpage>2401</lpage>. <pub-id pub-id-type="doi">10.1109/TBME.2018.2889705</pub-id><?supplied-pmid 30596565?><pub-id pub-id-type="pmid">30596565</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scherer</surname><given-names>R.</given-names></name><name><surname>Faller</surname><given-names>J.</given-names></name><name><surname>Friedrich</surname><given-names>E. V.</given-names></name><name><surname>Opisso</surname><given-names>E.</given-names></name><name><surname>Costa</surname><given-names>U.</given-names></name><name><surname>Kübler</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2015</year>). <article-title>Individually adapted imagery improves brain-computer interface performance in end-users with disability</article-title>. <source>PLoS ONE</source><volume>10</volume>, <fpage>e0123727</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pone.0123727</pub-id><?supplied-pmid 25992718?><pub-id pub-id-type="pmid">25992718</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schirrmeister</surname><given-names>R. T.</given-names></name><name><surname>Springenberg</surname><given-names>J. T.</given-names></name><name><surname>Fiederer</surname><given-names>L. D. J.</given-names></name><name><surname>Glasstetter</surname><given-names>M.</given-names></name><name><surname>Eggensperger</surname><given-names>K.</given-names></name><name><surname>Tangermann</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Deep learning with convolutional neural networks for eeg decoding and visualization</article-title>. <source>Hum. Brain Mapp</source>. <volume>38</volume>, <fpage>5391</fpage>–<lpage>5420</lpage>. <pub-id pub-id-type="doi">10.1002/hbm.23730</pub-id><?supplied-pmid 28782865?><pub-id pub-id-type="pmid">28782865</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shanker</surname><given-names>M.</given-names></name><name><surname>Hu</surname><given-names>M. Y.</given-names></name><name><surname>Hung</surname><given-names>M. S.</given-names></name></person-group> (<year>1996</year>). <article-title>Effect of data standardization on neural network training</article-title>. <source>Omega</source>
<volume>24</volume>, <fpage>385</fpage>–<lpage>397</lpage>. <pub-id pub-id-type="doi">10.1016/0305-0483(96)00010-2</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Q.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Chua</surname><given-names>T.-S.</given-names></name><name><surname>Schiele</surname><given-names>B.</given-names></name></person-group> (<year>2019</year>). <article-title>“Meta-transfer learning for few-shot learning,”</article-title> in <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Long Beach, CA</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>403</fpage>–<lpage>412</lpage>.</mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tangermann</surname><given-names>M.</given-names></name><name><surname>Müller</surname><given-names>K.-R.</given-names></name><name><surname>Aertsen</surname><given-names>A.</given-names></name><name><surname>Birbaumer</surname><given-names>N.</given-names></name><name><surname>Braun</surname><given-names>C.</given-names></name><name><surname>Brunner</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2012</year>). <article-title>Review of the bci competition iv</article-title>. <source>Front. Neurosci</source>. 6, 55. <pub-id pub-id-type="doi">10.3389/fnins.2012.00055</pub-id><?supplied-pmid 22811657?></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tariq</surname><given-names>M.</given-names></name><name><surname>Trivailo</surname><given-names>P. M.</given-names></name><name><surname>Simic</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>). <article-title>Eeg-based bci control schemes for lower-limb assistive-robots</article-title>. <source>Front. Hum. Neurosci</source>. 12, 312. <pub-id pub-id-type="doi">10.3389/fnhum.2018.00312</pub-id><?supplied-pmid 30127730?></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vanschoren</surname><given-names>J..</given-names></name></person-group> (<year>2018</year>). <article-title>Meta-learning: a survey</article-title>. <source>arXiv preprint arXiv:1810.03548</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1810.03548</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>D.</given-names></name><name><surname>Xu</surname><given-names>Y.</given-names></name><name><surname>Lu</surname><given-names>B. L.</given-names></name></person-group> (<year>2020</year>). <article-title>Transfer learning for EEG-based brain-computer interfaces: A review of progress made since 2016</article-title>. <source>IEEE Trans. Cogn. Develop. Syst</source>. 14, 4-19. Available online at: <ext-link xlink:href="https://arxiv.org/pdf/2004.06286.pdf" ext-link-type="uri">https://arxiv.org/pdf/2004.06286.pdf</ext-link></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>M.</given-names></name><name><surname>He</surname><given-names>F.</given-names></name><name><surname>Jung</surname><given-names>T.-P.</given-names></name><name><surname>Gu</surname><given-names>X.</given-names></name><name><surname>Ming</surname><given-names>D.</given-names></name></person-group> (<year>2021</year>). <article-title>Current challenges for the practical application of electroencephalography-based brain-computer interfaces</article-title>. <source>Engineering</source>
<volume>7</volume>, <fpage>1710</fpage>–<lpage>1712</lpage>. <pub-id pub-id-type="doi">10.1016/j.eng.2021.09.011</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>P.</given-names></name><name><surname>Ma</surname><given-names>X.</given-names></name><name><surname>Chen</surname><given-names>L.</given-names></name><name><surname>Zhou</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Decoder calibration with ultra small current sample set for intracortical brain-machine interface</article-title>. <source>J. Neural Eng</source>. 15, 026019. <pub-id pub-id-type="doi">10.1088/1741-2552/aaa8a4</pub-id><?supplied-pmid 29343650?></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Sabuncu</surname><given-names>M.</given-names></name></person-group> (<year>2018</year>). <article-title>“Generalized cross entropy loss for training deep neural networks with noisy labels,”</article-title> in <source>NIPS'18: Proceedings of the 32nd International Conference on Neural Information Processing Systems</source> Montréal, QC.</mixed-citation>
    </ref>
  </ref-list>
</back>
