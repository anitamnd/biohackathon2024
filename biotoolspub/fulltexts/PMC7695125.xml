<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7695125</article-id>
    <article-id pub-id-type="pmid">32618995</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btaa591</article-id>
    <article-id pub-id-type="publisher-id">btaa591</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Genome Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Epiviz File Server: Query, transform and interactively explore data from indexed genomic files</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-5855-5031</contrib-id>
        <name>
          <surname>Kancherla</surname>
          <given-names>Jayaram</given-names>
        </name>
        <aff>
          <institution>Center for Bioinformatics and Computational Biology</institution>
        </aff>
        <aff>
          <institution>Institute for Advanced Computer Studies</institution>
        </aff>
        <aff><institution>Department of Computer Science, University of Maryland</institution>, College Park, MD 20742, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yang</surname>
          <given-names>Yifan</given-names>
        </name>
        <aff><institution>Department of Computer Science, University of Maryland</institution>, College Park, MD 20742, USA</aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chae</surname>
          <given-names>Hyeyun</given-names>
        </name>
        <aff><institution>Biology and Computer Science, Swarthmore College</institution>, Swarthmore, PA 19081, USA</aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Corrada Bravo</surname>
          <given-names>Hector</given-names>
        </name>
        <xref rid="btaa591-cor1" ref-type="corresp"/>
        <aff>
          <institution>Center for Bioinformatics and Computational Biology</institution>
        </aff>
        <aff>
          <institution>Institute for Advanced Computer Studies</institution>
        </aff>
        <aff><institution>Department of Computer Science, University of Maryland</institution>, College Park, MD 20742, USA</aff>
        <!--hcorrada@umd.edu-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Valencia</surname>
          <given-names>Alfonso</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btaa591-cor1">To whom correspondence should be addressed. E-mail: <email>hcorrada@umd.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <day>15</day>
      <month>9</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2020-07-03">
      <day>03</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>03</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <volume>36</volume>
    <issue>18</issue>
    <fpage>4682</fpage>
    <lpage>4690</lpage>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>12</month>
        <year>2019</year>
      </date>
      <date date-type="rev-recd">
        <day>04</day>
        <month>5</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>26</day>
        <month>6</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2020</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btaa591.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Genomic data repositories like The Cancer Genome Atlas, Encyclopedia of DNA Elements, Bioconductor’s <italic toggle="yes">AnnotationHub</italic> and <italic toggle="yes">ExperimentHub</italic> etc., provide public access to large amounts of genomic data as flat files. Researchers often download a subset of data files from these repositories to perform exploratory data analysis. We developed Epiviz File Server, a Python library that implements an <italic toggle="yes">in situ</italic> data query system for local or remotely hosted indexed genomic files, not only for visualization but also data transformation. The File Server library decouples data retrieval and transformation from specific visualization and analysis tools and provides an abstract interface to define computations independent of the location, format or structure of the file. We demonstrate the File Server in two use cases: (i) integration with Galaxy workflows and (ii) using Epiviz to create a custom genome browser from the Epigenome Roadmap dataset.</p>
      </sec>
      <sec id="s2">
        <title>Availability and implementation</title>
        <p>Epiviz File Server is open source and is available on GitHub at <ext-link xlink:href="http://github.com/epiviz/epivizFileParser" ext-link-type="uri">http://github.com/epiviz/epivizFileServer</ext-link>. The documentation for the File Server library is available at <ext-link xlink:href="http://epivizfileparser.rtfd.io" ext-link-type="uri">http://epivizfileserver.rtfd.io</ext-link>.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>U.S. National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R01GM114267</award-id>
        <award-id>R24MH114815</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Genomic data repositories like The Cancer Genome Atlas (The Cancer Genome Atlas Research Network <italic toggle="yes">et al.</italic>, 2013), Encyclopedia of DNA Elements (ENCODE; <xref rid="btaa591-B8" ref-type="bibr">Davis <italic toggle="yes">et al.</italic>, 2018</xref>), Bioconductor’s (<xref rid="btaa591-B13" ref-type="bibr">Huber <italic toggle="yes">et al.</italic>, 2015</xref>) AnnotationHub (<xref rid="btaa591-B20" ref-type="bibr">Morgan <italic toggle="yes">et al.</italic>, 2019a</xref>) and ExperimentHub <xref rid="btaa591-B21" ref-type="bibr"> (Morgan, 2019b) </xref>etc., provide public access to large amounts of genomic data as flat files. Researchers often download a subset of data files from these repositories to perform their exploratory data analysis. Increasing data size requires longer time to download, pre-process and load files into a database to run queries efficiently. This time-consuming process is an impediment to interactive analysis of genomic data.</p>
    <p>Interactive visualization of data can be a powerful tool to enable exploratory analysis. As users get familiar with the data and gain insights, it would be even more efficient to interactively hypothesize, validate, visualize and compute the intermediate results of the analysis. Currently available interactive visualization tools for genomic data, namely genome browsers, fall into two broad categories. One that uses a database management system to load genomic data from files into tables, create indices or partitions for faster query of data by genomic intervals. The other category of genome browsers query data directly from indexed genomic file formats like BigBed, BigWig (<xref rid="btaa591-B16" ref-type="bibr">Kent <italic toggle="yes">et al.</italic>, 2010</xref>) or Tabix (<xref rid="btaa591-B18" ref-type="bibr">Li, 2011</xref>). However these tools are limited only to exploration of data from files.</p>
    <p>NoDB is a new database design philosophy to make database systems more accessible and reduce the data-to-query time (<xref rid="btaa591-B2" ref-type="bibr">Alagiannis <italic toggle="yes">et al.</italic>, 2012</xref>) by directly querying and transforming the raw data files instead of loading data from files into different storage, e.g. database tables. Based on these concepts, we developed Epiviz File Server (EFS), a Python library that implements an <italic toggle="yes">in situ</italic> data query and transformation system for local or remotely hosted indexed genomic files.</p>
    <sec>
      <title>1.1 Contributions</title>
      <p>Our design of the EFS library was based on the following goals:
</p>
      <list list-type="order">
        <list-item>
          <p>Efficiently parse minimal necessary bytes from an indexed genomic file to query data for a specific genomic region.</p>
        </list-item>
        <list-item>
          <p>Define transformations and summarizations directly over files and lazily compute these at query time.</p>
        </list-item>
        <list-item>
          <p>Scale operations to concurrently process multiple file query and transformation requests.</p>
        </list-item>
        <list-item>
          <p>Implement cache over files for faster access and improve repeat query performance</p>
        </list-item>
        <list-item>
          <p>Provide REST API (Representational state transfer Application Programming Interface) for developers and bioinformaticians to build interactive visualization and exploratory tools over genomic data stored in flat files.</p>
        </list-item>
        <list-item>
          <p>Integration with existing bioinformatic tools and software to interactively visualize and explore genomic data directly from files.</p>
        </list-item>
      </list>
      <p>EFS decouples data retrieval and transformation from specific visualization and analysis tools and provides an abstract interface to define computations independent of the location, format or structure of the file. Our major contribution on this research project was to efficiently and intuitively define transformations and summarizations directly over files, without the hassle of downloading the files locally or pre-process for exploratory data analysis. Using the library, researchers and analysts can author shareable and reproducible data exploration workflows in an intuitive and programmatic way. EFS can query and explore data directly from local and publicly hosted indexed genomic files. If the files are hosted on a public server, the library requires the server hosting the data files to support HTTP range requests (<ext-link xlink:href="https://tools.ietf.org/html/rfc7233" ext-link-type="uri">https://tools.ietf.org/html/rfc7233</ext-link>), so that the parser can only request the minimum necessary byte ranges needed to process the query. The library supports various file formats - BigBed, BigWig and any tabular file that can be indexed using Tabix. Once these data files are described, users can define summarizations and transformations on these files using NumPy (or NumPy-like) functions (<xref rid="btaa591-B33" ref-type="bibr">van der Walt <italic toggle="yes">et al.</italic>, 2011</xref>).</p>
      <p>EFS uses Dask (<xref rid="btaa591-B26" ref-type="bibr">Rocklin, 2015</xref>; <ext-link xlink:href="https://dask.org" ext-link-type="uri">https://dask.org</ext-link>) to scale and concurrently process multiple query and transformations over files. The cache implementation makes sure we only access bytes not already accessed and stored locally. Developers of bioinformatic tools and systems can use the library’s REST API to build interactive data visualization or exploratory tools over files. We demonstrate the integration in the use cases section of this paper by integration with Galaxy (<xref rid="btaa591-B1" ref-type="bibr">Afgan <italic toggle="yes">et al.</italic>, 2018</xref>), a widely used open source bioinformatic Web platform for analysis of genomic data and with the Epiviz browser (<xref rid="btaa591-B7" ref-type="bibr">Chelaru <italic toggle="yes">et al.</italic>, 2014</xref>), an interactive and integrative functional genome browser to visualize and explore these datasets. The browser supports various charts to explore genomic data, heatmap and scatter plots to visualize gene expression, block (linear and stacked) tracks for visualizing genomic regions of interest and line tracks (stacked, multi stacked) for visualizing signal (ChIP-seq, methylation etc.) data. Hovering over a region in one visualization highlights this region in other tracks providing instant visual feedback to the user. These visualizations are developed using standards-based Web component framework, are highly customizable, reusable and can be integrated with most frameworks that support HTML (<xref rid="btaa591-B14" ref-type="bibr">Kancherla <italic toggle="yes">et al.</italic>, 2018</xref>). <xref rid="btaa591-F1" ref-type="fig">Figure 1</xref> describes a high-level overview of these components in the EFS library.
</p>
      <fig position="float" id="btaa591-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>A high-level overview of the EFS Library. EFS library supports directly querying indexed genomic files. Data files are described in the measurements module and provides a programmatic interface to parse, query and define transformations over files using any NumPy(-like) function. Transformations are lazily computed at query time using Dask and the cache layer makes sure we only request for bytes not already accessed. Datasets and their transformations can be accessed using a REST API and allows developers to build interactive visualization and exploration tools</p>
        </caption>
        <graphic xlink:href="btaa591f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>1.2 Related work</title>
      <p>Several existing genome browsers and tools support visualization of genomic data from flat files. These include the UCSC Genome Browser (<xref rid="btaa591-B15" ref-type="bibr">Kent <italic toggle="yes">et al.</italic> 2002</xref>), Dalliance (<xref rid="btaa591-B9" ref-type="bibr">Down <italic toggle="yes">et al.</italic>, 2011</xref>), JBrowse (<xref rid="btaa591-B6" ref-type="bibr">Buels <italic toggle="yes">et al.</italic>, 2016</xref>), Integrated Genome Browser (<xref rid="btaa591-B10" ref-type="bibr">Freese <italic toggle="yes">et al.</italic>, 2016</xref>). However, these tools only visualize genomic data from files and do not perform transformations over data. Integrated Genome Viewer (<xref rid="btaa591-B25" ref-type="bibr">Robinson <italic toggle="yes">et al.</italic>, 2011</xref>) allows users to combine tracks through its interface and perform Add, Subtract, Multiply or Divide operations but they are tightly coupled to the visualization interface and not provided as a general-purpose library. RawVis (Bikakis <italic toggle="yes">et al.</italic>, 2018) converts user interaction queries into data access queries and dynamically builds an VALINOR (Visual Analysis Index on Raw data) index over the data attributes used in a specific visualization thus tightly coupling visualization and data workflows and is not specifically designed for genomic data files. HiGlass (<xref rid="btaa591-B17" ref-type="bibr">Kerpedjiev <italic toggle="yes">et al.</italic>, 2018</xref>) is a visual genomic data exploration tool that supports several genomic file formats without re-indexing data and using existing zoom levels. HiGlass supports client-based divide by operations but other transformations are currently not supported by the system. WebWorkers can be accessed through JavaScript to asynchronously run computationally intensive tasks while keeping the UI in the browser responsive. However, limitations of the WebWorker thread model, namely that data are not shared across workers, applying transformations in the browser over large data may not be efficient. Computing transformations on the server allows multiple clients to access these results without having to individually compute them locally. EFS decouples data retrieval from visualization workflows, providing an interface to define transformations and a REST API to access them. <xref rid="btaa591-T1" ref-type="table">Table 1</xref> provides an overview of these features across related tools.
</p>
      <table-wrap position="float" id="btaa591-T1">
        <label>Table 1.</label>
        <caption>
          <p>Comparison of features across <italic toggle="yes">in situ</italic> file querying and transformation tools</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Features</th>
              <th rowspan="1" colspan="1">IGV</th>
              <th rowspan="1" colspan="1">RawVis</th>
              <th rowspan="1" colspan="1">HiGlass</th>
              <th rowspan="1" colspan="1">UCSC</th>
              <th rowspan="1" colspan="1">EFS</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>Transformation across files</bold>
              </td>
              <td rowspan="1" colspan="1">Supports add, subtract, multiply and divide by operations.</td>
              <td rowspan="1" colspan="1">Limited—computes data aggregations at query time for each dimension in a visualization.</td>
              <td rowspan="1" colspan="1">Supports client-side divide by operations.</td>
              <td rowspan="1" colspan="1">Does not support transformations across files.</td>
              <td rowspan="1" colspan="1">Provides an interface to apply transformations across files using NumPy-like definition.</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>Modularity</bold>
              </td>
              <td rowspan="1" colspan="1">Visualization and data workflows are coupled together.</td>
              <td rowspan="1" colspan="1">Converts user interaction queries into data queries hence tightly couples visualization with data workflows.</td>
              <td rowspan="1" colspan="1">Separates the user interface and the server.</td>
              <td rowspan="1" colspan="1">Visualization and data workflows are coupled together in the browser.</td>
              <td rowspan="1" colspan="1">Decouples data from visualization workflows, allows tools developers to develop tools using the REST API.</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><bold>Native</bold> <bold>support</bold> <bold>genomic</bold> f<bold>ile</bold> <bold>formats</bold> (BigWig, BigBed, Tabix etc.)</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
              <td rowspan="1" colspan="1">Yes</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>REST API to query files</bold>
              </td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">No</td>
              <td rowspan="1" colspan="1">HiGlass-server provides an API to get tileset information and query by tiles.</td>
              <td rowspan="1" colspan="1">REST API supports querying files from a TrackHub.</td>
              <td rowspan="1" colspan="1">REST API supports querying both files and the transformations.</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 <italic toggle="yes">Traditional</italic> approach</title>
      <p>A naive approach to implement a query system over genomic files would be to first import the files into a database management system like MySQL. Accessing data from a database provides various advantages—we can query the data quickly by indexing on intervals (genomic position), we can use a standard declarative query language like SQL, and are able to modify the table schema on-the-fly. As the size of the data increases, there is a significant cost to initialize, load and prepare the dataset for queries. Our experience importing a histone modification data file from a ChIP-seq experiment into a database significantly increased the data-to-query time i.e. time to import the file into a table, time to index the table and time to query the table for a particular genomic region. Even after this, the database table had to be custom configured to use advanced indexing schemes like partitions and index by partition to improve the query performance. This is not a feasible approach to individually tune and optimize query performance when dealing with large public repositories like the NIH roadmap epigenomics (<xref rid="btaa591-B3" ref-type="bibr">Bernstein <italic toggle="yes">et al.</italic> 2010</xref>) or ENCODE that host datasets from hundreds of experiments.</p>
      <p>Assuming we do have all the data files imported into a database system, performing transformations on data directly is limited to the functions implemented in the system. Also to perform complex summarization or transformations across genomic datasets, we would need to first perform interval overlap operations that are limited or non-existent in database systems. To work around these limitations, one would implement backend programing logic (middleware) to first query the database for multiple datasets, perform interval overlap operations to align the data by genomic location, summarize and then apply transformations.</p>
    </sec>
    <sec>
      <title>2.2 EFS approach</title>
      <p>In the <italic toggle="yes">traditional</italic> approach, we expect the data-to-query time to significantly increase as the size of the data increases. The NoDB philosophy is a way of querying the data files directly, <italic toggle="yes">in situ</italic>, hence removing the loading time. A simple approach would be to load the data file into memory every time we need to query for a particular region. This is not efficient for (i) repeated query processing since we would load the same file into memory on every request and (ii) memory usage especially when querying several large data files at the same time. In order to work around the issue of loading the entire file into memory, we need to create and store an index based on genomic positions, query this index and only access the minimum necessary bytes of the file to process the query.</p>
      <p>The genomics community has long used specialized binary file formats like BigBed, BigWig or Tabix for quickly accessing particular regions of the genome. These file formats contain an index to quickly and efficiently access blocks of the file that contain the data, then only parse these blocks to process the query. These formats also support remote file access, allowing multiple parallel requests to process at the same time. Our NoDB approach uses these indexed genomic file formats as the base for <italic toggle="yes">in situ</italic> query processing. Some of these file formats also summarize the data at different zoom levels (base-pair resolution) which is extremely useful especially for interactive exploration and visualization. A comparison of features between our approach and a traditional database server are listed in <xref rid="btaa591-T2" ref-type="table">Table 2</xref>.
</p>
      <table-wrap position="float" id="btaa591-T2">
        <label>Table 2.</label>
        <caption>
          <p>Comparison of features between a traditional database server and EFS</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Feature</th>
              <th rowspan="1" colspan="1">Traditional DB (MySQL)</th>
              <th rowspan="1" colspan="1">EFS</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Time to query</td>
              <td rowspan="1" colspan="1">Time to query is longer since the data needs to be imported into the database. Often requires custom indexing to improve query time.</td>
              <td rowspan="1" colspan="1">EFS performs <italic toggle="yes">in situ</italic> query operations over indexed file formats.</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Cache</td>
              <td rowspan="1" colspan="1">Provides cache support for repeated query processing.</td>
              <td rowspan="1" colspan="1">Implements cache for repeated query processing.</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SQL</td>
              <td rowspan="1" colspan="1">Provides an easy to use query language to search tables.</td>
              <td rowspan="1" colspan="1">Does not provide a query language (but provides REST API).</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Schema</td>
              <td rowspan="1" colspan="1">Schema can be changed on the fly.</td>
              <td rowspan="1" colspan="1">Changes in schema result in regenerating the index file.</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Interval Overlap Operations</td>
              <td rowspan="1" colspan="1">Does not provide interval overlap operations to apply transformations across datasets.</td>
              <td rowspan="1" colspan="1">EFS supports overlap operations to apply transformations across files.</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Transformations</td>
              <td rowspan="1" colspan="1">SQL supports basic mathematical operations (average, min, max etc.) and often needs middleware to support any other transformations over query results.</td>
              <td rowspan="1" colspan="1">EFS supports any transformation over the query results.</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>In addition to querying data from files, EFS provides an interface to compute transformations over data directly from files and is independent of the visualization interface. The library decouples data query and analysis from visualization workflows. This way new transformations can be applied over files and can be queried dynamically.</p>
    </sec>
    <sec>
      <title>2.3 EFS library design and implementation</title>
      <p>In this section, we specify the design and implementation of the EFS NoDB approach to querying and operating over genomic files.</p>
      <sec>
        <label>2.3.1</label>
        <title>Parsing genomic file formats</title>
        <p>EFS supports indexed genomic file formats—BigWig, BigBed, bam (with bai), sam (with sai) or any tabular data file that can be indexed using Tabix. These files either have an index as part of the same file (BigBed, BigWig etc.) or create a separate index file (Tabix). The index (usually a BTree or an RTree) allows the library to quickly navigate (or seek) and only access the blocks of the file that contains the data for a genomic region. These formats also support remote file access, allowing multiple parallel requests to process at the same time. We implemented data retrieval on these file formats in the <italic toggle="yes">parser</italic> module of the library. The library supports both remote and local file access and random-access queries. Developers can extend the base classes in this module to add parsers to other bioinformatic file formats. Results from the files are converted into a Pandas <italic toggle="yes">DataFrame</italic> (<xref rid="btaa591-B19" ref-type="bibr">McKinney, 2010</xref>) using <italic toggle="yes">IntervalIndex</italic> to index intervals. This allows us to apply transformations, summarizations and other operations across files that contain different intervals.</p>
      </sec>
      <sec>
        <label>2.3.2</label>
        <title>Defining and managing files</title>
        <p>When accessing files from genomic data repositories that contain hundreds of files from various experiments, it is inefficient to individually describe these files. The <italic toggle="yes">measurements</italic> module in the library manages files within the library and implements several import functions to batch load data files from local or public repositories. To access files available through Bioconductor’s <italic toggle="yes">AnnotationHub</italic> or <italic toggle="yes">ExperimentHub</italic>, we implemented methods to directly import these into the library. The UCSC Genome Browser provides Track hubs (<xref rid="btaa591-B24" ref-type="bibr">Raney <italic toggle="yes">et al.</italic>, 2014</xref>), a useful tool for visualizing large number of genome-wide datasets. Track hubs are web-accessible directories that contain genomic data and can be viewed on the Genome Browser. Methods are available to load track hub</p>
        <p>
          <monospace>from</monospace>
          <monospace>epivizfileserver</monospace>
          <monospace>import</monospace>
          <monospace>MeasurementManager</monospace>
        </p>
        <p>
          <monospace># create measurements manager</monospace>
        </p>
        <p>
          <monospace>mMgr=MeasurementManager()</monospace>
        </p>
        <p>
          <monospace># add genome</monospace>
        </p>
        <p>
          <monospace>genome=mMgr.add_genome(</monospace>
          <monospace>“</monospace>
          <monospace>hg19</monospace>
          <monospace>”</monospace>
          <monospace>)</monospace>
        </p>
        <p>
          <monospace># i</monospace>
          <monospace>m</monospace>
          <monospace>port measurements from AnnotationHub</monospace>
        </p>
        <p>
          <monospace>roadmap=mMgr.import_ahub(os.getcwd() +</monospace>
        </p>
        <p>
          <monospace>“</monospace>
          <monospace>/roadmap.json</monospace>
          <monospace>”</monospace>
          <monospace>)</monospace>
        </p>
        <p>repositories into the library. EFS library also supports a JSON-based configuration to define files. The configuration file defines a collection of files and for each file, describes the location of the file (public or full local path), its format, name, and annotations. An example configuration file is described in the use case section (<xref rid="btaa591-F2" ref-type="fig">Fig. 2</xref>). Regardless of how files imported, users can always define transformations over the files. The snippet below shows how one can import the files from <italic toggle="yes">AnnotationHub</italic>. One needs to create a measurements manager object which describes and manages various files available in the system. We also support a number of genomes (indexed using Tabix) for navigational queries and querying for annotations in a given genomic region.
</p>
        <fig position="float" id="btaa591-F2">
          <label>Fig. 2.</label>
          <caption>
            <p>Overview of Epiviz integration with Galaxy. Users can include the Epiviz Galaxy Tool in a workflow to choose files and define annotations to generate an Epiviz configuration file. A Galaxy IE using the Epiviz configuration spins the Epiviz docker instance. Once the docker image loads, Galaxy embeds the user interface from the instance on its user interface as shown on the right</p>
          </caption>
          <graphic xlink:href="btaa591f2" position="float"/>
        </fig>
      </sec>
      <sec>
        <label>2.3.3</label>
        <title>Transformations over files (computed measurements)</title>
        <p>As previously stated, if an analyst needs to perform complex summarization or transformations across genomic datasets, they would need to first perform interval overlap operations that are limited or non-existent in database systems. To apply transformation across files that contain different intervals in a given genomic region, we should first find discrete intervals represented by the data, and then apply transformations over the overlapping regions. These interval operations and the transformation functions are limited in scope to the methods implemented in database management systems.</p>
        <p>Existing genome browsers that support file-based visualization are only limited to exploration of data from files, i.e. query data from files and visualizing this data as tracks. As an illustrative example, if a researcher is exploring ChIP-seq data for a particular histone (<italic toggle="yes">H3k24me3</italic>) marker across different tissues, and would like to visualize the difference in binding across these tissues, the typical way is to use a computational environment or tools to read the files, align the data from these files to apply transformations, compute the difference and then store the dataset as a file or into a database. For the purpose of exploration, it would be very efficient to be able to define these transformations over files without having to pre-compute and interactively explore these transformations.</p>
        <p>EFS allows defining transformations over files using methods available in NumPy or custom functions using a NumPy compatible definition (called <italic toggle="yes">computed measurements</italic> in the <italic toggle="yes">measurements</italic> module). These transformations are not pre-computed but lazily evaluated at query time. When querying these computed measurements for a particular genomic region, we first query the individual files for data, perform interval overlap operation to align the datasets and then apply the transformation. The snippet below describes how users can define computed measurements in the system. The first snippet shows how users can use an existing numpy function as a transformation; the other shows how to use a custom function that centers the signal around the mean as a transformation. The <italic toggle="yes">computeFunc</italic> parameter takes in the transformation you would like to apply to the files. The syntax is similar to the Pandas DataFrame <italic toggle="yes">apply</italic> function.</p>
        <p>
          <monospace># filter for files from the measurements (brain samples)</monospace>
        </p>
        <p>
          <monospace>brain=[“E071”, “E074”]</monospace>
        </p>
        <p>
          <monospace>brain_files=out=[m for m in roadmap</monospace>
          <monospace>if m.id in [brain]]</monospace>
        </p>
        <p>
          <monospace># using Numpy, compute difference in binding</monospace>
        </p>
        <p>
          <monospace>mMgr.add_computed_measurement(type=“computed”, </monospace>
        </p>
        <p>
          <monospace>name=“Diff_Signal”, id=“Diff_Signal”, measurements=brain_files, computeFunc= numpy.diff, computeAxis = 1)</monospace>
        </p>
        <p>
          <monospace># custom transformation</monospace>
        </p>
        <p>
          <monospace>def norm(col):</monospace>
        </p>
        <p>
          <monospace> mean = numpy.mean(col)</monospace>
        </p>
        <p>
          <monospace> return(col.apply(lambda x: x-mean))</monospace>
        </p>
        <p>
          <monospace>mMgr.add_computed_measurement(type=“computed”, </monospace>
        </p>
        <p>
          <monospace>name=“signal_norm”, id=“signal_norm”,</monospace>
        </p>
        <p>
          <monospace>measurements=m, computeFunc=norm,</monospace>
        </p>
        <p>
          <monospace>computeAxis = 0)</monospace>
        </p>
      </sec>
      <sec>
        <label>2.3.4</label>
        <title>Concurrency</title>
        <p>When concurrent queries are made to the system, the <italic toggle="yes">handler</italic> module distributes these queries over available system resources. The <italic toggle="yes">handler</italic> module uses Dask, a lightweight distributed computing library for Python to scale and compute transformations. Dask manages, distributes and schedules tasks dynamically. The Dask scheduler is asynchronous and event driven, simultaneously responding to requests for computation from multiple clients. This provides flexibility to concurrently handle a variety of workloads from multiple users at the same time while also handling node failures and additions. The <italic toggle="yes">handler</italic> module is a wrapper around Dask to schedule queries to the system. The system automatically creates the handler when the application is run, but the user also has the flexibility to create a handler object and submit queries.</p>
      </sec>
      <sec>
        <label>2.3.5</label>
        <title>Cache policy</title>
        <p>To efficiently process queries, a traditional database server implements cache management and can provide scalability as the system demands. A file-based web server always has to access the files to query for a genomic region. This may not be efficient for repeated query processing for the same region. When querying BigWig and BigBed using the tools (<italic toggle="yes">bigWigSummary</italic> and <italic toggle="yes">bigBedSummary</italic>) from UCSC utilities (<ext-link xlink:href="https://hgdownload.soe.ucsc.edu/admin/exe/" ext-link-type="uri">https://hgdownload.soe.ucsc.edu/admin/exe/</ext-link>) these tools implement a URL cache layer using the sparse file feature available in UNIX-based systems, which downloads and creates files locally as blocks of the files are accessed. One might quickly run out of space once you start caching hundreds of files from a repository locally.</p>
        <p>EFS implements a simple cache management system to only store frequently accessed blocks as part of the file objects. The file objects always keep the header, zoom level and chromosome tree information as part of the file. This reduces the number of requests to the file (if available locally) or server hosting the file. As queries are processed, we store the binary blocks (or byte ranges) of the file that were recently and frequently accessed. The cache allows the library to quickly process repeated queries for a genomic region. As the number of queries and files increase, it also increases the cache size and impacts memory usage. The library implements task schedulers (through the <italic toggle="yes">handler</italic> module) that automatically serialize (pickle) file objects from memory to disk. This process efficiently manages memory and frees up system resources for other tasks. After storing a file object to disk, if a new request is made to query for a genomic region from the same file, we load (deserialize) the file object into memory and process the query. We should note that this simple cache expiration policy could be implemented by other tools to reduce space usage as well.</p>
      </sec>
      <sec>
        <label>2.3.6</label>
        <title>Optimizations for visual analytics</title>
        <p>EFS performs a number of optimizations mostly when querying for data from a file that contains zoom levels (BigWig and BigBed). This is extremely useful especially for interactive visualization and exploration. For example, visualizing signal data from a roadmap ChIP-Seq dataset for a 10 000 base-pair (bp) region, on a Webpage canvas of size 800 (width) by 400 (height) pixels. If the visualization library plots 1 bp per pixel, it would be very inefficient to try to render 10 000 data points in an 800-pixel width screen. In such scenarios, the library automatically chooses the appropriate zoom level to query for data, i.e. for the same example above, we choose the zoom level that has a reduction of around 12 bp per bin. If no zoom levels are available, the library also summarizes the data into smaller bins and computes a mean across each bin. This decreases the response to query time, improves rendering performance and is efficient for interactive visualization and data exploration for large genomic datasets. For example in the snippet below, the first query returns all intervals from the file whereas the second uses a zoom level (if available) and/or summarizes the data to 1000 bins.</p>
        <p>
          <monospace># Query files</monospace>
        </p>
        <p>
          <monospace>result err = await measurement.get_data(“chr11”, start = 10550488, end = 11554489)</monospace>
        </p>
        <p>
          <monospace>result, err = await measurement.get_data(“chr11”, start = 10550488, end = 11554489, bins = 1000)</monospace>
        </p>
      </sec>
      <sec>
        <label>2.3.7</label>
        <title>REST API</title>
        <p>All datasets loaded into the system can be easily queried using a REST API using the <italic toggle="yes">server</italic> module. The library uses Sanic (<ext-link xlink:href="https://github.com/huge-success/sanic" ext-link-type="uri">https://github.com/huge-success/sanic</ext-link>), an asynchronous library to make API queries to files and integrates well with the Dask system for handling Web requests. The REST API syntax is available in our documentation website.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Use cases</title>
      <sec>
        <label>3.1.1</label>
        <title>Integration with Galaxy</title>
        <p>Galaxy (<xref rid="btaa591-B1" ref-type="bibr">Afgan <italic toggle="yes">et al.</italic> 2018</xref>) is one of the widely used open source Web-based platforms for analysis of genomic data. Galaxy aims to make computational biology accessible to researchers with less programing experience. It has an easy to use user interface to create reproducible and shareable workflows and installs various bioinformatic tools without all the complexity. At its core, Galaxy is a file-based workflow platform, every step in the workflow creates a file(s) and these files are used as inputs in the next step. Our goal with integrating Epiviz with Galaxy is to create a single computational environment where users can analyze and explore datasets generated by Galaxy workflows. Galaxy provides Galaxy Interactive Environments (IEs), a framework to integrate external tools with Galaxy workflows and user interface.</p>
        <p>To integrate Epiviz with Galaxy, we need to (i) Register and Run Epiviz as a tool with the Galaxy system, (ii) Define and access files generated at various steps in a Galaxy workflow and (iii) Query interactively visualize the files using Epiviz.</p>
        <sec>
          <title>3.1.1.1 Register and Run Epiviz with Galaxy</title>
          <p>To integrate external tools, Galaxy provides a framework called Galaxy IEs (<xref rid="btaa591-B11" ref-type="bibr">Grüning <italic toggle="yes">et al.</italic> 2017</xref>). The first step in IE is to create a docker container for Epiviz to manage its dependencies internally and also efficiently manage system resources. The docker container hosts both the EFS (the library to manage and query files) and the Epiviz user interface to interactively query and visualize datasets. IE requires a configuration file (mako) that spins up the docker instance on demand and run Epiviz inside Galaxy. The mako configuration file sets various parameters for the docker image and configures the ports to use from the image to serve the user interface. The library also needs access to the files generated during the Galaxy workflow. Instead of copying over files to the docker instance, we mount the data directory used by Galaxy to the docker image.</p>
        </sec>
        <sec>
          <title>3.1.1.2 Define files from Galaxy workflows</title>
          <p>To define and access files to visualize using Epiviz, we created a Galaxy Tool (Blankenberg <italic toggle="yes">et al.</italic>, 2014) that the user integrates with their final step of the workflow. This tool allows the user to choose various files generated in their workflow, define annotations and file formats, and generates an Epiviz configuration file as shown in the top panel in <xref rid="btaa591-F2" ref-type="fig">Figure 2</xref>. EFS library running in the docker image from the previous step uses this configuration file to load various files into the instance.</p>
        </sec>
        <sec>
          <title>3.1.1.3 Visualize files in Galaxy workflow</title>
          <p>After generating the configuration file in the previous step, users can visualize these datasets using Epiviz within the Galaxy interface. This displays the Epiviz application running inside the docker container on Galaxy and users can now visualize and interactively explore various files. The bottom panel in <xref rid="btaa591-F2" ref-type="fig">Figure 2</xref> illustrates the process of integration and describes various steps in the process.</p>
          <p>To demonstrate the integration, we use the Analysis of ChIP-seq data workflow from Galaxy (<ext-link xlink:href="https://galaxyproject.org/tutorials/chip/" ext-link-type="uri">https://galaxyproject.org/tutorials/chip/</ext-link>). The workflow uses data where immunoprecipitation was performed with antibodies from Reb1.Reb1 recognizes a specific sequence (TTACCCG) and is involved in many aspects of transcriptional regulation by all three yeast RNA polymerases and promotes formation of nucleosome-free regions (<xref rid="btaa591-B12" ref-type="bibr">Hartley and Madhani, 2009</xref>; <xref rid="btaa591-B23" ref-type="bibr">Raisner <italic toggle="yes">et al.</italic>, 2005</xref>). After executing the entire workflow, users can now run the Epiviz tool to generate the configuration file on the coverage and peak calling files. The Galaxy screenshot in the right panel of <xref rid="btaa591-F2" ref-type="fig">Figure 2</xref> shows the datasets visualized from the results of this workflow. The first track visualizes Replicate 1 and its Input signal, the second track visualizes Replicate 2 and its Input signal. After running the MACS2 peak calling tool on the dataset, the Galaxy workflow generates peak files and the last track visualizes these peak regions across both the replicates.</p>
        </sec>
      </sec>
      <sec>
        <label>3.1.2</label>
        <title>Epigenome roadmap</title>
        <p>The NIH Roadmap Epigenomics Mapping Consortium leverages next-generation sequencing technologies to map DNA methylation, histone modifications, chromatin accessibility and small RNA transcripts in tissues, selected to represent the normal counterparts of tissues and organ systems frequently involved in human disease. The data files from this consortium are available on the Roadmap Portal (<ext-link xlink:href="http://www.roadmapepigenomics.org/" ext-link-type="uri">http://www.roadmapepigenomics.org/</ext-link>).</p>
        <p>For this use case, we use BioConductor’s <italic toggle="yes">AnnotationHub</italic> to query for data files that are part of the NIH Roadmap Epigenomics project. We downloaded the <italic toggle="yes">AnnotationHub</italic> SQLite database to extract information for all available resources in the hub. We then query this database for resources associated with the roadmap project. We get a total of 9932 data resources from <italic toggle="yes">AnnotationHub</italic>. These include DNA methylation signal, ChIP-seq fold change signal and <italic toggle="yes">P</italic>-values for various tissues and histone markers. To easily import these <italic toggle="yes">AnnotationHub</italic> records into the EFS, we added a helper function ‘<italic toggle="yes">import_ahub</italic>’. We filtered samples for the brain region and defined transformations over these files. We define difference in histone modification between different brain tissues for this dataset. We provide this file server instance as a public AMI (ID: ami-0de924a41fb56f52b) on Amazon Web Services (AWS) to explore the roadmap dataset and the transformations as shown in <xref rid="btaa591-F3" ref-type="fig">Figure 3</xref>. A short version of how this dataset is setup is illustrated in the code snippet below with a full example available in the documentation.
</p>
        <fig position="float" id="btaa591-F3">
          <label>Fig. 3.</label>
          <caption>
            <p>Interactive visualization of data from the NIH Roadmap Epigenomics project. This figure demonstrates the EFS library querying and computing transformations over data available from the NIH Roadmap Epigenomics project. We chose the ESR1 and its neighboring gene region for this example. (Top to bottom) The first track is a hg19 genome annotation track. The line track in the middle is visualizing the H3K36me3 binding signal from the ChIP-seq experiments across three different brain tissues. This track queries the data directly from the files. The last line track is a transformation over files to compute difference in histone binding across different tissues</p>
          </caption>
          <graphic xlink:href="btaa591f3" position="float"/>
        </fig>
        <p>
          <monospace># create measurements </monospace>
        </p>
        <p>
          <monospace>managermMgr=MeasurementManager()</monospace>
        </p>
        <p>
          <monospace># create a Dask handler</monospace>
        </p>
        <p>
          <monospace>mHandler=create_fileHandler()</monospace>
        </p>
        <p>
          <monospace>rfile=open(os.getcwd()+”/roadmap.pickle”, ”rb”)</monospace>
        </p>
        <p>
          <monospace>roadmap=pickle.load(rfile)</monospace>
        </p>
        <p>
          <monospace># import measurements from AnnotationHub</monospace>
        </p>
        <p>
          <monospace>roadmap=mMgr.import_ahub(roadmap, mHandler)</monospace>
        </p>
        <p>
          <monospace># filter for brain</monospace>
        </p>
        <p>
          <monospace>datasetsbrain=[“E071”, “E074”]</monospace>
        </p>
        <p>
          <monospace>froadmap=out=[m for m in roadmap </monospace>
        </p>
        <p>
          <monospace>if m.id in [brain]]</monospace>
        </p>
        <p>
          <monospace># Apply transformations</monospace>
        </p>
        <p>
          <monospace>mMgr.add_computed_measurement(“computed”,</monospace>
        </p>
        <p>
          <monospace>”Diff_Signal”, ”Diff_Signal”, measurements=</monospace>
        </p>
        <p>
          <monospace>froadmap, computeFunc=numpy.diff)</monospace>
        </p>
        <p>
          <monospace># Run the server with the measurements</monospace>
        </p>
        <p>
          <monospace>app=setup_app(mMgr)</monospace>
        </p>
        <p>
          <monospace>app.run(port = 8000)</monospace>
        </p>
      </sec>
    </sec>
    <sec>
      <title>3.2 Benchmarks</title>
      <p>We performed several tests to evaluate (i) the performance with and without the cache implementation, and (ii) the overhead in lazily evaluating transformations. All tests were run on a standard Amazon AWS EC2 (t2.xlarge) instance with 4 vCPUs and 16 GB memory.</p>
      <sec>
        <title>3.2.1 Impact of cache on performance</title>
        <p>To evaluate the impact of cache on the system, we randomly generated 20 different genomic range queries and repeatedly queried these against the Web server for 60 s. We use <italic toggle="yes">wrk</italic> (<ext-link xlink:href="https://github.com/wg/wrk" ext-link-type="uri">https://github.com/wg/wrk</ext-link>), a HTTP benchmarking tool capable of generating significant load to test the API. We run the tool on its default settings using 2 threads and 10 connections concurrently to send requests to the system.</p>
        <p>We run the system on two different modes, one with the cache feature and the other without the cache. In the cache implementation, if the given genomic region already exists in the cache, it is fetched quickly and sent back to the user whereas in the non-cache setting, the library always parses the file to query data for the given genomic range. Since local file access is fast, our results are comparable between the cache and non-cache settings. Instead, we hosted the files on a S3 object store bucket at University of Maryland, adding network latency to the system. The results indicate the cache implementation significantly improves the performance of the system. <xref rid="btaa591-T3" ref-type="table">Table 3</xref> displays the results of these tests. To make sure other processes are not interfering in the benchmarking process, we disabled the serialization process for file objects discussed in Section 2.
</p>
        <table-wrap position="float" id="btaa591-T3">
          <label>Table 3.</label>
          <caption>
            <p>Impact of cache on processing requests</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="center" span="1"/>
              <col valign="top" align="center" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1">Implementation</th>
                <th rowspan="1" colspan="1">Average Latency (in ms; ± SD)</th>
                <th rowspan="1" colspan="1">Average Requests (per s; ± SD)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">EFS—no Cache (remote file)</td>
                <td rowspan="1" colspan="1">1152 (± 201.32)</td>
                <td rowspan="1" colspan="1">8.2 (± 0.44)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">EFS—cache (remote file)</td>
                <td rowspan="1" colspan="1">68.41 (± 83.55)</td>
                <td rowspan="1" colspan="1">179.4 (± 3.2)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">EFS (local file)</td>
                <td rowspan="1" colspan="1">36.05 (± 8.74)</td>
                <td rowspan="1" colspan="1">284.1 (± 41.31)</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PyBigWig (remote file)</td>
                <td rowspan="1" colspan="1">121.864 (± 40.67)</td>
                <td rowspan="1" colspan="1">—</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">PyBigWig (local file)</td>
                <td rowspan="1" colspan="1">0.52 (± 0.28)</td>
                <td rowspan="1" colspan="1">—</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic toggle="yes">Note</italic>: This table displays the average latency and the requests processed per second measured when benchmarking the File Server API with and without a cache implementation. With cache, the library was able to process a significantly higher number of genomic range queries resulting in higher throughput and lower latency. The extra overhead in EFS is because of using intermediate data representations so that transformations can be performed across files and the use of JSON as a portable output format for multiple clients to query the system.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
        <p>We also ran a similar experiment to compare the performance of EFS with existing bioinformatic tools. We use the PyBigWig python package (https://github.com/deeptools/pyBigWig), an extension to the C library, libBigWig that can read or parse local or remote BigWig and BigBed files. We run the same experiment as before, where we generate 20 random genomic ranges and execute these queries repeatedly for 60 s using the PyBigWig library. We read the same remotely hosted file and measure the average time per query and calculate SD. We notice that EFS performs significantly faster if the file is hosted remotely. Unsurprisingly direct access to the local file using PyBigWig is significantly faster compared with EFS. The extra overhead is EFS is due to the (i) use of an intermediate representation (Pandas <italic toggle="yes">DataFrame</italic>) so that transformations or summarizations can be performed across/within files and, (ii) using a portable data transfer representation of the results (JSON) so that multiple clients can query the system. PyBigWig only reports the intervals and the data in those intervals.</p>
      </sec>
      <sec>
        <label>3.2.2</label>
        <title>Overhead in lazily computing transformations</title>
        <p>EFS library lazily computes transformations or summarizations directly over files. We measure the overhead in computing transformations query time as opposed to pre-computing a transformation, storing and querying this file. For this test, we choose 20 different genomic datasets (bigwig files) from roadmap; we created an instance of the EFS that at query time computes a mean signal value with increasing number of files starting from 2 and up to 20 genomic datasets. In addition, we also pre-computed the mean signal using WiggleTools (<xref rid="btaa591-B32" ref-type="bibr">Zerbino <italic toggle="yes">et al.</italic>, 2014</xref>) and store these files. This allows us to compare the overhead in on-the-fly computation versus the pre-compute.</p>
        <p>We run similar benchmarks as before using the <italic toggle="yes">wrk</italic> tool, where we randomly generate 5 different genomic range queries and query the system for 60 s (2 threads and 10 connections). We measure the average latency and requests processed per second, and calculate mean and SD of these metric across five different runs. Our results are shown in <xref rid="btaa591-F4" ref-type="fig">Figure 4</xref>. As expected, as the transformation involves more files, the latency of the system increases hence serving fewer requests per second compared with directly querying a pre-computed file. However, the system is still interactive with reasonable query response times.
</p>
        <fig position="float" id="btaa591-F4">
          <label>Fig. 4.</label>
          <caption>
            <p>Impact of computing transformations at run time. We measure the overhead in computing transformations lazily (on-the-fly) versus querying a file that stores the pre-computed result. We measure the average latency and requests processed per second by the system across five different runs and the shows the mean and standard deviation for these metrics. The results indicate the latency of the system increases as we increase the number of files involved in the computation, hence lowering the number of requests processed per second</p>
          </caption>
          <graphic xlink:href="btaa591f4" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>3.3 Software availability</title>
      <p>EFS is open source and is available on GitHub at <ext-link xlink:href="https://github.com/epiviz/epivizFileParser" ext-link-type="uri">https://github.com/epiviz/epivizFileServer</ext-link>. The documentation for the File Server library is available at <ext-link xlink:href="http://epivizfileparser.rtfd.io" ext-link-type="uri">http://epivizfileserver.rtfd.io</ext-link>. The package is published to PyPI and is available at <ext-link xlink:href="https://pypi.org/project/epivizFileServer" ext-link-type="uri">https://pypi.org/project/epivizFileServer</ext-link>.</p>
      <p>For integration with Galaxy, we have two different GitHub repositories, one for the Epiviz Tool (<ext-link xlink:href="https://github.com/epiviz/epivizGalaxyTool" ext-link-type="uri">https://github.com/epiviz/epivizGalaxyTool</ext-link>) and the other for Epiviz IE (<ext-link xlink:href="https://github.com/epiviz/epivizGalaxyIE" ext-link-type="uri">https://github.com/epiviz/epivizGalaxyIE</ext-link>). The repositories also contain instructions to setup the IEs in Galaxy.</p>
      <p>For the use case describing the Roadmap dataset, the code is available in the GitHub repository inside the use cases folder. A tutorial of the library is also available at <ext-link xlink:href="https://epiviz.github.io/post/2019-02-04-epiviz-fileserver/" ext-link-type="uri">https://epiviz.github.io/post/2019-02-04-epiviz-fileserver/</ext-link>. We also provide an AWS image for this instance (AMI ID: ami-0de924a41fb56f52b).</p>
      <p>A collection of snippets to quickly spin an instance of the File Server with publicly available data and interactively visualize using Epiviz browser is hosted on GitHub gists at <ext-link xlink:href="https://gist.github.com/jkanche/1cd32ad8b2af9d59c834508ddc468359" ext-link-type="uri">https://gist.github.com/jkanche/1cd32ad8b2af9d59c834508ddc468359</ext-link>.</p>
      <p>The code for benchmarks is also available in the same repository in the benchmarks folder.</p>
      <p>The code is available under MIT License.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>EFS is a Python library to interactively query and transform data directly from indexed genomic files. The library implements several features provided by a traditional database system to query, transform, cache, scale and visualize the data from files. The library decouples data from analysis workflows and provides an abstract interface to define computations independent of the location, format or structure of the file. Because of this modularity in the implementation developers can use the library or the REST API to develop interactive genomic data visualization and exploration tools. This way new transformations can be applied over files and can be queried dynamically. With the integration of the Epiviz browser (but not limited to), EFS provides a quick way for researchers to perform exploratory visual data analysis directly over files.</p>
    <p>The library currently works with indexed genomic file formats like BigBed, BigWig or Tabix. Most genomic data analysis workflows and pipelines generate BigWigs or BigBeds and most public repositories provide genomic data from experiments in these file formats. These formats support random access, concurrent and remote queries without the need to download or read the entire file into memory. The EFS library is an approach to implement a scalable data query and transformation system directly over these indexed files. EFS does not support offline conversion of files into the indexed format. If the files are available remotely, the server hosting the files must support HTTP range requests. This allows the library to only requests the minimum necessary bytes to perform the query. Most modern Web and FTP servers support range-requests.</p>
    <p>Network latency is an important variable to consider when applying transformations over remote files. If the library is used to apply complex transformation over large number of remote files, we recommend running EFS with local access to files. This reduces both networks latency and speeds up query time. Our tests from the Benchmark section shows the performance of the File Server and the extra overhead in using the EFS library compared with existing bioinformatic tools like PyBigWig that can directly access local/remote BigWig/BigBed files. The extra overhead is due the use of an intermediate representation (Pandas <italic toggle="yes">DataFrame</italic>) to be able to apply transformations and a portable output format JSON to transfer data across the network for multiple clients. Even with the extra overhead, the system is highly performant and interactive. The PyBigWig library only returns the intervals and data from the file and to perform any analysis on top of these results would require extra processing of the data into a usable format.</p>
    <p>The system works well for moderate sized repositories to apply complex transformations over files at query time. This depends on available system resources (processors, memory) for Dask to scale and the location of files (local or remote). Our tests on transformations show that latency of the system increases as we apply transformations over increasing number of files. The <italic toggle="yes">parser</italic> module spends the majority of time reading the index tree to find positions within the file that contain the data. For example applying a transformation over 20 files, the system has to individually parse the index of the 20 files to get the data and then apply transformations.</p>
    <p>In addition to the usecases detailed above, Epiviz File Server is used to interactively query epigenetic data shared to the Neuroscience Multiomic Archive (NEMO, https://www.nemoanalytics.org/) and the gEAR portal (https://umgear.org/). These applications for visualization and analysis of multiomic data use Epiviz and EFS to visualize and analyze epigenetic data both in public and private domains.</p>
  </sec>
  <sec>
    <title>5 Future work</title>
    <p>Single-cell technologies generate large datasets measuring tens of thousands of features over thousands of single cells. Although very efficient to query by genomic region, if we are only interested in a few cells from such large matrices, the EFS library using the Tabix format still has to parse the entire row and filter the columns. Tabix is a commonly used indexing technique for any tabular genomic dataset (the first three columns must be chromosome location, start and end). Interactive analysis, including visualization of these datasets is a challenging task especially for queries to filter by columns (or cells) and efficiently transferring these long matrices between server and client. <xref rid="btaa591-B22" ref-type="bibr">Piccolo <italic toggle="yes">et al.</italic> (2019)</xref> recommend using coordinate based fixed width formats as a fast and scalable approach to query tabular genomic data. In addition, the genomics community has been using HDF5-based formats AnnData or H5ad (<xref rid="btaa591-B31" ref-type="bibr">Wolf <italic toggle="yes">et al.</italic>, 2018</xref>) and loom (<ext-link xlink:href="https://loompy.org" ext-link-type="uri">https://loompy.org</ext-link>) to store large genomics datasets. We are currently exploring ways to efficiently query HDF5, H5ad or loom format files. These formats do not natively support remote querying like BigWig or Tabix but require an additional server like h5serv (<ext-link xlink:href="https://support.hdfgroup.org/pubs/papers/RESTful_HDF5.pdf" ext-link-type="uri">https://support.hdfgroup.org/pubs/papers/RESTful_HDF5.pdf</ext-link>) setup for Web queries.</p>
    <p>Recent approaches like Pyranges (<xref rid="btaa591-B28" ref-type="bibr">Stovner and Sætrom, 2020</xref>) have implemented data structures for efficiently manipulating genomic intervals in Python. The EFS library can be extended to incorporate these file formats and data structures to efficiently query, perform interval overlap operations and interactively explore multi-omic datasets.</p>
  </sec>
  <sec>
    <title>6 Conclusion</title>
    <p>Based on the concepts of a NoDB paradigm, we present a file-based Python library, an <italic toggle="yes">in situ</italic> data query system for indexed genomic files, not only for visualization but also transformation. The library implements several features provided by a traditional database system for query, transformation and caching. The use cases demonstrate the flexibility in integrating the EFS library with existing bioinformatic tools and public repositories. We discussed new research approaches to build a comprehensive file-based data visualization and exploration system for genomics datasets.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the U.S. National Institutes of Health grant [R01GM114267 and R24MH114815].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btaa591-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Afgan</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> ( <year>2018</year>) 
<article-title>The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2018 update</article-title>. <source>Nucleic Acids Res</source>., <volume>46</volume>, <fpage>W537</fpage>–<lpage>W544</lpage>.<pub-id pub-id-type="pmid">29790989</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B2">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Alagiannis</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>). NoDB: Efficient Query Execution on Raw Data Files. In: Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data (SIGMOD ’12). Association for Computing Machinery, New York, NY, USA, 241–252. doi: <pub-id pub-id-type="doi">10.1145/2213836.2213864.</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bernstein</surname><given-names>B.E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) 
<article-title>The NIH Roadmap Epigenomics Mapping Consortium</article-title>. <source>Nat. Biotechnol</source>., <volume>28</volume>, <fpage>1045</fpage>–<lpage>1048</lpage>.<pub-id pub-id-type="pmid">20944595</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Bikakis</surname><given-names>N.</given-names></string-name></person-group>  <etal>et al</etal> (2018) <part-title>RawVis: visual exploration over raw data</part-title>. In: <source>European Conference on Advances in Databases and Information Systems</source>. 
<publisher-name>Springer</publisher-name>, 
<publisher-loc>Cham</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btaa591-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Blankenberg</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal>; The Galaxy Team (<year>2014</year>) 
<article-title>Dissemination of scientific software with Galaxy Tool Shed</article-title>. <source>Genome Biol</source>., <volume>15</volume>, <fpage>403</fpage>.<pub-id pub-id-type="pmid">25001293</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Buels</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>JBrowse: a dynamic Web platform for genome visualization and analysis</article-title>. <source>Genome Biol</source>., <volume>17</volume>, <fpage>66</fpage>.<pub-id pub-id-type="pmid">27072794</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chelaru</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) 
<article-title>Epiviz: interactive visual analytics for functional genomics data</article-title>. <source>Nat. Methods</source>, <volume>11</volume>, <fpage>938</fpage>–<lpage>940</lpage>.<pub-id pub-id-type="pmid">25086505</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Davis</surname><given-names>C.A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>The Encyclopedia of DNA elements (ENCODE): data portal update</article-title>. <source>Nucleic Acids Res</source>., <volume>46</volume>, <fpage>D794</fpage>–<lpage>D801</lpage>.<pub-id pub-id-type="pmid">29126249</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Down</surname><given-names>T.A.</given-names></string-name></person-group>  <etal>et al</etal> ( <year>2011</year>) 
<article-title>Dalliance: interactive genome viewing on the Web</article-title>. <source>Bioinformatics</source>, <volume>27</volume>, <fpage>889</fpage>–<lpage>890</lpage>.<pub-id pub-id-type="pmid">21252075</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Freese</surname><given-names>N.H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) 
<article-title>Integrated genome browser: visual analytics platform for genomics</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>2089</fpage>–<lpage>2095</lpage>.<pub-id pub-id-type="pmid">27153568</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Grüning</surname><given-names>B.A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) 
<article-title>Jupyter and Galaxy: easing entry barriers into complex data analyses for biomedical researchers</article-title>. <source>PLoS Comput. Biol</source>., <volume>13</volume>, <fpage>e1005425</fpage>.<pub-id pub-id-type="pmid">28542180</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hartley</surname><given-names>P.D.</given-names></string-name>, <string-name><surname>Madhani</surname><given-names>H.D.</given-names></string-name></person-group> (<year>2009</year>) 
<article-title>Mechanisms that specify promoter nucleosome location and identity</article-title>. <source>Cell</source>, <volume>137</volume>, <fpage>445</fpage>–<lpage>458</lpage>.<pub-id pub-id-type="pmid">19410542</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huber</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) 
<article-title>Orchestrating high-throughput genomic analysis with Bioconductor</article-title>. <source>Nat. Methods</source>, <volume>12</volume>, <fpage>115</fpage>–<lpage>121</lpage>.<pub-id pub-id-type="pmid">25633503</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kancherla</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>Epiviz Web Components: reusable and extensible component library to visualize functional genomic datasets</article-title>. <source>F1000Res</source>., <volume>7</volume>, <fpage>1096</fpage>.<pub-id pub-id-type="pmid">30135734</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kent</surname><given-names>W.</given-names></string-name></person-group>J <etal>et al</etal> (<year>2002</year>) 
<article-title>The Human Genome Browser at UCSC</article-title>. <source>Genome Res</source>., <volume>12</volume>, <fpage>996</fpage>–<lpage>1006</lpage>.<pub-id pub-id-type="pmid">12045153</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kent</surname><given-names>W.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) 
<article-title>BigWig and BigBed: enabling browsing of large distributed datasets</article-title>. <source>Bioinformatics</source>, <volume>26</volume>, <fpage>2204</fpage>–<lpage>2207</lpage>.<pub-id pub-id-type="pmid">20639541</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kerpedjiev</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>HiGlass: Web-based visual exploration and analysis of genome interaction maps</article-title>. <source>Genome Biol</source>., <volume>19</volume>, <fpage>1</fpage>–<lpage>12</lpage>.<pub-id pub-id-type="pmid">29301551</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Li</surname><given-names>H.</given-names></string-name></person-group> (<year>2011</year>) 
<article-title>Tabix: fast retrieval of sequence features from generic TAB-delimited files</article-title>. <source>Bioinformatics</source>, <volume>27</volume>, <fpage>718</fpage>–<lpage>719</lpage>.<pub-id pub-id-type="pmid">21208982</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B19">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>McKinney</surname><given-names>W.</given-names></string-name></person-group> (<year>2010</year>). 
<article-title>Data structures for statistical computing in python</article-title>. In: <italic toggle="yes">Proceedings of the 9th Python in Science Conference</italic>, vol. 445, pp. <fpage>51</fpage>–<lpage>56</lpage>, doi: 10.1109/MCSE.2011.37.</mixed-citation>
    </ref>
    <ref id="btaa591-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Morgan</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>a). AnnotationHub: client to access AnnotationHub resources. Version 2.18.0. doi: <pub-id pub-id-type="doi">10.18129/B9.bioc.AnnotationHub</pub-id>.</mixed-citation>
    </ref>
    <ref id="btaa591-B21">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Morgan</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>b). ExperimentHub: Client to access ExperimentHub resources. Version 1.12.0. doi: <pub-id pub-id-type="doi">10.18129/B9.bioc.ExperimentHub</pub-id>.</mixed-citation>
    </ref>
    <ref id="btaa591-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Piccolo</surname><given-names>S.R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) 
<article-title>Coordinate-based mapping of tabular data enables fast and scalable queries</article-title>. <source>bioRxiv</source>, <fpage>536979,</fpage> doi: 10.1101/536979.</mixed-citation>
    </ref>
    <ref id="btaa591-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raisner</surname><given-names>R.M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2005</year>) 
<article-title>Histone variant H2A.Z marks the 5’ ends of both active and inactive genes in euchromatin</article-title>. <source>Cell</source>, <volume>123</volume>, <fpage>233</fpage>–<lpage>248</lpage>.<pub-id pub-id-type="pmid">16239142</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Raney</surname><given-names>B.J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) 
<article-title>Track data hubs enable visualization of user-defined genome-wide annotations on the UCSC Genome Browser</article-title>. <source>Bioinformatics</source>, <volume>30</volume>, <fpage>1003</fpage>–<lpage>1005</lpage>.<pub-id pub-id-type="pmid">24227676</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Robinson</surname><given-names>J.T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2011</year>) 
<article-title>Integrative genomics viewer</article-title>. <source>Nat. Biotechnol</source>., <volume>29</volume>, <fpage>24</fpage>–<lpage>26</lpage>.<pub-id pub-id-type="pmid">21221095</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B26">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Rocklin</surname><given-names>M.</given-names></string-name></person-group> (<year>2015</year>). 
<article-title>Dask: parallel computation with blocked algorithms and task scheduling</article-title>. In: <italic toggle="yes">Proceedings of the 14th Python in Science Conference</italic>, doi: 10.25080/Majora-7b98e3ed-013.</mixed-citation>
    </ref>
    <ref id="btaa591-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Stovner,E</surname>.<given-names>B.</given-names></string-name> and 
<string-name><surname>Sætrom</surname>,<given-names>P.</given-names></string-name></person-group> (<year>2020</year>) 
<article-title>PyRanges: efficient comparison of genomic intervals in Python</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>918</fpage>–<lpage>919</lpage>.<pub-id pub-id-type="pmid">31373614</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van der Walt</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2011</year>) 
<article-title>The NumPy array: a structure for efficient numerical computation</article-title>. <source>Comp. Sci. Eng</source>., <volume>13</volume>, <fpage>22</fpage>–<lpage>30</lpage>.</mixed-citation>
    </ref>
    <ref id="btaa591-B29">
      <mixed-citation publication-type="journal">The Cancer Genome Atlas Research Network. <etal>et al</etal> (<year>2013</year>) 
<article-title>The Cancer Genome Atlas Pan-Cancer analysis project</article-title>. <source>Nat. Genet</source>., <volume>45</volume>, <fpage>1113</fpage>.<pub-id pub-id-type="pmid">24071849</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolf</surname><given-names>F.A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) 
<article-title>SCANPY: large-scale single-cell gene expression data analysis</article-title>. <source>Genome Biol</source>., <volume>19</volume>, <fpage>15</fpage>.<pub-id pub-id-type="pmid">29409532</pub-id></mixed-citation>
    </ref>
    <ref id="btaa591-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zerbino</surname><given-names>D.R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) 
<article-title>WiggleTools: parallel processing of large collections of genome wide datasets for visualization and statistical analysis</article-title>. <source>Bioinformatics</source>, <volume>30</volume>, <fpage>1008</fpage>–<lpage>1009</lpage>.<pub-id pub-id-type="pmid">24363377</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
