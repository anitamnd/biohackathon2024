<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5751770</article-id>
    <article-id pub-id-type="publisher-id">1973</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-017-1973-5</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>GTZ: a fast compression and cloud transmission tool optimized for FASTQ files</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Xing</surname>
          <given-names>Yuting</given-names>
        </name>
        <address>
          <email>xingyuting16@nudt.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Li</surname>
          <given-names>Gen</given-names>
        </name>
        <address>
          <email>gen.li@genetalks.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Zhenguo</given-names>
        </name>
        <address>
          <email>zhenguo.wang@genetalks.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Feng</surname>
          <given-names>Bolun</given-names>
        </name>
        <address>
          <email>bolun.feng@genetalks.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Song</surname>
          <given-names>Zhuo</given-names>
        </name>
        <address>
          <email>zhuosong@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Wu</surname>
          <given-names>Chengkun</given-names>
        </name>
        <address>
          <email>Chengkun_wu@nudt.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9548 2110</institution-id><institution-id institution-id-type="GRID">grid.412110.7</institution-id><institution>School of Computer Science, National University of Defense Technology, </institution></institution-wrap>Changsha, 410000 China </aff>
      <aff id="Aff2"><label>2</label>Genetalks Biotech. Co.,Ltd., Beijing, 100000 China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>28</day>
      <month>12</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>12</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2017</year>
    </pub-date>
    <volume>18</volume>
    <issue>Suppl 16</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editors declare that they have no competing interests.</issue-sponsor>
    <elocation-id>549</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s). 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">The dramatic development of DNA sequencing technology is generating real big data, craving for more storage and bandwidth. To speed up data sharing and bring data to computing resource faster and cheaper, it is necessary to develop a compression tool than can support efficient compression and transmission of sequencing data onto the cloud storage.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">This paper presents GTZ, a compression and transmission tool, optimized for FASTQ files. As a reference-free lossless FASTQ compressor, GTZ treats different lines of FASTQ separately, utilizes adaptive context modelling to estimate their characteristic probabilities, and compresses data blocks with arithmetic coding. GTZ can also be used to compress multiple files or directories at once. Furthermore, as a tool to be used in the cloud computing era, it is capable of saving compressed data locally or transmitting data directly into cloud by choice. We evaluated the performance of GTZ on some diverse FASTQ benchmarks. Results show that in most cases, it outperforms many other tools in terms of the compression ratio, speed and stability.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">GTZ is a tool that enables efficient lossless FASTQ data compression and simultaneous data transmission onto to cloud. It emerges as a useful tool for NGS data storage and transmission in the cloud environment. GTZ is freely available online at: <ext-link ext-link-type="uri" xlink:href="https://github.com/Genetalks/gtz">https://github.com/Genetalks/gtz</ext-link>.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (10.1186/s12859-017-1973-5) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>FASTQ</kwd>
      <kwd>Compression</kwd>
      <kwd>General-purpose</kwd>
      <kwd>Lossless</kwd>
      <kwd>Parallel compression and transmission</kwd>
      <kwd>Cloud computing</kwd>
    </kwd-group>
    <conference>
      <conf-name>16th International Conference on Bioinformatics (InCoB 2017)</conf-name>
      <conf-acronym>InCoB 2017</conf-acronym>
      <conf-loc>Shenzhen, China</conf-loc>
      <conf-date>20-22 September 2017</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2017</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par4">Next generation sequencing (NGS) has greatly facilitated the development of genome analyses, which is vital for reaching the goal of precision medicine. Yet the exponential growth of accumulated sequencing data poses serious challenges to the transmission and storage of NGS data. Efficient compression methods provide the possibility to address this increasingly prominent problem.</p>
    <p id="Par5">Previously, general-propose compression tools, such as gzip (<ext-link ext-link-type="uri" xlink:href="http://www.gzip.org/">http://www.gzip.org/</ext-link>), bzip2 (<ext-link ext-link-type="uri" xlink:href="http://www.bzip.org/">http://www.bzip.org/</ext-link>) and 7z (<ext-link ext-link-type="uri" xlink:href="http://www.7-zip.org">www.7-zip.org</ext-link>), have been utilized to compress NGS data. These tools do not take advantage of the characteristics of genome data, such as a small size alphabet and repeated sequences segments, which leaves space for performance optimization. Recently, some specialized compression tools have been developed for NGS data. These tools are either reference-based or reference-free. The main difference lies in whether extra genome sequences are used as references. Reference-based algorithms encode the differences between the target and reference sequences, and consume more memory to improve compression performance. GenCompress [<xref ref-type="bibr" rid="CR1">1</xref>] and SimGene [<xref ref-type="bibr" rid="CR2">2</xref>] use various entropy encoders, such as arithmetic, Golomb and Huffman to compress integer values. The values show properties of reads, like starting position, length of reads, etc. A statistical compression method, GReEn [<xref ref-type="bibr" rid="CR3">3</xref>], uses an adaptive model to estimate probabilities based on the frequencies of characters. The probabilities are then compressed with an arithmetic encoder. QUIP [<xref ref-type="bibr" rid="CR4">4</xref>] exploits arithmetic coding associated with models of order-3 and high-order Markov chains in all three parts of FASTQ data. LW-FQZip [<xref ref-type="bibr" rid="CR5">5</xref>] utilized incremental and run-length-limited encoding schemes to compress the metadata and quality scores, respectively. Reads are pre-processed by a light-weight mapping model and then three components are combined to be compressed by a general-purpose tool, like LZMA. Fqzcomp [<xref ref-type="bibr" rid="CR6">6</xref>] estimates character probabilities by order-k context modelling and compresses NGS data in FASTQ format with the help of arithmetic coders.</p>
    <p id="Par6">Nevertheless, reference-based algorithms can be inefficient if the similarity between target and reference sequences is low. Therefore, reference-free methods were also proposed to address this problem. Biocompress proposed in [<xref ref-type="bibr" rid="CR7">7</xref>] is a compression method dedicated to genomic sequences. Its main idea is based on the classical dictionary-based compression method --the Ziv and Lempel [<xref ref-type="bibr" rid="CR8">8</xref>] compression algorithm. Repeats and palindromes are encoded using the length and the position of their earliest occurrences. As an extension of biocompress [<xref ref-type="bibr" rid="CR7">7</xref>], biocompress-2 [<xref ref-type="bibr" rid="CR9">9</xref>] exploits the same scheme, and uses arithmetic coding of order-2 when no significant repetition exists. The DSRC [<xref ref-type="bibr" rid="CR10">10</xref>] algorithm splits sequences into blocks and compresses them independently with LZ77 [<xref ref-type="bibr" rid="CR8">8</xref>] and Huffman [<xref ref-type="bibr" rid="CR11">11</xref>] encoding. It is faster than QUIP both in compression and decompression speed, but inferior to the later in terms of compression ratio. DSRC2 [<xref ref-type="bibr" rid="CR12">12</xref>], the multithreaded version of DSRC [<xref ref-type="bibr" rid="CR10">10</xref>], splits the input into three streams for pre-processing. After pre-processing, metadata, reads, and quality scores are compressed separately in DRSC. A boosting algorithm, SCALCE [<xref ref-type="bibr" rid="CR13">13</xref>], which re-organizes the reads, can outperform other algorithms on most datasets both in the compression ratio and the compression speed.</p>
    <p id="Par7">Nowadays, it is evident that cloud computing has become increasingly important for genomic analyses. However, above-mentioned tools were developed for local usage. Compression has to be completed locally before a data transmission onto the cloud can begin.</p>
    <p id="Par8">AdOC proposed in [<xref ref-type="bibr" rid="CR14">14</xref>] is a general-propose tool that allows the overlap of compression and communication in the context of a distributed computing environment. It presents a model for transport level compression with dynamic compression level adaptation, which can be used in an environment where resource availability and bandwidth vary unpredictably.</p>
    <p id="Par9">Generally, the compression performances of the universal compression algorithms, such as AdOC, are unsatisfactory for NGS datasets.</p>
    <p id="Par10">In this paper, we present a tool GTZ, it is characterized as a lossless and efficient compression tool to be used jointly with cloud computing for large-scale genomic data analyses:<list list-type="order"><list-item><p id="Par11">GTZ exploits context model technology combined with multiple prediction modelling schemes. It employs paralleling processing to improve the compression speed.</p></list-item><list-item><p id="Par12">GTZ can compress directories or folders into a single archive, which is called a multi stream file system. The all-in-one scheme can satisfy purposes of transmission, validation and storage.</p></list-item><list-item><p id="Par13">GTZ supports random access to files or archives. GTZ utilizes block storage, such that users can extract some parts of genome sequences out of a FASTQ file or some files in a folder, without a complete decompression of the compressed archive.</p></list-item><list-item><p id="Par14">GTZ can transfer compressed blocks to the cloud storage while the compress is still in process, which is a novel feature compared with other compression tools. This feature enables the data transmission time to be can greatly reduce the total time needed for compression and data transmission onto the cloud. For instance, it could compress and transit a 200GB FASTQ file to cloud storages like AWS and Alibaba cloud storage within 14 min.</p></list-item><list-item><p id="Par15">GTZ provides a Python API, through which users can integrate GTZ in their own applications flexibly.</p></list-item></list>
</p>
    <p id="Par16">In the remaining of this paper, we will introduce how GTZ works and evaluate its performance on several benchmark datasets using the AWS service.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par17">GTZ supports efficient compression in parallel, parallel transmission and random fetching. Figure <xref rid="Fig1" ref-type="fig">1</xref> demonstrates the workflow of GTZ processing.<fig id="Fig1"><label>Fig. 1</label><caption><p>The workflow of GTZ</p></caption><graphic xlink:href="12859_2017_1973_Fig1_HTML" id="MO1"/></fig>
</p>
    <p id="Par18">GTZ involves procedures on clients and the cloud end.</p>
    <p id="Par19">A client takes the following steps:<list list-type="order"><list-item><p id="Par20">Read in streams of large data files.</p></list-item><list-item><p id="Par21">Pre-process the input by dividing data streams into three sub-streams: metadata, base sequence, and quality score.</p></list-item><list-item><p id="Par22">Buffer sub-streams in local memories and assemble them into different types of data blocks with a fixed size.</p></list-item><list-item><p id="Par23">Compress assembled data blocks and their descriptions, and then transmit output blocks into the cloud storage.</p></list-item></list>
</p>
    <p id="Par24">On the cloud, the followings steps are executed:<list list-type="order"><list-item><p id="Par25">Create three types of object-oriented containers (shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>), which define a tree structure.</p></list-item><list-item><p id="Par26">Loop and wait to receive output blocks sent by the client.</p></list-item><list-item><p id="Par27">Save received output blocks into block containers according to their types.</p></list-item><list-item><p id="Par28">Stop if no more output blocks are received.</p></list-item></list>
<fig id="Fig2"><label>Fig. 2</label><caption><p>The hierarchy of data containers</p></caption><graphic xlink:href="12859_2017_1973_Fig2_HTML" id="MO2"/></fig>
</p>
    <p id="Par29">We will explain all the steps in further details about processing FASTQ files below:</p>
    <sec id="Sec3">
      <title>The client reading streams of large data files</title>
      <p id="Par30">Raw NGS data files are typically stored in FASTQ format for the convenience of compression. A typical FASTQ file contains four lines per sequence: Line 1 begins with a character ‘@’ followed by a sequence identifier; Line 2 holds the raw sequence composed of A, C, T, and G; line 3 begins with a character ‘+’ and is optionally followed by the same sequence identifier (and any description) again; line 4 holds the corresponding quality scores in ASCII characters for the sequence characters in line 2. An example of a read is given in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The format of an FASTQ file</p></caption><table frame="hsides" rules="groups"><thead><tr><th>1</th><th>@ERR194147.1.HSQ1004:134:C0D8DACXX:1:1104:3874:86,238/1</th></tr></thead><tbody><tr><td>2</td><td>GGTTCCTACTTNAGGGTCATTAAATAGCCCACACGTC</td></tr><tr><td>3</td><td>+</td></tr><tr><td>4</td><td>CC@FFFFFHHH#JJJFHIIJJJJJJJIJHIJJJJJJJ</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec4">
      <title>Data pre-processing</title>
      <p id="Par31">During the second step, a data stream is split into metadata sub-streams, base sequence sub-streams and quality score sub-streams. (Since uninformative comment lines normally do not provide any useful information for compression, comment streams are omitted during pre-processing.) Three types of date pre-processing controllers buffer sub-streams and save them in data blocks at a fixed size respectively. Afterwards, data blocks with annotations (about numbers of blocks, sizes of blocks and types of streams) are sent to corresponding compression units. Figure <xref rid="Fig3" ref-type="fig">3</xref> demonstrates how to pre-process data files with the help of pre-processing controllers and compression units.<fig id="Fig3"><label>Fig. 3</label><caption><p>Pre-process data files with pre-processing controllers and compression units</p></caption><graphic xlink:href="12859_2017_1973_Fig3_HTML" id="MO3"/></fig>
</p>
    </sec>
    <sec id="Sec5">
      <title>Compressing data</title>
      <p id="Par32">GTZ is a general-purpose compression tool that uses statistical modelling (<ext-link ext-link-type="uri" xlink:href="http://marknelson.us/1991/02/01/arithmetic-coding-statistical-modeling-data-compression">http://marknelson.us/1991/02/01/arithmetic-coding-statistical-modeling-data-compression/</ext-link>) and arithmetic coding.</p>
      <p id="Par33">Statistical modelling can be categorized into two types: static and adaptive statistical modelling. Conventional methods are normally static, which means probabilities are calculated after sequences are scanned from the beginning to end. A static modelling keeps a static table that records character-frequency counts. Although they produce relatively accurate results, the drawbacks are obvious:<list list-type="order"><list-item><p id="Par34">It is time-consuming to read all the sequences into main memory before compression.</p></list-item><list-item><p id="Par35">If an input stream does not match well with the previously accumulated sequence, the compression ratio will be degraded, even the output stream will become larger than the input stream.</p></list-item></list>
</p>
      <p id="Par36">In GTZ, we employ an adaptive statistical data compression technique based on context modelling. An adaptive modeling needs not to scan the whole sequence and generate probabilities before coding. Instead, the adaptive prediction technology provides on-the-fly reading and compression, that is probabilities are calculated based on the characters already read into the memory. Probabilities may alter with more characters scanned. Initially, the performance of adaptive statistical modelling may be poor due to the lack of reads. However, with more sequences processed, the prediction tends to be more accurate.</p>
      <p id="Par37">Every time the compressor encodes a character, it will update the counter in the prediction table. When a new character X (suppose the sequence before X is ABCD) comes, GTZ will traverse the prediction table, find every character that has followed ABCD before, and compare their appearance frequencies. For instance, if both ABCDX appears 10 times, and ABCDY only once. Then GTZ will assign a higher probability for X.</p>
      <p id="Par38">The work flow of an adaptive model is depicted in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. The box ‘Update model’ means converting low-order modellings to high-order modellings (the meaning of low-order and high-order will be discussed in the next subsection.).<fig id="Fig4"><label>Fig. 4</label><caption><p>Work flow of a typical statistical modelling</p></caption><graphic xlink:href="12859_2017_1973_Fig4_HTML" id="MO4"/></fig>
</p>
      <p id="Par39">Adaptive prediction modelling can effectively reduce compression time. There is no need to read all sequences in a time and it introduces overlap of scanning and compression.</p>
      <p id="Par40">GTZ utilizes specific compression units for different kinds of data blocks: a low-order encoder for genetic sequences, a multi-order encoder for quality scores and mixed encoders for metadata. Finally, the outputs in this procedure are blocks at a fixed size.</p>
      <p id="Par41">The main idea about arithmetic coding is to convert reads into a floating point ranging from zero to one (precisely greater than or equal to zero and less than one) based on the predictive probabilities of characters. If the statistical modelling estimates every single character accurately for the compressor, we will have high compression performance. On the contrary, a poor prediction may result in expansion of the original sequence, instead of compression. Thus, the performance of a compressor largely relies on the whether the statistical modelling can output near-optimal predictive probabilities.</p>
    </sec>
    <sec id="Sec6">
      <title>A low-order encoder for reads</title>
      <p id="Par42">The simplest implementation of adaptive modeling is order-0. Exactly, it does not consider any context information, thus this short-sighted modeling can only see the current character and make prediction that is independent of the previous sequences. Similarly, an order-1 encoder makes prediction based on one preceding character. Consequently, the low-order modeling makes little contribution to the performance of compressors. Its main advantage is that it is very memory efficient. Hence, for quality score streams that do not have spatial locality, a low-order modeling is adequate for moderate compression rate.</p>
      <p id="Par43">Our tailored low-order encoder for reads is demonstrated in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. The first step is to transform sequences with the BWT algorithm. BWT (Burrows-Wheeler transform) rearranges reads into runs of similar characters. In the second step, the zero-order and the first-order prediction model are used to calculate appearance probability of each character. Since a poor probability accuracy contributes to undesirable encoding results, we add interpolation after quantizing the weighted average probability, to reduce prediction errors and improve compression ratios. In the last procedure, the bit arithmetic coding algorithm produces decimals ranging from zero to one as outputs to represent sequences.<fig id="Fig5"><label>Fig. 5</label><caption><p>A low-order encoder scheme</p></caption><graphic xlink:href="12859_2017_1973_Fig5_HTML" id="MO5"/></fig>
</p>
    </sec>
    <sec id="Sec7">
      <title>A multi-order encoder for quality scores</title>
      <p id="Par44">The statistical modeling needs non-uniform probability distribution for arithmetic algorithms. The high-order modeling enables high probabilities for those characters which appear frequently, and low probabilities for those which appear infrequently. As a result, compared with low-order encoders, higher-order encoders can enhance adaptive modeling.</p>
      <p id="Par45">A high-order modeling considers several characters preceding the current position. It can obtain better compression performance at the expense of more memory usage. Higher-order modeling was less used due to the limited memory capacity, which is no longer a problem anymore.</p>
      <p id="Par46">Without transformation, a multi-order encoder (See Fig. <xref rid="Fig6" ref-type="fig">6</xref>) for quality scores includes two procedures:<fig id="Fig6"><label>Fig. 6</label><caption><p>A multi-order encoder scheme</p></caption><graphic xlink:href="12859_2017_1973_Fig6_HTML" id="MO6"/></fig>
</p>
      <p id="Par47">Firstly, to generate probabilities of characters, input stream flows through an expanding character probability prediction model, which is composed of first-order, second-order, fourth-order, sixth-order prediction models and a matching model. Like a low-order encoder, probabilities of characters undergo weighted averaging, quantization and interpolation to obtain final results. Secondly, we use bit arithmetic coding algorithm for compression.</p>
    </sec>
    <sec id="Sec8">
      <title>A hybrid scheme for metadata</title>
      <p id="Par48">For metadata sub-streams, GTZ first uses delimiters (punctuations) to split them into different segments, then uses different ways to process metadata according to their fields:</p>
      <p id="Par49">For numbers in an ascending or descending order, we employ incremental encoding to represent the variations of one metadata to its preceding neighbors. For instance, ‘3458644’ will be compressed into 3,1,1,3,-2,-2,0. For continuous identical characters, we exploit run-length limited encoding to show their values and numbers of repetition. For random numbers with various precisions, we convert their formats by UTF-8 coding without adding a single separator, and then use a low-order encoder for compression. Otherwise, use the low-order encoder to compress metadata.</p>
      <p id="Par50">In conclusion, during this process, sub-streams are fed into a dynamic probability prediction model and an arithmetic encoder, and they are transformed into compressed blocks at a fixed size.</p>
    </sec>
    <sec id="Sec9">
      <title>Data transmission</title>
      <p id="Par51">The key objective is to transmit output blocks to a certain cloud storage platform, with annotations about types, sizes, numbers of data blocks.</p>
      <p id="Par52">To note, different types of encoders may lead to inconsistency in compression speed, which can lead to a data pipe blockage. Thus, in our system, the pipe-filter pattern is designed to synchronize input and output speed, e.g., the input flow will be blocked when the speed of input stream is faster than that of the output stream; The pipe will also be blocked when there is no input flow.</p>
    </sec>
    <sec id="Sec10">
      <title>Storage at the cloud end — Creating an object-oriented nested container system</title>
      <p id="Par53">GTZ creates containers as storage compartments that provide a way to manage instances and store file directories. They are organized in a tree structure. Containers can be nested to represent locations of instances: a root container represents a complete compressed file; a block container includes different types of sub-stream containers where specific instances are stored. The nesting structure is showed in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.</p>
      <p id="Par54">A root container represents a FASTQ file and it holds N block containers, each of which includes metadata sub-containers, base sequence sub-containers and quality score sub-containers. A metadata sub-container nests repetitive data blocks, random data blocks, incremental data blocks, etc. Base sequence sub-containers and quality score sub-containers nest 0 instance block to N instance block. Taking base sequences for examples, the 0 to (N-1) output blocks are stored in the 0th block container, and the N to (2 N-1) output blocks are stored in the 1st block container, and so on.</p>
      <p id="Par55">This kind of hierarchy allows users to maintain a directory structure to manage compressed files, thereby facilitating random access to specific sequence. Here, we show how to decompress and extract the target files from the compressed archive: in decompression mode, the system will index the start line number <italic>n</italic> (which is given by users through the command line), then fetch the certain sequence from their according block containers and compress certain (which are also specified by users) lines of the sequence.</p>
    </sec>
    <sec id="Sec11">
      <title>Receive data — Receive and store output blocks</title>
      <p id="Par56">Cloud storage platform receives output blocks and descriptive information such as numbers of data blocks, sizes of data blocks, most importantly, the line number of every base sequence within data blocks. The description enables us to directly index certain sequences with line numbers and decode their affiliated blocks rather than extract the whole file. Output blocks are stored in corresponding types of containers.</p>
      <p id="Par57">What is worth noting is that non-FASTQ files can also be compressed and transmitted through GTZ. Additionally, GTZ uses object-oriented programming, it is not restricted to interact with a specific type of cloud storage platform, but applicable to most existing cloud storage platforms, such as the Amazon Web Service and the Alibaba cloud.</p>
    </sec>
  </sec>
  <sec id="Sec12">
    <title>Results and discussion</title>
    <p id="Par58">In this section, we conducted experiments on a 32-core AWS R4.8xlarge instance with 244GB of memory to evaluate the performance of GTZ in terms of compression ratio and compression speed. During the experiments, the following points should be noted:<list list-type="order"><list-item><p id="Par59">Considering that our method is lossless, we exclude methods that allow losses as counterparts.</p></list-item><list-item><p id="Par60">NGS data can be stored in either FASTQ or SAM/BAM formats, we only take into account tools targeted at FASTQ format files.</p></list-item><list-item><p id="Par61">Comparison will be conducted among the algorithms that do not reorder input sequences.</p></list-item></list>
</p>
    <p id="Par62">We carried out tests on 8 publicly accessible FASTQ datasets, which are downloaded from the Sequence Read Archive(SRA) initiated by NCBI and the GCTA competition website (<ext-link ext-link-type="uri" xlink:href="https://tianchi.aliyun.com/mini/challenge.htm#training-profile)">https://tianchi.aliyun.com/mini/challenge.htm#training-profile</ext-link>). To ensure the comprehensiveness of our evaluation, we chose datasets that are heterogeneous: the size of datasets ranges from 556MBs to 202, 631MBs; different species and different types of data were chosen, including DNA reads, one RNA-seq dataset of <italic>Homo sapiens</italic>, one metagenome dataset and read 2 of NA12878 (the GCTA competition datasets). Different quality score encoding methods, such as Sanger and Illumina 1.8+, are selected to cover different numbers of quality scores in datasets. Quality scores are logarithmically linked to error probabilities, leading to a larger alphabet than meta data and reads, thus encodings with small numbers of quality scores normally contribute to a higher compression performance. Descriptions of the datasets are listed in Table <xref rid="Tab2" ref-type="table">2</xref>. Besides, for comparison, based on a comprehensive literature survey, we selected four state-of-the-art and widely-used lossless compression algorithms, including DSRC2 [<xref ref-type="bibr" rid="CR12">12</xref>] (the improved version of DSRC [<xref ref-type="bibr" rid="CR10">10</xref>]), quip [<xref ref-type="bibr" rid="CR4">4</xref>], LW-FQZip [<xref ref-type="bibr" rid="CR5">5</xref>], Fqzcomp [<xref ref-type="bibr" rid="CR6">6</xref>], LFQC [<xref ref-type="bibr" rid="CR15">15</xref>] and pigz. Among them, LW-FQZip [<xref ref-type="bibr" rid="CR5">5</xref>], Fqzcomp [<xref ref-type="bibr" rid="CR15">15</xref>] are representatives of reference-based tools; DSRC2 [<xref ref-type="bibr" rid="CR12">12</xref>] and quip [<xref ref-type="bibr" rid="CR4">4</xref>] are reference-free methods; pigz is a general-purpose tool for compression. All the experimental results are included in Additional file <xref rid="MOESM1" ref-type="media">1</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Descriptions of 8 FASTQ datasets used for performance evaluation</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Dataset</th><th>Species</th><th>Reference genome size</th><th>Encoding</th><th>No. of quality scores in data file</th></tr></thead><tbody><tr><td>ERR233152</td><td><italic>P. aeruginosa</italic></td><td>556</td><td>Sanger</td><td>32</td></tr><tr><td>SRR935126</td><td><italic>A. thaliana</italic></td><td>9755</td><td>Sanger</td><td>39</td></tr><tr><td>SRR489793</td><td><italic>C. elegans</italic></td><td>12,807</td><td>Illumina 1.8+</td><td>38</td></tr><tr><td>SRR801793</td><td>L. pneumophila</td><td>2756</td><td>Sanger</td><td>38</td></tr><tr><td>SRR125858</td><td><italic>H. sapiens</italic></td><td>50,744</td><td>Sanger</td><td>39</td></tr><tr><td>SRR5419422</td><td>RNA seq (H. sapiens)</td><td>15,095</td><td>Illumina 1.8+</td><td>6</td></tr><tr><td>ERR1137269</td><td>metagenomes</td><td>56,543</td><td>Illumina 1.8+</td><td>7</td></tr><tr><td>NA12878 (read 2)</td><td>H. sapiens</td><td>202,631</td><td>Sanger</td><td>38</td></tr></tbody></table></table-wrap>
</p>
    <sec id="Sec13">
      <title>Evaluation results</title>
      <p id="Par63">We evaluated the performance of different tools by the following related metrics: the compression ratio, the coefficient of variation (CV) of compression ratios, the compression speed, the total time of compression and transmission to cloud storages. Specifically, the compression ratio is defined as follows:</p>
      <p id="Par64">According to this definition, a smaller compression ratio represents a more effective compression in terms of size reduction; The coefficient of variation (CV) stands for the extent of variability in relation to the mean and it is defined as the ratio of the standard deviation (SD) divided by the average (avg):</p>
      <p id="Par65">A smaller CV reveals better robustness and stability; additionally, GTZ not only performs well in compression on local computers, but also gains satisfactory results in transmission to cloud storages. On local computers, the compression speed is chosen for evaluation, and it can be simply measured by the time used for the compression (for different tools applied on the same data). Under the latter circumstance, the run time of algorithms should be the sum of compression and transmission time, namely, from the start of compression to the completion of transmission onto the cloud.</p>
    </sec>
    <sec id="Sec14">
      <title>Compression ratio</title>
      <p id="Par66">Performance evaluation results are demonstrated in Table <xref rid="Tab3" ref-type="table">3</xref> and the best compression ratio, the best CV, which are the smallest, are boldfaced. Comparative results of CV are shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Compression ratios of different tools on 8 FASTQ datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Dataset</th><th colspan="7">Compression ratio (%)</th></tr><tr><th>GTZ</th><th>DSRC2</th><th>QUIP</th><th>LW-FQZip</th><th>Fqzcomp</th><th>LFQC</th><th>pigz</th></tr></thead><tbody><tr><td>ERR233152</td><td>15.9</td><td>16.7</td><td>19</td><td>19</td><td>16.8</td><td><bold>8</bold></td><td>26.4</td></tr><tr><td>SRR935126</td><td>18.6</td><td>19.6</td><td>17.7</td><td>20.5</td><td>17.8</td><td><bold>9.9</bold></td><td>30.2</td></tr><tr><td>SRR489793</td><td>22.8</td><td>22.7</td><td>22.6</td><td>25.5</td><td>22.5</td><td><bold>12.8</bold></td><td>34.4</td></tr><tr><td>SRR801793</td><td>21.4</td><td>21.9</td><td>21.1</td><td>21.2</td><td>20.8</td><td><bold>12.3</bold></td><td>34.1</td></tr><tr><td>SRR125858</td><td>19.4</td><td>19.5</td><td>18.9</td><td>23.1</td><td>28.9</td><td><bold>17.6</bold></td><td>31</td></tr><tr><td>SRR5419422</td><td>12.8</td><td>13.9</td><td><bold>10.9</bold></td><td>12.5</td><td>12</td><td>ERROR</td><td>22</td></tr><tr><td>ERR1137269</td><td>12.2</td><td>13.4</td><td>12.8</td><td>14.3</td><td><bold>11.9</bold></td><td>ERROR</td><td>21.9</td></tr><tr><td>NA12878 (read 2)</td><td><bold>19.8</bold></td><td>24</td><td>20.4</td><td>TLE</td><td>19.9</td><td>TLE</td><td>24.7</td></tr><tr><td>avg</td><td>17.86</td><td>18.96</td><td>17.93</td><td>19.44</td><td>18.83</td><td><bold>12.12</bold></td><td>28.09</td></tr><tr><td>SD</td><td>3.87</td><td>3.97</td><td>4.07</td><td>4.64</td><td>5.60</td><td>3.62</td><td>5.05</td></tr><tr><td>CV</td><td>0.22</td><td>0.21</td><td>0.23</td><td>0.24</td><td>0.30</td><td>0.30</td><td><bold>0.18</bold></td></tr></tbody></table><table-wrap-foot><p>The best results of all the tools are boldfaced</p></table-wrap-foot></table-wrap>
<fig id="Fig7"><label>Fig. 7</label><caption><p>CVs for the compression ratio of different tools</p></caption><graphic xlink:href="12859_2017_1973_Fig7_HTML" id="MO7"/></fig>
</p>
      <p id="Par67">To note, in Table <xref rid="Tab3" ref-type="table">3</xref>, some fields on datasets NA12878 (read 2, a very large dataset) are filled with “TLE” (Time Limit Exceeded, the threshold is empirically set as 6 h), and some fields of the LFQC tools on the SRR5419422, ERR137269 datasets are filled with “Error” (Cannot decompress after compression, those two datasets represent RNA sequences and metagenomics data respectively). Those “outliers” represent a low robustness (for convenience of CV calculation, we just filter out “TLE” and “Error”). For instance, LFQC [<xref ref-type="bibr" rid="CR15">15</xref>] yields the best result on 5 out of 8 datasets. However, it got “TLE” on three datasets, which means a poor stability in compression efficiency. In addition, despite the CV of pigz is the lowest, its average compression ratio ranks at the bottom. Moreover, GTZ ranks second with an average compression ratio of 17.86%, and the CV of GTZ is far below that of LFQC [<xref ref-type="bibr" rid="CR15">15</xref>] (which has the best compression ratio). In summary, GTZ not only maintains a relatively good average compression ratio than most of its counterparts, but also exhibits better stability and robustness when dealing with different datasets.</p>
    </sec>
    <sec id="Sec15">
      <title>Compression speed</title>
      <p id="Par68">Results for the compression speed tests are shown in Table <xref rid="Tab4" ref-type="table">4</xref> and the best results are boldfaced. LFQC [<xref ref-type="bibr" rid="CR15">15</xref>] and LW-FQZip [<xref ref-type="bibr" rid="CR5">5</xref>] fail to compress the GCTA dataset NA12878 (read 2) within 6 h(21,600 s, which is empirically set). On datasets SRR5419422 and ERR137269, compressed files generated by LFQC cannot be decompressed, which are considered as errors (possibly because SRR5419422 is a RNA dataset and ERR137269 is a metagenomics dataset). Table <xref rid="Tab4" ref-type="table">4</xref> reveals that the reference-based methods LW-FQZip [<xref ref-type="bibr" rid="CR5">5</xref>] and LFQC [<xref ref-type="bibr" rid="CR15">15</xref>] are very slow on large datasets like NA12878 (read 2). DSRC2 [<xref ref-type="bibr" rid="CR12">12</xref>], which is the representative of reference-free methods, performs best in terms of the average compression speed. GTZ ranks second in terms of compression time.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Compression time of different tools on 8 FASTQ datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Dataset</th><th rowspan="2">Size (MB)</th><th colspan="7">Compression Time (s)</th></tr><tr><th>GTZ</th><th>DSRC2</th><th>QUIP</th><th>LW-FQZip</th><th>Fqzcomp</th><th>LFQC</th><th>pigz</th></tr></thead><tbody><tr><td>ERR233152</td><td>556.1</td><td>19</td><td>13</td><td>10</td><td>284</td><td>13</td><td>297</td><td><bold>3</bold></td></tr><tr><td>SRR935126</td><td>9754.6</td><td>49</td><td><bold>40</bold></td><td>195</td><td>3966</td><td>191</td><td>3610</td><td>129</td></tr><tr><td>SRR489793</td><td>12,807</td><td>51</td><td><bold>49</bold></td><td>343</td><td>4893</td><td>289</td><td>4253</td><td>122</td></tr><tr><td>SRR801793</td><td>2756.2</td><td>43</td><td>28</td><td>59</td><td>1212</td><td>73</td><td>1143</td><td><bold>22</bold></td></tr><tr><td>SRR125858</td><td>50,744.2</td><td>178</td><td><bold>153</bold></td><td>1044</td><td>18,300</td><td>977</td><td>10,202</td><td>481</td></tr><tr><td>SRR5419422</td><td>15,094.6</td><td>26</td><td><bold>7</bold></td><td>329</td><td>4234</td><td>267</td><td>ERROR</td><td>67</td></tr><tr><td>ERR1137269</td><td>56,543</td><td>117</td><td><bold>32</bold></td><td>806</td><td>12,018</td><td>851</td><td>ERROR</td><td>213</td></tr><tr><td>NA12878 (read 2)</td><td>202,631</td><td>820</td><td>700</td><td>4703</td><td>TLE</td><td>4389</td><td>TLE</td><td><bold>620</bold></td></tr><tr><td>Average speed (MB/s)</td><td/><td>267.4</td><td><bold>648.8</bold></td><td>49.7</td><td>2.9</td><td>49.6</td><td>33.7</td><td>176.8</td></tr></tbody></table><table-wrap-foot><p>The best results of all the tools are boldfaced</p></table-wrap-foot></table-wrap>
</p>
      <p id="Par69">However, we are mostly interested in the total time of compression and transmission. Under the condition where the data transmission throughput is 10Gb/s (1.25 GB/s at best of AWS settings), we tested and estimated the total time of all tools and the results are listed in Table <xref rid="Tab5" ref-type="table">5</xref>. To note, this is a very optimistic optimization. Here, only GTZ supports data upload while compressing, other tools have to finish compression before submission. We can see the average compression and upload speed of GTZ (269.3 MB/s) is the highest, DSRC2 comes second with an average speed of 269.1 MB/s. In general, if the input data size is very large, GTZ will be even faster than DSRC2: 7% faster in the case of the SRR125858 dataset (a 50GB dataset).<table-wrap id="Tab5"><label>Table 5</label><caption><p>Total time of different tools on 8 FASTQ datasets with maximum bandwidth</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Dataset</th><th rowspan="2">Size (MB)</th><th colspan="7">Compression Time (s) + Data best upload time</th></tr><tr><th>GTZ</th><th>DSRC2</th><th>QUIP</th><th>LW-FQZip</th><th>Fqzcomp</th><th>LFQC</th><th>pigz</th></tr></thead><tbody><tr><td>ERR233152</td><td>556.1</td><td>19.0</td><td>13.4</td><td>10.4</td><td>284.4</td><td>13.4</td><td>297.4</td><td><bold>3.4</bold></td></tr><tr><td>SRR935126</td><td>9754.6</td><td>49.0</td><td><bold>48.8</bold></td><td>202.8</td><td>3973.8</td><td>198.8</td><td>3617.8</td><td>136.8</td></tr><tr><td>SRR489793</td><td>12,807</td><td><bold>51.0</bold></td><td>59.2</td><td>353.2</td><td>4903.2</td><td>299.2</td><td>4263.2</td><td>132.2</td></tr><tr><td>SRR801793</td><td>2756.2</td><td>43.0</td><td>30.2</td><td>61.2</td><td>1214.2</td><td>75.2</td><td>1145.2</td><td><bold>24.2</bold></td></tr><tr><td>SRR125858</td><td>50,744.2</td><td><bold>178.0</bold></td><td>193.6</td><td>1084.6</td><td>18,340.6</td><td>1017.6</td><td>10,242.6</td><td>521.6</td></tr><tr><td>SRR5419422</td><td>15,094.6</td><td>26.0</td><td><bold>19.1</bold></td><td>341.1</td><td>4246.1</td><td>279.1</td><td>ERROR</td><td>79.1</td></tr><tr><td>ERR1137269</td><td>56,543</td><td>117.0</td><td><bold>77.2</bold></td><td>851.2</td><td>12,063.2</td><td>896.2</td><td>ERROR</td><td>258.2</td></tr><tr><td>NA12878 (read 2)</td><td>202,631</td><td>820.0</td><td>862.1</td><td>4865.1</td><td>TLE</td><td>4551.1</td><td>TLE</td><td><bold>782.1</bold></td></tr><tr><td>Average speed (MB/s)</td><td/><td><bold>269.3</bold></td><td>269.1</td><td>45.2</td><td>7.8</td><td>47.9</td><td>17.9</td><td>181.1</td></tr></tbody></table><table-wrap-foot><p>The best results of all the tools are boldfaced</p></table-wrap-foot></table-wrap>
</p>
      <p id="Par70">To note, the upload time are estimated with the maximum bandwidth, while in practice, the upload speed could be much slower than that. To verify this, we carried out a real upload test using the relatively big dataset, SRR125858_2.fastq (about half of the SRR125858 dataset), which is 25GBs in size. The compression ratios of GTZ and DSRC2 happen to be the same on this dataset. It took GTZ 99 s to finish compression and transmission, while it took 122 s for DSRC2. Our optimistic estimation of a fast upload takes only 20.3 s, whereas in practice, it took about 45 s. The details are listed in Table <xref rid="Tab6" ref-type="table">6</xref>.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Total time of different tools on the SRR125858_2 dataset in a real test</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Metrics</th><th colspan="7">Comparative methods</th></tr><tr><th>GTZ</th><th>DSRC2</th><th>QUIP</th><th>LW-FQZip</th><th>Fqzcomp</th><th>LFQC</th><th>pigz</th></tr></thead><tbody><tr><td>Compression ratio (%)</td><td>19.2</td><td>19.2</td><td>18.7</td><td>23.2</td><td>28.7</td><td>18</td><td>30.7</td></tr><tr><td align="justify">Total time (s)</td><td>99</td><td>122</td><td>553</td><td>9283</td><td>549</td><td>4982</td><td>324</td></tr></tbody></table></table-wrap>
</p>
      <p id="Par71">In Table <xref rid="Tab7" ref-type="table">7</xref>, we present a qualitative performance summary of all tools. The parameters, high, moderate, and low show the comparison between different tools. Compression ratio of a tool is said to be high if it is the best compressor or close to the known best algorithm. GTZ achieves satisfactory results both in compression ratio and compression speed (as well as the total time considering data upload) on tested datasets.<table-wrap id="Tab7"><label>Table 7</label><caption><p>Qualitative performance summary</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Algorithm</th><th>Compression speed</th><th>Compression ratio</th></tr></thead><tbody><tr><td>GTZ</td><td>High</td><td>Moderate</td></tr><tr><td>DSRC2</td><td>High</td><td>Moderate</td></tr><tr><td>QUIP</td><td>Moderate</td><td>Moderate</td></tr><tr><td>LW-FQZip</td><td>Low</td><td>Moderate</td></tr><tr><td>Fqzcomp</td><td>Moderate</td><td>Low</td></tr><tr><td>LFQC</td><td>Moderate</td><td>Low</td></tr><tr><td>pigz</td><td>High</td><td>High</td></tr></tbody></table></table-wrap>
</p>
    </sec>
    <sec id="Sec16">
      <title>Compression rate on different data sections</title>
      <p id="Par72">The compression rates of GTZ on the three sections of a FASTQ file are reported in Table <xref rid="Tab8" ref-type="table">8</xref>.<table-wrap id="Tab8"><label>Table 8</label><caption><p>The compression ratio of GTZ on the three components of FASTQ files</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Dataset</th><th colspan="3">Compression ratio (%)</th></tr><tr><th>Metadata</th><th>Reads</th><th>Quality scores</th></tr></thead><tbody><tr><td>ERR233152</td><td>2.62</td><td>20.6</td><td>20.8</td></tr><tr><td>SRR935126</td><td>3.29</td><td>22.2</td><td>25.3</td></tr><tr><td>SRR489793</td><td>0.01</td><td>22.7</td><td>29.95</td></tr><tr><td>SRR801793</td><td>3.73</td><td>23.15</td><td>31.1</td></tr><tr><td>SRR125858</td><td>2.81</td><td>23.3</td><td>28.25</td></tr><tr><td>SRR5419422</td><td>0.01</td><td>22.9</td><td>9.5</td></tr><tr><td>ERR1137269</td><td>3.23</td><td>24.05</td><td>19.35</td></tr><tr><td>NA12878 (read 2)</td><td>7.59</td><td>20.4</td><td>27.3</td></tr><tr><td>Average</td><td>2.91</td><td>22.39</td><td>23.94</td></tr></tbody></table></table-wrap>
</p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Conclusions</title>
    <p id="Par73">The dramatic development of NGS technology has brought about challenge to store and transmit genome sequences. Efficient compression tools are feasible solutions to address this problem. Therefore, an efficient lossless compression tool for cloud computing of FASTQ files, GTZ, was proposed in this paper. GTZ is the champion winning solution of the GCTA competition (Reports can be found at <ext-link ext-link-type="uri" xlink:href="http://vcbeat.net/35028.html">http://vcbeat.net/35028.html</ext-link>. GTZ integrates the context modeling technology with multiple prediction modelling schemes. It also introduces the ability of paralleling processing technique for improved and steady efficiency of compression. Moreover, it enables random access to some certain specific reads. By virtue of block storage, users are allowed to only compress and read some parts of genome sequences, without the need for a complete decompression of the original FASTQ file. Another important feature is that it can overlap the data transmission with the compression process, which can greatly reduce the total time needed.</p>
    <p id="Par74">We evaluated the performance of GTZ on eight real-world FASTQ datasets and compared it with other state-of-the-art tools. Experimental results validate that GTZ performs well in terms of both compression rate and compression speed and its performance is steady across different datasets. GTZ managed to compress and transfer a 200GB FASTQ file to cloud storages like AWS and Alibaba cloud within 14 min.</p>
    <p id="Par75">For future work, we will investigate how DSRC2, which exhibits a good performance of compression alone, can be optimized for the cloud environment by utilizing data segmentation and the optimization techniques proposed in GTZ.</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec18">
        <title>Additional file</title>
        <p id="Par83">
          <media position="anchor" xlink:href="12859_2017_1973_MOESM1_ESM.xlsx" id="MOESM1">
            <label>Additional file 1:</label>
            <caption>
              <p>Compression ratios, compression time and descriptions of datasets are included in this file. (XLSX 19 kb)</p>
            </caption>
          </media>
        </p>
      </sec>
    </app>
  </app-group>
  <fn-group>
    <fn>
      <p>
        <bold>Electronic supplementary material</bold>
      </p>
      <p>The online version of this article (10.1186/s12859-017-1973-5) contains supplementary material, which is available to authorized users.</p>
    </fn>
  </fn-group>
  <ack>
    <sec id="FPar1">
      <title>Funding</title>
      <p id="Par76">Publication of this article was funded by the National Natural Science Foundation of China grant (No.31501073, No.81522048, No.81573511), the National Key Research and Development Program (No.2016YFC0905000), and the Genetalks Biotech. Co.,Ltd.</p>
    </sec>
    <sec id="FPar2">
      <title>Availability of data and materials</title>
      <p id="Par77">GTZ is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Genetalks/gtz">https://github.com/Genetalks/gtz</ext-link>.</p>
    </sec>
    <sec id="FPar3">
      <title>About this supplement</title>
      <p id="Par78">This article has been published as part of <italic>BMC Bioinformatics</italic> Volume 18 Supplement 16, 2017: 16th International Conference on Bioinformatics (InCoB 2017): Bioinformatics. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-18-supplement-16">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-18-supplement-16</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>Yuting Xing, Dr. Gen Li and Dr. Chengkun Wu developed the algorithms and drafted the manuscript; they developed the codes of GTZ together with Zhenguo Wang and Bolun Feng; Dr. Zhuo Song and Dr. Chengkun Wu proposed the idea of the project, prepared the 8 FASTQ datasets for testing, drafted the discussion and revised the whole manuscript. All the authors have read and approve the manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec id="FPar4">
      <title>Ethics approval and consent to participate</title>
      <p id="Par79">Not applicable.</p>
    </sec>
    <sec id="FPar5">
      <title>Consent for publication</title>
      <p id="Par80">Not applicable.</p>
    </sec>
    <sec id="FPar6">
      <title>Competing interests</title>
      <p id="Par81">The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="FPar7">
      <title>Publisher’s Note</title>
      <p id="Par82">Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <mixed-citation publication-type="other">Daily K, Rigor P, Christley S, Xie X, Baldi P. Data structures and compression algorithms for high-throughput sequencing technologies. BMC Bioinformatics. BioMed Central Ltd; 2010;11:514.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kozanitis</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Saunders</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kruglyak</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bafna</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Varghese</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Compressing genomic sequence fragments using SLIMGENE</article-title>
        <source>J Comput Biol</source>
        <year>2010</year>
        <volume>18</volume>
        <fpage>401</fpage>
        <lpage>413</lpage>
        <pub-id pub-id-type="doi">10.1089/cmb.2010.0253</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Pinho AJ, Pratas D, Garcia SP. GReEn: a tool for efficient compression of genome resequencing data. Nucleic Acids Res 2012;40:e27–7.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Jones DC, Ruzzo WL, Peng X, Katze MG. Compression of next-generation sequencing reads aided by highly efficient de novo assembly. Nucleic Acids Res 2012;40:e171–1.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Light-weight reference-based compression of FASTQ data</article-title>
        <source>BMC Bioinformatics</source>
        <year>2015</year>
        <volume>16</volume>
        <fpage>188</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-015-0628-7</pub-id>
        <?supplied-pmid 26051252?>
        <pub-id pub-id-type="pmid">26051252</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bonfield</surname>
            <given-names>JK</given-names>
          </name>
          <name>
            <surname>Mahoney</surname>
            <given-names>MV</given-names>
          </name>
        </person-group>
        <article-title>Compression of FASTQ and SAM format sequencing data. Gormley M, editor</article-title>
        <source>PLoS One</source>
        <year>2013</year>
        <volume>8</volume>
        <fpage>e59190</fpage>
        <lpage>e59110</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0059190</pub-id>
        <?supplied-pmid 23533605?>
        <pub-id pub-id-type="pmid">23533605</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Grumbach</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tahi</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Compression of DNA sequences</article-title>
        <source>I.N.R.I.A</source>
        <year>1994</year>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ziv</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lempel</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A universal algorithm for sequential data compression</article-title>
        <source>IEEE Trans Inf Theory</source>
        <year>1977</year>
        <volume>IT-23</volume>
        <fpage>337</fpage>
        <lpage>343</lpage>
        <pub-id pub-id-type="doi">10.1109/TIT.1977.1055714</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grumbach</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tahi</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A new challenge for compression algorithms: genetic sequences</article-title>
        <source>Inf Process Manag</source>
        <year>1994</year>
        <volume>30</volume>
        <fpage>875</fpage>
        <lpage>886</lpage>
        <pub-id pub-id-type="doi">10.1016/0306-4573(94)90014-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Deorowicz</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Grabowski</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Compression of DNA sequence reads in FASTQ format</article-title>
        <source>Bioinformatics</source>
        <year>2011</year>
        <volume>27</volume>
        <fpage>860</fpage>
        <lpage>862</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btr014</pub-id>
        <?supplied-pmid 21252073?>
        <pub-id pub-id-type="pmid">21252073</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huffman</surname>
            <given-names>DA</given-names>
          </name>
        </person-group>
        <article-title>A method for the construction of minimum-Redundacy codes</article-title>
        <source>Proc IRE</source>
        <year>1952</year>
        <volume>40</volume>
        <fpage>1908</fpage>
        <lpage>1911</lpage>
        <pub-id pub-id-type="doi">10.1109/JRPROC.1952.273898</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roguski</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Deorowicz</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>DSRC 2-industry-oriented compression of FASTQ files</article-title>
        <source>Bioinformatics</source>
        <year>2014</year>
        <volume>30</volume>
        <fpage>2213</fpage>
        <lpage>2215</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btu208</pub-id>
        <?supplied-pmid 24747219?>
        <pub-id pub-id-type="pmid">24747219</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hach</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Numanagic</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Alkan</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Sahinalp</surname>
            <given-names>SC</given-names>
          </name>
        </person-group>
        <article-title>SCALCE: boosting sequence compression algorithms using locally consistent encoding</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <fpage>3051</fpage>
        <lpage>3057</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts593</pub-id>
        <?supplied-pmid 23047557?>
        <pub-id pub-id-type="pmid">23047557</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Jeannot</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Knutsson</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Adaptive online data compression</article-title>
        <source>Proceedings th IEEE international symposium on high performance distributed computing</source>
        <year>2017</year>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Nicolae M, Pathak S, Rajasekaran S. LFQC: a lossless compression algorithm for FASTQ files. Bioinformatics. 2015;31:3276–81.</mixed-citation>
    </ref>
  </ref-list>
</back>
