<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Neuroinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Neuroinform</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Neuroinform.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Neuroinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1662-5196</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6776611</article-id>
    <article-id pub-id-type="doi">10.3389/fninf.2019.00062</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Neuroscience</subject>
        <subj-group>
          <subject>Original Research</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>odMLtables: A User-Friendly Approach for Managing Metadata of Neurophysiological Experiments</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Sprenger</surname>
          <given-names>Julia</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn001">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/699860/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zehl</surname>
          <given-names>Lyuba</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <xref ref-type="aff" rid="aff4">
          <sup>4</sup>
        </xref>
        <xref ref-type="author-notes" rid="fn001">
          <sup>†</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/149880/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Pick</surname>
          <given-names>Jana</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Sonntag</surname>
          <given-names>Michael</given-names>
        </name>
        <xref ref-type="aff" rid="aff5">
          <sup>5</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/227408/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Grewe</surname>
          <given-names>Jan</given-names>
        </name>
        <xref ref-type="aff" rid="aff6">
          <sup>6</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/13464/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wachtler</surname>
          <given-names>Thomas</given-names>
        </name>
        <xref ref-type="aff" rid="aff5">
          <sup>5</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/13465/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Grün</surname>
          <given-names>Sonja</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/8155/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Denker</surname>
          <given-names>Michael</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/39100/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Institute of Neuroscience and Medicine (INM-6) and Institute for Advanced Simulation (IAS-6) and JARA Institute Brain Structure-Function Relationships (INM-10), Jülich Research Centre</institution>, <addr-line>Jülich</addr-line>, <country>Germany</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Theoretical Systems Neurobiology, RWTH Aachen University</institution>, <addr-line>Aachen</addr-line>, <country>Germany</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Institute of Neuroscience and Medicine (INM-1), Jülich Research Centre</institution>, <addr-line>Jülich</addr-line>, <country>Germany</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Molecular and Systemic Neurophysiology, Department of Neurophysiology, Institute of Biology II, RWTH Aachen University</institution>, <addr-line>Aachen</addr-line>, <country>Germany</country></aff>
    <aff id="aff5"><sup>5</sup><institution>Department of Biology II, Ludwig-Maximilians-Universität München</institution>, <addr-line>Martinsried</addr-line>, <country>Germany</country></aff>
    <aff id="aff6"><sup>6</sup><institution>Institut for Neurobiology, Abteilung Neuroethologie, Eberhard-Karls-Universität Tübingen</institution>, <addr-line>Tübingen</addr-line>, <country>Germany</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Padraig Gleeson, University College London, United Kingdom</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Shreejoy J. Tripathy, University of British Columbia, Canada; Pierre Yger, INSERM U968 Institut de la Vision, France</p>
      </fn>
      <corresp id="c001">*Correspondence: Julia Sprenger <email>j.sprenger@fz-juelich.de</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>†These authors have contributed equally to this work</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>9</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>62</elocation-id>
    <history>
      <date date-type="received">
        <day>28</day>
        <month>3</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>02</day>
        <month>9</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2019 Sprenger, Zehl, Pick, Sonntag, Grewe, Wachtler, Grün and Denker.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Sprenger, Zehl, Pick, Sonntag, Grewe, Wachtler, Grün and Denker</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>An essential aspect of scientific reproducibility is a coherent and complete acquisition of metadata along with the actual data of an experiment. The high degree of complexity and heterogeneity of neuroscience experiments requires a rigorous management of the associated metadata. The odML framework represents a solution to organize and store complex metadata digitally in a hierarchical format that is both human and machine readable. However, this hierarchical representation of metadata is difficult to handle when metadata entries need to be collected and edited manually during the daily routines of a laboratory. With odMLtables, we present an open-source software solution that enables users to collect, manipulate, visualize, and store metadata in tabular representations (in xls or csv format) by providing functionality to convert these tabular collections to the hierarchically structured metadata format odML, and to either extract or merge subsets of a complex metadata collection. With this, odMLtables bridges the gap between handling metadata in an intuitive way that integrates well with daily lab routines and commonly used software products on the one hand, and the implementation of a complete, well-defined metadata collection for the experiment in a standardized format on the other hand. We demonstrate usage scenarios of the odMLtables tools in common lab routines in the context of metadata acquisition and management, and show how the tool can assist in exploring published datasets that provide metadata in the odML format.</p>
    </abstract>
    <kwd-group>
      <kwd>metadata management</kwd>
      <kwd>open metadata Markup Language (odML)</kwd>
      <kwd>reproducibility and tools</kwd>
      <kwd>graphical user interface (GUI)</kwd>
      <kwd>laboratory routines and automation</kwd>
      <kwd>electrophysiology</kwd>
    </kwd-group>
    <counts>
      <fig-count count="8"/>
      <table-count count="0"/>
      <equation-count count="0"/>
      <ref-count count="29"/>
      <page-count count="17"/>
      <word-count count="11842"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1. Introduction</title>
    <p>In recent years, the workflows involved in conducting and analyzing neurophysiological experiments have become increasingly complex (e.g., Coles et al., <xref rid="B4" ref-type="bibr">2008</xref>; Denker and Grün, <xref rid="B6" ref-type="bibr">2016</xref>; Brochier et al., <xref rid="B2" ref-type="bibr">2018</xref>). Several factors contribute to this development. Nowadays, a recording setup is usually comprised of several hardware and software components that are often produced by different companies, or might even be custom made. In addition, due to the technological progress in neuroscience during the last decades the task designs have become more and more sophisticated, as can be observed, for example, when considering experiments mimicking realistic, natural conditions. Neuronal or muscular signals (e.g., eye and arm movements) can be gathered in parallel from multiple optical or electrical recording sites (Nicolelis and Ribeiro, <xref rid="B18" ref-type="bibr">2002</xref>; Verkhratsky et al., <xref rid="B26" ref-type="bibr">2006</xref>; Obien et al., <xref rid="B19" ref-type="bibr">2014</xref>) together with complex behavioral measures (Maldonado et al., <xref rid="B15" ref-type="bibr">2008</xref>; Jacob et al., <xref rid="B12" ref-type="bibr">2010</xref>; Vargas-Irwin et al., <xref rid="B25" ref-type="bibr">2010</xref>; Schwarz et al., <xref rid="B22" ref-type="bibr">2014</xref>). Moreover, these signals can be experimentally manipulated in intricate ways, e.g., via multidimensional natural stimuli (Geisler, <xref rid="B10" ref-type="bibr">2008</xref>) or sophisticated optical or electrical stimulation methods (Deisseroth and Schnitzer, <xref rid="B5" ref-type="bibr">2013</xref>; Miyamoto and Murayama, <xref rid="B16" ref-type="bibr">2015</xref>). As a result, the amount of information required to fully describe all circumstances under which the experiment was conducted and data was recorded, here collectively referred to as “metadata”, has grown considerably at the same time. Therefore, metadata of neuroscientific studies are increasingly difficult to document and the implementation of specific software solutions to facilitate their management in daily routines involves a lot of time and effort (Zehl et al., <xref rid="B29" ref-type="bibr">2016</xref>).</p>
    <p>The complexity of collecting metadata originates from two factors: Firstly, the growing heterogeneity of setup equipment alone makes it difficult to fully track the exact circumstances under which the primary data were recorded and how the recorded signals were processed along an experimental recording session (“black box” effect, i.e., the difficulty to precisely relate inputs and outputs to the equipment). Secondly, the complexity of the signal types and manipulations using various tools within custom signal processing pipelines increases the effort needed for comprehensive metadata tracking across all parts of the recording system and all processing steps. In particular, the hardware components and software tools employed in these experimental setups typically do not provide a complete account of their metadata and store their output in non-standardized representations that impede gaining insights into the details of the recording process. Nonetheless, collecting and providing metadata of an experiment is a necessary step towards replicable experiments and therefore forms the basis for reproducible research (Tebaykin et al., <xref rid="B24" ref-type="bibr">2018</xref>). In this regard, metadata have to be human readable in order to give users semantic access to the data, similar to a traditional lab book. However, only standardized, machine-readable metadata can be systematically reproduced during automatized analysis processes, which makes them a crucial ingredient for tracking the data provenance leading to a research publication.</p>
    <p>A software approach to manage neuroscientific metadata is the <italic>open metadata Markup Language</italic> (odML) framework (Grewe et al., <xref rid="B11" ref-type="bibr">2011</xref>). odML provides a standardized format for organizing metadata of arbitrary type into a hierarchical structure that is both human and machine-readable. With this, it is possible to organize metadata originating from heterogeneous sources in a unified way and record them in an common, interoperable format. Providing metadata in such a standardized format along with the data files of an experiment facilitates the collaboration process between members of a scientific project, because metadata can be organized and made available to all members in a unified way, thus supporting rigor and reproducibility of data analysis through standardized and formalized access to the available metadata (Zehl et al., <xref rid="B29" ref-type="bibr">2016</xref>). The reference implementation of the odML format is based on the generic eXtensible Markup Language (XML). Version 1.4, which is considered here, also supports the JSON and YAML formats and provides an application programming interface (API) for Python and Matlab (<ext-link ext-link-type="uri" xlink:href="http://www.g-node.org/odml">http://www.g-node.org/odml</ext-link>).</p>
    <p><xref ref-type="fig" rid="F1">Figure 1</xref> shows a generic representation of an example workflow that results in the generation of a metadata collection represented in odML. The starting point are collections of files containing various subsets of the metadata for individual recordings of an experiment (e.g., different recording days). The data in these files are often organized in different formats within a collection, and files and metadata between different collections may differ due to factors, such as changes in the experiment. Therefore, it is possible and advisable to construct template structures for the metadata collection to enforce a systematic metadata structuring. However, in practical scenarios often custom scripts are required to populate these templates, e.g., to cover small variations between metadata collections when a certain piece of information is not present for a particular recording. In addition, the metadata collection must be manually enriched by information that is not digitally available in the first place. The outcome of this build process are odML files for each recording, adhering to a uniform template structure. In a final step, these individual metadata collections may be merged into a single odML file in order to provide scientists with the ability to perform full metadata queries on the complete experiment. Zehl et al. (<xref rid="B29" ref-type="bibr">2016</xref>) provides a complete account of this workflow including practical examples.</p>
    <fig id="F1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Generic workflow of generating metadata collections from source files using the odML framework. For a given metadata collection (top row, example metadata collections A–C), metadata are pooled from multiple files and enriched via manual entry (second row). These metadata are converted into individual collections via a scripted approach applying an odML template structure (third row). By further integrating individual multiple metadata collections (fourth row), a complete odML collection containing all recordings of a particular experiment can be created (bottom row).</p>
      </caption>
      <graphic xlink:href="fninf-13-00062-g0001"/>
    </fig>
    <p>Implementing and applying such a rigorous workflow as described in <xref ref-type="fig" rid="F1">Figure 1</xref> requires programming skills by the scientist. However, metadata handling is often performed by several experimenters with varying computational expertise. Furthermore, extensive manual editing of the metadata files via the present graphical user interface (GUI) included in the odML framework tends to be cumbersome for large metadata trees due to their hierarchical, complex organization. While visualization of the hierarchical organization is suited for an overview of the general structure and relation of the metadata, finding or comparing particular values can be difficult if they are distributed in different branches of the hierarchy. Furthermore, editing of distributed entries is laborious, because a hierarchical organization also requires navigation through the tree to access a particular entry. This makes this metadata management tool inefficient to use in an experimental laboratory where often (i) single particular entries need to be modified manually as the experiment progresses, (ii) a batch of similar entries need to be modified coherently as the data processing progresses. The combinations of all these factors results in many experimental laboratories frequently collecting metadata in flat tabular formats independent of an explicit, underlying hierarchical structure, using tools for generation and manipulation of tables that do not require programming expertise, are widely adopted, readily available and familiar to the experimenters.</p>
    <p>Thus, for these purposes a flat tabular representation of the metadata appears to be suitable. It has the advantage of providing easier access to, and simpler visualization of, the metadata than a hierarchical format. Tabular representations of hierarchical structures are implemented in a number of generic software tools for xml representation<xref ref-type="fn" rid="fn0001"><sup>1</sup></xref>. However, these generic xml editors do not provide support for using xml to handle scientific metadata in a concise way.</p>
    <p>We developed odMLtables as a Python package to complement the odML framework in simplifying working with, and in particular manually editing, the metadata stored in the hierarchical odML format. odMLtables facilitates the integration of the odML framework into the experimental workflow by converting between hierarchical odML and tabular representations in xls or csv format. As opposed to working on the hierarchical odML structure, these tabular formats are easily accessible via familiar spreadsheet tools (e.g., <italic>Microsoft Excel, LibreOffice Calc</italic>) that enable neuroscientists to manually extend or edit the content of an odML metadata file. Vice versa, the ability to convert configurable tabular representations of metadata to odML will help into a robust, self-consistent, and validated format, ready for automation tasks, such as batch analysis processing or integration into databases. Thus, odMLtables acts as a bridge between users and formal representation. In addition, the odMLtables package comprises a GUI that guides the user through all functional features of the tool. Besides the conversion between hierarchical and tabular formats, these features include operations identified as useful in the metadata acquisition process, such as merging or filtering metadata. Implementing these operations would require custom programming efforts in the absence of odMLtables, either by manipulation of odML files using the odML API, or by using programmatic capabilities of the tabular editor. odMLtables also opens access to the odML framework for scientists with little or no programming experience. All main functionality to interact with metadata files is directly accessible from the odML GUI since version 1.4.0. The software has benefited from the experiences gained in applying it in collaborative projects involving three different experiments collecting electrophysiological data: (i) cortical activity in macaque performing a visually-guided motor task (e.g., Brochier et al., <xref rid="B2" ref-type="bibr">2018</xref>; Denker et al., <xref rid="B7" ref-type="bibr">2018</xref>), (ii) cortical and hippocampal activity in a developmental study in mice (e.g., Bitzenhofer et al., <xref rid="B1" ref-type="bibr">2017</xref>), and (iii) cortical activity in a category learning task in gerbils (e.g., Ohl et al., <xref rid="B20" ref-type="bibr">2001</xref>).</p>
    <p>The embedding of odMLtables in a real-world metadata management workflow is described in Zehl et al. (<xref rid="B29" ref-type="bibr">2016</xref>), resulting in a published dataset with detailed metadata descriptions in the odML format (Brochier et al., <xref rid="B2" ref-type="bibr">2018</xref>). In these publications the focus was on the concepts of metadata management and the detailed experiment description. Here we complement these studies by a technical tool for convenient metadata capture. Because the complexities of the real experiment (an instructed, delayed reach to grasp task with multielectrode recordings from monkey motor cortex) would distract from the presentation of the features and usage of odMLtables, the examples presented here are abstracted from these studies.</p>
    <p>To demonstrate the application of odMLtables we present seven minimalistic scenarios of practical metadata management using odML and odMLtables. Together these scenarios form a complete metadata workflow based on an exemplary multi-day experiment as commonly encountered in neurophysiology (cf. <xref ref-type="fig" rid="F1">Figure 1</xref>). However, such scenarios also occur in other fields of science where data is aggregated in repetitive acquisition cycles (e.g., multiple days of measurements). Moreover, the scenarios are of sufficiently generic nature to translate to other situations where metadata information is collected. The first two scenarios demonstrate the first steps for setting up a new metadata workflow and daily metadata collection. Four scenarios deal with the ongoing metadata validation, enrichment and visualization. The last scenario introduces automation of metadata collection and management using odML and odMLtables.</p>
    <p>Using these scenarios we demonstrate how odMLtables facilitates access to sophisticated metadata management software odML for non-programmers and with that optimizes routine manual metadata acquisitions in any laboratory workflow. In addition, odMLtables can be used to create visually enhanced tabular overviews of complete or filtered metadata from any hierarchically structured odML files. For a scripted metadata approach a Python interface also permits programmers to benefit from odMLtables features.</p>
  </sec>
  <sec id="s2">
    <title>2. Software Description</title>
    <p>odMLtables is a Python package that provides a set of functions for working with metadata descriptions in the odML metadata framework, with a particular focus on making these metadata easily accessible for users. The key approach is to bring the typically complex, hierarchical structure of the odML format into a tabular and reduced representation, such that metadata can be more easily inspected or edited. Therefore, at its core, odMLtables provides functions to convert between the odML format and the corresponding tabular representation which can be represented in the <italic>Microsoft Excel</italic> (xls) or the generic comma separated value (csv) format (<xref ref-type="fig" rid="F2">Figure 2</xref>). Metadata converted to these tabular formats are accessible via widely used spreadsheet software (e.g., <italic>Microsoft Excel</italic><xref ref-type="fn" rid="fn0002"><sup>2</sup></xref> or <italic>LibreOffice Calc</italic><xref ref-type="fn" rid="fn0003"><sup>3</sup></xref>), such that users are able to intuitively view and edit the metadata. After editing, the metadata can be brought back to the standardized, hierarchical form defined by the odML framework (as illustrated in <xref ref-type="fig" rid="F2">Figure 2</xref>).</p>
    <fig id="F2" position="float">
      <label>Figure 2</label>
      <caption>
        <p>Minimal workflow for manually editing odML files via odMLtables. Metadata is manually edited in tabular form using spreadsheet software and stored in xls and csv formats (left). The minimal functionality of odMLtables is to convert between such tabular representations and the hierarchical odML structure (right). The hierarchical placement of individual metadata entries, i.e., the Sections of the odML tree, is encoded in a specific column of the table (gray boxes and circles), whereas the values and attributes of metadata entries, i.e., a Property represented as leaves of the odML tree, are stored in rows of the table (colored boxes).</p>
      </caption>
      <graphic xlink:href="fninf-13-00062-g0002"/>
    </fig>
    <p>Next to the functionality of converting between odML and the tabular formats, odMLtables provides four additional capabilities that address common tasks when working with metadata collections:</p>
    <list list-type="bullet">
      <list-item>
        <p>filtering (or reduction) of a metadata collection to a subset</p>
      </list-item>
      <list-item>
        <p>merging of two metadata collections</p>
      </list-item>
      <list-item>
        <p>generation of a basic odML structure to facilitate the design of a new metadata collection</p>
      </list-item>
      <list-item>
        <p>creating a tabular overview across multiple metadata entries within a metadata collection</p>
      </list-item>
    </list>
    <p>The functionality of odMLtables can be accessed in one of two ways. First, the API of odMLtables complements the original Python odML API (Grewe et al., <xref rid="B11" ref-type="bibr">2011</xref>). As such, odMLtables simplifies the scripting of automated metadata extraction and aggregation tasks in an experiment. Second, odMLtables includes a GUI that enables non-programmers access to the large majority of functionality offered by the library. In this way, odMLtables can aid work with odML-based metadata collections in metadata workflows that do not include scripted processing stages.</p>
    <p>In the following, we describe in detail the structure of the hierarchical and tabular metadata representations, the main capabilities of odMLtables illustrated by means of the GUI, and its internal architecture.</p>
    <sec>
      <title>2.1. Hierarchical and Tabular Representations of Metadata</title>
      <sec>
        <title>2.1.1. Hierarchical Metadata in the odML Format</title>
        <p>odML<xref ref-type="fn" rid="fn0004"><sup>4</sup></xref> is a versatile hierarchical format for metadata (Grewe et al., <xref rid="B11" ref-type="bibr">2011</xref>) developed by the German Neuroinformatics Node (G-Node). While it was originally designed for electrophysiological metadata, its generic structure makes it also applicable to other scientific contexts.</p>
        <p>The basic concept is to use a tree-like structure of <bold>Sections</bold> to store metadata as <bold>Properties</bold> (extended key-value pairs) in a common <bold>Document</bold> (<xref ref-type="fig" rid="F3">Figure 3B</xref>). For example, using this paradigm, parameter settings of a specific device used in the experiment would be represented as Properties collected in a specific Section for that device. For a detailed tutorial<xref ref-type="fn" rid="fn0005"><sup>5</sup></xref> on odML please refer to the online reference documentation<xref ref-type="fn" rid="fn0006"><sup>6</sup></xref>. The usage of odML in different environments with varying requirements has led to diversification, the identification of unused features, and the need for improvement of the original data model. In case of the odMLtables project, for example, the original internal data representation required only a subset of the complete odML data model. These and other re-implementations (NIX and RELACS projects) did not fully comply with the original specifications and led to a diversification of the de-facto implemented data models. In order to resolve this situation, with the latest release of odML version 1.4<xref ref-type="fn" rid="fn0007"><sup>7</sup></xref> (i) data model and implemented features were streamlined and adapted to ensure compatibility between the various project implementations and (ii) additional features were introduced. The following paragraph briefly reviews the changes of the data model since its publication in Grewe et al. (<xref rid="B11" ref-type="bibr">2011</xref>).</p>
        <fig id="F3" position="float">
          <label>Figure 3</label>
          <caption>
            <p>Evolution of the odML data-model. Each box represents an entity defined by the data model and is color coded. Connections between entities are illustrated using the UML aggregation relation where a diamond denotes the target of a connection; the numbers at source and target denote the cardinality of each entity in the connection. <bold>(A)</bold> Version 1.3 data model. Four entities are defined: The <italic>Document</italic> (marked in white) as the root element of a metadata file contains information about the author, document date, the document version and a default repository containing definitions used within the Document. It further contains grouping elements, <italic>Section</italic>s (marked in dark blue). These are defined via their name and type attributes and can hold subsections and provide semantic structure to an odML Document. The “definition” attribute provides information about the nature of a Section, while “link” and “include” refer to further Sections within the same or a different Document, respectively. Sections may contain named <italic>Property</italic> entities (marked in cyan) which hold at least one <italic>Value</italic> (marked in light blue) thus creating an extended key-value pair. <bold>(B)</bold> Version 1.4 data model: To simplify the use of the odML data model the Value entity was integrated into the Property taking over the attributes “dtype” (data type), “unit,” “uncertainty,” “value origin,” and “reference.” In this version a Property may contain a list of values, which must be identical in terms of the relocated attributes thus reducing the risk of ambiguities in the value list. For more information on attributes that have not been modified please refer to the original publication (Grewe et al., <xref rid="B11" ref-type="bibr">2011</xref>). Figure with permission adapted from Grewe et al. (<xref rid="B11" ref-type="bibr">2011</xref>).</p>
          </caption>
          <graphic xlink:href="fninf-13-00062-g0003"/>
        </fig>
        <sec>
          <title>2.1.1.1. odML model revision and streamlining</title>
          <p>A number of features were merged or moved by the change from odML version 1.3 to version 1.4 in order to simplify usage of the odML framework as originally described in Grewe et al. (<xref rid="B11" ref-type="bibr">2011</xref>), and to mitigate potential ambiguities in the data structure. In the following, we briefly explain two major changes that affected the design and use of odMLtables. The first change was the merging of Value and Property entities (compare <xref ref-type="fig" rid="F3">Figures 3A</xref> and <xref ref-type="fig" rid="F3">B</xref>). This prevents value ambiguities within a Property and reduces the effective file size since the value dependent attributes (“unit,” “uncertainty,” “data type,” and “reference”) are defined only once for a set of values. This change simplified also the tabular representations of lists of values created by odMLtables. Second, for compatibility with the NIX projects' odML implementation, entities now contain a universally unique identifier (UUID, auto-generated identifier with extremely low collision probability) for unique identification of odML entities even across unrelated files to ensure comprehensive provenance tracking, including the ability to create tabular metadata representations across projects using future odMLtables versions. Compatibility for odML files using the old format version is ensured via automatized conversion functionality.</p>
        </sec>
        <sec>
          <title>2.1.1.2. Additional features</title>
          <p>The odML core library already provides an in-built mechanism to search and retrieve Sections, Properties or values within a Document. The need to consistently search for metadata entities across Documents from different sources led to the development of an export feature of odML metadata to the Resource Description Framework (RDF) format<xref ref-type="fn" rid="fn0008"><sup>8</sup></xref>, a general and widely used storage format of graph databases. Multiple odML files exported to RDF can be loaded into any graph database supporting RDF and will be combined into a single graph. Moreover, while XML is still the default storage format, odML now additionally supports storing the metadata in the text based file formats JSON<xref ref-type="fn" rid="fn0009"><sup>9</sup></xref> and YAML<xref ref-type="fn" rid="fn0010"><sup>10</sup></xref>. JSON has become a de-facto data exchange standard between web based and standalone computer applications. The support of JSON makes odML metadata more easily consumable in machine-only workflows through modern applications. Since both XML and JSON primarily aim at machine-readability, their structure is not easily readable by humans. To ease reading of raw odML files by actual persons the YAML file format support was added.</p>
          <p>For easy visualization and manipulation of specific odML files, the graphical user interface of odMLtables was integrated into the native odML GUI (odml-ui<xref ref-type="fn" rid="fn0011"><sup>11</sup></xref>). Thus, the odML GUI now grants direct access to the main odMLtables features, making both software tools even easier to use back to back for both browsing and editing of metadata.</p>
        </sec>
      </sec>
      <sec>
        <title>2.1.2. Tabular Representation of the odML Format</title>
        <p>odMLtables converts the hierarchical odML structure (<xref ref-type="fig" rid="F4">Figure 4A</xref>) into a specific tabular (flat) representation (<xref ref-type="fig" rid="F4">Figure 4B</xref>), stored either in the xls or csv format. In this format, each row corresponds to one particular value entry. The columns further describe the Property and Section each value belongs to, e.g., the Property name, the Section and Subsections the Property belongs to, the physical units, or the Property definition. The hierarchy of Sections in which a Property is located in the original odML structure is represented by a path construct, where individual Section names are delineated by the “/” character. For increased readability, repetitive information (i.e., identical information to the cell above) is optionally displayed only at the first instance (e.g., “Path to Subject” entry (“/Subject”) in row 3, 4, and 5 in <xref ref-type="fig" rid="F4">Figure 4B</xref>). By default, the column headers are predefined (<xref ref-type="fig" rid="F4">Figure 4B</xref>, second row), however the header names can also be customized as long as a mapping between the predefined names and the custom names can be provided. The order of the columns of the table can be customized since the column header names are used to associate columns with attributes of the hierarchical odML structure. The odML Document attributes “author,” “date,” “version,” and “repository” are handled separately and are placed in the top row of the tabular odML representation.</p>
        <fig id="F4" position="float">
          <label>Figure 4</label>
          <caption>
            <p>Mapping of an odML structure in <bold>(A)</bold> hierarchical metadata format to <bold>(B)</bold> tabular format. Individual attributes of the odML entities are represented in different columns in the tabular representation (e.g., “Section Definition,” “Property Name,” “Data Uncertainty,” compare color code). Document attributes (“author,” “date,” “repository,” and “version”) are described separately in the first row of the tabular representation. The hierarchy of Sections is captured in an additional column (“Path to Section”) describing the path between the odML Document and the current Section. Each metadata entry in the hierarchical format corresponds to a single row in the tabular format. Items of a list are treated as individual entries.</p>
          </caption>
          <graphic xlink:href="fninf-13-00062-g0004"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>2.2. Software Functionalities</title>
      <p>odMLtables is a tool that provides five functionalities surrounding work in creating and accessing metadata collections. In the following, we describe the capabilities of these features, while their use is put into the context of a typical workflow in section 3.</p>
      <p>All main features of odMLtables are available via the odMLtables GUI (<xref ref-type="fig" rid="F5">Figure 5</xref>). Upon launching the application, it presents the user with five buttons, each leading to a series of dialogs (wizards) to perform a specific odMLtables functionality. For the more complex dialogs that include a large number of parameters to set, the GUI offers to save and load the dialog configuration to efficiently re-run a functionality with given parameters.</p>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Main window of the odMLtables GUI. The interface gives access to the main functionalities available by the tool: converting files from hierarchical to tabular (flat) representations, generating an empty generic odML table (template), comparing entries within a metadata collection, merging contents of two collections, and selecting a subset of a metadata collection (filtering). Each button starts a series of dialogs (wizards) that guide the user through the corresponding process.</p>
        </caption>
        <graphic xlink:href="fninf-13-00062-g0005"/>
      </fig>
      <p>In addition to the functionality offered by the GUI, the Python programming interface of odMLtables offers additional features, most notably, the ability to customize the default values for odML data types. The default values can be displayed using a highlighted coloring scheme to indicate to the researcher that a Property currently contains a default value (for details, see the odMLtables documentation<xref ref-type="fn" rid="fn0012"><sup>12</sup></xref>).</p>
      <p>The main features of odMLtables are described in detail below and are referred to as features F1–F5:</p>
      <list list-type="simple">
        <list-item>
          <p><bold>F1: Convert between odML and table format</bold>. This function converts metadata collections between the representations in the different file formats odml, xls, and csv. For the conversion to and from the tabular formats (xls/csv) a specific formatting of the table is required in order to interpret the table as hierarchical odML structure (see section 2.1.2). Nevertheless, odMLtables allows for a certain degree of flexibility in order to give researchers the ability to design tabular formats to best fit their workflow. In particular, this encompasses the inclusion or removal of certain optional columns, the arrangement of columns, column headers, or the coloring scheme. Note however, that for the reverse conversion from a tabular format back to the odML format, these customizations need to be known (e.g., custom column names, see section 3.1).</p>
        </list-item>
        <list-item>
          <p><bold>F2: Generate new metadata collection table</bold>. This function generates and saves an empty, generic (template) odML structure in the xls format. This generic structure provides a good starting point to design a metadata collection or template structure in a tabular format providing the required tabular structure for conversion to a hierarchical odML structure. Similar formatting options can be applied to the table as indicated above.</p>
        </list-item>
        <list-item>
          <p><bold>F3: Generate overview across entries within a metadata collection</bold>. This function creates a chart listing multiple entries within a single metadata collection. It is intended to develop overview sheets containing similar Properties, e.g., the animal weight at different ages. The generated table does not follow the tabular odML format and can therefore only be used for visualization and not for conversion into the hierarchical odML format. Using common spreadsheet software the comparison table can be saved as a figure and printed for usage in a laboratory notebook.</p>
        </list-item>
        <list-item>
          <p><bold>F4: Merge contents of two metadata collections</bold>. This function allows to merge multiple files (odML format) into a single file. Here, by default, Sections, Properties and values are added to existing entities during merging. However, for values of coinciding Properties the option exists to overwrite values during the process of merging.</p>
        </list-item>
        <list-item>
          <p><bold>F5: Filter content of a metadata collection</bold>. This function reduces the size of an odML file based on a filter mechanism, which can include multiple steps of filtering and custom filter functions to select only specific parts of an odML structure. The filter mechanism e.g., can extract all Properties containing no values to present the experimenter potential missing entries in the metadata collection.</p>
        </list-item>
      </list>
    </sec>
    <sec>
      <title>2.3. Software Architecture</title>
      <p>In the following, we explain the internal structure of the odMLtables software. For a detailed description, see the function reference in the odMLtables documentation.</p>
      <p>The core of odMLtables is the <monospace>OdmlTable</monospace> class, which provides the main functionality for loading and saving metadata collections in the different file formats. It implements basic operations on the loaded metadata independent of the file format they originate from. Within the class, metadata are internally represented as a list of dictionaries, where each dictionary corresponds to an odML Property. Functions that modify the metadata collection, like merging and filtering, act directly on this internal dictionary representation. The two tabular formats xls and csv require additional information regarding the table layout when being saved to disk, e.g., the color scheme. Therefore, two subclasses of the <monospace>OdmlTable</monospace> class (<monospace>OdmlXlsTable</monospace> and <monospace>OdmlCsvTable</monospace>) carry these additional output settings. Finally, a separate <monospace>CompareSectionTable</monospace> class implements the function for comparing Properties within one odML structure. As for the <monospace>OdmlTable</monospace> class, two specific subclasses for xls and csv output are defined to capture layout information (<monospace>CompareSectionXlsTable</monospace> and <monospace>CompareSectionCsvTable</monospace>).</p>
      <p>One feature of odMLtables in generating xls files is to highlight a value entry if it corresponds to the default value of the corresponding Property's data type. However, the odML library itself does not specify such default values for all of its data types. Moreover, it is not mandatory, nor always desired, to specify a data type in the odML in all circumstances, e.g., when leaving a value empty. Therefore, odMLtables provides functionality to work with default values for data types in the <monospace>OdmlDtypes</monospace> class. It manages the data types, synonyms, default values, and value conversions. The class is used for entering default entries when loading empty values from a tabular representation, and for default value highlighting.</p>
      <p>In addition to the core module, odMLtables provides a GUI that exposes most functionality of the core module. The GUI is based on the PyQt5<xref ref-type="fn" rid="fn0013"><sup>13</sup></xref> framework and consists of a main window (<xref ref-type="fig" rid="F5">Figure 5</xref>) and five wizards (see section 2.2). Each wizard inherits from the <monospace>OdmltablesWizard</monospace> class, which provides helper functions and error handling. The <monospace>Settings</monospace> class stores the current user settings for calls of odMLtables core functions, and provides functionality to save and restore user settings between different executions of the GUI.</p>
    </sec>
  </sec>
  <sec id="s3">
    <title>3. Embedding odMLtables in Data Acquisition and Analysis</title>
    <p>While most scientists would agree that accurate records of the minute details of an experiment are the foundation of good scientific practice, in the everyday routine of an experimental electrophysiology lab it is difficult for the scientist to record, sort, and maintain the wealth of metadata information that accumulates during an experiment. While the odML format is suitable for storing metadata information from different sources, lacking to date is a set of tools that allows the scientist to create, manipulate and visualize the data stored in this format. In the following, we present commonly encountered scenarios involving metadata handling that originate from our collaborative work. These scenarios touch the issues of how to design the hierarchical structure to store and organize the metadata, how to practically enter metadata before, during or after the experiment, and how to create a comparison of rich metadata buried within the odML structure. It turns out that for each of these scenarios a flattened tabular representation of the metadata is a practical solution that feels intuitive to the user. In the following we demonstrate how to implement these scenarios that consist of combining operations in odMLtables and a spreadsheet program. All scenarios are also available as an interactive Jupyter Notebook<xref ref-type="fn" rid="fn0014"><sup>14</sup></xref> accessible via the odMLtables documentation<xref ref-type="fn" rid="fn0015"><sup>15</sup></xref>, and available in pre-executed form as <xref ref-type="supplementary-material" rid="SM2">Data Sheet 1</xref>.</p>
    <sec>
      <title>Scenario 1: How to Generate a Metadata Template Without Programming</title>
      <p>In conducting animal experiments, a typical scenario where metadata are collected manually on a daily basis is the creation of an animal <italic>score sheet</italic>. Such score sheets record quantitative, and in part also qualitative, measures that are collected in order to document and judge the animal's health and state over the duration of the experiment. Often, these sheets are an obligatory piece of documentation of the experiment, such that only the availability of a defined workflow to create score sheets guarantees their consistency over multiple years and different experimenters. For example, for mouse experiments, typical measures are the body weight, water intake and breathing frequency, many of which can be used to assess the health of an animal, e.g., by calculating a health score for each mouse (Foltz and Ullman-Cullere, <xref rid="B8" ref-type="bibr">1999</xref>; Burkholder et al., <xref rid="B3" ref-type="bibr">2012</xref>). In <xref ref-type="fig" rid="F4">Figure 4</xref> we depicted how metadata of a single, minimized score sheet can be integrated into an odML document containing collective information on a subject.</p>
      <p>The measurements for such score sheets are typically easy to perform, and for this reason may be conducted by a number of different people in the lab. Therefore, the daily process must be simple, intuitive, and robust in order to be conducted by all members of the group. Collecting the information in a table format using common spreadsheet software tools, such as <italic>Microsoft Excel</italic> or <italic>LibreOffice Calc</italic>, satisfies these requirements.</p>
      <p>To guarantee a consistent structure of such a score sheet, initially a template needs to be set up, i.e., a table containing the measures that are to be recorded on a single day. In order to accomplish this, as a first step we generate an empty template table using odMLtables. To improve the readability, we enter custom column names in odMLtables to create the table (“Section” instead of “Path to Section,” “Measure” instead of “Property Name,” “Unit” instead of “Data Unit,” and “Type” instead of “odML Data Type”). Also we omit the attributes “Section Definition,” “Property Definition,” and “Data Uncertainty” in the context of these example scenarios. As second step, using a spreadsheet, we design the metadata structure for a single score sheet as shown in <xref ref-type="fig" rid="F6">Figure 6</xref>. The value field for each entry can be either left empty or a default value can be entered. The latter case is interesting for values that are likely to be constant for the majority of experiments, e.g., the name of the experimenter. Since the colors of a table saved in the xls format are ignored when converting to the odML format, it is possible to use arbitrary color coding within the spreadsheet software to improve the readability of the table for the experimenters entering the values.</p>
      <fig id="F6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>Template score sheet. The template score sheet contains the measures required for each measurement day, including optional default values (here: “Alice” for “Experimenter” and “g” as unit for “Weight”).</p>
        </caption>
        <graphic xlink:href="fninf-13-00062-g0006"/>
      </fig>
      <p>We designed the template table such that it matches the properties of the minimized score sheet section already depicted in <xref ref-type="fig" rid="F4">Figure 4</xref>. Notice that in the template the entry for column “Section” already includes a parent section to reference the animal (cf., <xref ref-type="fig" rid="F4">Figure 4</xref>). This is convenient for defining the position of each score sheet in the odML hierarchy to simplify a later merging process (cf., scenario 2).</p>
    </sec>
    <sec>
      <title>Scenario 2: Collecting Daily Observations in a Common odML Structure</title>
      <p>Once the template from scenario 1 is complete, it is copied to a new file on each measurement day, and the copy is filled out by the person taking the measurements. To avoid that metadata are spread across multiple files and potentially multiple locations, we aim to gather the data from multiple days into a single odML file. To achieve this, we use odMLtables to first convert the individual xls file containing an individual score sheet into the odML format, and to subsequently merge these into a common odML structure spanning multiple recording days.</p>
      <p>Specifically, for the conversion from the xls to the odML format we use the odMLtables feature F1 (for details of odML features F1–F5, see section 2.2). After the conversion, the current score sheet present in odML format is merged into the common odML document collecting the complete information of an animal using feature F4 on a daily basis. This extends the odML structure of the subject document by an additional Section each recording day. Note that this is possible because the first column of individual score sheets (<xref ref-type="fig" rid="F6">Figure 6</xref>) not only provides a unique Section name for each score sheet, but also indicates the location of the odML Section in the hierarchical structure of the subject document, (e.g., “Subject/Scores_2000-01-01”). The result is a single odML file containing measures collected on all recording days while the source files generated each day can be archived.</p>
      <p>The metadata collection containing the merged score sheets of 2 recording days might look like the following:
<graphic xlink:href="fninf-13-00062-i0001.jpg" position="float"/></p>
    </sec>
    <sec>
      <title>Scenario 3: Create a Tabular Representation of the odML File for Better Viewing Using the Color Options</title>
      <p>Once the recordings for a number of animals were performed and the corresponding metadata collection is completed, data and metadata should be shared among collaborators in a common repository. In order to get an overview of the data obtained across different animals, the metadata of each animal can be converted into the xls format to simplify the inspection of the associated metadata using spreadsheet software (cf., also <xref ref-type="fig" rid="F4">Figure 4B</xref>). Here, odMLtables provides the option to use color coding and highlighting of default/missing values to improve the readability (<xref ref-type="fig" rid="F7">Figure 7</xref>).</p>
      <fig id="F7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Metadata collection filtered to show only Properties with an empty value. Missing values entries are highlighted in red by odMLtables.</p>
        </caption>
        <graphic xlink:href="fninf-13-00062-g0007"/>
      </fig>
    </sec>
    <sec>
      <title>Scenario 4: How to Filter a Subset of an odML File to Edit It Later on</title>
      <p>As the common odML structure grows day by day it is of advantage to extract specific subsets of odML values of interest for visualization using the tabular format. Instead of visualizing the whole metadata collection to periodically verify that all Properties are filled with a value, we can extract a subset of the collection and visualize only the relevant (e.g., empty fields) entries. For this, we use odMLtables feature F5 which can be used to generate an odML that contains only Properties without value information specified. We then convert this reduced odML into a tabular xls representation using odMLtables feature F1. The generated table, as shown in <xref ref-type="fig" rid="F7">Figure 7</xref> indicating the two empty properties in the odML structure of scenario 2, can be visualized using spreadsheet software and, in case of values not being filled, these can be directly edited manually.</p>
    </sec>
    <sec>
      <title>Scenario 5: Merging the Edited Subset Back Into the Original Structure</title>
      <p>The enriched xls sheet generated in step 4 should now be merged back into the common odML structure. For this, we convert it back into the odML format and use the odMLtables merge feature F4 to replace the edited values in the common odML structure with the edited ones. Here, odMLtables merges the two odML files by extending the odML structure and appending metadata entries when the same Property is present in both files. However, when modifying already existing metadata entries in the filtered version this would result in duplication of entries. Therefore, odMLtables offers the possibility to overwrite already existing metadata entries when merging two odML structures. Note that a selective merge of a subset of metadata can be achieved by first filtering the file to be merged using feature F5.</p>
    </sec>
    <sec>
      <title>Scenario 6: Compare Entries in the odML File for Data Screening and Lab Book Usage</title>
      <p>In addition to the complete metadata representation as presented in scenario 3, it is possible to generate a reduced overview table containing only plain values of selected Properties. This feature can be used to create a tabular display of Properties of interest (e.g., weight of a specific animal, experimenter who performed the experiment and comments regarding the measurement) in rows for the individual recordings (days) in columns. An example of such a table is given below:</p>
      <table-wrap id="d35e812" position="float">
        <table frame="hsides" rules="groups">
          <tbody>
            <tr style="border-bottom: thin solid #000000;">
              <td rowspan="1" colspan="1"/>
              <td valign="top" align="left" rowspan="1" colspan="1">Scores_2000-01-01</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Scores_2000-01-02</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Date</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2000-01-01</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2000-01-02</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Weight</td>
              <td valign="top" align="left" rowspan="1" colspan="1">5.0g</td>
              <td valign="top" align="left" rowspan="1" colspan="1">5.5g</td>
            </tr>
            <tr style="border-bottom: thin solid #000000;">
              <td valign="top" align="left" rowspan="1" colspan="1">Experimenter</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Alice, ...</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Bob</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">Comment</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Blood sample was taken [...]</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Small scratch at the right ear</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>This type of overview tables can also be printed and used as part of the mandatory documentation of the experiment in a written or printed lab book. This way, the recorded data only need to be documented once in a digital fashion and consistency between documentation and digitally available metadata is guaranteed.</p>
    </sec>
    <sec>
      <title>Scenario 7: Automatized Processing of Metadata Collections</title>
      <p>After completion of an experiment covering many recording days, the processing steps presented in scenarios 1-6 can be performed in an automatized fashion on the complete metadata collection to generate a comprehensive metadata document and corresponding overviews. While it is possible to perform this action using the graphical user interface, an automated approach has the advantage that it can be repeatedly executed when one of the original files changes, e.g., by a retrospective update of metadata or loss of the generated metadata files. In addition an automated approach is more robust against errors introduced by the manual operation and can be at least partially reused for subsequent experiments.</p>
      <p>By use of the odML library together with the odMLtables Python API, users have a rich collection of functions to manipulate and convert metadata stored in the odML format. In this specific example, we show an example script in Listing 1 that loads all daily animal score sheets, adds them to a common metadata structure and exports the final document into an overview and comparative xls sheet for visualization. The code demonstrates the metadata handling workflow by structuring it into a sequence of three generic functions, which can be of use in creating related workflows for different projects.</p>
      <fig id="F9" position="float">
        <label>Listing 1</label>
        <caption>
          <p>Program to assemble a target odML document covering metadata of multiple recording days by pooling information from multiple csv files and generate visualizations and overviews. Individual functions are automatizing functionalities presented in previous scenarios.</p>
        </caption>
        <graphic xlink:href="fninf-13-00062-g0009"/>
      </fig>
    </sec>
    <sec>
      <title>Improved Handling and Visualization of Complex Metadata Structures</title>
      <p>Up to now we demonstrated the basic mechanisms of odMLtables based on highly simplified examples presented above. In a real-world example, however, metadata collections are inherently complex and corresponding metadata collections can easily encompass thousands of values. A publicly available example of this are electrophysiological recordings of macaque monkeys performing a reach to grasp task that include a rich metadata collection stored in the original odML files as well as the corresponding xls representation created by odMLtables (Brochier et al., <xref rid="B2" ref-type="bibr">2018</xref>). We demonstrate the usage of odMLtables to select and visualize a subset of the complete metadata collection as well as generation of overview tables in an interactive Jupyter Notebook in the odMLtables documentation<xref ref-type="fn" rid="fn0016"><sup>16</sup></xref> (available in pre-executed form as <xref ref-type="supplementary-material" rid="SM3">Data Sheet 2</xref>) as well as in a video tutorial (<xref ref-type="supplementary-material" rid="SM1">Supplementary Video 1</xref>).</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>We presented the odMLtables software, which facilitates the use of the odML metadata format in everyday experimental and data analysis work. To illustrate the application of odMLtables in real-world situations, we presented the features of odMLtables in seven scenarios describing a simplified realistic example, namely the definition of an animal score sheet and its use for controlled routine collection of metadata. More specifically, we showed in scenario 1 the setup of a template for an animal score sheet in the csv format and its conversion to odML (F1, F2). In the next scenario, we used this template to routinely collect the animal's health measures and aggregated them in a single odML file per animal (F1, F4). Besides a simplification of metadata acquisition in the csv format, we showed in scenario 3 the benefits of a colored tabular representation for visual inspection of the collected score sheets (F1). In scenario 4, we demonstrated how supplements of metadata values can be easily added by extracting the missing metadata entries from the complete collection (F5). Subsequently we demonstrated in scenario 5 the integration of the amended metadata back into the complete collection (F4). We generated a compressed overview table, summarizing the metadata from different routine collections in a concise format suitable for laboratory notebook in scenario 6 (F3). Finally, in scenario 7 we discussed the automation of the workflow presented in the previous scenarios and provided code examples showcasing the odMLtables Python interface.</p>
    <p>As odMLtables can be used by programmers and non-programmers alike, it simplifies the development of comprehensive metadata management in the scientific community by offering user-friendly interaction with the odML format. In this way, its usage is intended to improve reproducibility and replicability of experiments and to facilitate cooperative work, both within labs and across different laboratories. Complementing the model scenarios above, in <xref ref-type="fig" rid="F8">Figure 8</xref> we summarize and generalize the use of individual components of odMLtables during the course of an entire experiment. Although the minimalistic workflow presented here as well as the real-world workflow described in Zehl et al. (<xref rid="B29" ref-type="bibr">2016</xref>) and Brochier et al. (<xref rid="B2" ref-type="bibr">2018</xref>) are all set in the field of animal experiments covering multiple days, odML as well as odMLtables are not specific to neuroscience and can therefore be used for metadata management in other scientific disciplines. In virtually any experimental research, odMLtables provides benefits on multiple stages of the experiment: from setting up a specific metadata structure in the preparatory phase, manual enrichment of the metadata collection during the experiment, to the generation of overviews and summaries from metadata collections during data analysis. Also, for publicly available datasets with an odML metadata collection, odMLtables can be used to create a tabular representation of the odML files to quickly scan the metadata of the experiment. For example, considering the files comprising datasets hosted on public repositories, tabular, yet arbitrarily formatted representations are commonly used to supply additional information describing the dataset. This information must be parsed by custom codes in order to make it available in the analysis process. In contrast, using odMLtables, such metadata could be transformed into a structured, machine-readable representation with only moderate restrictions on formatting of the xls or csv files. Although odML and odMLtables can be used in a broader context, in the following we discuss specifically its embedding into a tools landscape developing in the field of electrophysiology.</p>
    <fig id="F8" position="float">
      <label>Figure 8</label>
      <caption>
        <p>Integrating odMLtables and other software tools in the different stages of an experiment from preparation to publication. During the preparation of an experiment odMLtables in combination with spreadsheet software is used to develop an experiment specific structure of the metadata collection (templates, F2). During the execution and documentation of the experiment, odMLtables converts (F1) between the tabular and odML representations. The compare functionality (F3) is used to generate overviews of odML Properties across different Sections of a metadata collection. The filter (F5) and merge (F4) functionalities are used to create and merge subsets of odML collections, respectively. For analysis and sharing, data can be represented using the Neo framework and annotated with metadata from the odML metadata collection using custom scripts. This combined representation can be saved in a single format using the NIX framework, e.g., to share of data and metadata via a database. In parallel, metadata collections can be incorporated in databases, for example using an export of the odML to the RDF standard.</p>
      </caption>
      <graphic xlink:href="fninf-13-00062-g0008"/>
    </fig>
    <sec>
      <title>4.1. Performance Estimation</title>
      <p>Since the release of the original version, odML has been used in various projects for storing metadata as they become available during data acquisition or analysis (e.g., in the NIX<xref ref-type="fn" rid="fn0017"><sup>17</sup></xref> and RELACS<xref ref-type="fn" rid="fn0018"><sup>18</sup></xref> projects), as metadata schema in the EEGbase database<xref ref-type="fn" rid="fn0019"><sup>19</sup></xref> (see also Mouček et al., <xref rid="B17" ref-type="bibr">2014</xref>), and as a part of the metadata data pipeline as described by Zehl et al. (<xref rid="B29" ref-type="bibr">2016</xref>) and Brochier et al. (<xref rid="B2" ref-type="bibr">2018</xref>). The advantage gained by comprehensive metadata management using odML can be demonstrated by a small example based on a published dataset (Brochier et al., <xref rid="B2" ref-type="bibr">2018</xref>) for which detailed metadata are stored in the odML format. Accessing information about the number of neurons recorded on different electrodes contained in the odML files using common desktop hardware requires ~0.5 s for this dataset using the odML iteration and filter mechanism. Extracting the same information not from the odML metadata but from the original data files using the Python library Neo version 0.7.1<xref ref-type="fn" rid="fn0020"><sup>20</sup></xref> requires about 25 s using the Neo filter and annotation mechanism. Comparing these times, the usage of odML in this example gives a speedup of a factor 50. However, for a fair comparison also the time for odML generation needs to be taken into account, where for a dataset of this complexity a realistic upper bound is on the order of 10 min, considering that the generation process needs to read the data files and a number of associated files (Zehl et al., <xref rid="B29" ref-type="bibr">2016</xref>), and perform various quality or automated preprocessing checks. Comparing this conservative estimate of the generation time of the odML file, the access time using the odML format and the access time using the original data files shows that using the odML format pays off after 25 times of metadata access. This is a relatively small number of metadata accesses for a single dataset considering the relevance of metadata in multiple steps of the experiment, e.g., exploratory analysis and parameter scans in analyses runs, and collaborative work, where different people access the same metadata on different computers. In the latter setting using odML is also of advantage because the potentially large original data files might not be present on all computers of all collaborators, whereas odML files are much smaller in file size and can therefore be shared more easily, e.g., via a version control system like git<xref ref-type="fn" rid="fn0021"><sup>21</sup></xref>.</p>
    </sec>
    <sec>
      <title>4.2. odMLtables as Conversion Tool</title>
      <p>One may argue that extending the existing odML editor to support a flattened view on the metadata is a more direct and efficient way to implement tabular representations, as opposed to a converter (such as odMLtables) between formats. However, such a solution has direct implications on (i) the maintainability of the tool, (ii) its adoption by the community, and (iii) its interoperability in the heterogeneous types of workflows typically encountered in data acquisition. Regarding (i), the development of graphical editors for tabular data is a time-consuming endeavor and leads to a complex code base that is difficult to maintain. This is even more true in a scientific environment, where software maintenance is often left to persons who are not expert in GUI programming and design patterns for graphical applications. Regarding (ii), spreadsheet software is already commonly used in laboratory environments to track metadata, and experimental scientists are used to efficiently use these tools in their daily routine. Therefore, integrating such software in a digitized workflow, rather than proposing an entirely new user-facing tool, is bound to lower the threshold for adoption in a laboratory. Finally, regarding (iii), data acquisition workflows in an experimental environment are often subject to constraints set by the individual formats in which metadata are generated by the components of the experimental setup. Tabular representations, and in particular those stored in the csv format, represent <italic>per-se</italic> one of the most commonly encountered and most simple formats to exchange data. Indeed, the capability to read csv data files is provided by the standard libraries of many programming languages, in particular those commonly used in data analysis and scientific computing, such as Python, Matlab, or R. Therefore, being able to convert between human readable tabular metadata generated automatically by various metadata sources of the experiment and their joint representation in a hierarchical odML metadata collection is helpful in creating a metadata acquisition workflow that is interoperable with the various components of the experiment. Combining such workflows with version control systems, such as git, to store the hierarchical or tabular metadata representations is a viable option to enable collaborative creation of metadata records, in particular when considering the text-based csv or odML formats.</p>
    </sec>
    <sec>
      <title>4.3. Relation to Electronic Laboratory Notebooks</title>
      <p>One particular case where flexible interoperability is in demand are electronic laboratory notebooks (ELNs) which are available from a large range of manufacturers and are becoming increasingly utilized by laboratories (Kwok and Kanza, <xref rid="B14" ref-type="bibr">2018</xref>). Their design is actively being researched in the process of digitizing the research process (Kanza et al., <xref rid="B13" ref-type="bibr">2017</xref>). ELNs are software tools originally designed to replace the hand-written lab book used in experimental sciences to document experiments, outcomes and analyses by providing a method to electronically enter such metadata in a digitally signed and potentially encrypted fashion that ensures protection from falsification. Some ELNs go beyond this functionality by integrating tightly with laboratory inventory management systems (LIMS) or analysis pipelines [comparisons of selected ELNs can be found in Rubacha et al. (<xref rid="B21" ref-type="bibr">2011</xref>) and various web resources<xref ref-type="fn" rid="fn0022"><sup>22</sup></xref>, <xref ref-type="fn" rid="fn0023"><sup>23</sup></xref>, <xref ref-type="fn" rid="fn0024"><sup>24</sup></xref>]. One major advantage of ELNs that store hard metadata (Grewe et al., <xref rid="B11" ref-type="bibr">2011</xref>) in form of key-value pairs is that they can be directly digitally accessed in analysis scripts, rather than having to manually copy the information from the hand-written lab book (Zehl et al., <xref rid="B29" ref-type="bibr">2016</xref>). While for some disciplines specialized lab notebook software packages have been developed (Kwok and Kanza, <xref rid="B14" ref-type="bibr">2018</xref>) that are aware of community standards for storing such metadata, most of these packages come with their own format for storing data that can only be accessed via file export functionality or specific APIs. In some disciplines this may be of little importance, since either the metadata records stored in the ELN are not required in the analysis process, or the metadata are captured using a domain-specific ELN that is integrated with functionality to directly perform the analysis steps from within the ELN. Nonetheless, other disciplines, such as neurophysiology, require detailed metadata available in an environment suitable for performing complex, exploratory analysis protocols that go beyond the capabilities of currently available ELNs. Here, odML is a potential candidate for implementing such features. In absence of a global standard to record metadata, csv represents one of the de-facto standards to export metadata from ELNs in a universal format. For this reason, the conversion to odML via odMLtables provides access to metadata recorded with ELNs for external analysis pipelines that rely on hierarchically structured metadata collections. The same holds true for the reverse direction, where metadata generated by tools building on the odML specifications can be imported into an ELN. For example, the feature of odMLtables to create tabular overviews of the metadata (feature F3, see section 2.2) would allow to generate current overview tables in terms of animal score sheets as csv that could be directly (and assuming the ELN has an API, even automatically) integrated into the documentation of an experiment contained within an ELN, assuming only basic csv import capabilities.</p>
      <p>Beyond ELNs, labs increasingly resort to institution-wide databases to manage and record their research activities, and, in some cases, even the data as such. Depending on the architecture, some systems are likely to implement data imports using tabular schemata. One example of such a tool implementing database and processing functionality is DataJoint<xref ref-type="fn" rid="fn0025"><sup>25</sup></xref> as a tool to assist in ingesting, combining and analyzing heterogeneous data in a relational database (Yatsenko et al., <xref rid="B28" ref-type="bibr">2015</xref>). It is easy to populate a DataJoint database using tabular data, as described in detail in the accompanying online documentation. For example, one may extract a subset of the metadata in form of a comparison table using the odMLtables feature F3, and then incorporate this table into a larger DataJoint database spanning all experiments using a generic function for populating from csv tables. In such a fashion, odMLtables presents a gateway to integrate structured metadata by the diverse tools used in a laboratory to organize the record keeping of an experiment.</p>
    </sec>
    <sec>
      <title>4.4. Outlook</title>
      <p>The current version of odMLtables provides a set of core functions that were identified as necessary in co-designing various data and metadata acquisition workflows in collaboration with multiple laboratories spanning different types of experiments and data modalities. Nevertheless, a number of additional features are envisioned as a result of feedback received from these collaborations to extend the range of applications for the tool and enhance its flexibility for heterogeneous metadata workflows. In addition, feature requests are welcome on the project's issue system on github. One next step will be to extend the capability to create tabular comparisons (feature F3) across metadata stores in multiple files. This would give researchers the option to query for metadata that are distributed over several, even differently structured, odML files. For example, in chronic recordings of brain activity accumulated over the course of multiple months, researchers may decide to generate a single odML file per recording day, and may want to utilize such a functionality to compare the number of trials and other performance measures across the entire recording period.</p>
      <p>A second planned feature addition to odMLtables, related to the previous aspect, is the ability to create complete tabular representations (i.e., feature F1) across multiple odML files, and vice versa. To this end, one may implement an additional column next to the odML path and Property name that indicates the file in which a certain metadata entry is found. As an example application, one may consider a complex experiment where metadata originating from different parts of the experiment are stored in separate odML files, but a large overview table is desired for manually browsing the metadata. While this is already possible by merging (F4) individual files and then converting (F1) the table, the information about the origin of metadata in the original file structure is lost.</p>
      <p>A third feature addition to odMLtables is the automatic generation of Python code based on the steps the user performs in the graphical user interface. For example, this may yield the Python code to perform a certain filter operation designed in the GUI. This would simplify the automation of metadata processing without specific knowledge about the odMLtables API.</p>
      <p>For communicating the structure of a complex metadata collection to new collaborators neither tabular nor hierarchical views have been found to be efficient. For this, a graphical representation of the metadata structure is likely to be more useful, especially for large metadata collections. For this reason, a fourth addition to odMLtables would be to introduce a common graphical representation as new output conversion format.</p>
      <p>Lastly, as a fifth feature addition, odMLtables could assist scientists in defining the links between data and metadata in an experiment. Typically, several metadata are accumulated from various sources in an experiment that are directly related to one particular part of the data, and in fact, may be crucial in performing data analysis. For example, the signal recorded from a particular electrode may contain the impedance as measured by the manufacturer as well as noise estimated from a pre-processing step. Due to the heterogeneity of experiments and metadata descriptions, it is currently not feasible to establish these connections between data and metadata automatically, e.g., using a predefined mapping based on Property values. Instead, the mapping is carried out manually by implementing customized code that annotates data with metadata during the loading process. Even when data can be loaded via standardized data framework (e.g., Neo<xref ref-type="fn" rid="fn0026"><sup>26</sup></xref>, see also Garcia et al., <xref rid="B9" ref-type="bibr">2014</xref>), the annotation of data objects with metadata taken from a standardized metadata collection (e.g., odML), has to be performed independently (see <xref ref-type="fig" rid="F8">Figure 8</xref>). This complicates the process of reading data, and is not transparent to an external user. A possible way of reducing the implementation effort to create experiment-specific annotation of data with metadata, would be to store the relations between data and metadata directly in the metadata structure. For example, using odMLtables, we suggest to add supplementary fields to the table that directly link blocks of metadata to specific data, e.g., to channels with a certain channel ID or to events with specific IDs. In this way, compact objects containing both data and selected metadata could be loaded using a single, generic loading routine. Moreover, by providing odMLtables with a feature to export to NIX<xref ref-type="fn" rid="fn0027"><sup>27</sup></xref>, e.g., using the odML-NIX conversion tool<xref ref-type="fn" rid="fn0028"><sup>28</sup></xref>, as an additional output file format that combines odML-like metadata with primary data (Stoewer et al., <xref rid="B23" ref-type="bibr">2014</xref>), such that combined data/metadata objects could be easily serialized to disk.</p>
      <p>Validation of user generated input is implemented on the level of odML: When saving or loading an odML file via odMLtables or any other method, the odML structure is checked for basic integrity (e.g., consistency of data types and values). It is intended to support custom, user defined validations in future releases. That is, users will be provided with the means of defining own validations to check for required Sections, Properties or Values and combinations thereof. These additional validations will be directly stored within the odML files. They can be applied to ensure metadata consistency even if the file is handled on a different system or by a different person. For example users would be able to define specific Values as required for a particular Property or make sure a Section tree with an experiment-specific content is present before the file can be saved.</p>
      <p>In recent years, the scientific community has begun to recognize the need for developing workflows that enable rigorous data management not only to ensure reproducibility, but also to expedite research through efficient data sharing among scientists. The principles governing corresponding data management practices are summarized under the FAIR (Findable, Accessible, Interoperable, Reusable) principles (Wilkinson et al., <xref rid="B27" ref-type="bibr">2016</xref>). The requirement to make data globally findable has lead to the emergence of multiple resources commonly subsurmised under the term “Knowledge Graph,” referring to a graph-like linkage of metadata through an appropriate ontologies, for example, as done in the Knowledge Graph of the Human Brain Project<xref ref-type="fn" rid="fn0029"><sup>29</sup></xref>. The resource description format, RDF<xref ref-type="fn" rid="fn0030"><sup>30</sup></xref>, is a semantic web technology that provides one possible standard interface to populate such metadata graphs (cf., <xref ref-type="fig" rid="F8">Figure 8</xref>). The complexity of creating RDF descriptions from scratch can be simplified by exploiting the functionality of odML to export RDF schemata from odML files. In this context, odMLtables can be incorporated as a bridge to support researchers in easily entering predefined metadata schemata to expose their data records in large-scale Knowledge Graph infrastructures.</p>
      <p>odMLtables is actively developed and a comprehensive documentation including a tutorial is available for release versions on ReadtheDocs<xref ref-type="fn" rid="fn0031"><sup>31</sup></xref> and the latest version can be obtained from GitHub<xref ref-type="fn" rid="fn0032"><sup>32</sup></xref>. Future developments of odMLtables include the ongoing embedding of odMLtables in different neuroscientific data and metadata aggregation workflows, and, as a long term prospect, odMLtables is planned to become a fully integrated component of the odML and NIX libraries.</p>
    </sec>
  </sec>
  <sec id="s5">
    <title>5. Current Code Version</title>
    <table-wrap id="d35e1066" position="float">
      <table frame="hsides" rules="groups">
        <tbody>
          <tr style="border-bottom: thin solid #000000;">
            <td valign="top" align="left" rowspan="1" colspan="1">Code version</td>
            <td valign="top" align="left" rowspan="1" colspan="1">1.0.0</td>
          </tr>
          <tr style="border-bottom: thin solid #000000;">
            <td valign="top" align="left" rowspan="1" colspan="1">Permanent link to code/repository</td>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <ext-link ext-link-type="uri" xlink:href="https://github.com/INM-6/python-odmltables">https://github.com/INM-6/python-odmltables</ext-link>
            </td>
          </tr>
          <tr style="border-bottom: thin solid #000000;">
            <td valign="top" align="left" rowspan="1" colspan="1">Documentation</td>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <ext-link ext-link-type="uri" xlink:href="https://odmltables.readthedocs.io">https://odmltables.readthedocs.io</ext-link>
            </td>
          </tr>
          <tr style="border-bottom: thin solid #000000;">
            <td valign="top" align="left" rowspan="1" colspan="1">Support</td>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <ext-link ext-link-type="uri" xlink:href="https://github.com/INM-6/python-odmltables/issues">https://github.com/INM-6/python-odmltables/issues</ext-link>
            </td>
          </tr>
          <tr style="border-bottom: thin solid #000000;">
            <td valign="top" align="left" rowspan="1" colspan="1">Programming Language</td>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <ext-link ext-link-type="uri" xlink:href="https://www.python.org/">Python</ext-link>
            </td>
          </tr>
          <tr style="border-bottom: thin solid #000000;">
            <td valign="top" align="left" rowspan="1" colspan="1">Key Dependencies</td>
            <td valign="top" align="left" rowspan="1" colspan="1"><ext-link ext-link-type="uri" xlink:href="http://g-node.github.io/python-odml/">odML</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://wiki.python.org/moin/PyQt">PyQt5</ext-link></td>
          </tr>
          <tr style="border-bottom: thin solid #000000;">
            <td valign="top" align="left" rowspan="1" colspan="1">Research Resource Identifier (RRID)</td>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/RRID:SCR_016228">SCR_016228</ext-link>
            </td>
          </tr>
          <tr style="border-bottom: thin solid #000000;">
            <td valign="top" align="left" rowspan="1" colspan="1">Legal Code License</td>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <ext-link ext-link-type="uri" xlink:href="https://github.com/INM-6/python-odmltables/blob/master/LICENSE.txt">BSD 3-Clause</ext-link>
            </td>
          </tr>
          <tr>
            <td valign="top" align="left" rowspan="1" colspan="1">Logo</td>
            <td valign="top" align="left" rowspan="1" colspan="1">
              <inline-graphic xlink:href="fninf-13-00062-i0002.jpg"/>
            </td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
  <sec sec-type="data-availability" id="s6">
    <title>Data Availability</title>
    <p>Publicly available datasets were analyzed in this study. This data can be found at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.12751/g-node.f83565">https://doi.org/10.12751/g-node.f83565</ext-link>.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>JS and LZ were involved in the design and development of the odMLtables software. JP developed a prototype of the odMLtables software. LZ initiated the odMLtables project. MS, JG, and TW were involved in the development of the odML software. All authors contributed to writing the manuscript.</p>
    <sec>
      <title>Conflict of Interest Statement</title>
      <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
    </sec>
  </sec>
</body>
<back>
  <ack>
    <p>We thank Carlos Canova who was involved in the implementation of example data management workflows on the basis of odMLtables. Furthermore, we thank our experimental partners Frédéric Barthélemy, Thomas Brochier, Alexa Riehle, Sebastian Bitzenhofer, Joachim Ahlbeck, Ileana Hanganu-Opatz, Kentaro Takagaki, and Frank Ohl for their support in setting up metadata acquisition workflows based on odMLtables.</p>
  </ack>
  <fn-group>
    <fn id="fn0001">
      <p><sup>1</sup>See e.g., <ext-link ext-link-type="uri" xlink:href="https://www.oxygenxml.com/xml_editor/xml_grid_editor.html">https://www.oxygenxml.com/xml_editor/xml_grid_editor.html</ext-link> or <ext-link ext-link-type="uri" xlink:href="http://rustemsoft.com/xfox.aspx">http://rustemsoft.com/xfox.aspx</ext-link></p>
    </fn>
    <fn id="fn0002">
      <p>
        <sup>2</sup>
        <ext-link ext-link-type="uri" xlink:href="https://products.office.com/en-us/excel">https://products.office.com/en-us/excel</ext-link>
      </p>
    </fn>
    <fn id="fn0003">
      <p>
        <sup>3</sup>
        <ext-link ext-link-type="uri" xlink:href="https://www.libreoffice.org/discover/calc">https://www.libreoffice.org/discover/calc</ext-link>
      </p>
    </fn>
    <fn id="fn0004">
      <p><sup>4</sup><ext-link ext-link-type="uri" xlink:href="https://github.com/G-Node/python-odml">https://github.com/G-Node/python-odml</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/RRID:SCR_001376">RRID:SCR_001376</ext-link></p>
    </fn>
    <fn id="fn0005">
      <p>
        <sup>5</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/G-Node/python-odml/blob/master/doc/tutorial.rst">https://github.com/G-Node/python-odml/blob/master/doc/tutorial.rst</ext-link>
      </p>
    </fn>
    <fn id="fn0006">
      <p>
        <sup>6</sup>
        <ext-link ext-link-type="uri" xlink:href="http://g-node.github.io/python-odml">http://g-node.github.io/python-odml</ext-link>
      </p>
    </fn>
    <fn id="fn0007">
      <p>
        <sup>7</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/G-Node/python-odml/releases/tag/v1.4.0">https://github.com/G-Node/python-odml/releases/tag/v1.4.0</ext-link>
      </p>
    </fn>
    <fn id="fn0008">
      <p>
        <sup>8</sup>
        <ext-link ext-link-type="uri" xlink:href="http://www.w3.org/TR/rdf-primer">http://www.w3.org/TR/rdf-primer</ext-link>
      </p>
    </fn>
    <fn id="fn0009">
      <p>
        <sup>9</sup>
        <ext-link ext-link-type="uri" xlink:href="https://json.org">https://json.org</ext-link>
      </p>
    </fn>
    <fn id="fn0010">
      <p>
        <sup>10</sup>
        <ext-link ext-link-type="uri" xlink:href="https://yaml.org">https://yaml.org</ext-link>
      </p>
    </fn>
    <fn id="fn0011">
      <p>
        <sup>11</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/G-Node/odml-ui">https://github.com/G-Node/odml-ui</ext-link>
      </p>
    </fn>
    <fn id="fn0012">
      <p>
        <sup>12</sup>
        <ext-link ext-link-type="uri" xlink:href="https://odmltables.readthedocs.io">https://odmltables.readthedocs.io</ext-link>
      </p>
    </fn>
    <fn id="fn0013">
      <p>
        <sup>13</sup>
        <ext-link ext-link-type="uri" xlink:href="https://wiki.python.org/moin/PyQt">https://wiki.python.org/moin/PyQt</ext-link>
      </p>
    </fn>
    <fn id="fn0014">
      <p>
        <sup>14</sup>
        <ext-link ext-link-type="uri" xlink:href="https://jupyter.org/">https://jupyter.org/</ext-link>
      </p>
    </fn>
    <fn id="fn0015">
      <p>
        <sup>15</sup>
        <ext-link ext-link-type="uri" xlink:href="https://odmltables.readthedocs.io/en/latest/tutorial.html">https://odmltables.readthedocs.io/en/latest/tutorial.html</ext-link>
      </p>
    </fn>
    <fn id="fn0016">
      <p>
        <sup>16</sup>
        <ext-link ext-link-type="uri" xlink:href="https://odmltables.readthedocs.io/en/latest/tutorial.html">https://odmltables.readthedocs.io/en/latest/tutorial.html</ext-link>
      </p>
    </fn>
    <fn id="fn0017">
      <p><sup>17</sup><ext-link ext-link-type="uri" xlink:href="https://github.com/G-Node/nix">https://github.com/G-Node/nix</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/RRID:SCR_016196">RRID:SCR_016196</ext-link></p>
    </fn>
    <fn id="fn0018">
      <p>
        <sup>18</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/relacs/relacs">https://github.com/relacs/relacs</ext-link>
      </p>
    </fn>
    <fn id="fn0019">
      <p><sup>19</sup><ext-link ext-link-type="uri" xlink:href="http://eeg2.kiv.zcu.cz:8080/home-page?1">http://eeg2.kiv.zcu.cz:8080/home-page?1</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/RRID:nif-0000-08190">RRID:nif-0000-08190</ext-link></p>
    </fn>
    <fn id="fn0020">
      <p>
        <sup>20</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/NeuralEnsemble/python-neo/releases/tag/0.7.1">https://github.com/NeuralEnsemble/python-neo/releases/tag/0.7.1</ext-link>
      </p>
    </fn>
    <fn id="fn0021">
      <p>
        <sup>21</sup>
        <ext-link ext-link-type="uri" xlink:href="https://git-scm.com">https://git-scm.com</ext-link>
      </p>
    </fn>
    <fn id="fn0022">
      <p>
        <sup>22</sup>
        <ext-link ext-link-type="uri" xlink:href="https://datamanagement.hms.harvard.edu/electronic-lab-notebooks">https://datamanagement.hms.harvard.edu/electronic-lab-notebooks</ext-link>
      </p>
    </fn>
    <fn id="fn0023">
      <p>
        <sup>23</sup>
        <ext-link ext-link-type="uri" xlink:href="https://www.labfolder.com/electronic-lab-notebook-eln-research-guide">https://www.labfolder.com/electronic-lab-notebook-eln-research-guide</ext-link>
      </p>
    </fn>
    <fn id="fn0024">
      <p>
        <sup>24</sup>
        <ext-link ext-link-type="uri" xlink:href="https://www.gurdon.cam.ac.uk/institute-life/computing/elnguidance">https://www.gurdon.cam.ac.uk/institute-life/computing/elnguidance</ext-link>
      </p>
    </fn>
    <fn id="fn0025">
      <p><sup>25</sup><ext-link ext-link-type="uri" xlink:href="https://datajoint.io/">https://datajoint.io/</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/RRID:SCR_014543">RRID:SCR_014543</ext-link></p>
    </fn>
    <fn id="fn0026">
      <p><sup>26</sup><ext-link ext-link-type="uri" xlink:href="https://github.com/NeuralEnsemble/python-neo">https://github.com/NeuralEnsemble/python-neo</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/RRID:SCR_000634">RRID:SCR_000634</ext-link></p>
    </fn>
    <fn id="fn0027">
      <p><sup>27</sup><ext-link ext-link-type="uri" xlink:href="http://g-node.github.io/nix">http://g-node.github.io/nix</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://scicrunch.org/resolver/RRID:SCR_016196">RRID:SCR_016196</ext-link></p>
    </fn>
    <fn id="fn0028">
      <p>
        <sup>28</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/G-Node/nix-odML-converter">https://github.com/G-Node/nix-odML-converter</ext-link>
      </p>
    </fn>
    <fn id="fn0029">
      <p>
        <sup>29</sup>
        <ext-link ext-link-type="uri" xlink:href="https://www.humanbrainproject.eu/en/explore-the-brain">https://www.humanbrainproject.eu/en/explore-the-brain</ext-link>
      </p>
    </fn>
    <fn id="fn0030">
      <p>
        <sup>30</sup>
        <ext-link ext-link-type="uri" xlink:href="https://www.w3.org/RDF">https://www.w3.org/RDF</ext-link>
      </p>
    </fn>
    <fn id="fn0031">
      <p>
        <sup>31</sup>
        <ext-link ext-link-type="uri" xlink:href="https://odmltables.readthedocs.io/en/latest">https://odmltables.readthedocs.io/en/latest</ext-link>
      </p>
    </fn>
    <fn id="fn0032">
      <p>
        <sup>32</sup>
        <ext-link ext-link-type="uri" xlink:href="https://github.com/INM-6/python-odmltables/">https://github.com/INM-6/python-odmltables/</ext-link>
      </p>
    </fn>
  </fn-group>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> Partial funding was obtained by SPP1665 (GR1753/4-2, DE2175/2-1), the Helmholtz Portfolio Supercomputing and Modeling for the Human Brain, and the European Union Horizon 2020 Framework Programme for Research and Innovation under Specific Grant Agreements No. 720270 (Human Brain Project SGA1) and 785907 (Human Brain Project SGA2), HDS-LEE: Helmholtz School for Data Science in Life, Earth and Energy (Jülich, Aachen, Köln), and funding for the development of the odML core library was obtained via BMBF grants 01GQ1302 and 01GQ1509.</p>
    </fn>
  </fn-group>
  <sec sec-type="supplementary-material" id="s8">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fninf.2019.00062/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/fninf.2019.00062/full#supplementary-material</ext-link></p>
    <supplementary-material content-type="local-data" id="SM1">
      <label>Supplementary Video 1</label>
      <caption>
        <p>Video demonstration of the graphical user interface of odMLtables.</p>
      </caption>
      <media xlink:href="Video_1.MP4">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM2">
      <label>Data Sheet 1</label>
      <caption>
        <p>Pre-executed Jupyter Notebook implementing scenarios 1 to 7.</p>
      </caption>
      <media xlink:href="Data_Sheet_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="SM3">
      <label>Data Sheet 2</label>
      <caption>
        <p>Pre-exectuted Jupyter Notebook demonstrating the use of odMLtables with a complex electrophysiology dataset.</p>
      </caption>
      <media xlink:href="Data_Sheet_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bitzenhofer</surname><given-names>S. H.</given-names></name><name><surname>Ahlbeck</surname><given-names>J.</given-names></name><name><surname>Wolff</surname><given-names>A.</given-names></name><name><surname>Wiegert</surname><given-names>J. S.</given-names></name><name><surname>Gee</surname><given-names>C. E.</given-names></name><name><surname>Oertner</surname><given-names>T. G.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Layer-specific optogenetic activation of pyramidal neurons causes betagamma entrainment of neonatal networks</article-title>. <source>Nat. Commun.</source>
<volume>8</volume>:<fpage>14563</fpage>
<pub-id pub-id-type="doi">10.1038/ncomms14563</pub-id><pub-id pub-id-type="pmid">28216627</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brochier</surname><given-names>T.</given-names></name><name><surname>Zehl</surname><given-names>L.</given-names></name><name><surname>Hao</surname><given-names>Y.</given-names></name><name><surname>Duret</surname><given-names>M.</given-names></name><name><surname>Sprenger</surname><given-names>J.</given-names></name><name><surname>Denker</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Massively parallel recordings in macaque motor cortex during an instructed delayed reach-to-grasp task</article-title>. <source>Sci. Data</source><volume>5</volume>:<fpage>180055</fpage>. <pub-id pub-id-type="doi">10.1038/sdata.2018.55</pub-id><?supplied-pmid 29633986?><pub-id pub-id-type="pmid">29633986</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkholder</surname><given-names>T.</given-names></name><name><surname>Foltz</surname><given-names>C.</given-names></name><name><surname>Karlsson</surname><given-names>E.</given-names></name><name><surname>Linton</surname><given-names>C. G.</given-names></name><name><surname>Smith</surname><given-names>J. M.</given-names></name></person-group> (<year>2012</year>). <article-title>Health evaluation of experimental laboratory mice</article-title>. <source>Curr. Protoc. Mouse Biol.</source>
<volume>2</volume>, <fpage>145</fpage>–<lpage>165</lpage>. <pub-id pub-id-type="doi">10.1002/9780470942390.mo110217</pub-id><?supplied-pmid 22822473?><pub-id pub-id-type="pmid">22822473</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Coles</surname><given-names>S.</given-names></name><name><surname>Carr</surname><given-names>L.</given-names></name><name><surname>Frey</surname><given-names>J.</given-names></name></person-group> (<year>2008</year>). <article-title>Experiences with repositories and blogs in laboratories</article-title>, in <source>Open Repositories 2008</source> (University of Southampton Institutional Repository). Available online at: <ext-link ext-link-type="uri" xlink:href="https://eprints.soton.ac.uk/id/eprint/50901">https://eprints.soton.ac.uk/id/eprint/50901</ext-link></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deisseroth</surname><given-names>K.</given-names></name><name><surname>Schnitzer</surname><given-names>M. J.</given-names></name></person-group> (<year>2013</year>). <article-title>Engineering approaches to illuminating brain structure and dynamics</article-title>. <source>Neuron</source>
<volume>80</volume>, <fpage>568</fpage>–<lpage>577</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.032</pub-id><?supplied-pmid 24183010?><pub-id pub-id-type="pmid">24183010</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Denker</surname><given-names>M.</given-names></name><name><surname>Grün</surname><given-names>S.</given-names></name></person-group> (<year>2016</year>). <article-title>Designing workflows for the reproducible analysis of electrophysiological data</article-title>, in <source>Brain-Inspired Computing</source>, Vol. <volume>10087</volume>, eds <person-group person-group-type="editor"><name><surname>Amunts</surname><given-names>K.</given-names></name><name><surname>Grandinetti</surname><given-names>L.</given-names></name><name><surname>Lippert</surname><given-names>T.</given-names></name><name><surname>Petkov</surname><given-names>N.</given-names></name></person-group> (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer International Publishing</publisher-name>), <fpage>58</fpage>–<lpage>72</lpage>. Bibtex: Denker16_58</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denker</surname><given-names>M.</given-names></name><name><surname>Zehl</surname><given-names>L.</given-names></name><name><surname>Kilavik</surname><given-names>B. E.</given-names></name><name><surname>Diesmann</surname><given-names>M.</given-names></name><name><surname>Brochier</surname><given-names>T.</given-names></name><name><surname>Riehle</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>LFP beta amplitude is linked to mesoscopic spatio-temporal phase patterns</article-title>. <source>Sci. Rep.</source><volume>8</volume>:<fpage>5200</fpage>. <pub-id pub-id-type="doi">10.1038/s41598-018-22990-7</pub-id><?supplied-pmid 29581430?><pub-id pub-id-type="pmid">29581430</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foltz</surname><given-names>C. J.</given-names></name><name><surname>Ullman-Cullere</surname><given-names>M.</given-names></name></person-group> (<year>1999</year>). <article-title>Guidelines for assessing the health and condition of mice</article-title>. <source>Lab Anim.</source>
<volume>28</volume>:<fpage>5</fpage>.</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia</surname><given-names>S.</given-names></name><name><surname>Guarino</surname><given-names>D.</given-names></name><name><surname>Jaillet</surname><given-names>F.</given-names></name><name><surname>Jennings</surname><given-names>T.</given-names></name><name><surname>Pröpper</surname><given-names>R.</given-names></name><name><surname>Rautenberg</surname><given-names>P. L.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Neo: an object model for handling electrophysiology data in multiple formats</article-title>. <source>Front. Neuroinform.</source><volume>8</volume>:<fpage>10</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2014.00010</pub-id><?supplied-pmid 24600386?><pub-id pub-id-type="pmid">24600386</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>W. S.</given-names></name></person-group> (<year>2008</year>). <article-title>Visual perception and the statistical properties of natural scenes</article-title>. <source>Annu. Rev. Psychol.</source>
<volume>59</volume>, <fpage>167</fpage>–<lpage>192</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.psych.58.110405.085632</pub-id><?supplied-pmid 17705683?><pub-id pub-id-type="pmid">17705683</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grewe</surname><given-names>J.</given-names></name><name><surname>Wachtler</surname><given-names>T.</given-names></name><name><surname>Benda</surname><given-names>J.</given-names></name></person-group> (<year>2011</year>). <article-title>A bottom-up approach to data annotation in neurophysiology</article-title>. <source>Front. Neuroinform.</source>
<volume>5</volume>:<fpage>16</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2011.00016</pub-id><?supplied-pmid 21941477?><pub-id pub-id-type="pmid">21941477</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>V.</given-names></name><name><surname>Estebanez</surname><given-names>L.</given-names></name><name><surname>Le Cam</surname><given-names>J.</given-names></name><name><surname>Tiercelin</surname><given-names>J.-Y.</given-names></name><name><surname>Parra</surname><given-names>P.</given-names></name><name><surname>Parésys</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2010</year>). <article-title>The matrix: a new tool for probing the whisker-to-barrel system with natural stimuli</article-title>. <source>J. Neurosci. Methods</source><volume>189</volume>, <fpage>65</fpage>–<lpage>74</lpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2010.03.020</pub-id><?supplied-pmid 20362614?><pub-id pub-id-type="pmid">20362614</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanza</surname><given-names>S.</given-names></name><name><surname>Willoughby</surname><given-names>C.</given-names></name><name><surname>Gibbins</surname><given-names>N.</given-names></name><name><surname>Whitby</surname><given-names>R.</given-names></name><name><surname>Frey</surname><given-names>J. G.</given-names></name><name><surname>Erjavec</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Electronic lab notebooks: can they replace paper?</article-title><source>J. Cheminform.</source><volume>9</volume>:<fpage>31</fpage>. <pub-id pub-id-type="doi">10.1186/s13321-017-0221-3</pub-id><?supplied-pmid 29086051?><pub-id pub-id-type="pmid">29086051</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwok</surname><given-names>R.</given-names></name><name><surname>Kanza</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>). <article-title>Lab notebooks go digital</article-title>. <source>Nature</source>
<volume>560</volume>:<fpage>269</fpage>. <pub-id pub-id-type="doi">10.1038/d41586-018-05895-3</pub-id><?supplied-pmid 30082695?><pub-id pub-id-type="pmid">30082695</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maldonado</surname><given-names>P.</given-names></name><name><surname>Babul</surname><given-names>C.</given-names></name><name><surname>Singer</surname><given-names>W.</given-names></name><name><surname>Rodriguez</surname><given-names>E.</given-names></name><name><surname>Berger</surname><given-names>D.</given-names></name><name><surname>Grün</surname><given-names>S.</given-names></name></person-group> (<year>2008</year>). <article-title>Synchronization of neuronal responses in primary visual cortex of monkeys viewing natural images</article-title>. <source>J. Neurophysiol.</source>
<volume>100</volume>, <fpage>1523</fpage>–<lpage>1532</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00076.2008</pub-id><?supplied-pmid 18562559?><pub-id pub-id-type="pmid">18562559</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miyamoto</surname><given-names>D.</given-names></name><name><surname>Murayama</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>). <article-title>The fiber-optic imaging and manipulation of neural activity during animal behavior</article-title>. <source>Neurosci. Res.</source>
<volume>103</volume>:<fpage>1</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.neures.2015.09.004</pub-id><?supplied-pmid 26427958?><pub-id pub-id-type="pmid">26427958</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mouček</surname><given-names>R.</given-names></name><name><surname>Brha</surname><given-names>P.</given-names></name><name><surname>Jezek</surname><given-names>P.</given-names></name><name><surname>Mautner</surname><given-names>P.</given-names></name><name><surname>Novotny</surname><given-names>J.</given-names></name><name><surname>Papez</surname><given-names>V.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Software and hardware infrastructure for research in electrophysiology</article-title>. <source>Front. Neuroinform.</source><volume>8</volume>:<fpage>20</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2014.00020</pub-id><?supplied-pmid 24639646?><pub-id pub-id-type="pmid">24639646</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicolelis</surname><given-names>M. A. L.</given-names></name><name><surname>Ribeiro</surname><given-names>S.</given-names></name></person-group> (<year>2002</year>). <article-title>Multielectrode recordings: the next steps</article-title>. <source>Curr. Opin. Neurobiol.</source>
<volume>12</volume>, <fpage>602</fpage>–<lpage>606</lpage>. <pub-id pub-id-type="doi">10.1016/S0959-4388(02)00374-4</pub-id><?supplied-pmid 12367642?><pub-id pub-id-type="pmid">12367642</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obien</surname><given-names>M. E. J.</given-names></name><name><surname>Deligkaris</surname><given-names>K.</given-names></name><name><surname>Bullmann</surname><given-names>T.</given-names></name><name><surname>Bakkum</surname><given-names>D. J.</given-names></name><name><surname>Frey</surname><given-names>U.</given-names></name></person-group> (<year>2014</year>). <article-title>Revealing neuronal function through microelectrode array recordings</article-title>. <source>Front. Neurosci.</source>
<volume>8</volume>:<fpage>423</fpage>. <pub-id pub-id-type="doi">10.3389/fnins.2014.00423</pub-id><?supplied-pmid 25610364?><pub-id pub-id-type="pmid">25610364</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohl</surname><given-names>F. W.</given-names></name><name><surname>Scheich</surname><given-names>H.</given-names></name><name><surname>Freeman</surname><given-names>W. J.</given-names></name></person-group> (<year>2001</year>). <article-title>Change in pattern of ongoing cortical activity with auditory category learning</article-title>. <source>Nature</source>
<volume>412</volume>, <fpage>733</fpage>–<lpage>736</lpage>. <pub-id pub-id-type="doi">10.1038/35089076</pub-id><?supplied-pmid 11507640?><pub-id pub-id-type="pmid">11507640</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubacha</surname><given-names>M.</given-names></name><name><surname>Rattan</surname><given-names>A. K.</given-names></name><name><surname>Hosselet</surname><given-names>S. C.</given-names></name></person-group> (<year>2011</year>). <article-title>A review of electronic laboratory notebooks available in the market today</article-title>. <source>J. Lab. Autom.</source>
<volume>16</volume>, <fpage>90</fpage>–<lpage>98</lpage>. <pub-id pub-id-type="doi">10.1016/j.jala.2009.01.002</pub-id><?supplied-pmid 21609689?><pub-id pub-id-type="pmid">21609689</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname><given-names>D. A.</given-names></name><name><surname>Lebedev</surname><given-names>M. A.</given-names></name><name><surname>Hanson</surname><given-names>T. L.</given-names></name><name><surname>Dimitrov</surname><given-names>D. F.</given-names></name><name><surname>Lehew</surname><given-names>G.</given-names></name><name><surname>Meloy</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Chronic, wireless recordings of large-scale brain activity in freely moving Rhesus monkeys</article-title>. <source>Nat. Meth.</source><volume>11</volume>, <fpage>670</fpage>–<lpage>676</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2936</pub-id><?supplied-pmid 24776634?><pub-id pub-id-type="pmid">24776634</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoewer</surname><given-names>A.</given-names></name><name><surname>Kellner</surname><given-names>C. J.</given-names></name><name><surname>Benda</surname><given-names>J.</given-names></name><name><surname>Wachtler</surname><given-names>T.</given-names></name><name><surname>Grewe</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>File format and library for neuroscience data and metadata</article-title>. <source>Front. Neuroinform.</source>
<pub-id pub-id-type="doi">10.3389/conf.fninf.2014.18.00027</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tebaykin</surname><given-names>D.</given-names></name><name><surname>Tripathy</surname><given-names>S. J.</given-names></name><name><surname>Binnion</surname><given-names>N.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name><name><surname>Gerkin</surname><given-names>R. C.</given-names></name><name><surname>Pavlidis</surname><given-names>P.</given-names></name></person-group> (<year>2018</year>). <article-title>Modeling sources of interlaboratory variability in electrophysiological properties of mammalian neurons</article-title>. <source>J. Neurophysiol.</source>
<volume>119</volume>, <fpage>1329</fpage>–<lpage>1339</lpage>. <pub-id pub-id-type="doi">10.1152/jn.00604.2017</pub-id><?supplied-pmid 29357465?><pub-id pub-id-type="pmid">29357465</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vargas-Irwin</surname><given-names>C. E.</given-names></name><name><surname>Shakhnarovich</surname><given-names>G.</given-names></name><name><surname>Yadollahpour</surname><given-names>P.</given-names></name><name><surname>Mislow</surname><given-names>J. M. K.</given-names></name><name><surname>Black</surname><given-names>M. J.</given-names></name><name><surname>Donoghue</surname><given-names>J. P.</given-names></name></person-group> (<year>2010</year>). <article-title>Decoding complete reach and grasp actions from local primary motor cortex populations</article-title>. <source>J. Neurosci.</source>
<volume>30</volume>, <fpage>9659</fpage>–<lpage>9669</lpage>. <pub-id pub-id-type="doi">10.1523/jneurosci.5443-09.2010</pub-id><?supplied-pmid 20660249?><pub-id pub-id-type="pmid">20660249</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verkhratsky</surname><given-names>A.</given-names></name><name><surname>Krishtal</surname><given-names>O. A.</given-names></name><name><surname>Petersen</surname><given-names>O. H.</given-names></name></person-group> (<year>2006</year>). <article-title>From galvani to patch clamp: the development of electrophysiology</article-title>. <source>Pflugers Arch.</source>
<volume>453</volume>, <fpage>233</fpage>–<lpage>247</lpage>. <pub-id pub-id-type="doi">10.1007/s00424-006-0169-z</pub-id><?supplied-pmid 17072639?><pub-id pub-id-type="pmid">17072639</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilkinson</surname><given-names>M. D.</given-names></name><name><surname>Dumontier</surname><given-names>M.</given-names></name><name><surname>Aalbersberg</surname><given-names>I. J.</given-names></name><name><surname>Appleton</surname><given-names>G.</given-names></name><name><surname>Axton</surname><given-names>M.</given-names></name><name><surname>Baak</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>The FAIR guiding principles for scientific data management and stewardship</article-title>. <source>Sci. Data</source><volume>3</volume>:<fpage>160018</fpage>. <pub-id pub-id-type="doi">10.1038/sdata.2016.18</pub-id><?supplied-pmid 26978244?><pub-id pub-id-type="pmid">26978244</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yatsenko</surname><given-names>D.</given-names></name><name><surname>Reimer</surname><given-names>J.</given-names></name><name><surname>Ecker</surname><given-names>A. S.</given-names></name><name><surname>Walker</surname><given-names>E. Y.</given-names></name><name><surname>Sinz</surname><given-names>F.</given-names></name><name><surname>Berens</surname><given-names>P.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>DataJoint: managing big scientific data using MATLAB or Python</article-title>. <source>bioRxiv</source>: 031658. <pub-id pub-id-type="doi">10.1101/031658</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zehl</surname><given-names>L.</given-names></name><name><surname>Jaillet</surname><given-names>F.</given-names></name><name><surname>Stoewer</surname><given-names>A.</given-names></name><name><surname>Grewe</surname><given-names>J.</given-names></name><name><surname>Sobolev</surname><given-names>A.</given-names></name><name><surname>Wachtler</surname><given-names>T.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Handling metadata in a neurophysiology laboratory</article-title>. <source>Front. Neuroinform.</source><volume>10</volume>:<fpage>26</fpage>. <pub-id pub-id-type="doi">10.3389/fninf.2016.00026</pub-id><?supplied-pmid 27486397?><pub-id pub-id-type="pmid">27486397</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
