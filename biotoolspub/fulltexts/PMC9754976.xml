<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_XOPS100209 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEmmc1 pdf ?>
<?FILEmmc2 pdf ?>
<?FILEsi1 svg ?>
<?FILEsi2 svg ?>
<?FILEsi3 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Ophthalmol Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Ophthalmol Sci</journal-id>
    <journal-title-group>
      <journal-title>Ophthalmology Science</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2666-9145</issn>
    <publisher>
      <publisher-name>Elsevier</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9754976</article-id>
    <article-id pub-id-type="pii">S2666-9145(22)00098-7</article-id>
    <article-id pub-id-type="doi">10.1016/j.xops.2022.100209</article-id>
    <article-id pub-id-type="publisher-id">100209</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Primary Open-Angle Glaucoma Diagnosis from Optic Disc Photographs Using a Siamese Network</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au1">
        <name>
          <surname>Lin</surname>
          <given-names>Mingquan</given-names>
        </name>
        <degrees>PhD</degrees>
        <xref rid="aff1" ref-type="aff">1</xref>
      </contrib>
      <contrib contrib-type="author" id="au2">
        <name>
          <surname>Liu</surname>
          <given-names>Lei</given-names>
        </name>
        <degrees>PhD</degrees>
        <xref rid="aff2" ref-type="aff">2</xref>
      </contrib>
      <contrib contrib-type="author" id="au3">
        <name>
          <surname>Gordon</surname>
          <given-names>Mae</given-names>
        </name>
        <degrees>PhD</degrees>
        <xref rid="aff3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author" id="au4">
        <name>
          <surname>Kass</surname>
          <given-names>Michael</given-names>
        </name>
        <degrees>MD</degrees>
        <xref rid="aff3" ref-type="aff">3</xref>
      </contrib>
      <contrib contrib-type="author" id="au5">
        <name>
          <surname>Wang</surname>
          <given-names>Fei</given-names>
        </name>
        <degrees>PhD</degrees>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="fn1" ref-type="fn">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au6">
        <name>
          <surname>Van Tassel</surname>
          <given-names>Sarah H.</given-names>
        </name>
        <degrees>MD</degrees>
        <xref rid="aff4" ref-type="aff">4</xref>
        <xref rid="fn1" ref-type="fn">∗</xref>
      </contrib>
      <contrib contrib-type="author" id="au7">
        <name>
          <surname>Peng</surname>
          <given-names>Yifan</given-names>
        </name>
        <degrees>PhD</degrees>
        <email>yip4002@med.cornell.edu</email>
        <xref rid="aff1" ref-type="aff">1</xref>
        <xref rid="cor1" ref-type="corresp">∗</xref>
        <xref rid="fn1" ref-type="fn">∗</xref>
      </contrib>
      <aff id="aff1"><label>1</label>Department of Population Health Sciences, Weill Cornell Medicine, New York, New York</aff>
      <aff id="aff2"><label>2</label>Institute for Public Health, Washington University School of Medicine, St. Louis, Missouri</aff>
      <aff id="aff3"><label>3</label>Department of Ophthalmology and Visual Sciences, Washington University School of Medicine, St. Louis, Missouri</aff>
      <aff id="aff4"><label>4</label>Department of Ophthalmology, Weill Cornell Medicine, New York, New York</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>∗</label>Correspondence: Yifan Peng, PhD, Weill Cornell Medicine, 425 E 61st DIV 305, New York, NY 10065. <email>yip4002@med.cornell.edu</email></corresp>
      <fn id="fn1">
        <label>∗</label>
        <p id="ntpara0065">Co-corresponding authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.-->
    <pub-date pub-type="collection">
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>13</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <volume>2</volume>
    <issue>4</issue>
    <elocation-id>100209</elocation-id>
    <history>
      <date date-type="received">
        <day>29</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>1</day>
        <month>8</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>5</day>
        <month>8</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 by the American Academy of Ophthalmology.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>American Academy of Ophthalmology</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="abs0010">
      <sec>
        <title>Purpose</title>
        <p>Primary open-angle glaucoma (POAG) is one of the leading causes of irreversible blindness in the United States and worldwide. Although deep learning methods have been proposed to diagnose POAG, these methods all used a single image as input. Contrastingly, glaucoma specialists typically compare the follow-up image with the baseline image to diagnose incident glaucoma. To simulate this process, we proposed a Siamese neural network, POAGNet, to detect POAG from optic disc photographs.</p>
      </sec>
      <sec>
        <title>Design</title>
        <p>The POAGNet, an algorithm for glaucoma diagnosis, is developed using optic disc photographs.</p>
      </sec>
      <sec>
        <title>Participants</title>
        <p>The POAGNet was trained and evaluated on 2 data sets: (1) 37 339 optic disc photographs from 1636 Ocular Hypertension Treatment Study (OHTS) participants and (2) 3684 optic disc photographs from the Sequential fundus Images for Glaucoma (SIG) data set. Gold standard labels were obtained using reading center grades.</p>
      </sec>
      <sec>
        <title>Methods</title>
        <p>We proposed a Siamese network model, POAGNet, to simulate the clinical process of identifying POAG from optic disc photographs. The POAGNet consists of 2 side outputs for deep supervision and uses convolution to measure the similarity between 2 networks.</p>
      </sec>
      <sec>
        <title>Main Outcome Measures</title>
        <p>The main outcome measures are the area under the receiver operating characteristic curve, accuracy, sensitivity, and specificity.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>In POAG diagnosis, extensive experiments show that POAGNet performed better than the best state-of-the-art model on the OHTS test set (area under the curve [AUC] 0.9587 versus 0.8750). It also outperformed the baseline models on the SIG test set (AUC 0.7518 versus 0.6434). To assess the transferability of POAGNet, we also validated the impact of cross-data set variability on our model. The model trained on OHTS achieved an AUC of 0.7490 on SIG, comparable to the previous model trained on the same data set. When using the combination of SIG and OHTS for training, our model achieved superior AUC to the single-data model (AUC 0.8165 versus 0.7518). These demonstrate the relative generalizability of POAGNet.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>By simulating the clinical grading process, POAGNet demonstrated high accuracy in POAG diagnosis. These results highlight the potential of deep learning to assist and enhance clinical POAG diagnosis. The POAGNet is publicly available on <ext-link ext-link-type="uri" xlink:href="https://github.com/bionlplab/poagnet" id="intref0015">https://github.com/bionlplab/poagnet</ext-link>.</p>
      </sec>
    </abstract>
    <kwd-group id="kwrds0010">
      <title>Keywords</title>
      <kwd>Deep learning</kwd>
      <kwd>Fundus photographs</kwd>
      <kwd>Primary open-angle glaucoma (POAG)</kwd>
      <kwd>Siamese network</kwd>
    </kwd-group>
    <kwd-group id="kwrds0015">
      <title>Abbreviations</title>
      <kwd>AUC, area under the curve</kwd>
      <kwd>OHTS, Ocular Hypertension Treatment Study</kwd>
      <kwd>POAG, primary open-angle glaucoma</kwd>
      <kwd>SIG, sequential fundus Images for Glaucoma</kwd>
      <kwd>VF, visual field</kwd>
    </kwd-group>
  </article-meta>
  <notes>
    <p id="misc0010">Manuscript no. XOPS-D-22-00058.</p>
  </notes>
</front>
<body>
  <p id="p0010">Primary open-angle glaucoma (POAG) is one of the leading causes of blindness worldwide.<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref> In the United States, POAG is the most common form of glaucoma and is the leading cause of blindness among African Americans<xref rid="bib2" ref-type="bibr"><sup>2</sup></xref> and Hispanics.<xref rid="bib3" ref-type="bibr"><sup>3</sup></xref> Unfortunately, POAG is asymptomatic until advanced loss of peripheral vision occurs very late in the disease. However, it is possible to screen POAG at a stage where early treatment and intervention may alter its course and preserve vision that would otherwise be lost.<xref rid="bib4" ref-type="bibr">4</xref>, <xref rid="bib5" ref-type="bibr">5</xref>, <xref rid="bib6" ref-type="bibr">6</xref></p>
  <p id="p0015">Optic disc photography has proven to be very useful for diagnosing glaucoma, showing a classic glaucomatous appearance to expert graders. While convenient and inexpensive, the low prevalence of glaucoma and screening limitations make it challenging to conduct meaningful screenings.<xref rid="bib7" ref-type="bibr"><sup>7</sup></xref> Therefore, it is important to develop an automatic model to assist clinicians in screening for and diagnosing incident POAG with high accuracy from optic disc photographs.</p>
  <p id="p0020">Developments in artificial intelligence have made automatic POAG diagnosis using optic disc photographs possible. Singh et al<xref rid="bib8" ref-type="bibr"><sup>8</sup></xref> obtained the vertical cup-to-disc ratio by segmenting optic disc and optic cup and performed the POAG classification after extracting the handcrafted feature from vertical cup-to-disc ratio. Acharya et al<xref rid="bib9" ref-type="bibr"><sup>9</sup></xref> detected POAG using support vector machines and Naïve Bayesian classifier based on the texture features and higher-order spectral features. Dua et al<xref rid="bib10" ref-type="bibr"><sup>10</sup></xref> also used support vector machines and Naïve Bayesian to classify POAG based on the wavelet-based energy features. Issac et al<xref rid="bib11" ref-type="bibr"><sup>11</sup></xref> adopted an adaptive threshold-based image processing method for POAG classification. However, the segmentation accuracy of optic disc and optic cup greatly impacts these methods. In addition, these methods only consider handcrafted features, and thus are hard to learn to generalize.</p>
  <p id="p0025">Recently, deep learning methods have demonstrated promising results in biology and medicine.<xref rid="bib12" ref-type="bibr">12</xref>, <xref rid="bib13" ref-type="bibr">13</xref>, <xref rid="bib14" ref-type="bibr">14</xref>, <xref rid="bib15" ref-type="bibr">15</xref>, <xref rid="bib16" ref-type="bibr">16</xref>, <xref rid="bib17" ref-type="bibr">17</xref>, <xref rid="bib18" ref-type="bibr">18</xref>, <xref rid="bib19" ref-type="bibr">19</xref>, <xref rid="bib20" ref-type="bibr">20</xref>, <xref rid="bib21" ref-type="bibr">21</xref>, <xref rid="bib22" ref-type="bibr">22</xref> In the ophthalmology domain, several methods have been proposed to detect POAG at its early stage.<xref rid="bib23" ref-type="bibr">23</xref>, <xref rid="bib24" ref-type="bibr">24</xref>, <xref rid="bib25" ref-type="bibr">25</xref>, <xref rid="bib26" ref-type="bibr">26</xref>, <xref rid="bib27" ref-type="bibr">27</xref>, <xref rid="bib28" ref-type="bibr">28</xref>, <xref rid="bib29" ref-type="bibr">29</xref> However, these approaches all used a single image as input. However, in clinical practice, glaucoma specialists often compare the follow-up image with the baseline image to trace the relevant features and assess for glaucomatous change (<xref rid="fig1" ref-type="fig">Fig 1</xref>). It is worth noting that the baseline image must be non-POAG. To simulate this process, we propose a Siamese neural network model, POAGNet, in this work. The Siamese neural network uses the same weights in a twin network while working on 2 different images to compute comparable output.<xref rid="bib30" ref-type="bibr"><sup>30</sup></xref> In our case, the output is the baseline image against which the other output is compared. To the best of our knowledge, it is the first time in the ophthalmology domain that 2 optic disc images have been utilized and compared for automated glaucoma detection via Siamese networks.<fig id="fig1"><label>Figure 1</label><caption><p>Longitudinal optic disc images of a patient. POAG = primary open-angle glaucoma.</p></caption><graphic xlink:href="gr1"/></fig></p>
  <p id="p0030">Unlike previous Siamese work, POAGNet used a convolution operation to study the feature difference between 2 outputs. Compared to the traditional absolute distance, our inner neural network approximates the difference between the 2 inputs more precisely by updating the parameter of convolution during the training. In addition, the POAGNet consists of side output<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> to ease the vanishing gradient problems in training and force the hidden layers to favor discriminative features.</p>
  <p id="p0035">Our study also aimed to evaluate the model’s generalization capacity for different data sets not employed in the training process. To this end, we assessed POAGNet on 2 large-scale, independent data sets, the Ocular Hypertension Treatment Study cohort (OHTS)<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> and Sequential fundus Images for Glaucoma data set (SIG),<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> with &gt; 35 000 optic disc images in total.</p>
  <p id="p0040">To the authors’ best knowledge, this work is innovative in leveraging the Siamese neural network to compare the differences between 2 optic disc photographs. Thus, our model closely matches the clinical decision-making process, which allows a glaucoma specialist to inspect the result rather than being presented with a “black-box” approach. In addition, the proposed model was validated on 2 large-scale, multi-institutional benchmarks and achieved superior results against several competitive baselines. Therefore, our model is robust and will likely be generalizable to new data. Finally, we make codes, models, and preprocessed data publicly available to catalyze future works that seek to develop deep learning models for POAG detection.</p>
  <sec id="sec2">
    <title>Methods</title>
    <sec id="sec2.1">
      <title>Data Acquisition</title>
      <p id="p0045">In this study, we include 2 independent data sets (<xref rid="tbl1" ref-type="table">Table 1</xref>). For the OHTS data set, the number of the non-POAG–non-POAG pairs used from eyes converted to POAG is 29 339. The number of the non-POAG–non-POAG pairs and non-POAG–POAG pairs used from eyes that converted to POAG are 2443 and 2285, respectively. For the SIG data set, these 3 numbers are 3015, 111, and 153, respectively. These 2 databases are large-scale, cross-sectional, and population-based studies. In this study, all eligible subjects are non-POAG at baseline.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>Characteristics of the OHTS and SIG Data Sets</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Data set</th><th colspan="3">OHTS<hr/></th><th colspan="3">SIG<hr/></th></tr><tr><th>Train</th><th>Dev</th><th>Test</th><th>Train</th><th>Dev</th><th>Test</th></tr></thead><tbody><tr><td>Eyes</td><td align="center">2440</td><td align="center">180</td><td align="center">652</td><td align="center">300</td><td align="center">35</td><td align="center">70</td></tr><tr><td>Pairs</td><td/><td/><td/><td/><td/><td/></tr><tr><td> POAG</td><td align="center">1481</td><td align="center">326</td><td align="center">478</td><td align="center">110</td><td align="center">15</td><td align="center">28</td></tr><tr><td> Normal</td><td align="center">23 803</td><td align="center">1690</td><td align="center">6289</td><td align="center">2236</td><td align="center">287</td><td align="center">561</td></tr></tbody></table><table-wrap-foot><fn><p>OHTS = Ocular Hypertension Treatment Study; POAG = primary open-angle glaucoma; SIG = Sequential fundus Images for Glaucoma.</p></fn></table-wrap-foot></table-wrap></p>
      <sec id="sec2.1.1">
        <title>OHTS</title>
        <p id="p0050">The first data set is obtained from the OHTS. The OHTS is one of the largest longitudinal clinical trials on POAG (1636 participants and 37 399 images) from 22 centers in the United States. Human subjects were included in this study. The study protocol was approved by the institutional review board at each clinical center and Weill Cornell Medicine.<xref rid="bib32" ref-type="bibr"><sup>32</sup></xref> All research adhered to the tenets of the Declaration of Helsinki. All participants provided informed consent. All risk factors were measured at baseline before the onset of the disease and collected for approximately 16 years.</p>
        <p id="p0055">The participants in this data set were selected according to eligibility and exclusion criteria.<xref rid="bib33" ref-type="bibr"><sup>33</sup></xref> Briefly, the eligibility criteria include intraocular pressure (between 24 mm Hg and 32 mm Hg in one eye and between 21 mm Hg and 32 mm Hg in the fellow eye) and age (between 40 and 80 years). The visual field (VF) tests were interpreted by the Visual Field Reading Center, and the optic discs at clinical examination and stereoscopic photographs were interpreted by the Optic Disc Reading Center. Exclusion criteria included previous intraocular surgery, visual acuity worse than 20/40 in either eye, and diseases that may cause optic disc deterioration and VF loss (such as diabetic retinopathy). The gold standard POAG labels were graded at the Optic Disc Reading Center. In brief, 2 masked certified readers were arranged to independently detect the optic disc deterioration. If there was a disagreement between 2 readers, a senior reader reviewed it in a masked fashion. The POAG diagnosis in a quality control sample of 86 eyes (50 normal eyes and 36 with progression) showed test-retest agreement at κ = 0.70 (95% confidence interval, 0.55–0.85). More details of the reading center workflow have been described by Gorden et al.<xref rid="bib32" ref-type="bibr"><sup>32</sup></xref></p>
      </sec>
      <sec id="sec2.1.2">
        <title>SIG Database</title>
        <p id="p0060">The second data set is obtained from the SIG data set (<ext-link ext-link-type="uri" xlink:href="https://github.com/XiaofeiWang2018/DeepGF" id="intref0020">https://github.com/XiaofeiWang2018/DeepGF</ext-link>). The study protocol was approved by the institutional review board at each clinical center and Beihang University. All research adhered to the tenets of the Declaration of Helsinki. All participants provided informed consent. The SIG contains 3837 optic disc images, of which 153 (3.99%) have POAG. In the SIG data set, all optic disc images are annotated with binary labels of glaucoma, that is, positive or negative glaucoma. The samples are labeled glaucomatous when they satisfy any of the 3 criteria, that is, retinal nerve fiber layer defect, rim loss, and optic disc hemorrhage.<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref></p>
      </sec>
    </sec>
    <sec id="sec2.2">
      <title>Model Development</title>
      <sec id="sec2.2.1">
        <title>Overall Architecture</title>
        <p id="p0065">The POAGNet comprises 2 convolutional blocks that share the weight and are followed by 7 layers (<xref rid="fig2" ref-type="fig">Fig 2</xref>). The prediction block in <xref rid="fig2" ref-type="fig">Figure 2</xref> shows the detail of the 7 layers. In the beginning, 2 optic disc images, <italic>x</italic><sub>1</sub> and <italic>x</italic><sub>2</sub><sub>,</sub> are passed through the convolutional neural network, DenseNet-201,<xref rid="bib34" ref-type="bibr"><sup>34</sup></xref> respectively. We used the output of last (<italic>F</italic><sub><italic>d</italic>4</sub> and <italic>F</italic><sub><italic>d</italic>4<italic>n</italic></sub>) and second last Dense Blocks (<italic>F</italic><sub><italic>d</italic>3</sub> and <italic>F</italic><sub><italic>d</italic>3<italic>n</italic></sub>). For each output, we concatenated two outputs, followed by using <inline-formula><mml:math id="M1" altimg="si1.svg"><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> convolution, a batch normalization,<xref rid="bib35" ref-type="bibr"><sup>35</sup></xref> and rectified linear units.<xref rid="bib36" ref-type="bibr"><sup>36</sup></xref> The advantage of a convolution is that, instead of vectors, we can compute the similarity between 2 feature maps at all kernels. Therefore, the output is not a single value but another feature map with spatial support. In the end, a global average pooling<xref rid="bib37" ref-type="bibr"><sup>37</sup></xref> and a fully connected layer with sigmoid activation are attached.<fig id="fig2"><label>Figure 2</label><caption><p>The architecture of the proposed POAGNet. BN = batch normalization; POAG = primary open-angle glaucoma; ReLu = rectified linear units.</p></caption><graphic xlink:href="gr2"/></fig></p>
        <p id="p0070">The POAGNet connects the side-output layer to the last 2 prediction blocks. The side-output layer generates the inherent scales of discriminative features and relieves the vanishing gradient problems in training. Finally, all side-output layers are averaged to generate a POAG prediction.</p>
      </sec>
      <sec id="sec2.2.2">
        <title>Loss Function</title>
        <p id="p0075">In this study, we use binary cross-entropy as the loss function in the POAGNet. In addition, to overcome the severe class imbalance for the POAG classification, we apply the weighted cross-entropy,<xref rid="bib38" ref-type="bibr"><sup>38</sup></xref> a commonly used loss function in classification. The adopted weighted cross-entropy was as follows:<disp-formula id="ufd1"><mml:math id="M2" altimg="si2.svg"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mo linebreak="goodbreak">−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true">[</mml:mo><mml:mi>β</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mspace width="0.25em"/><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo linebreak="badbreak">+</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>ˆ</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo stretchy="true">]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>N</italic> is the number of training examples and <italic>β</italic> is the balancing factor between positive and negative samples. Here, we used the inversely proportional to POAG frequency in the training data.</p>
        <p id="p0080">The overall loss function is the average of the losses associated with a prediction from the last 2 blocks:<disp-formula id="ufd2"><mml:math id="M3" altimg="si3.svg"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo linebreak="badbreak">=</mml:mo><mml:mi>α</mml:mi><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="goodbreak">+</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p>
      </sec>
      <sec id="sec2.2.3">
        <title>Image Augmentation</title>
        <p id="p0085">In this study, the following augmentation techniques were applied on the fly during training: (1) random rotation between 0° and 10°, (2) random translation: an image was randomly translated along the x- and y-axes by distances ranging from 0% to 10% of width or height of the image, and (3) random flipping. These augmentation operations increase the diversity of the data set.</p>
      </sec>
    </sec>
    <sec id="sec2.3">
      <title>Evaluation Metrics</title>
      <p id="p0090">Our experiments report accuracy, sensitivity (recall), and specificity. In addition, we report the area under the curve (AUC) receiver operating characteristics curve. A receiver operating characteristics curve plots true positive rate (also called sensitivity) versus false-positive rate at different classification thresholds.</p>
    </sec>
    <sec id="sec2.4">
      <title>Experimental Settings</title>
      <p id="p0095">We first used a single image as input without manual cropping in the POAG detection task to fine-tune a DenseNet-201, which has been pretrained on ImageNet. Then, we used this DenseNet-201 to initialize the subnets (DenseNet-201) in the POAGNet and fine-tuned the entire network in an end-to-end manner. Therefore, the loss is propagated back to the individual neural networks, creating better feature representations for each training iteration.</p>
      <p id="p0100">All images are resized to 224 × 224 × 3 as input of the proposed model. The models were implemented by Keras with a backend of TensorFlow. The proposed network was optimized using the Adam optimizer method.<xref rid="bib39" ref-type="bibr"><sup>39</sup></xref> The learning rate is 5 × 10<sup>−5</sup>. <italic>α</italic> is 0.8. The experiments were performed on Intel Core i9-9960 X 16 cores processor and NVIDIA Quadro RTX 6000 GPU.</p>
      <p id="p0105">For the OHTS data set, we split the entire data set randomly at the patient level. We took 1 group (20% of total subjects) as the hold-out test set and the remaining as the training set. For the SIG data set, we used the official training, development, and testing split in this study. In addition, we performed cross-data set bias and evaluation: (1) model trained on OHTS and tested on SIG; (2) model trained on SIG and tested on OHTS; (3) model trained on SIG and OHTS jointly and tested on SIG, OHTS, and their combination, respectively.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <title>Results</title>
    <p id="p0110">We compared our method with 6 models on POAG diagnosis on the OHTS data set, including the DenseNet-201<xref rid="bib34" ref-type="bibr"><sup>34</sup></xref> with a single image as input, EfficientNetB0<xref rid="bib40" ref-type="bibr"><sup>40</sup></xref> with a single image as input, MobileV2<xref rid="bib41" ref-type="bibr"><sup>41</sup></xref> with a single image as input that was used in Thakur et al,<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> ResNet-50 with a single image as input that was used in Fan et al,<xref rid="bib42" ref-type="bibr"><sup>42</sup></xref> the traditional Siamese network with absolute distance, and POAGNet using the last DenseNet Block (POAGNet w/o side output). DenseNet-201, EfficientNetB0, MobileNetV2, and ResNet-50 were pretrained on ImageNet and we modified the fully connected layers so that they could meet our binary classification requirement. We fined-tuned the entire network in an end-to-end manner.</p>
    <sec id="sec3.1">
      <title>POAG Diagnosis on the OHTS Data Set</title>
      <p id="p0115">We first trained and validated the models on the OHTS data set. <xref rid="tbl2" ref-type="table">Table 2</xref> shows the performance comparison. Our model achieved the best results, with an accuracy of 0.9283, a sensitivity of 0.7469, a specificity of 0.9421, and an AUC of 0.9587. The performance of POAGNet was then compared with that of the models with a single image as input. The performance of POAGNet was superior to that of the best single model, DenseNet-201, with 9.63% higher accuracy, 1.78% higher sensitivity, 1.28% higher specificity, and 8.37% higher AUC.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>The Results of Models Trained and Validated on the OHTS Data Set</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Accuracy</th><th>Sensitivity</th><th>Specificity</th><th>AUC</th></tr></thead><tbody><tr><td>DenseNet-201</td><td align="char">0.8320</td><td align="char">0.7291</td><td align="char">0.9393</td><td align="char">0.8750</td></tr><tr><td>EfficientNetB0</td><td align="char">0.8899</td><td align="char">0.5071</td><td align="char">0.9199</td><td align="char">0.8379</td></tr><tr><td>MobileV2</td><td align="char">0.7426</td><td align="char">0.7531</td><td align="char">0.7418</td><td align="char">0.8124</td></tr><tr><td>ResNet-50</td><td align="char">0.8653</td><td align="char">0.6823</td><td align="char">0.8795</td><td align="char">0.8650</td></tr><tr><td>POAGNet (absolute distance)</td><td align="char">0.9070</td><td align="char">0.6841</td><td align="char">0.9240</td><td align="char">0.9075</td></tr><tr><td>POAGNet (w/o side output)</td><td align="char">0.9059</td><td align="char">0.7803</td><td align="char">0.9154</td><td align="char">0.9236</td></tr><tr><td>POAGNet</td><td align="char">0.9283</td><td align="char">0.7469</td><td align="char">0.9421</td><td align="char">0.9587</td></tr></tbody></table><table-wrap-foot><fn><p>AUC = area under the curve; OHTS = Ocular Hypertension Treatment Study; POAG = primary open-angle glaucoma.</p></fn></table-wrap-foot></table-wrap></p>
      <p id="p0120">In addition, the performance of POAG was compared with 2 variations, 1 without side output and 1 with absolute distance. The POAGNet obtained an AUC of 0.9587, resulting in an improvement of 5.1% over the POAGNet with the absolute distance (row 4) and 3.5% over the POAGNet w/o side output (row 5). To check how many side outputs are optimal, we also performed the experiments where the POAGNet used the side output of the last 3 Basic Blocks as well as all Basic Blocks. It turns out that the proposed structure, which used the side output of the last 2 Basic Blocks, achieved the best results (<xref rid="appsec1" ref-type="sec">Table S1</xref>). We also tried the contrast loss instead of binary cross-entropy loss, and the results decreased (<xref rid="appsec1" ref-type="sec">Table S2</xref>).</p>
    </sec>
    <sec id="sec3.2">
      <title>POAG Diagnosis on the SIG Data Set</title>
      <p id="p0125">We then trained and validated POAGNet on the SIG data set. <xref rid="tbl3" ref-type="table">Table 3</xref> compares the performance of POAGNet on the SIG data set. The POAGNet obtained the best results, with an accuracy of 0.9176, a sensitivity of 0.1786, a specificity of 0.9519, and an AUC of 0.7518. Same as on the OHTS data set, the performance of POAGNet was superior to DenseNet-201, the best model with a single image input (10.8% in AUC), POAGNet with absolute distance (7.4% in AUC), and POAGNet w/o side output (5.5% in AUC).<table-wrap position="float" id="tbl3"><label>Table 3</label><caption><p>The Results of Models Trained and Validated on the SIG Data Set</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Method</th><th>Accuracy</th><th>Sensitivity</th><th>Specificity</th><th>AUC</th></tr></thead><tbody><tr><td>DenseNet-201</td><td align="char">0.8590</td><td align="char">0.1786</td><td align="char">0.8905</td><td align="char">0.6434</td></tr><tr><td>EfficientNetB0</td><td align="char">0.6846</td><td align="char">0.2143</td><td align="char">0.7065</td><td align="char">0.5288</td></tr><tr><td>MobileV2</td><td align="char">0.1284</td><td align="char">0.8571</td><td align="char">0.0945</td><td align="char">0.6040</td></tr><tr><td>ResNet-50</td><td align="char">0.6958</td><td align="char">0.3571</td><td align="char">0.7114</td><td align="char">0.5704</td></tr><tr><td>POAGNet (absolute distance)</td><td align="char">0.8130</td><td align="char">0.3214</td><td align="char">0.8358</td><td align="char">0.6774</td></tr><tr><td>POAGNet (w/o side output)</td><td align="char">0.8336</td><td align="char">0.2857</td><td align="char">0.8590</td><td align="char">0.6972</td></tr><tr><td>POAGNet</td><td align="char">0.9176</td><td align="char">0.1786</td><td align="char">0.9519</td><td align="char">0.7518</td></tr></tbody></table><table-wrap-foot><fn><p>AUC = area under the curve; POAG = primary open-angle glaucoma; SIG = Sequential fundus Images for Glaucoma.</p></fn></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="sec3.3">
      <title>Cross-Data Set Bias and Evaluation</title>
      <p id="p0130">In separate experiments, to assess the generalizability and transferability of POAGNet, we compared the performance of models trained on OHTS, SIG, and their combination (OHTS + SIG) (<xref rid="fig3" ref-type="fig">Fig 3</xref>).<fig id="fig3"><label>Figure 3</label><caption><p>The results of the POAGNet trained and validated on the Ocular Hypertension Treatment Study (OHTS), the Sequential fundus Image for Glaucoma (SIG) data set, and their combination (OHTS + SIG). AUC = area under the curve; POAG = primary open-angle glaucoma.</p></caption><graphic xlink:href="gr3"/></fig></p>
      <p id="p0135">For accuracy and specificity (<xref rid="fig3" ref-type="fig">Fig 3</xref>A, C), our models achieved comparable results in different training and testing scenarios, indicating that our model can generalize and transfer across different data sets.</p>
      <p id="p0140">For sensitivity and AUC (<xref rid="fig3" ref-type="fig">Fig 3</xref>B, D), POAGNet achieved comparable or better performance when trained on OHTS or OHTS + SIG but weak performance when trained on SIG only. When POAGNet is trained on SIG only, the training size is relatively small, which makes it difficult to fully train the model. We will explain observations in more detail in the “<xref rid="sec4" ref-type="sec">Discussion</xref>” section.</p>
      <p id="p0145">We also sampled the OHTS testing data set, making the number of instances in the OHTS testing data set the same as those in the SIG testing data set. The AUC is 0.8797 (<xref rid="appsec1" ref-type="sec">Table S3</xref>).</p>
    </sec>
  </sec>
  <sec id="sec4">
    <title>Discussion</title>
    <p id="p0150">This study proposed a new end-to-end deep learning network that simulates the clinical process of automatic POAG detection from optic disc photographs. Two data sets were used to evaluate the proposed model. The results demonstrated that the proposed network was superior to the state-of-the-art model. The proposed model has potential for eye services in the future after extensive validation across multiple and diverse image data sets. Unlike the previous networks that use a single image as input, our proposed model simulates the clinical process by comparing the differences between 2 input images (baseline and follow-up images).</p>
    <p id="p0155">The POAGNet achieved superior performance to the state-of-the-art methods for 3 reasons. First, the proposed network used the Siamese neural network and 2 images (baseline and follow-up) as input. This architecture simulates the clinical process of identifying POAG from optic disc photographs by comparing the follow-up image with the baseline image. <xref rid="tbl2" ref-type="table">Tables 2</xref> and <xref rid="tbl3" ref-type="table">3</xref> show that the Siamese neural networks were superior to DenseNet-201, suggesting that the Siamese network can leverage the differences between 2 images for a more accurate POAG diagnosis. Second, POAGNet used a convolution operation to measure the difference between 2 outputs. The advantage of the convolution operation is that it computes the similarity on all kernels, and the parameters in the convolution can be updated during the training. The output of this operation is not a single score but rather a feature map with spatial support. As shown in <xref rid="tbl2" ref-type="table">Tables 2</xref> and <xref rid="tbl3" ref-type="table">3</xref>, POAGNet w/o side output achieved a better result than the POAGNet with absolute distance, indicating that utilizing convolution to measure the similarity between 2 outputs could boost the performance. Third, the POAGNet used side output<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> to relieve the vanishing gradient problems in training and encourage the hidden layers to favor discriminative features. By comparing rows 5 and 6 in <xref rid="tbl2" ref-type="table">Tables 2</xref> and <xref rid="tbl3" ref-type="table">3</xref>, POAGNet can leverage multiscale information to boost the model’s performance.</p>
    <p id="p0160">We also analyzed the causes of misclassification. We found that the major reason for misclassification is due to the optic disc unobvious discrepancy in the early stage. <xref rid="appsec1" ref-type="sec">Fig S1</xref> shows the positive predictive value, sensitivity, specificity, accuracy, and AUC in each time interval. Specificity, accuracy, and AUC are almost identical during these time intervals. Additionally, there are only 1 and 4 POAG cases for the first 2 time intervals, which makes the sensitivity relatively high. The sensitivity is also relatively stable for the rest time intervals. On the other hand, the positive predictive value has a continuing upward trend, indicating that nonPOAG-POAG pairs are easier to detect at the late stage than at the early stage. Second, we counted the number of misclassified patients in each relative year after the POAG onset year. <xref rid="tbl4" ref-type="table">Table 4</xref> shows that more patients are misclassified when their POAG onset year is closer to the baseline visit. We also studied if the proposed model can focus on the important region to detect POAG. Because class activation maps are usually applied for the model with a single input, we used another method to reflect the regions that were more important for POAG diagnosis. Specifically, we used a 5 × 5 window size with a step size of 1 to mask the image and obtain the prediction probability for each window. We obtained the probability of each pixel by averaging all the probabilities of the windows it belongs to. Therefore, we can get the saliency map by drawing the probability of each pixel, which can reflect the important region for POAG detection. Four examples in <xref rid="fig4" ref-type="fig">Figure 4</xref> demonstrate that the proposed model focused on the region between the optic disc and cup to detect POAG.<table-wrap position="float" id="tbl4"><label>Table 4</label><caption><p>The Misclassified in Each Relative Year After the Year When the Participants Truly Converted to POAG</p></caption><table frame="hsides" rules="groups"><thead><tr><th>POAG Onset Year</th><th>Number</th><th>%</th></tr></thead><tbody><tr><td>0</td><td align="center">37</td><td align="char">30.58</td></tr><tr><td>1</td><td align="center">15</td><td align="char">12.40</td></tr><tr><td>2</td><td align="center">19</td><td align="char">15.70</td></tr><tr><td>3</td><td align="center">13</td><td align="char">10.74</td></tr><tr><td>4</td><td align="center">12</td><td align="char">9.92</td></tr><tr><td>5</td><td align="center">6</td><td align="char">4.96</td></tr><tr><td>6</td><td align="center">6</td><td align="char">4.96</td></tr><tr><td>7</td><td align="center">8</td><td align="char">6.61</td></tr><tr><td>8</td><td align="center">4</td><td align="char">3.31</td></tr><tr><td>9</td><td align="center">1</td><td align="char">0.83</td></tr><tr><td>10</td><td align="center">0</td><td align="char">0.00</td></tr><tr><td>11</td><td align="center">0</td><td align="char">0.00</td></tr></tbody></table><table-wrap-foot><fn><p>POAG = primary open-angle glaucoma.</p></fn></table-wrap-foot></table-wrap><fig id="fig4"><label>Figure 4</label><caption><p>The 4 examples of saliency maps are derived from POAGNet. The left side of each subfigure is the original image and the right side of each subfigure is the saliency map that overlaps the original image. POAG = primary open-angle glaucoma.</p></caption><graphic xlink:href="gr4"/></fig></p>
    <p id="p0165">We conducted further experiments to evaluate the POAGNet performance on external validation data sets. Comparing the models trained on OHTS, SIG, and their combination, AUCs are similar on the SIG test set. This observation demonstrates the relative generalizability of POAGNet. On the other hand, we observed that the AUC on OHTS drops from 0.9587 (trained on OHTS) to 0.7410 (trained on SIG). One potential reason is that the ophthalmologists carefully selected the images in SIG, and all glaucomatous images are due to the structural glaucomatous optic nerve abnormalities.<xref rid="bib31" ref-type="bibr"><sup>31</sup></xref> In contrast, some optic disc photographs in OHTS were graded as POAG due to the change of VF without glaucomatous disc changes over time. Therefore, detecting POAG due to VF defects remains challenging.</p>
    <p id="p0170">Finally, we compared our work with 2 previous works based on the OHTS data set. We found 2 experimental differences when comparing our work with the prior work of Thakur et al.<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> First, Thakur et al<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> discarded 24% of the optic disc photographs with poor image quality, while we used the whole OHTS data set. Second, Thakur et al<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> manually cropped the images in the data preprocessing stage, but we did not. It is possible that the use of these techniques might have improved the accuracy of the model. However, we deliberately avoided this extensive preprocessing to minimize the labor required and make our model as generalizable as possible. To make a fair comparison, we applied the same method in Thakur et al<xref rid="bib26" ref-type="bibr"><sup>26</sup></xref> to our data set (MobileV2 in <xref rid="tbl2" ref-type="table">Table 2</xref>). The AUC was 0.8124, lower than POAGNet under the same setting. In addition, our proposed model achieved an AUC of 0.9578, higher than the AUC obtained by Thakur et al without cropping and discarding parts of the poor-quality image. Both results suggest that our approach may be more suitable to deploy to health care centers with a quick turnaround time.</p>
    <p id="p0175">We have performed the same method as provided in the study by Fan et al<xref rid="bib42" ref-type="bibr"><sup>42</sup></xref> (ResNet-50) for our data for 3 models. The 3 models are Optic disc changes attributable to POAG by Endpoint Committee (Model 1), VF changes attributable to POAG by Endpoint Committee (Model 2), and Optic disc or VF changes attributable to POAG by Endpoint Committee (Model 3). The AUCs are 0.8483, 0.8115, and 0.8650 for these 3 models, respectively, and the result of Model 3 is similar to the result obtained by Fan et al<xref rid="bib42" ref-type="bibr"><sup>42</sup></xref> (0.88). Because we did not use the same training and testing data split and preprocessing methods, the results are not strictly comparable. We also used these 3 models to test our proposed model, and the AUCs are 0.9378, 0.9286, and 0.9587, respectively.</p>
    <sec id="sec4.1">
      <title>Limitations and Future Work</title>
      <p id="p0180">One limitation of our proposed model comes from the data imbalance. Only 6.2% and 3.99% of all images had POAG on OHTS and SIG, respectively. The low portion of POAG images may result in relatively lower sensitivity. We plan to incorporate data sets from different countries and populations to improve the model in the future.</p>
      <p id="p0185">Another potential limitation is that, as previously discussed, it remains challenging to only detect POAG due to VF defects. This is mainly because the VF defects lack the obvious structural sign of glaucomatous optic neuropathy. It would be interesting to study the difference between the results derived from VF defects and glaucomatous optic nerve separately and incorporate VF into the deep learning models for a multimodal study.</p>
      <p id="p0190">In this work, we used baseline images and “follow-up images” as input to obtain substantial differences between images for POAG detection. The longitudinal images between the baseline image and the “follow-up image” may also play an important role in tracing the temporal patterns of POAG progression. Thus, we recommend using sequential models such as long short-term memory or transformer for POAG detection in the future.</p>
      <p id="p0195">In conclusion, this study proposed a new end-to-end deep learning network that simulates the process for automatic POAG detection from optic disc photographs. Two data sets were used to evaluate the proposed model. The results demonstrated that the proposed network performs well on POAG diagnosis. The cross-data set validation further proves the generalizability and robustness of our method. To maximize this study’s transparency and reproducibility and provide a benchmark for further refinement and development of the algorithm, we make the deep learning model, training codes, and data partition publicly available (<ext-link ext-link-type="uri" xlink:href="https://github.com/bionlplab/poagnet" id="intref0025">https://github.com/bionlplab/poagnet</ext-link>).</p>
    </sec>
  </sec>
</body>
<back>
  <ref-list id="cebib0010">
    <title>Reference</title>
    <ref id="bib1">
      <label>1</label>
      <element-citation publication-type="journal" id="sref1">
        <person-group person-group-type="author">
          <name>
            <surname>Bourne</surname>
            <given-names>R.R.</given-names>
          </name>
          <name>
            <surname>Stevens</surname>
            <given-names>G.A.</given-names>
          </name>
          <name>
            <surname>White</surname>
            <given-names>R.A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Causes of vision loss worldwide, 1990–2010: a systematic analysis</article-title>
        <source>Lancet Glob Health</source>
        <volume>1</volume>
        <issue>6</issue>
        <year>2013</year>
        <fpage>e339</fpage>
        <lpage>e349</lpage>
        <pub-id pub-id-type="pmid">25104599</pub-id>
      </element-citation>
    </ref>
    <ref id="bib2">
      <label>2</label>
      <element-citation publication-type="journal" id="sref2">
        <person-group person-group-type="author">
          <name>
            <surname>Sommer</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Tielsch</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Katz</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Racial differences in the cause-specific prevalence of blindness in east Baltimore</article-title>
        <source>N Engl J Med</source>
        <volume>325</volume>
        <issue>20</issue>
        <year>1991</year>
        <fpage>1412</fpage>
        <lpage>1417</lpage>
        <pub-id pub-id-type="pmid">1922252</pub-id>
      </element-citation>
    </ref>
    <ref id="bib3">
      <label>3</label>
      <element-citation publication-type="journal" id="sref3">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Torres</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Varma</surname>
            <given-names>R.</given-names>
          </name>
          <collab>Los Angeles Latino Eye Study Group</collab>
        </person-group>
        <article-title>Variation in intraocular pressure and the risk of developing open-angle glaucoma: the Los Angeles Latino Eye Study</article-title>
        <source>Am J Ophthalmol</source>
        <volume>188</volume>
        <year>2018</year>
        <fpage>51</fpage>
        <lpage>59</lpage>
        <pub-id pub-id-type="pmid">29360458</pub-id>
      </element-citation>
    </ref>
    <ref id="bib4">
      <label>4</label>
      <element-citation publication-type="journal" id="sref4">
        <person-group person-group-type="author">
          <name>
            <surname>Doshi</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Ying-Lai</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Azen</surname>
            <given-names>S.P.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Sociodemographic, family history, and lifestyle risk factors for open-angle glaucoma and ocular hypertension: the Los Angeles Latino Eye Study</article-title>
        <source>Ophthalmology</source>
        <volume>115</volume>
        <issue>4</issue>
        <year>2008</year>
        <fpage>639</fpage>
        <lpage>647.e2</lpage>
        <pub-id pub-id-type="pmid">17900693</pub-id>
      </element-citation>
    </ref>
    <ref id="bib5">
      <label>5</label>
      <element-citation publication-type="journal" id="sref5">
        <person-group person-group-type="author">
          <name>
            <surname>Quigley</surname>
            <given-names>H.A.</given-names>
          </name>
          <name>
            <surname>Katz</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Derick</surname>
            <given-names>R.J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>An evaluation of optic disc and nerve fiber layer examinations in monitoring progression of early glaucoma damage</article-title>
        <source>Ophthalmology</source>
        <volume>99</volume>
        <issue>1</issue>
        <year>1992</year>
        <fpage>19</fpage>
        <lpage>28</lpage>
        <pub-id pub-id-type="pmid">1741133</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <label>6</label>
      <element-citation publication-type="journal" id="sref6">
        <person-group person-group-type="author">
          <name>
            <surname>Fleming</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Whitlock</surname>
            <given-names>E.P.</given-names>
          </name>
          <name>
            <surname>Beil</surname>
            <given-names>T.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Screening for primary open-angle glaucoma in the primary care setting: an update for the US Preventive Services Task Force</article-title>
        <source>Ann Fam Med</source>
        <volume>3</volume>
        <issue>2</issue>
        <year>2005</year>
        <fpage>167</fpage>
        <lpage>170</lpage>
        <pub-id pub-id-type="pmid">15798044</pub-id>
      </element-citation>
    </ref>
    <ref id="bib7">
      <label>7</label>
      <element-citation publication-type="journal" id="sref7">
        <person-group person-group-type="author">
          <name>
            <surname>Kolomeyer</surname>
            <given-names>N.N.</given-names>
          </name>
          <name>
            <surname>Katz</surname>
            <given-names>L.J.</given-names>
          </name>
          <name>
            <surname>Hark</surname>
            <given-names>L.A.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Lessons learned from two large community-based glaucoma screening studies</article-title>
        <source>J Glaucoma</source>
        <volume>30</volume>
        <year>2021</year>
        <fpage>875</fpage>
        <lpage>877</lpage>
        <pub-id pub-id-type="pmid">34334703</pub-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <label>8</label>
      <element-citation publication-type="journal" id="sref8">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Dutta</surname>
            <given-names>M.K.</given-names>
          </name>
          <name>
            <surname>ParthaSarathi</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Image processing based automatic diagnosis of glaucoma using wavelet features of segmented optic disc from fundus image</article-title>
        <source>Computer Methods Programs Biomed</source>
        <volume>124</volume>
        <year>2016</year>
        <fpage>108</fpage>
        <lpage>120</lpage>
      </element-citation>
    </ref>
    <ref id="bib9">
      <label>9</label>
      <element-citation publication-type="journal" id="sref9">
        <person-group person-group-type="author">
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
          <name>
            <surname>Dua</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Du</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chua</surname>
            <given-names>C.K.</given-names>
          </name>
        </person-group>
        <article-title>Automated diagnosis of glaucoma using texture and higher order spectra features</article-title>
        <source>IEEE Trans Inf Technol Biomed</source>
        <volume>15</volume>
        <issue>3</issue>
        <year>2011</year>
        <fpage>449</fpage>
        <lpage>455</lpage>
        <pub-id pub-id-type="pmid">21349793</pub-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <label>10</label>
      <element-citation publication-type="journal" id="sref10">
        <person-group person-group-type="author">
          <name>
            <surname>Dua</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>U.R.</given-names>
          </name>
          <name>
            <surname>Chowriappa</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Sree</surname>
            <given-names>S.V.</given-names>
          </name>
        </person-group>
        <article-title>Wavelet-based energy features for glaucomatous image classification</article-title>
        <source>IEEE Trans Inf Technol Biomed</source>
        <volume>16</volume>
        <issue>1</issue>
        <year>2011</year>
        <fpage>80</fpage>
        <lpage>87</lpage>
        <pub-id pub-id-type="pmid">22113813</pub-id>
      </element-citation>
    </ref>
    <ref id="bib11">
      <label>11</label>
      <element-citation publication-type="journal" id="sref11">
        <person-group person-group-type="author">
          <name>
            <surname>Issac</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Sarathi</surname>
            <given-names>M.P.</given-names>
          </name>
          <name>
            <surname>Dutta</surname>
            <given-names>M.K.</given-names>
          </name>
        </person-group>
        <article-title>An adaptive threshold based image processing technique for improved glaucoma detection and classification</article-title>
        <source>Computer Methods Programs Biomed</source>
        <volume>122</volume>
        <issue>2</issue>
        <year>2015</year>
        <fpage>229</fpage>
        <lpage>244</lpage>
      </element-citation>
    </ref>
    <ref id="bib12">
      <label>12</label>
      <element-citation publication-type="journal" id="sref12">
        <person-group person-group-type="author">
          <name>
            <surname>Ching</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Himmelstein</surname>
            <given-names>D.S.</given-names>
          </name>
          <name>
            <surname>Beaulieu-Jones</surname>
            <given-names>B.K.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Opportunities and obstacles for deep learning in biology and medicine</article-title>
        <source>J R Soc Interf</source>
        <volume>15</volume>
        <issue>141</issue>
        <year>2018</year>
        <object-id pub-id-type="publisher-id">20170387</object-id>
      </element-citation>
    </ref>
    <ref id="bib13">
      <label>13</label>
      <element-citation publication-type="journal" id="sref13">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Momin</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lei</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fully automated segmentation of brain tumor from multiparametric MRI using 3D context deep supervised U-net</article-title>
        <source>Med Phys</source>
        <volume>48</volume>
        <year>2021</year>
        <fpage>4365</fpage>
        <lpage>4374</lpage>
        <pub-id pub-id-type="pmid">34101845</pub-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <label>14</label>
      <element-citation publication-type="journal" id="sref14">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cascaded triplanar autoencoder M-Net for fully automatic segmentation of left ventricle myocardial scar from three-dimensional late gadolinium-enhanced MR images</article-title>
        <source>IEEE J Biomed Health Inform</source>
        <volume>26</volume>
        <year>2022</year>
        <fpage>2582</fpage>
        <lpage>2593</lpage>
        <pub-id pub-id-type="pmid">35077377</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <label>15</label>
      <element-citation publication-type="journal" id="sref15">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep-recursive residual network for image semantic segmentation</article-title>
        <source>Neural Comput Appl</source>
        <volume>32</volume>
        <issue>16</issue>
        <year>2020</year>
        <fpage>12935</fpage>
        <lpage>12947</lpage>
      </element-citation>
    </ref>
    <ref id="bib16">
      <label>16</label>
      <element-citation publication-type="journal" id="sref16">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>D.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated detection of clinically significant prostate cancer in mp-MRI images based on an end-to-end deep neural network</article-title>
        <source>IEEE Trans Med Imaging</source>
        <volume>37</volume>
        <issue>5</issue>
        <year>2018</year>
        <fpage>1127</fpage>
        <lpage>1139</lpage>
        <pub-id pub-id-type="pmid">29727276</pub-id>
      </element-citation>
    </ref>
    <ref id="bib17">
      <label>17</label>
      <element-citation publication-type="journal" id="sref17">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Keenan</surname>
            <given-names>T.D.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting risk of late age-related macular degeneration using deep learning</article-title>
        <source>NPJ Digit Med</source>
        <volume>3</volume>
        <issue>1</issue>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="pmid">31934645</pub-id>
      </element-citation>
    </ref>
    <ref id="bib18">
      <label>18</label>
      <element-citation publication-type="journal" id="sref18">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Dharssi</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Q.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepSeeNet: a deep learning model for automated classification of patient-based age-related macular degeneration severity from color fundus photographs</article-title>
        <source>Ophthalmology</source>
        <volume>126</volume>
        <issue>4</issue>
        <year>2019</year>
        <fpage>565</fpage>
        <lpage>575</lpage>
        <pub-id pub-id-type="pmid">30471319</pub-id>
      </element-citation>
    </ref>
    <ref id="bib19">
      <label>19</label>
      <element-citation publication-type="journal" id="sref19">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Wynne</surname>
            <given-names>J.F.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>B.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Artificial intelligence in tumor subregion analysis based on medical imaging: a review</article-title>
        <source>J Appl Clin Med Phys</source>
        <volume>22</volume>
        <issue>7</issue>
        <year>2021</year>
        <fpage>10</fpage>
        <lpage>26</lpage>
        <pub-id pub-id-type="pmid">34164913</pub-id>
      </element-citation>
    </ref>
    <ref id="bib20">
      <label>20</label>
      <element-citation publication-type="other" id="sref20">
        <person-group person-group-type="author">
          <name>
            <surname>Tan</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Vessel wall segmentation of common carotid artery via multi-branch light network. In: <italic>Proc. SPIE 11313, Medical Imaging 2020: Image Processing</italic>, 11313</article-title>
        <comment>Accessed March 10, 2020</comment>
        <pub-id pub-id-type="doi">10.1117/12.2549599</pub-id>
      </element-citation>
    </ref>
    <ref id="bib21">
      <label>21</label>
      <element-citation publication-type="journal" id="sref21">
        <person-group person-group-type="author">
          <name>
            <surname>Hou</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Ladizhinsky</surname>
            <given-names>G.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Clinical evidence engine: proof-of-concept for a clinical-domain-agnostic decision support infrastructure</article-title>
        <source>arXiv</source>
        <year>2021</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.2111.00621</pub-id>
      </element-citation>
    </ref>
    <ref id="bib22">
      <label>22</label>
      <element-citation publication-type="journal" id="sref22">
        <person-group person-group-type="author">
          <name>
            <surname>Hou</surname>
            <given-names>B.-J.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Z.-H.</given-names>
          </name>
        </person-group>
        <article-title>Learning with interpretable structure from gated RNN</article-title>
        <source>IEEE Trans Neural Netw Learn Syst</source>
        <volume>31</volume>
        <issue>7</issue>
        <year>2020</year>
        <fpage>2267</fpage>
        <lpage>2279</lpage>
        <pub-id pub-id-type="pmid">32071002</pub-id>
      </element-citation>
    </ref>
    <ref id="bib23">
      <label>23</label>
      <element-citation publication-type="book" id="sref23">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>D.W.K.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Glaucoma detection based on deep convolutional neural network</part-title>
        <source>2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source>
        <year>2015</year>
        <publisher-name>IEEE</publisher-name>
        <publisher-loc>New York</publisher-loc>
        <fpage>715</fpage>
        <lpage>718</lpage>
      </element-citation>
    </ref>
    <ref id="bib24">
      <label>24</label>
      <element-citation publication-type="journal" id="sref24">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>H.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A large-scale database and a CNN model for attention-based glaucoma detection</article-title>
        <source>IEEE Trans Med Imaging</source>
        <volume>39</volume>
        <issue>2</issue>
        <year>2019</year>
        <fpage>413</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="pmid">31283476</pub-id>
      </element-citation>
    </ref>
    <ref id="bib25">
      <label>25</label>
      <element-citation publication-type="journal" id="sref25">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Keel</surname>
            <given-names>S.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Efficacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs</article-title>
        <source>Ophthalmology</source>
        <volume>125</volume>
        <issue>8</issue>
        <year>2018</year>
        <fpage>1199</fpage>
        <lpage>1206</lpage>
        <pub-id pub-id-type="pmid">29506863</pub-id>
      </element-citation>
    </ref>
    <ref id="bib26">
      <label>26</label>
      <element-citation publication-type="journal" id="sref26">
        <person-group person-group-type="author">
          <name>
            <surname>Thakur</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Goldbaum</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Yousefi</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Predicting glaucoma before onset using deep learning</article-title>
        <source>Ophthalmol Glaucoma</source>
        <volume>3</volume>
        <issue>4</issue>
        <year>2020</year>
        <fpage>262</fpage>
        <lpage>268</lpage>
        <pub-id pub-id-type="pmid">33012331</pub-id>
      </element-citation>
    </ref>
    <ref id="bib27">
      <label>27</label>
      <element-citation publication-type="journal" id="sref27">
        <person-group person-group-type="author">
          <name>
            <surname>Christopher</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Belghith</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bowd</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Performance of deep learning architectures and transfer learning for detecting glaucomatous optic neuropathy in fundus photographs</article-title>
        <source>Sci Rep</source>
        <volume>8</volume>
        <issue>1</issue>
        <year>2018</year>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="pmid">29311619</pub-id>
      </element-citation>
    </ref>
    <ref id="bib28">
      <label>28</label>
      <element-citation publication-type="journal" id="sref28">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Disc-aware ensemble network for glaucoma screening from fundus image</article-title>
        <source>IEEE Trans Med Imaging</source>
        <volume>37</volume>
        <issue>11</issue>
        <year>2018</year>
        <fpage>2493</fpage>
        <lpage>2501</lpage>
        <pub-id pub-id-type="pmid">29994764</pub-id>
      </element-citation>
    </ref>
    <ref id="bib29">
      <label>29</label>
      <element-citation publication-type="book" id="sref29">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>D.W.K.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <part-title>Integrating holistic and local deep features for glaucoma classification</part-title>
        <source>2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source>
        <year>2016</year>
        <publisher-name>IEEE</publisher-name>
        <publisher-loc>New York</publisher-loc>
        <fpage>1328</fpage>
        <lpage>1331</lpage>
      </element-citation>
    </ref>
    <ref id="bib30">
      <label>30</label>
      <element-citation publication-type="book" id="sref30">
        <person-group person-group-type="author">
          <name>
            <surname>Chicco</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <part-title>Siamese neural networks: an overview</part-title>
        <source>Artificial Neural Networks</source>
        <year>2021</year>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
        <fpage>73</fpage>
        <lpage>94</lpage>
      </element-citation>
    </ref>
    <ref id="bib31">
      <label>31</label>
      <element-citation publication-type="book" id="sref31">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>DeepGF: glaucoma forecast using the sequential fundus images</part-title>
        <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>
        <year>2020</year>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
        <fpage>626</fpage>
        <lpage>635</lpage>
      </element-citation>
    </ref>
    <ref id="bib32">
      <label>32</label>
      <element-citation publication-type="journal" id="sref32">
        <person-group person-group-type="author">
          <name>
            <surname>Gordon</surname>
            <given-names>M.O.</given-names>
          </name>
          <name>
            <surname>Kass</surname>
            <given-names>M.A.</given-names>
          </name>
        </person-group>
        <article-title>The Ocular Hypertension Treatment Study: design and baseline description of the participants</article-title>
        <source>Arch Ophthalmol</source>
        <volume>117</volume>
        <issue>5</issue>
        <year>1999</year>
        <fpage>573</fpage>
        <lpage>583</lpage>
        <pub-id pub-id-type="pmid">10326953</pub-id>
      </element-citation>
    </ref>
    <ref id="bib33">
      <label>33</label>
      <element-citation publication-type="journal" id="sref33">
        <person-group person-group-type="author">
          <name>
            <surname>Kass</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Heuer</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Higginbotham</surname>
            <given-names>E.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The Ocular Hypertension Treatment Study: a randomized trial determines that topical ocular hypotensive medication delays or prevents the onset of primary open-angle glaucoma</article-title>
        <source>Arch Ophthalmol</source>
        <volume>120</volume>
        <issue>6</issue>
        <year>2002</year>
        <fpage>701</fpage>
        <lpage>713</lpage>
        <pub-id pub-id-type="pmid">12049574</pub-id>
      </element-citation>
    </ref>
    <ref id="bib34">
      <label>34</label>
      <element-citation publication-type="book" id="sref34">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Van Der Maaten</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Weinberger</surname>
            <given-names>K.Q.</given-names>
          </name>
        </person-group>
        <part-title>Densely connected convolutional networks</part-title>
        <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2017</year>
        <publisher-name>IEEE</publisher-name>
        <publisher-loc>New York</publisher-loc>
        <fpage>4700</fpage>
        <lpage>4708</lpage>
      </element-citation>
    </ref>
    <ref id="bib35">
      <label>35</label>
      <element-citation publication-type="book" id="sref35">
        <person-group person-group-type="author">
          <name>
            <surname>Ioffe</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Szegedy</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <part-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</part-title>
        <source>International Conference on Machine Learning</source>
        <year>2015</year>
        <publisher-name>PMLR</publisher-name>
        <publisher-loc>New York</publisher-loc>
        <fpage>448</fpage>
        <lpage>456</lpage>
      </element-citation>
    </ref>
    <ref id="bib36">
      <label>36</label>
      <element-citation publication-type="book" id="sref36">
        <person-group person-group-type="author">
          <name>
            <surname>Glorot</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Bordes</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <part-title>Deep sparse rectifier neural networks</part-title>
        <source>Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics: JMLR Workshop and Conference Proceedings</source>
        <edition>323rd</edition>
        <volume>315</volume>
        <year>2011</year>
        <publisher-name>JMLR</publisher-name>
        <publisher-loc>New York</publisher-loc>
      </element-citation>
    </ref>
    <ref id="bib37">
      <label>37</label>
      <element-citation publication-type="journal" id="sref37">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Bottou</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Haffner</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Gradient-based learning applied to document recognition</article-title>
        <source>Proc IEEE</source>
        <volume>86</volume>
        <issue>11</issue>
        <year>1998</year>
        <fpage>2278</fpage>
        <lpage>2324</lpage>
      </element-citation>
    </ref>
    <ref id="bib38">
      <label>38</label>
      <element-citation publication-type="journal" id="sref38">
        <person-group person-group-type="author">
          <name>
            <surname>Ho</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Wookey</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>The real-world-weight cross-entropy loss function: modeling the costs of mislabeling</article-title>
        <source>IEEE Access</source>
        <volume>8</volume>
        <year>2019</year>
        <fpage>4806</fpage>
        <lpage>4813</lpage>
      </element-citation>
    </ref>
    <ref id="bib39">
      <label>39</label>
      <element-citation publication-type="journal" id="sref39">
        <person-group person-group-type="author">
          <name>
            <surname>Kingma</surname>
            <given-names>D.P.</given-names>
          </name>
          <name>
            <surname>Ba</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Adam: a method for stochastic optimization</article-title>
        <source>arXiv</source>
        <year>2014</year>
        <pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id>
      </element-citation>
    </ref>
    <ref id="bib40">
      <label>40</label>
      <element-citation publication-type="book" id="sref40">
        <person-group person-group-type="author">
          <name>
            <surname>Tan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Le</surname>
            <given-names>Q.</given-names>
          </name>
        </person-group>
        <part-title>Efficientnet: rethinking model scaling for convolutional neural networks</part-title>
        <source>International Conference on Machine Learning</source>
        <year>2019</year>
        <publisher-name>PMLR</publisher-name>
        <publisher-loc>New York</publisher-loc>
        <fpage>6105</fpage>
        <lpage>6114</lpage>
      </element-citation>
    </ref>
    <ref id="bib41">
      <label>41</label>
      <element-citation publication-type="book" id="sref41">
        <person-group person-group-type="author">
          <name>
            <surname>Sandler</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Howard</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <part-title>Mobilenetv2: inverted residuals and linear bottlenecks</part-title>
        <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <year>2018</year>
        <publisher-name>IEEE</publisher-name>
        <publisher-loc>New York</publisher-loc>
        <fpage>4510</fpage>
        <lpage>4520</lpage>
      </element-citation>
    </ref>
    <ref id="bib42">
      <label>42</label>
      <element-citation publication-type="journal" id="sref42">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Bowd</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Christopher</surname>
            <given-names>M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Detecting glaucoma in the ocular hypertension study using deep learning</article-title>
        <source>JAMA Ophthalmol</source>
        <volume>140</volume>
        <issue>4</issue>
        <year>2022</year>
        <fpage>383</fpage>
        <lpage>391</lpage>
        <pub-id pub-id-type="pmid">35297959</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec id="appsec1" sec-type="supplementary-material">
    <title>Supplementary Data</title>
    <p id="p0200">
      <supplementary-material content-type="local-data" id="mmc1">
        <caption>
          <title>Supplementary Figure S1</title>
        </caption>
        <media xlink:href="mmc1.pdf"/>
      </supplementary-material>
      <supplementary-material content-type="local-data" id="mmc2">
        <caption>
          <title>Supplementary Table S1-S3</title>
        </caption>
        <media xlink:href="mmc2.pdf"/>
      </supplementary-material>
    </p>
  </sec>
  <fn-group>
    <fn id="d35e457">
      <p id="ntpara0010">
        <italic>Supplemental material available at</italic>
        <ext-link ext-link-type="uri" xlink:href="http://www.ophthalmologyscience.org" id="intref0010">
          <italic>www.ophthalmologyscience.org</italic>
        </ext-link>
        <italic>.</italic>
      </p>
    </fn>
    <fn id="d35e466">
      <p id="ntpara0015">Disclosure:</p>
    </fn>
    <fn id="d35e469">
      <p id="ntpara0020">All authors have completed and submitted the ICMJE disclosures form.</p>
    </fn>
    <fn id="d35e472">
      <p id="ntpara0025">The authors have made the following disclosures:</p>
    </fn>
    <fn id="d35e475">
      <p id="ntpara0030">No conflicting relationship exists for any author.</p>
    </fn>
    <fn id="d35e478">
      <p id="ntpara0035">This project was supported by the <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/100000092</institution-id><institution>National Library of Medicine</institution></institution-wrap></funding-source> under award number 4R00LM013001. This work was also supported by awards from the <funding-source id="gs2"><institution-wrap><institution-id institution-id-type="doi">10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source>, the <funding-source id="gs3"><institution-wrap><institution-id institution-id-type="doi">10.13039/100000096</institution-id><institution>National Center on Minority Health and Health Disparities</institution></institution-wrap></funding-source>, <funding-source id="gs4"><institution-wrap><institution-id institution-id-type="doi">10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source> (grants EY09341, EY09307), <funding-source id="gs5"><institution-wrap><institution-id institution-id-type="doi">10.13039/100007414</institution-id><institution>Horncrest Foundation</institution></institution-wrap></funding-source>, awards to the Department of Ophthalmology and Visual Sciences at <funding-source id="gs6">Washington University</funding-source>, the <funding-source id="gs7"><institution-wrap><institution-id institution-id-type="doi">10.13039/100000002</institution-id><institution>NIH</institution></institution-wrap></funding-source> Vision Core Grant P30 EY 02687, <funding-source id="gs8">Merck Research Laboratories</funding-source>, <funding-source id="gs9"><institution-wrap><institution-id institution-id-type="doi">10.13039/100004319</institution-id><institution>Pfizer, Inc</institution></institution-wrap></funding-source>, <funding-source id="gs10">White House Station</funding-source>, New Jersey, and unrestricted grants from <funding-source id="gs11"><institution-wrap><institution-id institution-id-type="doi">10.13039/100001818</institution-id><institution>Research to Prevent Blindness, Inc</institution></institution-wrap></funding-source>, New York, NY.</p>
    </fn>
    <fn id="d35e528">
      <p id="ntpara0040b">HUMAN SUBJECTS: Human Subjects were used in this study. The study protocol was approved by Institutional Review Board at each clinical center and Weill Cornell Medicine. All research adhered to the tenets of the Declaration of Helsinki. All participants provided informed consent.</p>
    </fn>
    <fn id="d35e531">
      <p id="ntpara0040bc">No animal subjects were used in this study.</p>
    </fn>
    <fn id="d35e534">
      <p id="ntpara0040">Author Contributions:</p>
    </fn>
    <fn id="d35e537">
      <p id="ntpara0045">Research design: Lin, Wang, Van Tassel, Peng</p>
    </fn>
    <fn id="d35e540">
      <p id="ntpara0050">Data acquisition and/or research execution: Liu, Gordon, Kass</p>
    </fn>
    <fn id="d35e544">
      <p id="ntpara0055">Data analysis and/or interpretation: Lin, Van Tassel</p>
    </fn>
    <fn id="d35e547">
      <p id="ntpara0040ddb">Obtained funding: N/A</p>
    </fn>
    <fn id="d35e550">
      <p id="ntpara0060">Manuscript preparation: Lin, Liu, Gordon, Kass, Wang, Van Tassel, Peng</p>
    </fn>
  </fn-group>
</back>
