<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?covid-19-tdm?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Multimed Tools Appl</journal-id>
    <journal-id journal-id-type="iso-abbrev">Multimed Tools Appl</journal-id>
    <journal-title-group>
      <journal-title>Multimedia Tools and Applications</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1380-7501</issn>
    <issn pub-type="epub">1573-7721</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8742670</article-id>
    <article-id pub-id-type="pmid">35035265</article-id>
    <article-id pub-id-type="publisher-id">11568</article-id>
    <article-id pub-id-type="doi">10.1007/s11042-021-11568-7</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>1210: Computer Vision for Clinical Images</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>AM-UNet: automated mini 3D end-to-end U-net based network for brain claustrum segmentation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Albishri</surname>
          <given-names>Ahmed Awad</given-names>
        </name>
        <address>
          <email>aa8w2@umsystem.edu</email>
          <email>a.albishri@seu.edu.sa</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7181-2079</contrib-id>
        <name>
          <surname>Shah</surname>
          <given-names>Syed Jawad Hussain</given-names>
        </name>
        <address>
          <email>shs6g7@umsystem.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kang</surname>
          <given-names>Seung Suk</given-names>
        </name>
        <address>
          <email>kangseung@umkc.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lee</surname>
          <given-names>Yugyung</given-names>
        </name>
        <address>
          <email>leeyu@umkc.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.266756.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2179 926X</institution-id><institution>School of Computing and Engineering, </institution><institution>University of Missouri-Kansas City, </institution></institution-wrap>Kansas City, MO 64110 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.449598.d</institution-id><institution-id institution-id-type="ISNI">0000 0004 4659 9645</institution-id><institution>College of Computing and Informatics, </institution><institution>Saudi Electronic University, </institution></institution-wrap>Riyadh, Saudi Arabia </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.266756.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2179 926X</institution-id><institution>Department of Psychiatry Biomedical Sciences, School of Medicine, </institution><institution>University of Missouri-Kansas City, </institution></institution-wrap>Kansas City, MO 64110 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>8</day>
      <month>1</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2022</year>
    </pub-date>
    <volume>81</volume>
    <issue>25</issue>
    <fpage>36171</fpage>
    <lpage>36194</lpage>
    <history>
      <date date-type="received">
        <day>17</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>8</day>
        <month>9</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>9</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2021</copyright-statement>
      <license>
        <license-p>This article is made available via the PMC Open Access Subset for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original source. These permissions are granted for the duration of the World Health Organization (WHO) declaration of COVID-19 as a global pandemic.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Recent advances in deep learning (DL) have provided promising solutions to medical image segmentation. Among existing segmentation approaches, the U-Net-based methods have been used widely. However, very few U-Net-based studies have been conducted on automatic segmentation of the human brain claustrum (CL). The CL segmentation is challenging due to its thin, sheet-like structure, heterogeneity of its image modalities and formats, imperfect labels, and data imbalance. We propose an automatic optimized U-Net-based 3D segmentation model, called AM-UNet, designed as an end-to-end process of the pre and post-process techniques and a U-Net model for CL segmentation. It is a lightweight and scalable solution which has achieved the state-of-the-art accuracy for automatic CL segmentation on 3D magnetic resonance images (MRI). On the T1/T2 combined MRI CL dataset, AM-UNet has obtained excellent results, including Dice, Intersection over Union (IoU), and Intraclass Correlation Coefficient (ICC) scores of 82%, 70%, and 90%, respectively. We have conducted the comparative evaluation of AM-UNet with other pre-existing models for segmentation on the MRI CL dataset. As a result, medical experts confirmed the superiority of the proposed AM-UNet model for automatic CL segmentation. The source code and model of the AM-UNet project is publicly available on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/AhmedAlbishri/AM-UNET">https://github.com/AhmedAlbishri/AM-UNET</ext-link>.</p>
    </abstract>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>national science foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1747751</award-id>
        <principal-award-recipient>
          <name>
            <surname>Albishri</surname>
            <given-names>Ahmed Awad</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>nasard young investigator grant</institution>
        </funding-source>
        <award-id>25158</award-id>
        <principal-award-recipient>
          <name>
            <surname>Shah</surname>
            <given-names>Syed Jawad Hussain</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© Springer Science+Business Media, LLC, part of Springer Nature 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">There has been considerable attention to medical imaging technologies and applications due to their substantial benefits for medical diagnosis and treatment. In particular, there have been advances in automated medical imaging segmentation. On the other hand, the demand and interest for new medical imaging applications have grown substantially. The advancements in deep learning, especially with image segmentation technologies, have been applied to medical imaging and automated segmentation [<xref ref-type="bibr" rid="CR31">31</xref>] for a broad range of studies, from cancer detection [<xref ref-type="bibr" rid="CR13">13</xref>] to brain tumor segmentation [<xref ref-type="bibr" rid="CR23">23</xref>] and treatment and prevention of brain aging-related conditions, Alzheimer’s disease (AD) [<xref ref-type="bibr" rid="CR51">51</xref>].</p>
    <p id="Par3">The image segmentation and detection studies using Convolutional Neural Networks (CNN) [<xref ref-type="bibr" rid="CR31">31</xref>] have shown its performance comparable to that of visual detection or segmentation performed by human operators. Segmentation tasks play a vital role in delineating different anatomical structures, and other regions [<xref ref-type="bibr" rid="CR7">7</xref>]. However, there are still challenges in applying deep learning technologies to neuroimaging segmentation in practice. Specifically, segmentation becomes a challenging task due to the following reasons [<xref ref-type="bibr" rid="CR18">18</xref>]:<list list-type="bullet"><list-item><p id="Par4">Variations in the size and structure of the objects.</p></list-item><list-item><p id="Par5">Images with different modalities, in different formats, and of different sizes due to the usage of different scanning systems.</p></list-item><list-item><p id="Par6">Insufficient manually labeled data for training.</p></list-item><list-item><p id="Par7">High class-imbalance in most training and evaluation datasets.</p></list-item><list-item><p id="Par8">Lack of high quality of data manually annotated by domain experts.</p></list-item><list-item><p id="Par9">Complicated and time-consuming task of manual segmentation.</p></list-item><list-item><p id="Par10">Vulnerability to human errors during experts’ annotation.</p></list-item></list>These challenges could be amplified on more complex data, e.g., computed tomography (CT) images or magnetic resonance images (MRI), which have been used to diagnose disease for the best treatment decision [<xref ref-type="bibr" rid="CR53">53</xref>]. In particular, an increasing number of neuroimaging studies have focused on identifying an intricate pattern or a small region in the human brain for predictive medicine. Medical imaging technologies need to handle multiple images or slices representing anatomical volumes in a specific data format acquired from imaging scanners [<xref ref-type="bibr" rid="CR18">18</xref>]. The automated segmentation can be conducted with specific human organs (e.g., liver, brain, lung) or their boundary annotations of labeled datasets [<xref ref-type="bibr" rid="CR31">31</xref>]. However, automated segmentation technologies are mainly developed for large cortical and subcortical areas (e.g., FreeSurfer [<xref ref-type="bibr" rid="CR20">20</xref>]). The conventional segmentation methods have limited capacity to obtain high accuracy in segmenting small regions of medical brain images, especially those of the deep brain [<xref ref-type="bibr" rid="CR49">49</xref>]. Therefore, it is imperative to develop more advanced automated segmentation methods that can be applied to small regions or complex and diverse data formats to facilitate various medical applications for medical research and clinical practice.</p>
    <p id="Par11">Automatic segmentation studies on small and complex brain regions have not been widely attempted. The present study focused on the claustrum (CL), an excellent example of an important and challenging target of the CNN approach. CL is a mysterious thin deep brain grey matter structure located at the center of each hemisphere. It is known as the brain’s most highly connected hub [<xref ref-type="bibr" rid="CR45">45</xref>]. CL has reciprocal connectivity with almost all cortical and subcortical brain areas, and massive input from all significant neuromodulator circuits [<xref ref-type="bibr" rid="CR44">44</xref>]. Based on the anatomy and animal neurophysiological findings, CL has been hypothesized as a brain network hub node for multisensory integration [<xref ref-type="bibr" rid="CR35">35</xref>], conscious percepts [<xref ref-type="bibr" rid="CR12">12</xref>], and bottom-up and top-down attention [<xref ref-type="bibr" rid="CR22">22</xref>]. Studies have reported CL volume reduction in post-mortem brains in people with autism spectrum disorders [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR48">48</xref>] and schizophrenia [<xref ref-type="bibr" rid="CR8">8</xref>] and CL abnormalities in patients with neurological disorders [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR43">43</xref>]. Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the MRI of the human brain with CL highlighted in orange.<fig id="Fig1"><label>Fig. 1</label><caption><p>The human claustrum (orange) delineated in a T1-weighted MRI</p></caption><graphic xlink:href="11042_2021_11568_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par12">Despite accumulating evidence of CL abnormalities in people with neuropsychiatric disorders [<xref ref-type="bibr" rid="CR36">36</xref>, <xref ref-type="bibr" rid="CR43">43</xref>], few neuroimaging studies have been conducted to investigate the functions of human CL. It is primarily due to (1) the methodological limitations of conventional neuroimaging techniques to isolate the thin structure of CL (i.e., the limited spatial resolutions of MRI and other neuroimaging techniques) and (2) lack of essential neuroimaging tools (e.g., no CL label in most widely used neuroimaging brain atlases, no reliable method to delineate CL). In addition, the region of interest (ROI) approach to CL requires manual segmentation of CL using structural MR images. An earlier study developed a manual tracing protocol for volumetric analysis of the human CL [<xref ref-type="bibr" rid="CR14">14</xref>]. However, the protocol did not provide details of CL’s unique anatomy and no clear boundary to delineate the sub-regions. Also, manual segmentation of CL on MRI’s is a time-consuming and challenging procedure that requires domain expertise.</p>
    <p id="Par13">In this paper, we propose an end-to-end mini U-Net-based network, AM-UNet, to achieve automatic 3D CL segmentation. For the training and validation of our AM-UNet network, we used the CL label maps that domain experts manually segmented. Furthermore, to improve the model accuracy while mainly reducing the training dataset, we have optimized the end-to-end workflow consisting of preprocessing, augmentation, and postprocessing. We have explored the proposed approach by experimenting with various deep learning architectures (U-Net variants) for automatic segmentation of the CL from T1, T2, and T1T2 weighted MRI datasets. Using multiple performance metrics, we extensively evaluated the model performance by comparing AM-UNet with two other state-of-the-art segmentation models. Our main contributions to this research are as follows:<list list-type="bullet"><list-item><p id="Par14">Design of an optimal U-Net architecture for smaller, simpler, and efficient CL segmentation.</p></list-item><list-item><p id="Par15">Development of an automated end-to-end 3D segmentation network for CL segmentation.</p></list-item><list-item><p id="Par16">Robust segmentation model for multi sequences (T1, T2, and T1T2) and multi-view (Axial, Coronal, or Sagittal) MRIs from the Human Connectome Project (HCP) [<xref ref-type="bibr" rid="CR46">46</xref>].</p></list-item><list-item><p id="Par17">State-of-the-art results of automatic CL segmentation based on extensive evaluations, including those of a domain expert.</p></list-item></list></p>
  </sec>
  <sec id="Sec2">
    <title>Related work</title>
    <p id="Par18">We have conducted extensive literature reviews on automated medical image segmentation. This section will first focus on medical imaging and then will discuss related work on deep learning methods for medical image segmentation.</p>
    <sec id="Sec3">
      <title>Medical imaging</title>
      <p id="Par19">Advanced techniques for medical image formats, including Nifti, Minc, and Dicom [<xref ref-type="bibr" rid="CR30">30</xref>], have shown excellent performance on detection of anatomies in medical search [<xref ref-type="bibr" rid="CR31">31</xref>]. Furthermore, medical image segmentation for partitioning the areas of interest and their boundaries has shown promising results [<xref ref-type="bibr" rid="CR31">31</xref>]. The machine learning techniques enhance the model performance, optimize the model weights while training, and remove the irrelevant aspect of the data [<xref ref-type="bibr" rid="CR39">39</xref>]. In deep learning algorithms, the more data the algorithm trains on, the better the results it can produce [<xref ref-type="bibr" rid="CR40">40</xref>]. The data augmentation technique is used to create more training data from the available dataset. The data augmentation is a general solution to reduce over-fitting on image data [<xref ref-type="bibr" rid="CR4">4</xref>]. Also, obtaining a large amount of accurately labeled medical images (e.g., manually segmented MRI data) is not easy. Therefore, it is highly recommended for automatic deep learning-based segmentation [<xref ref-type="bibr" rid="CR41">41</xref>]. In this technique, a slightly different stretch is applied to each image to create a new variation. As such, the same brain observed in two other embodiments may have different appearances, which should not influence the decision of whether a CL is present or not. Thus, elastic deformation is a commonly used technique to augment and model variations in data [<xref ref-type="bibr" rid="CR9">9</xref>].</p>
    </sec>
    <sec id="Sec4">
      <title>Deep learning for medical image segmentation</title>
      <p id="Par20">Recent works have presented CNN-based medical image segmentation approaches, grouped into 3D-based and 2D-based models. 3D CNN-based models primarily focus on extracting volumetric information across all three spatial dimensions. The 3D deeply supervised network (3D DSN) was proposed for automated segmentation of volumetric medical images [<xref ref-type="bibr" rid="CR17">17</xref>]. The 3D DSN model was evaluated on 3D CT scans and MRIs of the liver and heart. Their volume-to-volume learning and inferences and the 3D deep supervision mechanism effectively handled gradient vanishing/exploding problems, accelerating the convergence speed and improving the discrimination capability using an objective function that guides both the upper and lower layers in the network. Interestingly, the fully connected conditional random field was used in a postprocessing step to enhance the performance.</p>
      <p id="Par21">Another approach, which is similar to AM-UNet, was proposed with pre-and postprocessing image analysis techniques and 2D CNN to create a 3D segmentation network [<xref ref-type="bibr" rid="CR6">6</xref>]. Specifically, the 2D U-Net model was used for segmentation. The 3D construction approach was performed in the postprocessing step. This model performed well for image analysis of cervical vertebrae of patients and normal controls to resolve multiple cervical bone segmentation and separations [<xref ref-type="bibr" rid="CR6">6</xref>].</p>
      <p id="Par22">2D CNN models usually segment the CT scans or MRI data in a slice-wise fashion. In recent work, [<xref ref-type="bibr" rid="CR32">32</xref>], multi-view fully convolutional networks were used to segment the CL jointly. In this work, 181 T1-weighted MRI scans were to train two networks, Axial and Coronal. Then the probability ensemble method was applied to generate a segmentation map of CL, including Axial, Coronal, Sagittal, Axial + Coronal, and Axial + Coronal + Sagittal views; hence, they named it a multi-view model. Compared to the multi-view study using 181 subjects’ MRI scans, we used only 30 MRI scans for training, including various types of MRIs (T1-, T2-, and T1/T2-weighted), to produce segmentation maps for the Axial, Coronal, or Sagittal views. Another recent approach, UNet++, used a deeply-supervised encoder-decoder network for segmentation [<xref ref-type="bibr" rid="CR34">34</xref>]. In this method, the encoder and decoder path is connected through a series of nested, dense skip connections. Although UNet++ has shown some promising results, it uses more learning parameters than U-Net and AM-UNet due to its complex architecture.</p>
      <p id="Par23">The idea of using skip connection to upsample the network output was first introduced in Fully Convolutional Networks (FCNs) [<xref ref-type="bibr" rid="CR34">34</xref>]. However, FCNs’ main drawback is the difficulty in training the model from scratch. Segnet [<xref ref-type="bibr" rid="CR5">5</xref>] modified the skip connections to copy the indices from max pooling to the decoder path on the right. This study produced better segmentation results than FCNs while making the network lighter and faster to converge. Finally, Ronneberger et al. [<xref ref-type="bibr" rid="CR41">41</xref>] presented the idea of copying the feature maps from the encoder to the decoder part through skip connections designed with U-Net for image segmentation. This study showed good performance with a minimal amount of biomedical images, and data augmentation was the key to the success of this model.</p>
      <p id="Par24">Inf-Net used a modified U-Net architecture for automatic COVID-19 lung infection segmentation from CT images [<xref ref-type="bibr" rid="CR19">19</xref>]. This architecture is inspired by the observation that low-level features demand more computational resources due to larger spatial resolutions but contribute less to the performance compared with high-level features. Therefore, this model only aggregates high-level features. Inf-Net consists of three reverse attention (RA) modules connected to the paralleled partial decoder. In one recent work, UNET 3+, the researchers modified the U-Net architecture to take advantage of the full-scale skip connections, and deep supervisions [<xref ref-type="bibr" rid="CR25">25</xref>]. This model incorporates low-level information with high-level semantics from features maps, and deep supervision helps in learning hierarchical representations from the full-scale aggregated feature maps. Unfortunately, even though this approach showed promising results for organs at varying scales, this design is intricate. It creates many skip connections with lots of additional parameters than the original U-Net design that results in high training costs with low efficiency.</p>
      <p id="Par25">Moeskops et al. [<xref ref-type="bibr" rid="CR38">38</xref>] applied a CNN model that used multiple patch sizes and various convolution kernel sizes to get multi-scale information of each voxel. The authors successfully created a robust model that worked with different image modalities. Kayalibay et al. [<xref ref-type="bibr" rid="CR28">28</xref>] applied the U-Net model to segment two datasets, including hand and brain scans. They made two changes to U-Net architecture, including combining multiple feature maps at different scales and element-wise multiplication instead of concatenation. The first change harmed the model performance, while the second one could speed up the convergence time. Kushibar et al. [<xref ref-type="bibr" rid="CR29">29</xref>] proposed a novel method that took advantage of combining the convolutional features and the prior spatial features from a brain atlas. Their network was trained with 2.5D batches, instead of 3D, due to the memory constraint. Christ et al. [<xref ref-type="bibr" rid="CR10">10</xref>] applied the U-Net model to segment the liver and liver lesions in a cascaded way, and they obtained an improved accuracy using a conditional random field (CRF). We also presented a novel cascaded U-Net model for liver and liver tumor segmentation and summarization, called CU-Net [<xref ref-type="bibr" rid="CR2">2</xref>]. It consists of two U-Net models and works in a cascaded fashion to segment liver and liver tumors.<fig id="Fig2"><label>Fig. 2</label><caption><p>AM-UNet Architecture</p></caption><graphic xlink:href="11042_2021_11568_Fig2_HTML" id="MO2"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec5">
    <title>Methodology</title>
    <p id="Par26">AM-UNet is a fully automated 3D segmentation network that is based on a 2D U-Net model. AM-UNet consists of three parts, preprocessing, segmentation, and postprocessing, as seen in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. In the preprocessing step, the 3D MRI volumes are converted into a series of 2D slices, and the region of interests (ROIs) are created. The segmentation model is designed to segment a series of 2D ROIs based on 2D U-Net models. The postprocessing step reconstructs 3D MRI from the 2D segmented slices.<fig id="Fig3"><label>Fig. 3</label><caption><p>Three Views of MRI Scans (from left to right) (<bold>a</bold>) Axial, (<bold>b</bold>) Coronal, (<bold>c</bold>) Sagittal</p></caption><graphic xlink:href="11042_2021_11568_Fig3_HTML" id="MO3"/></fig></p>
    <sec id="Sec6">
      <title>Manual segmented human brain data</title>
      <p id="Par27">For this project, we used the MRI data of 30 healthy adults (13 males and 17 females; age range 21-35 years old) for CL segmentation. The dataset, including T1- and T2-weighted MRIs, were collected as a part of the Washington University-Minnesota Consortium Human Connectome Project (WU-Minn HCP) [<xref ref-type="bibr" rid="CR46">46</xref>], which are available from the ConnectomDB [<xref ref-type="bibr" rid="CR11">11</xref>]. The MRIs were acquired using a customized Siemens 3T Connectome Skyra scanner with the 3D MPRAGE T1 and T2 weighted sequence with 0.7 mm isotropic resolution (FOV=224 mm, matrix=320, 256 Sagittal slices in a single slab, TR=2400 ms, TE=2.14 ms, TI=1000 ms, flip angle=8<inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$^{\circ }$$\end{document}</tex-math><mml:math id="M2"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq1.gif"/></alternatives></inline-formula>). MRI scans can be viewed in the three orientational planes, Axial, Coronal, Sagittal as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. The details of MRI data collection and preprocessing methods are described in [<xref ref-type="bibr" rid="CR50">50</xref>]. Using this dataset, we created T1T2 combined MRI (T1T2w) by integrating T1w and T2w images to improve the contrast between the gray and white matters [<xref ref-type="bibr" rid="CR37">37</xref>] using the following formula:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} T1T2w = \frac{(T1 - sT2)}{(T1+sT2)} \end{aligned}$$\end{document}</tex-math><mml:math id="M4" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi><mml:mn>1</mml:mn><mml:mi>T</mml:mi><mml:mn>2</mml:mn><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mi>T</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par28">In Eq. <xref rid="Equ1" ref-type="">1</xref>, sT2 is a scaled T2 with the same median signal intensity (of brain part) as that of the T1 MRI. The combined MRI is further scaled to have the same median value as the original T1. Thus, T1T2w MRI has better grey/white matter contrasts, and due to the opposite directional signals of T1 and T2 and the calculations, the new MRI data has some high values in the brain’s margin. Figure <xref rid="Fig4" ref-type="fig">4</xref> shows the T1w, T2w, and T1T2w MRI images along with graphs of their respective pixel intensities (normalized). As seen from this figure, the data has a class imbalance issue, and the majority of the pixels (50000) have 0.0 intensity (black background color). In [<xref ref-type="bibr" rid="CR27">27</xref>], we reported the manual CL segmentation for the 30 T1 &amp; T2 MRI scans based on a protocol developed for the conventional 3T resolution (1.0 mm isotropic voxels). This protocol is widely used for MRI-based human brain atlas [<xref ref-type="bibr" rid="CR16">16</xref>]. The annotated CL label map in Nifti 3D format was used for the CL automatic segmentation.<fig id="Fig4"><label>Fig. 4</label><caption><p>T1w, T2w, and T1T2w MRIs and Normalized Pixel Intensities Graph </p></caption><graphic xlink:href="11042_2021_11568_Fig4_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec7">
      <title>Preprocessing</title>
      <p id="Par29">The preprocessing stage was designed to prepare the MRI data for the CL segmentation, as shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. It was composed of three sub-stages: (1) processing of the 3D MRI slicing for the 2D U-Net segmentation (selection of 2D CL slices from 3D MRI), (2) processing of ROI selection from the 2D selected slices (ROI operation from 2D selected slices to 2D ROI images), and (3) machine learning techniques (data normalization and augmentation). Also, note that the preprocessing steps were performed for the multi-view CL segmentation of the axial, coronal, and sagittal planes.<fig id="Fig5"><label>Fig. 5</label><caption><p>Preprocessing for CL Segmentation: (<bold>1</bold>) 3D MRI Slicing (<bold>2</bold>) Slice Selection (<bold>3</bold>) ROI</p></caption><graphic xlink:href="11042_2021_11568_Fig5_HTML" id="MO6"/></fig></p>
      <p id="Par30">The goal of the preprocessing step was to convert the 3D MRIs into 2D slices to generate ROIs, which will be used as an input by the 2D segmentation model. Initially, 2D slices were generated from 3D MR images to preprocess the data in a slice-wise fashion for optimal model performance, as shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a. Each MRI contains around 260 slices for each subject. Therefore, all the input slices were resized to have the same pixel size of 256<inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M6"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq2.gif"/></alternatives></inline-formula>256. Secondly, the selection task was performed to ensure that only the slices containing CL were included in training the CL segmentation model. This step reduced the number of slices on average from 260 to 36 for each MRI, as shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>b. Lastly, we processed to overcome the issue of a massive class imbalance between the target (CL is approximately 0.3%) and the extensive background regions, which is shown in the intensities graphs of the three data types (T1w, T2w, and T1T2w; Fig. <xref rid="Fig4" ref-type="fig">4</xref>). To this end, the filtered slices were superimposed onto the region of interest (ROI) with a size of 96<inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M8"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq3.gif"/></alternatives></inline-formula>128 from the full-size slice with a size of 256<inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M10"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq4.gif"/></alternatives></inline-formula>256, as shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>c. After applying the ROI, each slice contained approximately 98.4% of background pixels and 1.6% of the CL pixels. Table <xref rid="Tab1" ref-type="table">1</xref> compares the number of CL and the background pixels before and after the ROI procedure. The ROI procedure significantly reduced the number of background pixels without affecting the number of the CL pixels, therefore decreasing the class imbalance.</p>
      <p id="Par31">Besides the preprocessing of MRI images, machine learning (ML) techniques were applied to increase the performance. In our study, normalization was performed on the ROI’s pixel intensities ranging from 0 to 1. Further, data was augmented to produce the desired invariance properties and increase the training dataset. Additional slices were synthetically generated using the stretching technique. With data augmentation, the size of training data was increased from 855 to 1069 slices, as shown in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>The number of CL and the background pixels before and after an ROI procedure for three slices. The number of the CL pixels did not change before ROI (<inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_b$$\end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq5.gif"/></alternatives></inline-formula>) and after ROI (<inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_a$$\end{document}</tex-math><mml:math id="M14"><mml:msub><mml:mi>R</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq6.gif"/></alternatives></inline-formula>), but the number of background pixels is reduced significantly after the ROI procedure</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Slice</th><th align="left" colspan="3">CL Size</th><th align="left" colspan="2">Background Pixel#</th></tr><tr><th align="left">ID</th><th align="left">Pixel#</th><th align="left">Ratio(<inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_b$$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mi>R</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq7.gif"/></alternatives></inline-formula>)</th><th align="left">Ratio(<inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R_a$$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mi>R</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq8.gif"/></alternatives></inline-formula>)</th><th align="left">Before ROI</th><th align="left">After ROI</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">198</td><td align="left">0.3%</td><td align="left">1.6%</td><td align="left">65338</td><td align="left">12090</td></tr><tr><td align="left">2</td><td align="left">209</td><td align="left">0.3%</td><td align="left">1.7%</td><td align="left">65327</td><td align="left">12079</td></tr><tr><td align="left">3</td><td align="left">187</td><td align="left">0.28%</td><td align="left">1.5%</td><td align="left">65349</td><td align="left">12101</td></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec8">
      <title>CL segmentation model architecture</title>
      <p id="Par32">In our previous work [<xref ref-type="bibr" rid="CR3">3</xref>], we designed a U-Net architecture for human brain CL segmentation and obtained an average dice score of 72%. This U-Net architecture consists of an encoder and a decoder, which performed downsampling and upsampling with four layers, and are connected through skip connections for the construction of a high-resolution image. The proposed optimal U-Net architecture of AM-UNet, shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, is an extension of our previous architecture, making the model more simple and more efficient. In AM-UNet, we have reduced the segmentation model size to half by removing the last two layers of the original U-Net model and expanding the bottleneck layer of the segmentation model (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). AM-UNet consists of an encoder and a decoder connected with skip connections. On the left, the encoder path is called the contracting path, and it captures the context from the images in a Nifti format T1w, T2w, or T1T2w MRI file. On the right, the decoder path is called the expanding path, and it enables the precise localization of the target (i.e., CL) that needs to be segmented.</p>
      <p id="Par33">The features learned at each level in the encoder path are transferred to the decoder path through skip connections, where the features from the encorder path will be concatenated with the features from the decoder path [<xref ref-type="bibr" rid="CR41">41</xref>]. The encoder path maps a given input signal ’a’ that belongs to a set of signals ’A,’ which is a subset of all the signals ’R’ in the layer with the total number of dimensions ’d’, <inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$a \; \epsilon \, A \, \subset R^{d_{0}}$$\end{document}</tex-math><mml:math id="M20"><mml:mrow><mml:mi>a</mml:mi><mml:mspace width="0.277778em"/><mml:mi>ϵ</mml:mi><mml:mspace width="0.166667em"/><mml:mi>A</mml:mi><mml:mspace width="0.166667em"/><mml:mo>⊂</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq9.gif"/></alternatives></inline-formula>, to a feature space ’s’, which belongs to a set of feature spaces ’S,’ <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s \; \epsilon \; S \, \subset R^{d_{j}}$$\end{document}</tex-math><mml:math id="M22"><mml:mrow><mml:mi>s</mml:mi><mml:mspace width="0.277778em"/><mml:mi>ϵ</mml:mi><mml:mspace width="0.277778em"/><mml:mi>S</mml:mi><mml:mspace width="0.166667em"/><mml:mo>⊂</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq10.gif"/></alternatives></inline-formula>, and the decoder does the opposite of what the encoder does - taking the feature map as an input, processing it, and producing an output <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o \; \epsilon \; O \, \subset R^{d_{L}}$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi>o</mml:mi><mml:mspace width="0.277778em"/><mml:mi>ϵ</mml:mi><mml:mspace width="0.277778em"/><mml:mi>O</mml:mi><mml:mspace width="0.166667em"/><mml:mo>⊂</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq11.gif"/></alternatives></inline-formula>. The encoder and decoder paths are symmetric, and both have the same number of layers, say <italic>L</italic>. Eqs. <xref rid="Equ2" ref-type="">2</xref> and <xref rid="Equ3" ref-type="">3</xref> represent the input and output dimensions for the encoder layer, <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E^{L}$$\end{document}</tex-math><mml:math id="M26"><mml:msup><mml:mi>E</mml:mi><mml:mi>L</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq12.gif"/></alternatives></inline-formula>, and the decoder layer, <inline-formula id="IEq13"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$D^{L}$$\end{document}</tex-math><mml:math id="M28"><mml:msup><mml:mi>D</mml:mi><mml:mi>L</mml:mi></mml:msup></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq13.gif"/></alternatives></inline-formula>, respectively, where <inline-formula id="IEq14"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d_{L}$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mi>d</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq14.gif"/></alternatives></inline-formula> represents the total dimension of the feature at the L-th layer.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} E^{L} : R^{d_{L-1}}\longrightarrow R^{d_{L}} \end{aligned}$$\end{document}</tex-math><mml:math id="M32" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>E</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:msup><mml:mo stretchy="false">⟶</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} D^{L} : R^{d_{L}}\longrightarrow R^{d_{L-1}} \end{aligned}$$\end{document}</tex-math><mml:math id="M34" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo>:</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:msup><mml:mo stretchy="false">⟶</mml:mo><mml:msup><mml:mi>R</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par34">As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref> (Segmentation Model), the AM-UNet segmentation model consists of two layers along with the bottleneck layer, which is half of the size of the U-Net architecture. Each layer in the encoder path performs two rounds of convolution, batch normalization, and dropout, respectively. After that, the max-pooling reduces the image size by half and passes it to the next layer in the architecture. This process continues until the image reaches the last layer in the encoder path, called the bottleneck layer. We added three new blocks in this layer, each consisting of two rounds of convolution, batch normalization, and dropout, respectively. After the third block, we concatenated all the previous blocks’ output into the last block in the bottleneck layer. The three new block design was confirmed as optimal from the experiments with various numbers of blocks in the bottleneck layer.</p>
      <p id="Par35">The decoder path starts at the end of the bottleneck layer on the right side. Initially, upsampling is applied to increase the image resolution by a factor of 2. In regular upsampling, convolutions are replaced by transposed convolution to upsample the image. After that, the image is transferred to the next layer in the decoder path, which concatenates the feature maps from the contracting path (encoder), and performs two rounds of convolution, batch normalization, and dropout, respectively. After that image is upsampled again, the process continues until it reaches the last level in the decoder path, where image is restored to its original size of 96x128. The final layer in the decoder path has a sigmoid function that classifies each pixel in a binary way to a probability between 0 to 1.</p>
      <p id="Par36">We applied a window of 3<inline-formula id="IEq15"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\times$$\end{document}</tex-math><mml:math id="M36"><mml:mo>×</mml:mo></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq15.gif"/></alternatives></inline-formula>3 to the input image to construct a feature map in the convolution process. We used the ReLU activation function [<xref ref-type="bibr" rid="CR21">21</xref>] in these convolution processes. This activation function did not change the features’ positive weights, but it assigned a value of zero to all the negative weights of the features. Batch normalization was used to accelerate the network training by reducing the rate at which each layer’s inputs distribution changes during training, as the previous layer’s parameters change [<xref ref-type="bibr" rid="CR26">26</xref>]. To prevent overfitting, we implemented L2 and Dropout as regularization methods in the model. We also used early stopping [<xref ref-type="bibr" rid="CR47">47</xref>] for the same reason. Adam optimizer was used to optimize the weight and bias at each layer in the network, with a learning rate of 0.001 [<xref ref-type="bibr" rid="CR47">47</xref>]. The input to the model is an ROI image of a slice in a human brain MRI, and the output is the segmented CL image.</p>
      <p id="Par37">Compared to U-Net and UNet++, the AM-UNet architecture produces higher accurate results in less computation time on the CL dataset. It is half the size of U-Net and does not need the complex skip connections of UNet++. AM-UNet enables the model to produce quality segmentation results with a small amount of training data. Since we have removed the last two max-pooling layers from the original U-Net model, AM-UNet consumes fewer training parameters and is faster to converge. This model performs well for inter and intra-slice classification in detecting CL in identifying the coarse CL boundaries and segment them from the human brain MRI dataset.<fig id="Fig6"><label>Fig. 6</label><caption><p>Postprocessing for 3D MRI Reconstruction: (<bold>1</bold>) Filtering False Positives, (<bold>2</bold>) Reconstruction of 2D Slices, (<bold>3</bold>) Reconstruction of 3D MRI</p></caption><graphic xlink:href="11042_2021_11568_Fig6_HTML" id="MO9"/></fig></p>
    </sec>
    <sec id="Sec9">
      <title>Postprocessing</title>
      <p id="Par38">The postprocessing stage of the AM-UNet 3D segmentation network was designed to ensure a high prediction accuracy for 3D CL segmentation. It was composed of three sub-stages: (1) filtering false positives from the segmentation output, (2) reconstruction of the 2D slices from the segmented 2D ROIs (from 2D ROI images to 2D full images), (3) reconstruction of the 3D volumes from the 2D reconstructed slices (from 2D segmented slices to 3D MRI), as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. There was noise in the ground truth (manual annotation), which caused the segmentation model to falsely label some background pixels as CL. The false-positive removal step was designed to solve this issue. The removal of false positives was conducted through two steps: (i) each pixel was labeled with 1 or 0 that is 1 when probability <inline-formula id="IEq16"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p \ge \delta$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>p</mml:mi><mml:mo>≥</mml:mo><mml:mi>δ</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq16.gif"/></alternatives></inline-formula> (<inline-formula id="IEq17"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\delta = 0.5$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq17.gif"/></alternatives></inline-formula>), and 0 otherwise; (ii) the false positive pixels in the non-CL region were further detected and removed. The non-CL region was determined considering the ROI (represented by a rectangular matrix of 96x128) of each slice. In this matrix, CL pixels were supposed to present in the left and right CL regions. Hence, we filtered out the pixels falling out of the regions.</p>
      <p id="Par39">Secondly, the 2D segmented slices, as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>a, were restored to their original size of 256x256 using padding, as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>b. Thus, each ROI (or image) at this stage is a slice of MRI. Finally, all the annotated slices for CL segmentation were stacked back to reconstruct each subject’s 3D MRI brain. Each subject MRI may have a different number of slices for CL segmentation. Thus, the 3D MRI brain was reconstructed by replacing the original slices with the annotated slices, as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>c.</p>
    </sec>
    <sec id="Sec10">
      <title>Loss function</title>
      <p id="Par40">The AM-Net model used the stochastic gradient descent optimization algorithm and evaluated the model’s performance using the loss function of Weighted Binary Cross-Entropy (WBCE) [<xref ref-type="bibr" rid="CR24">24</xref>] during training. The higher value of the loss function indicates that the model is off, whereas the lower value indicates a good training performance. The WBCE loss function reduced the classification error due to the class imbalance between the target (CL) and the background pixels and resulted in the enhancement of the model performance. The WBCE is defined in Eq. <xref rid="Equ4" ref-type="">4</xref>.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} J_{wbce}= &amp; {} - \frac{1}{M}\sum _{m=1}^{M}[w\; x \; y_{m}\; x\; log(h_{\Theta }(x_{m})) \nonumber \\&amp;+ (1-y_{m})\; x\; log(1-h_{\Theta }(x_{m}))]\ \end{aligned}$$\end{document}</tex-math><mml:math id="M42" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi mathvariant="italic">wbce</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow/><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>w</mml:mi><mml:mspace width="0.277778em"/><mml:mi>x</mml:mi><mml:mspace width="0.277778em"/></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mspace width="0.277778em"/><mml:mi>x</mml:mi><mml:mspace width="0.277778em"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="normal">Θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mspace width="0.277778em"/><mml:mi>x</mml:mi><mml:mspace width="0.277778em"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="normal">Θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mspace width="4pt"/></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par41">Where <italic>M</italic> represents the number of the training examples, <italic>w</italic> is the weight, <inline-formula id="IEq18"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_{m}$$\end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mi>y</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq18.gif"/></alternatives></inline-formula> is the target label for the training example m, <inline-formula id="IEq19"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{m}$$\end{document}</tex-math><mml:math id="M46"><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq19.gif"/></alternatives></inline-formula> represents the input training example m, and <inline-formula id="IEq20"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h_{\Theta }$$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="normal">Θ</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq20.gif"/></alternatives></inline-formula> is the model with neural network weight <inline-formula id="IEq21"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\Theta }$$\end{document}</tex-math><mml:math id="M50"><mml:mi mathvariant="normal">Θ</mml:mi></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq21.gif"/></alternatives></inline-formula>.<table-wrap id="Tab2"><label>Table 2</label><caption><p>AM-UNet Dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="3">Subjects</th><th align="left" colspan="2">MRI Slices</th><th align="left" colspan="4">Selected Slices#</th><th align="left" colspan="2">ROI+Aug.</th></tr><tr><th align="left">Num.</th><th align="left">M/F</th><th align="left">Age</th><th align="left">Slice</th><th align="left">Size</th><th align="left">Min</th><th align="left">Avg</th><th align="left">Max</th><th align="left">Total</th><th align="left">Size</th><th align="left">Total</th></tr></thead><tbody><tr><td align="left">30</td><td align="left">13/17</td><td align="left">21-35</td><td align="left">260</td><td align="left">256x256</td><td align="left">21</td><td align="left">36</td><td align="left">47</td><td align="left">855</td><td align="left">96x128</td><td align="left">1069</td></tr></tbody></table></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Results and evaluation</title>
    <sec id="Sec12">
      <title>Dataset</title>
      <p id="Par42">We used the 30 subjects’ high-resolution HCP MRIs (0.7 mm isotropic voxels) that were manually traced by domain experts using the manual tracing protocol. The manual segmentation protocol [<xref ref-type="bibr" rid="CR27">27</xref>] was developed using a cellular-level human brain atlas [<xref ref-type="bibr" rid="CR15">15</xref>] that incorporated neuroimaging (T1 &amp; diffusion-weighted MRI), high-resolution histology, large-format cellular resolution Nissl, and immunohistochemistry anatomical plates of an intact adult brain. T1w, T2w, and T1T2w MRI were used to produce segmentation maps for the Axial, Coronal, and Sagittal views (see Table <xref rid="Tab2" ref-type="table">2</xref> for the details of this dataset).</p>
    </sec>
    <sec id="Sec13">
      <title>Evaluation metrics</title>
      <p id="Par43">To quantitatively evaluate the model accuracy, we have calculated various evaluation measures, including Dice score, Intersection over Union (IoU), Intraclass Correlation Coefficient (ICC), Sensitivity, Specificity, False Positive Rate (FPR), False Negative Rate (FNR), and Precision. True positive (TP) indicates the number of the CL pixels correctly predicted by the model. True negative (TN) indicates the number of the background pixels correctly predicted by the model. False positive (FP) indicates the number of background pixels that the model incorrectly predicted as CL. False negative (FN) indicates the number of CL pixels that the model incorrectly predicted as the background. Dice score is an index used to gauge the similarity of two samples in a range from 0 to 1 (see Eq. <xref rid="Equ5" ref-type="">5</xref>). The higher the value of the Dice score, the better is the accuracy of the model. IoU is an evaluation metric used to measure the accuracy of the image segmentation model by dividing the area of overlap between the predicted bounding box and the ground-truth bounding box by the total area encompassed by both, the predicted bounding box and the ground-truth bounding box, using Eq. <xref rid="Equ6" ref-type="">6</xref>.<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Dice Score = \frac{2TP}{2TP+FP+FN} \end{aligned}$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>S</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} IoU = \frac{\text {Area of Overlap}}{\text {Area of Union}} = \frac{TP}{TP+FP+FN} \end{aligned}$$\end{document}</tex-math><mml:math id="M54" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Area of Overlap</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Area of Union</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par44">For measuring the reliability of our evaluation, we used Intraclass Correlations (ICC), which describes how the observations in the same class resemble each other [<xref ref-type="bibr" rid="CR42">42</xref>]. It is the index that reflects both degrees of correlation and agreement between measurements. In ICC3, each subject was assessed by each rater, but the raters were the only raters of interest, and the reliability was calculated from a single measurement, while ICC(3,K) was assessed by taking an average of the k raters’ measurement. Eq. <xref rid="Equ7" ref-type="">7</xref> and <xref rid="Equ8" ref-type="">8</xref> shows the formula for ICC3 and ICC(3,k) calculations respectively, where <inline-formula id="IEq22"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MS_{R}$$\end{document}</tex-math><mml:math id="M56"><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq22.gif"/></alternatives></inline-formula> is the mean square for rows, <inline-formula id="IEq23"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MS_{W}$$\end{document}</tex-math><mml:math id="M58"><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>W</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq23.gif"/></alternatives></inline-formula> is the mean square for residual sources of variance, <inline-formula id="IEq24"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MS_{E}$$\end{document}</tex-math><mml:math id="M60"><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq24.gif"/></alternatives></inline-formula> is the mean square for error, <inline-formula id="IEq25"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MS_{C}$$\end{document}</tex-math><mml:math id="M62"><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq25.gif"/></alternatives></inline-formula> represents mean square for columns, and <italic>K</italic> represents the number of raters/measurements.<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} ICC3 = \frac{MS_{R} - MS_{E}}{MS_{R}+(K-1)MS_{E}} \end{aligned}$$\end{document}</tex-math><mml:math id="M64" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mn>3</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>K</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} ICC(3,K) = \frac{MS_{R} - MS_{E}}{MS_{R}} \end{aligned}$$\end{document}</tex-math><mml:math id="M66" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>I</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par45">We also computed the following measures: true positive rate (TPR) (Sensitivity) to determine how often our model can correctly identify the CL pixel (Eq. <xref rid="Equ9" ref-type="">9</xref>), true negative rate (TNR) (Specificity) to determine how often our model can correctly identify background pixels (Eq. <xref rid="Equ10" ref-type="">10</xref>), false positive rate (FPR) to determine how often the model incorrectly identifies background pixel as Cl (Eq. <xref rid="Equ11" ref-type="">11</xref>), False-negative rate (FNR) to measure how often the model will incorrectly identify CL pixel as background (Eq. <xref rid="Equ12" ref-type="">12</xref>), and Positive Predictive Value (PPV) (Precision) to measure how many pixels are CL out of all the pixels that the model has identified as CL for the CL segmentation (Eq. <xref rid="Equ13" ref-type="">13</xref>).<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} TPR = \frac{TP}{TP + FN} = 1 - FNR \end{aligned}$$\end{document}</tex-math><mml:math id="M68" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} TNR = \frac{TN}{TN + FP} = 1 - FPR \end{aligned}$$\end{document}</tex-math><mml:math id="M70" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} FPR = \frac{FP}{FP + TN} = 1 - TNR \end{aligned}$$\end{document}</tex-math><mml:math id="M72" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">FP</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} FNR = \frac{FN}{FN + TP} = 1 - TPR \end{aligned}$$\end{document}</tex-math><mml:math id="M74" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mi>R</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} Precision= PPV = \frac{TP}{TP + FP} \end{aligned}$$\end{document}</tex-math><mml:math id="M76" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mi>P</mml:mi><mml:mi>V</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="11042_2021_11568_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec14">
      <title>AM-UNet: slice and subject level performance</title>
      <p id="Par46">We calculated the evaluation metrics with k-fold cross-validation performed on the 30 subject MRI dataset. For CL segmentation, we evaluated AM-UNet performance on multiple MRIs, including T1w, T2w, and T1T2w, with three different views, including Axial, Coronal, and Sagittal views. We conducted the evaluation of the model on each of the three datasets separately, and for each run, we used k-fold cross-validation with the value of <italic>k = 5</italic>. The dataset was divided into five equal parts during each run. The model was trained on four parts and tested on one part. Then, the values of the evaluation parameters were recorded. This process was repeated five times so that model was tested on the whole dataset, and the average values for evaluation parameters were obtained at the end.<table-wrap id="Tab3"><label>Table 3</label><caption><p>AM-UNet performance on T1w only, T2w only, and T1T2w data</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="2">Axial view</th><th align="left" colspan="2">Coronal view</th></tr><tr><th align="left">Type</th><th align="left">Dice Score(%)</th><th align="left">IoU Score(%)</th><th align="left">Dice Score(%)</th><th align="left">IoU Score(%)</th></tr></thead><tbody><tr><td align="left">T1w</td><td align="left">82</td><td align="left">69</td><td align="left">81</td><td align="left">68</td></tr><tr><td align="left">T2w</td><td align="left">81</td><td align="left">68</td><td align="left">81</td><td align="left">68</td></tr><tr><td align="left">T1T2w</td><td align="left"><bold>82</bold></td><td align="left"><bold>70</bold></td><td align="left"><bold>82</bold></td><td align="left"><bold>70</bold></td></tr><tr><td align="left" colspan="5">Number of Subjects = 30</td></tr></tbody></table></table-wrap></p>
      <p id="Par47">We trained the model on the augmented dataset for CL segmentation and tested it on a non-augmented one. With this procedure, we obtained an average Dice score of 82%, 81%, and 82% and an average IoU score of 69%, 68%, and 70% for the CL segmentation for the Axial view of T1w, T2w, and T1T2w MRI, respectively. Table <xref rid="Tab3" ref-type="table">3</xref> shows the average Dice and IoU scores of AM-UNet on the three datasets for the Axial and Coronal views. The Sagittal results have been omitted due to its poor performance that does not provide meaningful results. To the best of our knowledge, these results are state-of-the-art for 3D human brain CL segmentation. ICC3 and ICC(3,K), whose manual tracer (raters) are the only raters of interest (3), are relevant to measure the reliability of the AM-UNet model. We evaluated the accuracy of the model’s CL segmentation, the single average measure of many pixels (K). Our model achieved 90% and 81% for ICC(3,K) and ICC3 respectively for T1T2w data set for the Axial view. The average values of Sensitivity, Specificity, FPR, FNR, and Precision of AM-UNet are shown in Table <xref rid="Tab4" ref-type="table">4</xref>. We achieved excellent overall scores on T1T2w data for the Axial view, such as Sensitivity (TPR) of 84.8%, Specificity (TNR) of 99.7%, FPR of 0.332%, FNR of 15.2% and Precision of 78.1%. We also evaluated AM-UNet on the slice level. Since our model was tested on the whole dataset, the model performance on individual slices was recorded. We analyzed the best and worst-case slices of our model for the Axial view T1T2w data. The details are shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>.<table-wrap id="Tab4"><label>Table 4</label><caption><p>AM-UNet accuracy analysis for T1T2w MRI in axial view</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Metric</th><th align="left">Average Value</th></tr></thead><tbody><tr><td align="left">Sensitivity (TPR)</td><td char="." align="char">84.8%</td></tr><tr><td align="left">Specificity (TNR)</td><td char="." align="char">99.7%</td></tr><tr><td align="left">False Positive Rate (FPR)</td><td char="." align="char">0.332%</td></tr><tr><td align="left">False Negative Rate (FNR)</td><td char="." align="char">15.2%</td></tr><tr><td align="left">Precision</td><td char="." align="char">78.1%</td></tr></tbody></table></table-wrap></p>
      <p id="Par48">
        <fig id="Fig7">
          <label>Fig. 7</label>
          <caption>
            <p>Best and worst cases of AM-UNet</p>
          </caption>
          <graphic xlink:href="11042_2021_11568_Fig7_HTML" id="MO20"/>
        </fig>
        <fig id="Fig8">
          <label>Fig. 8</label>
          <caption>
            <p>Subject-wise Evaluation for 3D CL Segmentation: Dice Coefficient Scores (on left), IoU Scores (on right)</p>
          </caption>
          <graphic xlink:href="11042_2021_11568_Fig8_HTML" id="MO21"/>
        </fig>
      </p>
      <p id="Par49">In addition, we evaluated AM-UNet performance on the subject level. As mentioned earlier, in the CL MRI dataset, each subject had 260 slices. Only slices containing CL (36 per subject on average) are presented to the segmentation model for prediction. Since we used the K-fold cross-validation approach for evaluation purposes, AM-UNet was tested on the whole dataset. The average Dice and IoU scores were calculated for the entire dataset. Figure <xref rid="Fig8" ref-type="fig">8</xref> shows the boxplots for Dice and IoU per case score for all the 30 subjects in the dataset. Figure <xref rid="Fig9" ref-type="fig">9</xref> depicts the ground truth (GT) and the model segmentation results of three subjects; the segmentation models precisely assigned each pixel of an image to one of the two classes, CL and the background. Figure <xref rid="Fig10" ref-type="fig">10</xref> shows the 3D reconstructed volumes of GT and AM-UNet predictions.<fig id="Fig9"><label>Fig. 9</label><caption><p>AM-UNet for CL Segmentation for 3 subjects: (<bold>a</bold>) Ground Truth, (<bold>b</bold>) Ground Truth (CL ROI), (<bold>c</bold>) Predicted Outcome</p></caption><graphic xlink:href="11042_2021_11568_Fig9_HTML" id="MO22"/></fig><fig id="Fig10"><label>Fig. 10</label><caption><p>AM-UNet 3D Segmentation (Green) and Ground Truth (Orange)</p></caption><graphic xlink:href="11042_2021_11568_Fig10_HTML" id="MO23"/></fig><table-wrap id="Tab5"><label>Table 5</label><caption><p>Comparative evaluation: AM-UNet &amp; multi-view CL model</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left" colspan="3">Dice score</th><th align="left">Subject</th></tr><tr><th align="left"/><th align="left">Axial</th><th align="left">Coronal</th><th align="left">Sagittal</th><th align="left"/></tr></thead><tbody><tr><td align="left">Multi-View Convol. Network (T1 Only) [<xref ref-type="bibr" rid="CR32">32</xref>]</td><td align="left">0.69</td><td align="left">0.70</td><td align="left">0.55</td><td align="left">181</td></tr><tr><td align="left">AM-UNet (T1 Only)</td><td align="left"><bold>0.82</bold></td><td align="left">0.81</td><td align="left"><bold>0.68</bold></td><td align="left"><bold>30</bold></td></tr><tr><td align="left">AM-UNet (T1T2)</td><td align="left"><bold>0.82</bold></td><td align="left"><bold>0.82</bold></td><td align="left"><bold>0.68</bold></td><td align="left"/></tr></tbody></table></table-wrap></p>
    </sec>
    <sec id="Sec15">
      <title>Comparative evaluation</title>
      <p id="Par50">One of the closely related approaches to our work uses a fully convolutional network with a multi-view method [<xref ref-type="bibr" rid="CR32">32</xref>]. This work employs a deep learning-based multi-view approach to segment CL from T1w data. Table <xref rid="Tab5" ref-type="table">5</xref> compares the Dice score accuracy of our model with the multi-view model. As seen in Table <xref rid="Tab5" ref-type="table">5</xref>, the multi-view model [<xref ref-type="bibr" rid="CR32">32</xref>] used 181 T1w MRI scans for training. As per the results in Table <xref rid="Tab5" ref-type="table">5</xref>, with only 30 MRIs as training data, our model has shown better performance in all three views. The multi-view model [<xref ref-type="bibr" rid="CR32">32</xref>] also generated segmentation maps for Axial + Coronal and Axial + Coronal + Sagittal views and obtained the Dice score of 0.718 and 0.710, respectively. AM-UNet achieved better performance in all these three views even with less data, as seen in Table <xref rid="Tab5" ref-type="table">5</xref>. Thus, we have not generated segmentation maps for Axial + Coronal and Axial + Coronal + Sagittal views that can be our future work.</p>
      <p id="Par51">Furthermore, AM-UNet was compared with the CL segmentation implementation using U-Net [<xref ref-type="bibr" rid="CR41">41</xref>] and UNet++ [<xref ref-type="bibr" rid="CR41">41</xref>]. Both of these models have shown promising results in the image segmentation domain. The CL segmentation models based on U-Net and UNet++ were trained in our experiment setting. Table <xref rid="Tab6" ref-type="table">6</xref> shows the results of this experiment for the Axial view using K-fold cross-validation with the value of <italic>k</italic> equal to 5. We computed the Dice and IoU scores for all three datasets (T1w, T2w, and T1T2w) but only reported the scores for T1T2w data.</p>
      <p id="Par52">As seen in Table <xref rid="Tab6" ref-type="table">6</xref>, AM-UNet has shown better results in most evaluation indicators. It has outperformed U-Net and UNet++ in the six metrics out of the nine ones. AM-UNet has achieved the best results for most of the critical evaluation parameters for the CL segmentation. Our method obtained the best scores for five evaluation metrics (Dice, IoU, ICC, TPR, FNR) and has used significantly fewer training parameters(#Par). Our method’s higher TPR score and lower FNR score verify that AM-UNet is optimized in detecting the CL pixels. UNet++ has achieved the best scores of three scores (TNR, precision, and FPR). UNet++ had the highest precision score because it predicted only a few CL pixels with high accuracy (and hence missed some CL pixels), and therefore it obtained a low FPR score. The high TNR of UNet++ shows its ability to identify the background pixels correctly. The U-Net model received poor results, such as the lowest precision and the highest score of FNR and FPR. The boxplots comparing the Dice and IoU scores across these three methods, Fig. <xref rid="Fig11" ref-type="fig">11</xref>, confirms the results presented in Table <xref rid="Tab6" ref-type="table">6</xref>. Figure <xref rid="Fig12" ref-type="fig">12</xref> shows the visual comparison of the three models on the T1T2w dataset for the Axial view. In this figure, MRI represents the input data, ROI represents the region of interest with CL highlighted in orange, GT represents the ground truth labels in white, AM-UNet represents the result of our model in green overlaid on white ground truth, U-Net represents the results of U-Net in red overlaid on white ground truth, and UNet++ represents the results of UNet++ in blue overlaid on white ground truth. The overlaid regions’ darker green, red, or blue pixels mean false positives, and white pixels represent false negatives. As seen in Fig. <xref rid="Fig12" ref-type="fig">12</xref>, U-Net and UNet++ had high false negative rates and lower true positive rates and therefore has more white pixels as compared to AM-UNet. In terms of false positive rates (the darker pixels), all the three models showed similar results, with UNet++ having marginally better performance than others.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Comparative evaluation: AM-UNet &amp; state-of-the-Art UNet networks</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left" colspan="3">Average dice score</th><th align="left" colspan="3">Average IoU</th><th align="left" colspan="2">ICC</th></tr><tr><th align="left"/><th align="left">T1</th><th align="left">T2</th><th align="left">T1T2</th><th align="left">T1</th><th align="left">T2</th><th align="left">T1T2</th><th align="left">ICC3</th><th align="left">ICC(3,K)</th></tr></thead><tbody><tr><td align="left">U-Net [<xref ref-type="bibr" rid="CR41">41</xref>]</td><td align="left">0.81</td><td align="left">0.78</td><td align="left">0.81</td><td align="left">0.68</td><td align="left">0.64</td><td align="left">0.69</td><td align="left">0.78</td><td align="left">0.87</td></tr><tr><td align="left">UNet++ [<xref ref-type="bibr" rid="CR41">41</xref>]</td><td align="left">0.80</td><td align="left">0.80</td><td align="left">0.81</td><td align="left">0.67</td><td align="left"><bold>0.69</bold></td><td align="left">0.68</td><td align="left">0.7</td><td align="left">0.87</td></tr><tr><td align="left">AM-UNet</td><td align="left"><bold>0.82</bold></td><td align="left"><bold>0.81</bold></td><td align="left"><bold>0.82</bold></td><td align="left"><bold>0.69</bold></td><td align="left">0.68</td><td align="left"><bold>0.70</bold></td><td align="left"><bold>0.81</bold></td><td align="left"><bold>0.90</bold></td></tr><tr><td align="left">Model</td><td align="left">TPR</td><td align="left">TNR</td><td align="left">FPR <inline-formula id="IEq26"><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><mml:math id="M78"><mml:mo stretchy="false">↓</mml:mo></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq26.gif"/></alternatives></inline-formula></td><td align="left">FNR <inline-formula id="IEq27"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\downarrow$$\end{document}</tex-math><mml:math id="M80"><mml:mo stretchy="false">↓</mml:mo></mml:math><inline-graphic xlink:href="11042_2021_11568_Article_IEq27.gif"/></alternatives></inline-formula></td><td align="left">Prec</td><td align="left">#Par</td><td align="left"/><td align="left"/></tr><tr><td align="left">U-Net [<xref ref-type="bibr" rid="CR41">41</xref>]</td><td align="left">0.80</td><td align="left">0.996</td><td align="left">0.0036</td><td align="left">0.76</td><td align="left">0.20</td><td align="left">8.6M</td><td align="left"/><td align="left"/></tr><tr><td align="left">UNet++ [<xref ref-type="bibr" rid="CR1">1</xref>]</td><td align="left">0.78</td><td align="left"><bold>0.998</bold></td><td align="left"><bold>0.0024</bold></td><td align="left">0.22</td><td align="left"><bold>0.82</bold></td><td align="left">9M</td><td align="left"/><td align="left"/></tr><tr><td align="left">AM-UNet</td><td align="left"><bold>0.85</bold></td><td align="left">0.997</td><td align="left">0.0033</td><td align="left"><bold>0.15</bold></td><td align="left">0.78</td><td align="left"><bold>0.34M</bold></td><td align="left"/><td align="left"/></tr></tbody></table></table-wrap></p>
      <p id="Par53">The performance of U-Net and UNet++ was also evaluated on individual MRI levels. In this evaluation, we identified the worst-cases of these two models, U-Net and UNet++, and compared the performance of all the three models, AM-UNet, U-Net, and UNet++, for these 2 MRIs. Figure <xref rid="Fig13" ref-type="fig">13</xref> shows the result of this comparison. This figure presents the visual comparison for the model performance and the values for the seven evaluation measures. As seen in Fig. <xref rid="Fig13" ref-type="fig">13</xref>, our model outperformed the other two models in terms of Dice score, IoU, Sensitivity, and FNR for both the MRIs. UNet++ had better results for Precision and FPR, mainly because the model has missed a lot of the CL pixels. This is also evident from the high FNR and low TPR of UNet++ and the visuals provided in Fig. <xref rid="Fig13" ref-type="fig">13</xref>. As seen in the figure, UNet++ can barely identify the CL pixels in its worst-case and completely misses the right half of the CL on the worst-case for U-Net. Similarly, U-Net suffers from high FNR, and low TPR for its worst-case completely misses the suitable CL pixels and shows higher FPR for the worst-case for UNet++.<fig id="Fig11"><label>Fig. 11</label><caption><p>Evaluation with Dice Score &amp; IoU Score: AM-UNet, U-Net, and UNet++</p></caption><graphic xlink:href="11042_2021_11568_Fig11_HTML" id="MO24"/></fig><fig id="Fig12"><label>Fig. 12</label><caption><p>Comparative Evaluation: Original and ROI Images, Ground Truth (white), AM-UNet (Green), U-Net (Red), and UNet++ (Blue)</p></caption><graphic xlink:href="11042_2021_11568_Fig12_HTML" id="MO25"/></fig><fig id="Fig13"><label>Fig. 13</label><caption><p>Comparative Analysis with Worst Cases of U-Net and UNet++</p></caption><graphic xlink:href="11042_2021_11568_Fig13_HTML" id="MO26"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Discussion</title>
    <p id="Par54">First, 3D segmentation modeling requires more complex modeling and much higher computational resources [<xref ref-type="bibr" rid="CR1">1</xref>], which in turn affects a modeling ability to carry out the training with large datasets. Some studies reported that 2D U-Net achieved better results than 3D U-Net’s in terms of accuracy, memory consumption, and training time [<xref ref-type="bibr" rid="CR33">33</xref>]. Targeting the 3D segmentation in AM-UNet, a series of multiple 2D segmentation models were designed instead of a 3D segmentation model. In AM-UNet, we utilized advanced pre and postprocessing image analysis techniques and the 2D segmentation model to create a fully automated end-to-end 3D segmentation network. As a result, we obtained a superior performance from AM-UNet compared to others. Our study showed that the simplicity of the AM-UNet model is sufficient to capture the spatial information along the third dimension, like 3D models. Furthermore, AM-UNet also can represent the 3D CL segmentation with a series of 2D segmentation models.</p>
    <p id="Par55">Second, compared to other subcortical brain structures that are often segmented manually (e.g., amygdala), CL is much harder to segment accurately due to the thin morphology and the limited spatial resolutions of MR images. In this study, our AM-UNet approach with optimized image analysis strategies solves the challenging problem of segmenting CL. The comparative evaluation demonstrates the AM-UNet’s ability for the CL segmentation. The results obtained in this research and the comparison with other works have shown the AM-UNet’s strengths. To the best of our knowledge, we achieved the state of the art results for automatic end-to-end 3D CL segmentation. The present study demonstrated that our AM-UNet approach has great potential for developing an automatic and accurate segmentation tool for very challenging medical imaging segmentation problems, such as CL segmentation.</p>
    <p id="Par56">Finally, there is still room to improve the AM-UNet model: We need an advanced method to overcome a challenging problem of the CL segmentation, such as the class imbalance problem (i.e., background pixels vs. CL pixels). We need more powerful ROI detection dealing with diverse MRI datasets. The deep learning networks could be extended for capturing contextual features of CL with advanced models, such as attention modeling or sequential modeling with LSTM. We need to consider a semi-supervised approach for CL segmentation through human and machine collaboration to overcome the lack of human-annotated data rather than supervised learning.</p>
  </sec>
  <sec id="Sec17">
    <title>Conclusion and future work</title>
    <p id="Par57">We proposed AM-UNet for CL segmentation of the human brain MRIs, consisting of preprocessing, optimal U-Net-based modeling, and postprocessing. AM-UNet produced the 3D CL segmentation maps for human brain MRIs effectively and efficiently. We developed and evaluated various deep learning models (U-Net variants) to automatically and accurately segment the CL from T1, T2, and T1T2 weighted MRI datasets. Among these models, we found that AM-UNet had the optimal performance and simplified architecture for 3D MRI segmentation. Our main contributions of this research are as follows: (i) designed a smaller, simpler, and efficient U-Net model architecture, (ii) developed an automated end-to-end 3D segmentation network for CL, (iii) produced multi-view segmentation models, and (iv) obtained the state-of-the-art results in the 3D CL segmentation. For automatic segmentation of 30 human brain MRIs (T1T2w) from the HCP dataset, the AM-UNet network has achieved superior segmentation performance with Dice, IoU, and ICC scores for the Axial view’s CL segmentation of 82%, 70%, and 90%, respectively. The 2D segmentation version of AM-UNet is publicly available on GitHub: <ext-link ext-link-type="uri" xlink:href="https://github.com/AhmedAlbishri/AM-UNET">https://github.com/AhmedAlbishri/AM-UNET</ext-link>.</p>
    <p id="Par58">In future work, we will enhance the AM-UNet approach for a 3D U-Net-based CL segmentation model. As a result, the segmentation model will be more flexible with various imaging datasets in a wide range of medical applications. Furthermore, the model will be extended to use segmentation maps for Axial + Coronal and Axial + Coronal + Sagittal views. We will also develop an interactive 3D segmentation tool for MRI and CT-Scan images in which experts can give immediate feedback during machine learning and pre/post-processing. In addition, this tool will provide advanced features, such as normalization, augmentation, transformation, visualization, optimization, and transfer learning for effective CL segmentation.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>Ahmed Awad Albishri and Syed Jawad Hussain Shah have contributed equally to this work.</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was partially supported by NSF CNS #1747751 awarded to Lee and NASARD Young Investigator Grant 25158 awarded to Kang. We appreciate Joseph Bodenheimer for his help with the MRI data used in the current study.</p>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Alalwan</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Abozeid</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>ElHabshy</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Alzahrani</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Efficient 3d deep learning modelfor medical image semantic segmentation</article-title>
        <source>Alex Eng J</source>
        <year>2021</year>
        <volume>60</volume>
        <issue>1</issue>
        <fpage>1231</fpage>
        <lpage>1239</lpage>
        <pub-id pub-id-type="doi">10.1016/j.aej.2020.10.046</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Albishri A, Shah SJ, Lee Y (2019) Cu-net: cascaded u-net model for automated liver and lesion segmentation and summarization. In: 2019 IEEE international conference on bioinformatics and biomedicine (IEEE BIBM 2019)</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <mixed-citation publication-type="other">Albishri AA, Shah SJH, Schmiedler A, Kang SS, Lee Y (2019) Automated human claustrum segmentation using deep learning technologies. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1911.07515">arXiv:1911.07515</ext-link></mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Asperti A, Mastronardo C (2017) The effectiveness of data augmentation for detection of gastrointestinal diseases from endoscopical images. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1712.03689">arXiv:1712.03689</ext-link></mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Badrinarayanan</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Kendall</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cipolla</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Segnet: a deep convolutional encoder-decoder architecture for image segmentation</article-title>
        <source>IEEE Trans Pattern Anal Machine Intell</source>
        <year>2017</year>
        <volume>39</volume>
        <issue>12</issue>
        <fpage>2481</fpage>
        <lpage>2495</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2644615</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bae</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>Hyun</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Byeon</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Shin</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Cho</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>YJ</given-names>
          </name>
          <name>
            <surname>Yi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kuh</surname>
            <given-names>SU</given-names>
          </name>
          <name>
            <surname>Yeom</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Fully automated 3d segmentation and separation of multiple cervical vertebrae in ct images using a 2d convolutional neural network</article-title>
        <source>Comput Methods Programs Biomed</source>
        <year>2020</year>
        <volume>184</volume>
        <fpage>105119</fpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2019.105119</pub-id>
        <pub-id pub-id-type="pmid">31627152</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Benjelloun</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mahmoudi</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Spine localization in x-ray images using interest point detection</article-title>
        <source>J Digital Imaging</source>
        <year>2009</year>
        <volume>22</volume>
        <issue>3</issue>
        <fpage>309</fpage>
        <lpage>318</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-007-9099-3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bernstein</surname>
            <given-names>HG</given-names>
          </name>
          <name>
            <surname>Ortmann</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dobrowolny</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Steiner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Brisch</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gos</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Bogerts</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Bilaterally reduced claustral volumes in schizophrenia and major depressive disorder: a morphometric postmortem study</article-title>
        <source>European Archives of Psychiatry and Clinical Neuroscience</source>
        <year>2016</year>
        <volume>266</volume>
        <issue>1</issue>
        <fpage>25</fpage>
        <lpage>33</lpage>
        <pub-id pub-id-type="doi">10.1007/s00406-015-0597-x</pub-id>
        <pub-id pub-id-type="pmid">25822416</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <mixed-citation publication-type="other">Castro E, Cardoso JS, Pereira JC (2018) Elastic deformations for data augmentation in breast cancer mass detection. In: 2018 IEEE EMBS international conference on biomedical &amp; health informatics (BHI). IEEE, pp 230–234</mixed-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Christ PF, Ettlinger F, Grün F, Elshaera MEA, Lipkova J, Schlecht S, Ahmaddy F, Tatavarty S, Bickel M, Bilic P, et al (2017) Automatic liver and tumor segmentation of ct and mri volumes using cascaded fully convolutional neural networks. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1702.05970">arXiv:1702.05970</ext-link></mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <mixed-citation publication-type="other">Connectomdb. <ext-link ext-link-type="uri" xlink:href="https://db.humanconnectome.org/">https://db.humanconnectome.org/</ext-link>. [Online; Accessed 17 Nov 2019]
</mixed-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Crick</surname>
            <given-names>FC</given-names>
          </name>
          <name>
            <surname>Koch</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>What is the function of the claustrum?</article-title>
        <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>
        <year>2005</year>
        <volume>360</volume>
        <issue>1458</issue>
        <fpage>1271</fpage>
        <lpage>1279</lpage>
        <pub-id pub-id-type="doi">10.1098/rstb.2005.1661</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <mixed-citation publication-type="other">Cruz-Roa AA, Ovalle JEA, Madabhushi A, Osorio FAG (2013) A deep learning architecture for image representation, visual interpretability and automated basal-cell carcinoma cancer detection. In: International conference on medical image computing and computer-assisted intervention. Springer, pp 403–410</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <mixed-citation publication-type="other">Davis WB (2008) The claustrum in autism and typically developing male children: A quanti-tative mri study. Brigham Young University-Provo</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>SL</given-names>
          </name>
          <name>
            <surname>Royall</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Sunkin</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Facer</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Lesnar</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Guillozet-Bongaarts</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>McMurray</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Szafer</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dolbeare</surname>
            <given-names>TA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comprehensive cellular-resolution atlas of the adult human brain</article-title>
        <source>Journal of Comparative Neurology</source>
        <year>2016</year>
        <volume>524</volume>
        <issue>16</issue>
        <fpage>3127</fpage>
        <lpage>3481</lpage>
        <pub-id pub-id-type="doi">10.1002/cne.24080</pub-id>
        <pub-id pub-id-type="pmid">27418273</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>SL</given-names>
          </name>
          <name>
            <surname>Royall</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Sunkin</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Facer</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Lesnar</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Guillozet-Bongaarts</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>McMurray</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Szafer</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dolbeare</surname>
            <given-names>TA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comprehensive cellular-resolution atlas of the adult human brain</article-title>
        <source>The Journal of Comparative Neurology</source>
        <year>2017</year>
        <volume>525</volume>
        <issue>2</issue>
        <fpage>407</fpage>
        <pub-id pub-id-type="doi">10.1002/cne.24130</pub-id>
        <pub-id pub-id-type="pmid">27917481</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dou</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Qin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>PA</given-names>
          </name>
        </person-group>
        <article-title>3d deeply supervised network for automated segmentation of volumetric medical images</article-title>
        <source>Medical Image Analysis</source>
        <year>2017</year>
        <volume>41</volume>
        <fpage>40</fpage>
        <lpage>54</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2017.05.001</pub-id>
        <pub-id pub-id-type="pmid">28526212</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Duncan</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Ayache</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Medical image analysis: progress over two decades and the challenges ahead</article-title>
        <source>IEEE Trans Pattern Anal Machine Intell</source>
        <year>2000</year>
        <volume>22</volume>
        <issue>1</issue>
        <fpage>85</fpage>
        <lpage>106</lpage>
        <pub-id pub-id-type="doi">10.1109/34.824822</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>DP</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>GP</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shao</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Inf-net: automatic covid-19 lung infection segmentation from ct images</article-title>
        <source>IEEE Trans Med Imaging</source>
        <year>2020</year>
        <volume>39</volume>
        <issue>8</issue>
        <fpage>2626</fpage>
        <lpage>2637</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2020.2996645</pub-id>
        <pub-id pub-id-type="pmid">32730213</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Fischl B (2012) Freesurfer. Neuroimage 62(2):774–781 </mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Glorot X, Bordes A, Bengio Y (2011) Deep sparse rectifier neural networks. In: Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp 315–323</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goll</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Atlan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Citri</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Attention: the claustrum</article-title>
        <source>Trends in Neurosciences</source>
        <year>2015</year>
        <volume>38</volume>
        <issue>8</issue>
        <fpage>486</fpage>
        <lpage>495</lpage>
        <pub-id pub-id-type="doi">10.1016/j.tins.2015.05.006</pub-id>
        <pub-id pub-id-type="pmid">26116988</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Havaei</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Davy</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Warde-Farley</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Biard</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Courville</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Pal</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Jodoin</surname>
            <given-names>PM</given-names>
          </name>
          <name>
            <surname>Larochelle</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Brain tumor segmentation with deep neural networks</article-title>
        <source>Medical Image Analysis</source>
        <year>2017</year>
        <volume>35</volume>
        <fpage>18</fpage>
        <lpage>31</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2016.05.004</pub-id>
        <pub-id pub-id-type="pmid">27310171</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ho</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wookey</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>The real-world-weight cross-entropy loss function: modeling the costs of mislabeling</article-title>
        <source>IEEE Access</source>
        <year>2019</year>
        <volume>8</volume>
        <fpage>4806</fpage>
        <lpage>4813</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2962617</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Huang H, Lin L, Tong R, Hu H, Zhang Q, Iwamoto Y, Han X, Chen YW, Wu J (2020) Unet 3+: a full-scale connected unet for medical image segmentation. In: ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, pp 1055–1059</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <mixed-citation publication-type="other">Ioffe S, Szegedy C (2015) Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1502.03167">arXiv:1502.03167</ext-link></mixed-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Kang SS, Bodenheimer J, Butler T (2020) A comprehensive protocol for manual segmentation of the human claustrum and its sub-regions using high-resolution mri. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2010.06423">arXiv:2010.06423</ext-link></mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Kayalibay B, Jensen G, van der Smagt P (2017) Cnn-based segmentation of medical imaging data. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1701.03056">arXiv:1701.03056</ext-link></mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Kushibar K, Valverde S, González-Villà S, Bernal J, Cabezas M, Oliver A, Lladó X (2017) Automated sub-cortical brain structure segmentation combining spatial and deep convolutional features. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1709.09075">arXiv:1709.09075</ext-link></mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Larobina</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Murino</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Medical image file formats</article-title>
        <source>Journal of Digital Imaging</source>
        <year>2014</year>
        <volume>27</volume>
        <issue>2</issue>
        <fpage>200</fpage>
        <lpage>206</lpage>
        <pub-id pub-id-type="doi">10.1007/s10278-013-9657-9</pub-id>
        <pub-id pub-id-type="pmid">24338090</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Scholar</surname>
            <given-names>Google</given-names>
          </name>
        </person-group>
        <article-title>Deep learning</article-title>
        <source>Nature</source>
        <year>2015</year>
        <volume>521</volume>
        <issue>7553</issue>
        <fpage>436</fpage>
        <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
        <pub-id pub-id-type="pmid">26017442</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Li H, Menegaux A, Shit S, Schmitz-Koep B, Sorg C, Menze B, Hedderich D, et al (2020) Complex grey matter structure segmentation in brains via deep learning: example of the claustrum. arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2008.03465">arXiv:2008.03465</ext-link></mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Fu</surname>
            <given-names>CW</given-names>
          </name>
          <name>
            <surname>Heng</surname>
            <given-names>PA</given-names>
          </name>
        </person-group>
        <article-title>H-denseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <issue>12</issue>
        <fpage>2663</fpage>
        <lpage>2674</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2018.2845918</pub-id>
        <pub-id pub-id-type="pmid">29994201</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <mixed-citation publication-type="other">Long J, Shelhamer E, Darrell T (2015) Fully convolutional networks for semantic segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition, pp 3431–3440</mixed-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathur</surname>
            <given-names>BN</given-names>
          </name>
        </person-group>
        <article-title>The claustrum in review</article-title>
        <source>Frontiers in Systems Neuroscience</source>
        <year>2014</year>
        <volume>8</volume>
        <fpage>48</fpage>
        <pub-id pub-id-type="doi">10.3389/fnsys.2014.00048</pub-id>
        <pub-id pub-id-type="pmid">24772070</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meletti</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Slonkova</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mareckova</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Monti</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Specchio</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Hon</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Giovannini</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Marcian</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Chiari</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Krupa</surname>
            <given-names>P</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Claustrum damage and refractory status epilepticus following febrile illness</article-title>
        <source>Neurology</source>
        <year>2015</year>
        <volume>85</volume>
        <issue>14</issue>
        <fpage>1224</fpage>
        <lpage>1232</lpage>
        <pub-id pub-id-type="doi">10.1212/WNL.0000000000001996</pub-id>
        <pub-id pub-id-type="pmid">26341869</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Misaki</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Savitz</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zotev</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Phillips</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Yuan</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Young</surname>
            <given-names>KD</given-names>
          </name>
          <name>
            <surname>Drevets</surname>
            <given-names>WC</given-names>
          </name>
          <name>
            <surname>Bodurka</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Contrast enhancement by combining t 1-and t 2-weighted structural brain mr images</article-title>
        <source>Magnetic Resonance in Medicine</source>
        <year>2015</year>
        <volume>74</volume>
        <issue>6</issue>
        <fpage>1609</fpage>
        <lpage>1620</lpage>
        <pub-id pub-id-type="doi">10.1002/mrm.25560</pub-id>
        <pub-id pub-id-type="pmid">25533337</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moeskops</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Viergever</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Mendrik</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>de Vries</surname>
            <given-names>LS</given-names>
          </name>
          <name>
            <surname>Benders</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Išgum</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Automatic segmentation of mr brain images with a convolutional neural network</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2016</year>
        <volume>35</volume>
        <issue>5</issue>
        <fpage>1252</fpage>
        <lpage>1261</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2016.2548501</pub-id>
        <pub-id pub-id-type="pmid">27046893</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mohamad</surname>
            <given-names>IB</given-names>
          </name>
          <name>
            <surname>Usman</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Standardization and its effects on k-means clustering algorithm</article-title>
        <source>Research Journal of Applied Sciences, Engineering and Technology</source>
        <year>2013</year>
        <volume>6</volume>
        <issue>17</issue>
        <fpage>3299</fpage>
        <lpage>3303</lpage>
        <pub-id pub-id-type="doi">10.19026/rjaset.6.3638</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Ren S, He K, Girshick R, Sun J (2015) Faster r-cnn: towards real-time object detection with region proposal networks. In: Advances in neural information processing systems, pp 91–99</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Ronneberger O, Fischer P, Brox T (2015) U-net: convolutional networks for biomedical image segmentation. In: International conference on medical image computing and computer-assisted intervention. Springer, pp 234–241</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shrout</surname>
            <given-names>PE</given-names>
          </name>
          <name>
            <surname>Fleiss</surname>
            <given-names>JL</given-names>
          </name>
        </person-group>
        <article-title>Intraclass correlations: uses in assessing rater reliability</article-title>
        <source>Psychological Bulletin</source>
        <year>1979</year>
        <volume>86</volume>
        <issue>2</issue>
        <fpage>420</fpage>
        <pub-id pub-id-type="doi">10.1037/0033-2909.86.2.420</pub-id>
        <pub-id pub-id-type="pmid">18839484</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Silva</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Jacob</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Melo</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Alves</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Costa</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Claustrum sign in a child with refractory status epilepticus after febrile illness: why does it happen?</article-title>
        <source>Acta Neurologica Belgica</source>
        <year>2018</year>
        <volume>118</volume>
        <issue>2</issue>
        <fpage>303</fpage>
        <lpage>305</lpage>
        <pub-id pub-id-type="doi">10.1007/s13760-017-0820-9</pub-id>
        <pub-id pub-id-type="pmid">28741106</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other"> Smythies JR, Edelstein LR, Ramachandran VS (2014) Hypotheses relating to the functionof the claustrum. In: The Claustrum, pp 299–352. Elsevier </mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Torgerson</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Irimia</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Goh</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Van Horn</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>The dti connectivity of the human claustrum</article-title>
        <source>Human Brain Mapping</source>
        <year>2015</year>
        <volume>36</volume>
        <issue>3</issue>
        <fpage>827</fpage>
        <lpage>838</lpage>
        <pub-id pub-id-type="doi">10.1002/hbm.22667</pub-id>
        <pub-id pub-id-type="pmid">25339630</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Essen</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Barch</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Behrens</surname>
            <given-names>TE</given-names>
          </name>
          <name>
            <surname>Yacoub</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Ugurbil</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Consortium</surname>
            <given-names>WMH</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The wu-minn human connectome project: an overview</article-title>
        <source>Neuroimage</source>
        <year>2013</year>
        <volume>80</volume>
        <fpage>62</fpage>
        <lpage>79</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.041</pub-id>
        <pub-id pub-id-type="pmid">23684880</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <mixed-citation publication-type="other">Wan L, Zeiler M, Zhang S, Le Cun Y, Fergus R (2013) Regularization of neural networks using dropconnect. In: International conference on machine learning, pp 1058–1066</mixed-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wegiel</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Flory</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kuchna</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Nowicki</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>SY</given-names>
          </name>
          <name>
            <surname>Imaki</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wegiel</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Frackowiak</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kolecka</surname>
            <given-names>BM</given-names>
          </name>
          <name>
            <surname>Wierzba-Bobrowicz</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Neuronal nucleus and cytoplasm volume deficit in children with autism and volume increase in adolescents and adults</article-title>
        <source>Acta Neuropathologica Communications</source>
        <year>2015</year>
        <volume>3</volume>
        <issue>1</issue>
        <fpage>2</fpage>
        <pub-id pub-id-type="doi">10.1186/s40478-015-0183-5</pub-id>
        <pub-id pub-id-type="pmid">25595448</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Woo</surname>
            <given-names>CW</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>LJ</given-names>
          </name>
          <name>
            <surname>Lindquist</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Wager</surname>
            <given-names>TD</given-names>
          </name>
        </person-group>
        <article-title>Building better biomarkers: brain models in translational neuroimaging</article-title>
        <source>Nature Neuroscience</source>
        <year>2017</year>
        <volume>20</volume>
        <issue>3</issue>
        <fpage>365</fpage>
        <pub-id pub-id-type="doi">10.1038/nn.4478</pub-id>
        <pub-id pub-id-type="pmid">28230847</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">WU-Minn H (2017) 1200 subjects data release reference manual. <ext-link ext-link-type="uri" xlink:href="https://www.humanconnectome.org">https://www.humanconnectome.org</ext-link></mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yamanakkanavar</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>JY</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Mri segmentation and classification of human brain using deep learning for diagnosis of alzheimer’s disease: a survey</article-title>
        <source>Sensors</source>
        <year>2020</year>
        <volume>20</volume>
        <issue>11</issue>
        <fpage>3243</fpage>
        <pub-id pub-id-type="doi">10.3390/s20113243</pub-id>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhou</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Siddiquee</surname>
            <given-names>MMR</given-names>
          </name>
          <name>
            <surname>Tajbakhsh</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Unet++: redesigning skip connections to exploit multiscale features in image segmentation</article-title>
        <source>IEEE Transactions on Medical Imaging</source>
        <year>2019</year>
        <volume>39</volume>
        <issue>6</issue>
        <fpage>1856</fpage>
        <lpage>1867</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2019.2959609</pub-id>
        <pub-id pub-id-type="pmid">31841402</pub-id>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">Zhu H (2003) Medical image processing overview. University of Calgary, pp 1–27</mixed-citation>
    </ref>
  </ref-list>
</back>
