<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.0 20120330//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.0?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Genome Res</journal-id>
    <journal-id journal-id-type="iso-abbrev">Genome Res</journal-id>
    <journal-id journal-id-type="hwp">genome</journal-id>
    <journal-id journal-id-type="publisher-id">GENOME</journal-id>
    <journal-title-group>
      <journal-title>Genome Research</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1088-9051</issn>
    <issn pub-type="epub">1549-5469</issn>
    <publisher>
      <publisher-name>Cold Spring Harbor Laboratory Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8494222</article-id>
    <article-id pub-id-type="pmid">33627475</article-id>
    <article-id pub-id-type="medline">9509184</article-id>
    <article-id pub-id-type="doi">10.1101/gr.268581.120</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Method</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Semisupervised adversarial neural networks for single-cell classification</article-title>
      <alt-title alt-title-type="left-running">Kimmel and Kelley</alt-title>
      <alt-title alt-title-type="right-running">Semisupervised single-cell classification</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-8285-619X</contrib-id>
        <name>
          <surname>Kimmel</surname>
          <given-names>Jacob C.</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-7782-3548</contrib-id>
        <name>
          <surname>Kelley</surname>
          <given-names>David R.</given-names>
        </name>
      </contrib>
    </contrib-group>
    <aff>Calico Life Sciences, LLC, South San Francisco, California 94080, USA</aff>
    <author-notes>
      <corresp>Corresponding authors: <email>jacob@calicolabs.com</email>, <email>drk@calicolabs.com</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <volume>31</volume>
    <issue>10</issue>
    <fpage>1781</fpage>
    <lpage>1793</lpage>
    <history>
      <date date-type="received">
        <day>30</day>
        <month>7</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>2</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>
        <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/site/misc/terms.xhtml" ext-link-type="uri">© 2021 Kimmel and Kelley; Published by Cold Spring Harbor Laboratory Press</ext-link>
      </copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref>
        <license-p>This article, published in <italic toggle="yes">Genome Research</italic>, is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), as described at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pdf" xlink:href="1781.pdf"/>
    <abstract>
      <p>Annotating cell identities is a common bottleneck in the analysis of single-cell genomics experiments. Here, we present scNym, a semisupervised, adversarial neural network that learns to transfer cell identity annotations from one experiment to another. scNym takes advantage of information in both labeled data sets and new, unlabeled data sets to learn rich representations of cell identity that enable effective annotation transfer. We show that scNym effectively transfers annotations across experiments despite biological and technical differences, achieving performance superior to existing methods. We also show that scNym models can synthesize information from multiple training and target data sets to improve performance. We show that in addition to high accuracy, scNym models are well calibrated and interpretable with saliency methods.</p>
    </abstract>
    <funding-group>
      <award-group id="funding-1">
        <funding-source>Calico Life Sciences, LLC</funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="13"/>
    </counts>
  </article-meta>
</front>
<body>
  <p>Single-cell genomics allows for simultaneous molecular profiling of thousands of diverse cells and has advanced our understanding of development (<xref rid="GR268581KIMC43" ref-type="bibr">Trapnell 2015)</xref>, aging (<xref rid="GR268581KIMC4" ref-type="bibr">Angelidis et al. 2019</xref>; <xref rid="GR268581KIMC20" ref-type="bibr">Kimmel et al. 2019</xref>; <xref rid="GR268581KIMC26" ref-type="bibr">Ma et al. 2020)</xref>, and disease (<xref rid="GR268581KIMC40" ref-type="bibr">Tanay and Regev 2017)</xref>. To derive biological insight from these data, each single-cell molecular profile must be annotated with a cell identity, such as a cell type or state label. Traditionally, this task has been performed manually by domain expert biologists. Manual annotation is time-consuming, somewhat subjective, and error prone. Annotations influence the results of nearly all downstream analyses, motivating more robust algorithmic approaches for cell type annotation.</p>
  <p>Automated classification tools have been proposed to transfer annotations across data sets (<xref rid="GR268581KIMC22" ref-type="bibr">Kiselev et al. 2018</xref>; <xref rid="GR268581KIMC1" ref-type="bibr">Abdelaal et al. 2019</xref>; <xref rid="GR268581KIMC2" ref-type="bibr">Alquicira-Hernandez et al. 2019</xref>; <xref rid="GR268581KIMC8" ref-type="bibr">de Kanter et al. 2019</xref>; <xref rid="GR268581KIMC30" ref-type="bibr">Pliner et al. 2019</xref>; <xref rid="GR268581KIMC39" ref-type="bibr">Tan and Cahan 2019</xref>; <xref rid="GR268581KIMC49" ref-type="bibr">Zhang et al. 2019)</xref>. These existing tools learn relationships between cell identity and molecular features from a training set with existing labels without considering the unlabeled target data set in the learning process. However, results from the field of semisupervised representation learning suggest that incorporating information from the target data during training can improve the performance of prediction models (<xref rid="GR268581KIMC21" ref-type="bibr">Kingma et al. 2014</xref>; <xref rid="GR268581KIMC28" ref-type="bibr">Oliver et al. 2018</xref>; <xref rid="GR268581KIMC6" ref-type="bibr">Berthelot et al. 2019</xref>; <xref rid="GR268581KIMC44" ref-type="bibr">Verma et al. 2019</xref>). This approach is especially beneficial when there are systematic differences—a domain shift—between the training and target data sets. Domain shifts are commonly introduced between single-cell genomics experiments when cells are profiled in different experimental conditions or with different sequencing technologies.</p>
  <p>A growing family of representation learning techniques encourages classification models to provide consistent interpolations between data points as an auxiliary training task to improve performance (<xref rid="GR268581KIMC6" ref-type="bibr">Berthelot et al. 2019</xref>; <xref rid="GR268581KIMC44" ref-type="bibr">Verma et al. 2019)</xref>. In the semisupervised setting, the MixMatch approach implements this idea by “mixing” observations and their labels with simple weighted averages. Mixed observations from the training and target data sets form a bridge in feature space, encouraging the model to learn a smooth interpolation across the domains. Another family of techniques seeks to improve classification performance in the presence of domain shifts by encouraging the model to learn a representation in which observations from different domains are embedded nearby, rather than occupying distinct regions of a latent space (<xref rid="GR268581KIMC45" ref-type="bibr">Wilson and Cook 2020)</xref>. One successful approach uses a “domain adversary” to encourage the classification model to learn a representation that is invariant to data set–specific features (<xref rid="GR268581KIMC13" ref-type="bibr">Ganin et al. 2016)</xref>. Both interpolation consistency and domain invariance are desirable in the single-cell genomic setting, in which domain shifts are common and complex gene expression boundaries separate cell types.</p>
  <p>Here, we introduce a cell type classification model that uses semisupervised and adversarial machine learning techniques to take advantage of both labeled and unlabeled single-cell data sets. We show that this model offers superior performance to existing methods and effectively transfers annotations across different animal ages, perturbation conditions, and sequencing technologies. Additionally, we show that our model learns biologically interpretable representations and offers well-calibrated metrics of annotation confidence that can be used to make new cell type discoveries.</p>
  <sec sec-type="results" id="s1">
    <title>Results</title>
    <sec id="s1a">
      <title>scNym</title>
      <p>In the typical supervised learning framework, the model touches the target unlabeled data set to predict labels only after training has concluded. In contrast, our semisupervised learning framework trains the model parameters on both the labeled and unlabeled data in order to leverage the structure in the target data set, the measurements of which may have been influenced by myriad sources of biological and technical bias and batch effects. Although our model uses observed cell profiles from the unlabeled target data set, at no point does the model access ground truth labels for the target data. Ground truth labels on the target data set are used exclusively to evaluate model performance. Some single-cell classification methods require manual marker gene specification before model training. scNym requires no prior manual specification of marker genes but rather learns relevant gene expression features from the data.</p>
      <p>scNym uses the unlabeled target data through a combination of MixMatch semisupervision (<xref rid="GR268581KIMC6" ref-type="bibr">Berthelot et al. 2019)</xref> and by training a domain adversary (<xref rid="GR268581KIMC13" ref-type="bibr">Ganin et al. 2016)</xref> in an iterative learning process (Methods) (<xref rid="GR268581KIMF1" ref-type="fig">Fig. 1</xref>A). The MixMatch semisupervision approach combines MixUp data augmentations (<xref rid="GR268581KIMC48" ref-type="bibr">Zhang et al. 2018</xref>; <xref rid="GR268581KIMC41" ref-type="bibr">Thulasidasan et al. 2019)</xref> with pseudolabeling of the target data (<xref rid="GR268581KIMC24" ref-type="bibr">Lee 2013</xref>; <xref rid="GR268581KIMC44" ref-type="bibr">Verma et al. 2019)</xref> to improve generalization across the training and target domains. At each training iteration, we “pseudolabel” unlabeled cells using predictions from the classification model and then augment each cell profile using a biased weighted average of gene expression and labels with another randomly chosen cell (<xref rid="GR268581KIMF1" ref-type="fig">Fig. 1</xref>B). The resulting mixed profiles are dominated by a single cell, adjusted modestly to more closely resemble another. As part of MixMatch, we mix profiles across the training and unlabeled data so that some of the resulting mixed profiles are interpolations between the two data sets. We fit the model parameters to minimize cell type classification error on these mixed profiles, encouraging the model to learn a general representation that allows for interpolation between observed cell states.</p>
      <fig position="float" id="GR268581KIMF1">
        <label>Figure 1.</label>
        <caption>
          <p>scNym combines semisupervised and adversarial training to learn performant single-cell classifiers. (<italic toggle="yes">A</italic>) scNym takes advantage of target data during training by estimating “pseudolabels” for each target data point using model predictions. Training and target cell profiles and their labels are then augmented using weighted averages in the MixMatch procedure. An adversary is also trained to discriminate training and target observations. We train model parameters using a combination of supervised classification, interpolation consistency, and adversarial objectives. Here, we use <italic toggle="yes">H</italic>(·,·) to represent the cross-entropy function. (<italic toggle="yes">B</italic>) Training and target cell profiles are separated by a domain shift in gene expression space. scNym pseudolabels target profiles and generates mixed cell profiles (arrows) by randomly pairing cells. Mixed profiles form a bridge between training and target data sets. (<italic toggle="yes">C</italic>) scNym models learn a discriminative representation of cell state in a hidden embedding layer. Train and target cell profiles initially segregate in this representation. During training, adversarial gradients (colored arrows) encourage cells of the same type to mix in the scNym embedding.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="1781f01" position="float"/>
      </fig>
      <p>The scNym classifier learns a representation of cell identity in the hidden neural network layers where cell types are linearly separable. Alongside, we train an adversarial model to predict the domain of origin for each cell (e.g., training set, target set) from this learned embedding. We train the scNym classifier to compete with this adversary, updating the classifier's embedding to make domain prediction more difficult. At each iteration, the adversary's gradients highlight features in the embedding that discriminate the different domains. We update the scNym classifier using the inverse of the adversarial gradients, reducing the amount of domain-specific information in the embedding as training progresses. This adversarial training procedure encourages the classification model to learn a domain-adapted embedding of the training and target data sets that improves classification performance (<xref rid="GR268581KIMF1" ref-type="fig">Fig. 1</xref>C). In inference mode, scNym predictions provide a probability distribution across all cell types in the training set for each target cell.</p>
    </sec>
    <sec id="s1b">
      <title>scNym transfers cell annotations across biological conditions</title>
      <p>We evaluated the performance of scNym, transferring cell identity annotations in 11 distinct tasks. These tasks were chosen to capture diverse kinds of technological and biological variation that complicate annotation transfer. Each task represents a true cell type transfer across different experiments, in contrast to some efforts that report within-experiment hold-out accuracy.</p>
      <p>We first evaluated cell type annotation transfer between animals of different ages. We trained scNym models on cells from young rats (5 mo old) from the Rat Aging Cell Atlas (<xref rid="GR268581KIMC26" ref-type="bibr">Ma et al. 2020)</xref> and predicted on cells from aged rats (27 mo old) (<xref rid="GR268581KIMF2" ref-type="fig">Fig. 2</xref>A; Methods). We found that predictions from our scNym model trained on young cells largely matched the ground truth annotations (92.2% accurate) on aged cells (<xref rid="GR268581KIMF2" ref-type="fig">Fig. 2</xref>B,C).</p>
      <fig position="float" id="GR268581KIMF2">
        <label>Figure 2.</label>
        <caption>
          <p>scNym transfers cell identity annotations between young and aged rat cells. (<italic toggle="yes">A</italic>) Young and aged cells from a rat aging cell atlas displayed in a UMAP projection (<xref rid="GR268581KIMC26" ref-type="bibr">Ma et al. 2020)</xref>. Some cell types show a domain shift between young and aged cells. scNym models were trained on young cells in the atlas and used to predict labels for aged cells. (<italic toggle="yes">B</italic>) Ground truth cell type annotations for the aged cells of the Rat Aging Cell Atlas shown in a UMAP projection. (<italic toggle="yes">C</italic>) scNym predicted cell types in the target aged cells. scNym predictions match ground truth annotation in the majority (&gt;90%) of cases. (<italic toggle="yes">D</italic>) Accuracy (<italic toggle="yes">left</italic>) and <italic toggle="yes">κ</italic>-scores (<italic toggle="yes">right</italic>) for scNym and other state of the art classification models. scNym yields significantly greater accuracy and <italic toggle="yes">κ</italic>-scores than baseline methods (<italic toggle="yes">P</italic> &lt; 0.01, Wilcoxon rank sums). Note that multiple existing methods could not complete this large task. (<italic toggle="yes">E</italic>) Aged skeletal muscle cells labeled with ground truth annotations (<italic toggle="yes">left</italic>) and the relative accuracy of scNym and scmap-cell (<italic toggle="yes">right</italic>) projected with UMAP. scNym accurately predicts multiple cell types that are confused by scmap-cell (arrows).</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="1781f02" position="float"/>
      </fig>
      <p>We compared scNym performance on this task to state-of-the-art single-cell identity annotation methods (<xref rid="GR268581KIMC22" ref-type="bibr">Kiselev et al. 2018</xref>; <xref rid="GR268581KIMC1" ref-type="bibr">Abdelaal et al. 2019</xref>; <xref rid="GR268581KIMC2" ref-type="bibr">Alquicira-Hernandez et al. 2019</xref>; <xref rid="GR268581KIMC8" ref-type="bibr">de Kanter et al. 2019</xref>; <xref rid="GR268581KIMC39" ref-type="bibr">Tan and Cahan 2019</xref>. We also compared scNym to state-of-the-art unsupervised data harmonization methods (<xref rid="GR268581KIMC23" ref-type="bibr">Korsunsky et al. 2019</xref>; <xref rid="GR268581KIMC34" ref-type="bibr">Stuart et al. 2019</xref>; <xref rid="GR268581KIMC42" ref-type="bibr">Tran et al. 2020</xref>; <xref rid="GR268581KIMC46" ref-type="bibr">Xu et al. 2021</xref>) followed by supervised classification with a support vector machine, for a total of 10 baseline approaches (Methods). scNym produced significantly improved labels over these methods, some of which could not complete this large task on our hardware (256 GB RAM; Wilcoxon rank sums on accuracy or <italic toggle="yes">κ</italic>-scores, <italic toggle="yes">P</italic> &lt; 0.01) (<xref rid="GR268581KIMF2" ref-type="fig">Fig. 2</xref>D; <xref rid="GR268581KIMTB1" ref-type="table">Table 1</xref>). scNym runtimes were competitive with baseline methods (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S1</ext-link>). We found that some of the largest differences in accuracy between scNym and the commonly used scmap-cell method were in the skeletal muscle. scNym models accurately classified multiple cell types in the muscle that were confused by scmap-cell (<xref rid="GR268581KIMF2" ref-type="fig">Fig. 2</xref>E), showing that the increased accuracy of scNym is meaningful for downstream analyses.</p>
      <table-wrap position="float" id="GR268581KIMTB1">
        <label>Table 1.</label>
        <caption>
          <p>Comparison of model performance across tasks</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="1781tb01" position="float"/>
      </table-wrap>
      <p>We next tested the ability of scNym to classify cell identities after perturbation. We trained on unstimulated human peripheral blood mononuclear cells (PBMCs) and predicted on PBMCs after stimulation with IFNB1 (<xref rid="GR268581KIMF3" ref-type="fig">Fig. 3</xref>A; <xref rid="GR268581KIMC19" ref-type="bibr">Kang et al. 2018</xref>). scNym achieved high accuracy (&gt;91%), superior to baseline methods (<xref rid="GR268581KIMF3" ref-type="fig">Fig. 3</xref>C; <xref rid="GR268581KIMTB1" ref-type="table">Table 1</xref>). The common scmap-cluster method frequently confused monocyte subtypes, whereas scNym did not (<xref rid="GR268581KIMF3" ref-type="fig">Fig. 3</xref>B).</p>
      <fig position="float" id="GR268581KIMF3">
        <label>Figure 3.</label>
        <caption>
          <p>scNym transfers annotations from unstimulated immune cells to stimulated immune cells. (<italic toggle="yes">A</italic>) UMAP projection of unstimulated PBMC training data and stimulated PBMC target data with stimulation condition labels. (<italic toggle="yes">B</italic>) UMAP projections of ground truth cell type labels (<italic toggle="yes">left</italic>), scmap-cluster predictions (<italic toggle="yes">center</italic>), and scNym predictions for both CD14<sup>+</sup> and FCGR3A<sup>+</sup> monocytes. scmap-cluster confuses these populations (arrow). (<italic toggle="yes">C</italic>) Classification accuracy for scNym and baseline cell identity classification methods. scNym is significantly more accurate than other approaches (<italic toggle="yes">P</italic> &lt; 0.01, Wilcoxon rank sums). (<italic toggle="yes">D</italic>) Integrated gradient analysis reveals genes that drive correct classification decisions. We recover known marker genes of many cell types (e.g., <italic toggle="yes">CD79A</italic> for B cells, <italic toggle="yes">PPBP</italic> for megakaryocytes). (<italic toggle="yes">E</italic>) Cell type specificity of the top salient genes in a UMAP projection of gene expression (log normalized counts per million). (<italic toggle="yes">F</italic>) Integrated gradient analysis reveals genes that drive incorrect classification of some FCGR3A<sup>+</sup> monocytes as CD14<sup>+</sup> monocytes. Several of the top 15 salient genes for misclassification are CD14<sup>+</sup> markers that are up-regulated in incorrectly classified FCGR3A<sup>+</sup> cells.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="1781f03" position="float"/>
      </fig>
      <p>Cross-species annotation transfer is another context in which distinct biology creates a domain shift across training and target domains. To evaluate if scNym could transfer labels across species, we trained on mouse cells with either rat or human cells as target data and observed high performance (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S2</ext-link>).</p>
    </sec>
    <sec id="s1c">
      <title>scNym models learn biologically meaningful cell type representations</title>
      <p>To interpret the classification decisions of our scNym models, we developed integrated gradient analysis tools to identify genes that influence model decisions (Methods) (<xref rid="GR268581KIMC35" ref-type="bibr">Sundararajan et al. 2017</xref>). The integrated gradient method attributes the prediction of a deep network to its input features while satisfying desirable axioms of interpretability that simpler methods like raw gradients do not. For the PBMC cross-stimulation task, we found that salient genes included known markers of specific cell types such as <italic toggle="yes">CD79A</italic> for B cells and <italic toggle="yes">GNLY</italic> for NK cells. Integrated gradient analysis also revealed specific cell type marker genes that may not have been selected a priori, such as <italic toggle="yes">NCOA4</italic> for megakaryocytes (<xref rid="GR268581KIMF3" ref-type="fig">Fig. 3</xref>D,E; <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S3</ext-link>). We also performed integrated gradient analysis for a cross-technology mouse cell atlas experiment (described below) and found that marker genes chosen using scNym-integrated gradients were superior to markers chosen using SVM feature importance scores based on Gene Ontology enrichment (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S4</ext-link>). These results suggest that our models learned biologically meaningful representations that are more generalizable to unseen cell profiles, regardless of condition or technology.</p>
      <p>We also used integrated gradient analysis to understand why the scNym model misclassified some FCGR3A<sup>+</sup> monocytes as CD14<sup>+</sup> monocytes in the PBMC cross-stimulation task (Methods). This analysis revealed genes driving these incorrect classifications, including some CD14<sup>+</sup> monocyte marker genes that are elevated in a subset of FCGR3A<sup>+</sup> monocytes (<xref rid="GR268581KIMF3" ref-type="fig">Fig. 3</xref>F). Domain experts may use integrated gradient analysis to understand and review model decisions for ambiguous cells.</p>
    </sec>
    <sec id="s1d">
      <title>scNym transfers annotations across single-cell sequencing technologies</title>
      <p>To evaluate the ability of scNym to transfer labels across different experimental technologies, we trained on single-cell profiles from 10 mouse tissues in the <italic toggle="yes">Tabula Muris</italic> captured using the 10x Chromium technology and predicted labels for cells from the same compendium captured using Smart-seq2 (<xref rid="GR268581KIMC38" ref-type="bibr">The Tabula Muris Consortium 2018)</xref>. We found that scNym predictions were highly accurate (&gt;90%) and superior to baseline methods (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S5A–C</ext-link>). scNym models accurately classified monocyte subtypes, whereas baseline methods frequently confused these cells (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S5D,E</ext-link>).</p>
      <p>In a second cross-technology task, we trained scNym on mouse lung data from the <italic toggle="yes">Tabula Muris</italic> and predicted on lung data from the Mouse Cell Atlas, a separate experimental effort that used microwell-seq technology (<xref rid="GR268581KIMC16" ref-type="bibr">Han et al. 2018)</xref>. We found that scNym yielded high classification accuracy (&gt;90%), superior to baseline methods, despite experimental batch effects and differences in the sequencing technologies (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S6</ext-link>). We also trained scNym models to transfer regional identity annotations in spatial transcriptomics data and found performance competitive with baseline methods (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S7</ext-link>). Together, these results show that scNym models can effectively transfer cell type annotations across technologies and experimental environments.</p>
    </sec>
    <sec id="s1e">
      <title>Multidomain training allows integration of multiple reference data sets</title>
      <p>The number of public single-cell data sets is increasing rapidly (<xref rid="GR268581KIMC36" ref-type="bibr">Svensson et al. 2018)</xref>. Integrating information across multiple reference data sets may improve annotation transfer performance on challenging tasks. The domain adversarial training framework in scNym naturally extends to training across multiple reference data sets. We hypothesized that a multidomain training approach would allow for more general representations that improve annotation transfer. To test this hypothesis, we evaluated the performance of scNym to transfer annotations between single-cell and single-nucleus RNA-seq experiments in the mouse kidney. These data contained six different single-cell preparation methods and three different single-nucleus methods, capturing a range of technical variation in nine distinct domains (<xref rid="GR268581KIMF4" ref-type="fig">Fig. 4</xref>A,B; <xref rid="GR268581KIMC9" ref-type="bibr">Denisenko et al. 2020</xref>).</p>
      <fig position="float" id="GR268581KIMF4">
        <label>Figure 4.</label>
        <caption>
          <p>Multidomain training improves cross-technology annotation transfer in the mouse kidney. Cell type (<italic toggle="yes">A</italic>) and sequencing protocol annotations (<italic toggle="yes">B</italic>) in a UMAP projection of single-cell and nucleus RNA-seq profiles from the mouse kidney (<xref rid="GR268581KIMC9" ref-type="bibr">Denisenko et al. 2020)</xref>. Each protocol represents a unique training domain that captures technical variation. (<italic toggle="yes">C</italic>) Performance of scNym and baseline approaches on single-cell–to–nucleus and single-nucleus–to–cell annotation transfer. Methods are rank ordered by performance across tasks. scNym is superior to each baseline method on at least one task (Wilcoxon rank sums, <italic toggle="yes">P</italic> &lt; 0.05). (<italic toggle="yes">D</italic>) Single-nucleus target data labeled with true cell types (<italic toggle="yes">left</italic>) or the relative accuracy of scNym and baseline methods (<italic toggle="yes">right</italic>) for the single-cell–to–single-nucleus task. scNym achieves more accurate labeling of mesangial cells and tubule cell types (arrows). (<italic toggle="yes">E</italic>) Kidney tubule cells from <italic toggle="yes">D</italic> visualized independently with true and predicted labels. scNym offers the closest match to true annotations. All methods make notable errors on this difficult task. (<italic toggle="yes">F</italic>) Comparison of scNym performance when trained on individual training data sets (1-domain) versus multidomain training across all available data sets. We found that multidomain training improves performance on both the cells-to-nuclei and nuclei-to-cells transfer tasks (Wilcoxon rank sums, <italic toggle="yes">P</italic> = 0.073 and <italic toggle="yes">P</italic> &lt; 0.01, respectively).</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="1781f04" position="float"/>
      </fig>
      <p>scNym achieved significantly greater accuracy than baseline methods transferring labels from single-nucleus to single-cell experiments using multidomain training. This result was also achieved for the inverse transfer task, transferring annotations from single-cell to single-nucleus experiments (tied with best baseline) (<xref rid="GR268581KIMF4" ref-type="fig">Fig. 4</xref>C; <xref rid="GR268581KIMTB1" ref-type="table">Table 1</xref>). We found that scNym delivered more accurate annotations for multiple cell types in the cell-to-nucleus transfer task, including mesangial cells and tubule cell types (<xref rid="GR268581KIMF4" ref-type="fig">Fig. 4</xref>D,E). These improved annotations highlight that the performance advantages of scNym are meaningful for downstream analysis and biological interpretation. We found that multidomain scNym models achieved greater accuracy than any single-domain model on both tasks and effectively synthesized information from single-domain training sets of varying quality (<xref rid="GR268581KIMF4" ref-type="fig">Fig. 4</xref>F; <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S8</ext-link>). We performed a similar experiment using data from mouse cortex nuclei profiled with four distinct single-cell sequencing methods, training on three methods at a time and predicting annotations for the held-out fourth method for a total of four unique tasks. scNym was the top-ranked method across tasks (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S9</ext-link>).</p>
    </sec>
    <sec id="s1f">
      <title>scNym confidence scores enable expert review and allow new cell type discoveries</title>
      <p>Calibrated predictions, in which the classification probability returned by the model precisely reflects the probability it is correct, enable more effective interaction of the human researcher with the model output. We investigated scNym calibration by comparing the prediction confidence scores to prediction accuracy (Methods). We found that semisupervised adversarial training improved model calibration, such that high-confidence predictions are more likely to be correct (<xref rid="GR268581KIMF5" ref-type="fig">Fig. 5</xref>A,B; <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Figs. S10A,B, S11</ext-link>). scNym confidence scores can therefore be used to highlight cells that may benefit from manual review (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Figs. S10C, S11B</ext-link>), further improving the annotation exercise when it contains a domain expert in the loop.</p>
      <fig position="float" id="GR268581KIMF5">
        <label>Figure 5.</label>
        <caption>
          <p>scNym confidence scores highlight unseen cell types. (<italic toggle="yes">A</italic>) scNym calibration error for models trained on the human PBMC cross-stimulation task. Semisupervised and adversarial training significantly reduced calibration error relative to models trained with only supervised methods (Base, MixUp). (<italic toggle="yes">B</italic>) Calibration curves capturing the relationship between model confidence and empirical accuracy for models in <italic toggle="yes">A</italic>. (<italic toggle="yes">C</italic>) scNym models were trained to transfer annotations from a mouse atlas without brain cell types to data from mouse brain tissue. We desire a model that provides low-confidence scores to the new cell types and high-confidence scores for endothelial cells seen in other tissues. (<italic toggle="yes">D</italic>) scNym confidence scores for target brain cells. New cell types receive low-confidence scores as desired (dashed outlines).</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="1781f05" position="float"/>
      </fig>
      <p>scNym confidence scores can also highlight new, unseen cell types in the target data set using an optional pseudolabel thresholding procedure during training, inspired by FixMatch (Methods) (<xref rid="GR268581KIMC31" ref-type="bibr">Sohn et al. 2020</xref>). The semisupervised and adversarial components of scNym encourage the model to find a matching identity for cells in the target data set. Pseudolabel thresholding allows scNym to exclude cells with low-confidence pseudolabels from the semisupervised and adversarial components of training, stopping these components from mismatching unseen cell types and resulting in correctly uncertain predictions.</p>
      <p>To test this approach, we simulated two experiments in which we “discover” multiple cell types by predicting annotations on the <italic toggle="yes">Tabula Muris</italic> brain cell data using models trained on nonbrain tissues (Methods) (<xref rid="GR268581KIMF5" ref-type="fig">Fig. 5</xref>A,B). We first used pretrained scNym models to predict labels for new cell types not present in the original training or target sets, and scNym correctly marked these cells with low-confidence scores (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S12</ext-link>). In the second experiment, we included new cell types in the target set during training and found that scNym models with pseudolabel thresholding correctly provided low-confidence scores to new cell types, highlighting these cells as potential cell type discoveries for manual inspection (<xref rid="GR268581KIMF5" ref-type="fig">Fig. 5</xref>C,D; <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S13</ext-link>).</p>
      <p>We found that scNym embeddings capture cell type differences even within the low-confidence cell population, such that clustering these cells in the scNym embedding can provide a hypothesis for how many new cell types might be present (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S14</ext-link>). We also found that putative new cell types could be discriminated from other low-confidence cells, like prediction errors on a cell type boundary (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S15</ext-link>). These results show that scNym confidence scores can highlight target cell types that were absent in the training data, potentially enabling new cell type discoveries.</p>
    </sec>
    <sec id="s1g">
      <title>Semisupervised and adversarial training components improve annotation transfer</title>
      <p>We ablated different components of scNym to determine which features were responsible for high performance. We found that semisupervision with MixMatch and training with a domain adversary improved model performance across multiple tasks (<xref rid="GR268581KIMF6" ref-type="fig">Fig. 6</xref>B; <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S16</ext-link>). We hypothesized that scNym models might benefit from domain adaptation through the adversarial model by integrating the cells into a latent space more effectively. Supporting this hypothesis, we found that training and target domains were significantly more mixed in scNym embeddings (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S17</ext-link>). These results suggest that semisupervision and adversarial training improve the accuracy of cell type classifications.</p>
      <fig position="float" id="GR268581KIMF6">
        <label>Figure 6.</label>
        <caption>
          <p>Comparison of semisupervised scNym to other single-cell classification methods and ablated scNym variants. (<italic toggle="yes">A</italic>) We assign each method a rank order (rank 1 is best) based on performance for each benchmark task. scNym is the top-ranked method across tasks and ranks highly on all tasks. A support vector machine (SVM) baseline is the next best method, consistent with a previous benchmarking study (<xref rid="GR268581KIMC1" ref-type="bibr">Abdelaal et al. 2019)</xref>. (<italic toggle="yes">B</italic>) Ablation experiments comparing simplified supervised scNym models (Base) against the full scNym model with semisupervised and adversarial training (SSL + Adv.). We found that semisupervised and adversarial training significantly improved scNym performance across diverse tasks (all tasks shown, Wilcoxon rank sums, <italic toggle="yes">P</italic> &lt; 0.05).</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="1781f06" position="float"/>
      </fig>
    </sec>
    <sec id="s1h">
      <title>scNym is robust to hyperparameter selection</title>
      <p>Hyperparameter selection can be an important determinant of classification model performance. In all tasks presented here, we have used the same set of default scNym parameters derived from past recommendations in the representation learning literature (Methods). To determine how sensitive scNym performance is to these hyperparameter choices, we trained scNym models on the hPBMC cross-stimulation task across a grid of hyperparameter values. We found that scNym is robust to hyperparameter changes within an order of magnitude of the default values, showing that our defaults are not “overfit” to the benchmark tasks presented here (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S18</ext-link>). We also performed hyperparameter optimization using reverse fivefold cross-validation for the top three baseline methods (SVM, SingleCellNet, scmap-cell-exact) to determine if an optimized baseline was superior to scNym across four benchmarking tasks (Methods). We found that scNym performance using default parameters was superior to the performance of baseline methods after hyperparameter tuning (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Table S3</ext-link>; <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S19</ext-link>).</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s2">
    <title>Discussion</title>
    <p>Single-cell genomics experiments have become more accessible owing to commercial technologies, enabling a rapid increase in the use of these methods (<xref rid="GR268581KIMC37" ref-type="bibr">Svensson et al. 2020)</xref>. Cell identity annotation is an essential step in the analysis of these experiments, motivating the development of high-performance, automated annotation methods that can take advantage of diverse data sets. Here, we introduced a semisupervised adversarial neural network model that learns to transfer annotations from one experiment to another, taking advantage of information in both labeled training sets and an unlabeled target data set.</p>
    <p>Our benchmark experiments show that scNym models provide high performance across a range of cell identity classification tasks, including cross-age, cross-perturbation, and cross-technology scenarios. scNym performs better in these varied conditions than 10 state-of-the-art baseline methods, including three unsupervised data integration approaches paired with supervised classifiers (<xref rid="GR268581KIMF6" ref-type="fig">Fig. 6</xref>A; <xref rid="GR268581KIMTB1" ref-type="table">Table 1</xref>). The superiority of scNym is consistent across diverse performance metrics, including accuracy, Cohen's <italic toggle="yes">κ</italic>-score, and the multiclass receiver operating characteristic (MCROC) (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S20</ext-link>; <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Tables S1, S2</ext-link>).</p>
    <p>The key idea that differentiates scNym from previous cell classification approaches is the use of semisupervised (<xref rid="GR268581KIMC6" ref-type="bibr">Berthelot et al. 2019)</xref> and adversarial training (<xref rid="GR268581KIMC13" ref-type="bibr">Ganin et al. 2016)</xref> to extract information from the unlabeled, target experiment we wish to annotate. Through ablation experiments, we showed that these training strategies improve the performance of our models. Performance improvements were most pronounced when there were large, systematic differences between the training and target data sets (<xref rid="GR268581KIMF3" ref-type="fig">Fig. 3</xref>). Semisupervision and adversarial training also allow scNym to integrate information across multiple training and target data sets, improving performance (<xref rid="GR268581KIMF4" ref-type="fig">Fig. 4</xref>). As large-scale single-cell perturbation experiments become more common (<xref rid="GR268581KIMC12" ref-type="bibr">Dixit et al. 2016</xref>; <xref rid="GR268581KIMC33" ref-type="bibr">Srivatsan et al. 2020)</xref> and multiple cell atlases are released for common model systems, our method's ability to adapt across distinct biological and technical conditions will only increase in value.</p>
    <p>Most downstream biological analyses rely upon cell identity annotations, so it is important that researchers are able to interpret the molecular features that drive model decisions. We showed that backpropagation-based saliency analysis methods are able to recover specific cell type markers, confirming that scNym models learn interpretable, biologically relevant features of cell type. In future work, we hope to extend upon these interpretability methods to infer perturbations that alter cell identity programs using the informative representations learned by scNym.</p>
  </sec>
  <sec sec-type="methods" id="s3">
    <title>Methods</title>
    <sec id="s3a">
      <title>scNym model</title>
      <p>Our scNym model <inline-formula id="il1"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL1" display="inline" overflow="scroll"><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula> consists of a neural network with an input layer; two hidden layers, each with 256 nodes; and an output layer with a node for each class. The first three layers are paired with batch normalization (<xref rid="GR268581KIMC18" ref-type="bibr">Ioffe and Szegedy 2015</xref>), rectified linear unit activation, and dropout (<xref rid="GR268581KIMC32" ref-type="bibr">Srivastava et al. 2014</xref>). The final layer is paired with a softmax activation to transform real number outputs of the neural network into a vector of class probabilities. The model maps cell profile vectors <italic toggle="yes">x</italic> to probability distributions <italic toggle="yes">p</italic>(<italic toggle="yes">y</italic>|<italic toggle="yes">x</italic>) over cell identity classes <italic toggle="yes">y</italic>:
<disp-formula id="GR268581KIMUM1"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM1" display="block" overflow="scroll"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo fence="false">|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
      <p>We train scNym to map cell profiles in a gene expression matrix <italic toggle="yes">x</italic> ∈ <bold><italic toggle="yes">X</italic></bold><sup>Cells×Genes</sup> to paired cell identity annotations <italic toggle="yes">y</italic> ∈ <bold><italic toggle="yes">y</italic></bold>. Transcript counts in the gene expression matrix are normalized to counts per million (CPM) and log-transformed after addition of a pseudocount (log(CPM + 1)). During training, we randomly mask 10% of genes in each cell with zero values and then renormalize to obtain an augmented profile.</p>
      <p>We use the Adadelta adaptive stochastic gradient descent method (<xref rid="GR268581KIMC47" ref-type="bibr">Zeiler 2012)</xref> with an initial learning rate of <italic toggle="yes">η</italic> = 1.0 to update model parameters on minibatches of cells, with batch sizes of 256. We apply a weight decay term of <italic toggle="yes">λ</italic><sub>WD</sub> = 10<sup>−4</sup> for regularization. We train scNym models to minimize a standard cross-entropy loss function for supervised training:<disp-formula id="GR268581KIMUM2"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM2" display="block" overflow="scroll"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CE</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic toggle="yes">y</italic><sub>(<italic toggle="yes">k</italic>)</sub> is an indicator variable for the membership of <italic toggle="yes">x</italic> in class <italic toggle="yes">k</italic>, and <italic toggle="yes">k</italic> ∈ <italic toggle="yes">K</italic> represents class indicators.</p>
      <p>We fit all scNym models for a maximum of 400 epochs and selected the optimal set of weights using early stopping on a validation set consisting of 10% of the training data. We initiate early stopping after training has completed at least 5% of the total epochs to avoid premature termination.</p>
      <p>Before passing each minibatch to the network, we perform dynamic data augmentation with the “MixUp” operation (<xref rid="GR268581KIMC48" ref-type="bibr">Zhang et al. 2018)</xref>. MixUp computes a weighted average of two samples <italic toggle="yes">x</italic> and <italic toggle="yes">x</italic><sup>′</sup> where the weights <italic toggle="yes">λ</italic> are randomly sampled from a beta distribution with a symmetric shape parameter <italic toggle="yes">α</italic>:<disp-formula id="GR268581KIMUM3"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM3" display="block" overflow="scroll"><mml:msub><mml:mrow><mml:mi mathvariant="normal">Mix</mml:mi></mml:mrow><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi>λ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">Beta</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
      <p>For all experiments here, we set <italic toggle="yes">α</italic> = 0.3 based on performance in the natural image domain (<xref rid="GR268581KIMC48" ref-type="bibr">Zhang et al. 2018)</xref>. Forcing models to interpolate predictions smoothly between samples shifts the decision boundary away from high-density regions of the input distribution, improving generalization. This procedure has been shown to improve classifier performance on multiple tasks (<xref rid="GR268581KIMC48" ref-type="bibr">Zhang et al. 2018)</xref>. Model calibration—the correctness of a model's confidence scores for each class—is generally also improved by this augmentation scheme (<xref rid="GR268581KIMC41" ref-type="bibr">Thulasidasan et al. 2019)</xref>.</p>
    </sec>
    <sec id="s3b">
      <title>Semisupervision with MixMatch</title>
      <p>We train semisupervised scNym models using the MixMatch framework (<xref rid="GR268581KIMC6" ref-type="bibr">Berthelot et al. 2019)</xref>, treating the target data set as unlabeled data <inline-formula id="il2"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL2" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow></mml:math></inline-formula>. At each iteration, MixMatch samples minibatches from both the labeled data set <inline-formula id="il3"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL3" display="inline" overflow="scroll"><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:math></inline-formula> and unlabeled data set <inline-formula id="il4"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL4" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow></mml:math></inline-formula>. We generate “pseudolabels” (<xref rid="GR268581KIMC24" ref-type="bibr">Lee 2013)</xref> using model predictions for each observation in the unlabeled minibatch (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>):<disp-formula id="GR268581KIMUM4"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM4" display="block" overflow="scroll"><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
      <p>We next “sharpen” the pseudolabels using a “temperature scaling” procedure (<xref rid="GR268581KIMC17" ref-type="bibr">Hinton et al. 2015</xref>; <xref rid="GR268581KIMC14" ref-type="bibr">Guo et al. 2017)</xref> with the temperature parameter <italic toggle="yes">T</italic> = 0.5 as a form of entropy minimization (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>). This entropy minimization encourages unlabeled examples to belong to one of the described classes.</p>
      <p>We then randomly mix each observation and label/pseudolabel pair in both the labeled and unlabeled minibatches with another observation using MixUp (<xref rid="GR268581KIMC48" ref-type="bibr">Zhang et al. 2018)</xref>. We allow labeled and unlabeled observations to mix together during this procedure (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>):<disp-formula id="GR268581KIMUM5"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM5" display="block" overflow="scroll"><mml:mi>λ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">Beta</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
<disp-formula id="GR268581KIMUM6"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM6" display="block" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Mix</mml:mi></mml:mrow><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">Mix</mml:mi></mml:mrow><mml:mi>λ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
where (<italic toggle="yes">w</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub>) is either a labeled observation and ground truth label (<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>) or an unlabeled observation and the pseudolabel (<italic toggle="yes">u</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic></sub>). This procedure yields a minibatch <bold><italic toggle="yes">X</italic><sup>′</sup></bold> of mixed labeled observations and a minibatch <bold><italic toggle="yes">U</italic><sup>′</sup></bold> of mixed unlabeled observations.</p>
      <p>We introduce a semisupervised interpolation consistency penalty during training in addition to the standard supervised loss. For observations and pseudolabels in the mixed unlabeled minibatch <italic toggle="yes">U</italic><sup>′</sup>, we penalize the mean squared error (MSE) between the mixed pseudolabels and the model prediction for the mixed observation (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>):<disp-formula id="GR268581KIMUM7"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM7" display="block" overflow="scroll"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SSL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:msub><mml:mo>∥</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msubsup><mml:mo>∥</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
      <p>This encourages the model to provide smooth interpolations between observations and their ground truth or pseudolabels, generalizing the decision boundary of the model. We weight this unsupervised loss relative to the supervised cross-entropy loss using the weighting function <italic toggle="yes">λ</italic><sub>SSL</sub> (<italic toggle="yes">t</italic>) → [0, 1]. We initialize this coefficient to <italic toggle="yes">λ</italic><sub>SSL</sub> = 0 and increase the weight to a final value of <italic toggle="yes">λ</italic><sub>SSL</sub> = 1 over 100 epochs using a sigmoid schedule (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>):<disp-formula id="GR268581KIMUM8"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM8" display="block" overflow="scroll"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CE</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SSL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SSL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
    </sec>
    <sec id="s3c">
      <title>Domain adaptation with domain adversarial networks</title>
      <p>We use domain adversarial networks (DANs) as an additional approach to incorporate information from the target data set during training (<xref rid="GR268581KIMC13" ref-type="bibr">Ganin et al. 2016)</xref>. The DAN method encourages the classification model to embed cells from the training and target data set with similar coordinates, such that training and target data sets are well mixed in the embedding. By encouraging the training and target data set to be well mixed, we take advantage of the inductive bias that cell identity classes in each data set are similar, despite technical variation or differences in conditions (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>).</p>
      <p>We introduce this technique into scNym by adding an adversarial domain classification network, <inline-formula id="il5"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL5" display="inline" overflow="scroll"><mml:msub><mml:mi>g</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula>. We implement <inline-formula id="il6"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL6" display="inline" overflow="scroll"><mml:msub><mml:mi>g</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> as a two-layer neural network with a single hidden layer of 256 units and a rectified linear unit activation, followed by a classification layer with one output per domain of origin and a softmax activation. This adversary attempts to predict the domain of origin <italic toggle="yes">d</italic> from the penultimate classifier embedding <italic toggle="yes">v</italic> of each observation. For each forward pass, it outputs a probability vector, <inline-formula id="il7"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL7" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, estimating the likelihood the observation came from the training or target domain.</p>
      <p>We assign a one-hot encoded domain label, <italic toggle="yes">d</italic>, to each molecular profile based on the experiment of origin (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>). During training, we pass a minibatch of labeled observations, <italic toggle="yes">x</italic> ∈ <bold><italic toggle="yes">X</italic></bold>, and unlabeled observations, <italic toggle="yes">u</italic> ∈ <bold><italic toggle="yes">U</italic></bold>, through the domain adversary to predict domain labels:<disp-formula id="GR268581KIMUM9"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM9" display="block" overflow="scroll"><mml:mrow><mml:mover><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
where <inline-formula id="il8"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL8" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi>d</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is the domain probability vector, and <inline-formula id="il9"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL9" display="inline" overflow="scroll"><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> denotes the embedding of <italic toggle="yes">x</italic> from the penultimate layer of the classification model <inline-formula id="il10"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL10" display="inline" overflow="scroll"><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula>. We fit the adversary using a multiclass cross-entropy loss, as described above for the main classification loss (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>).</p>
      <p>To make use of the adversary for training the classification model, we use the “gradient reversal” trick at each backward pass. We update the parameters <italic toggle="yes">ϕ</italic> of the adversary using standard gradient descent on the loss <italic toggle="yes">L</italic><sub>adv</sub>. At each backward pass, this optimization improves the adversarial domain classifier (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>). We update the parameters <italic toggle="yes">θ</italic> of the classification model using the <italic toggle="yes">inverse</italic> of the gradients computed during a backward pass from <italic toggle="yes">L</italic><sub>adv</sub>. Using the inverse gradients encourages the classification model <inline-formula id="il11"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL11" display="inline" overflow="scroll"><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula> to generate an embedding where it is difficult for the adversary to predict the domain (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>). Our update rule for the classification model parameters therefore becomes<disp-formula id="GR268581KIMUM10"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM10" display="block" overflow="scroll"><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CE</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SSL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SSL</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">adv</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">adv</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mstyle></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
      <p>We increase the weight of the adversary gradients from <italic toggle="yes">λ</italic><sub>adv</sub> → [0, 0.1] over the course of 20 epochs during training using a sigmoid schedule. We scale the adversarial <italic toggle="yes">gradients</italic> flowing to <italic toggle="yes">θ</italic>, rather than the adversarial loss term, so that full-magnitude gradients are used to train a robust adversary, <inline-formula id="il12"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL12" display="inline" overflow="scroll"><mml:msub><mml:mi>g</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:math></inline-formula> (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>). Incorporating both MixMatch and the domain adversary, our full loss function becomes<disp-formula id="GR268581KIMUM11"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM11" display="block" overflow="scroll"><mml:mi>L</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">CE</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SSL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">SSL</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">U</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">adv</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">U</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula>
</p>
    </sec>
    <sec id="s3d">
      <title>Pseudolabel thresholding for new cell type discovery</title>
      <p>Entropy minimization and domain adversarial training enforce an inductive bias that all cells in the target data set belong to a class in the training data set. For many cell type classification tasks, this assumption is valid and useful. However, it is violated in the case in which new, unseen cell types are present in the target data set. We introduce an alternative training configuration to allow for quantitative identification of new cell types in these instances.</p>
      <p>We have observed that new cell types will receive low-confidence pseudolabels, as they do not closely resemble any of the classes in the training set (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S12</ext-link>). We wish to exclude these low-confidence pseudolabels from our entropy minimization and domain adversarial training procedures, as these methods incorrectly encourage these new cell types to receive both high-confidence predictions and embeddings for a known cell type. We therefore adopt a notion of “pseudolabel confidence thresholding” introduced in the FixMatch method (<xref rid="GR268581KIMC31" ref-type="bibr">Sohn et al. 2020)</xref>. To identify confident pseudolabels to use during training, we set a minimum pseudolabel confidence <italic toggle="yes">τ</italic> = 0.9 and assign all pseudolabels a binary confidence indicator, <italic toggle="yes">c</italic><sub><italic toggle="yes">i</italic></sub> ∈ {0, 1} (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>).</p>
      <p>We make two modifications to the training procedure to prevent low-confidence pseudolabels from contributing to any component of the loss function. First, we use only high-confidence pseudolabels in the MixUp operation of the MixMatch procedure. This prevents low-confidence pseudolabels from contributing to the supervised classification or interpolation consistency losses (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>). Second, we use only unlabeled examples with high-confidence pseudolabels to train the domain adversary. These low-confidence unlabeled examples can therefore occupy a unique region in the model embedding, even if they are easily discriminated from training examples. Our adversarial loss is slightly modified to penalize domain predictions only on confident samples in the pseudolabeled minibatch (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>).</p>
      <p>We found that this pseudolabel thresholding configuration option was essential to provide accurate, quantitative information about the presence of new cell types in the target data set (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Fig. S13</ext-link>). However, this option does modestly decrease performance when new cell types are not present. We therefore enable this option when the possibility of new cell types violates the assumption that the training and target data share the same set of cell types. We have provided a simple toggle in our software implementation to allow users to enable or disable this feature.</p>
    </sec>
    <sec id="s3e">
      <title>scNym model embeddings</title>
      <p>We generate gene expression embeddings from our scNym model by extracting the activations of the penultimate neural network layer for each cell. We visualize these embeddings using UMAP (<xref rid="GR268581KIMC5" ref-type="bibr">Becht et al. 2019</xref>; <xref rid="GR268581KIMC27" ref-type="bibr">McInnes et al. 2020)</xref> by constructing a nearest-neighbor graph (<italic toggle="yes">k</italic> = 30) in principal component space derived from the penultimate activations. We set min_dist = 0.3 for the UMAP minimum distance parameter.</p>
      <p>We present single-cell experiments using a two-dimensional representation fit using the UMAP algorithm (<xref rid="GR268581KIMC5" ref-type="bibr">Becht et al. 2019)</xref>. For each experiment, we compute a PCA projection on a set of highly variable genes after log (CPM + 1) normalization. We construct a nearest-neighbor graph using the first 50 principal components and fit a UMAP projection from this nearest-neighbor graph.</p>
    </sec>
    <sec id="s3f">
      <title>Entropy of mixing</title>
      <p>We compute the “entropy of mixing” to determine the degree of domain adaptation between training and target data sets in an embedding <italic toggle="yes">X</italic>. The entropy of mixing is defined as the entropy of a vector of class membership in a local neighborhood of the embedding:<disp-formula id="GR268581KIMUM12"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM12" display="block" overflow="scroll"><mml:mi>H</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Local</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Local</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Local</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic toggle="yes">p</italic><sup>Local</sup> is a vector of class proportions in a local neighborhood, and <italic toggle="yes">k</italic> ∈ <italic toggle="yes">K</italic> are class indices. We compute the entropy of mixing for an embedding <italic toggle="yes">X</italic> by randomly sampling <italic toggle="yes">n</italic> = 1000 cells and computing the entropy of mixing on a vector of class proportions for the 100 nearest neighbors to each point.</p>
    </sec>
    <sec id="s3g">
      <title>Integrated gradient analysis</title>
      <p>We interpreted the predictions of our scNym models by performing integrated gradient analysis (<xref rid="GR268581KIMC35" ref-type="bibr">Sundararajan et al. 2017)</xref>. Given a trained model, <inline-formula id="il13"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL13" display="inline" overflow="scroll"><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:math></inline-formula>, and a target class, <italic toggle="yes">k</italic>, we computed an integrated gradient score IG as the sum of gradients on a class probability, <inline-formula id="il14"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL14" display="inline" overflow="scroll"><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, with respect to an input gene expression vector, <italic toggle="yes">x</italic>, at <italic toggle="yes">M</italic> = 100 points along a linear path between the zero vector and the input <italic toggle="yes">x</italic>. We then multiplied the sum of gradients for each gene by the expression values in the input <italic toggle="yes">x</italic>. Stated formally, we computed<disp-formula id="GR268581KIMUM13"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM13" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">IG</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo>⋅</mml:mo><mml:mstyle><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>M</mml:mi></mml:mfrac></mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:mfrac><mml:mi>m</mml:mi><mml:mi>M</mml:mi></mml:mfrac></mml:mrow><mml:mi>x</mml:mi></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∂</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mstyle></mml:math></disp-formula>
</p>
      <p>In the original integrated gradient formalism, this is equivalent to using the zero vector as a baseline. We average the integrated gradients across <italic toggle="yes">n</italic><sub><italic toggle="yes">s</italic></sub> cell input vectors <italic toggle="yes">x</italic> to obtain class-level maps IG<sub><italic toggle="yes">k</italic></sub>, where <italic toggle="yes">n</italic><sub><italic toggle="yes">s</italic></sub> = min (300, <italic toggle="yes">n</italic><sub><italic toggle="yes">k</italic></sub>), and <italic toggle="yes">n</italic><sub><italic toggle="yes">k</italic></sub> is the number of cells in the target class. To identify genes that drive incorrect classifications, we computed integrated gradients with respect to some class <italic toggle="yes">k</italic> for cells with true class <italic toggle="yes">k</italic><sup>′</sup> that were incorrectly classified as class <italic toggle="yes">k</italic>.</p>
    </sec>
    <sec id="s3h">
      <title>Interpretability comparison</title>
      <p>We compared the biological relevance of features selected by scNym and SVM as a baseline by computing cell type–specific Gene Ontology enrichments. We trained both scNym and an SVM to transfer labels from the <italic toggle="yes">Tabula Muris</italic> 10x Genomics data set to the <italic toggle="yes">Tabula Muris</italic> Smart-seq2 data set. We then extracted feature importance scores from the scNym model using integrated gradients and from the SVM model based on coefficient weights. We selected cell type markers for each model as the top <italic toggle="yes">k</italic> = 100 genes with the highest integrated gradient values or SVM coefficients.</p>
      <p>For 19 cell types with corresponding Gene Ontology terms, we computed the enrichment of the relevant cell type–specific Gene Ontology terms in scNym-derived and SVM-derived cell type markers using Fisher's exact test (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>). We present a sample of the gene sets used (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Table S4</ext-link>). We compared the mean odds ratio from Fisher's exact test across relevant Gene Ontology terms between scNym-derived markers and SVM-derived markers. To determine statistical significance of a difference in these mean odds ratios, we performed a paired <italic toggle="yes">t</italic>-test across cell types. We performed the procedure above using <italic toggle="yes">k</italic> ∈ {50, 100, 150} to determine the sensitivity of our results to this parameter. We found that scNym-integrated gradients had consistently stronger enrichments for relevant Gene Ontology terms across cell types for all values of <italic toggle="yes">k</italic>.</p>
    </sec>
    <sec id="s3i">
      <title>Model calibration analysis</title>
      <p>We evaluated scNym calibration by binning all cells in a query set based on the softmax probability of their assigned class —<inline-formula id="il15"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL15" display="inline" overflow="scroll"><mml:munder><mml:mrow><mml:mo movablelimits="true">max</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:munder><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">softmax</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mspace width="0.4em"/><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>—which we term the “confidence score.” We grouped cells into <italic toggle="yes">M</italic> = 10 bins <italic toggle="yes">B</italic><sub><italic toggle="yes">m</italic></sub> of equal width from [0, 1] and computed the mean accuracy of predictions within each bin:<disp-formula id="GR268581KIMUM14"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM14" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">acc</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>≡</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
<disp-formula id="GR268581KIMUM15"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM15" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">conf</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mo movablelimits="true">max</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mrow><mml:mspace width="0.4em"/><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>〉</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>
where 1(<italic toggle="yes">a</italic> ≡ <italic toggle="yes">b</italic>) denotes a binary equivalency operation that yields 1 if <italic toggle="yes">a</italic> and <italic toggle="yes">b</italic> are equivalent and 0 otherwise and <inline-formula id="il30"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL16" display="inline" overflow="scroll"><mml:mrow><mml:mo>〈</mml:mo><mml:mo>⋅</mml:mo><mml:mo>〉</mml:mo></mml:mrow></mml:math></inline-formula> denotes the arithmetic average.</p>
      <p>We computed the “expected calibration error” as previously proposed (<xref rid="GR268581KIMC41" ref-type="bibr">Thulasidasan et al. 2019)</xref>:
<disp-formula id="GR268581KIMUM16"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM16" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">ECE</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow><mml:mo fence="false">|</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">acc</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">conf</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo fence="false">|</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:math></disp-formula>
</p>
      <p>We also computed the “overconfidence error”, which specifically focuses on high confidence but incorrect predictions:
<disp-formula id="GR268581KIMUM17"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM17" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">oe</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">conf</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo movablelimits="true">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">conf</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="normal">acc</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula>
<disp-formula id="GR268581KIMUM18"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="UM18" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="normal">OE</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mstyle><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo fence="false">|</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mrow><mml:mrow><mml:mi mathvariant="normal">oe</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:math></disp-formula>
where <italic toggle="yes">N</italic> is the total number of samples, and |<italic toggle="yes">B</italic><sub><italic toggle="yes">m</italic></sub>| is the number of samples in bin <italic toggle="yes">B</italic><sub><italic toggle="yes">m</italic></sub>.</p>
      <p>We performed this analysis for each model trained in a fivefold cross-validation split to estimate calibration for a given model configuration. We evaluated calibrations for baseline neural network models, models with MixUp but not MixMatch, and models with the full MixMatch procedure.</p>
    </sec>
    <sec id="s3j">
      <title>Baseline methods</title>
      <p>As baseline methods, we used 10 cell identity classifiers: scmap-cell, scmap-cluster (<xref rid="GR268581KIMC22" ref-type="bibr">Kiselev et al. 2018</xref>; <xref rid="GR268581KIMC3" ref-type="bibr">Andrews and Hemberg 2019)</xref>, scmap-cell-exact (scmap-cell with exact k-NN search), a linear SVM (<xref rid="GR268581KIMC1" ref-type="bibr">Abdelaal et al. 2019)</xref>, scPred (<xref rid="GR268581KIMC2" ref-type="bibr">Alquicira-Hernandez et al. 2019)</xref>, SingleCellNet (<xref rid="GR268581KIMC39" ref-type="bibr">Tan and Cahan 2021)</xref>, CHETAH (<xref rid="GR268581KIMC8" ref-type="bibr">de Kanter et al. 2019)</xref>, Harmony followed by an SVM (<xref rid="GR268581KIMC23" ref-type="bibr">Korsunsky et al. 2019)</xref>, LIGER followed by an SVM (<xref rid="GR268581KIMC34" ref-type="bibr">Stuart et al. 2019)</xref>, and scANVI (<xref rid="GR268581KIMC25" ref-type="bibr">Lopez et al. 2018</xref>; <xref rid="GR268581KIMC46" ref-type="bibr">Xu et al. 2021)</xref>. For model training, we split data into fivefolds and trained five separate models, each using fourfolds for training and validation data. This allowed us to assess variation in model performance as a function of changes in the training data. No class balancing was performed before training, although some methods perform class balancing internally. All models, including scNym, were trained on the same fivefold splits to ensure equitable access to information. All methods were run with the best hyperparameters suggested by the investigators unless otherwise stated for our hyperparameter optimization comparisons (for full details, see <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>).</p>
      <p>We applied all baseline methods to all benchmarking tasks. If a method could not complete the task given 256 GB of RAM and 8 CPU cores, we reported the accuracy for that method as “undetermined.” Only the scNym and scANVI models required GPU resources. We trained models on Nvidia K80, GTX1080ti, Titan RTX, or RTX 8000 GPUs, using only a single GPU per model.</p>
    </sec>
    <sec id="s3k">
      <title>Performance benchmarking</title>
      <p>For all benchmarks, we computed the mean accuracy across cells (“accuracy”), Cohen's <italic toggle="yes">κ</italic>-score, and the multiclass receiver operating characteristic (MCROC). We computed the MCROC as the mean of ROC scores across cell types, treating each cell type as a binary classification problem. We used external methods to generate probabilistic outputs for baseline methods where appropriate (<xref rid="GR268581KIMC29" ref-type="bibr">Platt 1999</xref>). We performed quality-control filtering and preprocessing on each data set before training (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Methods</ext-link>).</p>
      <p>For the Rat Aging Cell Atlas (<xref rid="GR268581KIMC26" ref-type="bibr">Ma et al. 2020)</xref> benchmark, we trained scNym models on single-cell RNA-seq from young, ad libitum fed rats (5 mo old) and predicted on cells from aged rats (ad libitum fed or calorically restricted). For the human PBMC stimulation benchmark, we trained models on unstimulated PBMCs collected from multiple human donors and predicted on IFNB1-stimulated PBMCs collected in the same experiment (<xref rid="GR268581KIMC19" ref-type="bibr">Kang et al. 2018)</xref>.</p>
      <p>For the <italic toggle="yes">Tabula Muris</italic> cross-technology benchmark, we trained models on the <italic toggle="yes">Tabula Muris</italic> 10x Genomics Chromium platform and predicted on data generated using Smart-seq2. For the Mouse Cell Atlas (MCA) (<xref rid="GR268581KIMC16" ref-type="bibr">Han et al. 2018)</xref> benchmark, we trained models on single-cell RNA-seq from lung tissue in the <italic toggle="yes">Tabula Muris</italic> 10x Chromium data (<xref rid="GR268581KIMC38" ref-type="bibr">The Tabula Muris Consortium 2018)</xref> and predicted on MCA lung data. For the spatial transcriptomics benchmark, we trained models on spatial transcriptomics from a mouse sagittal–posterior brain section and predicted labels for another brain section (data downloaded from <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.10xgenomics.com/resources/datasets/">https://www.10xgenomics.com/resources/datasets/</uri>).</p>
      <p>For the single-cell to single-nucleus benchmark in the mouse kidney, we trained scNym models on all single-cell data from six unique sequencing protocols and predicted labels for single nuclei from three unique protocols (<xref rid="GR268581KIMC9" ref-type="bibr">Denisenko et al. 2020)</xref>. For the single-nucleus to single-cell benchmark, we inverted the training and target data sets above to train on the nuclei data sets and predict on the single-cell data sets. We set unique domain labels for each protocol during training in both benchmark experiments. To evaluate the impact of multidomain training, we also trained models on only one single-cell or single-nucleus protocol using the domains from the opposite technology as target data.</p>
      <p>For the multidomain cross-technology benchmark in mouse cortex nuclei, we generated four distinct subtasks from data generated using four distinct technologies to profile the same samples (<xref rid="GR268581KIMC11" ref-type="bibr">Ding et al. 2020)</xref>. We trained scNym and baseline methods to predict labels on one technology given the remaining three technologies as training data for all possible combinations. We used each technology as a unique domain label for scNym.</p>
      <p>For the cross-species mouse-to-rat demonstration, we selected a set of cell types with comparable annotations in the <italic toggle="yes">Tabula Muris</italic> and Rat Aging Cell Atlas (<xref rid="GR268581KIMC26" ref-type="bibr">Ma et al. 2020)</xref> to allow for quantitative evaluation. We trained scNym with mouse data as the source domain and rat data as the target domain. We used the new identity discovery configuration to account for the potential for new cell types in a cross-species experiment. For the cross-species mouse-to-human demonstration, we similarly selected a set of cell types with comparable cell annotation ontologies in the <italic toggle="yes">Tabula Muris</italic> l0x lung data and human lung cells from the IPF Cell Atlas (<xref rid="GR268581KIMC15" ref-type="bibr">Habermann et al. 2020)</xref>. We trained an scNym model using mouse data as the source domain and human data as the target, as for the mouse-to-rat demonstration.</p>
    </sec>
    <sec id="s3l">
      <title>Runtime benchmarking</title>
      <p>We measured the runtime of scNym and each baseline classification method using subsamples from the multidomain kidney single-cell and single-nuclei data set (<xref rid="GR268581KIMC9" ref-type="bibr">Denisenko et al. 2020)</xref>. We measured runtimes for annotation transfer from single cells to single-nuclei labels using subsamples of size <italic toggle="yes">n</italic> ∈ {1250, 2500, 5000, 10000, 20000, 40000} for each of the training and target data sets. All methods were run on four cores of a 2.1-GHz Intel Xeon Gold 6130 CPU and 64 GB of CPU memory. GPU-capable methods (scNym, scANVI) were provided with one Nvidia Titan RTX GPU (consumer grade CUDA compute device).</p>
    </sec>
    <sec id="s3m">
      <title>Hyperparameter optimization experiments</title>
      <p>We performed hyperparameter optimization across four tasks for the top three baseline methods: the SVM, SingleCellNet, and scmap-cell-exact. For the SVM, we optimized the regularization strength parameter <italic toggle="yes">C</italic> at 12 values <inline-formula id="il16"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL17" display="inline" overflow="scroll"><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mi>k</mml:mi></mml:msup><mml:mspace width="0.4em"/><mml:mi mathvariant="normal">∀</mml:mi><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo>[</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> with and without class weighting. For class weighting, we set class weights as either uniform or inversely proportional to the number of cells in each class to enforce class balancing (<inline-formula id="il17"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL18" display="inline" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula>, where <italic toggle="yes">w</italic><sub><italic toggle="yes">k</italic></sub> is the weight for class <italic toggle="yes">k</italic>, and <italic toggle="yes">n</italic><sub><italic toggle="yes">k</italic></sub> is the number of cells for that class). For scmap-cell-exact, we optimized (1) the number of nearest neighbors (<italic toggle="yes">k</italic> ∈ {5, 10, 30, 50, 100}), (2) the distance metric (<italic toggle="yes">d</italic>(·,·) ∈ {cosine, euclidean}), and (3) the number of features to select with M3Drop (<italic toggle="yes">n</italic><sub><italic toggle="yes">f</italic></sub> ∈ {500, 1000, 2000, 5000}). For SingleCellNet, we optimized with nTopGenes ∈ {10, 20}, nRand ∈ {35, 70, 140}, nTrees ∈ {100, 1000, 2000}, and nTopGenePairs ∈ {12, 25}.</p>
      <p>We optimized scNym for two of the four tasks, owing to computational expense and superiority of default parameters relative to baseline methods. For scNym, we optimized (1) weight decay (<italic toggle="yes">λ</italic><sub><italic toggle="yes">w</italic></sub> ∈ 10<sup>−5</sup>, 10<sup>−4</sup>, 10<sup>−3</sup>), (2) batch size (<italic toggle="yes">M</italic> ∈ {128, 256}), (3) the number of hidden units (<italic toggle="yes">h</italic> ∈ {256, 512}), (4) the maximum MixMatch weight (<italic toggle="yes">λ</italic><sub>SSL</sub> ∈ {0.01, 0.1, 1.0}), and (5) the maximum DAN weight (<italic toggle="yes">λ</italic><sub>adv</sub> ∈ {0.01, 0.1, 0.2}). We did not optimize weight decay for the PBMC cross-stimulation task. We performed a grid search for all methods.</p>
      <p>Hyperparameter optimization is nontrivial in the context of a domain shift between the training and test set. Traditional optimization using cross-validation on the training set alone may overfit parameters to the training domain, leading to suboptimal outcomes. This failure mode is especially problematic for domain adaptation models, in which decreasing the strength of domain adaptation regularizers may improve performance within the training data while actually decreasing performance on the target data.</p>
      <p>In light of these concerns, we adopted a procedure known as reverse cross-validation to evaluate each hyperparameter set (<xref rid="GR268581KIMC50" ref-type="bibr">Zhong et al. 2010)</xref>. Reverse cross-validation uses both the training and target data sets during training to account for the effect of hyperparameters on the effectiveness of transferring labels across domains. Formally, we first split the labeled training data <inline-formula id="il18"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL19" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:math></inline-formula> into a training set, validation set, and held-out test set <inline-formula id="il19"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL20" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow><mml:mspace width="0.4em"/><mml:msup><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mi>υ</mml:mi></mml:msup><mml:mrow><mml:mo>,</mml:mo></mml:mrow><mml:mspace width="0.4em"/><mml:msup><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>∗</mml:mo></mml:msup></mml:math></inline-formula>. We use 10% of the training data set for the validation set and 10% for the held-out test set. We then train a model, <inline-formula id="il20"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL21" display="inline" overflow="scroll"><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mi>x</mml:mi><mml:mo>→</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, to transfer labels from the training set <inline-formula id="il21"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL22" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to the target data <inline-formula id="il22"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL23" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow></mml:math></inline-formula>. We use the validation set <inline-formula id="il23"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL24" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mi>v</mml:mi></mml:msup></mml:math></inline-formula> for early stopping with scNym and concatenate it into the training set for other methods that do not use a validation set. We treat the predictions <inline-formula id="il24"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL25" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> as pseudolabels for the unlabeled data set and subsequently train a second model, <inline-formula id="il25"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL26" display="inline" overflow="scroll"><mml:msub><mml:mi>f</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mi>u</mml:mi><mml:mo>→</mml:mo><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, to transfer annotations from the “pseudolabeled” data set <inline-formula id="il26"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL27" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">U</mml:mi></mml:mrow></mml:math></inline-formula> back to the labeled data set <inline-formula id="il27"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL28" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:math></inline-formula>. We then evaluate the “reverse accuracy” as the accuracy of the labels <inline-formula id="il28"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL29" display="inline" overflow="scroll"><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> for the held-out test portion of the labeled data set <inline-formula id="il29"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" id="IL30" display="inline" overflow="scroll"><mml:msup><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo>∗</mml:mo></mml:msup></mml:math></inline-formula>.</p>
      <p>We performed this procedure using a standard fivefold split for each parameter set. We computed the mean reverse cross-validation accuracy as the performance metric for robustness. For each method that we optimized, we selected the optimal set of hyperparameters as the set with the top reverse cross-validation accuracy.</p>
    </sec>
    <sec id="s3n">
      <title>New cell type discovery experiments</title>
      <sec id="s3n1">
        <title>New cell type discovery with pretrained models</title>
        <p>We evaluated the ability of scNym to highlight new cell types, unseen in the training data by predicting cell type annotations in the <italic toggle="yes">Tabula Muris</italic> brain data (Smart-seq2) using models trained on the 10x Genomics data from the 10 tissues noted above with the Smart-seq2 data as a corresponding target data set. No neurons or glia were present in the training or target set for this experiment. This experiment simulates the scenario in which a pretrained model has been fit to transfer across technologies (10x to Smart-seq2) and is later used to predict cell types in a new tissue, unseen in the original training or target data.</p>
        <p>We computed scNym confidence scores for each cell as <italic toggle="yes">c</italic><sub><italic toggle="yes">i</italic></sub> = max <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>, where <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub> is the model prediction probability vector for cell <italic toggle="yes">i</italic> as noted above. To highlight potential cell type discoveries, we set a simple threshold on these confidence scores, <italic toggle="yes">d</italic><sub><italic toggle="yes">i</italic></sub> = <italic toggle="yes">c</italic><sub><italic toggle="yes">i</italic></sub> ≤ 0.5, where <italic toggle="yes">d</italic><sub><italic toggle="yes">i</italic></sub> ∈ {0, 1} is a binary indicator variable. We found that scNym assigned low confidence to the majority of cells from newly “discovered” types unseen in the training set using this method.</p>
      </sec>
      <sec id="s3n2">
        <title>New cell type discovery with semisupervised training</title>
        <p>We also evaluated the ability of scNym to discover new cell types in a scenario in which new cell types are present in the target data used for semisupervised training. We used the same training data and target data as the experiment above, but we now introduce the <italic toggle="yes">Tabula Muris</italic> brain data (Smart-seq2) into the target data set during semisupervised training. We performed this experiment using our default scNym training procedure, as well as the modified new cell type discovery procedure described above.</p>
        <p>As above, we computed confidence scores for each cell and set a threshold of <italic toggle="yes">d</italic><sub><italic toggle="yes">i</italic></sub> = <italic toggle="yes">c</italic><sub><italic toggle="yes">i</italic></sub> ≤ 0.5 to identify potential new cell type discoveries. We found that scNym models trained with the new cell type discovery procedure provided low-confidence scores to the new cell types, suitable for identification of these new cells. We considered all new cell type predictions to be incorrect when computing accuracy for the new cell type discovery task.</p>
      </sec>
      <sec id="s3n3">
        <title>Clustering candidate new cell types</title>
        <p>We used a community detection procedure in the scNym embedding to suggest the number of distinct cell states represented by low-confidence cells. First, we identify cells with a confidence score lower than a threshold <italic toggle="yes">t</italic><sub>conf</sub> to highlight putative cell type discoveries, <italic toggle="yes">d</italic><sub><italic toggle="yes">i</italic></sub> = <italic toggle="yes">c</italic><sub><italic toggle="yes">i</italic></sub> &lt; <italic toggle="yes">t</italic><sub>conf</sub>. We then extract the scNym penultimate embedding activations for these low-confidence cells and construct a nearest-neighbor graph using the <italic toggle="yes">k</italic> = 15 nearest neighbors for each cell. We compute a Leiden community detection partition for a range of different resolution parameters, <italic toggle="yes">r</italic> ∈ {0.1, 0.2, 0.3, 0.5, 1.0}, and compute the Calinski–Harabasz score for each partition (<xref rid="GR268581KIMC7" ref-type="bibr">Calinski and Harabasz 1974)</xref>. We select the optimal partition in the scNym embedding as the partition generated with the maximum Calinski–Harabasz score and suggest that communities in this partition may each represent a distinct cell state.</p>
      </sec>
      <sec id="s3n4">
        <title>Discriminating candidate new cell types from other low-confidence predictions</title>
        <p>Cells may receive low-confidence predictions for multiple reasons, including (1) a cell is on the boundary between two cell types, (2) a cell has very little training data for the predicted class, and (3) the cell represents a new cell type unseen in the training data set. To discriminate between these possibilities, we use a heuristic similar to the one we use for proposing a number of new cell types that might be present. First, we extract the scNym embedding coordinates from the penultimate layer activations for all cells and build a nearest-neighbor graph. We then optimize a Leiden cluster partition by scanning different resolution parameters to maximize the Calinksi–Harabasz score. We then compute the average prediction confidence across all cells in each of the resulting clusters. We also visualize the number of cells present in the training data for each predicted cell type.</p>
        <p>We consider cells with low prediction scores within an otherwise high-confidence cluster to be on the boundary between cell types. These cells may benefit from domain expert review of the specific criteria to use when discriminating between very similar cell identities. We consider low-confidence cell clusters with few training examples for the predicted class to warrant further domain expert review. Low-confidence clusters that are predicted to be a class with ample training data may represent new cell types and also warrant further review.</p>
      </sec>
    </sec>
    <sec id="s3o">
      <title>Software availability</title>
      <p>Open-source code for our software and preprocessed reference data sets analyzed in this study are available in the scNym repository (<uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/calico/scnym">https://github.com/calico/scnym</uri>) and as <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://genome.cshlp.org/lookup/suppl/doi:10.1101/gr.268581.120/-/DC1" ext-link-type="uri">Supplemental Code</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="PMC_1" content-type="local-data">
      <caption>
        <title>Supplemental Material</title>
      </caption>
      <media mimetype="text" mime-subtype="html" xlink:href="supp_31_10_1781__DC1.html"/>
      <media xlink:role="associated-file" mimetype="application" mime-subtype="x-zip-compressed" xlink:href="supp_gr.268581.120_Supplemental_code.zip"/>
      <media xlink:role="associated-file" mimetype="application" mime-subtype="pdf" xlink:href="supp_gr.268581.120_Supplemental_Information.pdf"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgments</title>
    <p>We thank Zhenghao Chen, Amoolya H. Singh, and Han Yuan for helpful discussions and comments. Funding for this study was provided by Calico Life Sciences, LLC.</p>
    <p><italic toggle="yes">Author contributions:</italic> J.C.K. conceived the study, implemented software, conducted experiments, and wrote the paper. D.R.K. conceived the study and wrote the paper.</p>
  </ack>
  <fn-group>
    <fn fn-type="supplementary-material">
      <p>[Supplemental material is available for this article.]</p>
    </fn>
    <fn>
      <p>Article published online before print. Article, supplemental material, and publication date are at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.genome.org/cgi/doi/10.1101/gr.268581.120" ext-link-type="uri">https://www.genome.org/cgi/doi/10.1101/gr.268581.120</ext-link>.</p>
    </fn>
    <fn>
      <p>Freely available online through the <italic toggle="yes">Genome Research</italic> Open Access option.</p>
    </fn>
  </fn-group>
  <sec id="s5">
    <title>Competing interest statement</title>
    <p>J.C.K. and D.R.K. are paid employees of Calico Life Sciences, LLC.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="GR268581KIMC1">
      <mixed-citation publication-type="journal"><string-name><surname>Abdelaal</surname><given-names>T</given-names></string-name>, <string-name><surname>Michielsen</surname><given-names>L</given-names></string-name>, <string-name><surname>Cats</surname><given-names>D</given-names></string-name>, <string-name><surname>Hoogduin</surname><given-names>D</given-names></string-name>, <string-name><surname>Mei</surname><given-names>H</given-names></string-name>, <string-name><surname>Reinders</surname><given-names>MJT</given-names></string-name>, <string-name><surname>Mahfouz</surname><given-names>A</given-names></string-name>. <year>2019</year>. <article-title>A comparison of automatic cell identification methods for single-cell RNA sequencing data</article-title>. <source>Genome Biol</source><volume>20</volume>: <fpage>194</fpage>. <pub-id pub-id-type="doi">10.1186/s13059-019-1795-z</pub-id><pub-id pub-id-type="pmid">31500660</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC2">
      <mixed-citation publication-type="journal"><string-name><surname>Alquicira-Hernandez</surname><given-names>J</given-names></string-name>, <string-name><surname>Sathe</surname><given-names>A</given-names></string-name>, <string-name><surname>Ji</surname><given-names>HP</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>Q</given-names></string-name>, <string-name><surname>Powell</surname><given-names>JE</given-names></string-name>. <year>2019</year>. <article-title><italic toggle="yes">scPred</italic>: accurate supervised method for cell-type classification from single-cell RNA-seq data</article-title>. <source>Genome Biol</source><volume>20</volume>: <fpage>264</fpage>. <pub-id pub-id-type="doi">10.1186/s13059-019-1862-5</pub-id><pub-id pub-id-type="pmid">31829268</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC3">
      <mixed-citation publication-type="journal"><string-name><surname>Andrews</surname><given-names>TS</given-names></string-name>, <string-name><surname>Hemberg</surname><given-names>M</given-names></string-name>. <year>2019</year>. <article-title>M3Drop: dropout-based feature selection for scRNASeq</article-title>. <source>Bioinformatics</source><volume>35</volume>: <fpage>2865</fpage>–<lpage>2867</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty1044</pub-id><pub-id pub-id-type="pmid">30590489</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC4">
      <mixed-citation publication-type="journal"><string-name><surname>Angelidis</surname><given-names>I</given-names></string-name>, <string-name><surname>Simon</surname><given-names>LM</given-names></string-name>, <string-name><surname>Fernandez</surname><given-names>IE</given-names></string-name>, <string-name><surname>Strunz</surname><given-names>M</given-names></string-name>, <string-name><surname>Mayr</surname><given-names>CH</given-names></string-name>, <string-name><surname>Greiffo</surname><given-names>FR</given-names></string-name>, <string-name><surname>Tsitsiridis</surname><given-names>G</given-names></string-name>, <string-name><surname>Ansari</surname><given-names>M</given-names></string-name>, <string-name><surname>Graf</surname><given-names>E</given-names></string-name>, <string-name><surname>Strom</surname><given-names>TM</given-names></string-name>, <etal/><year>2019</year>. <article-title>An atlas of the aging lung mapped by single cell transcriptomics and deep tissue proteomics</article-title>. <source>Nat Commun</source><volume>10</volume>: <fpage>963</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-019-08831-9</pub-id><pub-id pub-id-type="pmid">30814501</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC5">
      <mixed-citation publication-type="journal"><string-name><surname>Becht</surname><given-names>E</given-names></string-name>, <string-name><surname>McInnes</surname><given-names>L</given-names></string-name>, <string-name><surname>Healy</surname><given-names>J</given-names></string-name>, <string-name><surname>Dutertre</surname><given-names>CA</given-names></string-name>, <string-name><surname>Kwok</surname><given-names>IWH</given-names></string-name>, <string-name><surname>Ng</surname><given-names>LG</given-names></string-name>, <string-name><surname>Ginhoux</surname><given-names>F</given-names></string-name>, <string-name><surname>Newell</surname><given-names>EW</given-names></string-name>. <year>2019</year>. <article-title>Dimensionality reduction for visualizing single-cell data using UMAP</article-title>. <source>Nat Biotechnol</source><volume>37</volume>: <fpage>38</fpage>–<lpage>44</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.4314</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC6">
      <mixed-citation publication-type="journal"><string-name><surname>Berthelot</surname><given-names>D</given-names></string-name>, <string-name><surname>Carlini</surname><given-names>N</given-names></string-name>, <string-name><surname>Goodfellow</surname><given-names>I</given-names></string-name>, <string-name><surname>Papernot</surname><given-names>N</given-names></string-name>, <string-name><surname>Oliver</surname><given-names>A</given-names></string-name>, <string-name><surname>Raffel</surname><given-names>CA</given-names></string-name>. <year>2019</year>. <article-title>MixMatch: a holistic approach to semi-supervised learning</article-title>. <source>Advances in Neural Information Processing Systems</source><volume>32</volume>: <fpage>5049</fpage>–<lpage>5059</lpage>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC7">
      <mixed-citation publication-type="journal"><string-name><surname>Calinski</surname><given-names>T</given-names></string-name>, <string-name><surname>Harabasz</surname><given-names>J</given-names></string-name>. <year>1974</year>. <article-title>A dendrite method for cluster analysis</article-title>. <source>Commun Stat</source><volume>3</volume>: <fpage>1</fpage>–<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1080/03610927408827101</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC8">
      <mixed-citation publication-type="journal"><string-name><surname>de Kanter</surname><given-names>JK</given-names></string-name>, <string-name><surname>Lijnzaad</surname><given-names>P</given-names></string-name>, <string-name><surname>Candelli</surname><given-names>T</given-names></string-name>, <string-name><surname>Margaritis</surname><given-names>T</given-names></string-name>, <string-name><surname>Holstege</surname><given-names>FCP</given-names></string-name>. <year>2019</year>. <article-title>CHETAH: a selective, hierarchical cell type identification method for single-cell RNA sequencing</article-title>. <source>Nucleic Acids Res</source><volume>47</volume>: <fpage>e95</fpage>. <pub-id pub-id-type="doi">10.1093/nar/gkz543</pub-id><pub-id pub-id-type="pmid">31226206</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC9">
      <mixed-citation publication-type="journal"><string-name><surname>Denisenko</surname><given-names>E</given-names></string-name>, <string-name><surname>Guo</surname><given-names>BB</given-names></string-name>, <string-name><surname>Jones</surname><given-names>M</given-names></string-name>, <string-name><surname>Hou</surname><given-names>R</given-names></string-name>, <string-name><surname>de Kock</surname><given-names>L</given-names></string-name>, <string-name><surname>Lassmann</surname><given-names>T</given-names></string-name>, <string-name><surname>Poppe</surname><given-names>D</given-names></string-name>, <string-name><surname>Clément</surname><given-names>O</given-names></string-name>, <string-name><surname>Simmons</surname><given-names>RK</given-names></string-name>, <string-name><surname>Lister</surname><given-names>R</given-names></string-name>, <etal/><year>2020</year>. <article-title>Systematic assessment of tissue dissociation and storage biases in single-cell and single-nucleus RNA-seq workflows</article-title>. <source>Genome Biol</source><volume>21</volume>: <fpage>130</fpage>. <pub-id pub-id-type="doi">10.1186/s13059-020-02048-6</pub-id><pub-id pub-id-type="pmid">32487174</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC11">
      <mixed-citation publication-type="journal"><string-name><surname>Ding</surname><given-names>J</given-names></string-name>, <string-name><surname>Adiconis</surname><given-names>X</given-names></string-name>, <string-name><surname>Simmons</surname><given-names>SK</given-names></string-name>, <string-name><surname>Kowalczyk</surname><given-names>MS</given-names></string-name>, <string-name><surname>Hession</surname><given-names>CC</given-names></string-name>, <string-name><surname>Marjanovic</surname><given-names>ND</given-names></string-name>, <string-name><surname>Hughes</surname><given-names>TK</given-names></string-name>, <string-name><surname>Wadsworth</surname><given-names>MH</given-names></string-name>, <string-name><surname>Burks</surname><given-names>T</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>LT</given-names></string-name>, <etal/><year>2020</year>. <article-title>Systematic comparison of single-cell and single-nucleus RNA-sequencing methods</article-title>. <source>Nat Biotechnol</source><volume>38</volume>: <fpage>737</fpage>–<lpage>746</lpage>. <pub-id pub-id-type="doi">10.1038/s41587-020-0465-8</pub-id><pub-id pub-id-type="pmid">32341560</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC12">
      <mixed-citation publication-type="journal"><string-name><surname>Dixit</surname><given-names>A</given-names></string-name>, <string-name><surname>Parnas</surname><given-names>O</given-names></string-name>, <string-name><surname>Li</surname><given-names>B</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J</given-names></string-name>, <string-name><surname>Fulco</surname><given-names>CP</given-names></string-name>, <string-name><surname>Jerby-Arnon</surname><given-names>L</given-names></string-name>, <string-name><surname>Marjanovic</surname><given-names>ND</given-names></string-name>, <string-name><surname>Dionne</surname><given-names>D</given-names></string-name>, <string-name><surname>Burks</surname><given-names>T</given-names></string-name>, <string-name><surname>Raychowdhury</surname><given-names>R</given-names></string-name>, <etal/><year>2016</year>. <article-title>Perturb-Seq: dissecting molecular circuits with scalable single-cell RNA profiling of pooled genetic screens</article-title>. <source>Cell</source><volume>167</volume>: <fpage>1853</fpage>–<lpage>1866.e17</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2016.11.038</pub-id><pub-id pub-id-type="pmid">27984732</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC13">
      <mixed-citation publication-type="journal"><string-name><surname>Ganin</surname><given-names>Y</given-names></string-name>, <string-name><surname>Ustinova</surname><given-names>E</given-names></string-name>, <string-name><surname>Ajakan</surname><given-names>H</given-names></string-name>, <string-name><surname>Germain</surname><given-names>P</given-names></string-name>, <string-name><surname>Larochelle</surname><given-names>H</given-names></string-name>, <string-name><surname>Laviolette</surname><given-names>F</given-names></string-name>, <string-name><surname>Marchand</surname><given-names>M</given-names></string-name>, <string-name><surname>Lempitsky</surname><given-names>V</given-names></string-name>. <year>2016</year>. <article-title>Domain-adversarial training of neural networks</article-title>. <source>J Mach Learn Res</source><volume>17</volume>: <fpage>1</fpage>–<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC14">
      <mixed-citation publication-type="confproc"><string-name><surname>Guo</surname><given-names>C</given-names></string-name>, <string-name><surname>Pleiss</surname><given-names>G</given-names></string-name>, <string-name><surname>Sun</surname><given-names>Y</given-names></string-name>, <string-name><surname>Weinberger</surname><given-names>KQ</given-names></string-name>. <year>2017</year>. <article-title>On calibration of modern neural networks</article-title>. In <conf-name>Proceedings of the 34th International Conference on Machine Learning</conf-name><volume>70</volume>: <fpage>1321</fpage>–<lpage>1330</lpage>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://JMLR.org">JMLR.org</uri>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC15">
      <mixed-citation publication-type="journal"><string-name><surname>Habermann</surname><given-names>AC</given-names></string-name>, <string-name><surname>Gutierrez</surname><given-names>AJ</given-names></string-name>, <string-name><surname>Bui</surname><given-names>LT</given-names></string-name>, <string-name><surname>Yahn</surname><given-names>SL</given-names></string-name>, <string-name><surname>Winters</surname><given-names>NI</given-names></string-name>, <string-name><surname>Calvi</surname><given-names>CL</given-names></string-name>, <string-name><surname>Peter</surname><given-names>L</given-names></string-name>, <string-name><surname>Chung</surname><given-names>MI</given-names></string-name>, <string-name><surname>Taylor</surname><given-names>CJ</given-names></string-name>, <string-name><surname>Jetter</surname><given-names>C</given-names></string-name>, <etal/><year>2020</year>. <article-title>Single-cell RNA sequencing reveals profibrotic roles of distinct epithelial and mesenchymal lineages in pulmonary fibrosis</article-title>. <source>Sci Adv</source><volume>6</volume>: <fpage>eaba1972</fpage>. <pub-id pub-id-type="doi">10.1126/sciadv.aba1972</pub-id><pub-id pub-id-type="pmid">32832598</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC16">
      <mixed-citation publication-type="journal"><string-name><surname>Han</surname><given-names>X</given-names></string-name>, <string-name><surname>Wang</surname><given-names>R</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Y</given-names></string-name>, <string-name><surname>Fei</surname><given-names>L</given-names></string-name>, <string-name><surname>Sun</surname><given-names>H</given-names></string-name>, <string-name><surname>Lai</surname><given-names>S</given-names></string-name>, <string-name><surname>Saadatpour</surname><given-names>A</given-names></string-name>, <string-name><surname>Zhou</surname><given-names>Z</given-names></string-name>, <string-name><surname>Chen</surname><given-names>H</given-names></string-name>, <string-name><surname>Ye</surname><given-names>F</given-names></string-name>, <etal/><year>2018</year>. <article-title>Mapping the mouse cell atlas by microwell-Seq</article-title>. <source>Cell</source><volume>173</volume>: <fpage>1307</fpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2018.05.012</pub-id><pub-id pub-id-type="pmid">29775597</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC17">
      <mixed-citation publication-type="other"><string-name><surname>Hinton</surname><given-names>G</given-names></string-name>, <string-name><surname>Vinyals</surname><given-names>O</given-names></string-name>, <string-name><surname>Dean</surname><given-names>J</given-names></string-name>. <year>2015</year>. <article-title>Distilling the knowledge in a neural network</article-title>. <comment>arXiv:1503.02531 [stat.ML]</comment>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC18">
      <mixed-citation publication-type="confproc"><string-name><surname>Ioffe</surname><given-names>S</given-names></string-name>, <string-name><surname>Szegedy</surname><given-names>C</given-names></string-name>. <year>2015</year>. <article-title>Batch normalization: accelerating deep network training by reducing internal covariate shift</article-title>. In <conf-name>Proceedings of the 32th International Conference on Machine Learning</conf-name><volume>37</volume>: <fpage>448</fpage>–<lpage>456</lpage>. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://JMLR.org">JMLR.org</uri>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC19">
      <mixed-citation publication-type="journal"><string-name><surname>Kang</surname><given-names>HM</given-names></string-name>, <string-name><surname>Subramaniam</surname><given-names>M</given-names></string-name>, <string-name><surname>Targ</surname><given-names>S</given-names></string-name>, <string-name><surname>Nguyen</surname><given-names>M</given-names></string-name>, <string-name><surname>Maliskova</surname><given-names>L</given-names></string-name>, <string-name><surname>McCarthy</surname><given-names>E</given-names></string-name>, <string-name><surname>Wan</surname><given-names>E</given-names></string-name>, <string-name><surname>Wong</surname><given-names>S</given-names></string-name>, <string-name><surname>Byrnes</surname><given-names>L</given-names></string-name>, <string-name><surname>Lanata</surname><given-names>CM</given-names></string-name>, <etal/><year>2018</year>. <article-title>Multiplexed droplet single-cell RNA-sequencing using natural genetic variation</article-title>. <source>Nat Biotechnol</source><volume>36</volume>: <fpage>89</fpage>–<lpage>94</lpage>. <pub-id pub-id-type="doi">10.1038/nbt.4042</pub-id><pub-id pub-id-type="pmid">29227470</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC20">
      <mixed-citation publication-type="journal"><string-name><surname>Kimmel</surname><given-names>JC</given-names></string-name>, <string-name><surname>Penland</surname><given-names>L</given-names></string-name>, <string-name><surname>Rubinstein</surname><given-names>ND</given-names></string-name>, <string-name><surname>Hendrickson</surname><given-names>DG</given-names></string-name>, <string-name><surname>Kelley</surname><given-names>DR</given-names></string-name>, <string-name><surname>Rosenthal</surname><given-names>AZ</given-names></string-name>. <year>2019</year>. <article-title>Murine single-cell RNA-seq reveals cell-identity- and tissue-specific trajectories of aging</article-title>. <source>Genome Res</source><volume>29</volume>: <fpage>2088</fpage>–<lpage>2103</lpage>. <pub-id pub-id-type="doi">10.1101/gr.253880.119</pub-id><pub-id pub-id-type="pmid">31754020</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC21">
      <mixed-citation publication-type="book"><string-name><surname>Kingma</surname><given-names>DP</given-names></string-name>, <string-name><surname>Mohamed</surname><given-names>S</given-names></string-name>, <string-name><surname>Jimenez Rezende</surname><given-names>D</given-names></string-name>, <string-name><surname>Welling</surname><given-names>M</given-names></string-name>. <year>2014</year>. <article-title>Semi-supervised learning with deep generative models</article-title>. In <source>Advances in neural information processing systems</source> (ed. <string-name><surname>Ghahramani</surname><given-names>Z</given-names></string-name>, <etal/>). <publisher-name>Curran Associates</publisher-name>, <publisher-loc>Red Hook, NY</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC22">
      <mixed-citation publication-type="journal"><string-name><surname>Kiselev</surname><given-names>VY</given-names></string-name>, <string-name><surname>Yiu</surname><given-names>A</given-names></string-name>, <string-name><surname>Hemberg</surname><given-names>M</given-names></string-name>. <year>2018</year>. <article-title>scmap: projection of single-cell RNA-seq data across data sets</article-title>. <source>Nat Methods</source><volume>15</volume>: <fpage>359</fpage>–<lpage>362</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.4644</pub-id><pub-id pub-id-type="pmid">29608555</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC23">
      <mixed-citation publication-type="journal"><string-name><surname>Korsunsky</surname><given-names>I</given-names></string-name>, <string-name><surname>Millard</surname><given-names>N</given-names></string-name>, <string-name><surname>Fan</surname><given-names>J</given-names></string-name>, <string-name><surname>Slowikowski</surname><given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>F</given-names></string-name>, <string-name><surname>Wei</surname><given-names>K</given-names></string-name>, <string-name><surname>Baglaenko</surname><given-names>Y</given-names></string-name>, <string-name><surname>Brenner</surname><given-names>M</given-names></string-name>, <string-name><surname>Loh</surname><given-names>PR</given-names></string-name>, <string-name><surname>Raychaudhuri</surname><given-names>S</given-names></string-name>. <year>2019</year>. <article-title>Fast, sensitive and accurate integration of single-cell data with harmony</article-title>. <source>Nat Methods</source><volume>16</volume>: <fpage>1289</fpage>–<lpage>1296</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0619-0</pub-id><pub-id pub-id-type="pmid">31740819</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC24">
      <mixed-citation publication-type="confproc"><string-name><surname>Lee</surname><given-names>DH</given-names></string-name>. <year>2013</year>. <article-title>Pseudo-label: the simple and efficient semi-supervised learning method for deep neural networks</article-title>. In <conf-name>International Conference on Machine Learning (ICML) 2013 Workshop: Challenges in Representation Learning (WREPL)</conf-name>. ICML, Atlanta.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC25">
      <mixed-citation publication-type="journal"><string-name><surname>Lopez</surname><given-names>R</given-names></string-name>, <string-name><surname>Regier</surname><given-names>J</given-names></string-name>, <string-name><surname>Cole</surname><given-names>MB</given-names></string-name>, <string-name><surname>Jordan</surname><given-names>MI</given-names></string-name>, <string-name><surname>Yosef</surname><given-names>N</given-names></string-name>. <year>2018</year>. <article-title>Deep generative modeling for single-cell transcriptomics</article-title>. <source>Nat Methods</source><volume>15</volume>: <fpage>1053</fpage>–<lpage>1058</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0229-2</pub-id><pub-id pub-id-type="pmid">30504886</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC26">
      <mixed-citation publication-type="journal"><string-name><surname>Ma</surname><given-names>S</given-names></string-name>, <string-name><surname>Sun</surname><given-names>S</given-names></string-name>, <string-name><surname>Geng</surname><given-names>L</given-names></string-name>, <string-name><surname>Song</surname><given-names>M</given-names></string-name>, <string-name><surname>Wang</surname><given-names>W</given-names></string-name>, <string-name><surname>Ye</surname><given-names>Y</given-names></string-name>, <string-name><surname>Ji</surname><given-names>Q</given-names></string-name>, <string-name><surname>Zou</surname><given-names>Z</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S</given-names></string-name>, <string-name><surname>He</surname><given-names>X</given-names></string-name>, <etal/><year>2020</year>. <article-title>Caloric restriction reprograms the single-cell transcriptional landscape of <italic toggle="yes">Rattus norvegicus</italic> aging</article-title>. <source>Cell</source><volume>180</volume>: <fpage>984</fpage>–<lpage>1001</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2020.02.008</pub-id><pub-id pub-id-type="pmid">32109414</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC27">
      <mixed-citation publication-type="other"><string-name><surname>McInnes</surname><given-names>L</given-names></string-name>, <string-name><surname>Healy</surname><given-names>J</given-names></string-name>, <string-name><surname>Melville</surname><given-names>J</given-names></string-name>. <year>2020</year>. <article-title>UMAP: Uniform Manifold Approximation and Projection for dimension reduction</article-title>. <source>arXiv</source>:<comment>1802.03426 [stat.ML]</comment>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC28">
      <mixed-citation publication-type="book"><string-name><surname>Oliver</surname><given-names>A</given-names></string-name>, <string-name><surname>Odena</surname><given-names>A</given-names></string-name>, <string-name><surname>Raffel</surname><given-names>CA</given-names></string-name>, <string-name><surname>Cubuk</surname><given-names>ED</given-names></string-name>, <string-name><surname>Goodfellow</surname><given-names>I</given-names></string-name>. <year>2018</year>. <article-title>Realistic evaluation of deep semi-supervised learning algorithms</article-title>. In <source>Advances in neural information processing systems</source>, Vol. <volume>31</volume> (ed. <string-name><surname>Bengio</surname><given-names>S</given-names></string-name>, <etal/>), pp. <fpage>3235</fpage>–<lpage>3246</lpage>. <publisher-name>Curran Associates, Red Hook, NY</publisher-name>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC29">
      <mixed-citation publication-type="book"><string-name><surname>Platt</surname><given-names>JC</given-names></string-name>. <year>1999</year>. <article-title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</article-title>. In <source>Advances in large margin classifiers</source> (ed. <string-name><surname>Smola</surname><given-names>AJ</given-names></string-name>, <etal/>), pp. <fpage>61</fpage>–<lpage>74</lpage>. <publisher-name>MIT Press</publisher-name>, <publisher-loc>Cambridge, MA</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC30">
      <mixed-citation publication-type="journal"><string-name><surname>Pliner</surname><given-names>HA</given-names></string-name>, <string-name><surname>Shendure</surname><given-names>J</given-names></string-name>, <string-name><surname>Trapnell</surname><given-names>C</given-names></string-name>. <year>2019</year>. <article-title>Supervised classification enables rapid annotation of cell atlases</article-title>. <source>Nat Methods</source><volume>16</volume>: <fpage>983</fpage>–<lpage>986</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0535-3</pub-id><pub-id pub-id-type="pmid">31501545</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC31">
      <mixed-citation publication-type="book"><string-name><surname>Sohn</surname><given-names>K</given-names></string-name>, <string-name><surname>Berthelot</surname><given-names>D</given-names></string-name>, <string-name><surname>Li</surname><given-names>CL</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Z</given-names></string-name>, <string-name><surname>Carlini</surname><given-names>N</given-names></string-name>, <string-name><surname>Cubuk</surname><given-names>ED</given-names></string-name>, <string-name><surname>Kurakin</surname><given-names>A</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>H</given-names></string-name>, <string-name><surname>Raffel</surname><given-names>C</given-names></string-name>. <year>2020</year>. <article-title>FixMatch: simplifying semi-supervised learning with consistency and confidence</article-title>. In <source>Advances in neural information processing systems</source> (ed. <string-name><surname>Larochelle</surname><given-names>H</given-names></string-name>, <etal/>), pp. <fpage>596</fpage>–<lpage>608</lpage>. <publisher-name>Curran Associates</publisher-name>, <publisher-loc>Red Hook, NY</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC32">
      <mixed-citation publication-type="journal"><string-name><surname>Srivastava</surname><given-names>N</given-names></string-name>, <string-name><surname>Hinton</surname><given-names>GE</given-names></string-name>, <string-name><surname>Krizhevsky</surname><given-names>A</given-names></string-name>. <year>2014</year>. <article-title>Dropout: a simple way to prevent neural networks from overfitting</article-title>. <source>J Mach Learn Res</source><volume>15</volume>: <fpage>1929</fpage>–<lpage>1958</lpage>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC33">
      <mixed-citation publication-type="journal"><string-name><surname>Srivatsan</surname><given-names>SR</given-names></string-name>, <string-name><surname>McFaline-Figueroa</surname><given-names>JL</given-names></string-name>, <string-name><surname>Ramani</surname><given-names>V</given-names></string-name>, <string-name><surname>Saunders</surname><given-names>L</given-names></string-name>, <string-name><surname>Cao</surname><given-names>J</given-names></string-name>, <string-name><surname>Packer</surname><given-names>J</given-names></string-name>, <string-name><surname>Pliner</surname><given-names>HA</given-names></string-name>, <string-name><surname>Jackson</surname><given-names>DL</given-names></string-name>, <string-name><surname>Daza</surname><given-names>RM</given-names></string-name>, <string-name><surname>Christiansen</surname><given-names>L</given-names></string-name>, <etal/><year>2020</year>. <article-title>Massively multiplex chemical transcriptomics at single cell resolution.</article-title><source>Science</source><volume>367</volume>: <fpage>45</fpage>–<lpage>51</lpage>. <pub-id pub-id-type="doi">10.1126/science.aax6234</pub-id><pub-id pub-id-type="pmid">31806696</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC34">
      <mixed-citation publication-type="journal"><string-name><surname>Stuart</surname><given-names>T</given-names></string-name>, <string-name><surname>Butler</surname><given-names>A</given-names></string-name>, <string-name><surname>Hoffman</surname><given-names>P</given-names></string-name>, <string-name><surname>Hafemeister</surname><given-names>C</given-names></string-name>, <string-name><surname>Papalexi</surname><given-names>E</given-names></string-name>, <string-name><surname>Mauck</surname><given-names>WM</given-names><suffix>III</suffix></string-name>, <string-name><surname>Hao</surname><given-names>Y</given-names></string-name>, <string-name><surname>Stoeckius</surname><given-names>M</given-names></string-name>, <string-name><surname>Smibert</surname><given-names>P</given-names></string-name>, <string-name><surname>Satija</surname><given-names>R</given-names></string-name>. <year>2019</year>. <article-title>Comprehensive integration of single-cell data</article-title>. <source>Cell</source><volume>177</volume>: <fpage>1888</fpage>–<lpage>1902.e1</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2019.05.031</pub-id><pub-id pub-id-type="pmid">31178118</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC35">
      <mixed-citation publication-type="confproc"><string-name><surname>Sundararajan</surname><given-names>M</given-names></string-name>, <string-name><surname>Taly</surname><given-names>A</given-names></string-name>, <string-name><surname>Yan</surname><given-names>Q</given-names></string-name>. <year>2017</year>. <article-title>Axiomatic attribution for deep networks</article-title>. In <conf-name>Proceedings of the 34th International Conference on Machine Learning</conf-name>, <comment>Proceedings of Machine Learning Research</comment>, <conf-loc>Sydney, Australia</conf-loc> (ed. <string-name><surname>Precup</surname><given-names>D</given-names></string-name>, <string-name><surname>Teh</surname><given-names>YW</given-names></string-name>), Vol. <volume>70</volume>, pp. <fpage>3319</fpage>–<lpage>3328</lpage>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC36">
      <mixed-citation publication-type="journal"><string-name><surname>Svensson</surname><given-names>V</given-names></string-name>, <string-name><surname>Vento-Tormo</surname><given-names>R</given-names></string-name>, <string-name><surname>Teichmann</surname><given-names>SA</given-names></string-name>. <year>2018</year>. <article-title>Exponential scaling of single-cell RNA-seq in the past decade</article-title>. <source>Nat Protoc</source><volume>13</volume>: <fpage>599</fpage>–<lpage>604</lpage>. <pub-id pub-id-type="doi">10.1038/nprot.2017.149</pub-id><pub-id pub-id-type="pmid">29494575</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC37">
      <mixed-citation publication-type="journal"><string-name><surname>Svensson</surname><given-names>V</given-names></string-name>, <string-name><surname>da Veiga Beltrame</surname><given-names>E</given-names></string-name>, <string-name><surname>Pachter</surname><given-names>L</given-names></string-name>. <year>2020</year>. <article-title>A curated database reveals trends in single-cell transcriptomics</article-title>. <source>Database</source><volume>2020</volume>: <fpage>Baaa073</fpage>. <pub-id pub-id-type="doi">10.1093/database/baaa073</pub-id><pub-id pub-id-type="pmid">33247933</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC38">
      <mixed-citation publication-type="journal"><collab>The Tabula Muris Consortium</collab>. <year>2018</year>. <article-title>Single-cell transcriptomics of 20 mouse organs creates a tabula muris</article-title>. <source>Nature</source><volume>562</volume>: <fpage>367</fpage>–<lpage>372</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-018-0590-4</pub-id><pub-id pub-id-type="pmid">30283141</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC39">
      <mixed-citation publication-type="journal"><string-name><surname>Tan</surname><given-names>Y</given-names></string-name>, <string-name><surname>Cahan</surname><given-names>P</given-names></string-name>. <year>2019</year>. <article-title>SingleCellNet: a computational tool to classify single cell RNA-Seq data across platforms and across species</article-title>. <source>Cell Syst</source><volume>9</volume>: <fpage>207</fpage>–<lpage>213.e2</lpage>. <pub-id pub-id-type="doi">10.1016/j.cels.2019.06.004</pub-id><pub-id pub-id-type="pmid">31377170</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC40">
      <mixed-citation publication-type="journal"><string-name><surname>Tanay</surname><given-names>A</given-names></string-name>, <string-name><surname>Regev</surname><given-names>A</given-names></string-name>. <year>2017</year>. <article-title>Scaling single-cell genomics from phenomenology to mechanism</article-title>. <source>Nature</source><volume>541</volume>: <fpage>331</fpage>–<lpage>338</lpage>. <pub-id pub-id-type="doi">10.1038/nature21350</pub-id><pub-id pub-id-type="pmid">28102262</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC41">
      <mixed-citation publication-type="journal"><string-name><surname>Thulasidasan</surname><given-names>S</given-names></string-name>, <string-name><surname>Chennupati</surname><given-names>G</given-names></string-name>, <string-name><surname>Bilmes</surname><given-names>JA</given-names></string-name>, <string-name><surname>Bhattacharya</surname><given-names>T</given-names></string-name>, <string-name><surname>Michalak</surname><given-names>S</given-names></string-name>. <year>2019</year>. <article-title>On mixup training: improved calibration and predictive uncertainty for deep neural networks</article-title>. <source>Advances in Neural Information Processing Systems</source><volume>32</volume>: <fpage>13888</fpage>–<lpage>13899</lpage>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC42">
      <mixed-citation publication-type="journal"><string-name><surname>Tran</surname><given-names>HTN</given-names></string-name>, <string-name><surname>Ang</surname><given-names>KS</given-names></string-name>, <string-name><surname>Chevrier</surname><given-names>M</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>X</given-names></string-name>, <string-name><surname>Lee</surname><given-names>NYS</given-names></string-name>, <string-name><surname>Goh</surname><given-names>M</given-names></string-name>, <string-name><surname>Chen</surname><given-names>J</given-names></string-name>. <year>2020</year>. <article-title>A benchmark of batch-effect correction methods for single-cell RNA sequencing data</article-title>. <source>Genome Biol</source><volume>21</volume>: <fpage>12</fpage>. <pub-id pub-id-type="doi">10.1186/s13059-019-1850-9</pub-id><pub-id pub-id-type="pmid">31948481</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC43">
      <mixed-citation publication-type="journal"><string-name><surname>Trapnell</surname><given-names>C</given-names></string-name>. <year>2015</year>. <article-title>Defining cell types and states with single-cell genomics</article-title>. <source>Genome Res</source><volume>25</volume>: <fpage>1491</fpage>–<lpage>1498</lpage>. <pub-id pub-id-type="doi">10.1101/gr.190595.115</pub-id><pub-id pub-id-type="pmid">26430159</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC44">
      <mixed-citation publication-type="confproc"><string-name><surname>Verma</surname><given-names>V</given-names></string-name>, <string-name><surname>Lamb</surname><given-names>A</given-names></string-name>, <string-name><surname>Kannala</surname><given-names>J</given-names></string-name>, <string-name><surname>Bengio</surname><given-names>Y</given-names></string-name>, <string-name><surname>Lopez-Paz</surname><given-names>D</given-names></string-name>. <year>2019</year>. <article-title>Interpolation consistency training for semi-supervised learning</article-title>. In <conf-name>Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI'19</conf-name>, pp. <fpage>3635</fpage>–<lpage>3641</lpage>. <publisher-name>AAAI Press, Palo Alto, CA</publisher-name>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC45">
      <mixed-citation publication-type="journal"><string-name><surname>Wilson</surname><given-names>G</given-names></string-name>, <string-name><surname>Cook</surname><given-names>DJ</given-names></string-name>. <year>2020</year>. <article-title>A survey of unsupervised deep domain adaptation</article-title>. <source>ACM Trans Intell Syst Technol</source><volume>11</volume>: <fpage>1</fpage>–<lpage>46</lpage>. <pub-id pub-id-type="doi">10.1145/3400066</pub-id><pub-id pub-id-type="pmid">34336374</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC46">
      <mixed-citation publication-type="journal"><string-name><surname>Xu</surname><given-names>C</given-names></string-name>, <string-name><surname>Lopez</surname><given-names>R</given-names></string-name>, <string-name><surname>Mehlman</surname><given-names>E</given-names></string-name>, <string-name><surname>Regier</surname><given-names>J</given-names></string-name>, <string-name><surname>Jordan</surname><given-names>MI</given-names></string-name>, <string-name><surname>Yosef</surname><given-names>N</given-names></string-name>. <year>2021</year>. <article-title>Probabilistic harmonization and annotation of single-cell transcriptomics data with deep generative models</article-title>. <source>Mol Syst Biol</source><volume>17</volume>: <fpage>e9620.</fpage><pub-id pub-id-type="doi">10.15252/msb.20209620</pub-id><pub-id pub-id-type="pmid">33491336</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC47">
      <mixed-citation publication-type="other"><string-name><surname>Zeiler</surname><given-names>MD</given-names></string-name>. <year>2012</year>. <article-title>ADADELTA: an adaptive learning rate method</article-title>. <source>arXiv</source>:<comment>1212.5701 [cs.LG]</comment>.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC48">
      <mixed-citation publication-type="confproc"><string-name><surname>Zhang</surname><given-names>H</given-names></string-name>, <string-name><surname>Cisse</surname><given-names>M</given-names></string-name>, <string-name><surname>Dauphin</surname><given-names>YN</given-names></string-name>, <string-name><surname>Lopez-Paz</surname><given-names>D</given-names></string-name>. <year>2018</year>. <article-title>Mixup: beyond empirical risk minimization</article-title>. In <conf-name>International Conference on Learning Representations</conf-name>, Stockholm.</mixed-citation>
    </ref>
    <ref id="GR268581KIMC49">
      <mixed-citation publication-type="journal"><string-name><surname>Zhang</surname><given-names>AW</given-names></string-name>, <string-name><surname>O'Flanagan</surname><given-names>C</given-names></string-name>, <string-name><surname>Chavez</surname><given-names>EA</given-names></string-name>, <string-name><surname>Lim</surname><given-names>JLP</given-names></string-name>, <string-name><surname>Ceglia</surname><given-names>N</given-names></string-name>, <string-name><surname>McPherson</surname><given-names>A</given-names></string-name>, <string-name><surname>Wiens</surname><given-names>M</given-names></string-name>, <string-name><surname>Walters</surname><given-names>P</given-names></string-name>, <string-name><surname>Chan</surname><given-names>T</given-names></string-name>, <string-name><surname>Hewitson</surname><given-names>B</given-names></string-name>, <etal/><year>2019</year>. <article-title>Probabilistic cell-type assignment of single-cell RNA-seq for tumor microenvironment profiling</article-title>. <source>Nat Methods</source><volume>16</volume>: <fpage>1007</fpage>–<lpage>1015</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0529-1</pub-id><pub-id pub-id-type="pmid">31501550</pub-id></mixed-citation>
    </ref>
    <ref id="GR268581KIMC50">
      <mixed-citation publication-type="book"><string-name><surname>Zhong</surname><given-names>E</given-names></string-name>, <string-name><surname>Fan</surname><given-names>W</given-names></string-name>, <string-name><surname>Yang</surname><given-names>Q</given-names></string-name>, <string-name><surname>Verscheure</surname><given-names>O</given-names></string-name>, <string-name><surname>Ren</surname><given-names>J</given-names></string-name>. <year>2010</year>. <article-title>Cross validation framework to choose amongst models and datasets for transfer learning</article-title>. In <source>Machine learning and knowledge discovery in databases, lecture notes in computer science</source> (ed. <string-name><surname>Balcázar</surname><given-names>JL</given-names></string-name>, <etal/>), pp. <fpage>547</fpage>–<lpage>562</lpage>. <publisher-name>Springer</publisher-name>, <publisher-loc>Berlin</publisher-loc>.</mixed-citation>
    </ref>
  </ref-list>
</back>
