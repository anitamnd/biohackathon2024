<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with OASIS Tables with MathML3 v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-archive-oasis-article1-mathml3.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats-oasis2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Magn Reson Imaging</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Magn Reson Imaging</journal-id>
    <journal-id journal-id-type="doi">10.1002/(ISSN)1522-2586</journal-id>
    <journal-id journal-id-type="publisher-id">JMRI</journal-id>
    <journal-title-group>
      <journal-title>Journal of Magnetic Resonance Imaging</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1053-1807</issn>
    <issn pub-type="epub">1522-2586</issn>
    <publisher>
      <publisher-name>John Wiley &amp; Sons, Inc.</publisher-name>
      <publisher-loc>Hoboken, USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8360022</article-id>
    <article-id pub-id-type="pmid">33719168</article-id>
    <article-id pub-id-type="doi">10.1002/jmri.27599</article-id>
    <article-id pub-id-type="publisher-id">JMRI27599</article-id>
    <article-categories>
      <subj-group subj-group-type="overline">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="heading">
        <subject>Research Articles</subject>
        <subj-group subj-group-type="heading">
          <subject>Pelvis</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A Deep Learning Approach to Diagnostic Classification of Prostate Cancer Using Pathology–Radiology Fusion</article-title>
      <alt-title alt-title-type="right-running-head">Automated Diagnosis of <styled-content style="fixed-case" toggle="no">PCa</styled-content> Using <styled-content style="fixed-case" toggle="no">AI</styled-content>
</alt-title>
      <alt-title alt-title-type="left-running-head">Khosravi et al.</alt-title>
    </title-group>
    <contrib-group>
      <contrib id="jmri27599-cr-0001" contrib-type="author">
        <name>
          <surname>Khosravi</surname>
          <given-names>Pegah</given-names>
        </name>
        <degrees>PhD</degrees>
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-3497-2820</contrib-id>
        <xref rid="jmri27599-aff-0001" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="jmri27599-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0002" contrib-type="author">
        <name>
          <surname>Lysandrou</surname>
          <given-names>Maria</given-names>
        </name>
        <degrees>BS</degrees>
        <xref rid="jmri27599-aff-0004" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0003" contrib-type="author">
        <name>
          <surname>Eljalby</surname>
          <given-names>Mahmoud</given-names>
        </name>
        <degrees>MMS</degrees>
        <xref rid="jmri27599-aff-0005" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0004" contrib-type="author">
        <name>
          <surname>Li</surname>
          <given-names>Qianzi</given-names>
        </name>
        <degrees>BA</degrees>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="jmri27599-aff-0006" ref-type="aff">
          <sup>6</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0005" contrib-type="author">
        <name>
          <surname>Kazemi</surname>
          <given-names>Ehsan</given-names>
        </name>
        <degrees>PhD</degrees>
        <xref rid="jmri27599-aff-0007" ref-type="aff">
          <sup>7</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0006" contrib-type="author">
        <name>
          <surname>Zisimopoulos</surname>
          <given-names>Pantelis</given-names>
        </name>
        <degrees>MS</degrees>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="jmri27599-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0007" contrib-type="author">
        <name>
          <surname>Sigaras</surname>
          <given-names>Alexandros</given-names>
        </name>
        <degrees>MS</degrees>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="jmri27599-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0008" contrib-type="author">
        <name>
          <surname>Brendel</surname>
          <given-names>Matthew</given-names>
        </name>
        <degrees>MEng</degrees>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0009" contrib-type="author">
        <name>
          <surname>Barnes</surname>
          <given-names>Josue</given-names>
        </name>
        <degrees>MS</degrees>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="jmri27599-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0010" contrib-type="author">
        <name>
          <surname>Ricketts</surname>
          <given-names>Camir</given-names>
        </name>
        <degrees>PhD</degrees>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="jmri27599-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0011" contrib-type="author">
        <name>
          <surname>Meleshko</surname>
          <given-names>Dmitry</given-names>
        </name>
        <degrees>MS</degrees>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="jmri27599-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0012" contrib-type="author">
        <name>
          <surname>Yat</surname>
          <given-names>Andy</given-names>
        </name>
        <degrees>RT</degrees>
        <xref rid="jmri27599-aff-0008" ref-type="aff">
          <sup>8</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0013" contrib-type="author">
        <name>
          <surname>McClure</surname>
          <given-names>Timothy D.</given-names>
        </name>
        <degrees>MD</degrees>
        <xref rid="jmri27599-aff-0005" ref-type="aff">
          <sup>5</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0014" contrib-type="author">
        <name>
          <surname>Robinson</surname>
          <given-names>Brian D.</given-names>
        </name>
        <degrees>MD</degrees>
        <xref rid="jmri27599-aff-0009" ref-type="aff">
          <sup>9</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0015" contrib-type="author">
        <name>
          <surname>Sboner</surname>
          <given-names>Andrea</given-names>
        </name>
        <degrees>PhD</degrees>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="jmri27599-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="jmri27599-aff-0009" ref-type="aff">
          <sup>9</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0016" contrib-type="author">
        <name>
          <surname>Elemento</surname>
          <given-names>Olivier</given-names>
        </name>
        <degrees>PhD</degrees>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="jmri27599-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="jmri27599-aff-0010" ref-type="aff">
          <sup>10</sup>
        </xref>
      </contrib>
      <contrib id="jmri27599-cr-0017" contrib-type="author" corresp="yes">
        <name>
          <surname>Chughtai</surname>
          <given-names>Bilal</given-names>
        </name>
        <degrees>MD</degrees>
        <xref rid="jmri27599-aff-0005" ref-type="aff">
          <sup>5</sup>
        </xref>
        <address>
          <email>bic9008@med.cornell.edu</email>
        </address>
      </contrib>
      <contrib id="jmri27599-cr-0018" contrib-type="author" corresp="yes">
        <name>
          <surname>Hajirasouliha</surname>
          <given-names>Iman</given-names>
        </name>
        <degrees>PhD</degrees>
        <xref rid="jmri27599-aff-0002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="jmri27599-aff-0003" ref-type="aff">
          <sup>3</sup>
        </xref>
        <address>
          <email>imh2003@med.cornell.edu</email>
        </address>
      </contrib>
    </contrib-group>
    <aff id="jmri27599-aff-0001">
      <label>
        <sup>1</sup>
      </label>
      <named-content content-type="organisation-division">Computational Oncology, Department of Epidemiology and Biostatistics</named-content>
      <institution>Memorial Sloan Kettering Cancer Center</institution>
      <city>New York</city>
      <named-content content-type="country-part">New York</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="jmri27599-aff-0002">
      <label>
        <sup>2</sup>
      </label>
      <named-content content-type="organisation-division">Department of Physiology and Biophysics</named-content>
      <institution>Institute for Computational Biomedicine, Weill Cornell Medicine of Cornell University</institution>
      <city>New York</city>
      <named-content content-type="country-part">New York</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="jmri27599-aff-0003">
      <label>
        <sup>3</sup>
      </label>
      <named-content content-type="organisation-division">Caryl and Israel Englander Institute for Precision Medicine</named-content>
      <institution>The Meyer Cancer Center, Weill Cornell Medicine</institution>
      <city>New York</city>
      <named-content content-type="country-part">New York</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="jmri27599-aff-0004">
      <label>
        <sup>4</sup>
      </label>
      <named-content content-type="organisation-division">Neuroscience Institute</named-content>
      <institution>The University of Chicago</institution>
      <city>Chicago</city>
      <named-content content-type="country-part">Illinois</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="jmri27599-aff-0005">
      <label>
        <sup>5</sup>
      </label>
      <named-content content-type="organisation-division">Department of Urology</named-content>
      <institution>Weill Cornell Medicine of Cornell University</institution>
      <city>New York</city>
      <named-content content-type="country-part">New York</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="jmri27599-aff-0006">
      <label>
        <sup>6</sup>
      </label>
      <named-content content-type="organisation-division">Mathematics and Statistics Department</named-content>
      <institution>Carleton College</institution>
      <city>Northfield</city>
      <named-content content-type="country-part">Minnesota</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="jmri27599-aff-0007">
      <label>
        <sup>7</sup>
      </label>
      <institution>Yale University, Department of Electrical Engineering</institution>
    </aff>
    <aff id="jmri27599-aff-0008">
      <label>
        <sup>8</sup>
      </label>
      <named-content content-type="organisation-division">Department of Radiology</named-content>
      <institution>New York‐Presbyterian Hospital</institution>
      <city>New York</city>
      <named-content content-type="country-part">New York</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="jmri27599-aff-0009">
      <label>
        <sup>9</sup>
      </label>
      <named-content content-type="organisation-division">Department of Pathology</named-content>
      <institution>New York Presbyterian Hospital‐Weill Cornell Medical College</institution>
      <city>New York</city>
      <named-content content-type="country-part">New York</named-content>
      <country country="US">USA</country>
    </aff>
    <aff id="jmri27599-aff-0010">
      <label>
        <sup>10</sup>
      </label>
      <named-content content-type="organisation-division">WorldQuant Initiative for Quantitative Prediction</named-content>
      <institution>Weill Cornell Medicine</institution>
      <city>New York</city>
      <named-content content-type="country-part">New York</named-content>
      <country country="US">USA</country>
    </aff>
    <author-notes>
      <corresp id="correspondenceTo"><label>*</label>
Address reprint requests to: I.H., 1305 York Ave, New York, NY 10021, USA. E‐mail: imh2003@med.cornell.edu, or B.C., 425 E 61st St, New York, NY 10065, USA. E‐mail: bic9008@med.cornell.edu<break/>
</corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>14</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>8</month>
      <year>2021</year>
    </pub-date>
    <volume>54</volume>
    <issue seq="160">2</issue>
    <issue-id pub-id-type="doi">10.1002/jmri.v54.2</issue-id>
    <fpage>462</fpage>
    <lpage>471</lpage>
    <history>
      <date date-type="rev-recd">
        <day>22</day>
        <month>2</month>
        <year>2021</year>
      </date>
      <date date-type="received">
        <day>05</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>2</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <!--&#x000a9; 2021 International Society for Magnetic Resonance in Medicine-->
      <copyright-statement content-type="article-copyright">© 2021 The Authors. <italic toggle="yes">Journal of Magnetic Resonance Imaging</italic> published by Wiley Periodicals LLC. on behalf of International Society for Magnetic Resonance in Medicine.</copyright-statement>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the terms of the <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link> License, which permits use and distribution in any medium, provided the original work is properly cited, the use is non‐commercial and no modifications or adaptations are made.</license-p>
      </license>
    </permissions>
    <self-uri xmlns:xlink="http://www.w3.org/1999/xlink" content-type="pdf" xlink:href="file:JMRI-54-462.pdf"/>
    <abstract>
      <sec id="jmri27599-sec-0001">
        <title>Background</title>
        <p>A definitive diagnosis of prostate cancer requires a biopsy to obtain tissue for pathologic analysis, but this is an invasive procedure and is associated with complications.</p>
      </sec>
      <sec id="jmri27599-sec-0002">
        <title>Purpose</title>
        <p>To develop an artificial intelligence (AI)‐based model (named AI‐biopsy) for the early diagnosis of prostate cancer using magnetic resonance (MR) images labeled with histopathology information.</p>
      </sec>
      <sec id="jmri27599-sec-0003">
        <title>Study Type</title>
        <p>Retrospective.</p>
      </sec>
      <sec id="jmri27599-sec-0004">
        <title>Population</title>
        <p>Magnetic resonance imaging (MRI) data sets from 400 patients with suspected prostate cancer and with histological data (228 acquired in‐house and 172 from external publicly available databases).</p>
      </sec>
      <sec id="jmri27599-sec-0005">
        <title>Field Strength/Sequence</title>
        <p>1.5 to 3.0 Tesla, <styled-content style="fixed-case" toggle="no">T2</styled-content>‐weighted image pulse sequences.</p>
      </sec>
      <sec id="jmri27599-sec-0006">
        <title>Assessment</title>
        <p>MR images reviewed and selected by two radiologists (with 6 and 17 years of experience). The patient images were labeled with prostate biopsy including Gleason Score (6 to 10) or Grade Group (1 to 5) and reviewed by one pathologist (with 15 years of experience). Deep learning models were developed to distinguish 1) benign from cancerous tumor and 2) high‐risk tumor from low‐risk tumor.</p>
      </sec>
      <sec id="jmri27599-sec-0007">
        <title>Statistical Tests</title>
        <p>To evaluate our models, we calculated negative predictive value, positive predictive value, specificity, sensitivity, and accuracy. We also calculated areas under the receiver operating characteristic (ROC) curves (AUCs) and Cohen's kappa.</p>
      </sec>
      <sec id="jmri27599-sec-0008">
        <title>Results</title>
        <p>Our computational method (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ih-lab/AI-biopsy" ext-link-type="uri">https://github.com/ih-lab/AI-biopsy</ext-link>) achieved AUCs of 0.89 (95% confidence interval [CI]: [0.86–0.92]) and 0.78 (95% CI: [0.74–0.82]) to classify cancer vs. benign and high‐ vs. low‐risk of prostate disease, respectively.</p>
      </sec>
      <sec id="jmri27599-sec-0009">
        <title>Data Conclusion</title>
        <p>AI‐biopsy provided a data‐driven and reproducible way to assess cancer risk from MR images and a personalized strategy to potentially reduce the number of unnecessary biopsies. AI‐biopsy highlighted the regions of MR images that contained the predictive features the algorithm used for diagnosis using the class activation map method. It is a fully automatic method with a drag‐and‐drop web interface (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ai-biopsy.eipm-research.org" ext-link-type="uri">https://ai-biopsy.eipm-research.org</ext-link>) that allows radiologists to review AI‐assessed MR images in real time.</p>
      </sec>
      <sec id="jmri27599-sec-0010">
        <title>Level of Evidence</title>
        <p>1</p>
      </sec>
      <sec id="jmri27599-sec-0011">
        <title>Technical Efficacy Stage</title>
        <p>2</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="author-generated">
      <kwd id="jmri27599-kwd-0001">artificial intelligence</kwd>
      <kwd id="jmri27599-kwd-0002">biopsy</kwd>
      <kwd id="jmri27599-kwd-0003">deep neural networks</kwd>
      <kwd id="jmri27599-kwd-0004">MRI images</kwd>
      <kwd id="jmri27599-kwd-0005">PI‐RADS</kwd>
      <kwd id="jmri27599-kwd-0006">prostate cancer</kwd>
    </kwd-group>
    <funding-group>
      <award-group id="funding-0001">
        <funding-source>Weill Cornell Medicine</funding-source>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="3"/>
      <page-count count="10"/>
      <word-count count="6274"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>source-schema-version-number</meta-name>
        <meta-value>2.0</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>cover-date</meta-name>
        <meta-value>August 2021</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>details-of-publishers-convertor</meta-name>
        <meta-value>Converter:WILEY_ML3GV2_TO_JATSPMC version:6.0.5 mode:remove_FC converted:12.08.2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <fn-group id="jmri27599-ntgp-0101">
      <fn id="jmri27599-note-0101">
        <p>[Correction added on 04 May 2021, after first online publication: The affiliation for Ehsan Kazemi has been updated.]</p>
      </fn>
    </fn-group>
  </notes>
</front>
<body id="jmri27599-body-0001">
  <p>Prostate cancer is the most commonly diagnosed cancer in adult men.<xref rid="jmri27599-bib-0001" ref-type="bibr"><sup>1</sup></xref> Distinguishing patients with high‐risk (tumor tissue growing faster) and low‐risk (tumor tissue growing slowly) forms of prostate cancer is important because early detection of high‐risk prostate cancer improves survival rate<xref rid="jmri27599-bib-0002" ref-type="bibr"><sup>2</sup></xref> and accurate diagnosis prevents overtreatment.<xref rid="jmri27599-bib-0003" ref-type="bibr"><sup>3</sup></xref>
</p>
  <p>The European Society of Urogenital Radiology established the Prostate Imaging Reporting and Data System (PI‐RADS), a standardized guideline for interpretation and reporting prostate magnetic resonance imaging (MRI).<xref rid="jmri27599-bib-0004" ref-type="bibr"><sup>4</sup></xref> PI‐RADS is designed to improve and standardize detection, localization, characterization, and risk stratification in patients with suspected cancer.<xref rid="jmri27599-bib-0005" ref-type="bibr"><sup>5</sup></xref> Radiologists apply PI‐RADS' subjective features such as lesion shape and margins for categorization of prostate cancer<xref rid="jmri27599-bib-0006" ref-type="bibr"><sup>6</sup></xref> and assignment of a score ranging from 1 to 5.<xref rid="jmri27599-bib-0007" ref-type="bibr"><sup>7</sup></xref> Although PI‐RADS has been found to be effective in evaluating the clinical risk associated with prostate cancer,<xref rid="jmri27599-bib-0008" ref-type="bibr"><sup>8</sup></xref> it requires experts' visual assessment, which introduces an element of subjectivity.<xref rid="jmri27599-bib-0009" ref-type="bibr"><sup>9</sup></xref>
</p>
  <p>There are currently two main scoring systems used to assess histology slides for prostate cancer aggressiveness. The Gleason Score (GS) is the most commonly used prognostic score to predict the clinical status of prostate cancer based on biopsy material. The GS describes how much the tissue from a biopsy looks like healthy tissue (lower score) or abnormal tissue (higher score).<xref rid="jmri27599-bib-0010" ref-type="bibr"><sup>10</sup></xref> GS is the sum of primary and secondary scores, each with a range of 3 to 5. Thus, GSs range from 6 (3 + 3) to 10 (5 + 5) (Table <xref rid="jmri27599-tbl-0001" ref-type="table">1</xref>). Grade Group (GG) is an alternative scoring system that subdivides prostate cancer into five categories using pathological characteristics.<xref rid="jmri27599-bib-0011" ref-type="bibr"><sup>11</sup></xref> Pathologists use either of these scores in routine clinical practice.</p>
  <table-wrap position="float" id="jmri27599-tbl-0001" content-type="TABLE">
    <label>TABLE 1</label>
    <caption>
      <p>Grade Group and Gleason Score and Their Association With the Risk Level of Prostate Cancer</p>
    </caption>
    <table frame="hsides" rules="groups">
      <col align="left" span="1"/>
      <col align="left" span="1"/>
      <col align="left" span="1"/>
      <col align="left" span="1"/>
      <thead valign="bottom">
        <tr style="border-bottom:solid 1px #000000">
          <th align="left" valign="bottom" rowspan="1" colspan="1">Grade Group</th>
          <th align="left" valign="bottom" rowspan="1" colspan="1">Gleason Score</th>
          <th align="left" valign="bottom" rowspan="1" colspan="1">Combined Gleason Score</th>
          <th align="left" valign="bottom" rowspan="1" colspan="1">Aggressiveness degree</th>
        </tr>
      </thead>
      <tbody valign="top">
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Grade Group 1</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3 + 3</td>
          <td align="left" valign="top" rowspan="1" colspan="1">6</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Low risk</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Grade Group 2</td>
          <td align="left" valign="top" rowspan="1" colspan="1">3 + 4</td>
          <td align="left" valign="top" rowspan="1" colspan="1">7</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Intermediate risk but closer to low risk</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Grade Group 3</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4 + 3</td>
          <td align="left" valign="top" rowspan="1" colspan="1">7</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Intermediate risk but closer to high risk</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Grade Group 4</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4 + 4, 3 + 5, 5 + 3</td>
          <td align="left" valign="top" rowspan="1" colspan="1">8</td>
          <td align="left" valign="top" rowspan="1" colspan="1">High risk</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Grade Group 5</td>
          <td align="left" valign="top" rowspan="1" colspan="1">4 + 5, 5 + 4, 5 + 5</td>
          <td align="left" valign="top" rowspan="1" colspan="1">9 and 10</td>
          <td align="left" valign="top" rowspan="1" colspan="1">High risk</td>
        </tr>
      </tbody>
    </table>
    <table-wrap-foot id="jmri27599-ntgp-0001">
      <fn id="jmri27599-note-0001">
        <p>These two different systems are mapped together using the table that was provided and simplified based on the NCCN guidelines version 4.2018 prostate cancer.<xref rid="jmri27599-bib-0027" ref-type="bibr"><sup>27</sup></xref>
</p>
      </fn>
    </table-wrap-foot>
  </table-wrap>
  <p>Although a biopsy provides a definitive diagnosis of prostate cancer, patients undergoing prostate biopsy may experience incorrect staging and complications such as infection; 2% to 3% of patients will develop sepsis that is associated with life‐threatening organ dysfunction and death.<xref rid="jmri27599-bib-0012" ref-type="bibr"><sup>12</sup></xref>
</p>
  <p>We hypothesized that prostate cancer aggressiveness can be predicted directly from MR images using machine learning (ML) techniques, perhaps reducing the need for a tissue biopsy by optimizing PI‐RADS assessment and increasing diagnosis accuracy. In recent years, ML and especially deep learning (DL) approaches have been applied to a variety of medical conditions,<xref rid="jmri27599-bib-0013" ref-type="bibr"><sup>13</sup></xref>, <xref rid="jmri27599-bib-0014" ref-type="bibr"><sup>14</sup></xref>, <xref rid="jmri27599-bib-0015" ref-type="bibr"><sup>15</sup></xref> such as lung cancer subtype diagnosis using pathology images,<xref rid="jmri27599-bib-0016" ref-type="bibr"><sup>16</sup></xref> assessing human blastocyst quality after in vitro fertilization,<xref rid="jmri27599-bib-0017" ref-type="bibr"><sup>17</sup></xref> and prostate cancer classification by MR images.<xref rid="jmri27599-bib-0018" ref-type="bibr"><sup>18</sup></xref> In the latter study,<xref rid="jmri27599-bib-0018" ref-type="bibr"><sup>18</sup></xref> the authors used DL and non‐DL algorithms to differentiate benign prostate from prostate cancer using axial T2‐weighted (T2w) MR images of 172 patients. They were able to distinguish benign from malignant lesions with areas under the receiver operating characteristic (ROC) curves (AUCs) of 0.84 and 0.70 using DL and non‐DL methods, respectively.<xref rid="jmri27599-bib-0018" ref-type="bibr"><sup>18</sup></xref> In another related study, Kwon et al described a radiomics‐based approach to classify clinically important lesions in multiparametric MRI (mp‐MRI) using feature‐based methods such as regression trees and random forests. Random forest achieved the highest performance with an AUC of 0.82.<xref rid="jmri27599-bib-0019" ref-type="bibr"><sup>19</sup></xref>
</p>
  <p>Recent research indicates that multimodal diagnosis using DL methods has exhibited notable improvement over conventional unimodal approaches in classifying radiology and pathology images.<xref rid="jmri27599-bib-0020" ref-type="bibr"><sup>20</sup></xref> Moreover, when MR images are limited, using convolutional neural networks (CNNs) for feature extraction across data concatenation can yield better CNN‐based classification performance.</p>
  <p>The aim of this study is to develop a CNN‐based method that uses MR imaging data as input and recognizes benign from cancerous tumor and high‐risk prostate cancer from low‐risk forms, as defined by pathology assessments such as GS and GG. While the training combines MRI data with pathology assessment, our objective was to develop predictive models that could provide assessments from MR images alone.</p>
  <sec id="jmri27599-sec-0013">
    <title>Materials and Methods</title>
    <sec id="jmri27599-sec-0014">
      <title>
Ethics Statement
</title>
      <p>All experiments and methods were performed in accordance with the Institutional Review Board at Weill Cornell Medicine. The study used fully de‐identified data and was approved by the ethics committee of our institution (IRB number: 1601016896).</p>
    </sec>
    <sec id="jmri27599-sec-0015">
      <title>
Combined Database
</title>
      <p>This study included 228 patients from our own urology center (imaged between 2015/02 and 2019/03). We refer to this data set as in‐house throughout this manuscript. All images were acquired on GE and Siemens platforms confirming to PI‐RADS v2.1 specification (T2w—slice thickness 3 mm, no gap; field of view: generally, 12–20 cm; in‐plane dimension: ≤0.7 mm [phase] × ≤0.4 mm [frequency]). The MR images were labeled by cancer GS and GG information obtained from corresponding fusion‐guided biopsy (transrectal and transperineal). We also used four external public data sets obtained from The Cancer Imaging Archive (TCIA)<xref rid="jmri27599-bib-0021" ref-type="bibr"><sup>21</sup></xref> that de‐identifies and hosts a large archive of medical images from cancer patients that are accessible for public download. We used data from the PROSTATEx Challenge (<italic toggle="yes">n</italic> = 99 patients),<xref rid="jmri27599-bib-0022" ref-type="bibr"><sup>22</sup></xref>, <xref rid="jmri27599-bib-0023" ref-type="bibr"><sup>23</sup></xref> PROSTATE‐MRI (<italic toggle="yes">n</italic> = 26),<xref rid="jmri27599-bib-0024" ref-type="bibr"><sup>24</sup></xref> PROSTATE‐DIAGNOSIS (<italic toggle="yes">n</italic> = 38 patients),<xref rid="jmri27599-bib-0025" ref-type="bibr"><sup>25</sup></xref> and TCGA‐PRAD (<italic toggle="yes">n</italic> = 9 patients).<xref rid="jmri27599-bib-0026" ref-type="bibr"><sup>26</sup></xref> The collection of public data therefore comprised a total of 172 patients with T2w prostate MR images along with histopathology information from corresponding prostate core needle biopsy or prostatectomy specimens. All public and in‐house data were converted from DICOM to PNG format and regularized for intensity inhomogeneity using a Python script. For each patient, consistent sequences of seven axial image slices containing the prostate gland (=2800 images interpolated to 512 × 512 pixels) were reviewed and selected by two radiologists (one advanced imaging technologist with 6 years of experience and one uroradiologist with 17 years of experience) (Fig. <xref rid="jmri27599-fig-0001" ref-type="fig">1a</xref>). Also, for in‐house data, the assigned PI‐RADS<xref rid="jmri27599-bib-0001" ref-type="bibr"><sup>1</sup></xref>, <xref rid="jmri27599-bib-0002" ref-type="bibr"><sup>2</sup></xref>, <xref rid="jmri27599-bib-0003" ref-type="bibr"><sup>3</sup></xref>, <xref rid="jmri27599-bib-0004" ref-type="bibr"><sup>4</sup></xref>, <xref rid="jmri27599-bib-0005" ref-type="bibr"><sup>5</sup></xref> scores (if it is available) were reviewed by these two radiologists. Then, we categorized all MR images based on their corresponding pathology reports (Fig. <xref rid="jmri27599-fig-0001" ref-type="fig">1b</xref>). This means that all the patients' MR images were labeled with a benign, GS,<xref rid="jmri27599-bib-0006" ref-type="bibr"><sup>6</sup></xref>, <xref rid="jmri27599-bib-0007" ref-type="bibr"><sup>7</sup></xref>, <xref rid="jmri27599-bib-0008" ref-type="bibr"><sup>8</sup></xref>, <xref rid="jmri27599-bib-0009" ref-type="bibr"><sup>9</sup></xref>, <xref rid="jmri27599-bib-0010" ref-type="bibr"><sup>10</sup></xref> or GG<xref rid="jmri27599-bib-0001" ref-type="bibr"><sup>1</sup></xref>, <xref rid="jmri27599-bib-0002" ref-type="bibr"><sup>2</sup></xref>, <xref rid="jmri27599-bib-0003" ref-type="bibr"><sup>3</sup></xref>, <xref rid="jmri27599-bib-0004" ref-type="bibr"><sup>4</sup></xref>, <xref rid="jmri27599-bib-0005" ref-type="bibr"><sup>5</sup></xref> pathology evaluation performed by experienced pathologists (from different clinics) and then reviewed by one pathologist from our institution (with 15 years of experience). Table <xref rid="jmri27599-tbl-0001" ref-type="table">1</xref> shows how we mapped the cancer GG with GS that were obtained from different data sets to use for determining the risk level of prostate cancer. This table was provided and simplified based on the National Comprehensive Cancer Network guidelines version 4.2018 prostate cancer.<xref rid="jmri27599-bib-0027" ref-type="bibr"><sup>27</sup></xref> We have follow‐up biopsy for all in‐house cases (benign and malignant) but not for publicly available cases. When multiple biopsies were available, we considered the maximum GS or GG as the final label. In other words, only cases where all biopsies were benign were labeled as benign. Characteristics of all five data sets and their images are summarized in Table <xref rid="jmri27599-tbl-0002" ref-type="table">2</xref>.</p>
      <fig position="float" fig-type="FIGURE" id="jmri27599-fig-0001">
        <label>FIGURE 1</label>
        <caption>
          <p>Method flow chart. (a) Unsegmented consistent sequences of seven axial T2w magnetic resonance (MR) image slices for each patient were selected that represent the prostate glands. (b) Each patient's MRI slice labeled by their corresponding biopsy result based on its Grade Group (GG) and Gleason Score (GS). (c) A convolutional neural network (CNN)‐based model (Model 1) classifies the cancer vs. benign and subsequently, and the second CNN‐based model (Model 2) predicts the risk level for each patient. (d) We highlighted the regions of MR images that algorithms focus on for prediction and compared the output of Model 2 with Prostate Imaging Reporting and Data System (PI‐RADS) using pathology labels as ground truth for a subset of test set. Receiver operating characteristic curves (ROCs) were used to assess the performance of different models based on individual patient.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="JMRI-54-462-g004" position="anchor" id="jats-graphic-1"/>
      </fig>
      <table-wrap position="float" id="jmri27599-tbl-0002" content-type="TABLE">
        <label>TABLE 2</label>
        <caption>
          <p>Characteristics of All Five Cohorts and the Comprised Biopsy Reports and T2w Images Obtained from TCIA<xref rid="jmri27599-bib-0021" ref-type="bibr"><sup>21</sup></xref> and In‐House</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th style="border-bottom:solid 1px #000000" align="left" rowspan="4" valign="bottom" colspan="1">Databases and references</th>
              <th align="left" style="border-bottom:solid 1px #000000" rowspan="4" valign="bottom" colspan="1">Selected cases and MRI types</th>
              <th align="left" style="border-bottom:solid 1px #000000" rowspan="4" valign="bottom" colspan="1">Annotation method (biopsy types)</th>
              <th align="left" style="border-bottom:solid 1px #000000" colspan="4" valign="bottom" rowspan="1">Cancer patients</th>
              <th align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1" colspan="1">Benign cases</th>
            </tr>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1" colspan="1">High‐risk</th>
              <th align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1" colspan="1">Low‐risk</th>
              <th align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1" colspan="1">Intermediate‐risk</th>
              <th align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1" colspan="1">Intermediate‐risk</th>
              <th align="left" style="border-bottom:solid 1px #000000" rowspan="3" valign="bottom" colspan="1">Benign</th>
            </tr>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1" colspan="1">(GS ≥ 8)</th>
              <th align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1" colspan="1">(GS = 6)</th>
              <th align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1" colspan="1">(GS = 7)</th>
              <th align="left" style="border-bottom:solid 1px #000000" valign="bottom" rowspan="1" colspan="1">(GS = 7)</th>
            </tr>
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">(GG = 4 &amp; GG = 5)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">(GG = 1)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">(GG = 2)</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">(GG = 3)</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Weill Cornell Medicine</td>
              <td align="left" valign="top" rowspan="1" colspan="1">228, age (52–85), 3.0 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">GS and GG (fusion guided biopsy), PI‐RADS</td>
              <td align="left" valign="top" rowspan="1" colspan="1">11</td>
              <td align="left" valign="top" rowspan="1" colspan="1">48</td>
              <td align="left" valign="top" rowspan="1" colspan="1">37</td>
              <td align="left" valign="top" rowspan="1" colspan="1">15</td>
              <td align="left" valign="top" rowspan="1" colspan="1">117</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">PROSTATEx<xref rid="jmri27599-bib-0022" ref-type="bibr"><sup>22</sup></xref>, <xref rid="jmri27599-bib-0023" ref-type="bibr"><sup>23</sup></xref>
</td>
              <td align="left" valign="top" rowspan="1" colspan="1">99, 3.0 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">GG (core needle biopsy)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">13</td>
              <td align="left" valign="top" rowspan="1" colspan="1">29</td>
              <td align="left" valign="top" rowspan="1" colspan="1">38</td>
              <td align="left" valign="top" rowspan="1" colspan="1">19</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">PROSTATE‐DIAGNOSIS<xref rid="jmri27599-bib-0025" ref-type="bibr"><sup>25</sup></xref>
</td>
              <td align="left" valign="top" rowspan="1" colspan="1">38, 1.5 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">GS (core needle biopsy)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">9</td>
              <td align="left" valign="top" rowspan="1" colspan="1">5</td>
              <td align="left" valign="top" rowspan="1" colspan="1">15</td>
              <td align="left" valign="top" rowspan="1" colspan="1">9</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">PROSTATE‐MRI<xref rid="jmri27599-bib-0024" ref-type="bibr"><sup>24</sup></xref>
</td>
              <td align="left" valign="top" rowspan="1" colspan="1">26, 3.0 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">GS (prostatectomy)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">11</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
              <td align="left" valign="top" rowspan="1" colspan="1">13</td>
              <td align="left" valign="top" rowspan="1" colspan="1">2</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">TCGA‐PRAD<xref rid="jmri27599-bib-0026" ref-type="bibr"><sup>26</sup></xref>
</td>
              <td align="left" valign="top" rowspan="1" colspan="1">9, 3.0 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">GS and GG (core needle biopsy)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">4</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
              <td align="left" valign="top" rowspan="1" colspan="1">3</td>
              <td align="left" valign="top" rowspan="1" colspan="1">2</td>
              <td align="left" valign="top" rowspan="1" colspan="1">0</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Total</td>
              <td align="left" valign="top" rowspan="1" colspan="1">400, 1.5 T to 3.0 T</td>
              <td align="left" valign="top" rowspan="1" colspan="1">GG and GS (reviewed pathology report)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">48</td>
              <td align="left" valign="top" rowspan="1" colspan="1">82</td>
              <td align="left" valign="top" rowspan="1" colspan="1">106</td>
              <td align="left" valign="top" rowspan="1" colspan="1">47</td>
              <td align="left" valign="top" rowspan="1" colspan="1">117</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="jmri27599-ntgp-0002">
          <fn id="jmri27599-note-0002">
            <p>T = Tesla; GS = Gleason Score; GG = Grade Group; T2w = T2‐weighted; TCIA = The Cancer Imaging Archive; MRI = magnetic resonance imaging.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec id="jmri27599-sec-0016">
      <title>
Models' Architecture and Implementation
</title>
      <p>We used Google's Inception‐V1<xref rid="jmri27599-bib-0028" ref-type="bibr"><sup>28</sup></xref> (GoogLeNet) as the main architecture of our models, which offers an effective run‐time and computational cost. To train this architecture, we used transfer learning and pretrained the network on the ImageNet data set.<xref rid="jmri27599-bib-0029" ref-type="bibr"><sup>29</sup></xref> We then fine‐tuned all outer layers using the training set of MRI and evaluated the trained model by validation and test sets of MR images obtained from our in‐house and public resources.</p>
      <p>To implement our framework (AI‐biopsy), we used TensorFlow, version 1.7, and the TF‐Slim Python library for defining, training, and evaluating models. Training of our deep neural network (DNN) models were performed on a server running the SMP Linux operating system. This server was powered by four NVIDIA GeForce GTX 1080 GPUs with 8 GB of memory for each GPU and 12 1.7‐GHz Intel Xeon CPUs.<xref rid="jmri27599-bib-0030" ref-type="bibr"><sup>30</sup></xref> We used Python open‐source libraries such as Pydicom, scikit‐learn, NumPy, SciPy, and Matplotlib for all the statistical analyses.</p>
    </sec>
    <sec id="jmri27599-sec-0017">
      <title>
Training Method
</title>
      <p>The prostate cancer group (<italic toggle="yes">n</italic> = 283 patients) included high‐risk patients (<italic toggle="yes">n</italic> = 48, with GG = 4 and 5), intermediate‐risk patients (<italic toggle="yes">n</italic> = 153, with GG = 2 and 3), and low‐risk patients (<italic toggle="yes">n</italic> = 82, with GG = 1). The benign group contains 117 patients (Table <xref rid="jmri27599-tbl-0002" ref-type="table">2</xref>).</p>
      <p>For training the first model (cancer vs. benign), we grouped GG = 3, 4, and 5 together in one class (<italic toggle="yes">n</italic> = 95 patients) and trained the algorithm vs. the benign class (<italic toggle="yes">n</italic> = 117). We did not use GG = 1 and 2 patients for training this model so as to allow the algorithm to learn the two ends of the spectrum and take more associated features for classifying cancer vs. benign. However, we tested the trained model on all GGs (GG = 1, 2, 3, 4, 5) as well as benign subjects (Table <xref rid="jmri27599-tbl-0003" ref-type="table">3</xref>).</p>
      <table-wrap position="float" id="jmri27599-tbl-0003" content-type="TABLE">
        <label>TABLE 3</label>
        <caption>
          <p>Characteristics of Both Trained Models and the Comprised Patients</p>
        </caption>
        <table frame="hsides" rules="groups">
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <col align="left" span="1"/>
          <thead valign="bottom">
            <tr style="border-bottom:solid 1px #000000">
              <th align="left" valign="bottom" rowspan="1" colspan="1">Model</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Data resources</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Number of patients with cancerous tumor in training and validation sets</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Number of patients with benign tumor in training and validation sets</th>
              <th align="left" valign="bottom" rowspan="1" colspan="1">Number of patients in test set</th>
            </tr>
          </thead>
          <tbody valign="top">
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <p>Model 1:</p>
                <p>Benign vs. cancer</p>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">In‐house and public</td>
              <td align="left" valign="top" rowspan="1" colspan="1">75 patients (37 GG = 3, 38 GG = 4 and GG = 5)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">107 patients (benign)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <p>10 benign</p>
                <p>10 GG = 1</p>
                <p>10 GG = 2</p>
                <p>10 GG = 3</p>
                <p>10 GG = 4&amp;5</p>
              </td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">Model</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Data resources</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Number of patients with high‐risk tumor in training and validation sets</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Number of patients with low‐risk tumor in training and validation sets</td>
              <td align="left" valign="top" rowspan="1" colspan="1">Number of patients in test set</td>
            </tr>
            <tr>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <p>Model 2:</p>
                <p>High‐risk vs. low‐risk</p>
              </td>
              <td align="left" valign="top" rowspan="1" colspan="1">In‐house and public</td>
              <td align="left" valign="top" rowspan="1" colspan="1">75 patients (37 GG = 3, 38 GG = 4 and GG = 5)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">168 patients (72 GG = 1 and 96 GG = 2)</td>
              <td align="left" valign="top" rowspan="1" colspan="1">
                <p>10 GG = 1</p>
                <p>10 GG = 2</p>
                <p>10 GG = 3</p>
                <p>10 GG = 4&amp;5</p>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot id="jmri27599-ntgp-0003">
          <fn id="jmri27599-note-0003">
            <p>GG = Grade Group.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>For training the second model (high‐risk vs. low‐risk), we grouped GG = 3, 4, and 5 together in one class as high‐risk (<italic toggle="yes">n</italic> = 95 patients) and trained the algorithm vs. the low‐risk class that combined GG = 1 and 2 (<italic toggle="yes">n</italic> = 188). We used one of the oversampling techniques, adding Gaussian noise to the images, to address the class imbalance problem. Noise injection consists of injecting a matrix of random values usually drawn from a Gaussian distribution.<xref rid="jmri27599-bib-0031" ref-type="bibr"><sup>31</sup></xref> Then, we tested the trained model for all the GG groups (GG = 1, 2, 3, 4, 5) (Table <xref rid="jmri27599-tbl-0003" ref-type="table">3</xref>).</p>
      <p>The cancer/benign images (GG = 3, 4, 5/benign) from 212 patients include a total of 1484 images. One thousand two hundred and seventy‐four images (= 182 patients) were randomly selected for training and validation, and 210 remaining images (= 30 patients for GG = 3, 4, 5, benign) with the addition of 140 images (= 20 patients for GG = 1, 2) were selected for test set (Table <xref rid="jmri27599-tbl-0003" ref-type="table">3</xref>). Also, the high‐risk/low‐risk images (GG = 3, 4, 5/GG = 1, 2) for 283 patients include a total of 1981 images. Out of these, 1701 images (= 243 patients for GG = 1, 2, 3, 4, 5) were randomly selected for training and validation, and the 280 remaining images (=  40 patients for GG = 1, 2, 3, 4, 5) were selected for test set (Table <xref rid="jmri27599-tbl-0003" ref-type="table">3</xref>).</p>
    </sec>
    <sec id="jmri27599-sec-0018">
      <title>
Deep Feature Analysis
</title>
      <p>We applied class activation map (CAM)<xref rid="jmri27599-bib-0032" ref-type="bibr"><sup>32</sup></xref> using global average pooling (GAP) in CNNs. Before the final output layer (softmax) of the AI‐biopsy, we performed GAP on the convolutional feature maps and used those as features for a fully connected layer. Given this connectivity structure, we could identify the importance of the image regions by projecting back the weights of the output layer onto the convolutional feature maps.</p>
    </sec>
    <sec id="jmri27599-sec-0019">
      <title>
Evaluation and Statistical Analysis of the Developed Method
</title>
      <p>We divided the images into training, validation, and test groups. The images and the patients in training, validation, and test sets did not overlap. For each model (Fig. <xref rid="jmri27599-fig-0001" ref-type="fig">1c</xref>), we performed 5‐fold cross‐validation (resampling procedure) and measured the performance of the algorithm for the test set using AUCs with 95% confidence interval (CI) (Fig. <xref rid="jmri27599-fig-0001" ref-type="fig">1d</xref>). Characteristics of training, validation, and test set images of each model are summarized in Table <xref rid="jmri27599-tbl-0003" ref-type="table">3</xref>.</p>
      <p>To measure the accuracy of the trained algorithm for individual patients (with sequence of seven axial image slices), we used a simple voting system. For Model 1 (differentiating between malignant and benign tumor), if the number of image slices with cancer (with <italic toggle="yes">P</italic> ≥ 0.5) from a patient was ≥1, the final label of that patient was “cancer.” Otherwise, the final label of the patient was “benign.” For Model 2 (differentiating between high‐risk and low‐risk tumor), if the number of image slices with high‐risk (with <italic toggle="yes">P</italic> ≥ 0.5) from a patient was ≥2, the final label of the patient was “high‐risk.” Otherwise, the final label of the patient was “low‐risk.” We employed these threshold conditions on the outputs of the algorithms to reduce false‐negative prediction by giving more weight to the cancer and high‐risk classes.</p>
      <p>To evaluate our method, we used negative predictive value, positive predictive value, specificity, sensitivity, and accuracy. We also calculated the AUCs and Cohen's kappa.<xref rid="jmri27599-bib-0033" ref-type="bibr"><sup>33</sup></xref>
</p>
    </sec>
    <sec id="jmri27599-sec-0020">
      <title>
Code Availability
</title>
      <p>The source code and the guidelines for using the source code are publicly available at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ih-lab/AI-biopsy" ext-link-type="uri">https://github.com/ih-lab/AI-biopsy</ext-link>. In addition, AI‐biopsy is available through a web‐based user interface at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ai-biopsy.eipm-research.org" ext-link-type="uri">https://ai-biopsy.eipm-research.org</ext-link>.</p>
    </sec>
  </sec>
  <sec id="jmri27599-sec-0021">
    <title>Results</title>
    <sec id="jmri27599-sec-0022">
      <title>
Classification of <styled-content style="fixed-case" toggle="no">MR</styled-content> Images Based on their Pathology Labels
</title>
      <p>Our trained Model 1 was able to distinguish cancer patients from benign patients with an AUC of 0.89 (95% CI: [0.86–0.92]), negative predictive value (= 81.6), positive predictive value (= 81.9), specificity (= 82), sensitivity (= 81.5), and accuracy (= 81.8) (Fig. <xref rid="jmri27599-fig-0002" ref-type="fig">2a,b</xref>). Also, Model 2 was able to classify high‐risk vs. low‐risk (GS = 5 + 5, 5 + 4, 4 + 5, 4 + 4, 4 + 3 vs. GS = 3 + 3, 3 + 4) cancer with an AUC of 0.78 (95% CI: [0.74–0.82]), negative predictive value (= 73), positive predictive value (= 67), specificity (= 68.9), sensitivity (= 71.3), and accuracy (= 70) (Fig. <xref rid="jmri27599-fig-0002" ref-type="fig">2c,d</xref>). While the performance of Model 2 in classifying GS ≥ 8 vs. GS = 3 + 3 was high (AUC = 0.86), the ability of Model 2 to classify intermediate‐risk cases (GS = 3 + 4 vs. GS = 4 + 3) of prostate cancer was lower (AUC = 0.71).</p>
      <fig position="float" fig-type="FIGURE" id="jmri27599-fig-0002">
        <label>FIGURE 2</label>
        <caption>
          <p>Performance of two trained models for individual patient in the test set. (a) Model 1 performance for classifying cancer vs. benign. (b) The number of patients that were identified correct or incorrect by Model 1, negative predictive value, positive predictive value, specificity, sensitivity, and accuracy for cancer vs. benign. (c) Model 2 performance for classifying high risk vs. low risk. (d) The number of patients that were identified correct or incorrect by Model 2, negative predictive value, positive predictive value, specificity, sensitivity, and accuracy for high risk vs. low risk.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="JMRI-54-462-g003" position="anchor" id="jats-graphic-3"/>
      </fig>
    </sec>
    <sec id="jmri27599-sec-0023">
      <title>
Deep Learning Algorithm Outperforms <styled-content style="fixed-case" toggle="no">PI‐RADS</styled-content> for Classification
</title>
      <p>To further evaluate our model (AI‐biopsy), we were able to compare the PI‐RADS scores with the output of the trained model applied to images that have not been used in the training set. Specifically, we tested the trained model with 28 patients (11 high‐risk and 17 low‐risk, with available PI‐RADS scores), which were not used for training the algorithm (as a blind test set) obtained from the in‐house database. Our model correctly identified 75% (= 21/28 patients) of the labels for high‐risk (= 7/11 patients) and low‐risk (= 14/17 patients). Of the 28 patients, the PI‐RADS score identified 11 patients as high‐risk (PI‐RADS score = 4 and 5) and four patients as low‐risk (PI‐RADS score = 2 and 3). When compared to ground truth (pathology labels), the PI‐RADS score predicted 53.6% (= 15/28 patients) of the patients correctly.</p>
      <p>Cohen's kappa values for AI‐biopsy and PI‐RADS in comparison to pathology results as a reference standard were 0.467 (moderate) and 0.195 (slight), respectively.</p>
    </sec>
    <sec id="jmri27599-sec-0024">
      <title>
Discriminative Localization Using Deep Feature Detection
</title>
      <p>We reviewed the AI‐biopsy results for the above test set (<italic toggle="yes">n</italic> = 28 patients) to determine whether the disagreement between AI‐biopsy and pathology (reference standard) was due to incorrect feature selection by the AI‐biopsy. A comparison of the CAM result with the radiologists' results demonstrated that AI‐biopsy algorithm is able to detect the prostate gland when it predicts the pathology label correctly (Fig. <xref rid="jmri27599-fig-0003" ref-type="fig">3a</xref>), while the AI biopsy prediction is incorrect, when the algorithm does not detect the prostate gland (Fig. <xref rid="jmri27599-fig-0003" ref-type="fig">3b</xref>).</p>
      <fig position="float" fig-type="FIGURE" id="jmri27599-fig-0003">
        <label>FIGURE 3</label>
        <caption>
          <p>The highlighted prostate glands using class activation map (CAM) and radiologists. Model 2 classifies each image as high risk or low risk, and the deep feature analysis highlights the discriminative regions of the images. A radiologist marked the prostate gland of the images using green square dots. Biopsy results (based on Grade Groups [GGs]) as ground truth and Prostate Imaging Reporting and Data System (PI‐RADS) also are indicated in the figure. (a) Artificial intelligence (AI)‐biopsy predicts the risk level of cases (with a probability score for each class) and highlighted the prostate gland correctly. (b) AI‐biopsy is not able to predict the correct risk level of cases in which the prostate glands are not correctly detected. Red color illustrates features with higher weight.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="JMRI-54-462-g001" position="anchor" id="jats-graphic-5"/>
      </fig>
    </sec>
  </sec>
  <sec id="jmri27599-sec-0025">
    <title>Discussion</title>
    <p>The early and precise diagnosis of prostate cancer is important for proper management of patients. Integrating multimodal clinical data using DL methods has induced useful perceptions and denoted harmonious implementations of this approach to promise next‐generation diagnosis.</p>
    <p>The aim of this study was to determine whether a DL method using MRI data that were labeled according to histology results could improve accuracy of prostate cancer diagnosis. We trained a multimodal model to integrate the MR image and pathology score as predictors and further encode the interdependency between these diagnosis sets.</p>
    <p>There are two main levels of data integration: early fusion (data are integrated before feeding to the model) and late fusing (different trained model will be integrated using various ML techniques).<xref rid="jmri27599-bib-0020" ref-type="bibr"><sup>20</sup></xref> We used the early fusion technique and proposed CNN‐based system, AI‐biopsy, to fully utilize MR images and biopsy results to detect prostate cancer. We trained and validated AI‐biopsy using MR images of 400 patients that were labeled with histopathology information. In addition, compared to the PI‐RADS score, our model indicated higher agreement with biopsy results.</p>
    <p>Several groups have attempted to use DL‐based approaches for assessment of prostate cancer aggressiveness with varying degrees of success.<xref rid="jmri27599-bib-0034" ref-type="bibr"><sup>34</sup></xref>, <xref rid="jmri27599-bib-0035" ref-type="bibr"><sup>35</sup></xref>, <xref rid="jmri27599-bib-0036" ref-type="bibr"><sup>36</sup></xref> Cao and colleagues proposed a multiclass CNN (named FocalNet) to detect prostate lesions.<xref rid="jmri27599-bib-0036" ref-type="bibr"><sup>36</sup></xref> They used the MR images of 417 patients to predict GS using a homogeneous cohort and showed that their method outperformed U‐Net<xref rid="jmri27599-bib-0037" ref-type="bibr"><sup>37</sup></xref> and DeepLab,<xref rid="jmri27599-bib-0038" ref-type="bibr"><sup>38</sup></xref> both of which are CNN‐based methods.<xref rid="jmri27599-bib-0036" ref-type="bibr"><sup>36</sup></xref> They trained their model to predict four GSs: GS ≥ 7 vs. GS &lt; 7; GS ≥ 4 + 3 vs. GS ≤ 3 + 4; GS ≥ 8 vs. GS &lt; 8; and GS ≥ 9 vs. GS &lt; 9. Their result showed that FocalNet achieved AUCs of 0.81, 0.79, 0.67, and 0.57, respectively.<xref rid="jmri27599-bib-0036" ref-type="bibr"><sup>36</sup></xref> A recent study<xref rid="jmri27599-bib-0034" ref-type="bibr"><sup>34</sup></xref> has used apparent diffusion coefficient, a metric that is correlated with GS and an important component of mp‐MRI for determining aggressiveness of prostate cancer. They used MR images of 165 patients and predicted high‐risk (GS ≥ 7) from low‐risk (GS = 6) prostate cancer with an AUC of 0.79.<xref rid="jmri27599-bib-0034" ref-type="bibr"><sup>34</sup></xref> In addition, Yuan et al presented a DL‐based method to classify 123 patients with high‐risk cancer (GS = 4 + 3, and 8) and 98 patients with low‐risk cancer (GS = 3 + 4, and 3 + 3)<xref rid="jmri27599-bib-0035" ref-type="bibr"><sup>35</sup></xref> based on cropped mp‐MRI images. The best performance was obtained using a patch size of 28 × 28 pixels, which led to classifying the two groups with an AUC of 0.896.<xref rid="jmri27599-bib-0035" ref-type="bibr"><sup>35</sup></xref>
</p>
    <p>Although these methods achieved good accuracy in assessing prostate cancer aggressiveness, they required several time‐consuming preprocessing steps. Also, they were based on limited homogeneous data sets that did not cover all GSs. The advantage of our method is that instead of only focusing on predetermined, segmented features to analyze, the unsegmented image of the prostate (without bounding box) is assessed, allowing for quantification of all the available data. Our study used a large heterogeneous data set compared to those used in previous studies and included all GS lesion ranges (GS = 3 + 3, 3 + 4, 4 + 3, 4 + 4, 3 + 5, 5 + 3, 4 + 5, 5 + 4, and 5 + 5) as well as benign cases. Previous studies revealed that despite the heterogeneity between data, which is likely due to a combination of technical differences during data acquisition and the biological differences between study cohorts, the deep CNN models are able to accurately extract related signals from noises.<xref rid="jmri27599-bib-0016" ref-type="bibr"><sup>16</sup></xref> These studies found that the heterogeneity is gradually mitigated across the layers of the deep CNN model. The heterogeneity is strongest at the input layer but became insignificant at the output layer that makes a CNN model robust and generalizable to data outside the training data set.<xref rid="jmri27599-bib-0039" ref-type="bibr"><sup>39</sup></xref>
</p>
  </sec>
  <sec id="jmri27599-sec-0026">
    <title>Limitations</title>
    <p>Our data were obtained from five different data sets, and they were provided by different techniques (eg, imaging parameters and biopsy types) and annotated by various pathologists and radiologists who may use slightly different methods to assign the scores to each case. To address the heterogeneity among cases as well as lack of details about the clinical information of all cases (eg, patient's age and PSA level), we evaluated the algorithms through 5‐fold cross‐validation to indicate the generalizability of our models to various data sets. In addition, we only used axial T2w MR images in this study because we had more data in this category for both public and in‐house data sets. T2w MRI is routinely used for diagnosis and staging of prostate cancer; however, there is no limitation for using other types of images such as T1w by provided codes. Finally, our MR images were labeled using pathology labels that may include inaccurate histologic findings. Further studies are needed to consolidate the connection between MRI and prostate cancer diagnosis, particularly with available molecular subtypes of prostate cancer.</p>
  </sec>
  <sec id="jmri27599-sec-0027">
    <title>Conclusion</title>
    <p>AI‐biopsy is an automated DNN method (Fig. <xref rid="jmri27599-fig-0004" ref-type="fig">4</xref>) that increases the accuracy of PI‐RADS scoring for prostate cancer. The trained model integrates complementary information from biopsy report and improves prediction beyond what is possible with MR images alone. It does not require any manual segmentation for testing new images and can be implemented in clinical practice by providing a straightforward platform to use without requiring sophisticated computational knowledge (Fig. <xref rid="jmri27599-fig-0004" ref-type="fig">4</xref>).</p>
    <fig position="float" fig-type="FIGURE" id="jmri27599-fig-0004">
      <label>FIGURE 4</label>
      <caption>
        <p>AI‐biopsy is a fully automated framework to use in clinics for evaluation of the prostate cancer risk level. We employed a threshold condition on the output of both models for diagnosis using minimum seven T2w axial image slices. (a) While for prediction of benign diagnosis, all seven image slices should get <italic toggle="yes">P</italic> ≥ 0.5 for the benign class; (b) one image slice (out of seven imported image slices) with <italic toggle="yes">P</italic> ≥ 0.5 is enough for Model 1 to result in cancer prediction; (c) Model 2 needs at least two image slices (out of seven imported image slices) with high‐risk <italic toggle="yes">P</italic> ≥ 0.5 for a patient to result in high‐risk diagnosis; and (d) the result explanation could be seen by clicking on “N/A” option in the web interface (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://ai-biopsy.eipm-research.org" ext-link-type="uri">https://ai‐biopsy.eipm‐research.org</ext-link>).</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="JMRI-54-462-g002" position="anchor" id="jats-graphic-7"/>
    </fig>
  </sec>
  <sec sec-type="COI-statement" id="jmri27599-sec-0029">
    <title>Conflict of Interest</title>
    <p>The authors declare no competing interests.</p>
  </sec>
  <sec id="jmri27599-sec-0030">
    <title>Author Contributions</title>
    <p>P.K., A.Sb., O.E., B.C., and I.H. conceived the study. P.K., M.B., Q.L., E.K., and J.B. conceived the method and designed the algorithmic techniques. P.K., M.L., and M.E. generated the data sets and prepared and labeled the images for various Grade groups and Gleason scores. P.K., Q.L., E.K., C.R., and D.M. wrote the codes. P.K. performed computational analysis with input from A.Sb., O.E., B.C., and I.H. P.Z. and A.Si. developed the web interface. T.D.M. and A.Y. reviewed the MR images and PI‐RADS scores. B.D.R. reviewed pathological images, Gleason scores, and Grade groups. P.K. wrote the paper, and all authors read, edited, and approved the final manuscript.</p>
  </sec>
</body>
<back>
  <ack id="jmri27599-sec-0028">
    <title>Acknowledgments</title>
    <p>We thank Dr. Sohrab P. Shah and Dr. Nicole Rusk for helpful comments on the manuscript. This work was supported by start‐up funds (Weill Cornell Medicine) to I.H. This work used the Extreme Science and Engineering Discovery Environment (XSEDE) GPU servers through allocation IRI200018 to P.K. The authors thank Richard Kneppe, Tom Maiden, John Ruffing, and Hanif Khalak for their assistance with porting and optimization, which was made possible through the XSEDE Extended Collaborative Support Service (ECSS) program. P.K. also thanks Sinan Ramazanoglu and Hamid Mohamadi for their help with the source code and debugging. I.H. also acknowledges NVIDIA for the donation of Titan Xp to support this research through a GPU gift grant as well as XSEDE GPU servers through allocation ASC180052.</p>
  </ack>
  <ref-list id="jmri27599-bibl-0001" content-type="cited-references">
    <title>REFERENCES</title>
    <ref id="jmri27599-bib-0001">
      <label>1</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0001"><string-name><surname>Pilleron</surname><given-names>S</given-names></string-name>, <string-name><surname>Sarfati</surname><given-names>D</given-names></string-name>, <string-name><surname>Janssen‐Heijnen</surname><given-names>M</given-names></string-name>, et al. <article-title>Global cancer incidence in older adults, 2012 and 2035: A population‐based study</article-title>. <source>Int J Cancer</source><year>2019</year>;<volume>144</volume>(<issue>1</issue>):<fpage>49</fpage>‐<lpage>58</lpage>.<pub-id pub-id-type="pmid">29978474</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0002">
      <label>2</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0002"><string-name><surname>Hricak</surname><given-names>H</given-names></string-name>, <string-name><surname>Choyke</surname><given-names>PL</given-names></string-name>, <string-name><surname>Eberhardt</surname><given-names>SC</given-names></string-name>, <string-name><surname>Leibel</surname><given-names>SA</given-names></string-name>, <string-name><surname>Scardino</surname><given-names>PT</given-names></string-name>. <article-title>Imaging prostate cancer: A multidisciplinary perspective</article-title>. <source>Radiology</source><year>2007</year>;<volume>243</volume>(<issue>1</issue>):<fpage>28</fpage>‐<lpage>53</lpage>.<pub-id pub-id-type="pmid">17392247</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0003">
      <label>3</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0003"><string-name><surname>Barrett</surname><given-names>T</given-names></string-name>, <string-name><surname>Haider</surname><given-names>MA</given-names></string-name>. <article-title>The emerging role of MRI in prostate cancer active surveillance and ongoing challenges</article-title>. <source>AJR Am J Roentgenol</source><year>2017</year>;<volume>208</volume>(<issue>1</issue>):<fpage>131</fpage>‐<lpage>139</lpage>.<pub-id pub-id-type="pmid">27726415</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0004">
      <label>4</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0004"><string-name><surname>Hamdy</surname><given-names>FC</given-names></string-name>, <string-name><surname>Donovan</surname><given-names>JL</given-names></string-name>, <string-name><surname>Lane</surname><given-names>JA</given-names></string-name>, et al. <article-title>10‐year outcomes after monitoring, surgery, or radiotherapy for localized prostate cancer</article-title>. <source>N Engl J Med</source><year>2016</year>;<volume>375</volume>(<issue>15</issue>):<fpage>1415</fpage>‐<lpage>1424</lpage>.<pub-id pub-id-type="pmid">27626136</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0005">
      <label>5</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0005"><string-name><surname>Padhani</surname><given-names>AR</given-names></string-name>, <string-name><surname>Weinreb</surname><given-names>J</given-names></string-name>, <string-name><surname>Rosenkrantz</surname><given-names>AB</given-names></string-name>, <string-name><surname>Villeirs</surname><given-names>G</given-names></string-name>, <string-name><surname>Turkbey</surname><given-names>B</given-names></string-name>, <string-name><surname>Barentsz</surname><given-names>J</given-names></string-name>. <article-title>Prostate Imaging‐Reporting and Data System Steering Committee: PI‐RADS v2 status update and future directions</article-title>. <source>Eur Urol</source><year>2019</year>;<volume>75</volume>(<issue>3</issue>):<fpage>385</fpage>‐<lpage>396</lpage>.<pub-id pub-id-type="pmid">29908876</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0006">
      <label>6</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0006"><string-name><surname>Krishna</surname><given-names>S</given-names></string-name>, <string-name><surname>Schieda</surname><given-names>N</given-names></string-name>, <string-name><surname>McInnes</surname><given-names>MD</given-names></string-name>, <string-name><surname>Flood</surname><given-names>TA</given-names></string-name>, <string-name><surname>Thornhill</surname><given-names>RE</given-names></string-name>. <article-title>Diagnosis of transition zone prostate cancer using T2‐weighted (T2W) MRI: Comparison of subjective features and quantitative shape analysis</article-title>. <source>Eur Radiol</source><year>2019</year>;<volume>29</volume>(<issue>3</issue>):<fpage>1133</fpage>‐<lpage>1143</lpage>.<pub-id pub-id-type="pmid">30105411</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0007">
      <label>7</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0007"><string-name><surname>Vargas</surname><given-names>HA</given-names></string-name>, <string-name><surname>Hotker</surname><given-names>AM</given-names></string-name>, <string-name><surname>Goldman</surname><given-names>DA</given-names></string-name>, et al. <article-title>Updated prostate imaging reporting and data system (PIRADS v2) recommendations for the detection of clinically significant prostate cancer using multiparametric MRI: Critical evaluation using whole‐mount pathology as standard of reference</article-title>. <source>Eur Radiol</source><year>2016</year>;<volume>26</volume>(<issue>6</issue>):<fpage>1606</fpage>‐<lpage>1612</lpage>.<pub-id pub-id-type="pmid">26396111</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0008">
      <label>8</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0008"><string-name><surname>Portalez</surname><given-names>D</given-names></string-name>, <string-name><surname>Mozer</surname><given-names>P</given-names></string-name>, <string-name><surname>Cornud</surname><given-names>F</given-names></string-name>, et al. <article-title>Validation of the European Society of Urogenital Radiology Scoring System for prostate cancer diagnosis on multiparametric magnetic resonance imaging in a cohort of repeat biopsy patients</article-title>. <source>Eur Urol</source><year>2012</year>;<volume>62</volume>(<issue>6</issue>):<fpage>986</fpage>‐<lpage>996</lpage>.<pub-id pub-id-type="pmid">22819387</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0009">
      <label>9</label>
      <mixed-citation publication-type="book" id="jmri27599-cit-0009"><string-name><surname>Khalvati</surname><given-names>F</given-names></string-name>, <string-name><surname>Zhang</surname><given-names>Y</given-names></string-name>, <string-name><surname>Le</surname><given-names>PHU</given-names></string-name>, <string-name><surname>Gujrathi</surname><given-names>I</given-names></string-name>, <string-name><surname>Haider</surname><given-names>MA</given-names></string-name>. <chapter-title>PI‐RADS guided discovery radiomics for characterization of prostate lesions with diffusion‐weighted MRI</chapter-title>. <source>SPIE Medical Imaging</source>: <series>Computer‐Aided Diagnosis</series>, Vol. <volume>10950</volume>. <publisher-loc>San Diego, California, United States</publisher-loc>: <publisher-name>International Society for Optics and Photonics</publisher-name>; <year>2019</year>. 1095042 p. <pub-id pub-id-type="doi">10.1117/12.2512550</pub-id>.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0010">
      <label>10</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0010"><string-name><surname>Donovan</surname><given-names>MJ</given-names></string-name>, <string-name><surname>Fernandez</surname><given-names>G</given-names></string-name>, <string-name><surname>Scott</surname><given-names>R</given-names></string-name>, et al. <article-title>Development and validation of a novel automated Gleason grade and molecular profile that define a highly predictive prostate cancer progression algorithm‐based test</article-title>. <source>Prostate Cancer Prostatic Dis</source><year>2018</year>;<volume>21</volume>(<issue>4</issue>):<fpage>594</fpage>‐<lpage>603</lpage>.<pub-id pub-id-type="pmid">30087426</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0011">
      <label>11</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0011"><string-name><surname>Epstein</surname><given-names>JI</given-names></string-name>, <string-name><surname>Egevad</surname><given-names>L</given-names></string-name>, <string-name><surname>Amin</surname><given-names>MB</given-names></string-name>, <string-name><surname>Delahunt</surname><given-names>B</given-names></string-name>, <string-name><surname>Srigley</surname><given-names>JR</given-names></string-name>, <string-name><surname>Humphrey</surname><given-names>PA</given-names></string-name>. <article-title>The 2014 International Society of Urological Pathology (ISUP) consensus conference on Gleason grading of prostatic carcinoma: Definition of grading patterns and proposal for a new grading system</article-title>. <source>Am J Surg Pathol</source><year>2016</year>;<volume>40</volume>(<issue>2</issue>):<fpage>244</fpage>‐<lpage>252</lpage>.<pub-id pub-id-type="pmid">26492179</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0012">
      <label>12</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0012"><string-name><surname>Jones</surname><given-names>TA</given-names></string-name>, <string-name><surname>Radtke</surname><given-names>JP</given-names></string-name>, <string-name><surname>Hadaschik</surname><given-names>B</given-names></string-name>, <string-name><surname>Marks</surname><given-names>LS</given-names></string-name>. <article-title>Optimizing safety and accuracy of prostate biopsy</article-title>. <source>Curr Opin Urol</source><year>2016</year>;<volume>26</volume>(<issue>5</issue>):<fpage>472</fpage>‐<lpage>480</lpage>.<pub-id pub-id-type="pmid">27214580</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0013">
      <label>13</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0013"><string-name><surname>Litjens</surname><given-names>G</given-names></string-name>, <string-name><surname>Kooi</surname><given-names>T</given-names></string-name>, <string-name><surname>Bejnordi</surname><given-names>BE</given-names></string-name>, et al. <article-title>A survey on deep learning in medical image analysis</article-title>. <source>Med Image Anal</source><year>2017</year>;<volume>42</volume>:<fpage>60</fpage>‐<lpage>88</lpage>.<pub-id pub-id-type="pmid">28778026</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0014">
      <label>14</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0014"><string-name><surname>Shen</surname><given-names>D</given-names></string-name>, <string-name><surname>Wu</surname><given-names>G</given-names></string-name>, <string-name><surname>Suk</surname><given-names>HI</given-names></string-name>. <article-title>Deep learning in medical image analysis</article-title>. <source>Annu Rev Biomed Eng</source><year>2017</year>;<volume>19</volume>:<fpage>221</fpage>‐<lpage>248</lpage>.<pub-id pub-id-type="pmid">28301734</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0015">
      <label>15</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0015"><string-name><surname>Suzuki</surname><given-names>K</given-names></string-name>. <article-title>Overview of deep learning in medical imaging</article-title>. <source>Radiol Phys Technol</source><year>2017</year>;<volume>10</volume>(<issue>3</issue>):<fpage>257</fpage>‐<lpage>273</lpage>.<pub-id pub-id-type="pmid">28689314</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0016">
      <label>16</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0016"><string-name><surname>Khosravi</surname><given-names>P</given-names></string-name>, <string-name><surname>Kazemi</surname><given-names>E</given-names></string-name>, <string-name><surname>Imielinski</surname><given-names>M</given-names></string-name>, <string-name><surname>Elemento</surname><given-names>O</given-names></string-name>, <string-name><surname>Hajirasouliha</surname><given-names>I</given-names></string-name>. <article-title>Deep convolutional neural networks enable discrimination of heterogeneous digital pathology images</article-title>. <source>EBioMedicine</source><year>2018</year>;<volume>27</volume>:<fpage>317</fpage>‐<lpage>328</lpage>.<pub-id pub-id-type="pmid">29292031</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0017">
      <label>17</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0017"><string-name><surname>Khosravi</surname><given-names>P</given-names></string-name>, <string-name><surname>Kazemi</surname><given-names>E</given-names></string-name>, <string-name><surname>Zhan</surname><given-names>Q</given-names></string-name>, et al. <article-title>Deep learning enables robust assessment and selection of human blastocysts after in vitro fertilization</article-title>. <source>NPJ Digit Med</source><year>2019</year>;<volume>2</volume>(<issue>1</issue>):<fpage>21</fpage>.<pub-id pub-id-type="pmid">31304368</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0018">
      <label>18</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0018"><string-name><surname>Wang</surname><given-names>X</given-names></string-name>, <string-name><surname>Yang</surname><given-names>W</given-names></string-name>, <string-name><surname>Weinreb</surname><given-names>J</given-names></string-name>, et al. <article-title>Searching for prostate cancer by fully automated magnetic resonance imaging classification: Deep learning versus non‐deep learning</article-title>. <source>Sci Rep</source><year>2017</year>;<volume>7</volume>(<issue>1</issue>):<fpage>15415</fpage>.<pub-id pub-id-type="pmid">29133818</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0019">
      <label>19</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0019"><string-name><surname>Kwon</surname><given-names>D</given-names></string-name>, <string-name><surname>Reis</surname><given-names>IM</given-names></string-name>, <string-name><surname>Breto</surname><given-names>AL</given-names></string-name>, et al. <article-title>Classification of suspicious lesions on prostate multiparametric MRI using machine learning</article-title>. <source>J Med Imaging (Bellingham)</source><year>2018</year>;<volume>5</volume>(<issue>3</issue>):<elocation-id>034502</elocation-id>.<pub-id pub-id-type="pmid">30840719</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0020">
      <label>20</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0020"><string-name><surname>Lopez</surname><given-names>K</given-names></string-name>, <string-name><surname>Fodeh</surname><given-names>SJ</given-names></string-name>, <string-name><surname>Allam</surname><given-names>A</given-names></string-name>, <string-name><surname>Brandt</surname><given-names>CA</given-names></string-name>, <string-name><surname>Krauthammer</surname><given-names>M</given-names></string-name>. <article-title>Reducing annotation burden through multimodal learning</article-title>. <source>Front Big Data</source><year>2020</year>;<volume>3</volume>:<fpage>19</fpage>.<pub-id pub-id-type="pmid">33693393</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0021">
      <label>21</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0021"><string-name><surname>Clark</surname><given-names>K</given-names></string-name>, <string-name><surname>Vendt</surname><given-names>B</given-names></string-name>, <string-name><surname>Smith</surname><given-names>K</given-names></string-name>, et al. <article-title>The cancer imaging archive (TCIA): Maintaining and operating a public information repository</article-title>. <source>J Digit Imaging</source><year>2013</year>;<volume>26</volume>(<issue>6</issue>):<fpage>1045</fpage>‐<lpage>1057</lpage>.<pub-id pub-id-type="pmid">23884657</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0022">
      <label>22</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0022"><string-name><surname>Litjens</surname><given-names>G</given-names></string-name>, <string-name><surname>Debats</surname><given-names>O</given-names></string-name>, <string-name><surname>Barentsz</surname><given-names>J</given-names></string-name>, <string-name><surname>Karssemeijer</surname><given-names>N</given-names></string-name>, <string-name><surname>Huisman</surname><given-names>H</given-names></string-name>. <article-title>Computer‐aided detection of prostate cancer in MRI</article-title>. <source>IEEE Trans Med Imaging</source><year>2014</year>;<volume>33</volume>(<issue>5</issue>):<fpage>1083</fpage>‐<lpage>1092</lpage>.<pub-id pub-id-type="pmid">24770913</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0023">
      <label>23</label>
      <mixed-citation publication-type="miscellaneous" id="jmri27599-cit-0023"><string-name><surname>Geert Litjens</surname><given-names>OD</given-names></string-name>, <string-name><surname>Barentsz</surname><given-names>J</given-names></string-name>, <string-name><surname>Karssemeijer</surname><given-names>N</given-names></string-name>, <string-name><surname>Huisman</surname><given-names>H</given-names></string-name>. <article-title>ProstateX Challenge data. Cancer Imaging Arch</article-title>; <year>2017</year>;10:K9TCIA.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0024">
      <label>24</label>
      <mixed-citation publication-type="miscellaneous" id="jmri27599-cit-0024"><string-name><surname>Choyke</surname><given-names>PTB</given-names></string-name>, <string-name><surname>Pinto</surname><given-names>P</given-names></string-name>, <string-name><surname>Merino</surname><given-names>M</given-names></string-name>, <string-name><surname>Wood</surname><given-names>B</given-names></string-name>. <article-title>Data from PROSTATE‐MRI. Cancer Imaging Arch</article-title>; <year>2016</year>;9. http://doi.org/10.7937K.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0025">
      <label>25</label>
      <mixed-citation publication-type="miscellaneous" id="jmri27599-cit-0025"><string-name><surname>Bloch</surname><given-names>BN</given-names></string-name>, <string-name><surname>Jain</surname><given-names>A</given-names></string-name>, <string-name><surname>Jaffe</surname><given-names>CC</given-names></string-name>. <article-title>Data from PROSTATE‐DIAGNOSIS. Cancer Imaging Arch</article-title>; <year>2015</year>;9:10.7937.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0026">
      <label>26</label>
      <mixed-citation publication-type="miscellaneous" id="jmri27599-cit-0026"><string-name><surname>Zuley</surname><given-names>ML</given-names></string-name>, <string-name><surname>Jarosz</surname><given-names>R</given-names></string-name>, <string-name><surname>Drake</surname><given-names>BF</given-names></string-name>, et al. <article-title>Radiology Data from The Cancer Genome Atlas Prostate Adenocarcinoma [TCGA‐PRAD] collection. Cancer Imaging Arch</article-title>; <year>2016</year>;9. http://doi.org/10.7937K.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0027">
      <label>27</label>
      <mixed-citation publication-type="miscellaneous" id="jmri27599-cit-0027"><collab collab-type="authors">(NCCN) NCCN</collab>
. <article-title>Prostate Cancer (Version 4.2018)</article-title>. <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0028">
      <label>28</label>
      <mixed-citation publication-type="book" id="jmri27599-cit-0028"><string-name><surname>Szegedy</surname><given-names>C</given-names></string-name>, <string-name><surname>Liu</surname><given-names>W</given-names></string-name>, <string-name><surname>Jia</surname><given-names>Y</given-names></string-name>, et al. <chapter-title>Going deeper with convolutions</chapter-title>. <source>Computer vision and pattern recognition (CVPR)</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2015</year>. p <fpage>1</fpage>‐<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0029">
      <label>29</label>
      <mixed-citation publication-type="book" id="jmri27599-cit-0029"><string-name><surname>Deng</surname><given-names>J</given-names></string-name>, <string-name><surname>Dong</surname><given-names>W</given-names></string-name>, <string-name><surname>Socher</surname><given-names>R</given-names></string-name>, <string-name><surname>Li</surname><given-names>L</given-names></string-name>, <string-name><surname>Kai</surname><given-names>L</given-names></string-name>, <string-name><surname>Li</surname><given-names>F‐F</given-names></string-name>. <chapter-title>ImageNet: A large‐scale hierarchical image database</chapter-title>. <source>2009 IEEE Conference on Computer Vision and Pattern Recognition</source>. <publisher-loc>Miami, FL</publisher-loc>: <publisher-name>IEEE</publisher-name>; <year>2009</year>. p <fpage>248</fpage>‐<lpage>255</lpage>.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0030">
      <label>30</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0030"><string-name><surname>Towns</surname><given-names>J</given-names></string-name>, <string-name><surname>Cockerill</surname><given-names>T</given-names></string-name>, <string-name><surname>Dahan</surname><given-names>M</given-names></string-name>, et al. <article-title>XSEDE: Accelerating scientific discovery</article-title>. <source>Comput Sci Eng</source><year>2014</year>;<volume>16</volume>(<issue>5</issue>):<fpage>62</fpage>‐<lpage>74</lpage>.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0031">
      <label>31</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0031"><string-name><surname>Shorten</surname><given-names>C</given-names></string-name>, <string-name><surname>Taghi</surname><given-names>M</given-names></string-name>. <article-title>Khoshgoftaar. A survey on image data augmentation for deep learning</article-title>. <source>J Big Data</source><year>2019</year>;<volume>6</volume>(<issue>1</issue>):<fpage>1</fpage>‐<lpage>48</lpage>.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0032">
      <label>32</label>
      <mixed-citation publication-type="miscellaneous" id="jmri27599-cit-0032"><string-name><surname>Zhou</surname><given-names>B</given-names></string-name>, <string-name><surname>Khosla</surname><given-names>A</given-names></string-name>, <string-name><surname>Lapedriza</surname><given-names>A</given-names></string-name>, <string-name><surname>Oliva</surname><given-names>A</given-names></string-name>, <string-name><surname>Torralba</surname><given-names>A</given-names></string-name>. <article-title>Learning deep features for discriminative localization. In: <italic toggle="yes">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</italic>
</article-title>; <year>2016</year>. p <fpage>2921</fpage>–<lpage>2929</lpage>.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0033">
      <label>33</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0033"><string-name><surname>Cohen</surname><given-names>JA</given-names></string-name>. <article-title>Coefficient of agreement for nominal scales</article-title>. <source>Educ Psychol Meas</source><year>1960</year>;<volume>20</volume>(<issue>1</issue>):<fpage>37</fpage>‐<lpage>46</lpage>.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0034">
      <label>34</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0034"><string-name><surname>Woo</surname><given-names>S</given-names></string-name>, <string-name><surname>Kim</surname><given-names>SY</given-names></string-name>, <string-name><surname>Cho</surname><given-names>JY</given-names></string-name>, <string-name><surname>Kim</surname><given-names>SH</given-names></string-name>. <article-title>Preoperative evaluation of prostate cancer aggressiveness: Using ADC and ADC ratio in determining Gleason score</article-title>. <source>Am J Roentgenol</source><year>2016</year>;<volume>207</volume>(<issue>1</issue>):<fpage>114</fpage>‐<lpage>120</lpage>.<pub-id pub-id-type="pmid">27077643</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0035">
      <label>35</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0035"><string-name><surname>Yuan</surname><given-names>YX</given-names></string-name>, <string-name><surname>Qin</surname><given-names>WJ</given-names></string-name>, <string-name><surname>Buyyounouski</surname><given-names>M</given-names></string-name>, et al. <article-title>Prostate cancer classification with multiparametric MRI transfer learning model</article-title>. <source>Med Phys</source><year>2019</year>;<volume>46</volume>(<issue>2</issue>):<fpage>756</fpage>‐<lpage>765</lpage>.<pub-id pub-id-type="pmid">30597561</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0036">
      <label>36</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0036"><string-name><surname>Cao</surname><given-names>R</given-names></string-name>, <string-name><surname>Bajgiran</surname><given-names>AM</given-names></string-name>, <string-name><surname>Mirak</surname><given-names>SA</given-names></string-name>, et al. <article-title>Joint prostate cancer detection and Gleason score prediction in mp‐MRI via FocalNet</article-title>. <source>IEEE Trans Med Imaging</source><year>2019</year>;<volume>38</volume>:<fpage>2496</fpage>‐<lpage>2506</lpage>.<pub-id pub-id-type="pmid">30835218</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0037">
      <label>37</label>
      <mixed-citation publication-type="book" id="jmri27599-cit-0037"><string-name><surname>Ronneberger</surname><given-names>O</given-names></string-name>, <string-name><surname>Fischer</surname><given-names>P</given-names></string-name>, <string-name><surname>Brox</surname><given-names>T</given-names></string-name>. <chapter-title>U‐Net: Convolutional networks for biomedical image segmentation</chapter-title>. <source>Medical image computing and computer‐assisted intervention</source>, Pt III, Vol <volume>9351</volume>; <publisher-loc>Switzerland</publisher-loc>: <publisher-name>Springer</publisher-name>; <year>2015</year>. p <fpage>234</fpage>‐<lpage>241</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id>.</mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0038">
      <label>38</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0038"><string-name><surname>Chen</surname><given-names>LC</given-names></string-name>, <string-name><surname>Papandreou</surname><given-names>G</given-names></string-name>, <string-name><surname>Kokkinos</surname><given-names>I</given-names></string-name>, <string-name><surname>Murphy</surname><given-names>K</given-names></string-name>, <string-name><surname>Yuille</surname><given-names>AL</given-names></string-name>. <article-title>DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source><year>2018</year>;<volume>40</volume>(<issue>4</issue>):<fpage>834</fpage>‐<lpage>848</lpage>.<pub-id pub-id-type="pmid">28463186</pub-id></mixed-citation>
    </ref>
    <ref id="jmri27599-bib-0039">
      <label>39</label>
      <mixed-citation publication-type="journal" id="jmri27599-cit-0039"><string-name><surname>Hu</surname><given-names>Z</given-names></string-name>, <string-name><surname>Tang</surname><given-names>A</given-names></string-name>, <string-name><surname>Singh</surname><given-names>J</given-names></string-name>, <string-name><surname>Bhattacharya</surname><given-names>S</given-names></string-name>, <string-name><surname>Butte</surname><given-names>AJ</given-names></string-name>. <article-title>A robust and interpretable end‐to‐end deep learning model for cytometry data</article-title>. <source>Proc Natl Acad Sci U S A</source><year>2020</year>;<volume>117</volume>(<issue>35</issue>):<fpage>21373</fpage>‐<lpage>21380</lpage>.<pub-id pub-id-type="pmid">32801215</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
