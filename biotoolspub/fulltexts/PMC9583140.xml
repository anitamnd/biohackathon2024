<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Publishing DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName journalpublishing.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Plant Sci</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Plant Sci</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Plant Sci.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Plant Science</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-462X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9583140</article-id>
    <article-id pub-id-type="doi">10.3389/fpls.2022.964058</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Plant Science</subject>
        <subj-group>
          <subject>Technology and Code</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>JustDeepIt: Software tool with graphical and character user interfaces for deep learning-based object detection and segmentation in image analysis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Sun</surname>
          <given-names>Jianqiang</given-names>
        </name>
        <xref rid="fn001" ref-type="author-notes">
          <sup>*</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/487678"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cao</surname>
          <given-names>Wei</given-names>
        </name>
        <uri xlink:href="https://loop.frontiersin.org/people/2024458"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Yamanaka</surname>
          <given-names>Takehiko</given-names>
        </name>
        <uri xlink:href="https://loop.frontiersin.org/people/1560035"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><institution>Research Center for Agricultural Information Technology, National Agriculture and Food Research Organization (NARO)</institution>, <addr-line>Tsukuba</addr-line>, <country>Japan</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Jucheng Yang, Tianjin University of Science and Technology, China</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Michael Gomez Selvaraj, Consultative Group on International Agricultural Research (CGIAR), United States; Hui Feng, Huazhong Agricultural University, China; Yuan Wang, Tianjin University of Science and Technology, China</p>
      </fn>
      <corresp id="fn001">*Correspondence: Jianqiang Sun, <email xlink:href="mailto:sun@biunit.dev">sun@biunit.dev</email>
</corresp>
      <fn fn-type="other" id="fn002">
        <p>This article was submitted to Sustainable and Intelligent Phytoprotection, a section of the journal Frontiers in Plant Science</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>06</day>
      <month>10</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>13</volume>
    <elocation-id>964058</elocation-id>
    <history>
      <date date-type="received">
        <day>08</day>
        <month>6</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>9</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Sun, Cao and Yamanaka</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Sun, Cao and Yamanaka</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Image processing and analysis based on deep learning are becoming mainstream and increasingly accessible for solving various scientific problems in diverse fields. However, it requires advanced computer programming skills and a basic familiarity with character user interfaces (CUIs). Consequently, programming beginners face a considerable technical hurdle. Because potential users of image analysis are experimentalists, who often use graphical user interfaces (GUIs) in their daily work, there is a need to develop GUI-based easy-to-use deep learning software to support their work. Here, we introduce JustDeepIt, a software written in Python, to simplify object detection and instance segmentation using deep learning. JustDeepIt provides both a GUI and a CUI. It contains various functional modules for model building and inference, and it is built upon the popular PyTorch, MMDetection, and Detectron2 libraries. The GUI is implemented using the Python library FastAPI, simplifying model building for various deep learning approaches for beginners. As practical examples of JustDeepIt, we prepared four case studies that cover critical issues in plant science: (1) wheat head detection with Faster R-CNN, YOLOv3, SSD, and RetinaNet; (2) sugar beet and weed segmentation with Mask R-CNN; (3) plant segmentation with U<sup>2</sup>-Net; and (4) leaf segmentation with U<sup>2</sup>-Net. The results support the wide applicability of JustDeepIt in plant science applications. In addition, we believe that JustDeepIt has the potential to be applied to deep learning-based image analysis in various fields beyond plant science.</p>
    </abstract>
    <kwd-group>
      <kwd>Deep learning</kwd>
      <kwd>image recognition</kwd>
      <kwd>object detection</kwd>
      <kwd>instance segmentation</kwd>
      <kwd>leaf segmentation</kwd>
      <kwd>plant segmentation</kwd>
      <kwd>graphical user interface</kwd>
    </kwd-group>
    <counts>
      <fig-count count="3"/>
      <table-count count="0"/>
      <equation-count count="0"/>
      <ref-count count="44"/>
      <page-count count="9"/>
      <word-count count="3772"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="s1">
    <title>1 Introduction</title>
    <p>Over the past decade, remarkable advances have been made in image analysis based on deep learning in various fields (<xref rid="B17" ref-type="bibr">Jiang and Li, 2020</xref>; <xref rid="B4" ref-type="bibr">Ben Yedder et al., 2021</xref>; <xref rid="B22" ref-type="bibr">Liu and Wang, 2021</xref>). In practical applications in plant science field, deep learning for image analyses has been applied at different scales. For example, at the field scale, studies such as high-throughput phenotyping and yield prediction via images captured by drones or hyperspectral cameras are hot topics (<xref rid="B19" ref-type="bibr">Kamilaris and Prenafeta-Boldú, 2018</xref>; <xref rid="B17" ref-type="bibr">Jiang and Li, 2020</xref>). On an individual scale, studies including species classification, crop disease detection, and weed detection are well researched (<xref rid="B10" ref-type="bibr">Christin et al., 2019</xref>; <xref rid="B14" ref-type="bibr">Hasan et al., 2021</xref>; <xref rid="B22" ref-type="bibr">Liu and Wang, 2021</xref>). In addition, at the cell level, studies such as cell type identification and stomata identification via microscopic images have been performed (<xref rid="B25" ref-type="bibr">Moen et al., 2019</xref>; <xref rid="B44" ref-type="bibr">Zhu et al., 2021</xref>). The increased availability of these techniques in various fields enhances the importance of the roles they are expected to play in the future.</p>
    <p>Image analysis based on deep learning can be roughly categorized into three main tasks: object classification, object detection, and instance segmentation. Object classification determines the class of an object in an image. Object detection specifies the types of objects in an image and their locations, generally through bounding boxes (i.e., rectangular delimiting areas). Instance segmentation selects a pixel-wise mask for each object in an image. As images generally contain multiple objects, object detection and instance segmentation have broader practical applications than object classification. In addition to instance segmentation, salient object detection is often used to detect the primary object in an image at the pixel level (<xref rid="B39" ref-type="bibr">Wang et al., 2017</xref>; <xref rid="B6" ref-type="bibr">Borji et al., 2019</xref>). It can also be applied to one-class instance segmentation for background removal, leaf segmentation, and root segmentation.</p>
    <p>Applying machine learning models to images captured under conditions different from those of images captured for model training degrades the inference performance. This is called the frame problem in machine learning (<xref rid="B12" ref-type="bibr">Ford and Pylyshyn, 1996</xref>). It is impossible to solve this problem without collecting training images under all conditions. Hence, most practical applications restrict the usage conditions to ensure that the frame is not exceeded by limiting the target objects, shooting conditions, or by other means. In scientific studies, this problem is addressed through models devised for specific projects instead of previously developed models.</p>
    <p>Python programming language and its libraries PyTorch (<xref rid="B26" ref-type="bibr">Paszke et al., 2019</xref>), MMDetection (<xref rid="B9" ref-type="bibr">Chen et al., 2019</xref>), and Detectron2 (<xref rid="B40" ref-type="bibr">Wu et al., 2019</xref>), have facilitated image analysis using deep learning. However, programming experience and machine learning expertise are required to implement complicated neural networks (i.e., deep learning models) for object detection and instance segmentation tasks.</p>
    <p>Given the required programming skills or machine learning expertise, the application of deep learning remains challenging for most beginners. Many experimentalists working full-time on wet experiments use graphical user interfaces (GUIs) in their daily work. In contrast, informatics researchers use character user interfaces (CUIs) because it is easy to perform large-scale experiments under different parameter combinations owing to the scalabilities of CUIs. Thus, suitable software should be supported on GUIs and CUIs to serve various users, making it advantageous for experimental and informatics researchers to use the same software and conduct collaborative research. Nevertheless, most existing open-source GUI-based software for deep learning-based image analysis only supports segmentation or is intended for specific purposes. For instance, RootPainter (<xref rid="B35" ref-type="bibr">Smith et al., 2022</xref>) and DeepMIB (<xref rid="B3" ref-type="bibr">Belevich and Jokitalo, 2021</xref>) support biological image segmentation using U-Net (<xref rid="B33" ref-type="bibr">Ronneberger et al., 2015</xref>). ZeroCostDL4Mic (<xref rid="B38" ref-type="bibr">von Chamier et al., 2021</xref>) implements You Only Look Once version 2 (YOLOv2) (<xref rid="B30" ref-type="bibr">Redmon and Farhadi, 2017</xref>) and U-Net for object detection and instance segmentation against microscopy images. Maize-IAS (<xref rid="B42" ref-type="bibr">Zhou et al., 2021</xref>) partially uses a faster region-based convolutional neural network (Faster R-CNN) (<xref rid="B32" ref-type="bibr">Ren et al., 2017</xref>) for leaf segmentation and leaf counting of maize images captured under the controlled environment. Moreover, most solutions are focused on GUIs but neglect CUIs, thus hindering expansion on the user side. Therefore, there is a high demand for image analysis software based on deep learning supporting both easy-to-use GUIs and high-scalability CUIs, to satisfy a diverse user base.</p>
    <p>We developed the JustDeepIt software supporting GUI and CUI to train models and perform inference for object detection, instance segmentation, and salient object detection. JustDeepIt can be applied to many biological problems, such as wheat head detection, plant segmentation, and leaf segmentation. In addition, it provides an intuitive solution for biologists lacking programming experience and machine learning expertise, simplifying implementation compared with conventional programming schemes.</p>
  </sec>
  <sec id="s2">
    <title>2 Method</title>
    <p>JustDeepIt is implemented using Python and is easy to interoperate with various Python packages. It provides GUI and CUI for deep learning-based image analysis, including object detection, instance segmentation, and salient object detection (<xref rid="f1" ref-type="fig"><bold>Figure 1</bold></xref>). The source code is deposited in GitHub at <uri xlink:href="https://github.com/biunit/JustDeepIt">https://github.com/biunit/JustDeepIt</uri> under an MIT License. An overview of implementations of the user interfaces and main functions of JustDeepIt are described in the following subsections. Detailed documentation, including installation guides and tutorials, is available on the project website.</p>
    <fig position="float" id="f1">
      <label>Figure 1</label>
      <caption>
        <p>Overview of GUI and CUI implementation of JustDeepIt. For GUI usage, four main screens are implemented to select the analysis task, set standard parameters (e.g., neural network architecture, temporary folder), train a model, and infer new images with the trained model, respectively. For CUI usage, import the modules based on analysis tasks and then use the initialization method (e.g., <bold>OD</bold>), <bold>train, save, and inference</bold> methods to set up a model, train the model, save the model, and infer new images with the trained model, respectively.</p>
      </caption>
      <graphic xlink:href="fpls-13-964058-g001" position="float"/>
    </fig>
    <sec id="s2_1">
      <title>2.1 Implementation of user interfaces</title>
      <p>The GUI of JustDeepIt is implemented using FastAPI (<xref rid="B29" ref-type="bibr">Ramírez, 2018</xref>), a straightforward Python library for building simple web applications. It allows deep learning-based image analysis tasks with simple mouse and keyboard operations without writing codes (<xref rid="f1" ref-type="fig"><bold>Figure 1</bold></xref>). Upon launching the GUI, the user is prompted to select an analysis task; after this selection, the analysis screen is displayed. The analysis screen has three tabs, <italic>Preferences</italic>, <italic>Training</italic>, and <italic>Inference</italic>. The <italic>Preferences</italic> tab allows users to set standard parameters for training and inference, such as neural network architecture, class labels (object names targeted for detection or segmentation), a temporary directory to save the intermediate and final results, and more. The <italic>Training</italic> and <italic>Inference</italic> tabs are used to train a model by specifying training images and the corresponding annotations and inferring new images with the trained model, respectively.</p>
      <p>The CUI of JustDeepIt can be used via application programming interfaces (APIs). The most complicated procedures (e.g., data registration, model initialization, output adjustment) are encapsulated into the APIs containing a few intelligible functions to simplify usage. The three main API functions are <bold>train</bold> for training detection or segmentation models, <bold>save</bold> for saving the trained model weights, and <bold>inference</bold> for detection or segmentation on test images. Example codes for training object detection models and using the model for inference are shown in <xref rid="f1" ref-type="fig"><bold>Figure 1</bold></xref>. Additional usage examples (e.g., building a web application) and arguments of these functions are available on the project website.</p>
    </sec>
    <sec id="s2_2">
      <title>2.2 Object detection and instance segmentation</title>
      <p>Object detection and instance segmentation models in JustDeepIt are internally built based on the MMDetection (<xref rid="B9" ref-type="bibr">Chen et al., 2019</xref>) or Detectron2 (<xref rid="B40" ref-type="bibr">Wu et al., 2019</xref>) libraries. The user can choose MMDetection or Detectron2 as the backend to build the corresponding models. JustDeepIt supports various well-known neural network architectures. For object detection, Faster R-CNN (<xref rid="B32" ref-type="bibr">Ren et al., 2017</xref>), YOLOv3 (<xref rid="B31" ref-type="bibr">Redmon and Farhadi, 2018</xref>), Single-Shot Multibox Detector (SSD) (<xref rid="B21" ref-type="bibr">Liu et al., 2016</xref>), and RetinaNet (<xref rid="B20" ref-type="bibr">Lin et al., 2018</xref>) are available to meet different user needs. For instance segmentation, Mask R-CNN (<xref rid="B15" ref-type="bibr">He et al., 2017</xref>) is available. Furthermore, JustDeepIt allows user-customized neural network architectures to accommodate users who wish to use architectures that are not implemented in the software. For example, Faster R-CNN implemented in JustDeepIt uses VGGNet (<xref rid="B34" ref-type="bibr">Simonyan and Zisserman, 2015</xref>) for feature extraction during the detection process; the users may want to change VGGNet to other architectures such as ResNet (<xref rid="B16" ref-type="bibr">He et al., 2015</xref>). To accomplish this, users can (i) either download already-prepared architecture configuration files from MMDetection or Detectron2 GitHub repositories or create their own configuration file from scratch and then (ii) input the configuration file into JustDeepIt to build a model for training and inference.</p>
      <p>For model training, JustDeepIt requires image annotations in the Common Objects in Context (COCO) format, which can be generated using GUI-based free software such as COCO Annotator (<xref rid="B7" ref-type="bibr">Brooks, 2021</xref>) and Computer Vision Annotation Tool (<xref rid="B5" ref-type="bibr">Boris et al., 2021</xref>). After the user specifies the location of the image dataset and corresponding annotations through the <italic>Training</italic> tab of GUI or the training function of CUI, JustDeepIt uses them to build the related model.</p>
      <p>For object detection using the trained model, the user can specify the trained weights and folder containing the test images for detection through the <italic>Inference</italic> tab of GUI or the inference function of CUI. For GUI usage, the inference results are automatically saved as images with bounding boxes or contour lines around the detected objects and a JSON file of the inference results in COCO format. For CUI usage, the user can specify whether the inference results should be saved as annotated images or an annotation file.</p>
    </sec>
    <sec id="s2_3">
      <title>2.3 Salient object detection</title>
      <p>The module for salient object detection in JustDeepIt is based on U<sup>2</sup>-Net (<xref rid="B27" ref-type="bibr">Qin et al., 2020</xref>) and written using the PyTorch library. The GUI and the training and detection functions processing are similar to those used for object detection.</p>
      <p>JustDeepIt requires training images with annotations (either COCO format annotations or mask images) for model training. Although the U<sup>2</sup>-Net implementation for JustDeepIt requires images of 288 × 288 pixels as the canonical input, images of various sizes are captured for applications in plant science. Thus, JustDeepIt provides two approaches for training on images of various sizes: <italic>resizing</italic> and <italic>random cropping</italic> (<xref rid="f2" ref-type="fig"><bold>Figure 2A</bold></xref>). <italic>Resizing</italic> changes the image resolution to 288 × 288 pixels for training in U<sup>2</sup>-Net. This approach is suitable for images containing a few large target objects. In contrast, <italic>random cropping</italic> randomly selects small areas of <italic>p</italic> × <italic>p</italic> pixels at random angles from the original images, where <italic>p</italic> can be specified according to the complexity of the intended images and tasks. The images of <italic>p</italic> × <italic>p</italic> pixels are then resized to 288 × 288 pixels for training. This approach is suitable for images containing many small target objects and details.</p>
      <fig position="float" id="f2">
        <label>Figure 2</label>
        <caption>
          <p><bold>(A)</bold> Training approaches of <italic>resizing</italic> and <italic>random cropping</italic> are implemented in JustDeepIt. <bold>(B)</bold> Inference approaches of <italic>resizing</italic> and <italic>sliding</italic> (corresponding to training using <italic>resizing</italic> and <italic>random cropping</italic>, respectively) are implemented in JustDeepIt.</p>
        </caption>
        <graphic xlink:href="fpls-13-964058-g002" position="float"/>
      </fig>
      <p>JustDeepIt implements <italic>resizing</italic> and <italic>sliding</italic> for salient object detection (<xref rid="f2" ref-type="fig"><bold>Figure 2B</bold></xref>). <italic>Resizing</italic> changes the scale of the input images to 288× 288 pixels for detection and then restores the detection result to the input size. <italic>Sliding</italic> first crops square areas of <italic>p</italic> × <italic>p</italic> pixels from the top left to the bottom right of the image step-by-step, where <italic>p</italic> can be specified by the user. It then resizes the areas to 288 × 288 pixels to perform salient object detection in each area. Finally, it merges the processed areas into a single image. In addition to salient object detection, summarization functions (e.g., counting the number of objects in the image, quantifying colors, measuring the area of each object) are available in JustDeepIt.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>3 Results</title>
    <p>We prepared four case studies as practical examples of JustDeepIt and reported the results in this section. Detailed procedures for these case studies can be found in the JustDeepIt documentation.</p>
    <sec id="s3_1">
      <title>3.1 Wheat head detection</title>
      <p>We show an example of JustDeepIt performing object detection for the wheat head, a prevalent task in plant science. The global wheat head detection (GWHD) dataset, containing 4700 images of 1024 × 1024 pixels for wheat head detection evaluation (<xref rid="B11" ref-type="bibr">David et al., 2020</xref>), was used in this case study. We randomly selected 80% of the images in the GWHD dataset for training and used the remaining 20% for validation. We constructed Faster R-CNN, YOLOv3, SSD, and RetinaNet with MMDetection backend and Faster R-CNN and RetinaNet with Detectron2 backend for training and validation with the GWHD dataset. To initialize each architecture, we retrieved the pretraining weights from the GitHub repositories of MMDetection and Detectron2.</p>
      <p>The training was performed for 100 epochs with a batch size of eight and an initial learning rate of 0.0001. Validation was performed using the trained architectures against the validation images, and the mean average precision (mAP) was calculated from the validation results. Training and validation were independently repeated five times to mitigate the influence of randomness. These processes were executed on an Ubuntu 18.04 system equipped with an NVIDIA Tesla V100 SXM2 graphics processor, an Intel Xeon Gold 6254 processor, and 64 GB of memory.</p>
      <p>For the five training and validation runs, Faster R-CNN provided a relatively high validation mAP with a relatively slow training speed, YOLOv3 and SSD provided lower mAP with faster training speed, and RetinaNet provided intermediate mAP and training speed (<xref rid="f3" ref-type="fig"><bold>Figure 3A</bold></xref>). In addition, the Faster R-CNN and RetinaNet with MMDetection backend provided slower training and higher validation mAP than those with the Detectron2 backend. Hence, different backends and neural network architectures provided distinct performances and training speeds, and users should select the appropriate backend and architecture according to the application.</p>
      <fig position="float" id="f3">
        <label>Figure 3</label>
        <caption>
          <p>Results of three case studies. <bold>(A)</bold> Training time and validation mAP of object detection. <bold>(B)</bold> Training time and validation mAP of instance segmentation. <bold>(C)</bold> The process and analysis result (projected area and color hue) of plant segmentation with time-series plant images. <bold>(D)</bold> Processes and sample results of iterative training of U<sup>2</sup>-Net for leaf segmentation.</p>
        </caption>
        <graphic xlink:href="fpls-13-964058-g003" position="float"/>
      </fig>
    </sec>
    <sec id="s3_2">
      <title>3.2 Sugar beets and weeds segmentation</title>
      <p>To represent the case study of instance segmentation with JustDeepIt, we performed weed and crop (here, sugar beet) segmentation, which is one of the tasks in high-demand in the agriculture sector, on the SugarBeets2016 dataset (<xref rid="B8" ref-type="bibr">Chebrolu et al., 2017</xref>). The SugarBeets2016 dataset has 11,552 RGB images captured under fields and the annotations for sugar beets and weeds. We randomly selected 5,000 and 1,000 images for training and validation, respectively. We constructed Mask R-CNN with MMDetection and Detectron2 backends for training and validation. We retrieved the pretraining weights from the GitHub repositories for initializing each architecture, as in subsection 3.1.</p>
      <p>Training and validation were performed using the same parameters, processes, and system devices as in subsection 3.1, except that the batch size was set to four. The Dice coefficient was calculated from the validation results. For the five training and validation runs, Mask R-CNN with Detectron2 backend provided relatively higher Dice coefficients and faster training speed than that with MMDetection backend (<xref rid="f3" ref-type="fig"><bold>Figure 3B</bold></xref>). As in the case of object detection, the result was well characterized by the backend.</p>
    </sec>
    <sec id="s3_3">
      <title>3.3 Plant segmentation with U<sup>2</sup>-Net</title>
      <p>As a case study of salient object detection with JustDeepIt, we use the Plant Phenotyping Dataset, a popular benchmark dataset for plant segmentation (<xref rid="B23" ref-type="bibr">Minervini et al., 2015</xref>; <xref rid="B24" ref-type="bibr">Minervini et al., 2016</xref>). The dataset contains 27 images of 3108 × 2324 pixels, and each image contains 24 individual plants. We used U<sup>2</sup>-Net implemented in JustDeepIt for plant segmentation. We randomly selected three images to train U<sup>2</sup>-Net using the <italic>random cropping</italic> approach. The training was performed with the default parameters in a macOS Big Sur system equipped with a Quad-Core Intel Core i5 processor (2.3 GHz) and 16 GB of memory. The training took approximately 6.5 hours with four processors but without any graphics processor.</p>
      <p>Salient object detection was performed using the trained U<sup>2</sup>-Net with the <italic>sliding</italic> approach for the 27 images with default parameters. Detection took approximately 2.3 hours, as the 27 images (3108 × 2324 pixels) were sliced into 27 × 130 small images (320 × 320 pixels) by the <italic>sliding</italic> approach, requiring substantial processing time. In addition, as the 27 images show time-series data, we summarized object statistics (e.g., projected area, plant color) over time (<xref rid="f3" ref-type="fig"><bold>Figure 3C</bold></xref>). The result indicates that the areas of the plants increased, and their colors varied over time.</p>
    </sec>
    <sec id="s3_4">
      <title>3.4 Iterative training of U<sup>2</sup>-Net for improved leaf segmentation</title>
      <p>Extracting salient objects by removing the background may improve the performance of image analysis and can be applied to various image analysis tasks. As another case study for salient object detection, we trained U<sup>2</sup>-Net for leaf segmentation on the Pest Damage Image Dataset (<xref rid="B18" ref-type="bibr">National Agriculture and Food Research Organization (NARO), 2021</xref>). The dataset comprises images of four crops, including cucumber tagged by disease or pest names, whereas no annotations of bounding boxes or segmentation are available. Here, we proposed iterative training to U<sup>2</sup>-Net using unannotated images for leaf segmentation for cucumber.</p>
      <p>Iterative training proceeded as follows (<xref rid="f3" ref-type="fig"><bold>Figure 3D</bold></xref>). In step 1, we used U<sup>2</sup>-Net trained on the DUTS dataset (<xref rid="B39" ref-type="bibr">Wang et al., 2017</xref>) (0<sup>th</sup>-trained U<sup>2</sup>-Net) obtained from the corresponding GitHub repository (<xref rid="B28" ref-type="bibr">Qin et al., 2021</xref>) for leaf segmentation of cucumber leaf images. In step 2, the images whose nearly complete area was detected as a salient object (image 3 in <xref rid="f3" ref-type="fig"><bold>Figure 3D</bold></xref>) or without detection (image 4 in <xref rid="f3" ref-type="fig"><bold>Figure 3D</bold></xref>) were discarded. In step 3, we used the remaining images and detection results (i.e., mask images) to retrain U<sup>2</sup>-Net. In step 4, we used the trained U<sup>2</sup>-Net from step 3 to perform salient object detection for the cucumber leaf images again. Then, we repeated steps 2–4 to retrain U<sup>2</sup>-Net five times, obtaining the 5<sup>th</sup>-trained U<sup>2</sup>-Net, and training was performed using the CUI for efficiency.</p>
      <p>The 0<sup>th</sup>-trained U<sup>2</sup>-Net failed to detect leaves in images containing multiple leaves. In contrast, the 5<sup>th</sup>-trained U<sup>2</sup>-Net successfully detected the main salient leaf in every image (<xref rid="f3" ref-type="fig"><bold>Figure 3D</bold></xref>). Thus, even without annotations, we built a model for leaf segmentation using existing techniques. General users can use such approaches <italic>via</italic> the simple JustDeepIt API to extend the range of applications.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4 Discussion</title>
    <p>The widespread use of deep learning technologies is gradually contributing to various scientific fields. Thus, it is vital to support the ease of technology usage for everyone, regardless of their research backgrounds and programming skills. As experimental researchers use GUIs and informatics researchers use CUIs mostly, developing software, which supports GUIs and CUIs and is not restricted to any specific tasks, is essential.</p>
    <p>In the field of plant sciences, various GUI-based software, implemented for deep learning-based image analysis, has been developed. However, most software implementing one or a few neural network architectures to solve specific problems and only support GUI (<xref rid="B3" ref-type="bibr">Belevich and Jokitalo, 2021</xref>; <xref rid="B38" ref-type="bibr">von Chamier et al., 2021</xref>; <xref rid="B35" ref-type="bibr">Smith et al., 2022</xref>; <xref rid="B42" ref-type="bibr">Zhou et al., 2021</xref>). In contrast, JustDeepIt was developed to fill these gaps, enabling various image analysis tasks using deep learning technologies in a single software. JustDeepIt implements multiple neural network architectures suitable for different application scenarios and supports both GUI and CUI, increasing flexibility according to the task. As shown in our case studies with JustDeepIt, GUI is ideal for building models from available data effortlessly, and CUI is suitable for facilitating the use of a model as an extension (e.g., iterative training of U<sup>2</sup>-Net). Furthermore, we believe that by supporting both GUI and CUI, collaborative research between experimental and informatics researchers can proceed more efficiently.</p>
    <p>Other than deep learning-based software, scikit-image (<xref rid="B36" ref-type="bibr">van der Walt et al., 2014</xref>), ImageJ (<xref rid="B1" ref-type="bibr">Abràmoff et al., 2004</xref>), and PlantCV (<xref rid="B13" ref-type="bibr">Gehan et al., 2017</xref>) are also available for image processing and are broadly used in many applications (e.g., plant detection and leaf segmentation). Scikit-image and ImageJ require users to set thresholds manually for multiple color spaces to segment leaf areas. Therefore, if an image consists of various phenotypes of plants, for example, plants with green and red leaves due to some stress, simultaneously segmenting both plants may be challenging. PlantCV supports building task-specific machine learning models for instance segmentation. However, it does not support GUI and requires programming skills. Given these open-source packages, choosing the appropriate one for a task or a specific problem is often demanding. JustDeepIt is expected to address complicated issues and accelerate research on image analysis when used in combination with other software.</p>
    <p>In plant science and agriculture, fruit detection and plant segmentation are two high-priority tasks (<xref rid="B2" ref-type="bibr">Arya et al., 2022</xref>; <xref rid="B43" ref-type="bibr">Zhou et al., 2022</xref>). This is because these tasks can estimate growth stages and yields of plants, including crops, and improve plant environmental robustness (e.g., disease resistance, fruit quality, and fruit yields) by collaborating with genomic technologies (e.g., genome-wide association study and expression quantitative trait locus analysis) (<xref rid="B37" ref-type="bibr">Varshney et al., 2021</xref>; <xref rid="B41" ref-type="bibr">Xiao et al., 2022</xref>). In this study, we represented the four case studies covering the two high-priority tasks with both GUI and CUI of JustDeepIt. The results support the robustness of JustDeepIt against critical issues in plant science. In addition, although JustDeepIt was intended for plant research, it may be applicable for image analysis in various disciplines beyond plant science. Furthermore, in a future version, we will continue to update the software in response to user demand and the technology flow.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data availability statement</title>
    <p>The original contributions presented in the study are publicly available. This data can be found here: <uri xlink:href="https://github.com/biunit/JustDeepIt">https://github.com/biunit/JustDeepIt</uri>.</p>
  </sec>
  <sec sec-type="author-contributions" id="s6">
    <title>Author contributions</title>
    <p>JS conceived the ideas and designed the software. JS and WC developed the software. JS, WC, and TY wrote the manuscript. All authors have read and approved the final manuscript.</p>
  </sec>
  <sec sec-type="funding-information" id="s7">
    <title>Funding</title>
    <p>This study was supported by a research project of the Ministry of Agriculture, Forestry, and Fisheries and a Public/Private R&amp;D Investment Strategic Expansion Program (PRISM) of the cabinet office of Japan, and JSPS KAKENHI Grant Number 22H05179.</p>
  </sec>
  <sec sec-type="acknowledgement" id="s8">
    <title>Acknowledgments</title>
    <p>Computations were partially performed on the SHIHO supercomputer at the National Agriculture and Food Research Organization (NARO), Japan.</p>
  </sec>
  <sec sec-type="COI-statement" id="s9">
    <title>Conflict of interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s10">
    <title>Publisher’s note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abràmoff</surname><given-names>M. D.</given-names></name><name><surname>Magalhães</surname><given-names>P. J.</given-names></name><name><surname>Ram</surname><given-names>S. J.</given-names></name></person-group> (<year>2004</year>). <article-title>Image processing with ImageJ</article-title>. <source>Biophotonics Int.</source>
<volume>11</volume> (<issue>7</issue>), <fpage>36</fpage>–<lpage>42</lpage>.</mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arya</surname><given-names>S.</given-names></name><name><surname>Sandhu</surname><given-names>K. S.</given-names></name><name><surname>Singh</surname><given-names>J.</given-names></name><name><surname>Kumar</surname><given-names>S.</given-names></name></person-group> (<year>2022</year>). <article-title>Deep learning: as the new frontier in high-throughput plant phenotyping</article-title>. <source>Euphytica</source>
<volume>218</volume>, <fpage>47</fpage>. doi: <pub-id pub-id-type="doi">10.1007/s10681-022-02992-3</pub-id>
</mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belevich</surname><given-names>I.</given-names></name><name><surname>Jokitalo</surname><given-names>E.</given-names></name></person-group> (<year>2021</year>). <article-title>DeepMIB: User-friendly and open-source software for training of deep learning network for biological image segmentation</article-title>. <source>PloS Comput. Biol.</source>
<volume>17</volume>, <elocation-id>e1008374</elocation-id>. doi: <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008374</pub-id>
<pub-id pub-id-type="pmid">33651804</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben Yedder</surname><given-names>H.</given-names></name><name><surname>Cardoen</surname><given-names>B.</given-names></name><name><surname>Hamarneh</surname><given-names>G.</given-names></name></person-group> (<year>2021</year>). <article-title>Deep learning for biomedical image reconstruction: a survey</article-title>. <source>Artif. Intell. Rev.</source>
<volume>54</volume>, <fpage>215</fpage>–<lpage>251</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s10462-020-09861-2</pub-id>
</mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Boris</surname><given-names>S.</given-names></name><name><surname>Nikita</surname><given-names>M.</given-names></name><name><surname>Maxim</surname><given-names>Z.</given-names></name><name><surname>Andrey</surname><given-names>Z.</given-names></name><name><surname>Dmitry</surname><given-names>K.</given-names></name><name><surname>Ben</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2021</year>) <source>Computer vision annotation tool (CVAT)</source>. Available at: <uri xlink:href="https://github.com/openvinotoolkit/cvat">https://github.com/openvinotoolkit/cvat</uri> (Accessed <date-in-citation content-type="access-date">October 5, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borji</surname><given-names>A.</given-names></name><name><surname>Cheng</surname><given-names>M.-M.</given-names></name><name><surname>Hou</surname><given-names>Q.</given-names></name><name><surname>Jiang</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>J.</given-names></name></person-group> (<year>2019</year>). <article-title>Salient object detection: A survey</article-title>. <source>Comput. Visual Media</source>
<volume>5</volume>, <fpage>117</fpage>–<lpage>150</lpage>. doi: <pub-id pub-id-type="doi">10.1007/s41095-019-0149-9</pub-id>
</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Brooks</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>) <source>COCO annotator</source>. Available at: <uri xlink:href="https://github.com/jsbroks/coco-annotator">https://github.com/jsbroks/coco-annotator</uri> (Accessed <date-in-citation content-type="access-date">October 5, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chebrolu</surname><given-names>N.</given-names></name><name><surname>Lottes</surname><given-names>P.</given-names></name><name><surname>Schaefer</surname><given-names>A.</given-names></name><name><surname>Winterhalter</surname><given-names>W.</given-names></name><name><surname>Burgard</surname><given-names>W.</given-names></name><name><surname>Stachniss</surname><given-names>C.</given-names></name></person-group> (<year>2017</year>). <article-title>Agricultural robot dataset for plant classification, localization and mapping on sugar beet fields</article-title>. <source>Int. J. Rob. Res.</source>
<volume>36</volume> (<issue>10</issue>):<page-range>1045–1052</page-range>. doi: <pub-id pub-id-type="doi">10.1177/0278364917720510</pub-id>
</mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>K.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Pang</surname><given-names>J.</given-names></name><name><surname>Cao</surname><given-names>Y.</given-names></name><name><surname>Xiong</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2019</year>) <source>MMDetection: Open MMLab detection toolbox and benchmark</source> (<publisher-name>arXiv</publisher-name>). Available at: <uri xlink:href="http://arxiv.org/abs/1906.07155">http://arxiv.org/abs/1906.07155</uri> (Accessed <date-in-citation content-type="access-date">October 8, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christin</surname><given-names>S.</given-names></name><name><surname>Hervet</surname><given-names>É.</given-names></name><name><surname>Lecomte</surname><given-names>N.</given-names></name></person-group> (<year>2019</year>). <article-title>Applications for deep learning in ecology</article-title>. <source>Methods Ecol. Evol.</source>
<volume>10</volume> (<issue>10</issue>), <fpage>1632</fpage>–<lpage>1644</lpage>. doi: <pub-id pub-id-type="doi">10.1111/2041-210X.13256</pub-id>
</mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>E.</given-names></name><name><surname>Madec</surname><given-names>S.</given-names></name><name><surname>Sadeghi-Tehran</surname><given-names>P.</given-names></name><name><surname>Aasen</surname><given-names>H.</given-names></name><name><surname>Zheng</surname><given-names>B.</given-names></name><name><surname>Liu</surname><given-names>S.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Global wheat head detection (GWHD) Dataset: A large and diverse dataset of high-resolution RGB-Labelled images to develop and benchmark wheat head detection methods</article-title>. <source>Plant Phenomics</source>, <volume>2020</volume>, <fpage>3521852</fpage>. doi: <pub-id pub-id-type="doi">10.34133/2020/3521852</pub-id>
<pub-id pub-id-type="pmid">33313551</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ford</surname><given-names>K. M.</given-names></name><name><surname>Pylyshyn</surname><given-names>Z. W.</given-names></name></person-group> (<year>1996</year>). “<article-title>The frame problem in artificial intelligence</article-title>,” in <source>The robot’s dilemma revisited</source> (<publisher-loc>USA</publisher-loc>: <publisher-name>Ablex Publishing Corp</publisher-name>).</mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gehan</surname><given-names>M. A.</given-names></name><name><surname>Fahlgren</surname><given-names>N.</given-names></name><name><surname>Abbasi</surname><given-names>A.</given-names></name><name><surname>Berry</surname><given-names>J. C.</given-names></name><name><surname>Callen</surname><given-names>S. T.</given-names></name><name><surname>Chavez</surname><given-names>L.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>PlantCV v2: Image analysis software for high-throughput plant phenotyping</article-title>. <source>PeerJ</source>, <volume>5</volume>:<elocation-id>e4088</elocation-id>. doi: <pub-id pub-id-type="doi">10.7717/peerj.4088</pub-id>
<pub-id pub-id-type="pmid">29209576</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasan</surname><given-names>A. S. M. M.</given-names></name><name><surname>Sohel</surname><given-names>F.</given-names></name><name><surname>Diepeveen</surname><given-names>D.</given-names></name><name><surname>Laga</surname><given-names>H.</given-names></name><name><surname>Jones</surname><given-names>M. G. K.</given-names></name></person-group> (<year>2021</year>). <article-title>A survey of deep learning techniques for weed detection from images</article-title>. <source>Comput. Electron Agric.</source>, <volume>184</volume>:<fpage>106067</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.compag.2021.106067</pub-id>
</mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Gkioxari</surname><given-names>G.</given-names></name><name><surname>Dollár</surname><given-names>P.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Mask</surname><given-names>R-CNN.</given-names></name></person-group> (<year>2017</year>) (<publisher-name>arXiv</publisher-name>). Available at: <uri xlink:href="http://arxiv.org/abs/1703.06870">http://arxiv.org/abs/1703.06870</uri> (Accessed <date-in-citation content-type="access-date">October 8, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>) <source>Deep residual learning for image recognition</source> (<publisher-name>arXiv</publisher-name>). Available at: <uri xlink:href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</uri> (Accessed <date-in-citation content-type="access-date">October 8, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Y.</given-names></name><name><surname>Li</surname><given-names>C.</given-names></name></person-group> (<year>2020</year>). <article-title>Convolutional neural networks for image-based high-throughput plant phenotyping: a review</article-title>. <source>Plant Phenomics</source>, <volume>2020</volume>:<fpage>4152816</fpage>. doi: <pub-id pub-id-type="doi">10.34133/2020/4152816</pub-id>
<pub-id pub-id-type="pmid">33313554</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><collab>National Agriculture and Food Research Organization (NARO)</collab></person-group> (<year>2021</year>) <source>Pest Damage Image Database (in Japanese)</source>. Available at: <uri xlink:href="https://www.naro.affrc.go.jp/org/niaes/damage/">https://www.naro.affrc.go.jp/org/niaes/damage/</uri> (Accessed <date-in-citation content-type="access-date">October 5, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamilaris</surname><given-names>A.</given-names></name><name><surname>Prenafeta-Boldú</surname><given-names>F. X.</given-names></name></person-group> (<year>2018</year>). <article-title>Deep learning in agriculture: A survey</article-title>. <source>Comput. Electron Agric.</source>
<volume>147</volume>, <fpage>70</fpage>–<lpage>90</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.compag.2018.02.016</pub-id>
</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>T.-Y.</given-names></name><name><surname>Goyal</surname><given-names>P.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Dollar</surname><given-names>P.</given-names></name></person-group> (<year>2018</year>) <source>Focal loss for dense object detection</source> (<publisher-name>arXiv</publisher-name>). Available at: <uri xlink:href="http://arxiv.org/abs/1708.02002v2">http://arxiv.org/abs/1708.02002v2</uri> (Accessed <date-in-citation content-type="access-date">October 5, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>W.</given-names></name><name><surname>Anguelov</surname><given-names>D.</given-names></name><name><surname>Erhan</surname><given-names>D.</given-names></name><name><surname>Szegedy</surname><given-names>C.</given-names></name><name><surname>Reed</surname><given-names>S.</given-names></name><name><surname>Fu</surname><given-names>C.-Y.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>SSD: single shot multiBox detector</article-title>. <source>Comput. Vision – ECCV 2016</source>, <volume>9905</volume>:<fpage>21</fpage>–<lpage>37</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-319-46448-0_2</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name></person-group> (<year>2021</year>). <article-title>Plant diseases and pests detection based on deep learning: a review</article-title>. <source>Plant Methods</source>
<volume>17</volume>, <elocation-id>22</elocation-id>. doi: <pub-id pub-id-type="doi">10.1186/s13007-021-00722-9</pub-id>
<pub-id pub-id-type="pmid">33627131</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Minervini</surname><given-names>M.</given-names></name><name><surname>Fischbach</surname><given-names>A.</given-names></name><name><surname>Scharr</surname><given-names>H.</given-names></name><name><surname>Tsaftaris</surname><given-names>S. A.</given-names></name></person-group> (<year>2015</year>) <source>Plant phenotyping datasets</source>. Available at: <uri xlink:href="https://www.plant-phenotyping.org/datasets-home">https://www.plant-phenotyping.org/datasets-home</uri> (Accessed <date-in-citation content-type="access-date">October 8, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minervini</surname><given-names>M.</given-names></name><name><surname>Fischbach</surname><given-names>A.</given-names></name><name><surname>Scharr</surname><given-names>H.</given-names></name><name><surname>Tsaftaris</surname><given-names>S. A.</given-names></name></person-group> (<year>2016</year>). <article-title>Finely-grained annotated datasets for image-based plant phenotyping</article-title>. <source>Pattern Recognit. Lett.</source>
<volume>81</volume>, <fpage>80</fpage>–<lpage>89</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.patrec.2015.10.013</pub-id>
</mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moen</surname><given-names>E.</given-names></name><name><surname>Bannon</surname><given-names>D.</given-names></name><name><surname>Kudo</surname><given-names>T.</given-names></name><name><surname>Graf</surname><given-names>W.</given-names></name><name><surname>Covert</surname><given-names>M.</given-names></name><name><surname>Valen</surname><given-names>D. V.</given-names></name></person-group> (<year>2019</year>). <article-title>Deep learning for cellular image analysis</article-title>. <source>Nat. Methods</source>
<volume>16</volume>, <fpage>1233</fpage>–<lpage>1246</lpage>. doi: <pub-id pub-id-type="doi">10.1038/s41592-019-0403-1</pub-id>
<pub-id pub-id-type="pmid">31133758</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A.</given-names></name><name><surname>Gross</surname><given-names>S.</given-names></name><name><surname>Massa</surname><given-names>F.</given-names></name><name><surname>Lerer</surname><given-names>A.</given-names></name><name><surname>Bradbury</surname><given-names>J.</given-names></name><name><surname>Chanan</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2019</year>) <source>PyTorch: an imperative style, high-performance deep learning library</source> (<publisher-name>arXiv</publisher-name>). Available at: <uri xlink:href="http://arxiv.org/abs/1912.01703">http://arxiv.org/abs/1912.01703</uri> (Accessed <date-in-citation content-type="access-date">October 8, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Huang</surname><given-names>C.</given-names></name><name><surname>Dehghan</surname><given-names>M.</given-names></name><name><surname>Zaiane</surname><given-names>O.</given-names></name><name><surname>Jagersand</surname><given-names>M.</given-names></name></person-group> (<year>2020</year>). <article-title>U2-net: going deeper with nested U-structure for salient object detection</article-title>. <source>Pattern Recognition</source>
<volume>106</volume>, <fpage>107404</fpage>. doi: <pub-id pub-id-type="doi">10.1016/j.patcog.2020.107404</pub-id>
</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>Z.</given-names></name><name><surname>Huang</surname><given-names>C.</given-names></name><name><surname>Dehghan</surname><given-names>M.</given-names></name><name><surname>Zaiane</surname><given-names>O. R.</given-names></name><name><surname>Jagersand</surname><given-names>M.</given-names></name></person-group> (<year>2021</year>) <source>U2-net: going deeper with nested U-structure for salient object detection</source>. Available at: <uri xlink:href="https://github.com/xuebinqin/U-2-Net">https://github.com/xuebinqin/U-2-Net</uri> (Accessed <date-in-citation content-type="access-date">June 10, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Ramírez</surname><given-names>S.</given-names></name></person-group> (<year>2018</year>) <source>FastAPI</source>. Available at: <uri xlink:href="https://github.com/tiangolo/fastapi">https://github.com/tiangolo/fastapi</uri> (Accessed <date-in-citation content-type="access-date">October 8, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Redmon</surname><given-names>J.</given-names></name><name><surname>Farhadi</surname><given-names>A.</given-names></name></person-group> (<year>2017</year>). “<article-title>YOLO9000: Better, faster, stronger</article-title>,” in <conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (IEEE)</conf-name>. doi: <pub-id pub-id-type="doi">10.1109/cvpr.2017.690</pub-id>
</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Redmon</surname><given-names>J.</given-names></name><name><surname>Farhadi</surname><given-names>A.</given-names></name></person-group> (<year>2018</year>) <source>YOLOv3: An incremental improvement</source> (<publisher-name>arXiv</publisher-name>). Available at: <uri xlink:href="http://arxiv.org/abs/1804.02767">http://arxiv.org/abs/1804.02767</uri> (Accessed <date-in-citation content-type="access-date">October 8, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>). <article-title>Faster r-CNN: towards real-time object detection with region proposal networks</article-title>. <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
<volume>39</volume>, <fpage>1137</fpage>–<lpage>1149</lpage>. doi: <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id>
<pub-id pub-id-type="pmid">27295650</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O.</given-names></name><name><surname>Fischer</surname><given-names>P.</given-names></name><name><surname>Brox</surname><given-names>T.</given-names></name></person-group> (<year>2015</year>). <article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>. <source>Lect. Notes Comput. Sci.</source>
<volume>9351</volume>, <fpage>234</fpage>–<lpage>241</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id>
</mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group> (<year>2015</year>) <source>Very deep convolutional networks for large-scale image recognition</source> (<publisher-name>arXiv</publisher-name>). Available at: <uri xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</uri> (Accessed <date-in-citation content-type="access-date">October 8, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>A. G.</given-names></name><name><surname>Han</surname><given-names>E.</given-names></name><name><surname>Petersen</surname><given-names>J.</given-names></name><name><surname>Olsen</surname><given-names>N. A. F.</given-names></name><name><surname>Giese</surname><given-names>C.</given-names></name><name><surname>Athmann</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2022</year>). <article-title>RootPainter: deep learning segmentation of biological images with corrective annotation</article-title>. <source>New Phytol. press</source>. doi: <pub-id pub-id-type="doi">10.1111/nph</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Walt</surname><given-names>S.</given-names></name><name><surname>Schönberger</surname><given-names>J. L.</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J.</given-names></name><name><surname>Boulogne</surname><given-names>F.</given-names></name><name><surname>Warner</surname><given-names>J. D.</given-names></name><name><surname>Yager</surname><given-names>N.</given-names></name><etal/></person-group>. (<year>2014</year>). <article-title>Scikit-image: image processing in Python</article-title>. <source>PeerJ.</source><volume>2</volume>, <fpage>e453</fpage>. doi: <pub-id pub-id-type="doi">10.7717/peerj.453</pub-id><pub-id pub-id-type="pmid">25024921</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varshney</surname><given-names>R. K.</given-names></name><name><surname>Bohra</surname><given-names>A.</given-names></name><name><surname>Roorkiwal</surname><given-names>M.</given-names></name><name><surname>Barmukh</surname><given-names>R.</given-names></name><name><surname>Cowling</surname><given-names>W. A.</given-names></name><name><surname>Chitikineni</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Fast-forward breeding for a food-secure world</article-title>. <source>Treads Genet.</source><volume>37</volume> (<issue>12</issue>), <fpage>1124</fpage>–<lpage>1136</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.tig.2021.08.002</pub-id>
</mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Chamier</surname><given-names>L.</given-names></name><name><surname>Laine</surname><given-names>R. F.</given-names></name><name><surname>Jukkala</surname><given-names>J.</given-names></name><name><surname>Spahn</surname><given-names>C.</given-names></name><name><surname>Krentzel</surname><given-names>D.</given-names></name><name><surname>Nehme</surname><given-names>E.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>Democratising deep learning for microscopy with ZeroCostDL4Mic</article-title>. <source>Nat. Commun.</source><volume>12</volume>,<fpage>2276</fpage>. doi: <pub-id pub-id-type="doi">10.1038/s41467-021-22518-0</pub-id><pub-id pub-id-type="pmid">33859193</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L.</given-names></name><name><surname>Lu</surname><given-names>H.</given-names></name><name><surname>Wang</surname><given-names>Y.</given-names></name><name><surname>Feng</surname><given-names>M.</given-names></name><name><surname>Wang</surname><given-names>D.</given-names></name><name><surname>Yin</surname><given-names>B.</given-names></name><etal/></person-group>. (<year>2017</year>). “<article-title>Learning to detect salient objects with image-level supervision</article-title>,” in <conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name>. doi: <pub-id pub-id-type="doi">10.1109/cvpr.2017.404</pub-id>
</mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y.</given-names></name><name><surname>Kirillov</surname><given-names>A.</given-names></name><name><surname>Massa</surname><given-names>F.</given-names></name><name><surname>Lo</surname><given-names>W.-Y.</given-names></name><name><surname>Girshick</surname><given-names>R.</given-names></name></person-group> (<year>2019</year>) <source>Detectron2</source>. Available at: <uri xlink:href="https://github.com/facebookresearch/detectron2">https://github.com/facebookresearch/detectron2</uri> (Accessed <date-in-citation content-type="access-date">June 10, 2021</date-in-citation>).</mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>Q.</given-names></name><name><surname>Bai</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>C.</given-names></name><name><surname>He</surname><given-names>Y.</given-names></name></person-group> (<year>2022</year>). <article-title>Advanced high-throughput plant phenotyping techniques for genome-wide association studies: A review</article-title>. <source>J. Adv. Res.</source>
<volume>35</volume>, <fpage>215</fpage>–<lpage>230</lpage>. doi: <pub-id pub-id-type="doi">10.1016/j.jare.2021.05.002</pub-id>
<pub-id pub-id-type="pmid">35003802</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>S.</given-names></name><name><surname>Chai</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>H.</given-names></name><name><surname>Yang</surname><given-names>C.</given-names></name><name><surname>Sun</surname><given-names>T.</given-names></name></person-group> (<year>2021</year>). <article-title>Maize-IAS: a maize image analysis software using deep learning for high-throughput plant phenotyping</article-title>. <source>Plant Methods</source>
<volume>17</volume>, <fpage>48</fpage>. doi: <pub-id pub-id-type="doi">10.1186/s13007-021-00747-0</pub-id>
<pub-id pub-id-type="pmid">33926480</pub-id></mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J.</given-names></name><name><surname>Vong</surname><given-names>C. N.</given-names></name><name><surname>Zhou</surname><given-names>J.</given-names></name></person-group> (<year>2022</year>). “<article-title>Imaging technology for high-throughput plant phenotyping</article-title>,” in <source>Sensing, data managing, and control technologies for agricultural systems. agriculture automation and control</source>. Eds. <person-group person-group-type="editor"><name><surname>Ma</surname><given-names>S.</given-names></name><name><surname>Lin</surname><given-names>T.</given-names></name><name><surname>Mao</surname><given-names>E.</given-names></name><name><surname>Song</surname><given-names>Z.</given-names></name><name><surname>Ting</surname><given-names>K. C.</given-names></name></person-group> (<publisher-loc>Cham</publisher-loc>: <publisher-name>Springer</publisher-name>), <fpage>75</fpage>–<lpage>99</lpage>. doi: <pub-id pub-id-type="doi">10.1007/978-3-031-03834-1_4</pub-id>
</mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>C.</given-names></name><name><surname>Hu</surname><given-names>Y.</given-names></name><name><surname>Mao</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>S.</given-names></name><name><surname>Li</surname><given-names>F.</given-names></name><name><surname>Zhao</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>A deep learning-based method for automatic assessment of stomatal index in wheat microscopic images of leaf epidermis</article-title>. <source>Front. Plant Sci.</source><volume>12</volume>. doi: <pub-id pub-id-type="doi">10.3389/fpls.2021.716784</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
