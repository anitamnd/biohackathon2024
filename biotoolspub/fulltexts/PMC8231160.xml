<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 201905//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="publisher-id">sensors</journal-id>
    <journal-title-group>
      <journal-title>Sensors (Basel, Switzerland)</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1424-8220</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8231160</article-id>
    <article-id pub-id-type="doi">10.3390/s21124045</article-id>
    <article-id pub-id-type="publisher-id">sensors-21-04045</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Deep-Framework: A Distributed, Scalable, and Edge-Oriented Framework for Real-Time Analysis of Video Streams</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0575-902X</contrib-id>
        <name>
          <surname>Sassu</surname>
          <given-names>Alessandro</given-names>
        </name>
        <xref rid="c1-sensors-21-04045" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1800-2616</contrib-id>
        <name>
          <surname>Saenz-Cogollo</surname>
          <given-names>Jose Francisco</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Agelli</surname>
          <given-names>Maurizio</given-names>
        </name>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Jung</surname>
          <given-names>Yong Ju</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Lee</surname>
          <given-names>Joohyung</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Fumera</surname>
          <given-names>Giorgio</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
      <contrib contrib-type="editor">
        <name>
          <surname>Newaz</surname>
          <given-names>S. H. Shah</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <aff id="af1-sensors-21-04045">Center for Advanced Studies, Research and Development in Sardinia (CRS4), Località Pixina Manna, Edificio 1, 09010 Pula, CA, Italy; <email>jsaenz@crs4.it</email> (J.F.S.-C.); <email>agelli@crs4.it</email> (M.A.)</aff>
    <author-notes>
      <corresp id="c1-sensors-21-04045"><label>*</label>Correspondence: <email>asassu@crs4.it</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>11</day>
      <month>6</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>6</month>
      <year>2021</year>
    </pub-date>
    <volume>21</volume>
    <issue>12</issue>
    <elocation-id>4045</elocation-id>
    <history>
      <date date-type="received">
        <day>12</day>
        <month>5</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>09</day>
        <month>6</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 by the authors.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Edge computing is the best approach for meeting the exponential demand and the real-time requirements of many video analytics applications. Since most of the recent advances regarding the extraction of information from images and video rely on computation heavy deep learning algorithms, there is a growing need for solutions that allow the deployment and use of new models on scalable and flexible edge architectures. In this work, we present Deep-Framework, a novel open source framework for developing edge-oriented real-time video analytics applications based on deep learning. Deep-Framework has a scalable multi-stream architecture based on Docker and abstracts away from the user the complexity of cluster configuration, orchestration of services, and GPU resources allocation. It provides Python interfaces for integrating deep learning models developed with the most popular frameworks and also provides high-level APIs based on standard HTTP and WebRTC interfaces for consuming the extracted video data on clients running on browsers or any other web-based platform.</p>
    </abstract>
    <kwd-group>
      <kwd>real-time video analytics</kwd>
      <kwd>edge computing</kwd>
      <kwd>deep learning</kwd>
      <kwd>distributed systems</kwd>
      <kwd>software framework</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-sensors-21-04045">
    <title>1. Introduction</title>
    <p>Cities, industries, shops, and homes all around the world are experiencing a sharp increase in the number of installed video cameras for surveillance, traffic/process monitoring, and other applications that require real-time video content analysis. Traditionally, a human operator needs to manually watch camera feeds and press a button in case of an event. Since security personnel can hardly remain alert for monitoring tasks after 20 min [<xref rid="B1-sensors-21-04045" ref-type="bibr">1</xref>], in recent years intelligent video analytics systems have emerged as a solution to analyse video streams without human intervention.</p>
    <p>With the success of deep learning (DL), video analytics using DL is increasingly being employed to provide accurate and reliable detection, tracking, and classification of objects, people, and faces in video data. However, performing real-time and information-rich analytics using deep learning is a complex endeavor that requires not only high computational power but also a timely orchestration of several different algorithms and software modules. Most of the existing solutions for deploying DL models for video analytics rely on centralized architectures and/or cloud computing platforms hosted by large-scale IT-companies (Amazon Web Services, Google Cloud, Microsoft Azure, IBM Cloud). They provide flexible and scalable solutions with developer-friendly environments and standard interfaces but are not able to meet the Quality of Experience (QoE) demands often associated with network latency and Round Trip Time (RTT) constraints, especially for multi-camera applications.</p>
    <p>Given the large scale of the data generated by cameras and the real-time requirements often involved, an edge solution is the most practical approach as it conducts the video analytics close to (or on) the cameras [<xref rid="B2-sensors-21-04045" ref-type="bibr">2</xref>]. By deploying shared resources at the Internet edge and pushing computation close to the data sources, edge solutions not only benefit applications that require low latency but also those that involve privacy sensitive content. In recent years, several solutions for performing real-time video analytics at the edge have been proposed [<xref rid="B3-sensors-21-04045" ref-type="bibr">3</xref>,<xref rid="B4-sensors-21-04045" ref-type="bibr">4</xref>,<xref rid="B5-sensors-21-04045" ref-type="bibr">5</xref>,<xref rid="B6-sensors-21-04045" ref-type="bibr">6</xref>,<xref rid="B7-sensors-21-04045" ref-type="bibr">7</xref>,<xref rid="B8-sensors-21-04045" ref-type="bibr">8</xref>,<xref rid="B9-sensors-21-04045" ref-type="bibr">9</xref>,<xref rid="B10-sensors-21-04045" ref-type="bibr">10</xref>,<xref rid="B11-sensors-21-04045" ref-type="bibr">11</xref>,<xref rid="B12-sensors-21-04045" ref-type="bibr">12</xref>,<xref rid="B13-sensors-21-04045" ref-type="bibr">13</xref>,<xref rid="B14-sensors-21-04045" ref-type="bibr">14</xref>]. However, there is still a lack of contributions that can be scaled using distributed computing, that can be easily expanded with models developed with popular DL frameworks, and that can provide standard interfaces for both native and web-based client applications.</p>
    <p>In this work, we propose a distributed and scalable software framework for implementing real-time video analytics applications named Deep-Framework (DF). Deep-Framework exploits state-of-the-art distributed technologies and allows for the implementation of DL models developed with the most popular open-source DL libraries. The framework provides a simple interface for defining and configuring the computation cluster that abstracts the complex distribution and management of resources, a high-level API for consuming the extracted video data and monitoring the system performance in an open and easy to digest format, and a web-friendly streaming interface for consuming both the resulting data and the source video on client applications running on browsers or other web-based platforms. The contribution of the proposed framework to the state of the art resides in the novel combination of the following features:<list list-type="bullet"><list-item><p>Distributed architecture for deploying the real-time analysis of video on a cluster of small cost-effective machines as well as on a single powerful machine;</p></list-item><list-item><p>Resource allocation mechanism that automatically allocates every algorithm according to its deep learning framework, optimizing the use of GPU and CPU resources in the cluster;</p></list-item><list-item><p>Modular organization of components that allows efficient scaling to a large number of video cameras with multiple processing pipelines per video stream;</p></list-item><list-item><p>Frame-skipping policy, adopted for each component of the processing pipelines, that ensures a real-time behavior of the system;</p></list-item><list-item><p>Python interfaces that allow researchers and developers to easily include and deploy new deep learning models using the most mature deep learning frameworks;</p></list-item><list-item><p>Standard HTTP and WebRTC APIs for providing web-based video analytics services and allowing the integration of web-based client applications.</p></list-item></list></p>
    <p>The rest of this paper is organized as follows. Related work is discussed in <xref ref-type="sec" rid="sec2-sensors-21-04045">Section 2</xref>, while <xref ref-type="sec" rid="sec3-sensors-21-04045">Section 3</xref> describes the proposed Deep-Framework in detail and explains the tools provided by the framework for integrating new models and deploying an application; <xref ref-type="sec" rid="sec4-sensors-21-04045">Section 4</xref> presents some experimental evaluations and describes two examples of real-world applications designed with Deep-Framework; and finally, conclusions and future work are presented in <xref ref-type="sec" rid="sec6-sensors-21-04045">Section 6</xref>.</p>
  </sec>
  <sec id="sec2-sensors-21-04045">
    <title>2. Related Work</title>
    <p>Existing solutions for performing real-time video analytics at the edge can be divided between application (and model) specific systems [<xref rid="B3-sensors-21-04045" ref-type="bibr">3</xref>,<xref rid="B4-sensors-21-04045" ref-type="bibr">4</xref>,<xref rid="B5-sensors-21-04045" ref-type="bibr">5</xref>,<xref rid="B6-sensors-21-04045" ref-type="bibr">6</xref>], frame filtering strategies for reducing network usage and optimizing latency and computational resources [<xref rid="B7-sensors-21-04045" ref-type="bibr">7</xref>,<xref rid="B8-sensors-21-04045" ref-type="bibr">8</xref>,<xref rid="B9-sensors-21-04045" ref-type="bibr">9</xref>,<xref rid="B10-sensors-21-04045" ref-type="bibr">10</xref>], and general purpose architectures or software frameworks that can be used as skeletons or scaffolds for deploying models and developing applications [<xref rid="B11-sensors-21-04045" ref-type="bibr">11</xref>,<xref rid="B12-sensors-21-04045" ref-type="bibr">12</xref>,<xref rid="B13-sensors-21-04045" ref-type="bibr">13</xref>,<xref rid="B14-sensors-21-04045" ref-type="bibr">14</xref>]. Since the focus of the present paper is on the latter category, next we review with special attention the related relevant works.</p>
    <p>Nazare and Schwartz [<xref rid="B11-sensors-21-04045" ref-type="bibr">11</xref>] proposed Smart Surveillance Framework (SSF), one of the first general purpose real-time intelligent surveillance frameworks based on user-defined modules that communicate through a shared memory. It was built with the aim of helping researchers develop functional surveillance systems, as well as integrate new algorithms to solve problems related to video surveillance. SSF provided a set of mechanisms for data representation, storage, and communication and also provided tools for performing scene understanding. Although it allowed for the integration of modules written in C++ in a flexible way, it was a single camera/stream solution designed to be deployed on a standalone machine and was not optimized for executing DL models. In [<xref rid="B12-sensors-21-04045" ref-type="bibr">12</xref>], Ali et al., proposed a system for analyzing multiple video streams with DL models distributed between the edge and the cloud. Its aim was to bring the deep learning based analytics towards the source of the video streams by parallelizing and filtering streams on the network nodes. It implements a background filtering approach for filtering background frames in in-transit nodes, which help to reduce the video data, bandwidth, and the storage needed at the destination cloud. It demonstrated the throughput gain of not executing models entirely on the cloud but it did not provide any tool for configuring the system, integrating new models, or for easily consuming the resulting data. Rouhani et al. [<xref rid="B15-sensors-21-04045" ref-type="bibr">15</xref>] presented a framework for real-time background subtraction on FPGA called RISE. The goal of this framework is to provide a user-friendly interface that can be used for rapid prototyping and deployment of different video surveillance applications using SoC platforms. RISE takes the stream of a surveillance video as its input and learns/updates a fixed-size dictionary matrix as a subsample of the input pixels. The dictionary samples are selected to capture the varying background pixels in the input video. Though RISE is presented as a multipurpose framework, it is limited to embedded systems and its API only allows for the customization of the parameters of a single background subtraction algorithm. Liu et al. [<xref rid="B13-sensors-21-04045" ref-type="bibr">13</xref>] introduced EdgeEye, a service-oriented framework based on GStreamer that provides a high-level, task-specific API for developing and deploying real-time video analytics applications. The aim of EdgeEye is to bring computation entirely at the Internet edge, focusing on applications that require low latency and high privacy. The API allows to load and manage predefined DL models and video and data can be consumed via WebRTC-based peer-to-peer connections. Though this approach represents a flexible and ‘user-friendly’ solution it does not provide an interface for integrating user-defined models and relies on a single machine for performing all the processing work, thus limiting the application to lightweight models, few camera streams, or powerful machines with multiple GPUs. In [<xref rid="B14-sensors-21-04045" ref-type="bibr">14</xref>], Uddin et al., presented a distributed video analytics framework that allows one to process both real-time streams and batch video using Apache Kafka for capturing video frames from multiple sources and Apache Spark for the distributed computing of video streams. The aim of this framework was to provide a complete service-oriented ecosystem focusing on scalability, effectiveness, and fault-tolerance. It consists of five main layers, Big Data Curation Layer, Distributed Video Data Processing Layer, Distributed Video Data Mining Layer, Knowledge Curation Layer and Service Curation Layer. Moreover, it has three basic users with roles: Administrator, who provides and manages the platform, Third Party Developers, who can use the existing video processing APIs provided by the framework or can develop new APIs to produce new services, and End-Users, who can use the existing services. Though it provides a Java-based service-oriented ecosystem with specific interfaces for end-users and developers, it does not provide an interface for integrating user-defined models. None of the mentioned works enforce a frame-skipping policy for ensuring the real-time processing of video streams or simple interfaces for both developing a complete video analytics application and integrating new DL models built with popular Python DL frameworks.</p>
  </sec>
  <sec id="sec3-sensors-21-04045">
    <title>3. Materials and Methods</title>
    <p>Deep-Framework is designed as a software framework to enable DL researchers and developers rapid prototyping of distributed, scalable, multi-algorithm, and latency-focused solutions in edge computing contexts. The methods and strategies adopted for developing the framework are chosen with two types of users in mind: application developers and DL DevOps engineers. The idea is that application developers can implement, configure, and use the framework codebase and integrated models to quickly deploy a complete real-time video analytics backend focusing on the end-user application and data consumption without requiring to handle the orchestration of services and model integration. On the other hand, the framework allows DL DevOps engineers to focus on the integration of new DL models simplifying the creation of processing pipelines and the deployment of a test application.</p>
    <p>Being a distributed edge framework, Deep-Framework is designed to be executed in a cluster of computational nodes. Each component of the framework works as an independent service that is deployed in one of the nodes as a Docker service in swarm mode [<xref rid="B16-sensors-21-04045" ref-type="bibr">16</xref>]. Each service is scheduled on a node based on specific rules that depend on the configuration desired by the user. They communicate with each other through TCP sockets using ZeroMQ [<xref rid="B17-sensors-21-04045" ref-type="bibr">17</xref>] taking advantage of its predefined messaging-patterns that allow scalability in distributed systems with almost zero overhead [<xref rid="B17-sensors-21-04045" ref-type="bibr">17</xref>,<xref rid="B18-sensors-21-04045" ref-type="bibr">18</xref>]. The docker images are created in a single main node and are then distributed, via a docker registry, to the remaining nodes in a transparent manner to the user, who only needs to provide the IP address and the credentials of the machines.</p>
    <p>The creation and deployment of processing pipelines and the orchestration of all services is managed by a component called DF Initializer (as depicted in <xref ref-type="fig" rid="sensors-21-04045-f001">Figure 1</xref>), which provides a command-line interface that allows users to define the video sources, to specify the available computation nodes, and to choose the algorithms (models) that intend to execute. With this information the DF Initializer performs the following actions:<list list-type="bullet"><list-item><p>Initializes the components, builds the pipelines, and creates the related services;</p></list-item><list-item><p>Creates the cluster of nodes in which the services will be executed;</p></list-item><list-item><p>Allocates the services with DL models that require GPU to nodes with GPU capability, taking into account the number of nodes, the available memory, and the installed DL frameworks; the other services are scheduled on a different node whenever possible. In Algorithm 1, this procedure is explained in detail;</p></list-item><list-item><p>Defines the connection with video sources.</p></list-item></list></p>
    <fig id="sensors-21-04045-f001" orientation="portrait" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Overview of Deep-Framework workflow. The framework provides tools for the integration of new algorithms and for defining and launching a complete video analytics application on a computational cluster based on available models.</p>
      </caption>
      <graphic xlink:href="sensors-21-04045-g001"/>
    </fig>
    <array orientation="portrait">
      <tbody>
        <tr>
          <td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 1:</bold> Allocation of algorithm services to nodes.</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <inline-graphic xlink:href="sensors-21-04045-i001.jpg"/>
          </td>
        </tr>
      </tbody>
    </array>
    <sec id="sec3dot1-sensors-21-04045">
      <title>3.1. Architecture</title>
      <p>As shown in <xref ref-type="fig" rid="sensors-21-04045-f002">Figure 2</xref>, the framework manages each video stream independently through a Stream Manager module which distributes individual frames to one or more processing pipelines, that parallelize all video analysis tasks, and is able to stream out both the resulting data stream and the input video stream via peer-to-peer connections. A Server module provides the API for consuming the resulting data, managing the stream processing, and monitoring the performance of the individual components thanks to a dedicated Monitor component.</p>
      <p>Each processing pipeline detects and analyses a different type of object or Region of Interest. A pipeline performs the following tasks:<list list-type="bullet"><list-item><p><italic>Object detection</italic>: objects or areas of interest within an image are identified and tracked along subsequent images using detection and tracking methods. The component that performs this task is called Detector.</p></list-item><list-item><p><italic>Object description</italic>: objects are classified, characterized, or described using classification models, feature extractors, or generic analytical models. These tasks are performed by three components: Broker, Descriptor, Sub Collector. These components form a Descriptor Chain. Several Descriptor Chains may operate in parallel, each one dedicated to the extraction of a specific feature.</p></list-item><list-item><p><italic>Results collection</italic>: results are collected and aggregated to produce the output data stream. The component that performs this action is called Collector.</p></list-item></list></p>
      <sec id="sec3dot1dot1-sensors-21-04045">
        <title>3.1.1. Processing Pipeline</title>
        <p>As shown in <xref ref-type="fig" rid="sensors-21-04045-f003">Figure 3</xref>, a processing pipeline is composed essentially of: at least one Detector, one or more Descriptor Chains (not mandatory), one for each feature to be extracted and a Collector. A Descriptor Chain is composed of Broker, Descriptor, and Sub Collector (as illustrated in <xref ref-type="fig" rid="sensors-21-04045-f004">Figure 4</xref>). Next, every component is described in detail.</p>
      </sec>
      <sec>
        <title>Detector</title>
        <p>This component is responsible for extracting the coordinates of an object in an image and for tracking it along all received images. The object could be an entity that is depicted in a particular region of the image or in the entire image. Once the coordinates of the object have been estimated, they are sent, together with the corresponding Region of Interest (ROI), to the components dedicated to the information extraction. The framework provides a Python abstract class that defines a common interface for allowing the integration of custom detection and tracking models. In this way, developers only have to deal with the analysis of images and inference-related operations. Each Detector defines a different pipeline. The framework, therefore, wraps the custom code and performs functions as follows:<list list-type="bullet"><list-item><p>It instantiates the desired Detector;</p></list-item><list-item><p>It receives images and enforces the frame skipping mechanism (as described below in this section);</p></list-item><list-item><p>It creates a list of objects, each composed of an identifier, the coordinates of the bounding box, and and/or a list of keypoints, if available, and sends them to the Collector component;</p></list-item><list-item><p>It sends the list of objects and their ROIs to the connected descriptors, using a publish–subscribe messaging pattern.</p></list-item></list></p>
        <p>Due to its fundamental function, the Detector gets priority in memory allocation procedure performed by the DF Initializer and described in Algorithm 1.</p>
      </sec>
      <sec>
        <title>Descriptor</title>
        <p>This is the component that carries out the analysis of the ROIs and/or coordinates, retrieved by the Detector, with the aim of extracting information about objects and/or their trajectories. In order to scale up this process, it is possible to create several instances (workers) for each Descriptor. Similar to the Detector, the DF provides a Python abstract class that defines a common interface for the integration of custom models. The framework, therefore, wraps the custom code and performs functions as follows:<list list-type="bullet"><list-item><p>It instantiates the desired Descriptor;</p></list-item><list-item><p>It receives images and enforces the frame skipping mechanism;</p></list-item><list-item><p>It extracts feature from ROIs and performs an average of the results obtained on N successive images in which the object is tracked.</p></list-item></list></p>
      </sec>
      <sec>
        <title>Broker</title>
        <p>This component receives data from the detector and distributes them across all the instances of the Descriptor.</p>
      </sec>
      <sec>
        <title>Sub Collector</title>
        <p>It aggregates the results obtained by the workers instantiated for each Descriptor; only the most recent results are sent to the main Collector.</p>
      </sec>
      <sec>
        <title>Collector</title>
        <p>For every result coming from Detector (objects coordinates, identifiers), it produces an output message aggregating the latest available results obtained from Sub Collectors (features) and the timestamps of the corresponding frames. The algorithm used by Collector to aggregate data and construct the output message is shown in Algorithm 2. Note that retrieving results from Descriptor Chains is a not blocking operation and that, if no new results are available from a specific Descriptor, the latest ones are aggregated. Therefore, though each output message contains results from all running algorithms, each result may be related to a different frame and thus be associated to a different timestamp. The resulting data stream is made available to the components that implement the streaming APIs.
<array orientation="portrait"><tbody><tr><td align="left" valign="middle" style="border-top:solid thin;border-bottom:solid thin" rowspan="1" colspan="1"><bold>Algorithm 2:</bold> Collector data aggregation and output message creation algorithm.</td></tr><tr><td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-graphic xlink:href="sensors-21-04045-i002.jpg"/></td></tr></tbody></array></p>
        <p>Parallelization of video analysis tasks is therefore achieved at 3 different levels: (1) routing the video stream to different pipelines, each one dedicated to the analysis of a specific type of objects or ROI, (2) extracting in parallel different features through dedicated Descriptor Chains, and (3) distributing across several workers the feature extracting tasks of each Descriptor.</p>
        <p>Regardless of the amount of computational resources allocated, real-time behavior is ensured by a frame-skipping algorithm, implemented in the Detector and the Descriptor, which basically consists in skipping the frames received during the previous computation and analysing only the most recent one.</p>
        <p>The DF is intended as a feature extractor engine, so none of the above components store anything permanently. In fact, in DF, the images received by the components remain in memory for the time needed by the computation operations. The video and the data resulting from the analysis can be possibly stored by an external application.</p>
      </sec>
      <sec id="sec3dot1dot2-sensors-21-04045">
        <title>3.1.2. Stream Manager</title>
        <p>The Stream Manager establishes the connection with the video source in order to grab the individual frames and send them, together with a timestamp, to the processing components. Sources can be RTSP/HTTP streams (IP cameras), WebRTC streams (web-based video providers), or video files (for non-real-time applications or testing purposes). The DF can process multiple sources at once. For every source a Stream Manager instance is created. It also receives the resulting data from the Collector component and is able to stream it together with the input video stream through a WebRTC connection mediated by the Server. Stream Manager and all pipeline components are implemented in Python.</p>
      </sec>
      <sec id="sec3dot1dot3-sensors-21-04045">
        <title>3.1.3. Server</title>
        <p>The Server provides an HTTP API for distributing resulting data streams, sending statistics about the performance of individual components, controlling the execution of pipelines, and listing the available resources. Data streams, received from Collectors, are distributed through endpoints based on Server-Sent Events (SSE). Statistics are also distributed through SSE but are collected from a dedicated Monitor component that will be described in the following section. The control of pipeline execution is exerted through connections with Stream Managers and allows one to start or stop the grabbing of frames from a video source. The Server also acts as a WebRTC signaling server based on Websockets and allows client applications to establish peer-to-peer connections with single Stream Managers for receiving both video and data streams. Since WebRTC connections are bidirectional, this kind of communication allows external applications to also act as a video source, thus opening the possibility for clients or remote agents to use a single connection for both sending the video stream and receiving the resulting data. The Server is implemented in Node.js.</p>
      </sec>
      <sec id="sec3dot1dot4-sensors-21-04045">
        <title>3.1.4. Monitor</title>
        <p>The Monitor is connected with all pipeline components and receives and aggregates from them their operating metrics, which include the throughput (results per second) and the number of frames received, processed, and skipped. These metrics, once collected and aggregated, are made available through the Server and can be used to evaluate the performance of a specific component and of the whole system.</p>
      </sec>
    </sec>
    <sec id="sec3dot2-sensors-21-04045">
      <title>3.2. Using Deep-Framework</title>
      <p>Deep-Framework, as described above, can be used from application developers and DL DevOps engineers. Application developers can use the framework by interacting with DF Initializer in order to configure and deploy the application services using already defined Detectors and Descriptors (for instance those used in <xref ref-type="sec" rid="sec4dot2-sensors-21-04045">Section 4.2</xref>, which are included in the framework codebase repository). DF Initializer allows users to specify via the command line interface the following information:<list list-type="bullet"><list-item><p>Cluster data (number of nodes and their type, IP addresses, user credentials);</p></list-item><list-item><p>Frequency for generating statistics;</p></list-item><list-item><p>Video sources and their types (video file, IP stream, WebRTC stream);</p></list-item><list-item><p>Detectors to use (among those available) for each video source along with their computation mode (CPU/GPU).</p></list-item><list-item><p>Descriptors to use (among those available) for each detector (pipeline) along with their computation mode (CPU/GPU) and number of instances (workers).</p></list-item></list></p>
      <p>DL DevOps engineers can also develop and include their own algorithms among Detectors or Descriptors. The procedure to integrate a custom algorithm is quite similar in both cases and will be described in the following sections and, more in detail, in <xref ref-type="app" rid="app1-sensors-21-04045">Appendix A</xref>.</p>
      <sec id="sec3dot2dot1-sensors-21-04045">
        <title>3.2.1. Integrating a Custom Detector</title>
        <p>The interface for integrating custom detection and tracking algorithms is provided through a Python abstract class called <monospace>AbstractDetector</monospace> as illustrated in <xref ref-type="fig" rid="sensors-21-04045-f005">Figure 5</xref>. For example, the custom detector and tracker classes, named in the <xref ref-type="fig" rid="sensors-21-04045-f005">Figure 5</xref>
<monospace>SampleDetector</monospace> and <monospace>SampleTracker</monospace>, implement the detection and tracking techniques chosen by the user. The <monospace>SampleExecutor</monospace> class extends the <monospace>AbstractDetector</monospace> class and implements the <monospace>extract_features</monospace> method wrapping these classes and using their methods in order to provide the features of interest along subsequent frames. The <monospace>ObjectProvider</monospace> class, on the platform side, dynamically creates an instance of the executor chosen by the user taking as input the corresponding configuration file. More details are provided in <xref ref-type="sec" rid="secAdot1-sensors-21-04045">Appendix A.1</xref>.</p>
      </sec>
      <sec id="sec3dot2dot2-sensors-21-04045">
        <title>3.2.2. Integrating a Custom Descriptor</title>
        <p>Similarly to the integration of a custom Detector, the interface for integrating custom object description algorithms is provided through a Python abstract class called <monospace>AbstractDescriptor</monospace>. As shown in <xref ref-type="fig" rid="sensors-21-04045-f006">Figure 6</xref>, <monospace>GenericNetwork</monospace> may be the class that implements the algorithm of feature extraction. This class can be wrapped by a custom Descriptor <monospace>GenericDescriptor</monospace> class that extends <monospace>AbstractDescriptor</monospace> and implements the <monospace>detect_batch</monospace> method, which provides the classification result of the objects list received by the Detector, obtained on a single frame, and the <monospace>refine_classification</monospace> method, which should return as output the results averaged over a number of frames specified by the <monospace>win_size</monospace> variable. The <monospace>DescriptionProvider</monospace> class, on the Deep-Framework side, dynamically creates an instance of the Descriptor chosen by the user taking as input the corresponding configuration file. More details are provided in <xref ref-type="sec" rid="secAdot2-sensors-21-04045">Appendix A.2</xref>.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="sec4-sensors-21-04045">
    <title>4. Results</title>
    <sec id="sec4dot1-sensors-21-04045">
      <title>4.1. Experimental Results</title>
      <p>In order to evaluate the performance and scalability of the framework, we conducted two experiments aimed at evaluating the behavior of the system when scaling up a single Descriptor chain (increasing the number of Descriptor workers) and the computational cluster (increasing the number of nodes) for a constant load task. Two computers with the following characteristics are used:<list list-type="bullet"><list-item><p>PC1: OS Ubuntu 16.04 LTS, processor Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20 GHz with 20 cores, 64 GB RAM, NVIDIA GeForce GTX 1080 Ti;</p></list-item><list-item><p>PC2: OS Ubuntu 20.04 LTS, processor Intel(R) Core(TM) i9-9900K CPU @ 3.60 GHz with 16 cores, 32 GB RAM, NVIDIA GeForce RTX 2080 Ti.</p></list-item></list></p>
      <p>In the first experiment, a video with 24 fps, generated from a static image of size 1280 × 720 and containing four equal crops 151 × 176 of a face, was analyzed. The framework was deployed on a single node (PC1) with a single pipeline composed of a Detector based on MTCNN [<xref rid="B19-sensors-21-04045" ref-type="bibr">19</xref>] and optical flow and a single Descriptor based on a popular gender estimation model [<xref rid="B20-sensors-21-04045" ref-type="bibr">20</xref>]. We performed this test both in CPU and GPU mode (only for Descriptor) with an increasing number of Descriptor workers while monitoring the resulting performance in terms of Descriptor throughput and system throughput, averaged over 100 frames. The Descriptor throughput represents the frequency to which the features of each detected object are updated, whereas the system throughput is the frequency of the output messages produced by the Collector which produces one message for every result coming from Detector.</p>
      <p><xref ref-type="fig" rid="sensors-21-04045-f007">Figure 7</xref>a shows that, when using CPU mode, Descriptor throughput increases significantly in an almost linear way with the number of workers. Since in this case all services are in a single node, increasing the number of Descriptor workers also decreases the performance of the Detector, which results in a reduced system throughput as is shown in <xref ref-type="fig" rid="sensors-21-04045-f007">Figure 7</xref>b. However, this reduction is quite limited in comparison with the gain observed in the Descriptor throughput: when six workers are used, there were nearly 5× times faster Descriptor updates with only a 10% reduction in system throughput. After a certain limit (eight workers in this case), increases in the number of workers produce a sharp decline in system performance, probably due the exhaustion of the machine computational resources. In this experiment the Detector does not represent a bottleneck, its response time being negligible (~100 ms) compared to the response time of a single Descriptor (~10 s). In this case, distributing the Descriptors across several workers can sensibly increment the Descriptor throughput.</p>
      <p><xref ref-type="fig" rid="sensors-21-04045-f008">Figure 8</xref> shows the results of the same experiment in GPU mode. This time the gain scaling on several workers slightly improves the Descriptor throughput up to about 30% with three workers, above which the throughput reaches a plateau very close to the value of system throughput which indicates that Descriptors manage to process all Detector results. The overall system throughput shows an essentially regular pattern before decreasing from worker number 12 and beyond, due to the machine reaching its operational capability limits.</p>
      <p>For the second experiment, the same video stream was analyzed with the same Detector, but this time using six different Descriptors with equal computational complexity (i.e., running on each the same gender estimation algorithm). We measured the average Descriptor throughput and the system throughput in four system configurations: one node in CPU mode, one node in GPU mode, two nodes in CPU mode, and two nodes in GPU mode. PC1 was used for one node measurements and both PC1 and PC2 for two node measurements. All Descriptors were deployed with one worker. The results are shown in <xref ref-type="fig" rid="sensors-21-04045-f009">Figure 9</xref>. Distributing the application on two computational nodes produced an improvement of Descriptor throughput of 30% in CPU mode and of 68% in GPU mode. The improvement of the system throughput, being related to the Detector performance which runs in all cases in CPU mode, shows only a small increase due to the indirect effect of a better distribution of resources.</p>
    </sec>
    <sec id="sec4dot2-sensors-21-04045">
      <title>4.2. Application Examples</title>
      <p>In this section, the design and implementation of two real-world applications using Deep-Framework is described. The development of these applications intends to demonstrate the usefulness and flexibility of Deep-Framework as a tool for designing real-time video analytics solutions using state-of-the-art DL models. Since the goal is to show how these applications can be developed with Deep-Framework, details of the adopted DL models are not presented. In order to monitor the performance and visualize the results of both applications, a demo web application, based on AngularJS, was developed to show the extracted data overlaid on video in real-time. The performance results are summarized for the two applications and evaluated in terms of the respective real-time requirements.</p>
      <sec id="sec4dot2dot1-sensors-21-04045">
        <title>4.2.1. Customer Intelligence Application</title>
        <p>The goal of this application is to use a camera feed to characterize the customers of a clothing shop. The idea is to extract customer information at both face and body levels. From face analysis the application recognizes registered customers, estimates age and gender, estimates gaze direction and face emotive expression, and detects whether customers wear glasses or not. From body analysis the application recognizes the clothing type. The resulting data can be used for statistical analysis and triggering messages to shop assistants whenever some specific conditions are met. As shown in <xref ref-type="fig" rid="sensors-21-04045-f010">Figure 10</xref>, this type of application naturally defines a two-pipeline solution using Deep-Framework.</p>
        <p>In the Detector of the face pipeline, a face detection and tracking approach is implemented based on the combination of MTCNN [<xref rid="B19-sensors-21-04045" ref-type="bibr">19</xref>] and optical flow. Briefly, faces are first detected in images using MTCNN which output their bounding boxes and a set of facial landmarks that are then tracked through successive images with the OpenCV implementation of the optical flow-based Lukas–Kanade method. This approach allows for the detection of faces faster than using the MTCNN model on each image. The next section of the face pipeline is composed of seven Descriptor Chains; in each of them a specific DL model is implemented for the extraction of the required information using the following algorithms: age estimation [<xref rid="B20-sensors-21-04045" ref-type="bibr">20</xref>], gender estimation [<xref rid="B20-sensors-21-04045" ref-type="bibr">20</xref>], face recognition based on [<xref rid="B21-sensors-21-04045" ref-type="bibr">21</xref>], glasses detection based on [<xref rid="B22-sensors-21-04045" ref-type="bibr">22</xref>], face expression recognition based on [<xref rid="B23-sensors-21-04045" ref-type="bibr">23</xref>], yaw and pitch estimation [<xref rid="B24-sensors-21-04045" ref-type="bibr">24</xref>]. In the Detector of the body pipeline, an object detector based on MobileNet-SSD [<xref rid="B25-sensors-21-04045" ref-type="bibr">25</xref>] and a tracker based on [<xref rid="B26-sensors-21-04045" ref-type="bibr">26</xref>] were used. A model based on [<xref rid="B22-sensors-21-04045" ref-type="bibr">22</xref>] was used for the classification of the clothing type. <xref ref-type="fig" rid="sensors-21-04045-f011">Figure 11</xref> illustrates the composition of the processing pipelines.</p>
        <p>In order to demonstrate the performance of the DF in this example application, the proposed solution was deployed on a cluster of two nodes (PC1 and PC2 as described in <xref ref-type="sec" rid="sec4dot1-sensors-21-04045">Section 4.1</xref>) and the Descriptor and the system throughput were measured while analyzing a video that captured one person as in <xref ref-type="fig" rid="sensors-21-04045-f010">Figure 10</xref>. Since in this case results are produced by two independent pipelines, the system throughput is actually described by the individual pipeline throughputs which, as already explained, depend on the frequency of Detectors. Both Detectors (face and body) were deployed in CPU mode, while the eight Descriptors (yaw, pitch, glasses, face recognition, clothing, age, gender, emotion), each of them with one worker, were deployed in GPU mode. <xref rid="sensors-21-04045-t001" ref-type="table">Table 1</xref> summarizes the results. Note that most Descriptors have a throughput close to that of their respective pipeline which means that they manage to process most of the frames processed by the Detector.</p>
        <p>Considering that the aim of this application is to collect customer information which typically does not change considerably during an observation period of a few seconds, the measured throughput is appreciably above the expected system capacity. Moreover, assuming a response time of 1 s is sufficient for alerting a shop assistant when a specific event occurs, real-time requirements are largely met. With many people/faces detected at the same time or with more computation demanding descriptors, it may be necessary to allocate more workers for extracting those features which are expected to change more rapidly.</p>
      </sec>
      <sec id="sec4dot2dot2-sensors-21-04045">
        <title>4.2.2. Crowd/Traffic Monitoring Application</title>
        <p>In this application the goal is to monitor the movements of pedestrians and vehicles at critical and/or strategic locations of an urban scenario. The idea is to extract the trajectories of pedestrians and vehicles and also to characterize the scene captured by each camera in terms of flux, velocity/speed, and occupation of each type of object (as depicted in <xref ref-type="fig" rid="sensors-21-04045-f012">Figure 12</xref>). In order to implement this application with Deep-Framework, a single pipeline per video stream was defined using a Detector for identifying, tracking, and labeling objects and two Descriptors for analyzing the distribution of persons and vehicles over the captured area (<xref ref-type="fig" rid="sensors-21-04045-f013">Figure 13</xref>). In the Detector component, YOLOv4 [<xref rid="B27-sensors-21-04045" ref-type="bibr">27</xref>] was employed in order to detect pedestrians, two wheeled vehicles, small four wheeled vehicles and large four wheeled vehicles, and a MOT approach based on Deep SORT [<xref rid="B28-sensors-21-04045" ref-type="bibr">28</xref>] in order to track them. For the Descriptors, a simple algorithm is implemented that divides the image into several small square segments of equal size and calculates the average flux, velocity/speed, and occupation of all the objects that cross each segment over a fixed period of time.</p>
        <p>As done in the <xref ref-type="sec" rid="sec4dot2dot1-sensors-21-04045">Section 4.2.1</xref>, some measurements were conducted in order to evaluate the performance of the system. For this application, only PC2, whose characteristics are described in the <xref ref-type="sec" rid="sec4dot1-sensors-21-04045">Section 4.1</xref>, was used. The measurements, performed with one worker for each Descriptor (vehicle flux analysis and person flux analysis), revealed a system throughput of 3.07. As depicted in the <xref rid="sensors-21-04045-t002" ref-type="table">Table 2</xref>, all Descriptors obtained the same throughput because they could process at most the same number of frames as the Detector, whose throughput represents the maximum value obtainable from any component.</p>
        <p>In this application scenario the system throughput is largely limited by the detection of the many vehicles and pedestrians. However, considering that results can be collected approximately every 300 ms and that a vehicle running at 100 km/h covers a distance of about 9 m in this interval, real-time requirements should be met for a camera coverage similar to the one depicted in <xref ref-type="fig" rid="sensors-21-04045-f012">Figure 12</xref>.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec5-sensors-21-04045">
    <title>5. Discussion</title>
    <p>We have presented Deep-Framework, a software framework for developing real-time video analytics applications in edge computing contexts. DF represents a new open source tool for allowing DL researchers and developers rapid prototyping of distributed, scalable, multi-algorithm, multi-stream, web-ready, and latency-focused solutions.</p>
    <p>DF exploits the lightweight virtualization and service orchestration of Docker containers and Docker swarm which have demonstrated to be a powerful solution for modularizing, parallelizing, and distributing analytics operations at the edge with minimal overhead [<xref rid="B29-sensors-21-04045" ref-type="bibr">29</xref>,<xref rid="B30-sensors-21-04045" ref-type="bibr">30</xref>]. Going beyond simply using containers and services, here we propose a tool for abstracting away from researchers and developers the complexities of resource allocation and cluster management. Moreover, the only platform requirement for building the cluster is to have machines that are visible on the network. A local docker registry is used in the main node to distribute docker images to the remaining nodes in the cluster.</p>
    <p>At its core, DF handles the analysis of a video stream into independent processing pipelines where DL-based information extraction algorithms can be shaped using two types of components (Detector and Descriptor), which characterize the two stages of the processing pipeline. The Detector identifies (and may also classify) objects and regions of interests that are subsequently passed to Descriptors to extract additional specific features. As shown in <xref ref-type="sec" rid="sec4dot2-sensors-21-04045">Section 4.2</xref> (example applications), multiple Detectors allow for the scaling up of the processing of multiple types of objects with multiple pipelines, while multiple Descriptors allow a pipeline to scale up the extraction of multiple features. Along with the possibility of executing different algorithms in parallel, the framework can run multiple instances or workers of the same Descriptor for increasing its throughput. This was explored in <xref ref-type="sec" rid="sec4dot1-sensors-21-04045">Section 4.1</xref> where experiments show that for a given task in CPU mode this feature allows for the achievement of up to 5× times faster Descriptor updates before additional workers start to sensibly reduce the overall system performance. Selectively increasing the number of Descriptor workers can be a throttle that can be used for increasing the performance of an application by tuning the latency of most demanding feature extraction tasks on the basis of available resources.</p>
    <p>Different to other works that report the development of model-specific solutions [<xref rid="B3-sensors-21-04045" ref-type="bibr">3</xref>,<xref rid="B4-sensors-21-04045" ref-type="bibr">4</xref>,<xref rid="B5-sensors-21-04045" ref-type="bibr">5</xref>,<xref rid="B6-sensors-21-04045" ref-type="bibr">6</xref>], DF not only proposes a general purpose architecture but also provides a model and application agnostic Python-based scaffold that is able to wrap deep learning models developed with the most popular and mature Python libraries leveraging on their power to utilize multi-CPU or multi-GPU hardware configurations. DF is better seen as a software framework for rapid-prototyping of applications that require real-time analysis of video streams. To the best of our knowledge there are very few works that report this kind of solution. The Smart Surveillance Framework (SSF) proposed by Nazare and Shwart [<xref rid="B11-sensors-21-04045" ref-type="bibr">11</xref>] is similar in spirit to DF, and though it allows the creation of complex pipelines with user-defined modules thanks to a memory sharing mechanism, it does not offer the possibility to distribute the computation over a cluster or manage the allocation of computational resources taking into account the availability of CPUs and GPUs. Moreover, it does not offer the possibility to exploit the use of DL tools or libraries, which can be important for researchers and developers interested in rapidly deploying modern DL models and testing applications. Another framework that is specifically proposed for using DL models at the edge is EdgeEye [<xref rid="B13-sensors-21-04045" ref-type="bibr">13</xref>], which, similarly to DF, provides high-level HTTP and WebRTC APIs for using integrated models and consuming the output data stream. EdgeEye inherits from GStreamer its capability to reuse many of its open-source plugins and, though it allows the integration of DL models by means of specific inference engines like TensorRT, it does not provide an interface for directly integrating models using the popular Python-based DL libraries like Tensorflow or PyTorch. More importantly, EdgeEye does not exploit distributed computing and relies on having a single powerful machine, which is not always feasible or practical in edge scenarios. In contrast, DF proposes a solution for deploying applications over a cluster of cost-effective machines as well as on a single powerful machine with multiple GPUs. In this regard, DF can be compared with the distributed video analytics framework SIAT [<xref rid="B14-sensors-21-04045" ref-type="bibr">14</xref>], but being based on a Java enterprise-oriented ecosystem and focusing on data persistence and mining, SIAT represents more a cloud centralized solution than an edge alternative.</p>
    <p>DF is designed for processing live video streams by implementing a frame-skipping policy which ensures a real-time behavior by always processing the latest available frame. The data stream resulting from a processing pipeline provides an output message for each frame analyzed by the Detector which determines the latency of the individuated objects or ROIs. On the other hand, the latency of each extracted feature strictly depends on the specificity of each Descriptor, which is in turn determined by the complexity of the inference model and the number of workers involved. For those reasons, not only the output stream will not contain an output message for each frame of the input stream, but not all features in an output message will be updated. This implies that a given output message may contain information about different frames. However, the associated timestamps, which are assigned by the Stream Manager at the moment of grabbing (see <xref ref-type="sec" rid="sec3dot1dot2-sensors-21-04045">Section 3.1.2</xref>), always allow linking the output results to a specific input frame. Moreover, considering that the delay of the video communication can be easily measured experimentally, a client application can link the output messages to external events detected by the camera and synchronize the results from different cameras.</p>
  </sec>
  <sec sec-type="conclusions" id="sec6-sensors-21-04045">
    <title>6. Conclusions</title>
    <p>In this paper, we present a novel edge-oriented framework to develop real-time multi-stream video analytics applications based on deep learning models. Deep-Framework allows the distribution of the computing effort among one or several machines with almost zero user configuration effort, automatizing the creation of the computation cluster and allocation of GPU resources. The framework has a scalable multi-stream architecture based on Docker services where each video feed can be independently processed by multiple processing pipelines, each of them able to execute several algorithms with multiple instances each. For deep learning DevOps engineers the framework provides a Python interface that allows the integration and deployment of models using the most deep learning popular frameworks. By providing standard HTTP and WebRTC APIs, Deep-Framework allows application developers to easily consume the resulting data and video streams with desktop, mobile, or browser clients.</p>
    <p>The experiments show how the scalability of the framework allows applications to fully exploit the computational resources available both at single-machine level and at cluster level in order to deliver timely results. Two different example applications that demonstrate the capability, scalability, and flexibility of the framework on real-world scenarios are also presented.</p>
    <p>Deep-Framework is an ongoing open-source project; those interested in learning more or using it in their own work can find more information and the source code at <uri xlink:href="https://github.com/crs4/deep_framework">https://github.com/crs4/deep_framework</uri> (accessed on 10 June 2021). Finally, Deep-Framework is under active development; feel free to suggest features at <uri xlink:href="https://github.com/crs4/deep_framework/issues">https://github.com/crs4/deep_framework/issues</uri> (accessed on 10 June 2021) so that we can continue to improve this tool for the deep learning video analytics community.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes>
    <title>Author Contributions</title>
    <p>Conceptualization, A.S. and M.A.; methodology, software, validation, formal analysis, and writing—original draft preparation, A.S. and J.F.S.-C.; investigation, A.S.; writing—review and editing, A.S., J.F.S.-C., and M.A.; project administration, funding acquisition, M.A. All authors have read and agreed to the published version of the manuscript.</p>
  </notes>
  <notes>
    <title>Funding</title>
    <p>This research has been funded by the Sardinian Regional Authorities through the projects: PO FESR 2007-2013 Asse VI, Linea di attività 6.2.2.d - Pacchetti Integrati di Agevolazione (PIA) Industria, Artigianato e Servizi “DEEP”, PO FESR 2014-2020 Asse I, Azione 1.2.2, Area di specializzazione Aerospazio “SAURON” and Art 9 LR 20/2015 “PIF”.</p>
  </notes>
  <notes>
    <title>Institutional Review Board Statement</title>
    <p>Ethical review and approval were waived for this study, due to the fact that the only person shown in this paper is one of the authors.</p>
  </notes>
  <notes>
    <title>Informed Consent Statement</title>
    <p>Informed consent was waived due to the fact that the only person shown in this paper is one of the authors.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data Availability Statement</title>
    <p>All models and computer code used during this study for obtaining the presented results are available in the following public repository: <uri xlink:href="https://github.com/crs4/deep_framework">https://github.com/crs4/deep_framework</uri> (accessed on 10 June 2021).</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest.</p>
  </notes>
  <app-group>
    <app id="app1-sensors-21-04045">
      <title>Appendix A</title>
      <sec id="secAdot1-sensors-21-04045">
        <title>Appendix A.1. Integrating a Custom Detector</title>
        <p>The interface for integrating custom detection and tracking algorithms is provided through a Python abstract class called <monospace>AbstractDetector</monospace>. The easiest way to start is by cloning the <monospace>sample_detector</monospace> folder in the <monospace>/detector/object_extractors</monospace> path inside the framework repo and renaming it with the name of the new custom detector. The code inside a very simple detector executor would look something like:</p>
        <p> </p>
        <p>
          <monospace> <styled-content style="color:#8DB086"> from</styled-content><styled-content style="color:#0100C3">utils.features</styled-content><styled-content style="color:#8DB086">import</styled-content> Object, Rect, Point</monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#8DB086"> from</styled-content><styled-content style="color:#0100C3">utils.abstract.detector</styled-content><styled-content style="color:#8DB086">import</styled-content> AbstractDetector</monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#8DB086"> from</styled-content><styled-content style="color:#0100C3">.detector</styled-content><styled-content style="color:#8DB086">import</styled-content> SampleDetector</monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#8DB086"> from</styled-content><styled-content style="color:#0100C3">.tracker</styled-content><styled-content style="color:#8DB086">import</styled-content> SampleTracker</monospace>
        </p>
        <p> </p>
        <p>
          <monospace> <styled-content style="color:#8DB086">class</styled-content><styled-content style="color:#0100C3">SampleExecutor</styled-content>(AbstractDetector):</monospace>
        </p>
        <p>
          <monospace>   <styled-content style="color:#8DB086">def</styled-content><styled-content style="color:#0100C3">__ init__</styled-content>(<styled-content style="color:#8DB086">self</styled-content>):</monospace>
        </p>
        <p>
          <monospace>     <styled-content style="color:#8DB086">self</styled-content><styled-content style="color:#A5A5A5">.</styled-content>detector <styled-content style="color:#A5A5A5">=</styled-content> SampleDetector()</monospace>
        </p>
        <p>
          <monospace>     <styled-content style="color:#8DB086">self</styled-content><styled-content style="color:#A5A5A5">.</styled-content>tracker <styled-content style="color:#A5A5A5">=</styled-content> SampleTracker()</monospace>
        </p>
        <p> </p>
        <p>
          <monospace>   <styled-content style="color:#8DB086">def</styled-content><styled-content style="color:#0100C3">extract_features</styled-content>(<styled-content style="color:#8DB086">self</styled-content>,current_frame,executor_dict):</monospace>
        </p>
        <p>
          <monospace>     detected_objects <styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#8DB086">self</styled-content><styled-content style="color:#A5A5A5">.</styled-content>detector.detect(current_frame)</monospace>
        </p>
        <p>
          <monospace>     tracked_objects <styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#8DB086">self</styled-content><styled-content style="color:#A5A5A5">.</styled-content>tracker.update(detected_objects)</monospace>
        </p>
        <p>
          <monospace>     <styled-content style="color:#8DB086">return</styled-content> tracked_objects</monospace>
        </p>
        <p> </p>
        <p>Note that the actual implementation of the custom detection and tracking algorithms should be done inside the <monospace>SampleDetector</monospace> and <monospace>SampleTracker</monospace> modules. The framework knows where it can find the custom code by looking at the configuration file inside the folder which looks like this:</p>
        <p> </p>
        <p>
          <monospace> <styled-content style="color:#7F8257">CATEGORY</styled-content><styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#AB423F"> sample_category</styled-content></monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#7F8257">PATH</styled-content><styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#AB423F">sample_detector.sample_executor</styled-content></monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#7F8257">CLASS</styled-content><styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#AB423F">SampleExecutor</styled-content></monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#7F8257">FRAMEWORK</styled-content><styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#AB423F">tensorflow</styled-content></monospace>
        </p>
        <p> </p>
        <p>Note that it is necessary to specify the type of DL framework used by the custom algorithms (Tensorflow in the example); this allows the framework to better allocate the GPU resources. All the dependencies and environment requirements of the custom code should be included in Dockerfiles, one per processing mode (CPU/GPU). The custom Detector will be available to be included in the video processing application at the next execution of the DF Initializer.</p>
      </sec>
      <sec id="secAdot2-sensors-21-04045">
        <title>Appendix A.2. Integrating a Custom Descriptor</title>
        <p>Similarly to the integration of a custom Detector, the interface for integrating custom object description algorithms is provided through a Python abstract class called <monospace>AbstractDescriptor</monospace>. As before, the easiest way to start is by cloning a sample folder, this time the <monospace>generic_descriptor</monospace> folder in the <monospace>/descriptor/feature_extractors</monospace> path inside the framework repo and renaming it with the name of the new custom Descriptor. The code inside a very simple descriptor would look something like:</p>
        <p> </p>
        <p>
          <monospace> <styled-content style="color:#8DB086">from</styled-content><styled-content style="color:#0100C3">.generic_network</styled-content><styled-content style="color:#8DB086">import</styled-content> GenericNet</monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#8DB086">from</styled-content><styled-content style="color:#0100C3">utils.abstract_descriptor</styled-content><styled-content style="color:#8DB086">import</styled-content> AbstractDescriptor</monospace>
        </p>
        <p> </p>
        <p>
          <monospace> <styled-content style="color:#8DB086">class</styled-content><styled-content style="color:#0100C3">GenericDescriptor</styled-content>(AbstractDescriptor):</monospace>
        </p>
        <p>
          <monospace>   <styled-content style="color:#8DB086">def</styled-content><styled-content style="color:#0100C3">__init__</styled-content>(<styled-content style="color:#8DB086">self</styled-content>):</monospace>
        </p>
        <p>
          <monospace>     <styled-content style="color:#8DB086">self</styled-content><styled-content style="color:#A5A5A5">.</styled-content>net <styled-content style="color:#A5A5A5">=</styled-content> GenericNet()</monospace>
        </p>
        <p> </p>
        <p>
          <monospace>   win_size <styled-content style="color:#A5A5A5"> = 10 </styled-content></monospace>
        </p>
        <p> </p>
        <p>
          <monospace>   <styled-content style="color:#8DB086">def</styled-content><styled-content style="color:#0100C3">detect_batch</styled-content>(<styled-content style="color:#8DB086">self</styled-content>detector_results,images):</monospace>
        </p>
        <p>
          <monospace>     inference_results <styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#8DB086">self</styled-content><styled-content style="color:#A5A5A5">.</styled-content>net.classify(images)</monospace>
        </p>
        <p>
          <monospace>     <styled-content style="color:#8DB086">return</styled-content> inference_results</monospace>
        </p>
        <p> </p>
        <p>
          <monospace>   <styled-content style="color:#8DB086">def</styled-content><styled-content style="color:#0100C3">refine_classification</styled-content>(<styled-content style="color:#8DB086">self</styled-content>,class_results):</monospace>
        </p>
        <p>
          <monospace>     <styled-content style="color:#8DB086">return</styled-content> class_results.mean()</monospace>
        </p>
        <p> </p>
        <p>Note that the abstract class allows for the implementation of two methods (<monospace>detect_batch</monospace> and <monospace>refine_classification</monospace>) and one class variable (<monospace>win_size</monospace>). On one hand, the <monospace>detect_batch</monospace> method receives the objects individuated by the Detector and their ROIs and should return the classification result. The <monospace>refine_classification</monospace> method, on the other hand, should return as output the results averaged over a number of frames specified by the <monospace>win_size</monospace> variable. The configuration file for defining the Descriptor and telling the framework the location of the custom code may look like this:</p>
        <p> </p>
        <p>
          <monospace> <styled-content style="color:#7F8257">NAME</styled-content><styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#AB423F">generic</styled-content></monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#7F8257">PATH</styled-content><styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#AB423F">genericdescriptor.generic_de</styled-content></monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#7F8257">CLASS</styled-content><styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#AB423F">GenericDescriptor</styled-content></monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#7F8257">FRAMEWORK</styled-content><styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#AB423F">tensorflow</styled-content></monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#7F8257">TYPE</styled-content><styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#AB423F"> objectoriented </styled-content></monospace>
        </p>
        <p>
          <monospace> <styled-content style="color:#7F8257">RELATED_TO</styled-content><styled-content style="color:#A5A5A5">=</styled-content><styled-content style="color:#AB423F"> samplecategory</styled-content></monospace>
        </p>
        <p> </p>
        <p>Note that compared with Detector settings, this time two additional things have to be specified. One is the Descriptor type that tells the framework whether its results will be related to individual objects (<monospace>object_oriented</monospace>) or if they will be related to the whole image (<monospace>image_oriented</monospace>). This will affect the structure of the resulting data messages. The other, <monospace>related_to</monospace>, is the category specified by the detector configuration file.</p>
      </sec>
    </app>
  </app-group>
  <ref-list>
    <title>References</title>
    <ref id="B1-sensors-21-04045">
      <label>1.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Shan</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Porikli</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Xiang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Gong</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <source>Video Analytics for Business Intelligence</source>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>Berlin/Heidelberg, Germany</publisher-loc>
        <year>2012</year>
      </element-citation>
    </ref>
    <ref id="B2-sensors-21-04045">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ananthanarayanan</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Bahl</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Bodík</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Chintalapudi</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Philipose</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ravindranath</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Sinha</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Real-Time Video Analytics: The Killer App for Edge Computing</article-title>
        <source>Computer</source>
        <year>2017</year>
        <volume>50</volume>
        <fpage>58</fpage>
        <lpage>67</lpage>
        <pub-id pub-id-type="doi">10.1109/MC.2017.3641638</pub-id>
      </element-citation>
    </ref>
    <ref id="B3-sensors-21-04045">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cob-Parro</surname>
            <given-names>A.C.</given-names>
          </name>
          <name>
            <surname>Losada-Gutiérrez</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Marrón-Romera</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Gardel-Vicente</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Bravo-Muñoz</surname>
            <given-names>I.</given-names>
          </name>
        </person-group>
        <article-title>Smart Video Surveillance System Based on Edge Computing</article-title>
        <source>Sensors</source>
        <year>2021</year>
        <volume>21</volume>
        <elocation-id>2958</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s21092958</pub-id>
        <?supplied-pmid 33922548?>
        <pub-id pub-id-type="pmid">33922548</pub-id>
      </element-citation>
    </ref>
    <ref id="B4-sensors-21-04045">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Barthélemy</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Verstaevel</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Forehead</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Perez</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Edge-Computing Video Analytics for Real-Time Traffic Monitoring in a Smart City</article-title>
        <source>Sensors</source>
        <year>2019</year>
        <volume>19</volume>
        <elocation-id>2048</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s19092048</pub-id>
        <?supplied-pmid 31052514?>
        <pub-id pub-id-type="pmid">31052514</pub-id>
      </element-citation>
    </ref>
    <ref id="B5-sensors-21-04045">
      <label>5.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Xie</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y.</given-names>
          </name>
          <name>
            <surname>Shou</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Design and Implementation of Video Analytics System Based on Edge Computing</article-title>
        <source>Proceedings of the 2018 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)</source>
        <conf-loc>Zhengzhou, China</conf-loc>
        <conf-date>18–20 October 2018</conf-date>
        <fpage>130</fpage>
        <lpage>1307</lpage>
        <pub-id pub-id-type="doi">10.1109/CyberC.2018.00035</pub-id>
      </element-citation>
    </ref>
    <ref id="B6-sensors-21-04045">
      <label>6.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Bailas</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Marsden</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>O’Connor</surname>
            <given-names>N.E.</given-names>
          </name>
          <name>
            <surname>Little</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Performance of video processing at the edge for crowd-monitoring applications</article-title>
        <source>Proceedings of the 2018 IEEE 4th World Forum on Internet of Things (WF-IoT)</source>
        <conf-loc>Singapore</conf-loc>
        <conf-date>5–8 February 2018</conf-date>
        <fpage>482</fpage>
        <lpage>487</lpage>
        <pub-id pub-id-type="doi">10.1109/WF-IoT.2018.8355170</pub-id>
      </element-citation>
    </ref>
    <ref id="B7-sensors-21-04045">
      <label>7.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>George</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Bala</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Pillai</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>S.W.</given-names>
          </name>
          <name>
            <surname>Satyanarayanan</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Bandwidth-efficient live video analytics for drones via edge computing</article-title>
        <source>Proceedings of the 2018 IEEE/ACM Symposium on Edge Computing (SEC)</source>
        <conf-loc>Seattle, WA, USA</conf-loc>
        <conf-date>25–27 October 2018</conf-date>
        <fpage>159</fpage>
        <lpage>173</lpage>
      </element-citation>
    </ref>
    <ref id="B8-sensors-21-04045">
      <label>8.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ran</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>X.</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Deepdecision: A mobile deep learning framework for edge video analytics</article-title>
        <source>Proceedings of the IEEE INFOCOM 2018-IEEE Conference on Computer Communications</source>
        <conf-loc>Honolulu, HI, USA</conf-loc>
        <conf-date>16–19 April 2018</conf-date>
        <fpage>1421</fpage>
        <lpage>1429</lpage>
      </element-citation>
    </ref>
    <ref id="B9-sensors-21-04045">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Canel</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Andersen</surname>
            <given-names>D.G.</given-names>
          </name>
          <name>
            <surname>Kaminsky</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Dulloor</surname>
            <given-names>S.R.</given-names>
          </name>
        </person-group>
        <article-title>Scaling video analytics on constrained edge nodes</article-title>
        <source>arXiv</source>
        <year>2019</year>
        <pub-id pub-id-type="arxiv">1905.13536</pub-id>
      </element-citation>
    </ref>
    <ref id="B10-sensors-21-04045">
      <label>10.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ananthanarayanan</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Bahl</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Cox</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Crown</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Nogbahi</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Shu</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Video analytics-killer app for edge computing</article-title>
        <source>Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services</source>
        <conf-loc>Seoul, Korea</conf-loc>
        <conf-date>17–21 June 2019</conf-date>
        <fpage>695</fpage>
        <lpage>696</lpage>
      </element-citation>
    </ref>
    <ref id="B11-sensors-21-04045">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nazare</surname>
            <given-names>A.C.</given-names>
            <suffix>Jr.</suffix>
          </name>
          <name>
            <surname>Schwartz</surname>
            <given-names>W.R.</given-names>
          </name>
        </person-group>
        <article-title>A scalable and flexible framework for smart video surveillance</article-title>
        <source>Comput. Vis. Image Underst.</source>
        <year>2016</year>
        <volume>144</volume>
        <fpage>258</fpage>
        <lpage>275</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cviu.2015.10.014</pub-id>
      </element-citation>
    </ref>
    <ref id="B12-sensors-21-04045">
      <label>12.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Ali</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Anjum</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Yaseen</surname>
            <given-names>M.U.</given-names>
          </name>
          <name>
            <surname>Zamani</surname>
            <given-names>A.R.</given-names>
          </name>
          <name>
            <surname>Balouek-Thomert</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Rana</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Parashar</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Edge enhanced deep learning system for large-scale video stream analytics</article-title>
        <source>Proceedings of the 2018 IEEE 2nd International Conference on Fog and Edge Computing (ICFEC)</source>
        <conf-loc>Washington, DC, USA</conf-loc>
        <conf-date>1–3 May 2018</conf-date>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="B13-sensors-21-04045">
      <label>13.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Qi</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Banerjee</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>Edgeeye: An edge service framework for real-time intelligent video analytics</article-title>
        <source>Proceedings of the 1st International Workshop on Edge Systems, Analytics and Networking</source>
        <conf-loc>Munich, Germany</conf-loc>
        <conf-date>8–15 June 2018</conf-date>
        <fpage>1</fpage>
        <lpage>6</lpage>
      </element-citation>
    </ref>
    <ref id="B14-sensors-21-04045">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Uddin</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>Alam</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Tu</surname>
            <given-names>N.A.</given-names>
          </name>
          <name>
            <surname>Islam</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>Y.K.</given-names>
          </name>
        </person-group>
        <article-title>SIAT: A distributed video analytics framework for intelligent video surveillance</article-title>
        <source>Symmetry</source>
        <year>2019</year>
        <volume>11</volume>
        <elocation-id>911</elocation-id>
        <pub-id pub-id-type="doi">10.3390/sym11070911</pub-id>
      </element-citation>
    </ref>
    <ref id="B15-sensors-21-04045">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rouhani</surname>
            <given-names>B.D.</given-names>
          </name>
          <name>
            <surname>Mirhoseini</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Koushanfar</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Rise: An automated framework for real-time intelligent video surveillance on fpga</article-title>
        <source>ACM Trans. Embed. Comput. Syst. (TECS)</source>
        <year>2017</year>
        <volume>16</volume>
        <fpage>1</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1145/3126549</pub-id>
      </element-citation>
    </ref>
    <ref id="B16-sensors-21-04045">
      <label>16.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Soppelsa</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Kaewkasi</surname>
            <given-names>C.</given-names>
          </name>
        </person-group>
        <source>Native Docker Clustering with Swarm</source>
        <publisher-name>Packt Publishing Ltd.</publisher-name>
        <publisher-loc>Birmingham, UK</publisher-loc>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="B17-sensors-21-04045">
      <label>17.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hintjens</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <source>ZeroMQ: Messaging for Many Applications</source>
        <publisher-name>O’Reilly Media, Inc.</publisher-name>
        <publisher-loc>Sebastopol, CA, USA</publisher-loc>
        <year>2013</year>
      </element-citation>
    </ref>
    <ref id="B18-sensors-21-04045">
      <label>18.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Gougeaud</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Zertal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lafoucriere</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Deniel</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Using ZeroMQ as communication/synchronization mechanisms for IO requests simulation</article-title>
        <source>Proceedings of the 2017 International Symposium on Performance Evaluation of Computer and Telecommunication Systems (SPECTS)</source>
        <conf-loc>Seattle, WA, USA</conf-loc>
        <conf-date>9–12 July 2017</conf-date>
        <fpage>1</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="B19-sensors-21-04045">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>Qiao</surname>
            <given-names>Y.</given-names>
          </name>
        </person-group>
        <article-title>Joint face detection and alignment using multitask cascaded convolutional networks</article-title>
        <source>IEEE Signal Process. Lett.</source>
        <year>2016</year>
        <volume>23</volume>
        <fpage>1499</fpage>
        <lpage>1503</lpage>
        <pub-id pub-id-type="doi">10.1109/LSP.2016.2603342</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-sensors-21-04045">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rothe</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Timofte</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Van Gool</surname>
            <given-names>L.</given-names>
          </name>
        </person-group>
        <article-title>Deep expectation of real and apparent age from a single image without facial landmarks</article-title>
        <source>Int. J. Comput. Vis.</source>
        <year>2018</year>
        <volume>126</volume>
        <fpage>144</fpage>
        <lpage>157</lpage>
        <pub-id pub-id-type="doi">10.1007/s11263-016-0940-3</pub-id>
      </element-citation>
    </ref>
    <ref id="B21-sensors-21-04045">
      <label>21.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Schroff</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Kalenichenko</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Philbin</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Facenet: A unified embedding for face recognition and clustering</article-title>
        <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        <conf-loc>Boston, MA, USA</conf-loc>
        <conf-date>7–12 June 2015</conf-date>
        <fpage>815</fpage>
        <lpage>823</lpage>
      </element-citation>
    </ref>
    <ref id="B22-sensors-21-04045">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Simonyan</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Zisserman</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Very deep convolutional networks for large-scale image recognition</article-title>
        <source>arXiv</source>
        <year>2014</year>
        <pub-id pub-id-type="arxiv">1409.1556</pub-id>
      </element-citation>
    </ref>
    <ref id="B23-sensors-21-04045">
      <label>23.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Levi</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Hassner</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Emotion recognition in the wild via convolutional neural networks and mapped binary patterns</article-title>
        <source>Proceedings of the 2015 ACM on International Conference on Multimodal Interaction</source>
        <conf-loc>Seattle, WA, USA</conf-loc>
        <conf-date>9–13 November 2015</conf-date>
        <fpage>503</fpage>
        <lpage>510</lpage>
      </element-citation>
    </ref>
    <ref id="B24-sensors-21-04045">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Patacchiola</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Cangelosi</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Head pose estimation in the wild using Convolutional Neural Networks and adaptive gradient methods</article-title>
        <source>Pattern Recognit.</source>
        <year>2017</year>
        <volume>71</volume>
        <fpage>132</fpage>
        <lpage>143</lpage>
        <pub-id pub-id-type="doi">10.1016/j.patcog.2017.06.009</pub-id>
      </element-citation>
    </ref>
    <ref id="B25-sensors-21-04045">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Howard</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Kalenichenko</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Weyand</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Andreetto</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Adam</surname>
            <given-names>H.</given-names>
          </name>
        </person-group>
        <article-title>Mobilenets: Efficient convolutional neural networks for mobile vision applications</article-title>
        <source>arXiv</source>
        <year>2017</year>
        <pub-id pub-id-type="arxiv">1704.04861</pub-id>
      </element-citation>
    </ref>
    <ref id="B26-sensors-21-04045">
      <label>26.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Danelljan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Häger</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Khan</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Felsberg</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Accurate scale estimation for robust visual tracking</article-title>
        <source>Proceedings of the British Machine Vision Conference</source>
        <conf-loc>Nottingham, UK</conf-loc>
        <conf-date>1–5 September 2014</conf-date>
      </element-citation>
    </ref>
    <ref id="B27-sensors-21-04045">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bochkovskiy</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>C.Y.</given-names>
          </name>
          <name>
            <surname>Liao</surname>
            <given-names>H.Y.M.</given-names>
          </name>
        </person-group>
        <article-title>Yolov4: Optimal speed and accuracy of object detection</article-title>
        <source>arXiv</source>
        <year>2020</year>
        <pub-id pub-id-type="arxiv">2004.10934</pub-id>
      </element-citation>
    </ref>
    <ref id="B28-sensors-21-04045">
      <label>28.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Wojke</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Bewley</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Paulus</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Simple online and realtime tracking with a deep association metric</article-title>
        <source>Proceedings of the 2017 IEEE International Conference on Image Processing (ICIP)</source>
        <conf-loc>Beijing, China</conf-loc>
        <conf-date>17–20 September 2017</conf-date>
        <fpage>3645</fpage>
        <lpage>3649</lpage>
      </element-citation>
    </ref>
    <ref id="B29-sensors-21-04045">
      <label>29.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Morabito</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Beijar</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Enabling Data Processing at the Network Edge through Lightweight Virtualization Technologies</article-title>
        <source>Proceedings of the 2016 IEEE International Conference on Sensing, Communication and Networking (SECON Workshops)</source>
        <conf-loc>London, UK</conf-loc>
        <conf-date>27–30 June 2016</conf-date>
        <fpage>1</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1109/SECONW.2016.7746807</pub-id>
      </element-citation>
    </ref>
    <ref id="B30-sensors-21-04045">
      <label>30.</label>
      <element-citation publication-type="confproc">
        <person-group person-group-type="author">
          <name>
            <surname>Mendki</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>Docker container based analytics at IoT edge Video analytics usecase</article-title>
        <source>Proceedings of the 2018 3rd International Conference On Internet of Things: Smart Innovation and Usages (IoT-SIU)</source>
        <conf-loc>Bhimtal, India</conf-loc>
        <conf-date>23–24 February 2018</conf-date>
        <fpage>1</fpage>
        <lpage>4</lpage>
        <pub-id pub-id-type="doi">10.1109/IoT-SIU.2018.8519852</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="sensors-21-04045-f002" orientation="portrait" position="float">
    <label>Figure 2</label>
    <caption>
      <p>Functional architecture of Deep-Framework. The system is able to process multiple video streams at once through one or more pipelines that are responsible for information extraction.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g002"/>
  </fig>
  <fig id="sensors-21-04045-f003" orientation="portrait" position="float">
    <label>Figure 3</label>
    <caption>
      <p>Processing pipeline. A processing pipeline is composed essentially of: at least one Detector, one or more Descriptor Chain (not mandatory), one for each feature to be extracted, and a Collector.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g003"/>
  </fig>
  <fig id="sensors-21-04045-f004" orientation="portrait" position="float">
    <label>Figure 4</label>
    <caption>
      <p>Components of a pipeline Descriptor Chain. A Descriptor Chain allows a scalable method of feature extraction distributing the messages coming from Detector across the N Descriptor workers. The Broker and the Sub Collector act, respectively, as a producer and a result collector in a push/pull messaging pattern.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g004"/>
  </fig>
  <fig id="sensors-21-04045-f005" orientation="portrait" position="float">
    <label>Figure 5</label>
    <caption>
      <p>Detector class diagram showing the relation between the Deep-Framework classes and the user-defined classes related to the Detector component. More details are provided in <xref ref-type="sec" rid="secAdot1-sensors-21-04045">Appendix A.1</xref>.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g005"/>
  </fig>
  <fig id="sensors-21-04045-f006" orientation="portrait" position="float">
    <label>Figure 6</label>
    <caption>
      <p>Descriptor class diagram, showing the relation between the Deep-Framework classes and the user-defined classes related to the Descriptor component.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g006"/>
  </fig>
  <fig id="sensors-21-04045-f007" orientation="portrait" position="float">
    <label>Figure 7</label>
    <caption>
      <p>Scalability of a single Descriptor chain increasing the number of workers in CPU mode. (<bold>a</bold>) Descriptor throughput. It indicates the number of feature updates per second. (<bold>b</bold>) System throughput. It indicates the number of output messages per second.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g007"/>
  </fig>
  <fig id="sensors-21-04045-f008" orientation="portrait" position="float">
    <label>Figure 8</label>
    <caption>
      <p>Scalability of a single Descriptor chain increasing the number of workers in GPU mode. (<bold>a</bold>) Descriptor throughput. It indicates the number of feature updates per second. (<bold>b</bold>) System throughput. It indicates the number of output messages per second.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g008"/>
  </fig>
  <fig id="sensors-21-04045-f009" orientation="portrait" position="float">
    <label>Figure 9</label>
    <caption>
      <p>Scalability of six identical Descriptor chains using one or two nodes in both CPU and GPU modes. (<bold>a</bold>) Average Descriptor throughput. It indicates the number of feature updates per second averaged among the six Descriptor chains. (<bold>b</bold>) System throughput. It indicates the number of output messages per second.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g009"/>
  </fig>
  <fig id="sensors-21-04045-f010" orientation="portrait" position="float">
    <label>Figure 10</label>
    <caption>
      <p>Example of a customer intelligence application designed with Deep-Framework.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g010"/>
  </fig>
  <fig id="sensors-21-04045-f011" orientation="portrait" position="float">
    <label>Figure 11</label>
    <caption>
      <p>Composition of the processing pipelines for the customer intelligence application.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g011"/>
  </fig>
  <fig id="sensors-21-04045-f012" orientation="portrait" position="float">
    <label>Figure 12</label>
    <caption>
      <p>Example of a crowd/traffic monitoring application designed with Deep-Framework.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g012"/>
  </fig>
  <fig id="sensors-21-04045-f013" orientation="portrait" position="float">
    <label>Figure 13</label>
    <caption>
      <p>Composition of the processing pipeline for the crowd/traffic monitoring application.</p>
    </caption>
    <graphic xlink:href="sensors-21-04045-g013"/>
  </fig>
  <table-wrap id="sensors-21-04045-t001" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-04045-t001_Table 1</object-id>
    <label>Table 1</label>
    <caption>
      <p>Descriptor throughput and pipeline throughput resulting for the customer intelligence application measurements obtained deploying the system in a cluster of two nodes and one worker for each Descriptor.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Descriptor Name</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Throughput (Results/second)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">yaw</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">13.18</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">face recognition</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">13.22</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">glasses</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">13.48</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">age</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">12.5</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">gender</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">12.3</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">emotion</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">12.95</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">pitch</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">13.47</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">clothing</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">22.57</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <bold>Pipeline Name</bold>
          </td>
          <td align="center" valign="middle" rowspan="1" colspan="1">
            <bold>Throughput (Results/second)</bold>
          </td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Person</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">22.63</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Face</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">13.55</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-21-04045-t002" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-04045-t002_Table 2</object-id>
    <label>Table 2</label>
    <caption>
      <p>Descriptor throughput resulting for the crowd/traffic monitoring application experiment conducted on a single node, with one worker for vehicle flux analysis and one for person flux analysis Descriptors.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Descriptor Name</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Throughput (Results/second)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">vehicle flux analysis</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.05</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">person flux analysis</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.05</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">
            <bold>System throughput</bold>
          </td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.07</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
