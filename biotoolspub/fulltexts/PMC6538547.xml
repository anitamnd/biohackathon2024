<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6538547</article-id>
    <article-id pub-id-type="publisher-id">2813</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-2813-6</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CollaboNet: collaboration of deep neural networks for biomedical named entity recognition</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Yoon</surname>
          <given-names>Wonjin</given-names>
        </name>
        <address>
          <email>wonjin.info@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>So</surname>
          <given-names>Chan Ho</given-names>
        </name>
        <address>
          <email>chanhoso@korea.ac.kr</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lee</surname>
          <given-names>Jinhyuk</given-names>
        </name>
        <address>
          <email>jinhyuk_lee@korea.ac.kr</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Kang</surname>
          <given-names>Jaewoo</given-names>
        </name>
        <address>
          <email>kangj@korea.ac.kr</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0840 2678</institution-id><institution-id institution-id-type="GRID">grid.222754.4</institution-id><institution>Department of Computer Science and Engineering, Korea University, </institution></institution-wrap>Seoul, 02841 Republic of Korea </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 0840 2678</institution-id><institution-id institution-id-type="GRID">grid.222754.4</institution-id><institution>Interdisciplinary Graduate Program in Bioinformatics, Korea University, </institution></institution-wrap>Seoul, 02841 Republic of Korea </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>29</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <issue>Suppl 10</issue>
    <issue-sponsor>Publication of this supplement has not been supported by sponsorship. Information about the source of funding for publication charges can be found in the individual articles. The articles have undergone the journal's standard peer review process for supplements. The Supplement Editor declares that he has no competing interests.</issue-sponsor>
    <elocation-id>249</elocation-id>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Finding biomedical named entities is one of the most essential tasks in biomedical text mining. Recently, deep learning-based approaches have been applied to biomedical named entity recognition (BioNER) and showed promising results. However, as deep learning approaches need an abundant amount of training data, a lack of data can hinder performance. BioNER datasets are scarce resources and each dataset covers only a small subset of entity types. Furthermore, many bio entities are polysemous, which is one of the major obstacles in named entity recognition.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>To address the lack of data and the entity type misclassification problem, we propose CollaboNet which utilizes a combination of multiple NER models. In CollaboNet, models trained on a different dataset are connected to each other so that a target model obtains information from other collaborator models to reduce false positives. Every model is an expert on their target entity type and takes turns serving as a target and a collaborator model during training time. The experimental results show that CollaboNet can be used to greatly reduce the number of false positives and misclassified entities including polysemous words. CollaboNet achieved state-of-the-art performance in terms of precision, recall and F1 score.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>We demonstrated the benefits of combining multiple models for BioNER. Our model has successfully reduced the number of misclassified entities and improved the performance by leveraging multiple datasets annotated for different entity types. Given the state-of-the-art performance of our model, we believe that CollaboNet can improve the accuracy of downstream biomedical text mining applications such as bio-entity relation extraction.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>NER</kwd>
      <kwd>Deep learning</kwd>
      <kwd>Named entity recognition</kwd>
      <kwd>Text mining</kwd>
    </kwd-group>
    <conference xlink:href="http://dtmbio.net/dtmbio2018/">
      <conf-name>International Workshop on Data and Text Mining in Biomedical Informatics</conf-name>
      <conf-acronym>DTMBIO 2018</conf-acronym>
      <conf-loc>Turin, Italy</conf-loc>
      <conf-date>22 - 26 October 2018</conf-date>
    </conference>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>The amount of biomedical text continues to increase rapidly. There were 4.7 million full-text online accessible articles in PubMed Central [<xref ref-type="bibr" rid="CR1">1</xref>] in 2017. One of the obstacles in utilizing biomedical text data is that it is too large for a human to read or even search for needed information. This has led to the demand for automated extraction of valuable information. Text mining can be used to turn the time-consuming task into a fully automated job [<xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR7">7</xref>].</p>
    <p>Named Entity Recognition (NER) is the computerized procedure of recognizing and labeling entities in given texts. In the biomedical domain, typical entity types include disease, chemical, gene and protein.</p>
    <p>Biomedical named entity recognition (BioNER) is an essential building block of many downstream text mining applications such as extracting drug-drug interactions [<xref ref-type="bibr" rid="CR8">8</xref>] and disease-treatment relations [<xref ref-type="bibr" rid="CR9">9</xref>]. BioNER is also used when building a sophisticated biomedical entity search tool [<xref ref-type="bibr" rid="CR10">10</xref>] that enables users to pose complex queries to search for bio-entities.</p>
    <p>NER in biomedical text mining is focused mainly on dictionary-, rule-, and machined learning-based approaches [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR16">16</xref>]. Dictionary based systems have a simple and intuitive structure but they cannot handle unseen entities or polysemous words, resulting in low recall [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>]. Moreover, building and maintaining a comprehensive and up-to-date dictionary involves a considerable amount of manual work. The rule based approach is more scalable, but it needs hand crafted feature sets to fit a model to a dataset [<xref ref-type="bibr" rid="CR13">13</xref>, <xref ref-type="bibr" rid="CR14">14</xref>]. These rule and dictionary-based approaches can achieve high precision [<xref ref-type="bibr" rid="CR10">10</xref>] but can produce incorrect predictions when a new word, which is not in the training data, appears in a sentence (out-of-vocabulary problem). This out-of-vocabulary problem occurs frequently especially in the biomedical domain, as it is common for a new biomedical term, such as a new drug name, to be registered in this domain.</p>
    <p>Recently, studies have demonstrated the effectiveness of deep learning based methods. Sahu and Anand [<xref ref-type="bibr" rid="CR17">17</xref>] demonstrated the efficiency of Recurrent Neural Network (RNN) for NER in biomedical text. The model by Sahu and Anand is composed of a bidirectional Long Short-Term Memory Network (BiLSTM) and Conditional Random Field (CRF). Sahu and Anand [<xref ref-type="bibr" rid="CR17">17</xref>] also used character level word embeddings but could not demonstrate their benefits. Habibi et al. [<xref ref-type="bibr" rid="CR18">18</xref>] combined the BiLSTM-CRF model implementation of Lample et al. [<xref ref-type="bibr" rid="CR19">19</xref>] and the word embeddings of Pyysalo et al. [<xref ref-type="bibr" rid="CR20">20</xref>]. Habibi et al. [<xref ref-type="bibr" rid="CR18">18</xref>] utilized character level word embeddings to capture characteristics, such as orthographic features, of bio-medical entities and achieved state-of-the-art performance, demonstrating the effectiveness of character level word embeddings in BioNER.</p>
    <p>Although these models showed some promising results, NER is still a very challenging task in the biomedical domain for the following reasons. First, a limited amount of training data is available for BioNER tasks. Gold-standard datasets contain annotations of one or two entity types. For example, the NCBI corpus [<xref ref-type="bibr" rid="CR21">21</xref>] includes annotations of diseases but not of other types of entities such as genes and proteins. On the other hand, the JNLPBA corpus [<xref ref-type="bibr" rid="CR22">22</xref>] contains annotations of only genes and proteins. Therefore, the data for each entity type comprises only a small portion of the total amount of annotated data.</p>
    <p>Multi-task learning (MTL) is a method for training a single model for multiple tasks at the same time. MTL can leverage different datasets that are collected for different but related tasks [<xref ref-type="bibr" rid="CR23">23</xref>]. Although extracting genes is different from extracting chemicals, both tasks require learning some common features that can help understand the linguistic expressions of biomedical texts. Crichton et al. [<xref ref-type="bibr" rid="CR24">24</xref>] developed an MTL model that was trained on various source datasets containing annotations of different subsets of entity types. An MTL model by Wang et al. [<xref ref-type="bibr" rid="CR25">25</xref>] achieved performance comparable to that of the state-of-the-art single task NER models. Inspired by the previous studies, we propose CollaboNet which uses the collaboration of multiple models. Unlike the conventional MTL methods which use only a single static model, CollaboNet is composed of multiple models trained on different datasets for different tasks. Each model in CollaboNet is trained on dataset annotated on a specific type of entity and becomes an expert on their own entity type.</p>
    <p>Despite the high recall obtained by the MTL based models, the precision of these models is relatively low. Since MTL based models are trained on multiple types of entities and larger training data, they have a broader coverage of various biomedical entities, which naturally results in high recall. On the other hand, as the MTL models are trained on combinations of different entity types, they tend to have difficulty in differentiating among entity types, resulting in lower precision.</p>
    <p>Another reason NER is difficult in the biomedical domain is that an entity could be labeled as different entity types depending on its textual context. In our experiments, we observed that many incorrect predictions were a result of the polysemy problem, in which a word, for example, can be used as both a gene and disease name. Models designed to predict disease entities misidentify some genes as diseases. This misidentification of entity types increases the false positive rate. For instance, BiLSTM-CRF based models for disease entities mistakenly label the gene name <italic>“BRCA1”</italic> as a disease entity because there are disease names such as <italic>“BRCA1 abnormalities”</italic> or <italic>“Brca1-deficient”</italic> in the training set. Besides, the training set that annotates <italic>“VHL”</italic> (Von Hippel-Lindau disease) as a disease entity confuses the models because VHL is also used as a gene name, since the mutation of this gene causes VHL disease.</p>
    <p>To solve the false positive problems due to polysemous words, CollaboNet aggregates the results of collaborator models, and uses them as an additional input to the target model. Consider the case of predicting the disease entity VHL utilizing the outputs of gene and chemical models. Once a gene model predicts VHL as a gene, the gene model informs a disease model that VHL is a gene entity so that the disease model will not predict VHL as a disease. In CollaboNet, each model is individually trained on an entity type and then further trained on the outputs of other models that are trained on the other entity types. The models in CollaboNet take turns in being the target and collaborator models during training. Consequently, each model is an expert in its own domain and helps improve the accuracy by leveraging the multi-domain information from the other models.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p>In the following section, we first discuss a BiLSTM-CRF model for biomedical named entity recognition. The overall structure of the BiLSTM-CRF model is illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Next, we introduce the structure of CollaboNet, which is comprised of a set of BiLSTM-CRF models as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Character level word embedding using CNN and an overview of Bidirectional LSTM with Conditional Random Field (BiLSTM-CRF). Single-task model structure</p></caption><graphic xlink:href="12859_2019_2813_Fig1_HTML" id="MO1"/></fig><fig id="Fig2"><label>Fig. 2</label><caption><p>Structure of CollaboNet. Arrows show the flow of information when target model <italic>M</italic><sub><italic>Target</italic></sub> is training. The models in CollaboNet take turns in being the target model</p></caption><graphic xlink:href="12859_2019_2813_Fig2_HTML" id="MO2"/></fig>
</p>
    <sec id="Sec3">
      <title>Problem Definition</title>
      <p>Named entity recognition involves annotating words in a sentence as named entities. More formally, given an input sequence <italic>S</italic>=[<italic>w</italic><sub>1</sub>,<italic>w</italic><sub>2</sub>,...,<italic>w</italic><sub><italic>N</italic></sub>], we predict corresponding labels <italic>Y</italic>=[<italic>y</italic><sub>1</sub>,<italic>y</italic><sub>2</sub>,...,<italic>y</italic><sub><italic>N</italic></sub>]. We use the BIOES scheme [<xref ref-type="bibr" rid="CR26">26</xref>] for representing <italic>y</italic><sub><italic>t</italic></sub>, where B stands for Beginning, I for Inside, O for Out, E for End, and S for Single.</p>
    </sec>
    <sec id="Sec4">
      <title>Embedding layer</title>
      <sec id="Sec5">
        <title>Word Embedding (WE)</title>
        <p>Word embedding is an effective way of representing words. As word embeddings capture semantic and syntactic meanings of words, they have been widely used in various natural language processing tasks including named entity recognition. The experiment of Habibi et al. [<xref ref-type="bibr" rid="CR18">18</xref>] showed that word embeddings trained on biomedical corpora notably improved the performance of BioNER models. Pyysalo et al. [<xref ref-type="bibr" rid="CR20">20</xref>] were the first to suggest training word embeddings on biomedical corpora from PubMed, PubMed Central (PMC), and Wikipedia. The results of Pyysalo et al. [<xref ref-type="bibr" rid="CR20">20</xref>] and Habibi et al. [<xref ref-type="bibr" rid="CR18">18</xref>] suggest that using word embeddings trained on biomedical corpora is essential for BioNER. We also use the trained word embeddings provided by Pyysalo et al. [<xref ref-type="bibr" rid="CR20">20</xref>]. For each word <italic>w</italic><sub><italic>t</italic></sub> in a sequence <italic>S</italic>, we denote a word represented by a word embedding as <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$x_{t} \in \mathbb {R}^{d^{word}}$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">word</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq1.gif"/></alternatives></inline-formula> where <italic>d</italic><sup><italic>w</italic><italic>o</italic><italic>r</italic><italic>d</italic></sup> is a dimension of the word embedding.</p>
      </sec>
      <sec id="Sec6">
        <title>Character Level Word Embedding (CLWE)</title>
        <p>To give our model character level morphological information (<italic>e.g.,</italic> ‘<italic>-ase</italic>’ is common in protein entities), we also leverage the character level information of each word. We build character level word embeddings (CLWEs) using a convolution neural network (CNN), similar to the work of Santos and Zadrozny [<xref ref-type="bibr" rid="CR27">27</xref>]. Given a word <italic>w</italic><sub><italic>t</italic></sub>, composed of <italic>M</italic> number of characters, we represent <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$w_{t} = \left \{c_{1}^{t},c_{2}^{t},...,c_{M}^{t}\right \}$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="}" open="{" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi>...</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq2.gif"/></alternatives></inline-formula> where <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$c_{i}^{t} \in \mathbb {R}^{d^{char}}$\end{document}</tex-math><mml:math id="M6"><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">char</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq3.gif"/></alternatives></inline-formula> is a randomly initialized character embedding for each unique character. Note that unlike the word embeddings trained on separate biomedical corpora, character embeddings are learned from only the BioNER task. For the CNN, padding of the proper size ((<italic>k</italic>−1)/2) according to window size <italic>k</italic> should be attached before and after each word. We obtain a window vector <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$C_{i}^{t}$\end{document}</tex-math><mml:math id="M8"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq4.gif"/></alternatives></inline-formula> by simply concatenating the character embeddings of <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$C_{i}^{t}$\end{document}</tex-math><mml:math id="M10"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq5.gif"/></alternatives></inline-formula> with the character embeddings of (<italic>k</italic>−1)/2 characters on both sides: 
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ C_{i}^{t} = \left[c^{t}_{i- (k-1)/{2}}, \cdots c_{i}^{t}, \cdots c^{t}_{i+(k-1)/{2}}\right] \in \mathbb{R}^{k d^{char}}   $$ \end{document}</tex-math><mml:math id="M12"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:msubsup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">char</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><graphic xlink:href="12859_2019_2813_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>From the window vector <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$C_{i}^{t}$\end{document}</tex-math><mml:math id="M14"><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq6.gif"/></alternatives></inline-formula>, we perform a convolution operation as follows: 
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \left[ x^{c}_{t} \right]_{j} = \max_{1 \leq i \leq M}\left[ W_{char} C_{i}^{t} + b_{char} \right]_{j}   $$ \end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>max</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">char</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">char</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="12859_2019_2813_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{char} \in \mathbb {R}^{d^{clwe} \times k d^{char}}$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">char</mml:mtext></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">clwe</mml:mtext></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>k</mml:mi><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">char</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq7.gif"/></alternatives></inline-formula> and <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$b_{char} \in \mathbb {R}^{d^{clwe}}$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">char</mml:mtext></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">clwe</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq8.gif"/></alternatives></inline-formula> denote a trainable filter and bias, respectively. We obtain the element-wise maximum values, and the output is a character level word embedding denoted as <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$x_{t}^{c} \in \mathbb {R}^{d^{clwe}}$\end{document}</tex-math><mml:math id="M22"><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">clwe</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq9.gif"/></alternatives></inline-formula>. We concatenate the character level word embedding with the word embedding trained on biomedical corpora as <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {x}_{t} = \left [x_{t}, x^{c}_{t}\right ]$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq10.gif"/></alternatives></inline-formula> to utilize both representations in our model.</p>
      </sec>
    </sec>
    <sec id="Sec7">
      <title>Long Short-Term Memory (LSTM)</title>
      <p>A Recurrent Neural Network (RNN) is a neural network that effectively handles variable-length inputs. RNNs have proven to be useful in various natural language processing tasks including language modeling, speech recognition and machine translation [<xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR30">30</xref>]. Long Short-Term Memory (LSTM) [<xref ref-type="bibr" rid="CR31">31</xref>] is one of the most frequently used variants of recurrent neural networks. Our model uses the LSTM architecture from Graves et al. [<xref ref-type="bibr" rid="CR29">29</xref>]. Given the outputs of an embedding layer <inline-formula id="IEq11"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left [\hat {x}_{1},..., \hat {x}_{N}\right ]$\end{document}</tex-math><mml:math id="M26"><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>...</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq11.gif"/></alternatives></inline-formula>, the hidden states of LSTM are calculated as follows: 
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ i_{t}=\sigma \left(W_{xi} \hat{x}_{t} + W_{hi} h_{t-1} + b_{i} \right)  $$ \end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">xi</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">hi</mml:mtext></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2019_2813_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ4">
          <label>4</label>
          <alternatives>
            <tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ f_{t}=\sigma \left(W_{xf} \hat{x}_{t} + W_{hf} h_{t-1} + b_{f} \right)  $$ \end{document}</tex-math>
            <mml:math id="M30">
              <mml:msub>
                <mml:mrow>
                  <mml:mi>f</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mi>σ</mml:mi>
              <mml:mfenced close=")" open="(" separators="">
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>W</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext mathvariant="italic">xf</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mover accent="true">
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>W</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext mathvariant="italic">hf</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>h</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                      <mml:mo>−</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>f</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:mfenced>
            </mml:math>
            <graphic xlink:href="12859_2019_2813_Article_Equ4.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ5">
          <label>5</label>
          <alternatives>
            <tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ c_{t} = f_{t} \odot c_{t-1} + i_{t} \odot \tanh \left(W_{xc} \hat{x}_{t} + W_{hc} h_{t-1} + b_{c} \right)  $$ \end{document}</tex-math>
            <mml:math id="M32">
              <mml:msub>
                <mml:mrow>
                  <mml:mi>c</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>f</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>⊙</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>c</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                  <mml:mo>−</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
              </mml:msub>
              <mml:mo>+</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>⊙</mml:mo>
              <mml:mo>tanh</mml:mo>
              <mml:mfenced close=")" open="(" separators="">
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>W</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext mathvariant="italic">xc</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mover accent="true">
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>W</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext mathvariant="italic">hc</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>h</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                      <mml:mo>−</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:mfenced>
            </mml:math>
            <graphic xlink:href="12859_2019_2813_Article_Equ5.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ6">
          <label>6</label>
          <alternatives>
            <tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ o_{t}=\sigma \left(W_{xo} \hat{x}_{t} + W_{ho} h_{t-1} + b_{o} \right)  $$ \end{document}</tex-math>
            <mml:math id="M34">
              <mml:msub>
                <mml:mrow>
                  <mml:mi>o</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mi>σ</mml:mi>
              <mml:mfenced close=")" open="(" separators="">
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>W</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext mathvariant="italic">xo</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mover accent="true">
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>W</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext mathvariant="italic">ho</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>h</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                      <mml:mo>−</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>o</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:mfenced>
            </mml:math>
            <graphic xlink:href="12859_2019_2813_Article_Equ6.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ7">
          <label>7</label>
          <alternatives>
            <tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ h_{t} = o_{t} \odot \tanh \left(c_{t} \right)  $$ \end{document}</tex-math>
            <mml:math id="M36">
              <mml:msub>
                <mml:mrow>
                  <mml:mi>h</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>o</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>⊙</mml:mo>
              <mml:mo>tanh</mml:mo>
              <mml:mfenced close=")" open="(" separators="">
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>c</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:mfenced>
            </mml:math>
            <graphic xlink:href="12859_2019_2813_Article_Equ7.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>where <italic>σ</italic> and tanh denote a logistic sigmoid function and a hyperbolic tangent function, respectively, and ⊙ is an element-wise product. We use a forward LSTM that extracts the representations of inputs in the forward direction, and we use a backward LSTM that represents the inputs in the backward direction.</p>
      <p>We concatenate the two states coming from the forward LSTM and the backward LSTM to form the hidden states of the bi-directional LSTM (BiLSTM). BiLSTM, proposed by Schuster and Paliwal [<xref ref-type="bibr" rid="CR32">32</xref>], was extensively used in various sequence encoding tasks. We obtain a set of hidden states <inline-formula id="IEq12"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$h_{t}^{bi} = \left [h_{t}^{f}, h_{t}^{b}\right ] \in \mathbb {R}^{2d^{lstm}}$\end{document}</tex-math><mml:math id="M38"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">bi</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">lstm</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq12.gif"/></alternatives></inline-formula> where <inline-formula id="IEq13"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$h_{t}^{f}$\end{document}</tex-math><mml:math id="M40"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq13.gif"/></alternatives></inline-formula> and <inline-formula id="IEq14"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$h_{t}^{b}$\end{document}</tex-math><mml:math id="M42"><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq14.gif"/></alternatives></inline-formula> are hidden states of forward and backward LSTMs, respectively, at a time step <italic>t</italic>.</p>
    </sec>
    <sec id="Sec8">
      <title>Bidirectional LSTM with Conditional Random Field (BiLSTM-CRF)</title>
      <p>While BiLSTM handles long term dependency problems as well as backward dependency issues, modeling dependencies among adjacent output tags helps improve the performance of the sequence labeling models [<xref ref-type="bibr" rid="CR25">25</xref>]. We applied a Conditional Random Field (CRF) to the output layer of the BiLSTM to capture these dependencies.</p>
      <p>First, we compute the probability of each label given the sequence <italic>S</italic>=[<italic>w</italic><sub>1</sub>,...,<italic>w</italic><sub><italic>N</italic></sub>] as follows: 
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  z_{t} = W_{y} h_{t}^{bi} + b_{y}  $$ \end{document}</tex-math><mml:math id="M44"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">bi</mml:mtext></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:math><graphic xlink:href="12859_2019_2813_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ9">
          <label>9</label>
          <alternatives>
            <tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} p(y_{t}|w_{1},..., w_{N}; \Theta) = \text{softmax}(z_{t})\\ \text{softmax}(\mathbf{a}_{j}) = \frac{\exp{a_{j}}}{\sum_{k} \exp{a_{k}}} \end{aligned}  $$ \end{document}</tex-math>
            <mml:math id="M46">
              <mml:mtable>
                <mml:mtr>
                  <mml:mtd>
                    <mml:mi>p</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>|</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>w</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:mi>...</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>w</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>;</mml:mo>
                    <mml:mi>Θ</mml:mi>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mtext>softmax</mml:mtext>
                    <mml:mo>(</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>)</mml:mo>
                  </mml:mtd>
                </mml:mtr>
                <mml:mtr>
                  <mml:mtd>
                    <mml:mtext>softmax</mml:mtext>
                    <mml:mo>(</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold">a</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mo>exp</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>a</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:munder>
                          <mml:mrow>
                            <mml:mo>∑</mml:mo>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>k</mml:mi>
                          </mml:mrow>
                        </mml:munder>
                        <mml:mo>exp</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>a</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>k</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_2813_Article_Equ9.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>where <inline-formula id="IEq15"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{y} \in \mathbb {R}^{5 \times 2d^{lstm}}$\end{document}</tex-math><mml:math id="M48"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">lstm</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq15.gif"/></alternatives></inline-formula> and <inline-formula id="IEq16"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$b_{y} \in \mathbb {R}^{5}$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>ℝ</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq16.gif"/></alternatives></inline-formula> are parameters of the fully connected layer for BIOES tags, and the softmax(·) function computes the probability of each tag. Based on the probability <italic>p</italic> and the CRF layer, our training objective to minimize is defined as follows: 
<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$  L_{LSTM} = -\sum_{t=1}^{N} \log p(y_{t}|w_{1},..., w_{N}; \Theta)  $$ \end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">LSTM</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>log</mml:mo><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>...</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>Θ</mml:mi><mml:mo>)</mml:mo></mml:math><graphic xlink:href="12859_2019_2813_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ11">
          <label>11</label>
          <alternatives>
            <tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ L_{CRF} = -\sum_{t=1}^{T} {\left(A_{y_{t-1},y_{t}} + z_{t,y_{t}} \right)}  $$ \end{document}</tex-math>
            <mml:math id="M54">
              <mml:msub>
                <mml:mrow>
                  <mml:mi>L</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">CRF</mml:mtext>
                </mml:mrow>
              </mml:msub>
              <mml:mo>=</mml:mo>
              <mml:mo>−</mml:mo>
              <mml:munderover>
                <mml:mrow>
                  <mml:mo>∑</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>t</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>T</mml:mi>
                </mml:mrow>
              </mml:munderover>
              <mml:mfenced close=")" open="(" separators="">
                <mml:mrow>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>A</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>y</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>t</mml:mi>
                          <mml:mo>−</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>y</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>t</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>z</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>y</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>t</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:mfenced>
            </mml:math>
            <graphic xlink:href="12859_2019_2813_Article_Equ11.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>
        <disp-formula id="Equ12">
          <label>12</label>
          <alternatives>
            <tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Loss = L_{LSTM} + L_{CRF}  $$ \end{document}</tex-math>
            <mml:math id="M56">
              <mml:mtext mathvariant="italic">Loss</mml:mtext>
              <mml:mo>=</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>L</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">LSTM</mml:mtext>
                </mml:mrow>
              </mml:msub>
              <mml:mo>+</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>L</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mtext mathvariant="italic">CRF</mml:mtext>
                </mml:mrow>
              </mml:msub>
            </mml:math>
            <graphic xlink:href="12859_2019_2813_Article_Equ12.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>where <italic>L</italic><sub><italic>LSTM</italic></sub> is the cross entropy loss for the label <italic>y</italic><sub><italic>t</italic></sub>, and <italic>L</italic><sub><italic>CRF</italic></sub> is the negative sentence-level log likelihood. The score of a tag is the summation of the transition score <inline-formula id="IEq17"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$A_{y_{t-1},y_{t}}$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq17.gif"/></alternatives></inline-formula> and the emission score from our LSTM <inline-formula id="IEq18"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$z_{t,y_{t}}$\end{document}</tex-math><mml:math id="M60"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq18.gif"/></alternatives></inline-formula> at time step <italic>t</italic>.</p>
      <p>At test time, we use Viterbi decoding to find the most probable sequence given the outputs of the BiLSTM-CRF model.</p>
    </sec>
    <sec id="Sec9">
      <title>CollaboNet</title>
      <p>CollaboNet, our novel NER model, is composed of multiple BiLSTM-CRF models (Fig. <xref rid="Fig2" ref-type="fig">2</xref>), and following the terminology of [<xref ref-type="bibr" rid="CR25">25</xref>], we call each BiLSTM-CRF model a single-task model (STM). In CollaboNet, each STM is trained on a specific dataset and each STM is regarded as an expert on a particular entity type. These experts help each other since the knowledge of each expert is transferred to all the other experts. Training CollaboNet consists of phases and in each phase, except for the first preparation phase, only the target STM is trained on a single dataset for one epoch while the other STMs are not trained but only used to generate input for the target STM which is trained.</p>
      <p>More formally, let us denote a set of datasets as <italic>D</italic>, and a single-task model as <inline-formula id="IEq19"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{k}^{n}$\end{document}</tex-math><mml:math id="M62"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq19.gif"/></alternatives></inline-formula>, which is trained on the <italic>k</italic>-th dataset in phase <italic>P</italic><sup><italic>n</italic></sup>. In the preparation phase (<italic>P</italic><sup>0</sup>) of CollaboNet, each STM is trained independently on a corresponding dataset until the performance of each model converges.</p>
      <p>Note that an STM in the preparation phase <inline-formula id="IEq20"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left (M_{k}^{0}\right)$\end{document}</tex-math><mml:math id="M64"><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq20.gif"/></alternatives></inline-formula> is the same as a single BiLSTM-CRF model. In the preparation phase, we assume that each model <inline-formula id="IEq21"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{k}^{0}$\end{document}</tex-math><mml:math id="M66"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq21.gif"/></alternatives></inline-formula> has obtained the maximum amount of knowledge about the <italic>k</italic>-th dataset.</p>
      <p>In the subsequent phases <italic>P</italic><sup><italic>n</italic></sup>, where <italic>n</italic>≥1, we select an STM <inline-formula id="IEq22"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{d}^{n-1}$\end{document}</tex-math><mml:math id="M68"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq22.gif"/></alternatives></inline-formula> which is an expert on the dataset <italic>d</italic>. We refer to the target STM <inline-formula id="IEq23"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{d}^{n-1}$\end{document}</tex-math><mml:math id="M70"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq23.gif"/></alternatives></inline-formula> as the <italic>target model</italic>, and the remaining STMs as the <italic>collaborator models</italic>. To train the target model <inline-formula id="IEq24"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{d}^{n-1}$\end{document}</tex-math><mml:math id="M72"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq24.gif"/></alternatives></inline-formula>, we use inputs from the target dataset <italic>d</italic> and BiLSTM outputs from collaborator models <inline-formula id="IEq25"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{k}^{n-1}, \left \{k \vert k \neq d, k \in D \right \}$\end{document}</tex-math><mml:math id="M74"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mfenced close="}" open="{" separators=""><mml:mrow><mml:mi>k</mml:mi><mml:mo>|</mml:mo><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq25.gif"/></alternatives></inline-formula>. We train each STM on its dataset for one epoch, and change the target STM <inline-formula id="IEq26"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{d}^{n-1}$\end{document}</tex-math><mml:math id="M76"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq26.gif"/></alternatives></inline-formula> as follows: 
<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} \hat{S_{d}^{n}}= \alpha_{k_{1}}M_{k_{1}}^{n-1}([S_{d}; \mathbf{0}]) \oslash \cdots \oslash \alpha_{k_{m}}M_{k_{m}}^{n-1}([S_{d}; \mathbf{0}]),\\ \left\{k_{i} \vert k_{i} \neq d, k_{i} \in S \right\} \end{aligned}  $$ \end{document}</tex-math><mml:math id="M78"><mml:mtable><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>]</mml:mo><mml:mo>)</mml:mo><mml:mo>⊘</mml:mo><mml:mo>⋯</mml:mo><mml:mo>⊘</mml:mo><mml:msub><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>]</mml:mo><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfenced close="}" open="{" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>≠</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2019_2813_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>
        <disp-formula id="Equ14">
          <label>14</label>
          <alternatives>
            <tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \hat{p}(Y_{d}|S_{d}) = M_{d}^{n-1}\left([S_{d};\hat{S_{d}^{n}}]\right)  $$ \end{document}</tex-math>
            <mml:math id="M80">
              <mml:mover accent="true">
                <mml:mrow>
                  <mml:mi>p</mml:mi>
                </mml:mrow>
                <mml:mo>^</mml:mo>
              </mml:mover>
              <mml:mo>(</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>Y</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>d</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>|</mml:mo>
              <mml:msub>
                <mml:mrow>
                  <mml:mi>S</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>d</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:mo>)</mml:mo>
              <mml:mo>=</mml:mo>
              <mml:msubsup>
                <mml:mrow>
                  <mml:mi>M</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>d</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>n</mml:mi>
                  <mml:mo>−</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
              </mml:msubsup>
              <mml:mfenced close=")" open="(" separators="">
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mi>S</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>;</mml:mo>
                  <mml:mover accent="true">
                    <mml:mrow>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi>S</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>d</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>n</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:mrow>
                    <mml:mo>^</mml:mo>
                  </mml:mover>
                  <mml:mo>]</mml:mo>
                </mml:mrow>
              </mml:mfenced>
            </mml:math>
            <graphic xlink:href="12859_2019_2813_Article_Equ14.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
      <p>where [·;·] denotes concatenation and ⊘ denotes an aggregation operation such as max pooling or concatenation. We used weighted max pooling for the aggregation operation. <italic>S</italic><sub><italic>d</italic></sub> is the input sequences of <italic>d</italic>-th dataset, and <inline-formula id="IEq27"><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{d}^{n-1}(\cdot)$\end{document}</tex-math><mml:math id="M82"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mo>·</mml:mo><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq27.gif"/></alternatives></inline-formula> is output <italic>h</italic><sub><italic>t</italic></sub>, defined by Eq. <xref rid="Equ7" ref-type="">7</xref>. When aggregating the results of collaborator models, we multiply each of the results by a weight <italic>α</italic><sub><italic>k</italic></sub>, which is a trainable parameter. The results are used to train the model <inline-formula id="IEq28"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{d}^{n-1}$\end{document}</tex-math><mml:math id="M84"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq28.gif"/></alternatives></inline-formula>. Using the outputs obtained by Eq. <xref rid="Equ14" ref-type="">14</xref>, we train <inline-formula id="IEq29"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{d}^{n-1}$\end{document}</tex-math><mml:math id="M86"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq29.gif"/></alternatives></inline-formula> for one epoch, and it becomes <inline-formula id="IEq30"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{d}^{n}$\end{document}</tex-math><mml:math id="M88"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq30.gif"/></alternatives></inline-formula> in the next phase. The CRF layer is attached to the final output of <inline-formula id="IEq31"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{k}^{n}$\end{document}</tex-math><mml:math id="M90"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq31.gif"/></alternatives></inline-formula>. Once we iterate all the target datasets <italic>d</italic>∈<italic>D</italic>, the next phase begins.</p>
      <p>During the training phase <italic>P</italic><sup><italic>n</italic></sup> for <italic>d</italic>, the target STM, which is composed of the BiLSTM layer and the CRF layer, and weights <italic>α</italic><sub><italic>k</italic></sub>{<italic>k</italic>|<italic>k</italic>≠<italic>d</italic>,<italic>k</italic>∈<italic>D</italic>} are trained. Parameters of the other STMs are not trained but the STMs generate only inferences on dataset <italic>d</italic> in the training phase <italic>P</italic><sup><italic>n</italic></sup>. For example, when the disease dataset is the target dataset, the BiLSTM of the other STMs produces inferences about the other entity types for the disease dataset. More specifically, inferences about genes for the disease dataset <inline-formula id="IEq32"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$M_{gene}^{n-1}([S_{{disease}}; \mathbf {0}])$\end{document}</tex-math><mml:math id="M92"><mml:msubsup><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">gene</mml:mtext></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>(</mml:mo><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">disease</mml:mtext></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mn mathvariant="bold">0</mml:mn><mml:mo>]</mml:mo><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2019_2813_Article_IEq32.gif"/></alternatives></inline-formula> which has rich information on gene entities, will benefit the disease STM.</p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Experiments</title>
    <sec id="Sec11">
      <title>Datasets</title>
      <p>We used 5 datasets (BC2GM [<xref ref-type="bibr" rid="CR33">33</xref>], BC4CHEMD [<xref ref-type="bibr" rid="CR34">34</xref>], BC5CDR [<xref ref-type="bibr" rid="CR35">35</xref>–<xref ref-type="bibr" rid="CR38">38</xref>], JNLPBA [<xref ref-type="bibr" rid="CR22">22</xref>], NCBI [<xref ref-type="bibr" rid="CR21">21</xref>]), all of which were collected by Crichton et al. [<xref ref-type="bibr" rid="CR24">24</xref>] (Table <xref rid="Tab1" ref-type="table">1</xref>). Each of the 5 datasets were constructed from MEDLINE abstracts, and we used the BIOES notation format for named entity labels [<xref ref-type="bibr" rid="CR26">26</xref>]. Each dataset focuses on one of the three biomedical entity types: disease, chemical, and gene/protein. We did not use cell-type entity tags from JNLPBA for the entity types.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Descriptions of datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Datasets</th><th align="left">Entity type</th><th align="left"># of sentence</th><th align="left"># of annotations</th><th align="left">Data Size</th></tr></thead><tbody><tr><td align="left">NCBI-Disease (Dogan et al., 2014)</td><td align="left">Disease</td><td align="left">7639</td><td align="left">6881</td><td align="left">793 abstracts</td></tr><tr><td align="left">JNLPBA (Kim et al., 2004)</td><td align="left">Gene/Proteins</td><td align="left">22,562</td><td align="left">35,336</td><td align="left">2404 abstracts</td></tr><tr><td align="left">BC5CDR (Li et al., 2016)</td><td align="left">Chemicals</td><td align="left">14,228</td><td align="left">15,935</td><td align="left">1500 articles</td></tr><tr><td align="left">BC5CDR (Li et al., 2016)</td><td align="left">Diseases</td><td align="left">14,228</td><td align="left">12,852</td><td align="left">1500 articles</td></tr><tr><td align="left">BC4CHEMD (Krallinger et al., 2015a)</td><td align="left">Chemicals</td><td align="left">86,679</td><td align="left">84,310</td><td align="left">10,000 abstracts</td></tr><tr><td align="left">BC2GM (Akhondi et al., 2014)</td><td align="left">Gene/Proteins</td><td align="left">20,510</td><td align="left">24,583</td><td align="left">20,000 sentences</td></tr></tbody></table></table-wrap>
</p>
      <p>All the datasets are comprised of pairs of input sentences and biomedical entity labels for the sentences. While the JNLPBA dataset has only training and test sets, the other four datasets contain training, development and test sets. For JNLPBA, we used part of its training set as its development set which is the same size as its test set. Also, we found that the JNLPBA dataset from Crichton et al. [<xref ref-type="bibr" rid="CR24">24</xref>] contained sentences that were incorrectly split. So we preprocessed the original dataset by Kim et al. [<xref ref-type="bibr" rid="CR22">22</xref>] with a more accurate sentence separation.</p>
      <p>The BC5CDR dataset has the sub-datasets BC5CDR-chem, BC5CDR-disease and BC5CDR-both, and they contain chemical entity types, disease entity types, and both entity types, respectively. We reported the performance on BC5CDR-chem and BC5CDR-disease. We have a total of six datasets: BC2GM, BC4CHEMD, BC5CDR-chem, BC5CDR-disease, JNLPBA, and NCBI.</p>
    </sec>
    <sec id="Sec12">
      <title>Metric</title>
      <p>For the evaluation of the named entity recognition task, true positives are counted from exact matches between predicted entity spans and ground truth spans based on the BIOES notation.</p>
      <p>We also designed and applied a simple post-processing step that corrects invalid BIOES sequences. This simple step improved precision by about 0.1 to 0.5%, and thus boosted the F1 score by about 0.04 to 0.3%.</p>
      <p>Precision, recall and F1 scores were used to evaluate the models. 
<list list-type="bullet"><list-item><p>M = total number of predicted entities in the sequence.</p></list-item><list-item><p>N = total number of ground truth entities in the sequence.</p></list-item><list-item><p>C = total number of correct entities.</p></list-item></list></p>
      <p>
        <disp-formula id="Equ15">
          <label>15</label>
          <alternatives>
            <tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} {Precision}=P=\frac{C}{M}, {Recall}=R=\frac{C}{N}, \\ {F}_{1} score=\frac{2PR}{P+R} \end{aligned}  $$ \end{document}</tex-math>
            <mml:math id="M94">
              <mml:mtable>
                <mml:mtr>
                  <mml:mtd>
                    <mml:mtext mathvariant="italic">Precision</mml:mtext>
                    <mml:mo>=</mml:mo>
                    <mml:mi>P</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>M</mml:mi>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:mo>,</mml:mo>
                    <mml:mtext mathvariant="italic">Recall</mml:mtext>
                    <mml:mo>=</mml:mo>
                    <mml:mi>R</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:mo>,</mml:mo>
                  </mml:mtd>
                </mml:mtr>
                <mml:mtr>
                  <mml:mtd>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mtext mathvariant="italic">score</mml:mtext>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mn>2</mml:mn>
                        <mml:mtext mathvariant="italic">PR</mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>P</mml:mi>
                        <mml:mo>+</mml:mo>
                        <mml:mi>R</mml:mi>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:math>
            <graphic xlink:href="12859_2019_2813_Article_Equ15.gif" position="anchor"/>
          </alternatives>
        </disp-formula>
      </p>
    </sec>
    <sec id="Sec13">
      <title>Settings and hyperparameters</title>
      <p>We used the 200 dimensional word embedding (WE) by Pyysalo et al. [<xref ref-type="bibr" rid="CR20">20</xref>] which was trained on PubMed, PubMed Central (PMC) and Wikipedia text, and it contains about 5 million words. Word2vec [<xref ref-type="bibr" rid="CR39">39</xref>] was used to train the word embedding. For character level word embedding (CLWE), we used window sizes of 3, 5, and 7.</p>
      <p>We used AdaGrad optimizer [<xref ref-type="bibr" rid="CR40">40</xref>] with an initial learning rate of 0.01 which was exponentially decayed for each epoch by 0.95. The dimension of the character embedding (<italic>d</italic><sup><italic>c</italic><italic>h</italic><italic>a</italic><italic>r</italic></sup>) was 30 and dimension of the character level word embedding (<italic>d</italic><sup><italic>c</italic><italic>l</italic><italic>w</italic><italic>e</italic></sup>) was 200*3. We used 300 hidden units for both forward and backward LSTMs. Weights for aggregating the results of collaborator models were uniformly initialized with 1. We applied dropout [<xref ref-type="bibr" rid="CR41">41</xref>] to two parts of CollaboNet: output of CLWE (0.5) and output of BiLSTM (0.3). The mini-batch size for our experiment was 10.</p>
      <p>Most of our hyperparameter settings are similar to those of Wang et al. [<xref ref-type="bibr" rid="CR25">25</xref>]. Only a few settings such as the dropout rates were different from the hyperparameters of Wang. We tuned these hyperparameters using validation sets.</p>
      <p>The preparation phase <italic>P</italic><sup>0</sup> for 6 datasets takes approximately 900 min, which is the same amount of time it takes to train 6 single-task models. The rest of the phases <italic>P</italic><sup><italic>n</italic></sup>,<italic>n</italic>≥1 require 3000 min for complete training. If we exclude BC4CHEMD, the largest dataset, then the training time for <italic>P</italic><sup><italic>n</italic></sup> is reduced to 1500 min, which is half the time required for the remainder phases. Experiments were conducted on a 10-core CPU (Intel Xeon E5-260 v4 CPU 2.2 GHz) with one graphics processing unit (NVIDIA Titan Xp). Our code is written in TensorFlow 1.7 (GPU enabled version) for Python 2.7.</p>
    </sec>
  </sec>
  <sec id="Sec14" sec-type="results">
    <title>Results</title>
    <p>The experimental results of the baseline models and CollaboNet are provided in Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref>, respectively. Table <xref rid="Tab2" ref-type="table">2</xref> shows the results of the single-task models (STMs) where Table <xref rid="Tab3" ref-type="table">3</xref> shows the comparison between the existing state-of-the-art multi-task learning model (MTM) and our CollaboNet.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Performances of single-task models</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left" colspan="3">Habibi et al. (2017) STM</th><th align="left" colspan="3">Wang et al. (2018) STM</th><th align="left" colspan="3">Our STM</th></tr><tr><th align="left">Dataset</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1 Score</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1 Score</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1 Score</th></tr></thead><tbody><tr><td align="left">NCBI-disease</td><td align="left">85.31</td><td align="left">83.58</td><td align="left">84.44</td><td align="left">84.95</td><td align="left">82.92</td><td align="left">83.92</td><td align="left">83.95</td><td align="left">85.45</td><td align="left"><bold>84.69</bold> (±0.54)</td></tr><tr><td align="left">JNLPBA</td><td align="left">74.83</td><td align="left">79.82</td><td align="left">77.25</td><td align="left">69.60</td><td align="left">74.95</td><td align="left">72.17</td><td align="left">72.51</td><td align="left">82.98</td><td align="left"><bold>77.39</bold> (±0.24)</td></tr><tr><td align="left">BC5CDR-chem</td><td align="left">92.57</td><td align="left">88.77</td><td align="left">90.63</td><td align="left">*93.05</td><td align="left">*86.87</td><td align="left">*89.85</td><td align="left">94.02</td><td align="left">91.50</td><td align="left"><bold>92.74</bold> (±0.47)</td></tr><tr><td align="left">BC5CDR-disease</td><td align="left">84.19</td><td align="left">82.79</td><td align="left"><bold>83.49</bold></td><td align="left">*84.09</td><td align="left">*81.32</td><td align="left">*82.68</td><td align="left">82.98</td><td align="left">82.25</td><td align="left">82.61 (±0.25)</td></tr><tr><td align="left">BC4CHEMD</td><td align="left">87.83</td><td align="left">85.45</td><td align="left">86.62</td><td align="left">90.53</td><td align="left">87.04</td><td align="left"><bold>88.75</bold></td><td align="left">90.50</td><td align="left">85.96</td><td align="left">88.19 (±0.23)</td></tr><tr><td align="left">BC2GM</td><td align="left">77.50</td><td align="left">78.13</td><td align="left">77.82</td><td align="left">81.11</td><td align="left">78.91</td><td align="left"><bold>80.00</bold></td><td align="left">79.70</td><td align="left">77.47</td><td align="left">78.56 (±0.38)</td></tr><tr><td align="left">Macro Average</td><td align="left">83.71</td><td align="left">83.09</td><td align="left">83.38</td><td align="left">83.89</td><td align="left">82.00</td><td align="left">82.90</td><td align="left"><bold>83.94</bold></td><td align="left"><bold>84.27</bold></td><td align="left"><bold>84.03</bold></td></tr></tbody></table><table-wrap-foot><p>Our STM achieved the best performance on 3 datasets among 6. Scores in the asterisked (*) cells are obtained in the experiments that we conducted; these scores are not reported in the original papers. The best scores from these experiments are in bold</p></table-wrap-foot></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Performance of CollaboNet and the Multi-Task Model by Wang et al. [<xref ref-type="bibr" rid="CR25">25</xref>]</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Model</th><th align="left" colspan="3">Wang et al. (2018) MTM</th><th align="left" colspan="3">CollaboNet</th></tr><tr><th align="left">Dataset</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1 Score</th><th align="left">Precision</th><th align="left">Recall</th><th align="left">F1 Score</th></tr></thead><tbody><tr><td align="left">NCBI-disease</td><td align="left">85.86</td><td align="left">86.42</td><td align="left">86.14</td><td align="left">85.48</td><td align="left">87.27</td><td align="left"><bold>86.36</bold>(±0.54)</td></tr><tr><td align="left">JNLPBA</td><td align="left">70.91</td><td align="left">76.34</td><td align="left">73.52</td><td align="left">74.43</td><td align="left">83.22</td><td align="left"><bold>78.58</bold></td></tr><tr><td align="left">BC5CDR-chem</td><td align="left">*93.09</td><td align="left">*89.56</td><td align="left">*91.29</td><td align="left">94.26</td><td align="left">92.38</td><td align="left"><bold>93.31</bold></td></tr><tr><td align="left">BC5CDR-disease</td><td align="left">*83.73</td><td align="left">*82.93</td><td align="left">*83.33</td><td align="left">85.61</td><td align="left">82.61</td><td align="left"><bold>84.08</bold></td></tr><tr><td align="left">BC4CHEMD</td><td align="left">91.30</td><td align="left">87.53</td><td align="left"><bold>89.37</bold></td><td align="left">90.78</td><td align="left">87.01</td><td align="left">88.85</td></tr><tr><td align="left">BC2GM</td><td align="left">82.10</td><td align="left">79.42</td><td align="left"><bold>80.74</bold></td><td align="left">80.49</td><td align="left">78.99</td><td align="left">79.73</td></tr><tr><td align="left">Macro Average</td><td align="left">84.50</td><td align="left">83.70</td><td align="left">84.07</td><td align="left"><bold>85.18</bold></td><td align="left"><bold>85.25</bold></td><td align="left"><bold>85.15</bold></td></tr></tbody></table><table-wrap-foot><p>Scores in the asterisked (*) cells are obtained in the experiments that we conducted; these scores are not reported in the original papers. The best scores from these experiments are in bold</p></table-wrap-foot></table-wrap>
</p>
    <p>Since Wang et al. [<xref ref-type="bibr" rid="CR25">25</xref>] used BC5CDR-both for their experiments, we reran their models on BC5CDR-chem and BC5CDR-disease for a fair comparison with other models. The rerun scores are denoted with asterisks. We conducted 10 experiments with 10 different random initializations on our STM. We take arithmetic mean over the 6 datasets to compare the overall performance of each model.</p>
    <sec id="Sec15">
      <title>Performance of single-task models</title>
      <p>Table <xref rid="Tab2" ref-type="table">2</xref> shows the results of the STMs of Habibi et al. [<xref ref-type="bibr" rid="CR18">18</xref>] and Wang et al. [<xref ref-type="bibr" rid="CR25">25</xref>] (baseline STMs), and our STM on the 6 datasets. While the baseline STMs applied BiLSTM for the Character Level Word Embedding (CLWE) layer [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR25">25</xref>], our STM used Convolution Neural Network (CNN) for the CLWE layer.</p>
      <p>On average, our STM outperforms the baseline STMs in terms of precision, recall and F1 score. Although, Sahu and Anand [<xref ref-type="bibr" rid="CR17">17</xref>] tried to improve the performance of NER models with CNN based CLWE layer, they have failed to do so. In our experiments, however, our STM outperforms other baseline STMs, demonstrating the effectiveness of STM with CNN based CLWE layer.</p>
    </sec>
    <sec id="Sec16">
      <title>Performance of CollaboNet</title>
      <p>Comparing Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref>, CollaboNet achieves higher precision and F1 score than most STM models on all datasets. On average, CollaboNet has improved both precision and recall. CollaboNet also outperforms the multi-task model (MTM) from Wang et al. [<xref ref-type="bibr" rid="CR25">25</xref>] on 4 out of 6 datasets (Table <xref rid="Tab3" ref-type="table">3</xref>). While multi-task learning has improved performance in previous studies [<xref ref-type="bibr" rid="CR25">25</xref>], using CollaboNet, which consists of expert models trained for each entity type, could further improve biomedical named entity recognition performance.</p>
    </sec>
  </sec>
  <sec id="Sec17" sec-type="discussion">
    <title>Discussion</title>
    <p>Compared to baseline models, CollaboNet achieves higher performance on macro average (Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref>). The increase in precision is supportive when considering the practical use of the bioNER systems. In a number of biomedical text mining systems, important information tends to be repeated in a large size text corpus. Therefore, missing a few entities may not hinder the performance of an entire system, as this can be compensated elsewhere. However, incorrect information and the propagation of errors can effect the entire system.</p>
    <p>In Table <xref rid="Tab4" ref-type="table">4</xref>, we report the error types of our STM and CollaboNet. We define <italic>bio-entity</italic><italic>error</italic> as recognizing different types of biomedical entities as target entity types. For instance, recognizing ‘<italic>VHL</italic>’ as a gene when it was used as a disease in a sentence is a bio-entity error. Note that a bio-entity error could occur when an entity is a polysemous word (e.g. VHL), or comprised of multiple words (e.g. BRCA1 deficient), and thus correcting bio-entity errors requires contextual information or supervision of other entity type models. The error analysis was conducted on 4334 errors of our STM and 3966 errors of CollaboNet on 5 datasets (BC2GM, BC5CDR-chem, BC5CDR-disease, JNLPBA, NCBI). Error analysis was conducted on models which showed best performance in our experiments.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>The number of bio-entity type errors, the total number of errors, and the ratio of bio-entity errors to the total numbers of errors for each model prediction</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="3">Our STM</th><th align="left" colspan="3">CollaboNet</th><th align="left"/></tr><tr><th align="left">Dataset</th><th align="left">Bio Entity</th><th align="left">Total</th><th align="left">Ratio of Bio Entity</th><th align="left">Bio Entity</th><th align="left">Total</th><th align="left">Ratio of Bio Entity</th><th align="left">Difference</th></tr></thead><tbody><tr><td align="left">NCBI-disease</td><td align="left">54</td><td align="left">167</td><td align="left">32.3%</td><td align="left">38</td><td align="left">131</td><td align="left">29.0%</td><td align="left">-3.3%</td></tr><tr><td align="left">JNLPBA</td><td align="left">749</td><td align="left">1520</td><td align="left">49.3%</td><td align="left">227</td><td align="left">1437</td><td align="left">15.8%</td><td align="left"><bold>-33.5%</bold></td></tr><tr><td align="left">BC5CDR-chem</td><td align="left">142</td><td align="left">503</td><td align="left">28.2%</td><td align="left">122</td><td align="left">505</td><td align="left">24.2%</td><td align="left">-4.1%</td></tr><tr><td align="left">BC5CDR-disease</td><td align="left">199</td><td align="left">867</td><td align="left">23.0%</td><td align="left">131</td><td align="left">728</td><td align="left">18.0%</td><td align="left">-5.0%</td></tr><tr><td align="left">BC2GM</td><td align="left">189</td><td align="left">1277</td><td align="left">14.8%</td><td align="left">218</td><td align="left">1165</td><td align="left">18.7%</td><td align="left">3.9%</td></tr></tbody></table><table-wrap-foot><p>Negative values at the difference tab indicate that CollaboNet reduced the number of false positives, especially false biomedical entities</p></table-wrap-foot></table-wrap>
</p>
    <p>The error analysis of our STM, which is a single BiLSTM-CRF model, shows that the majority of errors are classified as bio-entity errors which comprise up to 49.3% of the total errors in JNLPBA. According to the error analysis of our STM model, bio-entity errors constitute 1333 errors out of 4334 errors, comprising 30.8% of all the errors. Although bio-entity error was not the most common error type, the importance of bio-entity error is much greater that of other errors such as span error which was the most common error type, constituting 38% of incorrect errors. While most span errors can be easily fixed by non-experts, bio-entity errors are difficult to detect and fix, even for biomedical researchers. Also, for biomedical text mining tasks such as drug-drug interaction (DDI) extraction, span errors of an NER system have a minor effect on DDI results but bio-entity errors could lead to completely different results.</p>
    <p>The performance improvement of CollaboNet over STM may not seem significant when considering the increased complexity of CollaboNet’s structure. We found by error analysis that CollaboNet had an increased number of span errors. As our metric is based on the exact match evaluation, consistent annotation of the ground truth dataset is important for reducing span errors which are caused by modifiers. For instance, in the phrase “acute adult renal failure,” “adult renal failure” may be labeled as an entity in some datasets. In this case, predicting “acute adult renal failure” or “renal failure” as an entity will be counted as a false negative and a false positive. On the other hand, some other datasets may include the modifier “acute” in an entity, considering “acute adult renal failure” as the only true prediction. Therefore, unlike STM, CollaboNet uses various datasets that have been annotated differently. Even though CollaboNet outperforms STM, its results may be lower due to this inconsistency in annotation.</p>
    <p>In CollaboNet, each expert model is trained on a single entity type dataset, and their training inputs are a concatenation of word embeddings and outputs of the other expert models. We expect that the other expert models will transfer knowledge on their respective entity to the target model, and thus improve the bio-entity type error problem by collaboration. As Table <xref rid="Tab4" ref-type="table">4</xref> shows, CollaboNet performs better than our STM in detecting polysemy and other entity types. Among 3966 errors from CollaboNet, 736 errors are bio-entity errors, comprising 18.6% of all the errors.</p>
    <sec id="Sec18">
      <title>Case study</title>
      <p>We sampled the predictions of CollaboNet and those of our STM (single-task model) to further understand the strengths of CollaboNet in Table <xref rid="Tab5" ref-type="table">5</xref>.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>Case study</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left"/><td align="left">Chemical dataset</td><td align="left"/></tr><tr><td align="left">Our STM</td><td align="left">No prophylaxis with <underline>antilymphocyte globulin</underline> was used</td><td align="left">- globulin : Protein</td></tr><tr><td align="left">CollaboNet</td><td align="left">No prophylaxis with antilymphocyte globulin was used</td><td align="left"/></tr><tr><td align="left">Ground Truth</td><td align="left">No prophylaxis with antilymphocyte globulin was used</td><td align="left"/></tr><tr><td align="left">Our STM</td><td align="left">elderly patients using <underline>ACE / ARB</underline> in combination with <underline>potassium</underline></td><td align="left">ACE : Gene/Protein</td></tr><tr><td align="left">CollaboNet</td><td align="left">elderly patients using ACE / ARB in combination with <underline>potassium</underline></td><td align="left"/></tr><tr><td align="left">Ground Truth</td><td align="left">elderly patients using ACE / ARB in combination with <underline>potassium</underline></td><td align="left"/></tr><tr><td align="left"/><td align="left">Disease Dataset</td><td align="left"/></tr><tr><td align="left">Our STM</td><td align="left">The ATM (<underline>A-T, mutated</underline>) gene on human chromosome 11q22.</td><td align="left">A-T, mutated : Gene</td></tr><tr><td align="left">CollaboNet</td><td align="left">The ATM (A-T, mutated) gene on human chromosome 11q22.</td><td align="left"/></tr><tr><td align="left">Ground Truth</td><td align="left">The ATM (A-T, mutated) gene on human chromosome 11q22.</td><td align="left"/></tr><tr><td align="left">Our STM</td><td align="left">to bind to the human <underline>cardiac troponin T</underline> (cTNT) pre-messenger RNA</td><td align="left">cTNT : Gene/Protein</td></tr><tr><td align="left">CollaboNet</td><td align="left">to bind to the human cardiac troponin T (cTNT) pre-messenger RNA</td><td align="left"/></tr><tr><td align="left">Ground Truth</td><td align="left">to bind to the human cardiac troponin T (cTNT) pre-messenger RNA</td><td align="left"/></tr><tr><td align="left"/><td align="left">Gene / Protein Dataset</td><td align="left"/></tr><tr><td align="left">Our STM</td><td align="left">which is inhibited by the <underline>cytotoxin leptomycin B (LMB)</underline>, and also by its interaction</td><td align="left">LMB : Chemical, Drug</td></tr><tr><td align="left">CollaboNet</td><td align="left">which is inhibited by the cytotoxin leptomycin B (LMB), and also by its interaction</td><td align="left"/></tr><tr><td align="left">Ground Truth</td><td align="left">which is inhibited by the cytotoxin leptomycin B (LMB), and also by its interaction</td><td align="left"/></tr><tr><td align="left">Our STM</td><td align="left">Classic Hodgkin disease (<underline>cHD</underline>) is derived from B cells with high loads of mutations.</td><td align="left">cHD : Disease</td></tr><tr><td align="left">CollaboNet</td><td align="left">Classic Hodgkin disease (cHD) is derived from B cells with high loads of mutations</td><td align="left"/></tr><tr><td align="left">Ground Truth</td><td align="left">Classic Hodgkin disease (cHD) is derived from B cells with high loads of mutations</td><td align="left"/></tr></tbody></table><table-wrap-foot><p>This table contains sentences that were incorrectly predicted by of our STM but were correctly predicted by CollaboNet. The predicted labels or the ground truth labels are underlined</p></table-wrap-foot></table-wrap>
</p>
      <p>The first example from chemical dataset in Table <xref rid="Tab5" ref-type="table">5</xref> shows our expected result from CollaboNet. Our STM annotates <italic>antilymphocyte globulin</italic> as a chemical entity. However, it is clear that the entity is not a chemical but a type of globulin which is a protein. The second example sentence from the chemical dataset is about an <italic>ACE / ARB</italic> entity. Again, our STM misidentifies the entity as a chemical entity. On the other hand, in CollaboNet, the target model (chemical model) obtains knowledge from one of the collaborator models (the gene/protein model) to avoid mistakenly recognizing the entity as a chemical entity. As <italic>globulin</italic> or <italic>ACE</italic> entities appear in the gene/protein dataset, the chemical model obtains information from the gene/protein model.</p>
      <p>In the disease dataset, the first example shows a multi-word entity in parentheses. As a gene model can pass syntactic and semantic information about a word <italic>e.g., mutated</italic> and its surrounding words to a disease model, CollaboNet can abstain from predicting <italic>A-T, mutated</italic> as the disease entity, which our STM model failed to do. The second example in the disease dataset is on <italic>cardiac troponin T</italic>. Since <italic>cardiac + noun</italic> in biomedical text can be easily considered as a disease name, our STM misidentified this word as a disease entity. However, with the help of a gene model, CollaboNet did not mark it as a disease entity.</p>
      <p>The gene/protein entity type further demonstrates the effectiveness of CollaboNet in reducing bio-entity type errors. Two example sentences contain abbreviations, which are one of the distinct characteristics of gene entities. <italic>LMB</italic> and <italic>cHD</italic> are incorrectly predicted as gene/protein entities by our STM, since lots of gene/protein entities are abbreviations. However, the target model (gene/protein model) in CollaboNet can obtain information on <italic>leptomycin</italic> and <italic>disease</italic> from the chemical and disease models, respectively. With the help of information from collaborator models, CollaboNet can effectively increase the precision of other entity type models.</p>
      <p>In addition, we found some labels in the ground truth set, which we believe are incorrect. Tsai et al. [<xref ref-type="bibr" rid="CR15">15</xref>] also reported that the inconsistent annotations in the JNLPBA corpus limit the NER system. We report our findings in Table <xref rid="Tab6" ref-type="table">6</xref>.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Case study</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left"/><td align="left">Gene / Protein Dataset</td></tr><tr><td align="left">CollaboNet</td><td align="left">Troglitazone, a <underline>PPARgamma ligand</underline>, inhibits <underline>osteopontin gene</underline> expression in THP-1 cells.</td></tr><tr><td align="left">Ground Truth</td><td align="left">Troglitazone, a PPARgamma ligand, inhibits osteopontin gene expression in THP-1 cells</td></tr><tr><td align="left">CollaboNet</td><td align="left">The <underline>translesion DNA polymerase zeta</underline> plays a major role in <underline>lg</underline> and <underline>bcl-6</underline> somatic hypermutation.</td></tr><tr><td align="left">Ground Truth</td><td align="left">The <underline>translesion DNA polymerase zeta</underline> plays a major role in lg and bcl-6 somatic hypermutation.</td></tr><tr><td align="left"/><td align="left">Chemical Dataset</td></tr><tr><td align="left">CollaboNet</td><td align="left">recently identified Delta22-isomer of <underline>beta-muricholate</underline> contribute for 5.4%</td></tr><tr><td align="left">Ground Truth</td><td align="left">recently identified Delta22-isomer of beta-muricholate contribute for 5.4%</td></tr><tr><td align="left">CollaboNet</td><td align="left"><underline>Hexabrix and polyvidone</underline> are considered the best contrast media for hysterosalpingography.</td></tr><tr><td align="left">Ground Truth</td><td align="left"><underline>Hexabrix and polyvidone</underline> are considered the best <underline>contrast media</underline> for hysterosalpingography.</td></tr></tbody></table><table-wrap-foot><p>This table shows the questionable answers from the ground truth datasets. Our model achieves better performance in detecting entities in these example sentences. The predicted labels or the ground truth labels are underlined</p></table-wrap-foot></table-wrap>
</p>
      <p>In the first row of Table <xref rid="Tab6" ref-type="table">6</xref>, the gene/protein entity <italic>osteopontin</italic> was not marked in the ground truth labels, whereas our network correctly predicted it as a gene entity. The second row also displays questionable results of the ground truth labels. Although <italic>lg</italic> and <italic>bcl-6</italic>, which are abbreviations of <italic>Immunoglobulin</italic> and <italic>B-cell lymphoma 6</italic>, where not labeled in the ground truth labels, our model detected them as a gene / protein entity. The example sentences of gene/protein annotations in Table <xref rid="Tab6" ref-type="table">6</xref> were reviewed by several domain experts and medical doctors. As shown in the third row, <italic>beta-muricholate</italic> is a chemical entity but it was not annotated in the ground truth labels. However, the last row shows another type of annotation error. <italic>Contrast media</italic> is a general term for a medium used in medical imaging and since is not a proper noun, it is not a named entity.</p>
      <p>These examples shows the presence of incorrect ground truth labels, which can harm the performance of bioNER models. However, we believe that these missed or misidentified ground truth labels can be corrected by our system.</p>
    </sec>
    <sec id="Sec19">
      <title>Future works</title>
      <p>For future work, we plan to cover more target entity types and use more datasets. For example, CRAFT [<xref ref-type="bibr" rid="CR42">42</xref>], LINNAEUS [<xref ref-type="bibr" rid="CR43">43</xref>] and Variome [<xref ref-type="bibr" rid="CR44">44</xref>] are manually annotated datasets and are valuable resources that can be used for expanding our model. Second, we plan to apply CollaboNet to downstream biomedical text mining systems. For example, entity search engines such as BEST [<xref ref-type="bibr" rid="CR10">10</xref>] could be improved by using more accurate NER models.</p>
    </sec>
  </sec>
  <sec id="Sec20" sec-type="conclusion">
    <title>Conclusion</title>
    <p>In this paper, we introduced CollaboNet, which consists of multiple BiLSTM-CRF models, for biomedical named entity recognition. While existing models were only able to handle datasets with a single entity type, CollaboNet leverages multiple datasets and achieves the highest F1 scores. Unlike recently proposed multi-task models, CollaboNet is built upon multiple single-task NER models (STMs) that send information to each other for more accurate predictions. In addition to the performance improvement over multi-task models, CollaboNet differentiates between biomedical entities that are polysemous or have similar orthographic features. As a result, our model achieved state-of-the-art performance on four bioNER datasets in terms of F1 score, precision and recall. Although our model requires a large amount of memory and time, which existing multi-task models require as well, the simple structure of CollaboNet allows researchers to build another expert model for different entity types in CollaboNet. As CollaboNet obtains higher precision than other models, we plan to apply CollaboNet in a biomedical text mining system.</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>BiLSTM</term>
        <def>
          <p>Bidirectional long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>BioNER</term>
        <def>
          <p>Biomedical named entity recognition</p>
        </def>
      </def-item>
      <def-item>
        <term>CE</term>
        <def>
          <p>Character embedding</p>
        </def>
      </def-item>
      <def-item>
        <term>CLWE</term>
        <def>
          <p>Character level word embedding</p>
        </def>
      </def-item>
      <def-item>
        <term>CNN</term>
        <def>
          <p>convolution neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>CRF</term>
        <def>
          <p>Conditional random field</p>
        </def>
      </def-item>
      <def-item>
        <term>DDI</term>
        <def>
          <p>Drug-Drug Interaction</p>
        </def>
      </def-item>
      <def-item>
        <term>LSTM</term>
        <def>
          <p>long short-term memory</p>
        </def>
      </def-item>
      <def-item>
        <term>MTL</term>
        <def>
          <p>Multi-task learning</p>
        </def>
      </def-item>
      <def-item>
        <term>MTM</term>
        <def>
          <p>Multi-task model</p>
        </def>
      </def-item>
      <def-item>
        <term>NER</term>
        <def>
          <p>Named entity recognition</p>
        </def>
      </def-item>
      <def-item>
        <term>NLP</term>
        <def>
          <p>Natural language processing</p>
        </def>
      </def-item>
      <def-item>
        <term>PMC</term>
        <def>
          <p>PubMed Central</p>
        </def>
      </def-item>
      <def-item>
        <term>STM</term>
        <def>
          <p>Single-task model</p>
        </def>
      </def-item>
      <def-item>
        <term>RNN</term>
        <def>
          <p>Recurrent neural network</p>
        </def>
      </def-item>
      <def-item>
        <term>WE</term>
        <def>
          <p>Word embedding</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ack>
    <title>Acknowledgements</title>
    <p>We are sincerely grateful to Inah Chang for conducting manual error counting. We appreciate Susan Kim for editing the manuscript.</p>
    <sec id="d29e4534">
      <title>Funding</title>
      <p>The design of the study and collection, analysis, and interpretation of data were funded by the National Research Foundation of Korea (NRF-2017M3C4A7065887, 2016M3A9A7916996) and National IT Industry Promotion Agency grant funded by the Ministry of Science and ICT and Ministry of Health and Welfare (NO. C1202-18-1001, Development Project of The Precision Medicine Hospital Information System (P-HIS)). Publication costs were funded by the National Research Foundation of Korea (NRF-2016M3A9A7916996).</p>
    </sec>
    <sec id="d29e4539" sec-type="data-availability">
      <title>Availability of data and materials</title>
      <p>The source code of CollaboNet and the datasets are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/wonjininfo/CollaboNet">https://github.com/wonjininfo/CollaboNet</ext-link>.</p>
    </sec>
    <sec id="d29e4549">
      <title>About this supplement</title>
      <p>This article has been published as part of <italic>BMC Bioinformatics Volume 20 Supplement 10, 2019: Proceedings of the 12th International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO 2018)</italic>. The full contents of the supplement are available online at <ext-link ext-link-type="uri" xlink:href="https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-10">https://bmcbioinformatics.biomedcentral.com/articles/supplements/volume-20-supplement-10</ext-link>.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>WY, CHS, JL and JK conceived the idea. WY and JL designed the model. WY and CHS developed CollaboNet. CHS experimented and collected analysis examples and results. WY, JL and JK wrote the manuscript. JK, as the supervisor of WY, CHS and JL, provided guidance on the experiment. All authors read and approved the final manuscript.</p>
  </notes>
  <notes>
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <notes>
    <title>Publisher’s Note</title>
    <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <mixed-citation publication-type="other">Home-PMC-NCBI. <ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pmc/">https://www.ncbi.nlm.nih.gov/pmc/</ext-link>. Accessed 01 Apr 2018.</mixed-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Hersh</surname>
            <given-names>WR</given-names>
          </name>
        </person-group>
        <article-title>A survey of current work in biomedical text mining</article-title>
        <source>Brief Bioinform</source>
        <year>2005</year>
        <volume>6</volume>
        <issue>1</issue>
        <fpage>57</fpage>
        <lpage>71</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/6.1.57</pub-id>
        <pub-id pub-id-type="pmid">15826357</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miwa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Sætre</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Miyao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Tsujii</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Protein–protein interaction extraction by leveraging multiple kernels and parsers</article-title>
        <source>Int J Med Inform</source>
        <year>2009</year>
        <volume>78</volume>
        <issue>12</issue>
        <fpage>39</fpage>
        <lpage>46</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ijmedinf.2009.04.010</pub-id>
        <pub-id pub-id-type="pmid">18723389</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zheng</surname>
            <given-names>JG</given-names>
          </name>
          <name>
            <surname>Howsmon</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Hahn</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>McGuinness</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hendler</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Entity linking for biomedical literature</article-title>
        <source>Proceedings of the ACM 8th International Workshop on Data and Text Mining in Bioinformatics</source>
        <year>2014</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Tsutsui</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Meng</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Machine reading approach to understand alzheimers disease literature</article-title>
        <source>Proceedings of the Tenth International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO)</source>
        <year>2016</year>
        <publisher-loc>London</publisher-loc>
        <publisher-name>BioMed Central</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Preiss</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Stevenson</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The effect of word sense disambiguation accuracy on literature based discovery</article-title>
        <source>Proceedings of the ACM Ninth International Workshop on Data and Text Mining in Biomedical Informatics. DTMBIO ’15</source>
        <year>2015</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>ACM</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <mixed-citation publication-type="other">Heo GE, Kang KY, Song M. Examining the field of bioinformatics by the multi-faceted informetric approach. In: Proceedings of the ACM 10th International Workshop on Data and Text Mining in Bioinformatics.2016.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Drug drug interaction extraction from the literature using a recursive neural network</article-title>
        <source>PloS ONE</source>
        <year>2018</year>
        <volume>13</volume>
        <issue>1</issue>
        <fpage>0190926</fpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Rosario</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Hearst</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Classifying semantic relations in bioscience texts</article-title>
        <source>Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</source>
        <year>2004</year>
        <publisher-loc>Stroudsburg</publisher-loc>
        <publisher-name>Association for Computational Linguistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Jeon</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>A-C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Best: next-generation biomedical entity search tool for knowledge discovery from biomedical literature</article-title>
        <source>PloS ONE</source>
        <year>2016</year>
        <volume>11</volume>
        <issue>10</issue>
        <fpage>0164680</fpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hettne</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Stierum</surname>
            <given-names>RH</given-names>
          </name>
          <name>
            <surname>Schuemie</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Hendriksen</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Schijvenaars</surname>
            <given-names>BJ</given-names>
          </name>
          <name>
            <surname>Mulligen</surname>
            <given-names>E. M. v.</given-names>
          </name>
          <name>
            <surname>Kleinjans</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kors</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>A dictionary to identify small molecules and drugs in free text</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>25</volume>
        <issue>22</issue>
        <fpage>2983</fpage>
        <lpage>91</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp535</pub-id>
        <pub-id pub-id-type="pmid">19759196</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Han</surname>
            <given-names>W-S</given-names>
          </name>
        </person-group>
        <article-title>Developing a hybrid dictionary-based bio-entity recognition technique</article-title>
        <source>BMC Med Inform Decis Mak</source>
        <year>2015</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>9</fpage>
        <pub-id pub-id-type="doi">10.1186/1472-6947-15-S1-S9</pub-id>
        <pub-id pub-id-type="pmid">25889930</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <mixed-citation publication-type="other">Fukuda K-I, Tsunoda T, Tamura A, Takagi T, et al. Toward information extraction: identifying protein names from biological papers. In: Pac Symp Biocomput: 1998. p. 707–18.</mixed-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Proux</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Rechenmann</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Julliard</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Pillet</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Jacq</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Detecting gene symbols and names in biological texts</article-title>
        <source>Genome Inform</source>
        <year>1998</year>
        <volume>9</volume>
        <fpage>72</fpage>
        <lpage>80</lpage>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Tsai</surname>
            <given-names>RT-H</given-names>
          </name>
          <name>
            <surname>Sung</surname>
            <given-names>C-L</given-names>
          </name>
          <name>
            <surname>Dai</surname>
            <given-names>H-J</given-names>
          </name>
          <name>
            <surname>Hung</surname>
            <given-names>H-C</given-names>
          </name>
          <name>
            <surname>Sung</surname>
            <given-names>T-Y</given-names>
          </name>
          <name>
            <surname>Hsu</surname>
            <given-names>W-L</given-names>
          </name>
        </person-group>
        <article-title>Nerbio: using selected word conjunctions, term normalization, and global patterns to improve biomedical named entity recognition</article-title>
        <source>BMC Bioinformatics</source>
        <year>2006</year>
        <publisher-loc>London</publisher-loc>
        <publisher-name>BioMed Central</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ju</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Miwa</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ananiadou</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>A neural layered model for nested named entity recognition</article-title>
        <source>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</source>
        <year>2018</year>
        <publisher-loc>New Orleans</publisher-loc>
        <publisher-name>Association for Computational Linguistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Sahu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Anand</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Recurrent neural network models for disease name recognition using domain invariant features</article-title>
        <source>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</source>
        <year>2016</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Association for Computational Linguistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Habibi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Weber</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Neves</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wiegandt</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Leser</surname>
            <given-names>U</given-names>
          </name>
        </person-group>
        <article-title>Deep learning with word embeddings improves biomedical named entity recognition</article-title>
        <source>Bioinformatics</source>
        <year>2017</year>
        <volume>33</volume>
        <issue>14</issue>
        <fpage>37</fpage>
        <lpage>48</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btx228</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lample</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ballesteros</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Subramanian</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kawakami</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Dyer</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Neural architectures for named entity recognition</article-title>
        <source>HLT-NAACL</source>
        <year>2016</year>
        <publisher-loc>San Diego</publisher-loc>
        <publisher-name>The Association for Computational Linguistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Pyysalo S, Ginter F, Moen H, Salakoski T, Ananiadou S. Distributional semantics resources for biomedical text processing. In: Proceedings of the 5th International Symposium on Languages in Biology and Medicine, Tokyo, Japan: 2013. p. 39–43.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doğan</surname>
            <given-names>RI</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Ncbi disease corpus: a resource for disease name recognition and concept normalization</article-title>
        <source>J Biomed Inform</source>
        <year>2014</year>
        <volume>47</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1016/j.jbi.2013.12.006</pub-id>
        <pub-id pub-id-type="pmid">24393765</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>J-D</given-names>
          </name>
          <name>
            <surname>Ohta</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tsuruoka</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Tateisi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Collier</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Introduction to the bio-entity recognition task vat jnlpba</article-title>
        <source>Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications</source>
        <year>2004</year>
        <publisher-loc>Geneva</publisher-loc>
        <publisher-name>Association for Computational Linguistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caruana</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Multitask learning</article-title>
        <source>Mach Learn</source>
        <year>1997</year>
        <volume>28</volume>
        <issue>1</issue>
        <fpage>41</fpage>
        <lpage>75</lpage>
        <pub-id pub-id-type="doi">10.1023/A:1007379606734</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Crichton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Pyysalo</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chiu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Korhonen</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A neural network multi-task learning approach to biomedical named entity recognition</article-title>
        <source>BMC Bioinformatics</source>
        <year>2017</year>
        <volume>18</volume>
        <issue>1</issue>
        <fpage>368</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1776-8</pub-id>
        <pub-id pub-id-type="pmid">28810903</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <mixed-citation publication-type="other">Wang X, Zhang Y, Ren X, Zhang Y, Zitnik M, Shang J, Langlotz C, Han J. Cross-type biomedical named entity recognition with deep multi-task learning. Bioinformatics. 2018. ISSN = 1367-4803, <pub-id pub-id-type="doi">10.1093/bioinformatics/bty869</pub-id>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty869</pub-id>.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Ratinov</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Roth</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Design challenges and misconceptions in named entity recognition</article-title>
        <source>Proceedings of the Thirteenth Conference on Computational Natural Language Learning</source>
        <year>2009</year>
        <publisher-loc>Stroudsburg</publisher-loc>
        <publisher-name>Association for Computational Linguistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <mixed-citation publication-type="other">Santos CD, Zadrozny B. Learning character-level representations for part-of-speech tagging. In: Proceedings of the 31st International Conference on Machine Learning (ICML-14). JMLR.org: 2014. p. 1818–26.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <mixed-citation publication-type="other">Kim Y, Jernite Y, Sontag D, Rush AM. Character-aware neural language models. In: AAAI. Phoenix:AAAI Press: 2016. p. 2741–9.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Graves</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mohamed</surname>
            <given-names>A-R</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Speech recognition with deep recurrent neural networks</article-title>
        <source>Acoustics, Speech and Signal Processing (icassp), 2013 Ieee International Conference On</source>
        <year>2013</year>
        <publisher-loc>Vancouver</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <mixed-citation publication-type="other">Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. In: 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings: 2015. http://arxiv.org/abs/1409.0473.</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hochreiter</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schmidhuber</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Long short-term memory</article-title>
        <source>Neural Comput</source>
        <year>1997</year>
        <volume>9</volume>
        <issue>8</issue>
        <fpage>1735</fpage>
        <lpage>80</lpage>
        <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        <pub-id pub-id-type="pmid">9377276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schuster</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Paliwal</surname>
            <given-names>KK</given-names>
          </name>
        </person-group>
        <article-title>Bidirectional recurrent neural networks</article-title>
        <source>IEEE Trans Signal Process</source>
        <year>1997</year>
        <volume>45</volume>
        <issue>11</issue>
        <fpage>2673</fpage>
        <lpage>81</lpage>
        <pub-id pub-id-type="doi">10.1109/78.650093</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Smith</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Tanabe</surname>
            <given-names>LK</given-names>
          </name>
          <name>
            <surname>nee Ando</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Kuo</surname>
            <given-names>C-J</given-names>
          </name>
          <name>
            <surname>Chung</surname>
            <given-names>I-F</given-names>
          </name>
          <name>
            <surname>Hsu</surname>
            <given-names>C-N</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Y-S</given-names>
          </name>
          <name>
            <surname>Klinger</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Friedrich</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Ganchev</surname>
            <given-names>K</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Overview of biocreative ii gene mention recognition</article-title>
        <source>Genome Biol</source>
        <year>2008</year>
        <volume>9</volume>
        <issue>2</issue>
        <fpage>2</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2008-9-s2-s2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Krallinger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rabal</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Leitner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Vazquez</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Salgado</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lowe</surname>
            <given-names>DM</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The chemdner corpus of chemicals and drugs and its annotation principles</article-title>
        <source>J Cheminformatics</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>S1</issue>
        <fpage>2</fpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S2</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <mixed-citation publication-type="other">Wei C-H, Peng Y, Leaman R, Davis AP, Mattingly CJ, Li J, Wiegers TC, Lu Z. Overview of the biocreative v chemical disease relation (cdr) task. In: Proceedings of the Fifth BioCreative Challenge Evaluation Workshop: 2015. p. 154–66.</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <mixed-citation publication-type="other">Li J, Sun Y, Johnson R, Sciaky D, Wei C-H, Leaman R, Davis AP, Mattingly CJ, Wiegers TC, Lu Z. Annotating chemicals, diseases, and their interactions in biomedical literature. In: Proceedings of the Fifth BioCreative Challenge Evaluation Workshop: 2015. p. 173–82.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Islamaj Doğan</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Dnorm: disease name normalization with pairwise learning to rank</article-title>
        <source>Bioinformatics</source>
        <year>2013</year>
        <volume>29</volume>
        <issue>22</issue>
        <fpage>2909</fpage>
        <lpage>17</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btt474</pub-id>
        <pub-id pub-id-type="pmid">23969135</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Leaman</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>C-H</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>tmchem: a high performance approach for chemical named entity recognition and normalization</article-title>
        <source>J Cheminformatics</source>
        <year>2015</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>3</fpage>
        <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S3</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <mixed-citation publication-type="other">Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. 2013.</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Duchi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hazan</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Singer</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Adaptive subgradient methods for online learning and stochastic optimization</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <issue>Jul</issue>
        <fpage>2121</fpage>
        <lpage>59</lpage>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <mixed-citation publication-type="other">Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov RR. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580. 2012.</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>KB</given-names>
          </name>
          <name>
            <surname>Lanfranchi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Choi</surname>
            <given-names>M. J. -y.</given-names>
          </name>
          <name>
            <surname>Bada</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Baumgartner</surname>
            <given-names>WA</given-names>
          </name>
          <name>
            <surname>Panteleyeva</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Verspoor</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Palmer</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hunter</surname>
            <given-names>LE</given-names>
          </name>
        </person-group>
        <article-title>Coreference annotation and resolution in the colorado richly annotated full text (craft) corpus of biomedical journal articles</article-title>
        <source>BMC Bioinformatics</source>
        <year>2017</year>
        <volume>18</volume>
        <issue>1</issue>
        <fpage>372</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1775-9</pub-id>
        <pub-id pub-id-type="pmid">28818042</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gerner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nenadic</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bergman</surname>
            <given-names>CM</given-names>
          </name>
        </person-group>
        <article-title>Linnaeus: A species name identification system for biomedical literature</article-title>
        <source>BMC Bioinformatics</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>1</issue>
        <fpage>85</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-11-85</pub-id>
        <pub-id pub-id-type="pmid">20149233</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Verspoor</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Jimeno Yepes</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cavedon</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>McIntosh</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Herten-Crabb</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Thomas</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Plazzer</surname>
            <given-names>J-P</given-names>
          </name>
        </person-group>
        <article-title>Annotating the biomedical literature for the human variome</article-title>
        <source>Database</source>
        <year>2013</year>
        <volume>2013</volume>
        <fpage>019</fpage>
        <pub-id pub-id-type="doi">10.1093/database/bat019</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
