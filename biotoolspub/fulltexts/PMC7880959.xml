<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7880959</article-id>
    <article-id pub-id-type="pmid">32710238</article-id>
    <article-id pub-id-type="publisher-id">1428</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-020-01428-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>REMoDNaV: robust eye-movement classification for dynamic stimulation</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Dar</surname>
          <given-names>Asim H.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Wagner</surname>
          <given-names>Adina S.</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6398-6370</contrib-id>
        <name>
          <surname>Hanke</surname>
          <given-names>Michael</given-names>
        </name>
        <address>
          <email>michael.hanke@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.418723.b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2109 6265</institution-id><institution>Special Lab Non-Invasive Brain Imaging, </institution><institution>Leibniz Institute for Neurobiology, </institution></institution-wrap>Brenneckestraße 6, Magdeburg, Germany </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.8385.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2297 375X</institution-id><institution>Psychoinformatics Lab, Institute of Neuroscience and Medicine (INM-7: Brain and Behaviour), </institution><institution>Research Centre Jülich, </institution></institution-wrap>Jülich, Germany </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.411327.2</institution-id><institution-id institution-id-type="ISNI">0000 0001 2176 9917</institution-id><institution>Institute of Systems Neuroscience, Medical Faculty, </institution><institution>Heinrich Heine University Düsseldorf, </institution></institution-wrap>Düsseldorf, Germany </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>24</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>24</day>
      <month>7</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2021</year>
    </pub-date>
    <volume>53</volume>
    <issue>1</issue>
    <fpage>399</fpage>
    <lpage>414</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Tracking of eye movements is an established measurement for many types of experimental paradigms. More complex and more prolonged visual stimuli have made algorithmic approaches to eye-movement event classification the most pragmatic option. A recent analysis revealed that many current algorithms are lackluster when it comes to data from viewing dynamic stimuli such as video sequences. Here we present an event classification algorithm—built on an existing velocity-based approach—that is suitable for both static and dynamic stimulation, and is capable of classifying saccades, post-saccadic oscillations, fixations, and smooth pursuit events. We validated classification performance and robustness on three public datasets: 1) manually annotated, trial-based gaze trajectories for viewing static images, moving dots, and short video sequences, 2) lab-quality gaze recordings for a feature-length movie, and 3) gaze recordings acquired under suboptimal lighting conditions inside the bore of a magnetic resonance imaging (MRI) scanner for the same full-length movie. We found that the proposed algorithm performs on par or better compared to state-of-the-art alternatives for static stimulation. Moreover, it yields eye-movement events with biologically plausible characteristics on prolonged dynamic recordings. Lastly, algorithm performance is robust on data acquired under suboptimal conditions that exhibit a temporally varying noise level. These results indicate that the proposed algorithm is a robust tool with improved classification accuracy across a range of use cases. The algorithm is cross-platform compatible, implemented using the Python programming language, and readily available as free and open-source software from public sources.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Eye tracking</kwd>
      <kwd>Adaptive classification algorithm</kwd>
      <kwd>Saccade classification algorithm</kwd>
      <kwd>Statistical saccade analysis</kwd>
      <kwd>Glissade classification</kwd>
      <kwd>Adaptive threshold algorithm</kwd>
      <kwd>Data preprocessing</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Psychonomic Society, Inc. 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">A spreading theme in cognitive neuroscience is to use dynamic and naturalistic stimuli such as video clips or movies as opposed to static and isolated stimuli (Matusz et al., <xref ref-type="bibr" rid="CR35">2019</xref>). Using dynamic stimuli promises to observe the nuances of cognition in a more life-like environment (Maguire, <xref ref-type="bibr" rid="CR33">2012</xref>). Some interesting applications include the determination of neural response to changes in facial expression (Harris et al., <xref ref-type="bibr" rid="CR18">2014</xref>), understanding complex social interactions by using videos (Tikka et al., <xref ref-type="bibr" rid="CR46">2012</xref>), and more untouched themes such as the underlying processing of music (Toiviainen et al., <xref ref-type="bibr" rid="CR47">2014</xref>). In such studies, an unobtrusive behavioral measurement is required to quantify the relationship between stimulus and response. Tracking the focus of participants’ gaze is a suitable, well-established method that has been successfully employed in a variety of studies ranging from the understanding of visual attention (Liu &amp; Heynderickx, <xref ref-type="bibr" rid="CR32">2011</xref>), memory (Hannula et al., <xref ref-type="bibr" rid="CR17">2010</xref>), and language comprehension (Gordon et al., <xref ref-type="bibr" rid="CR12">2006</xref>). Regardless of use case, the raw eye-tracking data (gaze position coordinates) provided by eye-tracking devices are rarely used “as is”. Instead, in order to disentangle different cognitive, oculomotor, or perceptive states associated with different types of eye movements, most research relies on the classification of eye-gaze data into distinct eye-movement event categories (Schutz et al., <xref ref-type="bibr" rid="CR41">2011</xref>). The most feasible approach for doing this lies in the application of appropriate event classification algorithms.</p>
    <p id="Par3">However, a recent comparison of algorithms found that while many readily available algorithms for eye-movement classification performed well on data from static stimulation or short trial-based acquisitions with simplified moving stimuli, none worked particularly well on data from complex dynamic stimulation, such as video clips, when compared to human coders (Andersson et al., <xref ref-type="bibr" rid="CR2">2017</xref>). And indeed, when we evaluated an algorithm by Nyström and Holmqvist (<xref ref-type="bibr" rid="CR37">2010</xref>), one of the winners in the aforementioned comparison, on data from prolonged stimulation (≈ 15 min) with a feature film, we found the average and median durations of labeled fixations to exceed literature reports (e.g., Holmqvist et al., <xref ref-type="bibr" rid="CR21">2011</xref>; Dorr et al., <xref ref-type="bibr" rid="CR9">2010</xref>) by up to a factor of two. Additionally, and in particular for increasing levels of noise in the data, the algorithm classified too few fixations, as also noted by Friedman et al., (<xref ref-type="bibr" rid="CR10">2018</xref>), because it discarded potential fixation events that contained data artifacts such as signal loss and distortion associated with blinks.</p>
    <p id="Par4">Therefore, our objective was to improve upon the available eye-movement classification algorithms, and develop a tool that performs robustly on data from dynamic, feature-rich stimulation, without sacrificing classification accuracy for static and simplified stimulation. Importantly, we aimed for applicability to prolonged recordings that potentially exhibit periods of signal loss and non-stationary noise levels. Finally, one of our main objectives was to keep the algorithm as accessible and easily available as possible in order to ease the difficulties associated with closed-source software or non-publicly available source code of published algorithms.</p>
    <p id="Par5">Following the best practices proposed by Hessels et al., (<xref ref-type="bibr" rid="CR20">2018</xref>), we define the different eye movements that are supported by our algorithm on a functional and oculomotor dimension as follows: A <italic>fixation</italic> is a period of time during which a part of the visual stimulus is looked at and thereby projected to a relatively constant location on the retina. This type of eye movement is necessary for visual intake, and characterized by a relatively still gaze position with respect to the world (e.g., a computer screen used for stimulus presentation) in the eye-tracker signal. A fixation event therefore excludes periods of <italic>smooth pursuit</italic>. These events are eye movements during which a part of the visual stimulus that moves with respect to the world is looked at for visual intake (e.g., a moving dot on a computer screen). Like fixations, the stimulus is projected to a relatively constant location on the retina (Carl and Gellman, <xref ref-type="bibr" rid="CR4">1987</xref>), however, the event is characterized by steadily changing gaze position in the eye-tracker signal. If this type of eye movement is not properly classified, erroneous fixation and saccade events (which smooth pursuits would be classified into instead) are introduced (Andersson et al., <xref ref-type="bibr" rid="CR2">2017</xref>). Contemporary algorithms rarely provide this functionality (but see e.g., Larsson et al., <xref ref-type="bibr" rid="CR31">2015</xref>; Komogortsev &amp; Karpov, <xref ref-type="bibr" rid="CR28">2013</xref>, for existing algorithms with smooth pursuit classification). <italic>Saccades</italic> on the other hand are also characterized by changing gaze positions, but their velocity trace is usually higher than that of pursuit movements. They serve to shift the position of the eye to a target region, and, unlike during pursuit or fixation events, visual intake is suppressed (Schutz et al., <xref ref-type="bibr" rid="CR41">2011</xref>). Lastly, <italic>post-saccadic oscillations</italic> are periods of ocular instability after a saccade (Nyström &amp; Holmqvist, <xref ref-type="bibr" rid="CR37">2010</xref>).</p>
    <p id="Par6">Here we introduce REMoDNaV (robust eye-movement classification for dynamic stimulation), a new tool that aims to meet our objectives and classifies the eye-movement events defined above. It is built on the aforementioned algorithm by Nyström and Holmqvist (<xref ref-type="bibr" rid="CR37">2010</xref>) (subsequently labeled NH) that employs an adaptive approach to velocity-based eye-movement event classification. REMoDNaV enhances NH with the use of robust statistics, and a compartmentalization of prolonged time series into short, more homogeneous segments with more uniform noise levels. Furthermore, it adds support for pursuit event classification. Just as the original algorithm, its frame of reference is world centered, i.e., the gaze coordinates have a reference to a stimulation setup with a fixed position in the world such as <italic>x</italic> and <italic>y</italic> coordinates in pixel of a computer screen, and it is meant to be used with eye-tracking data from participants viewing static (e.g., images) or dynamic (e.g., videos) stimuli, recorded with remote or tower-mounted eye trackers. Importantly, it is built and distributed as free, open-source software, and can be easily obtained and executed with free tools. We evaluated REMoDNaV on three different datasets from conventional paradigms, and dynamic, feature-rich stimulation (high and lower quality), and relate its performance to the algorithm comparison by Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>).</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <p id="Par7">Event classification algorithms can be broadly grouped into <italic>velocity-</italic> and <italic>dispersion-</italic> based algorithms. The former rely on velocity thresholds to differentiate between different eye-movement events, while the latter classify eye movements based on the size of the region the recorded data falls into for a given amount of time (Holmqvist et al., <xref ref-type="bibr" rid="CR21">2011</xref>). Both types of algorithms are common (see e.g., Hessels et al., (<xref ref-type="bibr" rid="CR19">2017</xref>) for a recent dispersion-based, and e.g., van Renswoude et al., (<xref ref-type="bibr" rid="CR39">2018</xref>) for a recent velocity-based solution, and see Dalveren and Cagiltay (<xref ref-type="bibr" rid="CR8">2019</xref>) for an evaluation of common algorithms of both types). Like NH, REMoDNaV is a <italic>velocity-based</italic> event classification algorithm. The algorithm comprises two major steps: preprocessing and event classification. The following sections detail individual analysis steps. For each step, relevant algorithm parameters are given in parenthesis. Figure <xref rid="Fig1" ref-type="fig">1</xref> provides an overview of the algorithm’s main components. Table <xref rid="Tab1" ref-type="table">1</xref> summarizes all parameters, and lists their default values. The computational definitions of the different eye movements (Hessels et al., <xref ref-type="bibr" rid="CR20">2018</xref>) are given within the event classification description. Note, however, that some of the computational definitions of eye movements can be adjusted to comply to alternative definitions by changing the algorithms’ parameters.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Schematic algorithm overview. <bold>a</bold> Preprocessing. The two plots show raw (<italic>blue</italic>) and processed (<italic>black</italic>) time series after preprocessing with the default parameter values (see Table <xref rid="Tab1" ref-type="table">1</xref> for details). <bold>b</bold> Adaptive saccade velocity computation and time series chunking. Starting from an initial velocity threshold (velthresh_startvelocity), a global velocity threshold is iteratively determined. The time series is chunked into intervals between the fastest saccades across the complete recording. <bold>c</bold> Saccade and PSO classification. Saccade on- and offsets, and PSO on- and offsets are classified based on adaptive velocity thresholds computed within the respective event contexts. The default context is either 1 s centered on the peak velocity for saccadic events used for time series chunking, or the entire time series chunk for intersaccadic intervals. PSOs are classified into low- or high-velocity PSOs depending on whether they exceed the saccade onset- or peak-velocity threshold. <bold>d</bold> Fixation and pursuit classification. Remaining unlabeled segments are filtered with a low-pass Butterworth filter. Samples exceeding a configurable pursuit velocity threshold (pursuit_velthresh) are classified as pursuits, and segments that do not qualify as pursuits are classified as fixations</p></caption><graphic xlink:href="13428_2020_1428_Fig1_HTML" id="MO1"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Exhaustive list of algorithm parameters, their default values, and units</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Name</th><th align="left">Description</th><th align="left">Value</th></tr></thead><tbody><tr><td align="left" colspan="3">Preprocessing (in order of application during processing)</td></tr><tr><td align="left">px2deg</td><td align="left">size of a single (square) pixel</td><td align="left">no default [deg]</td></tr><tr><td align="left">sampling_rate</td><td align="left">temporal data sampling rate/frequency</td><td align="left">no default [Hz]</td></tr><tr><td align="left">min_blink_duration</td><td align="left">missing data windows shorter than this duration will not be considered for dilate_nan</td><td align="left">0.02 s</td></tr><tr><td align="left">dilate_nan</td><td align="left">duration for which to replace data by missing data markers on either side of a signal-loss window (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, P2)</td><td align="left">0.01 s</td></tr><tr><td align="left">median_filter_length</td><td align="left">smoothing median-filter size (for initial data chunking only) (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, P3)</td><td align="left">0.05 s</td></tr><tr><td align="left">savgol_length</td><td align="left">size of Savitzky–Golay filter for noise reduction (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, P3)</td><td align="left">0.019 s</td></tr><tr><td align="left">savgol_polyord</td><td align="left">polynomial order of Savitzky–Golay filter for noise reduction (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, P3)</td><td align="left">2</td></tr><tr><td align="left">max_vel</td><td align="left">maximum velocity threshold, will replace value with maximum, and issue warning if exceeded to inform about potentially inappropriate filter settings (default value based on Holmqvist et al., <xref ref-type="bibr" rid="CR21">2011</xref>)</td><td align="left">1000 deg/s</td></tr><tr><td align="left" colspan="3">Event classification</td></tr><tr><td align="left">min_saccade_duration</td><td align="left">minimum duration of a saccade event candidate (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E3)</td><td align="left">0.01 s</td></tr><tr><td align="left">max_pso_duration</td><td align="left">maximum duration of a post-saccadic oscillation (glissade) (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E3)</td><td align="left">0.04 s</td></tr><tr><td align="left">min_fixation_duration</td><td align="left">minimum duration of a fixation event candidate (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E4)</td><td align="left">0.04 s</td></tr><tr><td align="left">min_pursuit_duration</td><td align="left">minimum duration of a pursuit event candidate (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E4)</td><td align="left">0.04 s</td></tr><tr><td align="left">min_intersaccade_duration</td><td align="left">no saccade classification is performed in windows shorter than twice this value, plus minimum saccade and PSO duration (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E2)</td><td align="left">0.04 s</td></tr><tr><td align="left">noise_factor</td><td align="left">adaptive saccade onset threshold velocity is the median absolute deviation of velocities in the window of interest, times this factor (peak velocity threshold is twice the onset velocity); increase for noisy data to reduce false positives (Nyström &amp; Holmqvist, <xref ref-type="bibr" rid="CR37">2010</xref>, equivalent: 3.0) (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E1)</td><td align="left">5</td></tr><tr><td align="left">velthresh_startvelocity</td><td align="left">start value for adaptive velocity threshold algorithm (Nyström &amp; Holmqvist, <xref ref-type="bibr" rid="CR37">2010</xref>), should be larger than any conceivable minimum saccade velocity (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E1)</td><td align="left">300 deg/s</td></tr><tr><td align="left">max_initial_saccade_freq</td><td align="left">maximum saccade frequency for initial classification of major saccades, initial data chunking is stopped if this frequency is reached (should be smaller than an expected (natural) saccade frequency in a particular context), default based on literature reports of a natural, free-viewing saccade frequency of <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sim $\end{document}</tex-math><mml:math id="M2"><mml:mo>∼</mml:mo></mml:math><inline-graphic xlink:href="13428_2020_1428_Article_IEq1.gif"/></alternatives></inline-formula>1.7 ± 0.3 Hz during a movie stimulus (Amit et al., <xref ref-type="bibr" rid="CR1">2017</xref>) (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E1)</td><td align="left">2 Hz</td></tr><tr><td align="left">saccade_context_window_length</td><td align="left">size of a window centered on any velocity peak for adaptive determination of saccade velocity thresholds (for initial data chunking only) (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E2)</td><td align="left">1 s</td></tr><tr><td align="left">lowpass_cutoff_freq</td><td align="left">cut-off frequency of a Butterworth low-pass filter applied to determine drift velocities in a pursuit event candidate (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E4)</td><td align="left">4 Hz</td></tr><tr><td align="left">pursuit_velthresh</td><td align="left">fixed drift velocity threshold to distinguish periods of pursuit from periods of fixation; higher than natural ocular drift velocities during fixations (e.g., Goltz et al., <xref ref-type="bibr" rid="CR11">1997</xref>; Cherici et al., <xref ref-type="bibr" rid="CR5">2012</xref>) (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E4)</td><td align="left">2 deg/s</td></tr></tbody></table></table-wrap></p>
    <sec id="Sec3">
      <title>Preprocessing</title>
      <p id="Par8">The goal of data preprocessing is to compute a time series of eye-movement velocities on which the event classification algorithm can be executed, while jointly reducing non-eye-movement-related noise in the data as much as possible.</p>
      <p id="Par9">First, implausible spikes in the coordinate time series are removed with a heuristic spike filter (Stampe, <xref ref-type="bibr" rid="CR43">1993</xref>) (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, P1). This filter is standard in many eye-tracking toolboxes and often used for preprocessing (e.g., Friedman et al., <xref ref-type="bibr" rid="CR10">2018</xref>). Data samples around signal loss (e.g., eye blinks) can be set to non-numeric values (NaN) in order to eliminate spurious movement signals without shortening the time series (dilate_nan, min_blink_duration; Fig. <xref rid="Fig1" ref-type="fig">1</xref>, P2). This is motivated by the fact that blinks can produce artifacts in the eye-tracking signal when the eyelid closes and re-opens (Choe et al., <xref ref-type="bibr" rid="CR6">2016</xref>). Coordinate time series are temporally filtered in two different ways Fig. <xref rid="Fig1" ref-type="fig">1</xref>, P3). A relatively large median filter (median_filter_length) is used to emphasize large amplitude saccades. This type of filtered data is later used for a coarse segmentation of a time series into shorter intervals between major saccades. Separately, data are also smoothed with a Savitzky–Golay filter (savgol_{length,polyord}). All event classification beyond the localization of major saccades for time series chunking is performed on this type of filtered data.</p>
      <p id="Par10">After spike-removal and temporal filtering, movement velocities are computed. To disregard biologically implausible measurements, a configurable maximum velocity (max_vel) is enforced—any samples exceeding this threshold are replaced by this set value.</p>
    </sec>
    <sec id="Sec4">
      <title>Event classification</title>
      <sec id="Sec5">
        <title>Saccade velocity threshold</title>
        <p id="Par11">Except for a few modifications, REMoDNaV employs the adaptive saccade classification algorithm proposed by (Nyström &amp; Holmqvist, <xref ref-type="bibr" rid="CR37">2010</xref>), where saccades are initially located by thresholding the velocity time series by a critical value. Starting from an initial velocity threshold (velthresh_startvelocity, termed <italic>P</italic><italic>T</italic><sub>1</sub> in NH), the critical value is determined adaptively by computing the variance of sub-threshold velocities (<italic>V</italic> ), and placing the new velocity threshold at:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ PT_{n} = \overline{V}_{n-1} + F \times \sqrt{{\sum(V_{n-1} - \overline{V}_{n-1})^{2}} \over {N-1}} $$\end{document}</tex-math><mml:math id="M4"><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>×</mml:mo><mml:msqrt><mml:mrow><mml:mfrac class="tfrac"><mml:mrow><mml:mo>∑</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt></mml:math><graphic xlink:href="13428_2020_1428_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>F</italic> determines how many standard deviations above the average velocity the new threshold is located. This procedure is repeated until it stabilizes on a threshold velocity.
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ |PT_{n} - PT_{n-1}| &lt; 1^{\circ}/sec $$\end{document}</tex-math><mml:math id="M6"><mml:mo>|</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∘</mml:mo></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi></mml:math><graphic xlink:href="13428_2020_1428_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par12">REMoDNaV alters this algorithm by using robust statistics that are more suitable for the non-normal distribution of velocities (Friedman et al., <xref ref-type="bibr" rid="CR10">2018</xref>), such that the new threshold is computed by:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ PT_{n} = median({V}_{n-1}) + F \times MAD({V}_{n-1}) $$\end{document}</tex-math><mml:math id="M8"><mml:mi>P</mml:mi><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><graphic xlink:href="13428_2020_1428_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <italic>MAD</italic> is the median absolute deviation, and <italic>F</italic> is a scalar parameter of the algorithm. This iterative process is illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E1 (upper panel).</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Time series chunking</title>
      <p id="Par13">As the algorithm aims to be applicable to prolonged recordings with potentially inhomogeneous noise levels, the time series needs to be split into shorter chunks to prevent the negative impact of sporadic noise flares on the aforementioned adaptive velocity thresholding procedure.
</p>
      <p id="Par14">REMoDNaV implements this time-series chunking by determining a critical velocity on a median-filtered (median_filter_length) time series comprising the full duration of a recording (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E2). Due to potentially elevated noise levels, the resulting threshold tends to overestimate an optimal threshold. Consequently, only periods of fastest eye movements will exceed this threshold. All such periods of consecutive above-threshold velocities are weighted by the sum of these velocities. Boundaries of time series chunks are determined by selecting such events sequentially (starting with the largest sums), until a maximum average frequency across the whole time series is reached (max_initial_saccade_freq). The resulting chunks represent data intervals between saccades of maximum magnitude in the respective data. Figure <xref rid="Fig1" ref-type="fig">1</xref>, E3 (right) exemplifies event classification within such an intersaccadic interval.
</p>
    </sec>
    <sec id="Sec7">
      <title>Classification of saccades and post-saccadic oscillations</title>
      <p id="Par15">Classification of these event types is identical to the NH algorithm, only the data context and metrics for determining the velocity thresholds differ. For saccades that also represent time series chunk boundaries (event label SACC), a context of 1 s (saccade_context_window_ length) centered on the peak velocity is used by default, for any other saccade (event label ISAC) the entire time series chunk represents that context (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E3).</p>
      <p id="Par16">Peak velocity threshold and on/offset velocity threshold are then determined by equation <xref rid="Equ3" ref-type="">3</xref> with <italic>F</italic> set to 2 ×_ and noise_factor, respectively. Starting from a velocity peak, the immediately preceding and the following velocity minima that do not exceed the on/offset threshold are located and used as event boundaries. Qualifying events are rejected if they do not exceed a configurable minimum duration or violate the set saccade maximum proximity criterion (min_saccade_duration, min_intersaccade_duration).</p>
      <p id="Par17">As in NH, post-saccadic oscillations are events that immediately follow a saccade, where the velocity exceeds the saccade velocity threshold within a short time window (max_pso_duration). REMoDNaV distinguishes low-velocity (event label LPSO for chunk boundary event, ILPS otherwise) and high-velocity oscillations (event label HPSO or IHPS), where the velocity exceeds the saccade onset or peak velocity threshold, respectively.</p>
    </sec>
    <sec id="Sec8">
      <title>Pursuit and fixation classification</title>
      <p id="Par18">For all remaining, unlabeled time series segments that are longer than a minimum duration (min_fixation_ duration), velocities are low-pass filtered (Butterworth, lowpass_cutoff_freq). Any segments exceeding a minimum velocity threshold (pursuit_velthresh) are classified as pursuit (event label PURS). Pursuit on/offset classification uses the same approach as that for saccades: search for local minima preceding and following the above threshold velocities. Any remaining segment that does not qualify as a pursuit event is classified as a fixation (event label FIXA) (Fig. <xref rid="Fig1" ref-type="fig">1</xref>, E4).</p>
    </sec>
    <sec id="Sec9">
      <title>Operation</title>
      <p id="Par19">REMoDNaV is free and open-source software, written in the Python language and released under the terms of the MIT license. In addition to the Python standard library it requires the Python packages NumPy (Oliphant, <xref ref-type="bibr" rid="CR38">2006</xref>), Matplotlib (Hunter, <xref ref-type="bibr" rid="CR25">2007</xref>), statsmodels (Seabold &amp; Perktold, <xref ref-type="bibr" rid="CR42">2010</xref>), and SciPy (Jones et al., <xref ref-type="bibr" rid="CR27">2001</xref>) as software dependencies. Furthermore, DataLad (Halchenko et al., <xref ref-type="bibr" rid="CR14">2018</xref>), and Pandas (McKinney &amp; et al. <xref ref-type="bibr" rid="CR36">2010</xref>) have to be available to run the test battery. REMoDNaV itself, and all software dependencies are available on all major operating systems. There are no particular hardware requirements for running the software other than sufficient memory to load and process the data.</p>
      <p id="Par20">A typical program invocation looks like</p>
      <p id="Par21">
remodnav &lt;inputfile&gt; &lt;outputfile&gt; \ &lt;px2deg&gt; &lt;samplingrate&gt;</p>
      <p id="Par2100">where &lt;inputfile&gt; is the name of a tab-separated-value (TSV) text file with one gaze coordinate sample per line. An input file can have any number of columns, only the first two columns are read and interpreted as <italic>X</italic> and <italic>Y</italic> coordinates. Note that this constrains input data to a dense data representation, i.e., either data from eye trackers with fixed sampling frequency throughout the recording, or sparse data that has been transformed into a dense representation beforehand. The second argument &lt;outputfile&gt; is the file name of a BIDS-compliant (Gorgolewski et al., <xref ref-type="bibr" rid="CR13">2016</xref>) TSV text file that will contain a report on one classified eye movement event per line, with onset and offset time, onset and offset coordinates, amplitude, peak velocity, median velocity and average velocity. The remaining arguments are the only two mandatory parameters: the conversion factor from pixels to visual degrees, i.e., the visual angle of a single (square) pixel (&lt;px2deg&gt; in deg), and the temporal sampling rate (&lt;sampling_rate&gt; in Hz). Any other supported parameter can be added to the program invocation to override the default values.</p>
      <p id="Par22">A complete list of supported parameters (sorted by algorithm step) with their description and default value, are listed in Table <xref rid="Tab1" ref-type="table">1</xref>. While the required user input is kept minimal, the number of configurable parameters is purposefully large to facilitate optimal parameterization for data with specific properties. Besides the list of classified events, a visualization of the classification results, together with a time course of horizontal and vertical gaze position, and velocities is provided for illustration and initial quality assessment of algorithm performance on each input data file.</p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Validation analyses</title>
    <p id="Par23">The selection of datasets and analyses for validating algorithm performance was guided by three objectives: 1) compare to other existing solutions; 2) demonstrate plausible results on data from prolonged gaze coordinate recordings during viewing of dynamic, feature-rich stimuli; and 3) illustrate result robustness on lower-quality data. The following three sections each introduce a dataset and present the validation results for these objectives. All analysis presented here are performed using default parameters (Table <xref rid="Tab1" ref-type="table">1</xref>), with no dataset-specific tuning other than the built-in adaptive behavior.</p>
    <sec id="Sec11">
      <title>Algorithm comparison</title>
      <p id="Par24">Presently, Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>) represents the most comprehensive comparative study on eye-movement classification algorithms. Moreover, the dataset employed in that study was made publicly available. Consequently, evaluating REMoDNaV performance on these data and using their metrics offers a straightforward approach to relate this new development to alternative solutions.</p>
      <p id="Par25">The dataset provided by Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>)<xref ref-type="fn" rid="Fn1">1</xref> consists of monocular eye-gaze data produced from viewing stimuli from three distinct categories—images, moving dots, and videos. The data release contains gaze coordinate time series (500 Hz sampling rate), and metadata on stimulus size and viewing distance. Importantly, each time point was manually classified by two expert human raters as one of six event categories: fixation, saccade, PSO, smooth pursuit, blink and undefined (a sample that did not fit any other category). A minor labeling mistake reported in Zemblys et al., (<xref ref-type="bibr" rid="CR48">2018</xref>) was fixed prior to this validation analysis.</p>
      <p id="Par27">For each stimulus category, we computed the proportion of misclassifications per event type, comparing REMoDNaV to each of the human coders, and, as a baseline measure, the human coders against each other. A time point was counted as misclassified if the two compared classifications did not assign the same label. We limited this analysis to all time points that were labeled as fixation, saccade, PSO, or pursuit by any method, hence ignoring the rarely used NaN/blinks or “undefined” category. For a direct comparison with the results in Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>), the analysis was repeated while also excluding samples labeled as pursuit. In the labeled data, there was no distinction made between high- and low-velocity PSOs, potentially because the literature following Nyström and Holmqvist (<xref ref-type="bibr" rid="CR37">2010</xref>) did not adopt their differentiation of PSOs into velocity categories. All high- and low-velocity PSOs classified by REMoDNaV were therefore collapsed into a single PSO category. Table <xref rid="Tab2" ref-type="table">2</xref> shows the misclassification rates for all pairwise comparisons, in all stimulus types. In comparison to the NH algorithm, after which the proposed work was modeled, REMoDNaV performed consistently better (32/93/70% average misclassification for NH, vs. 6.5/10.8/ 9.1% worst misclassification for REMoDNaV in categories images, dots, and videos). Compared to all ten algorithms evaluated in Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>), REMoDNaV exhibits the lowest misclassification rates across all stimulus categories. When taking smooth pursuit events into account, the misclassification rate naturally increases, but remains comparably low. Importantly, it still exceeds the performance of all algorithms tested in Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>) in the dots and video category, and performs among the best in the images category. Additionally, both with and without smooth pursuit, REMoDNaV s performance exceeds also that of a recent deep neural network trained specifically on video clips (Startsev et al., <xref ref-type="bibr" rid="CR44">2018</xref>, compare Table 7: 34% misclassification versus 31.5% for REMoDNaV).
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Proportion of samples in each stimulus category classified in disagreement between human coders (MN, RA) and the REMoDNaV algorithm (AL)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Comp</th><th align="left">MC</th><th align="left">w/oP</th><th align="left">Coder</th><th align="left">Fix</th><th align="left">Sac</th><th align="left">PSO</th><th align="left">SP</th></tr></thead><tbody><tr><td align="left">Images</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">MN-RA</td><td align="left">6.1</td><td align="left">3.0</td><td align="left">MN</td><td align="left">70</td><td align="left">9</td><td align="left">21</td><td align="left">0</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">RA</td><td align="left">13</td><td align="left">15</td><td align="left">20</td><td align="left">53</td></tr><tr><td align="left">MN-AL</td><td align="left">23.1</td><td align="left">6.5</td><td align="left">MN</td><td align="left">86</td><td align="left">2</td><td align="left">11</td><td align="left">2</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">AL</td><td align="left">5</td><td align="left">13</td><td align="left">6</td><td align="left">75</td></tr><tr><td align="left">RA-AL</td><td align="left">22.8</td><td align="left">6.4</td><td align="left">RA</td><td align="left">77</td><td align="left">3</td><td align="left">11</td><td align="left">9</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">AL</td><td align="left">13</td><td align="left">13</td><td align="left">6</td><td align="left">68</td></tr><tr><td align="left">Dots</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">MN-RA</td><td align="left">10.7</td><td align="left">4.2</td><td align="left">MN</td><td align="left">11</td><td align="left">10</td><td align="left">9</td><td align="left">71</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">RA</td><td align="left">64</td><td align="left">7</td><td align="left">6</td><td align="left">23</td></tr><tr><td align="left">MN-AL</td><td align="left">18.6</td><td align="left">8.2</td><td align="left">MN</td><td align="left">9</td><td align="left">5</td><td align="left">8</td><td align="left">78</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">AL</td><td align="left">77</td><td align="left">6</td><td align="left">2</td><td align="left">15</td></tr><tr><td align="left">RA-AL</td><td align="left">22.8</td><td align="left">10.8</td><td align="left">RA</td><td align="left">28</td><td align="left">4</td><td align="left">6</td><td align="left">61</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">AL</td><td align="left">59</td><td align="left">7</td><td align="left">2</td><td align="left">31</td></tr><tr><td align="left">Videos</td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">MN-RA</td><td align="left">18.5</td><td align="left">4.0</td><td align="left">MN</td><td align="left">75</td><td align="left">3</td><td align="left">8</td><td align="left">15</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">RA</td><td align="left">16</td><td align="left">4</td><td align="left">3</td><td align="left">77</td></tr><tr><td align="left">MN-AL</td><td align="left">31.5</td><td align="left">7.9</td><td align="left">MN</td><td align="left">57</td><td align="left">1</td><td align="left">6</td><td align="left">36</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">AL</td><td align="left">36</td><td align="left">5</td><td align="left">3</td><td align="left">55</td></tr><tr><td align="left">RA-AL</td><td align="left">28.5</td><td align="left">9.1</td><td align="left">RA</td><td align="left">38</td><td align="left">3</td><td align="left">5</td><td align="left">55</td></tr><tr><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">AL</td><td align="left">53</td><td align="left">6</td><td align="left">5</td><td align="left">35</td></tr></tbody></table><table-wrap-foot><p>The MC (misclassification) column lists proportions considering all four event categories (fixation, saccade, PSO, pursuit), while the w/oP (without pursuit) column excludes pursuit events for a direct comparison with Andersson et al.,(<xref ref-type="bibr" rid="CR2">2017</xref>, Tables 8–10). The remaining columns show the percentage of labels assigned to incongruent time points by each rater (deviation of their sum from 100% is due to rounding)</p></table-wrap-foot></table-wrap></p>
      <p id="Par28">Figure <xref rid="Fig2" ref-type="fig">2</xref> shows confusion patterns for a comparison of algorithm classifications with human labeling and displays the similarity between classification decisions with Jaccard indices (JI; Jaccard, <xref ref-type="bibr" rid="CR26">1901</xref>). The JI is bound in range [0, 1] with higher values indicating higher similarity. A value of 0.93 in the upper left cell of the very first matrix in Fig. <xref rid="Fig2" ref-type="fig">2</xref> for example indicates that 93% of frames that are labeled as a fixation by human coders RA and MN are the same. This index allows to quantify the similarity of classifications independent of values in other cells. While REMoDNaV does not achieve a labeling similarity that reaches the human inter-rater agreement, it still performs well. In particular, the relative magnitude of agreement with each individual human coder for fixations, saccades, and PSOs, resembles the agreement between the human coders. Classification of smooth pursuits is consistent with human labels for the categories moving dots, and videos. However, there is a substantial confusion of fixation and pursuit for the static images. In a real-world application of REMoDNaV, pursuit classification could be disabled (by setting a high pursuit velocity threshold) for data from static images, if the occurrence of pursuit events can be ruled out a priori. For this evaluation, however, no such intervention was made.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Confusion patterns for pairwise eye-movement classification comparison of both human raters (MN and RA; Andersson et al., <xref ref-type="bibr" rid="CR2">2017</xref>) and the REMoDNaV algorithm (AL) for gaze recordings from stimulation with static images (<italic>left column</italic>), moving dots (<italic>middle column</italic>), and video clips (<italic>right column</italic>). All matrices present gaze sample-based Jaccard indices (JI; Jaccard, <xref ref-type="bibr" rid="CR26">1901</xref>). Consequently, the diagonals depict the fraction of time points labeled congruently by both raters in relation to the number of timepoints assigned to a particular event category by any rater</p></caption><graphic xlink:href="13428_2020_1428_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par29">In addition to the confusion analysis and again following Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>), we computed Cohen’s kappa (Cohen, <xref ref-type="bibr" rid="CR7">1960</xref>) for an additional measure of similarity between human and algorithm performance. It quantifies the sample-by-sample agreement between two ratings following Eq. <xref rid="Equ4" ref-type="">4</xref>:
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ K = \frac{P_{o} - P_{c}}{1- P_{c}} $$\end{document}</tex-math><mml:math id="M10"><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mfrac class="tfrac"><mml:mrow><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="13428_2020_1428_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <italic>P</italic><sub><italic>o</italic></sub> is the observed proportion of agreement between the ratings, and <italic>P</italic><sub><italic>c</italic></sub> is the proportion of chance agreement. A value of <italic>K</italic> = 1 indicates perfect agreement, and <italic>K</italic> = 0 indicates chance level agreement. Table <xref rid="Tab3" ref-type="table">3</xref> displays the resulting values between the two human experts, and REMoDNaV with each of the experts, for each stimulus category and the three event types used in Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>), namely fixations, saccades, and PSOs (compare to Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>), Table 7). For all event types and stimulus categories, REMoDNaV performs on par or better than the original NH algorithm, and in many cases on par or better than the best of all algorithms evaluated in Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>) within an event or stimulus type.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Cohen’s kappa reliability between human coders (MN, RA), and REMoDNaV (AL) with each of the human coders</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Comparison</th><th align="left">Images</th><th align="left">Dots</th><th align="left">Videos</th></tr></thead><tbody><tr><td align="left">Fixations</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">MN versus RA</td><td align="left">0.84</td><td align="left">0.65</td><td align="left">0.65</td></tr><tr><td align="left">AL versus RA</td><td align="left">0.55</td><td align="left">0.37</td><td align="left">0.44</td></tr><tr><td align="left">AL versus MN</td><td align="left">0.52</td><td align="left">0.45</td><td align="left">0.39</td></tr><tr><td align="left">Saccades</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">MN versus RA</td><td align="left">0.91</td><td align="left">0.81</td><td align="left">0.87</td></tr><tr><td align="left">AL versus RA</td><td align="left">0.78</td><td align="left">0.72</td><td align="left">0.76</td></tr><tr><td align="left">AL versus MN</td><td align="left">0.78</td><td align="left">0.78</td><td align="left">0.79</td></tr><tr><td align="left">PSOs</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left">MN versus RA</td><td align="left">0.76</td><td align="left">0.62</td><td align="left">0.65</td></tr><tr><td align="left">AL versus RA</td><td align="left">0.59</td><td align="left">0.38</td><td align="left">0.45</td></tr><tr><td align="left">AL versus MN</td><td align="left">0.58</td><td align="left">0.41</td><td align="left">0.51</td></tr></tbody></table></table-wrap></p>
      <p id="Par30">In order to further rank the performance of the proposed algorithm with respect to the ten algorithms studied in Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>), we followed their approach to compute root mean square deviations (RMSD) from human labels for event duration distribution characteristics (mean and standard deviation of durations, plus number of events) for each stimulus category (images, dots, videos) and event type (fixations, saccades, PSOs, pursuits). This measure represents a scalar distribution dissimilarity score that can be used as an additional comparison metric of algorithm performance that focuses on overall number and durations of classified events, instead of sample-by-sample misclassification. The RMSD measure has a lower bound of 0.0 (identical to the average of both human raters), with higher values indicating larger differences (for detail information on the calculation of this metric see Andersson et al., <xref ref-type="bibr" rid="CR2">2017</xref>).</p>
      <p id="Par31">Table <xref rid="Tab4" ref-type="table">4</xref> is modeled after Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>, Tables 3–6), appended with REMoDNaV, showing RMSD based on the scores of human raters given in the original tables. As acknowledged by the authors, the absolute value of the RMSD scores is not informative due to scaling with respect to the respective maximum value of each characteristic. Therefore, we converted RMSDs for each algorithm and event type into zero-based ranks (lower is more human-like).
<table-wrap id="Tab4"><label>Table 4</label><caption><p>Comparison of event duration statistics (mean, standard deviation, and number of events) for image, dot, and video stimuli</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left" colspan="4">Images</th><th align="left" colspan="4">Dots</th><th align="left" colspan="4">Videos</th></tr><tr><th align="left">Algorithm</th><th align="left">Mean</th><th align="left">SD</th><th align="left">#</th><th align="left">rank</th><th align="left">Mean</th><th align="left">SD</th><th align="left">#</th><th align="left">rank</th><th align="left">Mean</th><th align="left">SD</th><th align="left">#</th><th align="left">rank</th></tr></thead><tbody><tr><td align="left" colspan="13">Fixations</td></tr><tr><td align="left">MN</td><td align="left">248</td><td align="left">271</td><td align="left">380</td><td align="left">1</td><td align="left">161</td><td align="left">30</td><td align="left">2</td><td align="left">1</td><td align="left">318</td><td align="left">289</td><td align="left">67</td><td align="left">0</td></tr><tr><td align="left">RA</td><td align="left">242</td><td align="left">273</td><td align="left">369</td><td align="left">0</td><td align="left">131</td><td align="left">99</td><td align="left">13</td><td align="left">0</td><td align="left">240</td><td align="left">189</td><td align="left">67</td><td align="left">1</td></tr><tr><td align="left">CDT</td><td align="left">397</td><td align="left">559</td><td align="left">251</td><td align="left">10</td><td align="left">60</td><td align="left">127</td><td align="left">165</td><td align="left">9</td><td align="left">213</td><td align="left">297</td><td align="left">211</td><td align="left">7</td></tr><tr><td align="left">EM</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">IDT</td><td align="left">399</td><td align="left">328</td><td align="left">242</td><td align="left">7</td><td align="left">323</td><td align="left">146</td><td align="left">8</td><td align="left">5</td><td align="left">554</td><td align="left">454</td><td align="left">48</td><td align="left">8</td></tr><tr><td align="left">IKF</td><td align="left">174</td><td align="left">239</td><td align="left">513</td><td align="left">5</td><td align="left">217</td><td align="left">184</td><td align="left">72</td><td align="left">6</td><td align="left">228</td><td align="left">296</td><td align="left">169</td><td align="left">4</td></tr><tr><td align="left">IMST</td><td align="left">304</td><td align="left">293</td><td align="left">333</td><td align="left">3</td><td align="left">268</td><td align="left">140</td><td align="left">12</td><td align="left">3</td><td align="left">526</td><td align="left">825</td><td align="left">71</td><td align="left">10</td></tr><tr><td align="left">IHMM</td><td align="left">133</td><td align="left">216</td><td align="left">701</td><td align="left">8</td><td align="left">214</td><td align="left">286</td><td align="left">67</td><td align="left">8</td><td align="left">234</td><td align="left">319</td><td align="left">194</td><td align="left">6</td></tr><tr><td align="left">IVT</td><td align="left">114</td><td align="left">204</td><td align="left">827</td><td align="left">9</td><td align="left">203</td><td align="left">282</td><td align="left">71</td><td align="left">7</td><td align="left">202</td><td align="left">306</td><td align="left">227</td><td align="left">9</td></tr><tr><td align="left">NH</td><td align="left">258</td><td align="left">299</td><td align="left">292</td><td align="left">2</td><td align="left">380</td><td align="left">333</td><td align="left">30</td><td align="left">10</td><td align="left">429</td><td align="left">336</td><td align="left">83</td><td align="left">2</td></tr><tr><td align="left">BIT</td><td align="left">209</td><td align="left">136</td><td align="left">423</td><td align="left">4</td><td align="left">189</td><td align="left">113</td><td align="left">67</td><td align="left">4</td><td align="left">248</td><td align="left">215</td><td align="left">170</td><td align="left">3</td></tr><tr><td align="left">LNS</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">REMoDNaV</td><td align="left">187</td><td align="left">132</td><td align="left">426</td><td align="left">6</td><td align="left">116</td><td align="left">65</td><td align="left">43</td><td align="left">2</td><td align="left">147</td><td align="left">107</td><td align="left">144</td><td align="left">5</td></tr><tr><td align="left" colspan="13">Saccades</td></tr><tr><td align="left">MN</td><td align="left">30</td><td align="left">17</td><td align="left">376</td><td align="left">0</td><td align="left">23</td><td align="left">10</td><td align="left">47</td><td align="left">0</td><td align="left">26</td><td align="left">13</td><td align="left">116</td><td align="left">0</td></tr><tr><td align="left">RA</td><td align="left">31</td><td align="left">15</td><td align="left">372</td><td align="left">1</td><td align="left">22</td><td align="left">11</td><td align="left">47</td><td align="left">1</td><td align="left">25</td><td align="left">12</td><td align="left">126</td><td align="left">1</td></tr><tr><td align="left">CDT</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">EM</td><td align="left">25</td><td align="left">22</td><td align="left">787</td><td align="left">9</td><td align="left">17</td><td align="left">14</td><td align="left">93</td><td align="left">8</td><td align="left">20</td><td align="left">16</td><td align="left">252</td><td align="left">6</td></tr><tr><td align="left">IDT</td><td align="left">35</td><td align="left">15</td><td align="left">258</td><td align="left">3</td><td align="left">32</td><td align="left">14</td><td align="left">10</td><td align="left">7</td><td align="left">24</td><td align="left">53</td><td align="left">41</td><td align="left">9</td></tr><tr><td align="left">IKF</td><td align="left">62</td><td align="left">37</td><td align="left">353</td><td align="left">10</td><td align="left">60</td><td align="left">26</td><td align="left">29</td><td align="left">10</td><td align="left">55</td><td align="left">20</td><td align="left">107</td><td align="left">8</td></tr><tr><td align="left">IMST</td><td align="left">17</td><td align="left">10</td><td align="left">335</td><td align="left">6</td><td align="left">13</td><td align="left">5</td><td align="left">18</td><td align="left">6</td><td align="left">18</td><td align="left">10</td><td align="left">76</td><td align="left">4</td></tr><tr><td align="left">IHMM</td><td align="left">48</td><td align="left">26</td><td align="left">368</td><td align="left">8</td><td align="left">41</td><td align="left">17</td><td align="left">27</td><td align="left">9</td><td align="left">42</td><td align="left">18</td><td align="left">109</td><td align="left">7</td></tr><tr><td align="left">IVT</td><td align="left">41</td><td align="left">22</td><td align="left">373</td><td align="left">5</td><td align="left">36</td><td align="left">14</td><td align="left">28</td><td align="left">4</td><td align="left">36</td><td align="left">16</td><td align="left">112</td><td align="left">5</td></tr><tr><td align="left">NH</td><td align="left">50</td><td align="left">20</td><td align="left">344</td><td align="left">7</td><td align="left">43</td><td align="left">16</td><td align="left">42</td><td align="left">5</td><td align="left">44</td><td align="left">18</td><td align="left">1104</td><td align="left">10</td></tr><tr><td align="left">BIT</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td><td align="left">–</td></tr><tr><td align="left">LNS</td><td align="left">29</td><td align="left">12</td><td align="left">390</td><td align="left">2</td><td align="left">26</td><td align="left">11</td><td align="left">53</td><td align="left">2</td><td align="left">28</td><td align="left">12</td><td align="left">122</td><td align="left">2</td></tr><tr><td align="left">REMoDNaV</td><td align="left">39</td><td align="left">20</td><td align="left">388</td><td align="left">4</td><td align="left">30</td><td align="left">13</td><td align="left">40</td><td align="left">3</td><td align="left">33</td><td align="left">15</td><td align="left">118</td><td align="left">3</td></tr><tr><td align="left" colspan="13">Post-saccadic oscillations</td></tr><tr><td align="left">MN</td><td align="left">21</td><td align="left">11</td><td align="left">312</td><td align="left">1</td><td align="left">15</td><td align="left">5</td><td align="left">33</td><td align="left">0</td><td align="left">20</td><td align="left">11</td><td align="left">97</td><td align="left">1</td></tr><tr><td align="left">RA</td><td align="left">21</td><td align="left">9</td><td align="left">309</td><td align="left">0</td><td align="left">15</td><td align="left">8</td><td align="left">28</td><td align="left">1</td><td align="left">17</td><td align="left">8</td><td align="left">89</td><td align="left">2</td></tr><tr><td align="left">NH</td><td align="left">28</td><td align="left">13</td><td align="left">237</td><td align="left">4</td><td align="left">24</td><td align="left">12</td><td align="left">17</td><td align="left">4</td><td align="left">28</td><td align="left">13</td><td align="left">78</td><td align="left">4</td></tr><tr><td align="left">LNS</td><td align="left">25</td><td align="left">9</td><td align="left">319</td><td align="left">2</td><td align="left">20</td><td align="left">9</td><td align="left">31</td><td align="left">2</td><td align="left">24</td><td align="left">10</td><td align="left">87</td><td align="left">3</td></tr><tr><td align="left">REMoDNaV</td><td align="left">19</td><td align="left">8</td><td align="left">277</td><td align="left">3</td><td align="left">18</td><td align="left">8</td><td align="left">14</td><td align="left">3</td><td align="left">18</td><td align="left">8</td><td align="left">86</td><td align="left">0</td></tr><tr><td align="left" colspan="13">Pursuit</td></tr><tr><td align="left">MN</td><td align="left">363</td><td align="left">187</td><td align="left">3</td><td align="left">1</td><td align="left">375</td><td align="left">256</td><td align="left">37</td><td align="left">1</td><td align="left">521</td><td align="left">347</td><td align="left">50</td><td align="left">1</td></tr><tr><td align="left">RA</td><td align="left">305</td><td align="left">184</td><td align="left">16</td><td align="left">0</td><td align="left">378</td><td align="left">364</td><td align="left">33</td><td align="left">0</td><td align="left">472</td><td align="left">319</td><td align="left">68</td><td align="left">0</td></tr><tr><td align="left">REMoDNaV</td><td align="left">197</td><td align="left">73</td><td align="left">118</td><td align="left">2</td><td align="left">440</td><td align="left">385</td><td align="left">34</td><td align="left">2</td><td align="left">314</td><td align="left">229</td><td align="left">97</td><td align="left">2</td></tr></tbody></table><table-wrap-foot><p>This table is modeled after Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>, Tables 3–6), and root-mean-square-deviations (RMSD) from human raters are shown for fixations, saccades, PSOs, and pursuit as zero-based ranks (rank zero is closest to the average of the two human raters). Summary statistics for all algorithms used in Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>) were taken from their publicly available GitHub repository (<ext-link ext-link-type="uri" xlink:href="http://www.github.com/richardandersson/EyeMovementDetectorEvaluation">github.com/richardandersson/EyeMovementDetectorEvaluation</ext-link>). Cohens kappa was computed for the complete set of algorithms in Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>) and REMoDNaV</p></table-wrap-foot></table-wrap></p>
      <p id="Par32">The LNS algorithm (Larsson et al., <xref ref-type="bibr" rid="CR30">2013</xref>) was found to have the most human-like performance for saccade and PSO classification in Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>). REMoDNaV performs comparable to LNS for both event types (saccades: 2.0 vs. 3.3; PSOs: 2.3 vs. 2.0, mean rank across stimulus categories for LNS and REMoDNaV, respectively).</p>
      <p id="Par33">Depending on the stimulus type, different algorithms performed best for fixation classification. NH performed best for images and videos, but worst for moving dots. REMoDNaV outperforms all other algorithms in the dots category, and achieves rank 5 and 6 (middle range) for videos and images, respectively. Across all stimulus and event categories, REMoDNaV achieves a mean ranking of 2.9, and a mean ranking of 3.2 when not taking smooth pursuit into account.
</p>
      <p id="Par34">Taken together, REMoDNaV yields classification results that are, on average, more human-like than any other algorithm tested on the dataset and metrics put forth by Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>). In particular, its performance largely equals or exceeds that of the original NH algorithm. NH outperforms it only for fixation classification in the image and video category, but in these categories REMoDNaV also classifies comparatively well. These results are an indication that the changes to the NH algorithm proposed here to improve upon its robustness are not detrimental to its performance on data from conventional paradigms and stimuli.</p>
    </sec>
    <sec id="Sec12">
      <title>Prolonged viewing of dynamic stimuli</title>
      <p id="Par35">Given that REMoDNaV yielded plausible results for the “video” stimulus category data in the Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>) dataset (Fig. <xref rid="Fig2" ref-type="fig">2</xref>, and Table <xref rid="Tab4" ref-type="table">4</xref>, right columns), we determined whether it is capable of analyzing data from dynamic stimulation in prolonged (15 min) recordings.</p>
      <p id="Par36">As a test dataset, we used publicly available eye-tracking data from the <italic>studyforrest.org</italic> project, where 15 participants were recorded watching a feature-length (≈ 2 h) movie in a laboratory setting (Hanke et al., <xref ref-type="bibr" rid="CR15">2016</xref>). Eye movements were measured by an EyeLink 1000 with a standard desktop mount (software version 4.51; SR Research Ltd., Mississauga, Ontario, Canada) and a sampling rate of 1000 Hz. The movie stimulus was presented on a 522 × 294 mm LCD monitor at a resolution of 1920 × 1280 px and a viewing distance of 85 cm. Participants watched the movie in eight approximately 15-min-long segments, with recalibration of the eye tracker before every segment.</p>
      <p id="Par37">As no manual eye-movement event labeling exists for these data, algorithm evaluation was limited to a comparison of marginal distributions and well-known properties, such as the log-log-linear relationship of saccade amplitude and saccade peak velocity (Bahill et al., <xref ref-type="bibr" rid="CR3">1975</xref>). Figure <xref rid="Fig3" ref-type="fig">3</xref> (top row) depicts this main sequence relationship. Additionally, Fig. <xref rid="Fig4" ref-type="fig">4</xref> (top row) shows duration histograms for all four event types across all participants. Shapes and locations of these distributions match previous reports in the literature, such as a strong bias towards short (less than 500 ms) fixations for dynamic stimuli (Dorr et al., <xref ref-type="bibr" rid="CR9">2010</xref>, Fig. 3), peak number of PSOs with durations between 10 and 20 ms (Nyström and Holmqvist, <xref ref-type="bibr" rid="CR37">2010</xref>, Fig. 11), and a non-Gaussian saccade duration distribution located below 100 ms (Nyström &amp; Holmqvist, <xref ref-type="bibr" rid="CR37">2010</xref>, Fig. 8, albeit for static scene perception). Overall, the presented summary statistics suggest that REMoDNaV is capable of classifying eye movements with plausible characteristics, in prolonged gaze recordings. A visualization of such a classification result is depicted in Fig. <xref rid="Fig5" ref-type="fig">5</xref> (top row).
<fig id="Fig3"><label>Fig. 3</label><caption><p>Main sequence of eye-movement events during one 15-min sequence of the movie (segment 2) for lab (<italic>top</italic>), and MRI participants (<italic>bottom</italic>). Data across all participants per dataset is shown on the left, and data for a single exemplary participant on the right</p></caption><graphic xlink:href="13428_2020_1428_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>Comparison of eye-movement event duration distributions for the high-quality lab sample (<italic>top row</italic>), and the lower-quality MRI sample (<italic>bottom row</italic>) across all participants (each <italic>N</italic> = 15), and the entire duration of the same feature-length movie stimulus. All histograms depict absolute number of events. Visible differences are limited to an overall lower number of events, and fewer long saccades for the MRI sample. These are attributable to a higher noise level and more signal loss (compare Hanke et al., <xref ref-type="bibr" rid="CR15">2016</xref>, Fig. 4b) in the MRI sample, and to stimulus size differences (23.75 ° MRI vs. 34 ° lab)</p></caption><graphic xlink:href="13428_2020_1428_Fig4_HTML" id="MO4"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>Exemplary eye-movement classification results for the same 10 s excerpt of a movie stimulus for a single participant in the high-quality lab sample (<italic>top</italic>), and in the lower-quality MRI sample (<italic>middle</italic>). The plots show filtered gaze coordinates (<italic>black</italic>), computed velocity time series (<italic>gray</italic>) overlaid on the eye-movement event segmentation with periods of fixation (<italic>green</italic>), pursuit (<italic>beige</italic>), saccades (<italic>blue</italic>), and high/low-velocity post-saccadic oscillations (<italic>dark</italic>/<italic>light purple</italic>). The <italic>bottom panel</italic> shows the first 2 s of unfiltered gaze coordinates (<italic>black</italic>) and unfiltered velocity time series (<italic>gray</italic>) for lab (<italic>left</italic>) and MRI (<italic>right</italic>) sample in greater detail. The variable noise level, and prolonged signal loss (<italic>white in top panel</italic>) visible in the MRI sample represent a challenge for algorithms. REMoDNaV uses an adaptive approach that determines major saccade events first, and subsequently tunes the velocity threshold to short time windows between these events. Figures like this accompany the program output to facilitate quality control and discovery of inappropriate preprocessing and classification parameterization</p></caption><graphic xlink:href="13428_2020_1428_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec13">
      <title>Lower-quality data</title>
      <p id="Par38">An explicit goal for REMoDNaV development was robust performance on lower-quality data. While a lack of quality can inevitably lead to misses in eye-movement classification if too severe and cannot be arbitrarily compensated, it is beneficial for any further analysis if operation on noisy data does not introduce unexpected event property biases.</p>
      <p id="Par39">In order to investigate noise-robustness we ran REMoDNaV on another publicly available dataset from the <italic>studyforrest.org</italic> project, where 15 different participants watched the exact same movie stimulus, but this time while lying on their back in the bore of an MRI scanner (Hanke et al., <xref ref-type="bibr" rid="CR15">2016</xref>). These data were recorded with a different EyeLink 1000 (software version 4.594) equipped with an MR-compatible telephoto lens and illumination kit (SR Research Ltd., Mississauga, Ontario, Canada) at 1000 Hz during simultaneous fMRI acquisition. The movie was presented at a viewing distance of 63 cm on a 26 cm (1280 × 1024 px) LCD screen in 720p resolution at full width, yielding a substantially smaller stimulus size, compared to the previous stimulation setup. The eye-tracking camera was mounted outside the scanner bore and recorded the participants’ left eye at a distance of about 100 cm. Compared to the lab-setup, physical limitations of the scanner environment, and sub-optimal infrared illumination led to substantially noisier data, as evident from a larger spatial uncertainty (Hanke et al., <xref ref-type="bibr" rid="CR15">2016</xref>, Technical Validation), a generally higher amount of data loss, and more samples with a velocity above 800 deg/s. Following common data quality criteria used to warrant exclusion by Holmqvist et al., (<xref ref-type="bibr" rid="CR22">2012</xref>), a higher amount of data loss, a greater number of samples with a velocity above 800 deg/s, and lower spatial accuracy can be indicative of lower quality data. The average amount of data loss in the MRI sample was three times higher than in the laboratory setting (15.1% versus 4.1% in the lab), with six out of 15 subjects having one or more movie segments with data loss greater than 30%. In the laboratory setting, in comparison, zero out of 15 subjects had one or more movie segments with data loss greater than 30% (Hanke et al., <xref ref-type="bibr" rid="CR15">2016</xref>, Table 1). Figure <xref rid="Fig6" ref-type="fig">6</xref> highlights the higher amount of extreme velocities in the MRI sample, even though the stimulus size was smaller than in the laboratory setting. Finally, the average spatial accuracy at the start of a recording, assessed with a 13-point calibration procedure, was 0.58 degrees of visual angle for the MRI sample and 0.45 degrees for the lab sample (Hanke et al., <xref ref-type="bibr" rid="CR15">2016</xref>, Technical Validation). An example of the amplified and variable noise pattern is shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref> (bottom row, gray lines). Except for the differences in stimulation setup, all other aspects of data acquisition, eye-tracker calibration, and data processing were identical to the previous dataset.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Comparison of sample velocity distributions for MRI and laboratory setting across all measurements and participants (excluding samples during periods of signal loss). The MRI sample exhibits a larger fraction of higher velocities, despite a 30% smaller stimulus size</p></caption><graphic xlink:href="13428_2020_1428_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par40">We performed the identical analysis as before, in order to compare performance between a high- and lower-quality data acquisition. This approach differs from the common approach of adding increasing levels of artificial noise to data (as done for example in Hessels et al., (<xref ref-type="bibr" rid="CR19">2017</xref>)), but bears the important advantage of incorporating real lower-quality data characteristics instead of potentially inappropriate or unnatural noise. Figures <xref rid="Fig3" ref-type="fig">3</xref>, <xref rid="Fig4" ref-type="fig">4</xref> and <xref rid="Fig5" ref-type="fig">5</xref> depict the results for the lab-quality dataset, and the MRI scanner dataset in the top and bottom rows, respectively.</p>
      <p id="Par41">Overall, the classification results exhibit strong similarity, despite the potential behavioral impact of watching a movie while lying on their back and looking upwards on the participants, or the well-known effect of increasing fatigue (Tagliazucchi and Laufs, <xref ref-type="bibr" rid="CR45">2014</xref>) during a 2-h session in an MRI scanner. In particular, saccade amplitude and peak velocity exhibit a clear main-sequence relationship that resembles that found for the lab acquisition (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). Duration distributions for fixations, PSOs, and pursuits are strikingly similar between the two datasets (Fig. <xref rid="Fig4" ref-type="fig">4</xref>), except for a generally lower number of classified events for the MRI experiment, which could be explained by the higher noise level and fraction of signal loss. There is a notable difference regarding the saccade duration distributions, with a bias towards shorter saccades in the MRI dataset. This effect may be attributable to the differences in stimulus size (30% smaller in the MRI environment).</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Conclusion</title>
    <p id="Par42">Based on the adaptive, velocity-based algorithm for fixation, saccade, and PSO classification by Nyström and Holmqvist (<xref ref-type="bibr" rid="CR37">2010</xref>), we have developed an improved algorithm that performs robustly on prolonged or short recordings with dynamic stimulation, with potentially variable noise levels, and also supports the classification of smooth pursuit events. Through a series of validation analyses, we have shown that its performance is comparable to or better than ten other contemporary algorithms, and that plausible classification results are achieved on high and lower quality data. These aspects of algorithm capabilities and performance suggest that REMoDNaV is a state-of-the-art tool for eye-movement classification with particular relevance for emerging complex data collections paradigms with dynamic stimulation, such as the combination of eye tracking and functional MRI in simultaneous measurements.</p>
    <p id="Par43">The proposed algorithm is rule-based, hence can be applied to data without prior training, apart from the adaptive estimation of velocity thresholds. This aspect distinguishes it from other recent developments based on deep neural networks (Startsev et al., <xref ref-type="bibr" rid="CR44">2018</xref>), and machine-learning in general (Zemblys et al., <xref ref-type="bibr" rid="CR48">2018</xref>). Some statistical learning algorithms require (labeled) training data, which can be a limitation in the context of a research study. However, in its present form REMoDNaV cannot be used for real-time data analysis, as its approach for time series chunking is based on an initial sorting of major saccade events across the entire time series. The proposed algorithm presently does not support the classification of eye blinks as a category distinct from periods of general signal loss. While such a feature could potentially be added, the current default preprocessing aims at removing blink-related signal. The algorithm maintains a distinction between high- and low-velocity PSOs first introduced by Nyström and Holmqvist (<xref ref-type="bibr" rid="CR37">2010</xref>), although, to our knowledge, the present literature does not make use of such a distinction. Algorithm users are encouraged to decide on a case-by-case basis whether to lump these event categories together into a general PSO category, as done in our own validation analyses. As a general remark it is also noteworthy that eye-tracking systems using pupil corneal reflection (pupil-CR) eye tracking may bias data towards premature PSO onset times and inflated PSO peak velocities (see Hooge et al., (<xref ref-type="bibr" rid="CR23">2016</xref>)). In deciding whether and how to interpret PSO events, it needs to be considered whether the eye-tracking device may have introduced biases in the data. Lastly, the evaluation results presented here are based on data with a relatively high temporal resolution (0.5 and 1 kHz). While the algorithm does not impose any hard constraints on data acquisition parameters, its performance on data from low-end, consumer grade hardware (e.g., 50-Hz sampling rate) has not been tested.</p>
    <p id="Par44">Just as Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>), we considered human raters as a gold standard reference for event classification when evaluating algorithms. The implications of the results presented herein are hence only valid if this assumption is warranted. Some authors voice concerns (e.g., Komogortsev et al., <xref ref-type="bibr" rid="CR29">2010</xref>), regarding potential biases that may limit generalizability. Nevertheless, human-made event labels are a critical component of algorithm validation, as pointed out by Hooge et al., (<xref ref-type="bibr" rid="CR24">2018</xref>).</p>
    <p id="Par45">The validation analyses presented here are based on three different datasets: a manually annotated dataset (Andersson et al., <xref ref-type="bibr" rid="CR2">2017</xref>), and two datasets with prolonged recordings using movie stimuli (Hanke et al., <xref ref-type="bibr" rid="CR15">2016</xref>). Beyond our own validation, a recent evaluation of nine different smooth pursuit algorithms by Startsev, Agtzidis, and Dorr as part of their recent paper (Startsev et al., <xref ref-type="bibr" rid="CR44">2018</xref>) also provides metrics for REMoDNaV. In their analysis, algorithm performance was evaluated against a partially hand-labelled eye- movement annotation of the Hollywood2 dataset (Mathe and Sminchisescu, <xref ref-type="bibr" rid="CR34">2012</xref>). We refrain from restating their methodology or interpreting their results here, but encourage readers to consult this independent report.<xref ref-type="fn" rid="Fn2">2</xref></p>
    <p id="Par47">REMoDNaV aims to be a readily usable tool, available as cross platform compatible, free and open-source software, with a simple command line interface and carefully chosen default settings. However, as evident from numerous algorithm evaluations (e.g., Andersson et al., <xref ref-type="bibr" rid="CR2">2017</xref>; Larsson et al., <xref ref-type="bibr" rid="CR30">2013</xref>; Zemblys et al., <xref ref-type="bibr" rid="CR48">2018</xref>; Komogortsev et al., <xref ref-type="bibr" rid="CR29">2010</xref>), different underlying stimulation, and data characteristics can make certain algorithms or parameterizations more suitable than others for particular applications. The provided implementation of the REMoDNaV algorithm (Hanke et al., <xref ref-type="bibr" rid="CR16">2019</xref>) acknowledges this fact by exposing a range of parameters through its user interface that can be altered in order to tune the classification for a particular use case.</p>
    <p id="Par48">The latest version of REMoDNaV can be installed from PyPi<xref ref-type="fn" rid="Fn3">3</xref> via pip install remodnav. The source code of the software can be found on Github.<xref ref-type="fn" rid="Fn4">4</xref> All reports on defects and enhancement can be submitted there. The analysis code underlying all results and figures presented in this paper, as well as the LATE X sources, are located in another GitHub repository.<xref ref-type="fn" rid="Fn5">5</xref> All required input data, from Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>) and the <italic>studyforrest.org</italic> project, are referenced in this repository at precise versions as DataLad<xref ref-type="fn" rid="Fn6">6</xref> subdatasets, and can be obtained on demand. The repository constitutes an automatically reproducible research object, and readers interested in verifying the results and claims of our paper can recompute and plot all results with a single command after cloning the repository.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par26">
        <ext-link ext-link-type="uri" xlink:href="http://www.github.com/richardandersson/EyeMovementDetectorEvaluation">github.com/richardandersson/EyeMovementDetectorEvaluation</ext-link>
      </p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p id="Par46">
        <ext-link ext-link-type="uri" xlink:href="https://www.michaeldorr.de/smoothpursuit/">https://www.michaeldorr.de/smoothpursuit/</ext-link>
      </p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p id="Par49">
        <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/remodnav">https://pypi.org/project/remodnav</ext-link>
      </p>
    </fn>
    <fn id="Fn4">
      <label>4</label>
      <p id="Par50">
        <ext-link ext-link-type="uri" xlink:href="https://github.com/psychoinformatics-de/remodnav">https://github.com/psychoinformatics-de/remodnav</ext-link>
      </p>
    </fn>
    <fn id="Fn5">
      <label>5</label>
      <p id="Par51">
        <ext-link ext-link-type="uri" xlink:href="https://github.com/psychoinformatics-de/paper-remodnav/">https://github.com/psychoinformatics-de/paper-remodnav/</ext-link>
      </p>
    </fn>
    <fn id="Fn6">
      <label>6</label>
      <p id="Par52">
        <ext-link ext-link-type="uri" xlink:href="http://datalad.org">http://datalad.org</ext-link>
      </p>
    </fn>
    <fn>
      <p>Asim H. Dar and Adina S. Wagner contributed equally to this work</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>
        <bold>Grant information</bold>
      </p>
      <p>Michael Hanke was supported by funds from the German federal state of Saxony-Anhalt and the European Regional Development Fund (ERDF), Project: Center for Behavioral Brain Sciences (CBBS). Adina Wagner was supported by the German Academic Foundation.</p>
      <p>
        <italic>The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</italic>
      </p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This work is based on an earlier Python implementation and evaluation of the original NH algorithm by Ulrike Schnaithmann and Isabel Dombrowe (Schnaithman, <xref ref-type="bibr" rid="CR40">2017</xref>). We are grateful to Andersson et al., (<xref ref-type="bibr" rid="CR2">2017</xref>) for releasing the labeled eye-tracking dataset used for validation under an open-source license.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author Contributions</title>
    <p>AD, MH conceived and implemented the algorithm. AD, AW, MH validated algorithm performance. AD, AW, MH wrote the manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding Information</title>
    <p>Open Access funding provided by Projekt DEAL.</p>
  </notes>
  <notes>
    <title>Compliance with Ethical Standards</title>
    <notes id="FPar1" notes-type="COI-statement">
      <title>
        <bold>Competing interests</bold>
      </title>
      <p id="Par53">There are no competing interests to disclose.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Amit</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Abeles</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bar-Gad</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Yuval-Greenberg</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Temporal dynamics of saccades explained by a self-paced process</article-title>
        <source>Scientific Reports</source>
        <year>2017</year>
        <volume>7</volume>
        <issue>1</issue>
        <fpage>886</fpage>
        <pub-id pub-id-type="doi">10.1038/s41598-017-00881-7</pub-id>
        <?supplied-pmid 28428540?>
        <pub-id pub-id-type="pmid">28428540</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Andersson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Larsson</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Holmqvist</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Stridh</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>One algorithm to rule them all? An evaluation and discussion of ten eye movement event-detection algorithms</article-title>
        <source>Behavior Research Methods</source>
        <year>2017</year>
        <volume>49</volume>
        <issue>2</issue>
        <fpage>616</fpage>
        <lpage>637</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-016-0738-9</pub-id>
        <?supplied-pmid 27193160?>
        <pub-id pub-id-type="pmid">27193160</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bahill</surname>
            <given-names>AT</given-names>
          </name>
          <name>
            <surname>Clark</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Stark</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>The main sequence, a tool for studying human eye movements</article-title>
        <source>Mathematical Biosciences</source>
        <year>1975</year>
        <volume>24</volume>
        <issue>3-4</issue>
        <fpage>191</fpage>
        <lpage>204</lpage>
        <pub-id pub-id-type="doi">10.1016/0025-5564(75)90075-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carl</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Gellman</surname>
            <given-names>RS</given-names>
          </name>
        </person-group>
        <article-title>Human smooth pursuit: stimulus-dependent responses</article-title>
        <source>Journal of Neurophysiology</source>
        <year>1987</year>
        <volume>57</volume>
        <issue>5</issue>
        <fpage>1446</fpage>
        <lpage>1463, pMID: 3585475</lpage>
        <pub-id pub-id-type="doi">10.1152/jn.1987.57.5.1446</pub-id>
        <?supplied-pmid 3585475?>
        <pub-id pub-id-type="pmid">3585475</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cherici</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Kuang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Poletti</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Rucci</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Precision of sustained fixation in trained and untrained observers</article-title>
        <source>Journal of Vision</source>
        <year>2012</year>
        <volume>12</volume>
        <issue>6</issue>
        <fpage>31</fpage>
        <lpage>31</lpage>
        <pub-id pub-id-type="doi">10.1167/12.6.31</pub-id>
        <?supplied-pmid 22728680?>
        <pub-id pub-id-type="pmid">22728680</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choe</surname>
            <given-names>KW</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>SH</given-names>
          </name>
        </person-group>
        <article-title>Pupil size dynamics during fixation impact the accuracy and precision of video-based gaze estimation</article-title>
        <source>Vision Research</source>
        <year>2016</year>
        <volume>118</volume>
        <fpage>48</fpage>
        <lpage>59</lpage>
        <pub-id pub-id-type="doi">10.1016/j.visres.2014.12.018</pub-id>
        <pub-id pub-id-type="pmid">25578924</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A coefficient of agreement for nominal scales</article-title>
        <source>Educational and Psychological Measurement</source>
        <year>1960</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>37</fpage>
        <lpage>46</lpage>
        <pub-id pub-id-type="doi">10.1177/001316446002000104</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dalveren</surname>
            <given-names>GGM</given-names>
          </name>
          <name>
            <surname>Cagiltay</surname>
            <given-names>NE</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of ten open-source eye-movement classification algorithms in simulated surgical scenarios</article-title>
        <source>IEEE Access</source>
        <year>2019</year>
        <volume>7</volume>
        <fpage>161794</fpage>
        <lpage>161804</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2951506</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dorr</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Martinetz</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Gegenfurtner</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Barth</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Variability of eye movements when viewing dynamic natural scenes</article-title>
        <source>Journal of Vision</source>
        <year>2010</year>
        <volume>10</volume>
        <issue>10</issue>
        <fpage>28</fpage>
        <lpage>28</lpage>
        <pub-id pub-id-type="doi">10.1167/10.10.28</pub-id>
        <?supplied-pmid 20884493?>
        <pub-id pub-id-type="pmid">20884493</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Rigas</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Abdulin</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Komogortsev</surname>
            <given-names>OV</given-names>
          </name>
        </person-group>
        <article-title>A novel evaluation of two related and two independent algorithms for eye movement classification during reading</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>4</issue>
        <fpage>1374</fpage>
        <lpage>1397</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-018-1050-7</pub-id>
        <?supplied-pmid 29766396?>
        <pub-id pub-id-type="pmid">29766396</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goltz</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Irving</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Steinbach</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Eizenman</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Vertical eye position control in darkness: orbital position and body orientation interact to modulate drift velocity</article-title>
        <source>Vision Research</source>
        <year>1997</year>
        <volume>37</volume>
        <issue>6</issue>
        <fpage>789</fpage>
        <lpage>798</lpage>
        <pub-id pub-id-type="doi">10.1016/S0042-6989(96)00217-9</pub-id>
        <?supplied-pmid 9156224?>
        <pub-id pub-id-type="pmid">9156224</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gordon</surname>
            <given-names>PC</given-names>
          </name>
          <name>
            <surname>Hendrick</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Similarity-based interference during language comprehension: evidence from eye tracking during reading</article-title>
        <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>
        <year>2006</year>
        <volume>32</volume>
        <issue>6</issue>
        <fpage>1304</fpage>
        <lpage>1321</lpage>
        <pub-id pub-id-type="doi">10.1037/0278-7393.32.6.1304</pub-id>
        <?supplied-pmid 17087585?>
        <pub-id pub-id-type="pmid">17087585</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gorgolewski</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Auer</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Calhoun</surname>
            <given-names>VD</given-names>
          </name>
          <name>
            <surname>Craddock</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Duff</surname>
            <given-names>EP</given-names>
          </name>
          <name>
            <surname>Flandin</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ghosh</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Glatard</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Halchenko</surname>
            <given-names>YO</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title>
        <source>Scientific Data</source>
        <year>2016</year>
        <volume>3</volume>
        <fpage>160044</fpage>
        <pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id>
        <?supplied-pmid 27326542?>
        <pub-id pub-id-type="pmid">27326542</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <mixed-citation publication-type="other">Halchenko, Y.O., Hanke, M., &amp; et al. (2018). DataLad: perpetual decentralized management of digital objects. 10.5281/zenodo.1470735, <ext-link ext-link-type="uri" xlink:href="http://datalad.org">http://datalad.org</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR15">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hanke</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Adelhöfer</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kottke</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Iacovella</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Sengupta</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kaule</surname>
            <given-names>FR</given-names>
          </name>
          <name>
            <surname>Nigbur</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Waite</surname>
            <given-names>AQ</given-names>
          </name>
          <name>
            <surname>Baumgartner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Stadler</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A studyforrest extension, simultaneous fMRI and eye gaze recordings during prolonged natural stimulation</article-title>
        <source>Scientific Data</source>
        <year>2016</year>
        <volume>3</volume>
        <fpage>160092</fpage>
        <pub-id pub-id-type="doi">10.1038/sdata.2016.92</pub-id>
        <?supplied-pmid 27779621?>
        <pub-id pub-id-type="pmid">27779621</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <mixed-citation publication-type="other">Hanke, M., Dar, A.H., &amp; Wagner, A. (2019). Psychoinformatics-de/remodnav: submission time. 10.5281/zenodo.2651042.</mixed-citation>
    </ref>
    <ref id="CR17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hannula</surname>
            <given-names>DE</given-names>
          </name>
          <name>
            <surname>Althoff</surname>
            <given-names>RR</given-names>
          </name>
          <name>
            <surname>Warren</surname>
            <given-names>DE</given-names>
          </name>
          <name>
            <surname>Riggs</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>NJ</given-names>
          </name>
          <name>
            <surname>Ryan</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>Worth a glance: using eye movements to investigate the cognitive neuroscience of memory</article-title>
        <source>Frontiers in Human Neuroscience</source>
        <year>2010</year>
        <volume>4</volume>
        <fpage>166</fpage>
        <pub-id pub-id-type="doi">10.3389/fnhum.2010.00166</pub-id>
        <?supplied-pmid 21151363?>
        <pub-id pub-id-type="pmid">21151363</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Harris</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Young</surname>
            <given-names>AW</given-names>
          </name>
          <name>
            <surname>Andrews</surname>
            <given-names>TJ</given-names>
          </name>
        </person-group>
        <article-title>Dynamic stimuli demonstrate a categorical representation of facial expression in the amygdala</article-title>
        <source>Neuropsychologia</source>
        <year>2014</year>
        <volume>56</volume>
        <issue>100</issue>
        <fpage>47</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.01.005</pub-id>
        <?supplied-pmid 24447769?>
        <pub-id pub-id-type="pmid">24447769</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hessels</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Niehorster</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Kemner</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hooge</surname>
            <given-names>IT</given-names>
          </name>
        </person-group>
        <article-title>Noise-robust fixation detection in eye movement data: identification by two-means clustering (i2mc)</article-title>
        <source>Behavior Research Methods</source>
        <year>2017</year>
        <volume>49</volume>
        <issue>5</issue>
        <fpage>1802</fpage>
        <lpage>1823</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-016-0822-1</pub-id>
        <pub-id pub-id-type="pmid">27800582</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hessels</surname>
            <given-names>RS</given-names>
          </name>
          <name>
            <surname>Niehorster</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Andersson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hooge</surname>
            <given-names>IT</given-names>
          </name>
        </person-group>
        <article-title>Is the eye-movement field confused about fixations and saccades? A survey among 124 researchers</article-title>
        <source>Royal Society Open Science</source>
        <year>2018</year>
        <volume>5</volume>
        <issue>8</issue>
        <fpage>180502</fpage>
        <pub-id pub-id-type="doi">10.1098/rsos.180502</pub-id>
        <pub-id pub-id-type="pmid">30225041</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <mixed-citation publication-type="other">Holmqvist, K., Nyström, M., Andersson, R., Dewhurst, R., Jarodzka, H., &amp; Van de Weijer, J. (2011). Eye tracking: a comprehensive guide to methods and measures. OUP Oxford.</mixed-citation>
    </ref>
    <ref id="CR22">
      <mixed-citation publication-type="other">Holmqvist, K., Nyström, M., &amp; Mulvey, F. (2012). Eye tracker data quality. In <italic>Proceedings of the symposium on eye tracking research and applications - ETRA ’12</italic>. 10.1145/2168556.2168563 (p. 45). New York: ACM Press.</mixed-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hooge</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Holmqvist</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>The pupil is faster than the corneal reflection (CR): are video-based pupil-CR eye trackers suitable for studying detailed dynamics of eye movements?</article-title>
        <source>Vision Research</source>
        <year>2016</year>
        <volume>128</volume>
        <fpage>6</fpage>
        <lpage>18</lpage>
        <pub-id pub-id-type="doi">10.1016/j.visres.2016.09.002</pub-id>
        <pub-id pub-id-type="pmid">27656785</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hooge</surname>
            <given-names>ITC</given-names>
          </name>
          <name>
            <surname>Niehorster</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Andersson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hessels</surname>
            <given-names>RS</given-names>
          </name>
        </person-group>
        <article-title>Is human classification by experienced untrained observers a gold standard in fixation detection?</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>5</issue>
        <fpage>1864</fpage>
        <lpage>1881</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-017-0955-x</pub-id>
        <?supplied-pmid 29052166?>
        <pub-id pub-id-type="pmid">29052166</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hunter</surname>
            <given-names>JD</given-names>
          </name>
        </person-group>
        <article-title>Matplotlib: a 2D graphics environment</article-title>
        <source>Computing in Science &amp; Engineering</source>
        <year>2007</year>
        <volume>9</volume>
        <issue>3</issue>
        <fpage>90</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jaccard</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Étude comparative de la distribution florale dans une portion des alpes et des jura</article-title>
        <source>Bull Soc Vaudoise Sci Nat</source>
        <year>1901</year>
        <volume>37</volume>
        <fpage>547</fpage>
        <lpage>579</lpage>
      </element-citation>
    </ref>
    <ref id="CR27">
      <mixed-citation publication-type="other">Jones, E., Oliphant, T., Peterson, P., &amp; et al. (2001). SciPy: open source scientific tools for Python. <ext-link ext-link-type="uri" xlink:href="http://www.scipy.org">http://www.scipy.org</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR28">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Komogortsev</surname>
            <given-names>OV</given-names>
          </name>
          <name>
            <surname>Karpov</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Automated classification and scoring of smooth pursuit eye movements in the presence of fixations and saccades</article-title>
        <source>Behavior Research Methods</source>
        <year>2013</year>
        <volume>45</volume>
        <issue>1</issue>
        <fpage>203</fpage>
        <lpage>215</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-012-0234-9</pub-id>
        <?supplied-pmid 22806708?>
        <pub-id pub-id-type="pmid">22806708</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Komogortsev</surname>
            <given-names>OV</given-names>
          </name>
          <name>
            <surname>Gobert</surname>
            <given-names>DV</given-names>
          </name>
          <name>
            <surname>Jayarathna</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Koh</surname>
            <given-names>DH</given-names>
          </name>
          <name>
            <surname>Gowda</surname>
            <given-names>SM</given-names>
          </name>
        </person-group>
        <article-title>Standardization of automated analyses of oculomotor fixation and saccadic behaviors</article-title>
        <source>IEEE Transactions on Biomedical Engineering</source>
        <year>2010</year>
        <volume>57</volume>
        <issue>11</issue>
        <fpage>2635</fpage>
        <lpage>2645</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2010.2057429</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Larsson</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Stridh</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Detection of saccades and postsaccadic oscillations in the presence of smooth pursuit</article-title>
        <source>IEEE Transactions on Biomedical Engineering</source>
        <year>2013</year>
        <volume>60</volume>
        <issue>9</issue>
        <fpage>2484</fpage>
        <lpage>2493</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2013.2258918</pub-id>
        <?supplied-pmid 23625350?>
        <pub-id pub-id-type="pmid">23625350</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Larsson</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Andersson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Stridh</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Detection of fixations and smooth pursuit movements in high-speed eye-tracking data</article-title>
        <source>Biomedical Signal Processing and Control</source>
        <year>2015</year>
        <volume>18</volume>
        <fpage>145</fpage>
        <lpage>152</lpage>
        <pub-id pub-id-type="doi">10.1016/j.bspc.2014.12.008</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Heynderickx</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Visual attention in objective image quality assessment: based on eye-tracking data</article-title>
        <source>IEEE Transactions on Circuits and Systems for Video Technology</source>
        <year>2011</year>
        <volume>21</volume>
        <issue>7</issue>
        <fpage>971</fpage>
        <lpage>982</lpage>
        <pub-id pub-id-type="doi">10.1109/TCSVT.2011.2133770</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maguire</surname>
            <given-names>EA</given-names>
          </name>
        </person-group>
        <article-title>Studying the freely-behaving brain with fMRI</article-title>
        <source>NeuroImage</source>
        <year>2012</year>
        <volume>62</volume>
        <issue>2</issue>
        <fpage>1170</fpage>
        <lpage>1176</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.009</pub-id>
        <pub-id pub-id-type="pmid">22245643</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <mixed-citation publication-type="other">Mathe, S., &amp; Sminchisescu, C. (2012). Dynamic eye movement datasets and learnt saliency models for visual action recognition. In <italic>Proceedings, Part II, of the 12th European conference on computer vision — ECCV 2012</italic>, (Vol. 7573 pp. 842–856). Berlin: Springer.</mixed-citation>
    </ref>
    <ref id="CR35">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Matusz</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Dikker</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Huth</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Perrodin</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Are we ready for real-world neuroscience?</article-title>
        <source>Journal of Cognitive Neuroscience</source>
        <year>2019</year>
        <volume>31</volume>
        <issue>3</issue>
        <fpage>327</fpage>
        <lpage>338, pMID: 29916793</lpage>
        <pub-id pub-id-type="doi">10.1162/jocn_e_01276</pub-id>
        <?supplied-pmid 29916793?>
        <pub-id pub-id-type="pmid">29916793</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <mixed-citation publication-type="other">McKinney, W., et al. (2010). Data structures for statistical computing in Python. In <italic>Proceedings of the 9th Python in science conference, Austin, TX</italic>, (Vol. 445 pp. 51–56).</mixed-citation>
    </ref>
    <ref id="CR37">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nyström</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Holmqvist</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>An adaptive algorithm for fixation, saccade, and glissade detection in eyetracking data</article-title>
        <source>Behavior Research Methods</source>
        <year>2010</year>
        <volume>42</volume>
        <issue>1</issue>
        <fpage>188</fpage>
        <lpage>204</lpage>
        <pub-id pub-id-type="doi">10.3758/BRM.42.1.188</pub-id>
        <?supplied-pmid 20160299?>
        <pub-id pub-id-type="pmid">20160299</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <mixed-citation publication-type="other">Oliphant, T.E. (2006). <italic>A guide to NumPy</italic>, vol 1. Trelgol Publishing USA.</mixed-citation>
    </ref>
    <ref id="CR39">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van Renswoude</surname>
            <given-names>DR</given-names>
          </name>
          <name>
            <surname>Raijmakers</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Koornneef</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Hunnius</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Visser</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Gazepath: an eye-tracking analysis tool that accounts for individual differences and data quality</article-title>
        <source>Behavior Research Methods</source>
        <year>2018</year>
        <volume>50</volume>
        <issue>2</issue>
        <fpage>834</fpage>
        <lpage>852</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-017-0909-3</pub-id>
        <pub-id pub-id-type="pmid">28593606</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <mixed-citation publication-type="other">Schnaithman, U. (2017). Combining and testing filter and detection algorithms for post-experimental analysis of eye tracking data on dynamic stimuli. B.Sc thesis submitted to the faculty of natural sciences at the Otto von Guericke University, Magdeburg, Germany.</mixed-citation>
    </ref>
    <ref id="CR41">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schutz</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Braun</surname>
            <given-names>DI</given-names>
          </name>
          <name>
            <surname>Gegenfurtner</surname>
            <given-names>KR</given-names>
          </name>
        </person-group>
        <article-title>Eye movements and perception: a selective review</article-title>
        <source>Journal of Vision</source>
        <year>2011</year>
        <volume>11</volume>
        <issue>5</issue>
        <fpage>9</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1167/11.5.9</pub-id>
        <?supplied-pmid 21917784?>
        <pub-id pub-id-type="pmid">21917784</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <mixed-citation publication-type="other">Seabold, S., &amp; Perktold, J. (2010). Statsmodels: econometric and statistical modeling with python. In <italic>9th Python in science conference</italic>.</mixed-citation>
    </ref>
    <ref id="CR43">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stampe</surname>
            <given-names>DM</given-names>
          </name>
        </person-group>
        <article-title>Heuristic filtering and reliable calibration methods for video-based pupil-tracking systems</article-title>
        <source>Behavior Research Methods, Instruments, &amp; Computers</source>
        <year>1993</year>
        <volume>25</volume>
        <issue>2</issue>
        <fpage>137</fpage>
        <lpage>142</lpage>
        <pub-id pub-id-type="doi">10.3758/BF03204486</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <mixed-citation publication-type="other">Startsev, M., Agtzidis, I., &amp; Dorr, M. (2018). 1D CNN with BLSTM for automated classification of fixations, saccades, and smooth pursuits. <italic>Behavior Research Methods</italic>10.3758/s13428-018-1144-2.</mixed-citation>
    </ref>
    <ref id="CR45">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tagliazucchi</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Laufs</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Decoding wakefulness levels from typical fMRI resting-state data reveals reliable drifts between wakefulness and sleep</article-title>
        <source>Neuron</source>
        <year>2014</year>
        <volume>82</volume>
        <issue>3</issue>
        <fpage>695</fpage>
        <lpage>708</lpage>
        <pub-id pub-id-type="doi">10.1016/j.neuron.2014.03.020</pub-id>
        <pub-id pub-id-type="pmid">24811386</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tikka</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Väljamäe</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>de Borst</surname>
            <given-names>AW</given-names>
          </name>
          <name>
            <surname>Pugliese</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ravaja</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Kaipainen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Takala</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Enactive cinema paves way for understanding complex real-time social interaction in neuroimaging experiments</article-title>
        <source>Frontiers in Human Neuroscience</source>
        <year>2012</year>
        <volume>6</volume>
        <fpage>298</fpage>
        <pub-id pub-id-type="doi">10.3389/fnhum.2012.00298</pub-id>
        <?supplied-pmid 23125829?>
        <pub-id pub-id-type="pmid">23125829</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Toiviainen</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Alluri</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Brattico</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Wallentin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Vuust</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Capturing the musical brain with Lasso: dynamic decoding of musical features from fMRI data</article-title>
        <source>NeuroImage</source>
        <year>2014</year>
        <volume>88</volume>
        <fpage>170</fpage>
        <lpage>180</lpage>
        <pub-id pub-id-type="doi">10.1016/J.NEUROIMAGE.2013.11.017</pub-id>
        <?supplied-pmid 24269803?>
        <pub-id pub-id-type="pmid">24269803</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <mixed-citation publication-type="other">Zemblys, R., Niehorster, D.C., &amp; Holmqvist, K. (2018). gazeNet: end-to-end eye-movement event detection with deep neural networks. <italic>Behavior Research Methods</italic>10.3758/s13428-018-1133-5.</mixed-citation>
    </ref>
  </ref-list>
</back>
