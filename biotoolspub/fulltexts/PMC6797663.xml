<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6797663</article-id>
    <article-id pub-id-type="publisher-id">1285</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-019-01285-3</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Obtaining psychological embeddings through joint kernel and metric learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Roads</surname>
          <given-names>Brett D.</given-names>
        </name>
        <address>
          <phone>+1 720-205-2971</phone>
          <email>brett.roads@colorado.edu</email>
          <email>b.roads@ucl.ac.uk</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Mozer</surname>
          <given-names>Michael C.</given-names>
        </name>
        <address>
          <email>mozer@colorado.edu</email>
          <email>mcmozer@google.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.266190.a</institution-id><institution-id institution-id-type="ISNI">0000000096214564</institution-id><institution>Department of Computer Science, </institution><institution>University of Colorado Boulder, </institution></institution-wrap>Boulder, CO 80309-0430 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.83440.3b</institution-id><institution-id institution-id-type="ISNI">0000000121901201</institution-id><institution>Present Address: Department Experimental Psychology, </institution><institution>University College London, </institution></institution-wrap>26 Bedford Way, London, WC1H 0AP UK </aff>
      <aff id="Aff3"><label>3</label>Present Address: Google Brain, 1600 Amphitheater Parkway, Mountain View, CA 94304 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>20</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>8</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2019</year>
    </pub-date>
    <volume>51</volume>
    <issue>5</issue>
    <fpage>2180</fpage>
    <lpage>2193</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Psychological embeddings provide a powerful formalism for characterizing human-perceived similarity among members of a stimulus set. Obtaining high-quality embeddings can be costly due to algorithm design, software deployment, and participant compensation. This work aims to advance state-of-the-art embedding techniques and provide a comprehensive software package that makes obtaining high-quality psychological embeddings both easy and relatively efficient. Contributions are made on four fronts. First, the embedding procedure allows multiple trial configurations (e.g., triplets) to be used for collecting similarity judgments from participants. For example, trials can be configured to collect triplet comparisons or to sort items into groups. Second, a likelihood model is provided for three classes of similarity kernels allowing users to easily infer the parameters of their preferred model using gradient descent. Third, an active selection algorithm is provided that makes data collection more efficient by proposing comparisons that provide the strongest constraints on the embedding. Fourth, the likelihood model allows the specification of group-specific attention weight parameters. A series of experiments are included to highlight each of these contributions and their impact on converging to a high-quality embedding. Collectively, these incremental improvements provide a powerful and complete set of tools for inferring psychological embeddings. The relevant tools are available as the Python package <italic>PsiZ</italic>, which can be cloned from GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/roads/psiz">https://github.com/roads/psiz</ext-link>).</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Cognitive modeling</kwd>
      <kwd>Similarity kernel</kwd>
      <kwd>Psychological embedding</kwd>
      <kwd>Active learning</kwd>
      <kwd>Python package</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Psychonomic Society, Inc. 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">In many interactive software systems, it is essential to model an individual’s behavior during a perceptual task. Decision support applications anticipate and adjust for novice perception in order to help novice users arrive at expert-like categorization decisions (Fang &amp; Geman, <xref ref-type="bibr" rid="CR5">2005</xref>; Ferecatu &amp; Geman, <xref ref-type="bibr" rid="CR6">2009</xref>; Roads &amp; Mozer, <xref ref-type="bibr" rid="CR23">2017</xref>). Human-in-the-loop computer vision algorithms utilize a model of human similarity to improve machine categorization performance (Wah et al., <xref ref-type="bibr" rid="CR39">2014</xref>). Human category-training applications use cognitive models to predict learning outcomes (e.g., Nosofsky <xref ref-type="bibr" rid="CR20">1986</xref>; Kruschke <xref ref-type="bibr" rid="CR11">1992</xref>; Love, Medin, &amp; Gureckis, <xref ref-type="bibr" rid="CR15">2004</xref>; Nosofsky, Sanders, &amp; McDaniel, <xref ref-type="bibr" rid="CR21">2018</xref>). At the core of these applications is the notion of stimulus feature representations and psychological similarity.</p>
    <p id="Par3">The primary objective of this work is to provide a method for <italic>jointly</italic> inferring a multi-dimensional feature representation and a corresponding similarity function. Given a feature representation, a similarity function specifies the degree that responses associated with one stimulus transfer to another (Shepard, <xref ref-type="bibr" rid="CR30">1987</xref>; Nosofsky, <xref ref-type="bibr" rid="CR20">1986</xref>; Tenenbaum, <xref ref-type="bibr" rid="CR34">1999</xref>). The more similar stimuli are, the more likely generalization is to occur. Similarity is based not on external properties of the stimuli, but rather on an individual’s internal representation. We refer to this internal representation—coupled with a similarity function—as a <italic>psychological embedding</italic>.</p>
    <p id="Par4">A mature set of algorithms exists that uses proximity data (e.g., pair-wise similarity ratings) to infer a low-dimensional embedding (Gower, <xref ref-type="bibr" rid="CR7">1966</xref>; Torgerson, <xref ref-type="bibr" rid="CR36">1958</xref>). These <italic>multidimensional scaling</italic> or<italic>MDS</italic> algorithms can be classified based on two properties: the type of observations used to support inference and the form of the similarity function. In psychology, two dominant types of observations are similarity (dissimilarity) <italic>ratings</italic> (e.g., Torgerson <xref ref-type="bibr" rid="CR35">1952</xref>, <xref ref-type="bibr" rid="CR36">1958</xref>) and ordinal similarity <italic>rankings</italic> (e.g., Agarwal et al., <xref ref-type="bibr" rid="CR2">2007</xref>; Tamuz, Liu, Belongie, Shamir, &amp; Kalai, <xref ref-type="bibr" rid="CR32">2011</xref>; van der Maaten &amp; Weinberger <xref ref-type="bibr" rid="CR37">2012</xref>). Similarity ratings are elicited by asking participants to rate the similarity between pairs of stimuli on a predefined scale. Similarity rankings are elicited by asking participants to rank stimuli in order of similarity; typically by presenting participants with triplets and asking them to select the pair that is most similar. This work focuses on similarity rankings given their benefits of rater consistency (Demiralp, Bernstein, &amp; Heer, <xref ref-type="bibr" rid="CR4">2014</xref>), subject-specific precision (Li, Malave, Song, &amp; Yu, <xref ref-type="bibr" rid="CR14">2016</xref>), and cost-effective scalability (Wilber, Kwak, &amp; Belongie, <xref ref-type="bibr" rid="CR40">2014</xref>).</p>
    <p id="Par5">The second property of MDS algorithms concerns the form of the similarity function. The similarity function specifies how distance in the embedding space translates to psychological similarity. The least-constrained form assumes only that similarity decays monotonically as a function of distance (Shepard <xref ref-type="bibr" rid="CR27">1962a</xref>, <xref ref-type="bibr" rid="CR28">1962b</xref>; Kruskal <xref ref-type="bibr" rid="CR12">1968a</xref>, <xref ref-type="bibr" rid="CR13">1968b</xref>). While being the most flexible, such a similarity function has an increased risk of over-fitting and discounts existing psychological research on stimulus generalization. At the other extreme, similarity is defined as a fixed, parameter- free function (Gower, <xref ref-type="bibr" rid="CR7">1966</xref>; Torgerson, <xref ref-type="bibr" rid="CR36">1958</xref>). In between these two extremes, similarity is specified as a function with one or more free parameters (Shepard, <xref ref-type="bibr" rid="CR25">1957</xref>; Nosofsky, <xref ref-type="bibr" rid="CR19">1985</xref>; Tamuz et al., <xref ref-type="bibr" rid="CR32">2011</xref>; van der Maaten &amp; Weinberger, <xref ref-type="bibr" rid="CR37">2012</xref>). Our work is also situated in between these two extremes, and continues a tradition of using parameterized similarity functions that are well motivated by psychological theory (Shepard <xref ref-type="bibr" rid="CR25">1957</xref>, <xref ref-type="bibr" rid="CR26">1958</xref>; Nosofsky <xref ref-type="bibr" rid="CR19">1985</xref>, <xref ref-type="bibr" rid="CR20">1986</xref>).</p>
    <p id="Par6">The primary purpose of this work is to provide a unified set of state-of-the-art tools for individuals interested in inferring psychological embeddings. These tools are collected in a Python package called PsiZ, short-hand for psychological embedding. The Greek letter <italic>Ψ</italic> is commonly associated with psychology, while <bold><italic>z</italic></bold> is often used in machine learning to denote a latent feature vector, and <bold><italic>Z</italic></bold> ≡{<bold><italic>z</italic></bold><sub>1</sub>,...,<bold><italic>z</italic></bold><sub><italic>n</italic></sub>} is a matrix containing a collection of vectors. PsiZ unites four facets that can be adjusted to suit the needs of the user. First, observations used for inference can be collected using a variety of different trial configurations (e.g., ranking, clustering). Second, a number of different similarity functions can be used for performing inference, some derived from psychological research and others widely used in machine learning. Additional similarity functions can easily be implemented by the user. Third, the embedding algorithm can be used to infer group-specific attention weights in the same spirit as INDSCAL (Carroll &amp; Chang, <xref ref-type="bibr" rid="CR3">1970</xref>). Lastly, high-quality embeddings can be constructed with less data via an active-selection algorithm that intelligently determines which stimulus comparisons should be collected next. The active-selection algorithm is similar to the capabilities provided by the powerful NEXT system (Jamieson, Jain, Fernandez, Glattard, &amp; Nowak, <xref ref-type="bibr" rid="CR8">2015</xref>; Rau, Mason, &amp; Nowak, <xref ref-type="bibr" rid="CR22">2016</xref>; Sievert et al., <xref ref-type="bibr" rid="CR31">2017</xref>). Unlike the more general-purpose NEXT system, PsiZ focuses exclusively on psychological embeddings and aims to provide a comprehensive set of features, classes, and utilities. Each of these facets is described in turn, followed by experiments highlighting the potential benefits to researchers. The relevant tools are available as the Python package <italic>PsiZ</italic>, which can be cloned from <ext-link ext-link-type="uri" xlink:href="https://github.com/roads/psiz">https://github.com/roads/psiz</ext-link>. The code used to run the experiments can be cloned from <ext-link ext-link-type="uri" xlink:href="https://github.com/roads/psiz-brm">https://github.com/roads/psiz-brm</ext-link>.</p>
  </sec>
  <sec id="Sec2">
    <title>Data collection</title>
    <p id="Par7">Inference of a psychological embedding requires a set of observations, or judged trials. On each trial, subjects compare a <italic>query</italic> stimulus to a set of <italic>reference</italic> images <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {R}$\end{document}</tex-math><mml:math id="M2"><mml:mi mathvariant="script">R</mml:mi></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq1.gif"/></alternatives></inline-formula>. In the simplest case, a trial contains two reference images and participants must select the reference image they believe is most similar to the query (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a). In addition to this basic trial configuration, PsiZ is designed to work with more complicated configurations—like those that have become popular in the machine learning literature (e.g., Wilber et al.,<xref ref-type="bibr" rid="CR40">2014</xref>; Wah et al., <xref ref-type="bibr" rid="CR39">2014</xref>).
<fig id="Fig1"><label>Fig. 1</label><caption><p>Sample displays shown to subjects. The <italic>center image</italic> is the query stimulus while the surrounding images are the reference stimuli. <bold>a</bold> Given two reference images, subjects select the one reference image that is most similar to the query. <bold>b</bold> Given eight reference images, subjects select the two reference images that are most similar to the query. <bold>c</bold> Given eight reference images, subjects select two reference images in the order of their similarity</p></caption><graphic xlink:href="13428_2019_1285_Fig1_HTML" id="MO1"/></fig></p>
    <p id="Par8">Let us consider all the different trial configurations that PsiZ can handle. Since PsiZ allows the configuration to switch on each trial, we denote the configuration parameters of the <italic>i</italic> th trial using the subscript <italic>i</italic>. The set of references <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {R}_{i}$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq2.gif"/></alternatives></inline-formula> can contain 2-8 stimuli. Given a set of references, subjects select a predefined number of reference stimuli that they consider most similar to the query. The set of selected reference images <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {S}_{i}$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq3.gif"/></alternatives></inline-formula>, may be 1 to <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {R}_{i}}|-1$\end{document}</tex-math><mml:math id="M8"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq4.gif"/></alternatives></inline-formula>. The last configuration parameter determines whether subjects must rank their selection. In the basic case, where <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {R}_{i}}|=2$\end{document}</tex-math><mml:math id="M10"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq5.gif"/></alternatives></inline-formula> and <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {S}_{i}}|=1$\end{document}</tex-math><mml:math id="M12"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq6.gif"/></alternatives></inline-formula>, subjects provide triplet similarity judgments (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a). In a more complicated scenario, subjects can partition the set of references into a similar and dissimilar group (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b). Alternatively, subjects may be asked to rank their selections (Fig. <xref rid="Fig1" ref-type="fig">1</xref>c).</p>
    <p id="Par9">In order to simplify the description of the embedding models—as well as the actual code implementation—all observations are assumed to be notated in a specific way. The set of all observations is denoted by <inline-formula id="IEq7"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {D}$\end{document}</tex-math><mml:math id="M14"><mml:mi mathvariant="script">D</mml:mi></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq7.gif"/></alternatives></inline-formula>. Assuming that each stimulus has been assigned a unique index, <inline-formula id="IEq8"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {R}_{i}$\end{document}</tex-math><mml:math id="M16"><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq8.gif"/></alternatives></inline-formula> and <inline-formula id="IEq9"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {S}_{i}$\end{document}</tex-math><mml:math id="M18"><mml:msub><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq9.gif"/></alternatives></inline-formula> are sets of stimulus indices on trial <italic>i</italic>. Each judged trial collects one observation. Response information for trial <italic>i</italic>, denoted <inline-formula id="IEq10"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {D}_{i}$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq10.gif"/></alternatives></inline-formula>, is coded as a triple, <inline-formula id="IEq11"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {D}_{i} = \left (q_{i}, \boldsymbol {s}_{i}, \boldsymbol {u}_{i}\right )$\end{document}</tex-math><mml:math id="M22"><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq11.gif"/></alternatives></inline-formula>, where <italic>q</italic><sub><italic>i</italic></sub> indicates the index of the query stimulus, <bold><italic>s</italic></bold><sub><italic>i</italic></sub> is a row vector comprised of the selected reference stimulus indices, and <bold><italic>u</italic></bold><sub><italic>i</italic></sub> is a row vector comprised of the unselected reference stimulus indices. Depending on the trial configuration, the length of <inline-formula id="IEq12"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {D}_{i}$\end{document}</tex-math><mml:math id="M24"><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq12.gif"/></alternatives></inline-formula> will vary. For example, if <inline-formula id="IEq13"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {R}_{i}}|=2$\end{document}</tex-math><mml:math id="M26"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq13.gif"/></alternatives></inline-formula> and <inline-formula id="IEq14"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {S}_{i}}|=1$\end{document}</tex-math><mml:math id="M28"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq14.gif"/></alternatives></inline-formula>, <inline-formula id="IEq15"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {D}_{i}=\left (q_{i}, a_{i}, b_{i} \right )$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq15.gif"/></alternatives></inline-formula>, where <italic>a</italic><sub><italic>i</italic></sub> indicates the index of the selected reference stimulus and <italic>b</italic><sub><italic>i</italic></sub> indicates the index of the unselected reference stimulus. If <inline-formula id="IEq16"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {R}_{i}}|=8$\end{document}</tex-math><mml:math id="M32"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq16.gif"/></alternatives></inline-formula>, <inline-formula id="IEq17"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {S}_{i}}|=2$\end{document}</tex-math><mml:math id="M34"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq17.gif"/></alternatives></inline-formula> and subjects make ranked selections, then <inline-formula id="IEq18"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {D}_{i}=\left (q_{i}, a_{i}, b_{i}, c_{i}, d_{i}, e_{i}, f_{i}, g_{i}, h_{i} \right )$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq18.gif"/></alternatives></inline-formula>. Now <italic>a</italic><sub><italic>i</italic></sub> indicates the index of the most similar reference and <italic>b</italic><sub><italic>i</italic></sub> indicates the index of the second most similar reference, and <italic>c</italic><sub><italic>i</italic></sub>-<italic>h</italic><sub><italic>i</italic></sub> indicate the remaining unselected references.</p>
    <p id="Par10">PsiZ uses the class psiz.trials.Observations to create a set of judged trials. You can also create a set of unjudged trials using the class psiz.trials.Docket. The initialization format of these classes is shown below.
<graphic position="anchor" xlink:href="13428_2019_1285_Figa_HTML" id="MO2"/></p>
    <p id="Par11">The stimulus set argument for a Docket is a matrix where each row indicates the set of stimuli used in a trial. The first column indicates the query stimulus, while the remaining columns indicate the references. The stimulus set argument for a Docket is also a matrix where each row indicates the responses of a single trial. The columns of the matrix are organized according to formatting described above, i.e., the first column indicates the query, the second column indicates the first selected reference, and so on. By default, both Docket and Observations will assume that n_select= 1 and is_ranked=True. The Observations class also allows you to pass in a group indicator (group_id), which is used when inferring group-specific parameters (described in more detail below).</p>
    <p id="Par12">One way to think about the information contained in the different trial configurations is to consider how many noisy triplet constraints are <italic>implied</italic> by a single trial (Wah et al., <xref ref-type="bibr" rid="CR39">2014</xref>). Holding all other factors constant, providing more triplet constraints improves the quality of the inferred solution. In the basic case (<inline-formula id="IEq19"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {R}}|=2$\end{document}</tex-math><mml:math id="M38"><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq19.gif"/></alternatives></inline-formula>, <inline-formula id="IEq20"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {S}}|=1$\end{document}</tex-math><mml:math id="M40"><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq20.gif"/></alternatives></inline-formula>), each trial provides one triplet constraint of the form <italic>q</italic> : <italic>a</italic> &gt; <italic>b</italic>, where <italic>a</italic> is the reference stimulus that was selected as more similar to the query <italic>q</italic>. More generally, for unordered selections, each display yields <inline-formula id="IEq21"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {S}}|\left (|{\mathcal {R}}| - |{\mathcal {S}}|\right )$\end{document}</tex-math><mml:math id="M42"><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq21.gif"/></alternatives></inline-formula> triplet constraints. For ordered selections, each trial yields <inline-formula id="IEq22"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {S}}|\left (|{\mathcal {R}}| - |{\mathcal {S}}|\right )+ \binom {|{\mathcal {S}}|}{2}$\end{document}</tex-math><mml:math id="M44"><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">R</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mfrac linethickness="0.0pt"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mfenced></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq22.gif"/></alternatives></inline-formula> triplet constraints. The best trial configuration will depend in part upon the time needed to complete a single trial and the reliability of subject responses (Wilber et al., <xref ref-type="bibr" rid="CR40">2014</xref>).</p>
  </sec>
  <sec id="Sec3">
    <title>Embedding model</title>
    <p id="Par13">Given a set of observations, the goal is to infer a psychological embedding, which consists of two parts: a multi-dimensional feature representation and a corresponding similarity function. To improve conceptual clarity, the free parameters representing and the feature representation (<bold><italic>Z</italic></bold>) and the parameters controlling the similarity function (<bold><italic>𝜃</italic></bold>) are written separately. Observed behavior is linked to the parameters of interest (<bold><italic>Z</italic></bold>, <bold><italic>𝜃</italic></bold>) using a generative model. The generative model describes how the observed behavior is generated from the parameters of interest. Given a set of observations <inline-formula id="IEq23"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {D}$\end{document}</tex-math><mml:math id="M46"><mml:mi mathvariant="script">D</mml:mi></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq23.gif"/></alternatives></inline-formula>, the likelihood of the observations given the model parameters is:
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathcal{L} = \prod\limits_{i} p\left( \mathcal{D}_{i} | \boldsymbol{Z}, \boldsymbol{\theta} \right) $$\end{document}</tex-math><mml:math id="M48"><mml:mi mathvariant="script">ℒ</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>∏</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>It should be noted that the likelihood allows researchers to combine different trial configurations since each trial is assumed to contribute independently to the likelihood. In the remainder of this section, we walk through the generative model, starting with the distance function.</p>
    <sec id="Sec4">
      <title>Distance function</title>
      <p id="Par14">To start, we assume the feature representation <bold><italic>Z</italic></bold> is composed of points in a <italic>D</italic>-dimensional space. Following decades of psychological research (e.g., Nosofsky<xref ref-type="bibr" rid="CR19">1985</xref>), we further assume that distance between points is computed using the weighted Minkowski distance:
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \|{\boldsymbol{z}_{m} - \boldsymbol{z}_{m}}\|_{\rho,\boldsymbol{w}_{k}} = \left( \sum\limits_{j=1}^{D}w_{j}|{z_{m,j} - z_{n,j}}|^{\rho}\right)^{\frac{1}{\rho}}, $$\end{document}</tex-math><mml:math id="M50"><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mi>∑</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>where <italic>w</italic><sub><italic>j</italic></sub> ≥ 0 and <inline-formula id="IEq24"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}${\sum }_{j=1}^{D} w_{j} = D$\end{document}</tex-math><mml:math id="M52"><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>D</mml:mi></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq24.gif"/></alternatives></inline-formula>. Note that the weights sum to <italic>D</italic>, so that when all the weights are equal, i.e., <italic>w</italic><sub><italic>j</italic></sub> = 1, we recover the standard (unweighted) Minkowski distance. The arguments <bold><italic>z</italic></bold><sub><italic>m</italic></sub> and <bold><italic>z</italic></bold><sub><italic>n</italic></sub> indicate two arbitrary feature representations. The parameter <italic>ρ</italic> controls the type of distance (e.g., <italic>ρ</italic> = 2 results in Euclidean distance).</p>
      <p id="Par15">The weights correspond to attention weights and allow the model to capture differences in how individuals or groups attend to different dimensions in the psychological embedding. When inferring a population-level model (i.e., there is only one group), all the weights are set to one. In the single group case, setting the weights to one does not eliminate any degrees of freedom. Since the weights are jointly inferred with the embedding vectors, the vectors can adjust themselves during inference to incorporate any stretching or shrinking of the dimensions. In the most general case, these weights are allowed to vary by individual or group. In the remainder of this work, we use <bold><italic>w</italic></bold><sub><italic>k</italic></sub> to indicate the attention weights of group <italic>k</italic>.</p>
    </sec>
    <sec id="Sec5">
      <title>Similarity function</title>
      <p id="Par16">Equipped with a distance function, we assume that psychological similarity is described by a parameterized function based on the weighted Minkowski distance:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ s\left( \boldsymbol{z}_{m}, \boldsymbol{z}_{n} \right) = f\left( \|{\boldsymbol{z}_{m} - \boldsymbol{z}_{n}}\|_{\rho,\boldsymbol{w}_{k}}\right). $$\end{document}</tex-math><mml:math id="M54"><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mn>.</mml:mn></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>Given two embedding points <bold><italic>z</italic></bold><sub><italic>k</italic></sub> and <bold><italic>z</italic></bold><sub><italic>l</italic></sub>, the similarity function <inline-formula id="IEq25"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$s\left (\boldsymbol {z}_{k}, \boldsymbol {z}_{l} \right )$\end{document}</tex-math><mml:math id="M56"><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq25.gif"/></alternatives></inline-formula> returns a value, where 0 indicates that the two points are maximally dissimilar. Three popular parameterizations assume that similarity decays exponentially (Shepard <xref ref-type="bibr" rid="CR25">1957</xref>, <xref ref-type="bibr" rid="CR26">1958</xref>; Nosofsky <xref ref-type="bibr" rid="CR19">1985</xref>, <xref ref-type="bibr" rid="CR20">1986</xref>), inversely with respect to distance (Agarwal et al., <xref ref-type="bibr" rid="CR2">2007</xref>; Tamuz et al., <xref ref-type="bibr" rid="CR32">2011</xref>), or according to the Student’s-<italic>t</italic> kernel (van der Maaten &amp; Weinberger, <xref ref-type="bibr" rid="CR37">2012</xref>).</p>
      <p id="Par17">In addition to these approaches, there are two other popular approaches: classical MDS (Gower, <xref ref-type="bibr" rid="CR7">1966</xref>; Torgerson, <xref ref-type="bibr" rid="CR36">1958</xref>) and standard non-metric MDS (Shepard <xref ref-type="bibr" rid="CR27">1962a</xref>, <xref ref-type="bibr" rid="CR28">1962b</xref>; Kruskal <xref ref-type="bibr" rid="CR12">1968a</xref>, <xref ref-type="bibr" rid="CR13">1968b</xref>). Classical MDS assumes an identity relationship between distances and <italic>dissimilarity</italic>. While there are methods for converting similarity to dissimilarity (e.g., by subtracting dissimilarity from a constant), introducing these transformations quickly moves away from a faithful implementation of the approach. Standard non-metric MDS only assumes a monotonic relationship between distance and similarity. Typically, this monotonic relationship is determined using isotonic regression. Isotonic regression results in a piece-wise linear function with multiple discontinuities, which creates problems for gradient-based inference.</p>
      <sec id="Sec6">
        <title>Exponential-family kernel</title>
        <p id="Par18">Integrating various psychological models (e.g., Jones, Love, &amp; Maddox, <xref ref-type="bibr" rid="CR9">2006</xref>; Jones, Maddox, &amp; Love, <xref ref-type="bibr" rid="CR10">2006</xref>; Nosofsky <xref ref-type="bibr" rid="CR20">1986</xref>; Shepard <xref ref-type="bibr" rid="CR30">1987</xref>) into their most general form, we obtain:
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ s\left( \boldsymbol{z}_{m}, \boldsymbol{z}_{n} \right) = \exp\left( -\beta \|{\boldsymbol{z}_{m} - \boldsymbol{z}_{n}}\|_{\rho,\boldsymbol{w}_{k}}^{\tau}\right) + \gamma, $$\end{document}</tex-math><mml:math id="M58"><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>where <italic>β</italic>, <italic>τ</italic>, and <italic>γ</italic> are free parameters that control the gradient of generalization. Since the most common parameter settings result in a Laplacian kernel (<italic>τ</italic> = 1, <italic>ρ</italic> = 2, <italic>γ</italic> = 0) and Gaussian kernel (<italic>τ</italic> = 2, <italic>ρ</italic> = 2, <italic>γ</italic> = 0), we refer to this class of similarity functions as the <italic>exponential-family</italic> kernel.</p>
      </sec>
      <sec id="Sec7">
        <title>Student’s-<italic>t</italic> kernel</title>
        <p id="Par19">Although substantial psychological evidence supports the idea that individuals use an exponential similarity function, other similarity functions have been used with success. In machine learning, a popular similarity function is the Student’s-<italic>t</italic> kernel (van der Maaten &amp; Weinberger, <xref ref-type="bibr" rid="CR37">2012</xref>):
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ s\left( \boldsymbol{z}_{m}, \boldsymbol{z}_{n} \right) = \left( 1 + \frac{\|{\boldsymbol{z}_{m} - \boldsymbol{z}_{n}}\|_{2,\boldsymbol{w}_{k}}^{2}}{\alpha} \right)^{-\frac{\alpha + 1}{2}}. $$\end{document}</tex-math><mml:math id="M60"><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac class="tfrac"><mml:mrow><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mn>.</mml:mn></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>A primary advantage of the Student’s-<italic>t</italic> kernel is that it has a heavy tail. The heavy tail is advantageous during inference because it provides a signal to the inference algorithm to continue pushing similar points together and dissimilar points apart.</p>
      </sec>
      <sec id="Sec8">
        <title>Inverse distance kernel</title>
        <p id="Par20">A second similarity function that has been widely used in machine learning is the inverse distance kernel (Agarwal et al., <xref ref-type="bibr" rid="CR2">2007</xref>; Tamuz et al., <xref ref-type="bibr" rid="CR32">2011</xref>):
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ s\left( \boldsymbol{z}_{m}, \boldsymbol{z}_{n} \right) = \frac{1}{\mu + \|{\boldsymbol{z}_{m} - \boldsymbol{z}_{n}}\|_{\rho,\boldsymbol{w}_{k}}^{\tau}} $$\end{document}</tex-math><mml:math id="M62"><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac class="tfrac"><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>+</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>, where <italic>μ</italic> and <italic>τ</italic> are free parameters that govern similarity. The free parameter <italic>τ</italic> serves the same role as in the exponential-family kernel. The parameter <italic>μ</italic> is included to make the approach numerically stable.</p>
      </sec>
      <sec id="Sec9">
        <title>Heavy-tailed kernel</title>
        <p id="Par21">By itself, the Student’s-<italic>t</italic> kernel lacks the flexibility of the exponential kernel. By generalizing the Student’s-<italic>t</italic> kernel with additional free parameters, one obtains a heavy-tailed kernel with comparable flexibility to the exponential kernel:
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ s\left( \boldsymbol{z}_{m}, \boldsymbol{z}_{n} \right) = \left( \kappa + \|{\boldsymbol{z}_{m} - \boldsymbol{z}_{n}}\|_{\rho,\boldsymbol{w}_{k}}^{\tau}\right)^{-\alpha}. $$\end{document}</tex-math><mml:math id="M64"><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>κ</mml:mi><mml:mo>+</mml:mo><mml:mo>∥</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msup><mml:mn>.</mml:mn></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
    </sec>
    <sec id="Sec10">
      <title>Selection function</title>
      <p id="Par22">The last component is the selection function, which specifies how perceived similarity is converted into observed behavior. Given a similarity function, the likelihood of subject selections are modeled in the same spirit as Luce’s ratio of strengths formulation (Luce, <xref ref-type="bibr" rid="CR16">1959</xref>) and classic similarity choice models (Shepard <xref ref-type="bibr" rid="CR25">1957</xref>, <xref ref-type="bibr" rid="CR26">1958</xref>; Nosofsky<xref ref-type="bibr" rid="CR19">1985</xref>, <xref ref-type="bibr" rid="CR20">1986</xref>). The basic principle is that the probability of selecting a given reference is proportional to the similarity between the query and that reference. For example, when subjects make only one selection (<inline-formula id="IEq26"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {R}_{i}}| \in \left [2,8\right ]$\end{document}</tex-math><mml:math id="M66"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>∈</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq26.gif"/></alternatives></inline-formula>, <inline-formula id="IEq27"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {S}_{i}}|=1$\end{document}</tex-math><mml:math id="M68"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq27.gif"/></alternatives></inline-formula>), the likelihood of the observed outcome is,
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ p\left( \mathcal{D}_{i} | \boldsymbol{Z}, \boldsymbol{\theta} \right) = \frac{s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{a} \right)}{{\sum}_{r \in \mathcal{R}_{i}} s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{r} \right)}. $$\end{document}</tex-math><mml:math id="M70"><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac class="tfrac"><mml:mrow><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mn>.</mml:mn></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>This basic principle is expanded following the rules of probability in order to describe more complicated trial configurations. For example, when a trial requires participants to select two (unranked) references (<inline-formula id="IEq28"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {R}_{i}}| \in \left [3,8\right ]$\end{document}</tex-math><mml:math id="M72"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>∈</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq28.gif"/></alternatives></inline-formula> and <inline-formula id="IEq29"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$|{\mathcal {S}_{i}}|=2$\end{document}</tex-math><mml:math id="M74"><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq29.gif"/></alternatives></inline-formula>),
<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{@{}rcl@{}} p\left( \mathcal{D}_{i} | \boldsymbol{Z}, \boldsymbol{\theta} \right) &amp;=&amp; \frac{s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{a} \right)}{{\sum}_{r \in \mathcal{R}_{i}} s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{r} \right)}\frac{s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{b} \right)}{{\sum}_{r \in \mathcal{R}_{i\neg a}} s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{r} \right)} \\&amp;&amp;+ \frac{s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{b} \right)}{{\sum}_{r \in \mathcal{R}_{i}} s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{r} \right)}\frac{s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{a} \right)}{{\sum}_{r \in \mathcal{R}_{i\neg b}} s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{r} \right)}. \end{array} $$\end{document}</tex-math><mml:math id="M76"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:mfenced></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mfrac class="tfrac"><mml:mrow><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfrac class="tfrac"><mml:mrow><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>¬</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"/><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"><mml:mo>+</mml:mo><mml:mfrac class="tfrac"><mml:mrow><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfrac class="tfrac"><mml:mrow><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>¬</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mn>.</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>The selection function is similar when a participant is required to select and rank two reference,
<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ p\left( \mathcal{D}_{i} | \boldsymbol{Z}, \boldsymbol{\theta} \right) = \frac{s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{a} \right)}{{\sum}_{r \in \mathcal{R}_{i}} s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{r} \right)}\frac{s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{b} \right)}{{\sum}_{r \in \mathcal{R}_{i \neg a}} s\left( \boldsymbol{z}_{q}, \boldsymbol{z}_{r} \right)}. $$\end{document}</tex-math><mml:math id="M78"><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac class="tfrac"><mml:mrow><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfrac class="tfrac"><mml:mrow><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>¬</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mn>.</mml:mn></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Inference procedure</title>
    <p id="Par23">Equipped with a likelihood a set of observations, it is now possible to perform inference. The PsiZ package leverages the TensorFlow Python library (Abadi et al., <xref ref-type="bibr" rid="CR1">2015</xref>) to perform gradient-based inference on the log-likelihood of the data given the model parameters:
<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \max\limits_{\boldsymbol{Z}, \boldsymbol{\theta}} \sum\limits_{i} \log p\left( \mathcal{D}_{i} | \boldsymbol{Z}, \boldsymbol{\theta} \right). $$\end{document}</tex-math><mml:math id="M80"><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>∑</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:mfenced><mml:mn>.</mml:mn></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula>The burden of solving this optimization problem is almost completely removed, allowing researchers to call a few high-level methods in order to achieve their goals. For example, it is simple to infer a psychological embedding using an exponential similarity function.
<graphic position="anchor" xlink:href="13428_2019_1285_Figb_HTML" id="MO3"/></p>
    <p id="Par24">The above example loads a set of observations, initializes a three-dimensional embedding model, and then fits the model to the loaded observations. While this example requests a three-dimensional embedding, the researcher must decide which dimensionality is appropriate for their problem. The PsiZ package includes a separate procedure that researchers can use to help them decide on the appropriate dimensionality.
<graphic position="anchor" xlink:href="13428_2019_1285_Figc_HTML" id="MO4"/></p>
    <p id="Par25">The dimension_search procedure first partitions the provided observations into a train and hold-out set. For each of the candidate dimensionalities provided by the user, an embedding is fit using the training set and is evaluated using the validation set. The procedure continues to try embeddings with a larger dimensionality until validation loss stops improving. When validation loss is worse for the current dimensionality under consideration, the procedure terminates and returns the dimensionality that resulted in the last observed improvement.</p>
    <p id="Par26">When inferring an embedding, it is important to assess whether a sufficient number of observations have been collected. If an insufficient number of observations have been collected, the inferred embedding is much more likely to model noise rather than actual behavior. One method for determining if enough data have been collected is to perform a simple convergence analysis. To perform a convergence analysis, the observations are split up into multiple partitions. A separate embedding is inferred using observations from an increasing number of partitions. Each embedding is compared to the previous embedding by computing a Pearson correlation coefficient between the two embeddings. If a sufficient amount of data have been collected, then adding more data should not change the inferred embedding and the correlation score should be high. PsiZ provides a simple function called assess_convergence for performing a convergence analysis.
<graphic position="anchor" xlink:href="13428_2019_1285_Figd_HTML" id="MO5"/></p>
    <p id="Par27">In the above example, n_shuffle indicates the number of times the analysis should be repeated. In between each analysis, the data is shuffled, giving a different set of partitions each time.</p>
  </sec>
  <sec id="Sec12">
    <title>Intelligent trial selection</title>
    <p id="Par28">The final aspect of this work focuses on intelligently selecting trials to collect maximally informative data. A simple strategy is to randomly select images for each trial, with the sole constraint that all images be unique. Although a reasonable approach, random displays have a drawback. Imagine for the moment that judged displays have been collected using all but one stimulus. Ideally, the next trial shown to a participant would include that unused stimulus. More generally, the embedding procedure will be less confident about the position of some images in an embedding. An active-selection approach constructs trials that have the best chance of minimizing uncertainty associated with the embedding. Active selection proceeds via multiple iterations of generating trials and collecting the corresponding observations. This component is heavily inspired by previous active selection research (Tamuz et al., <xref ref-type="bibr" rid="CR32">2011</xref>), but has been generalized to handle arbitrary trial configurations and uses a different set of heuristics.</p>
    <sec id="Sec13">
      <title>Formalizing uncertainty</title>
      <p id="Par29">The uncertainty of an embedding point’s location is formally characterized using a posterior distribution,
<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ p\left( \boldsymbol{z}_{k} | \mathcal{D}, \boldsymbol{Z}_{\neg k}, \boldsymbol{\theta}\right) \propto p\left( \mathcal{D} | \boldsymbol{Z}, \boldsymbol{\theta}\right) p\left( \boldsymbol{z}_{k} | \boldsymbol{Z}_{\neg k}, \boldsymbol{\theta}\right). $$\end{document}</tex-math><mml:math id="M82"><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mo>¬</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:mfenced><mml:mo>∝</mml:mo><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:mfenced><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mo>¬</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:mfenced><mml:mn>.</mml:mn></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ12.gif" position="anchor"/></alternatives></disp-formula>For simplicity, we assume that the prior distribution of the embedding points is characterized using a Gaussian distribution
<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ p\left( \boldsymbol{z}_{k} | \boldsymbol{Z}_{\neg k} \boldsymbol{\theta}\right) \sim \mathcal{N}\left( {\mu}, \boldsymbol{\Sigma} \right). $$\end{document}</tex-math><mml:math id="M84"><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mo>¬</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:mfenced><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:mfenced><mml:mn>.</mml:mn></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ13.gif" position="anchor"/></alternatives></disp-formula>The likelihood is the same as previously described and predicts how participants select references given a particular query,
<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ p\left( \mathcal{D} | \boldsymbol{Z}, \boldsymbol{\theta}\right). $$\end{document}</tex-math><mml:math id="M86"><mml:mi>p</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">𝜃</mml:mi></mml:mrow></mml:mfenced><mml:mn>.</mml:mn></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ14.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par30">The posterior distribution is approximated by sampling from the joint distribution using Gibbs sampling. Since the posterior distribution has a Gaussian prior, elliptical slice sampling (Murray, Adams, &amp; MacKay, <xref ref-type="bibr" rid="CR17">2010</xref>) can be used to sample points in a relatively efficient manner. In effect, the sampling procedure produces a set of points for each stimulus. If the distribution of points is tightly clustered, then there is relatively low uncertainty about the position of that stimulus. If the distribution is wide, then there is relatively high uncertainty about the location of the stimulus in the embedding. For clarity, the posterior samples are denoted using a three-dimensional tensor <italic>ζ</italic> such that <inline-formula id="IEq30"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$ {\zeta }^{(s)}_{k}$\end{document}</tex-math><mml:math id="M88"><mml:msubsup><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq30.gif"/></alternatives></inline-formula> indicates the <italic>s</italic>’th sample of the <italic>k</italic>’th stimulus, a <italic>d</italic>-dimensional vector corresponding to a point in the embedding space. The matrix <italic>ζ</italic><sup>(<italic>s</italic>)</sup> can be thought of as a sampling snapshot of the entire embedding.</p>
    </sec>
    <sec id="Sec14">
      <title>Maximizing information gain</title>
      <p id="Par31">To maximize information gain, we need to compute the expected information gain for a candidate trial. Given a candidate trial <bold><italic>c</italic></bold>, the expected information gain is equal to the mutual information,
<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ I\left( \boldsymbol{Z}; Y | {\mathcal{D}}, \boldsymbol{c} \right) = H\left( \boldsymbol{Z} | {\mathcal{D}} \right) - H\left( \boldsymbol{Z} | {\mathcal{D}}, Y, \boldsymbol{c} \right), $$\end{document}</tex-math><mml:math id="M90"><mml:mi>I</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>;</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ15.gif" position="anchor"/></alternatives></disp-formula>where <italic>Y</italic> is a discrete random variable indicating all possible outcomes when the candidate trial is shown to a participant. For example, if the candidate trial displays two references and participants must select one reference, then there are two possible outcomes. The first term indicates the entropy (i.e., uncertainty) associated with the current embedding. The second term indicates the expected entropy of the embedding if we collect an observation for the candidate trial. Since we would like to minimize entropy associated with the embedding, we are looking for a candidate trial such that <inline-formula id="IEq31"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$H\left (\boldsymbol {Z} |  {\mathcal {D}} \right ) &gt; H\left (\boldsymbol {Z} |  {\mathcal {D}}, Y, \boldsymbol {c} \right )$\end{document}</tex-math><mml:math id="M92"><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">D</mml:mi></mml:mrow></mml:mfenced><mml:mo>&gt;</mml:mo><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq31.gif"/></alternatives></inline-formula> and information gain is positive.</p>
      <p id="Par32">Since <bold><italic>Z</italic></bold> is a continuous variable, computing information gain appears non-trivial. Fortunately, the computation can be simplified by exploiting the identity of mutual information (i.e., <italic>H</italic>(<italic>A</italic>) − <italic>H</italic>(<italic>A</italic>|<italic>B</italic>) = <italic>H</italic>(<italic>B</italic>) − <italic>H</italic>(<italic>B</italic>|<italic>A</italic>)) and using our previously obtained samples taken from the posterior distribution in order to approximate the integrals. After all modifications and approximations, the equation for information gain becomes,
<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{@{}rcl@{}} I\left( \boldsymbol{Z};Y | \mathcal{D}, \boldsymbol{c} \right) &amp; = &amp; \!- \sum\limits_{i=1}^{M} P\left( y_{i} | \mathcal{D},\! \boldsymbol{c} \right) \log P\left( y_{i} | \mathcal{D}, \boldsymbol{c} \right) \\&amp;&amp;\!+ \frac{1}{N} {\sum\limits_{s}^{N}} {\sum\limits_{i}^{M}} p (y_{i} | {\zeta}^{(s)},\! \mathcal{D} ) \log p(y_{i} | {\zeta}^{(s)}, \mathcal{D} ) ,\\ \end{array} $$\end{document}</tex-math><mml:math id="M94"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>I</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>;</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:mfenced></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mspace width="0.3em"/><mml:mo>−</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mi>∑</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>P</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.3em"/><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:mfenced><mml:mi>log</mml:mi><mml:mi>P</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"/><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"><mml:mspace width="0.3em"/><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mi>∑</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mi>∑</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.3em"/><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mrow><mml:mi>ζ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"/><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"/></mml:mtr></mml:mtable></mml:math><graphic xlink:href="13428_2019_1285_Article_Equ16.gif" position="anchor"/></alternatives></disp-formula>where <italic>M</italic> indicates the number of possible outcomes associated with the candidate trial and <italic>N</italic> is the number of samples being used to approximate the integral.</p>
    </sec>
    <sec id="Sec15">
      <title>Heuristic search procedure</title>
      <p id="Par33">For simple scenarios, it is possible to evaluate all candidate trials in order find the trial that maximizes expected information gain. Unfortunately, for most scenarios, particularly those involving larger stimulus sets, exhaustive search becomes prohibitively expensive. As an alternative, we employ a two-stage heuristic search strategy. In the first stage, a query stimulus is stochastically selected based on its relative uncertainty. In the second stage, a candidate set of references is stochastically selected based on their similarity to the query stimulus. This process is repeated until the desired number of trials have been created. Ideally, the embedding would be updated to take into account observations for the new trial. In practice, multiple trials can be generated at once by limiting the number of times a particular stimulus can be used as a query.</p>
      <p id="Par34">In the first stage, relative uncertainty is determined by summing the Kullback–Leibler divergence between the stimulus of interest and all other stimuli. Intuitively, this prioritizes stimuli that exhibit high uncertainty in the embedding. However, not all uncertainty is equivalent from the perspective of constraining the embedding. Rather, high uncertainty stimuli with close neighbors should be prioritized over high uncertainty stimuli with distant neighbors. The asymmetric nature of Kullback–Leibler divergence is also exploited in this heuristic. Given two stimuli, one that has high uncertainty and one that has low uncertainty, only the stimulus with high uncertainty should be given higher priority. Once the relative uncertainty has been determined for all stimuli, a query is stochastically selected proportional to its relative uncertainty such that higher uncertainty stimuli are more likely to be chosen as query stimuli.</p>
      <p id="Par35">In the second stage, a set of candidate references are selected based on similarity using the current best estimate of the similarity function. The candidate references are selected stochastically such that more similar neighbors are more likely to be chosen. In effect, this heuristic biases the reference set to include stimuli that are close neighbors of the query stimulus. If all the reference stimuli are excessively dissimilar from the query stimulus, the corresponding similarity judgments will not provide much information. When few observations have been collected, this heuristic prioritizes unevaluated stimuli.</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Experiment and model recovery simulations</title>
    <sec id="Sec17">
      <title>Experiment 1: kernel comparison</title>
      <p id="Par36">The following experiment compares the ability of three different similarity kernels to predict human similarity judgments. The exponential-family kernel is motivated by psychological theory, while the inverse distance kernel, Student’s-<italic>t</italic> kernel, and heavy-tailed kernel are largely motivated by common practice in machine learning.</p>
      <sec id="Sec18">
        <title>Methods</title>
        <sec id="FPar1">
          <title>Participants</title>
          <p id="Par37">A population of 342 participants were recruited from Amazon Mechanical Turk and paid at a rate of approximately $6.00 per hour.</p>
        </sec>
        <sec id="FPar2" sec-type="materials">
          <title>Materials</title>
          <p id="Par38">A small dataset of 16 species of birds was assembled from the CUB 200 image dataset (Wah, Branson, Welinder, Perona, &amp; Belongie, <xref ref-type="bibr" rid="CR38">2011</xref>). Species were chosen such that there were four groups of birds composed of four similar-looking species, roughly corresponding to four taxonomic families (Fig. <xref rid="Fig2" ref-type="fig">2</xref>a–d). For each species, we selected 13 images, yielding a total of 208 unique images. Images were selected and cropped such that each image displayed a single bird, the bird was clearly visible, the image was of a good resolution, and no text was present.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Example stimuli of the different bird species used in this work. Each row contains four similar bird species, each of which belongs to the same or similar taxonomic family. The images shown in this figure were drawn from the set of 208 images used in the experiments</p></caption><graphic xlink:href="13428_2019_1285_Fig2_HTML" id="MO6"/></fig></p>
        </sec>
        <sec id="FPar3">
          <title>Procedure</title>
          <p id="Par39">Similarity judgments were collected during short, 10-min sessions via a web-based application deployed on Amazon Mechanical Turk. Each 10-min session used one of two possible trial configurations. Participants either saw trials with two references and selected the most similar reference (2-choose-1) or saw eight references and selected two references in a ranked order (8-choose-2). During a 2-choose-1 session, participants saw between 60 and 120 trials. The number of displays varied in order to calibrate each session to be approximately 10 min. During an 8-choose-2 session, participants saw 30 trials. Participants were allowed to complete more than one 10-min session. Collectively, participants judged 7520 2-choose-1 trials and 8772 8-choose-2 trials. All judged trials were combined to create a single dataset of observations (<inline-formula id="IEq32"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mathcal {D}$\end{document}</tex-math><mml:math id="M96"><mml:mi mathvariant="script">D</mml:mi></mml:math><inline-graphic xlink:href="13428_2019_1285_Article_IEq32.gif"/></alternatives></inline-formula>).</p>
          <p id="Par40">The collected similarity judgments were used in a tenfold, subject-stratified cross-validation procedure in order to evaluate the capabilities of an exponential-family kernel, an inverse distance kernel, a heavy-tailed kernel, and a Student’s-<italic>t</italic> kernel. Similarity judgments were partitioned into ten roughly equal folds such that each fold had the same proportion of 2-choose-1 and 8-choose-2 trials. For each fold, the dimensionality was estimated using the dimension_search routine. Once a dimensionality was selected, an embedding was inferred using nine folds as training data. The remaining fold was used as a validation set. For each fold, the validation loss (i.e., negative log-likelihood) and validation accuracy was recorded. Accuracy was computed differently for each trial configuration. For 2-choose-1 configurations, accuracy was determined by computing the proportion of time the model correctly predicted the chosen reference (i.e., top-1 accuracy). For 8-choose-2 configurations, there are 56 possible outcomes. The prediction of an 8-choose-2 was graded as correct if the actual outcome was among the top five most probable outcomes (top-5 accuracy). Since individuals may perceive similarity differently and individuals themselves may not be consistent, we do not expect to infer embeddings with zero loss or perfect accuracy.</p>
        </sec>
      </sec>
      <sec id="Sec19">
        <title>Results</title>
        <p id="Par41">The primary goal of the model comparison is to determine which model is best able to predict unseen human similarity judgments. The results are presented in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. During the cross-validation procedure, the dimension_search procedure selected a dimensionality that varied between 2 and 4. The modal dimensionality for the inverse, exponential, heavy-tailed, and Student’s-<italic>t</italic> model was 4, 3, 2, and 3, respectively. Significance tests use a Bonferroni corrected alpha value of .05 (.008 corrected). Focusing on validation loss, pairwise <italic>t</italic> tests of the tenfold cross-validation validation procedure reveal that the differences between the inverse kernel (<italic>M</italic> = 2.99, <italic>S</italic><italic>D</italic> = 0.14), exponential-family kernel (<italic>M</italic> = 2.93, <italic>S</italic><italic>D</italic> = 0.15), the heavy-tailed kernel (<italic>M</italic> = 2.99, <italic>S</italic><italic>D</italic> = 0.14), and the Student’s-<italic>t</italic> kernel (<italic>M</italic> = 2.92, <italic>S</italic><italic>D</italic> = 0.13) are all non-significant. Likewise, pairwise <italic>t</italic> tests of top-<italic>N</italic> accuracy were all non-significant.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Model fitting results using a tenfold cross-validation procedure for four different similarity kernels: inverse, exponential, heavy-tailed, and Student’s-<italic>t</italic>. Validation results are shown for <bold>a</bold> validation loss (i.e., negative log-likelihood) and <bold>b</bold> top-n validation accuracy. Top-1 accuracy is used evaluate 2-choose-1 trials while top-5 accuracy is used to evaluate 8-choose-2 trials. All <italic>error bars</italic> indicate standard error of the mean across the ten folds</p></caption><graphic xlink:href="13428_2019_1285_Fig3_HTML" id="MO7"/></fig></p>
        <p id="Par42">To ensure that the null results were not merely a result of insufficient data, a convergence analysis was performed using the assess_convergence function. When splitting the data into ten partitions and performing three different restarts, the Pearson correlation between the similarity matrices of an embedding that uses all of the data and an embedding that uses nine partitions of the data is very high (<italic>ρ</italic> = .95). The high correlation value suggests that the inferred embeddings are consistent and a sufficient amount of data have been collected.</p>
      </sec>
      <sec id="Sec20">
        <title>Discussion</title>
        <p id="Par43">The three kernels appear equally capable of predicting human similarity judgments. For domains that are similar to the set of birds used here, it is likely reasonable to use either kernel. One advantage of using the exponential-family kernel is that many computational models of human category learning also use an exponential-family kernel. By assuming an exponential-family kernel, the resulting psychological embedding could be integrated with a category learning model (e.g., Shepard <xref ref-type="bibr" rid="CR25">1957</xref>, <xref ref-type="bibr" rid="CR26">1958</xref>; Nosofsky <xref ref-type="bibr" rid="CR19">1985</xref>, <xref ref-type="bibr" rid="CR20">1986</xref>; Roads, Xu, Robinson, &amp; Tanaka, <xref ref-type="bibr" rid="CR24">2018</xref>).</p>
        <p id="Par44">Given a domain as complicated as birds, it may be surprising that the dimensionality search choose relatively low dimensionality embeddings. While birds unquestionably involve a plethora of features, the embedding algorithm is constrained by three factors. First, the chosen bird dataset does not span the entire space of bird features. If a particular type of feature variability is not represented in the dataset, it is unlikely to be captured in the inferred embedding. Second, the inferred embeddings represent nonlinear manifolds of a higher-dimensional feature space. Consequently, a single embedding dimension is unlikely to model a single visual feature, but will capture a mixture of features. Lastly, the embeddings are constrained by the number of observations that are available. Each additional dimension adds an additional degree of freedom for <italic>every</italic> embedding vector. Without sufficient data, these additional degrees of freedom will lead to model overfitting and poor generalization. Under ideal conditions, an infinite number of observations would be collected in order to allow the dimensionality search procedure to consider higher dimensional spaces.</p>
      </sec>
    </sec>
    <sec id="Sec21">
      <title>Simulation 1: data collection strategies</title>
      <p id="Par45">Having compared different kernels, we turn to the issue of comparing different strategies for collecting observations. The goal is to determine the strategy that results in the highest-quality embedding at the lowest cost. Since the primary cost of collecting similarity judgments is paying participants for their time, we evaluate different collection strategies based on how many worker hours are required to reach a given quality level.</p>
      <p id="Par46">Data collection strategies are evaluated along two dimensions. First, the trial itself can take on many different configurations. Second, trials can be generated randomly or via active selection. The different collection strategies are evaluated using simulations of human similarity judgments.</p>
      <sec id="Sec22">
        <title>Methods</title>
        <sec id="FPar4">
          <title>Participants</title>
          <p id="Par47">No new participants were recruited for this experiment. All similarity judgments collected for the previous experiment were re-used to infer a ground-truth model of human behavior.</p>
        </sec>
        <sec id="FPar5" sec-type="materials">
          <title>Materials</title>
          <p id="Par48">The experiment used the same set of 208 bird images as the previous experiment.</p>
        </sec>
        <sec id="FPar6">
          <title>Procedure</title>
          <p id="Par49">Three different collection strategies were evaluated using simulated human responses. The first collection strategy presented trials containing two references, where simulated participants selected one reference. The content of the trials was chosen randomly, subject to the constrain that a single image could not appear more than once on a trial (random 2-choose-1). The second collection strategy presented trials containing eight references and required participants to select two references, in ranked order. The particular images for each trial were chosen randomly (random 8-choose-2). The last strategy used an 8-choose-2 trial configuration, but selected the trial content using active selection (active 8-choose-2).</p>
          <p id="Par50">Simulated responses were generated by treating a fitted psychological embedding as a generative model of human behavior (i.e., a virtual subject). Once a psychological embedding predicts the probability of all possible response outcomes for a particular trial (see “<xref rid="Sec10" ref-type="sec">Selection function</xref>”), a specific response is generated by stochastically sampling from the possible outcomes. To ensure that the simulated responses mirror human behavior, an exponential-family psychological embedding was fitted to all human similarity judgments described in the previous experiment (7520 2-choose-1 trials and 4733 8-choose-2 trials). The fitted model served as a virtual subject and the ground-truth psychological embedding by which other models were evaluated.</p>
          <p id="Par51">Each collection strategy is used to generate trials, collect observations, and infer a strategy-specific psychological embedding. The quality of a strategy-specific embedding is determined by comparing its predictions to those of the ground-truth embedding. The critical predictions of a psychological embedding can be summarized by generating a corresponding pair-wise similarity matrix <italic>S</italic>. The element <italic>s</italic><sub><italic>i</italic><italic>j</italic></sub> indicates the similarity between the <italic>i</italic> th and <italic>j</italic> th stimulus. The predictions of a strategy-specific and ground-truth psychological embedding can be compared by computing the Pearson correlation coefficient between the respective similarity matrices. When computing the Pearson correlation, we only use the upper diagonal portion of the matrix less the diagonal elements, since the matrix is symmetric and the diagonal elements indicate self-similarity. If the strategy-specific embedding has successfully modeled the ground-truth embedding, the Pearson correlation will be high.</p>
          <p id="Par52">Each strategy-specific embedding was inferred using a different number of trials in order to map out how the number of trials affects the quality of the inferred embedding. Starting with an initial set of observations, additional observations were added in an incremental fashion. Each strategy-specific embedding was evaluated based on how many worker hours it took to reach a Pearson correlation of .95. Since there are two sources of stochasticity (trial generation and response simulation), five separate runs were conducted for each strategy. For each run, random 2-choose-1, random 8-choose-2, and active 8-choose-2 were seeded with 500, 50, and 50 trials, respectively. For all strategies, the seed trials had their content generated randomly. During active selection, 40 trials (each with a unique query image) were generated per round. In between every round, the posterior distribution of the embedding points was updated, while holding constant the parameters of the similarity function. Every fifth round, the parameters of the similarity function were updated. For simplicity, all inference is performed assuming a dimensionality of three—matching the dimensionality of the ground-truth embedding.</p>
        </sec>
      </sec>
      <sec id="Sec23">
        <title>Results</title>
        <p id="Par53">From the actual human data, it is clear that a 2-choose-1 display (<italic>M</italic> = 4.73, <italic>S</italic><italic>D</italic> = 11.18) and 8-choose-2 (<italic>M</italic> = 13.07, <italic>S</italic><italic>D</italic> = 23.49) display require different amounts of time to complete (<italic>t</italic>(12251) = 26.41, <italic>p</italic> &lt; 0.001). The number of trials is converted to worker hours based on the median human response time of the 2-choose-1 (median 3.06 s) and 8-choose-2 (median 8.98 s) trials. Since the human response times include dramatic outliers, median response times provide an appropriate measure of central tendency.</p>
        <p id="Par54">The simulation results for three different collection strategies are presented in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. The simulation results show that random 8-choose-2 (M = 28.0 h) is more efficient than random 2-choose-1 (M = 82.0 h) in reaching a Pearson correlation of .95. For the same embedding quality, only about 34% of the worker hours are necessary when using randomly selected 8-choose-2 versus 2-choose-1 trials. The results also reveal that active 8-choose-2 (M = 16.0 h) is more efficient than either random strategy. When using an 8-choose-2 trial configuration, active selection requires about 57% of the worker hours compared to random selection.
<fig id="Fig4"><label>Fig. 4</label><caption><p>Results of Simulation 1. Each <italic>line</italic> indicates a different collection strategy. Since cost is determined by total number of worker hours needed, the quality of the inferred embeddings is plotted with respect to worker hours. Each <italic>line</italic> indicates the mean between five independent simulation runs. The <italic>shaded regions</italic> indicate the maximum and minimum envelope across runs</p></caption><graphic xlink:href="13428_2019_1285_Fig4_HTML" id="MO8"/></fig></p>
      </sec>
      <sec id="Sec24">
        <title>Discussion</title>
        <p id="Par55">Assuming reliability of responses is in accordance with our simulations, using 8-choose-2 displays is more cost-effective than 2-choose-1 displays, allowing a high-quality embedding to be inferred at nearly a third of the cost. If the goal is to obtain a psychological embedding using the most effective trial configuration, the 8-choose-2 trial configuration is a good way to save money. These results replicate the findings of Wilber et al., (<xref ref-type="bibr" rid="CR40">2014</xref>), except with a proper likelihood model.</p>
        <p id="Par56">Additional savings are achieved when switching to a strategy that uses active selection. The benefit of active selection appears to be greatest when the quality of the inferred embedding is starting to asymptote. Without much data, active selection behaves similarly to random selection. In effect, the active selection procedure is highly uncertain about <italic>all</italic> embedding points and the chosen displays provide the same amount of information at randomly generated displays. As data accumulate, active selection is able to focus on uncertain stimuli, allowing the inferred embedding to reach asymptote more quickly. According to these simulations, active selection provides an efficient and cost-effective way to obtain high-quality embeddings.</p>
      </sec>
    </sec>
    <sec id="Sec25">
      <title>Simulation 2: group-specific attention weights</title>
      <p id="Par57">In the final simulation, we demonstrate how the embedding procedure is capable of inferring group-specific attention weights in a similar spirit to MDS procedure INDSCAL (Carroll and Chang, <xref ref-type="bibr" rid="CR3">1970</xref>). Attention weights assume that all agents use the same feature space, but may differ in how important they view each dimension. For example, an expert birdwatcher may place more weight on the color of feathers surrounding a bird’s eye, while a novice may place very little weight on this feature. Since the development of INDSCAL, many cognitive models have captured individual and group differences using attention weights (e.g., Nosofsky <xref ref-type="bibr" rid="CR18">1984</xref>, <xref ref-type="bibr" rid="CR20">1986</xref>; Kruschke <xref ref-type="bibr" rid="CR11">1992</xref>; Love et al.,<xref ref-type="bibr" rid="CR15">2004</xref>).</p>
      <p id="Par58">In addition to demonstrating the ability to learn group-specific attention weights, this simulation also demonstrates how inferring a shared embedding has the potential to reduce the cost of collecting data. The demonstration uses a shared set of fictitious stimuli and two simulated groups. These two groups can be likened to novices and experts. Inspired by novice and expert attention differences with musical notes (Shepard, <xref ref-type="bibr" rid="CR29">1982</xref>), we assume a scenario where novices pay attention to one set of feature dimensions, while experts attend to a complementary set of feature dimensions.</p>
      <sec id="Sec26">
        <title>Methods</title>
        <sec id="FPar7">
          <title>Participants</title>
          <p id="Par59">No human participants were used in this experiment. All observations were simulated.</p>
        </sec>
        <sec id="FPar8" sec-type="materials">
          <title>Materials</title>
          <p id="Par60">A fictitious set of 100 stimuli was used in this experiment. The stimuli coordinates were drawn from a four-dimensional Gaussian distribution with zero mean and a spherical covariance matrix of 0.03 (equal variance along each dimension).</p>
        </sec>
        <sec id="FPar9">
          <title>Procedure</title>
          <p id="Par61">Following the design of Simulation 1, we assume a known ground-truth psychological embedding. In contrast to the previous experiment, this embedding is not based on actual human behavior, but assumes that a set of stimuli are distributed in a four-dimensional space. Furthermore, it is assumed that novices focus on the first two dimensions (<bold>w</bold> = [1.8,1.8,.2,.2]) while experts focus on the last two dimensions (<bold>w</bold> = [.2,.2,1.8,1.8]) when judging similarity. Novice and expert responses are simulated using the respective attention weights.</p>
          <p id="Par62">Multiple-group inference is examined using two conditions: a naive approach (independent) and an information-sharing approach (shared). The naive approach is to infer an independent embedding for each group. There are two primary disadvantages with this approach. First, independently inferred embeddings are not directly comparable, since MDS solutions exhibit rotation and scale invariance. Second, independently inferred embeddings will not be able to leverage any mutual information between the observations of the two groups. The shared condition infers a shared psychological embedding with group-specific attention weights.</p>
          <p id="Par63">The quality of an inferred embedding is evaluated in the same manner as Simulation 1, with a small twist. Since there are two groups, there are group-specific similarity matrices: a novice similarity matrix and an expert similarity matrix. The quality of an inferred embedding is determined by comparing each group-specific similarity matrix to the corresponding ground-truth similarity matrix.</p>
          <p id="Par64">The independent and shared condition require different considerations when deciding how to collect observations. In the independent condition, each group is treated independently and the researcher collects whatever number of observations are necessary for each group. In the shared condition, the researcher could collect an equal number of novice and expert observations. Since it is assumed that there is mutual information between the two groups, the researcher can choose to collect an imbalanced set of observations that optimizes some external utility function. For example, it is often the case that compensating experts costs more than compensating novices. If the external utility function is to minimize cost, the researcher can collect a larger proportion of novice observations. The best proportion will be determined by the particular circumstances of the researcher. As a proof of concept, we consider the case where novice observations are collected approximately 74% of the time.</p>
          <p id="Par65">Using five independent runs, we determine how many expert worker hours are necessary to reach psychological embeddings that correctly capture novice and expert behavior with at least a .95 Pearson correlation. Worker hours are estimated using the same conversion values used in Simulation 1.</p>
        </sec>
      </sec>
      <sec id="Sec27">
        <title>Results</title>
        <p id="Par66">The simulation expert-specific results for the two different conditions are presented in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. When inferring an expert-specific psychological embedding for the independent condition, approximately 14.8 expert worker hours are required to reach criterion. Inferring a novice-specific psychological embedding requires 14.8 novice worker hours to reach criterion (not shown in the figure). When inferring a shared embedding, criterion can be met for both groups using 8.9 novice worker hours and 5.3 expert worker hours. From the perspective of total worker hours, the independent condition requires 29.6 worker hours while the shared condition requires 14.2 worker hours.
<fig id="Fig5"><label>Fig. 5</label><caption><p>Results of Simulation 2. <bold>a</bold> Total worker hours for the independent and shared condition. The results indicate an average of five independent runs. <bold>b</bold> A breakdown of expert-specific convergence as a function of expert worker hours. Each <italic>line</italic> indicates a different simulated scenario evaluating how many expert worker hours are necessary. The <italic>blue line</italic> indicates the number of expert worker hours necessary to reach criterion when a independent psychological embedding is inferred for each group. The <italic>red line</italic> indicates the number of expert worker hours necessary to reach criterion when a shared psychological embedding is inferred. Each line indicates the mean between five independent simulation runs. The <italic>shaded regions</italic> indicate the maximum and minimum envelope across runs</p></caption><graphic xlink:href="13428_2019_1285_Fig5_HTML" id="MO9"/></fig></p>
      </sec>
      <sec id="Sec28">
        <title>Discussion</title>
        <p id="Par67">Fewer total worker hours are required when inferring a single psychological embedding with group-specific attention weights than when inferring two independent embeddings. It is notable that the shared condition only requires about half the total worker hours as the independent condition. The large difference is likely driven by three factors. First, a shared embedding involves fewer free parameters in total. Second, a shared embedding can take advantage of any mutual information between the novice and expert groups. Lastly, inference likely struggles to determine the location of stimuli along the feature dimensions that receive little attention weight. By combining observations from subjects that have complementary attention weights, it becomes easier to determine the location of each stimulus in the psychological embedding. Combined, these factors make the shared condition a clear winner.</p>
        <p id="Par68">In addition to fewer total worker hours, a shared psychological embedding can reach criterion for both experts and novices by using relative fewer <italic>expert</italic> hours. Since experts are typically paid more for their time (and expertise), reducing the required number of expert worker hours can substantially reduce the financial burden of collecting data. It is possible that more extreme savings can be achieved by shifting more of the inference burden onto novice observations. The above analysis assumed that novice and experts complete trials in the same amount of time. However, novices and experts may differ on their throughput. One possibility is that experts would be faster given their ability to make quick fine-grained judgments about their domain of expertise (Tanaka &amp; Taylor, <xref ref-type="bibr" rid="CR33">1991</xref>).</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec29">
    <title>Conclusions</title>
    <p id="Par69">Psychological embeddings are useful in many domains of research. Despite the substantial progress that has been made, a unified and coherent set of tools has been slow to emerge. This work presents the key aspects of a publicly available Python package that makes it easy for researchers to infer their own psychological embeddings. In an effort to make the tools as useful as possible, the algorithms have been designed to handle a variety of trial configurations and handle inference of group-specific attention weights. In addition, the package includes an active selection routine to help researchers get the most out of their budget. While these facets were discussed in the context of visual similarity, the software package can work with similarity judgment based on other modalities.</p>
    <p id="Par70">To accompany the description of the algorithm, three experiments demonstrated the various ways the package can be used. Experiment 1 demonstrated how different similarity kernels can easily be compared, allowing the researcher to select the one that makes the most sense for their project. Simulation 1 highlighted how different collection strategies can make data acquisition more cost-effective. In particular, active selection combined with 8-choose-2 trial configurations beat out the other options. Lastly, Simulation 3 illustrated how group-specific attention weights can be inferred using a single model—potentially reducing the cost of collecting data. In isolation, the results presented in this work make incremental contributions on four different fronts. As a whole, a meaningful contribution is made by providing a complete top-to-bottom software package.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This research was supported by NSF grants SES-1461535, SBE-0542013, SMA-1041755, and DRL-1631428.</p>
    <p>The data and materials for all experiments are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/roads/psiz">https://github.com/roads/psiz</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/roads/psiz-brm">https://github.com/roads/psiz-brm</ext-link>. None of the experiments were preregistered.</p>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <mixed-citation publication-type="other">Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., &amp; Zheng, X. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link> (Software available from <ext-link ext-link-type="uri" xlink:href="https://tensorflow.org">https://tensorflow.org</ext-link>).</mixed-citation>
    </ref>
    <ref id="CR2">
      <mixed-citation publication-type="other">Agarwal, S., Wills, J., Cayton, L., Lanckriet, G., Kriegman, D., &amp; Belongie, S. (2007). Generalized non-metric multidimensional scaling, (2007 21–24 Mar). In M. MeilaX, &amp; X. Shen (Eds.) <italic>Proceedings of the eleventh international conference on artificial intelligence and statistics</italic>. San Juan, Puerto Rico PMLR. <ext-link ext-link-type="uri" xlink:href="http://proceedings.mlr.press/v2/agarwal07a.html">http://proceedings.mlr.press/v2/agarwal07a.html</ext-link>, (Vol. 2 pp. 11–18).</mixed-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Carroll</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>Analysis of individual differences in multidimensional scaling via an N-way generalization of “Eckart–Young”? decomposition</article-title>
        <source>Psychometrika</source>
        <year>1970</year>
        <volume>35</volume>
        <issue>3</issue>
        <fpage>283</fpage>
        <lpage>319</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02310791</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Demiralp</surname>
            <given-names>Ç</given-names>
          </name>
          <name>
            <surname>Bernstein</surname>
            <given-names>MS</given-names>
          </name>
          <name>
            <surname>Heer</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Learning perceptual kernels for visualization design</article-title>
        <source>IEEE Transactions on Visualization and Computer Graphics</source>
        <year>2014</year>
        <volume>20</volume>
        <issue>12</issue>
        <fpage>1933</fpage>
        <lpage>1942</lpage>
        <pub-id pub-id-type="doi">10.1109/TVCG.2014.2346978</pub-id>
        <pub-id pub-id-type="pmid">26356907</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <mixed-citation publication-type="other">Fang, Y., &amp; Geman, D. (2005). Experiments in mental face retrieval. In T. Kanade, A. Jain, &amp; N. K. Ratha (Eds.) <italic>Audio-and video-based biometric person authentication</italic> (pp. 637–646): Springer-Verlag.</mixed-citation>
    </ref>
    <ref id="CR6">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ferecatu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Geman</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>A statistical framework for image category search from a mental picture</article-title>
        <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        <year>2009</year>
        <volume>31</volume>
        <issue>6</issue>
        <fpage>1087</fpage>
        <lpage>1101</lpage>
        <pub-id pub-id-type="doi">10.1109/TPAMI.2008.259</pub-id>
        <?supplied-pmid 19372612?>
        <pub-id pub-id-type="pmid">19372612</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gower</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>Some distance properties of latent root and vector methods used in multivariate analysis</article-title>
        <source>Biometrika</source>
        <year>1966</year>
        <volume>53</volume>
        <fpage>325</fpage>
        <lpage>338</lpage>
        <pub-id pub-id-type="doi">10.1093/biomet/53.3-4.325</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <mixed-citation publication-type="other">Jamieson, K. G., Jain, L., Fernandez, C., Glattard, N. J., &amp; Nowak, R. (2015). Next: A system for real-world development, evaluation, and application of active learning. In <italic>Advances in neural information processing systems</italic> (pp. 2656–2664).</mixed-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jones</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Love</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Maddox</surname>
            <given-names>WT</given-names>
          </name>
        </person-group>
        <article-title>Recency effects as a window to generalization: Separating decisional and perceptual sequential effects in category learning</article-title>
        <source>Journal of Experimental Psychology: Learning, Memory, &amp; Cognition</source>
        <year>2006</year>
        <volume>32</volume>
        <fpage>316</fpage>
        <lpage>332</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <mixed-citation publication-type="other">Jones, M., Maddox, W. T., &amp; Love, B. C. (2006). The role of similarity in generalization. In <italic>Proceedings of the 28th annual meeting of the Cognitive Science Society</italic> (pp. 405–410).</mixed-citation>
    </ref>
    <ref id="CR11">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kruschke</surname>
            <given-names>JK</given-names>
          </name>
        </person-group>
        <article-title>ALCOVE: an exemplar-based connectionist model of category learning</article-title>
        <source>Psychological Review</source>
        <year>1992</year>
        <volume>99</volume>
        <issue>1</issue>
        <fpage>22</fpage>
        <lpage>44</lpage>
        <pub-id pub-id-type="doi">10.1037/0033-295X.99.1.22</pub-id>
        <pub-id pub-id-type="pmid">1546117</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kruskal</surname>
            <given-names>JB</given-names>
          </name>
        </person-group>
        <article-title>Multidimensional scaling by optimizing goodness of fitto a nonmetric hypothesis</article-title>
        <source>Psychometrika</source>
        <year>1968</year>
        <volume>29</volume>
        <fpage>1</fpage>
        <lpage>27</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02289565</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kruskal</surname>
            <given-names>JB</given-names>
          </name>
        </person-group>
        <article-title>Nonmetric multidimensional scaling: A numerical method</article-title>
        <source>Psychometrika</source>
        <year>1968</year>
        <volume>29</volume>
        <fpage>115</fpage>
        <lpage>130</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02289694</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Malave</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Extracting human face similarity judgments: Pairs or triplets?</article-title>
        <source>Journal of Vision</source>
        <year>2016</year>
        <volume>16</volume>
        <fpage>719</fpage>
        <pub-id pub-id-type="doi">10.1167/16.12.719</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Love</surname>
            <given-names>BC</given-names>
          </name>
          <name>
            <surname>Medin</surname>
            <given-names>DL</given-names>
          </name>
          <name>
            <surname>Gureckis</surname>
            <given-names>TM</given-names>
          </name>
        </person-group>
        <article-title>Sustain: A network model of category learning</article-title>
        <source>Psychological Review</source>
        <year>2004</year>
        <volume>111</volume>
        <issue>2</issue>
        <fpage>309</fpage>
        <lpage>332</lpage>
        <pub-id pub-id-type="doi">10.1037/0033-295X.111.2.309</pub-id>
        <pub-id pub-id-type="pmid">15065912</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Luce</surname>
            <given-names>RD</given-names>
          </name>
        </person-group>
        <source>Individual choice behavior: A theoretical analysis</source>
        <year>1959</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Wiley</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR17">
      <mixed-citation publication-type="other">Murray, I., Adams, R. P., &amp; MacKay, D. J. (2010). Elliptical slice sampling. In <italic>Proceedings of the 13th international conference on artificial intelligence and statistics (AISTATS)</italic>.</mixed-citation>
    </ref>
    <ref id="CR18">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nosofsky</surname>
            <given-names>RM</given-names>
          </name>
        </person-group>
        <article-title>Choice, similarity, and the context theory of classification</article-title>
        <source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source>
        <year>1984</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>104</fpage>
        <lpage>114</lpage>
        <pub-id pub-id-type="doi">10.1037/0278-7393.10.1.104https://doi.org/10.1037/0278-7393.10.1.104</pub-id>
        <?supplied-pmid 6242730?>
        <pub-id pub-id-type="pmid">6242730</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nosofsky</surname>
            <given-names>RM</given-names>
          </name>
        </person-group>
        <article-title>Overall similarity and the identification of separable-dimension stimuli: A choice model analysis</article-title>
        <source>Perception &amp; Psychophysics</source>
        <year>1985</year>
        <volume>38</volume>
        <issue>5</issue>
        <fpage>415</fpage>
        <lpage>432</lpage>
        <pub-id pub-id-type="doi">10.3758/BF03207172</pub-id>
        <pub-id pub-id-type="pmid">3831920</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nosofsky</surname>
            <given-names>RM</given-names>
          </name>
        </person-group>
        <article-title>Attention, similarity, and the identification-categorization relationship</article-title>
        <source>Journal of Experimental Psychology: General</source>
        <year>1986</year>
        <volume>115</volume>
        <fpage>39</fpage>
        <lpage>57</lpage>
        <pub-id pub-id-type="doi">10.1037/0096-3445.115.1.39</pub-id>
        <pub-id pub-id-type="pmid">2937873</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nosofsky</surname>
            <given-names>RM</given-names>
          </name>
          <name>
            <surname>Sanders</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>McDaniel</surname>
            <given-names>MA</given-names>
          </name>
        </person-group>
        <article-title>Tests of an exemplar-memory model of classification learning in a high-dimensional natural-science category domain</article-title>
        <source>Journal of Experimental Psychology: General</source>
        <year>2018</year>
        <volume>147</volume>
        <issue>3</issue>
        <fpage>328</fpage>
        <lpage>353</lpage>
        <pub-id pub-id-type="doi">10.1037/xge0000369</pub-id>
        <pub-id pub-id-type="pmid">29058939</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <mixed-citation publication-type="other">Rau, M. A., Mason, B., &amp; Nowak, R. (2016). How to model implicit knowledge? Similarity learning methods to assess perceptions of visual representations. In <italic>International Educational Data Mining Society</italic>.</mixed-citation>
    </ref>
    <ref id="CR23">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Roads</surname>
            <given-names>BD</given-names>
          </name>
          <name>
            <surname>Mozer</surname>
            <given-names>MC</given-names>
          </name>
        </person-group>
        <article-title>Improving human-machine cooperative classification via cognitive theories of similarity</article-title>
        <source>Cognitive Science: an Multidisciplinary Journal</source>
        <year>2017</year>
        <volume>41</volume>
        <fpage>1394</fpage>
        <lpage>1411</lpage>
        <pub-id pub-id-type="doi">10.1111/cogs.12400</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <mixed-citation publication-type="other">Roads, B. D., Xu, B., Robinson, J. K., &amp; Tanaka, J. W. (2018). The easy-to-hard training advantage with real-world medical images. Cognitive Research: Principles and Implications, 3(38). 10.1186/s41235-018-0131-6</mixed-citation>
    </ref>
    <ref id="CR25">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shepard</surname>
            <given-names>RN</given-names>
          </name>
        </person-group>
        <article-title>Stimulus and response generalization: A stochastic model relating generalization to distance in psychological space</article-title>
        <source>Psychometrika</source>
        <year>1957</year>
        <volume>22</volume>
        <issue>4</issue>
        <fpage>325</fpage>
        <lpage>345</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02288967</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shepard</surname>
            <given-names>RN</given-names>
          </name>
        </person-group>
        <article-title>Stimulus and response generalization: Tests of a model relating generalization to distance in psychological space</article-title>
        <source>Journal of Experimental Psychology</source>
        <year>1958</year>
        <volume>55</volume>
        <issue>6</issue>
        <fpage>509</fpage>
        <lpage>523</lpage>
        <pub-id pub-id-type="doi">10.1037/h0042354</pub-id>
        <pub-id pub-id-type="pmid">13563763</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shepard</surname>
            <given-names>RN</given-names>
          </name>
        </person-group>
        <article-title>The analysis of proximities: Multidimensional scaling with an unknown distance function. I</article-title>
        <source>Psychometrika</source>
        <year>1962</year>
        <volume>27</volume>
        <issue>2</issue>
        <fpage>125</fpage>
        <lpage>140</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02289630</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shepard</surname>
            <given-names>RN</given-names>
          </name>
        </person-group>
        <article-title>The analysis of proximities: Multidimensional scaling with an unknown distance function. II</article-title>
        <source>Psychometrika</source>
        <year>1962</year>
        <volume>27</volume>
        <issue>3</issue>
        <fpage>219</fpage>
        <lpage>246</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02289621</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shepard</surname>
            <given-names>RN</given-names>
          </name>
        </person-group>
        <article-title>Geometrical approximations to the structure of musical pitch</article-title>
        <source>Psychological Review</source>
        <year>1982</year>
        <volume>89</volume>
        <issue>4</issue>
        <fpage>305</fpage>
        <lpage>333</lpage>
        <pub-id pub-id-type="doi">10.1037/0033-295X.89.4.305</pub-id>
        <?supplied-pmid 7134331?>
        <pub-id pub-id-type="pmid">7134331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Shepard</surname>
            <given-names>RN</given-names>
          </name>
        </person-group>
        <article-title>Toward a universal law of generalization for psychological science</article-title>
        <source>Science</source>
        <year>1987</year>
        <volume>237</volume>
        <fpage>1317</fpage>
        <lpage>1323</lpage>
        <pub-id pub-id-type="doi">10.1126/science.3629243</pub-id>
        <pub-id pub-id-type="pmid">3629243</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <mixed-citation publication-type="other">Sievert, S., Ross, D., Jain, L., Jamieson, K., Nowak, R., &amp; Mankoff, R. (2017). NEXT: A system to easily connect crowd sourcing and adaptive data collection. In <italic>Proceedings of the 16th Python in science conference</italic> (pp. 113–119).</mixed-citation>
    </ref>
    <ref id="CR32">
      <mixed-citation publication-type="other">Tamuz, O., Liu, C., Belongie, S., Shamir, O., &amp; Kalai, A. T. (2011). Adaptively learning the crowd kernel. arXiv preprint arXiv:<ext-link ext-link-type="uri" xlink:href="http://arXiv.org/abs/1105.1033">http://arXiv.org/abs/1105.1033</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR33">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tanaka</surname>
            <given-names>JW</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Object categories and expertise: Is the basic level in the eye of the beholder?</article-title>
        <source>Cognitive Psychology</source>
        <year>1991</year>
        <volume>23</volume>
        <issue>3</issue>
        <fpage>457</fpage>
        <lpage>482</lpage>
        <pub-id pub-id-type="doi">10.1016/0010-0285(91)90016-H</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <mixed-citation publication-type="other">Tenenbaum, J. B. (1999). Bayesian modeling of human concept learning. In D. C. M. Kearns, &amp; S. Solla (Eds.) <italic>Advances in neural information processing systems 11</italic> (pp. 59–65). Cambridge: MIT Press.</mixed-citation>
    </ref>
    <ref id="CR35">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Torgerson</surname>
            <given-names>WS</given-names>
          </name>
        </person-group>
        <article-title>Multidimensional scaling: I. Theory and method</article-title>
        <source>Psychometrika</source>
        <year>1952</year>
        <volume>17</volume>
        <issue>4</issue>
        <fpage>401</fpage>
        <lpage>419</lpage>
        <pub-id pub-id-type="doi">10.1007/BF02288916</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Torgerson</surname>
            <given-names>WS</given-names>
          </name>
        </person-group>
        <source>Theory and methods of scaling</source>
        <year>1958</year>
        <publisher-loc>Wiley</publisher-loc>
        <publisher-name>New York</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR37">
      <mixed-citation publication-type="other">van der Maaten, L., &amp; Weinberger, K. (2012). Stochastic triplet embedding. In <italic>2012 IEEE international workshop on machine learning for signal processing (MLSP)</italic>. 10.1109/MLSP.2012.6349720 (pp. 1–6).</mixed-citation>
    </ref>
    <ref id="CR38">
      <mixed-citation publication-type="other">Wah, C., Branson, S., Welinder, P., Perona, P., &amp; Belongie, S. (2011). The Caltech-UCSDBirds-200-2011 Dataset (Tech. Rep. No. CNS-TR-2011-001). California Institute of Technology.</mixed-citation>
    </ref>
    <ref id="CR39">
      <mixed-citation publication-type="other">Wah, C., Horn, G. V., Branson, S., Maji, S., Perona, P., &amp; Belongie, S. (2014). Similarity comparisons for interactive fine-grained categorization. In <italic>Computer Vision and Pattern Recognition (CVPR). Columbus, OH</italic>.</mixed-citation>
    </ref>
    <ref id="CR40">
      <mixed-citation publication-type="other">Wilber, M. J., Kwak, I. S., &amp; Belongie, S. J. (2014). Cost-effective hits for relative similarity comparisons. In <italic>Second AAAI conference on human computation and crowd sourcing</italic>.</mixed-citation>
    </ref>
  </ref-list>
</back>
