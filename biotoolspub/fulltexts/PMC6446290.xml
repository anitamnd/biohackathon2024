<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6446290</article-id>
    <article-id pub-id-type="publisher-id">2754</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-019-2754-0</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>FeatureSelect: a software for feature selection based on machine learning approaches</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Masoudi-Sobhanzadeh</surname>
          <given-names>Yosef</given-names>
        </name>
        <address>
          <email>masoudi.sobhanzad@ut.ac.ir</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Motieghader</surname>
          <given-names>Habib</given-names>
        </name>
        <address>
          <email>habib_moti@ut.ac.ir</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Masoudi-Nejad</surname>
          <given-names>Ali</given-names>
        </name>
        <address>
          <email>amasoudin@ut.ac.ir</email>
          <uri>http://LBB.ut.ac.ir</uri>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 0612 7950</institution-id><institution-id institution-id-type="GRID">grid.46072.37</institution-id><institution>Laboratory of system Biology and Bioinformatics, Institute of Biochemistry and Biophysics, </institution><institution>University of Tehran, </institution></institution-wrap>Tehran, Iran </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>3</day>
      <month>4</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>3</day>
      <month>4</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>20</volume>
    <elocation-id>170</elocation-id>
    <history>
      <date date-type="received">
        <day>6</day>
        <month>12</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>19</day>
        <month>3</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s). 2019</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Feature selection, as a preprocessing stage, is a challenging problem in various sciences such as biology, engineering, computer science, and other fields. For this purpose, some studies have introduced tools and softwares such as WEKA. Meanwhile, these tools or softwares are based on filter methods which have lower performance relative to wrapper methods. In this paper, we address this limitation and introduce a software application called FeatureSelect. In addition to filter methods, FeatureSelect consists of optimisation algorithms and three types of learners. It provides a user-friendly and straightforward method of feature selection for use in any kind of research, and can easily be applied to any type of balanced and unbalanced data based on several score functions like accuracy, sensitivity, specificity, etc.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In addition to our previously introduced optimisation algorithm (WCC), a total of 10 efficient, well-known and recently developed algorithms have been implemented in FeatureSelect. We applied our software to a range of different datasets and evaluated the performance of its algorithms. Acquired results show that the performances of algorithms are varying on different datasets, but WCC, LCA, FOA, and LA are suitable than others in the overall state. The results also show that wrapper methods are better than filter methods.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">FeatureSelect is a feature or gene selection software application which is based on wrapper methods. Furthermore, it includes some popular filter methods and generates various comparison diagrams and statistical measurements. It is available from GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/LBBSoft/FeatureSelect">https://github.com/LBBSoft/FeatureSelect</ext-link>) and is free open source software under an MIT license.</p>
      </sec>
      <sec>
        <title>Electronic supplementary material</title>
        <p>The online version of this article (10.1186/s12859-019-2754-0) contains supplementary material, which is available to authorized users.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Feature selection</kwd>
      <kwd>Gene selection</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Classification</kwd>
      <kwd>Regression</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2019</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par24">Data preprocessing is an essential component of many classification and regression problems. Some data have an identical effect, some have a misleading effect and others have no effect on classification or regression problems, and the selection of an optimal and minimum size for features can therefore be useful [<xref ref-type="bibr" rid="CR1">1</xref>]. A classification or regression problem will involve a high time complexity and low performance when a large number of features is used, but will have a low time complexity and high performance for a minimum size and the most effective features. The selection of an optimal set of features with which a classifier or a model can achieve its maximum performance is an nondeterministic polynomial (NP) problem [<xref ref-type="bibr" rid="CR2">2</xref>]. Meta-heuristic and heuristic approaches can be applied to NP problems. Optimisation algorithms, which are a type of meta-heuristic algorithm, are usually more efficient than other meta-heuristic algorithms. After selecting an optimal subset of features, a classifier can properly classify the data, or a regression model can be constructed to estimate the relationships between variables. A classifier or a regression model can be created using three methods [<xref ref-type="bibr" rid="CR3">3</xref>]: (i) a supervised method, in which a learner is aware of data labels; (ii) an unsupervised method, in which a learner is unaware of data labels and tries to find the relationship between data; and (iii) a semi-supervised method in which labels of some data are determined whereas others are not specified. In this method, a learner is usually trained using the both labeled and unlabeled samples. This paper introduces a software application named FeatureSelect in which three types of learner are available in: 1- SVM: A support vector machine (SVM) is one possible supervised learning method that can be applied to classification and regression problems. The aim of an SVM is to determine a line that divides two groups with the greatest margin of confidence [<xref ref-type="bibr" rid="CR4">4</xref>]. 2- ANN: Like SVM, an artificial neural network (ANN) is a supervised learner and tries to find relation between inputs and outputs. 3- DT: Decision tree (DT) is one of the other supervised learners which can be employed for machine learning applications. FeatureSelect comprises two steps: (i) it selects an optimal subset of features using optimisation algorithms; and (ii) it uses a learner (SVM, ANN and DT) to create a classification or a regression model. After each run, FeatureSelect calculates the required statistical results for regression and classification problems, including sensitivity, fall-out, precision, convergence and stability diagrams for error, accuracy and classification, standard deviation, confidence interval and many other essential statistical results. FeatureSelect is straightforward to use and can be applied within many different fields.</p>
    <p id="Par25">Feature extraction and selection are two main steps in machine learning applications. In feature extraction, some attributes of the existing data, intended to be informative, are extracted. As an instance, we can point out some biologically related works such as Pse-in-One [<xref ref-type="bibr" rid="CR5">5</xref>] and ProtrWeb [<xref ref-type="bibr" rid="CR6">6</xref>] which enable users to acquire some features from biological sequences like DNA, RNA, or protein. However, all of the derived features are not constructive in process of learning a machine. Therefore, feature selection methods which are used in various fields such as drug design, disease classification, image processing, text mining, handwriting recognition, spoken word recognition, social networks, and many others, are essential. We divide related works into five categories: (i) filter-based; (ii) wrapper-based; (iii) embedded-based; (iv) online-based; (v) and hybrid-based. Some of the more recently proposed methods and algorithms based on mentioned categories are described below.</p>
    <sec id="Sec2">
      <title>(i) Filter-based</title>
      <p id="Par26">Because filter methods, which does not use a learning method and only considers the relevance between features, have low time complexity; many of researchers focused on these methods. In one of related works, a filter-based method has been introduced for use in online stream feature selection applications. This method has acceptable stability and scalability, and can also be used in offline feature selection applications. However, filter feature selection methods may ignore certain informative features [<xref ref-type="bibr" rid="CR7">7</xref>]. In some cases, data are unbalanced; in other words, they are in a state of skewness. Feature selection for linear data types has also been studied, in a work that provides a framework and selects features with maximum relevance and minimum redundancy. This framework has been compared with state-of-the-art algorithms, and has been applied to nonlinear data [<xref ref-type="bibr" rid="CR8">8</xref>].</p>
    </sec>
    <sec id="Sec3">
      <title>(ii) wrapper-based</title>
      <p id="Par27">These methods evaluate usefulness of selected features using learner’s performance [<xref ref-type="bibr" rid="CR9">9</xref>]. In a separate study, a feature selection method was proposed in which both unbalanced and balanced data can be classified, based on a genetic algorithm. However, it has been proved that other optimisation algorithms can be more efficient than the genetic algorithm [<xref ref-type="bibr" rid="CR10">10</xref>]. Feature selection methods not only improve the performance of the model but also facilitate the analysis of the results. One study examines the use of SVMs in multiclass problems. This work proposes an iterative method based on a features list combination that ranks the features and examines only features list combination strategies. The results show that a one-by-one strategy is better than the other strategies examined, for real-world datasets [<xref ref-type="bibr" rid="CR11">11</xref>].</p>
    </sec>
    <sec id="Sec4">
      <title>(iii) embedded-based</title>
      <p id="Par28">Embedded methods select features when a model is made. For example, the methods which select features using decision tree are placed in this category. One of the embedded methods investigates feature selection with regard to the relationships between features and labels and the relationships among features. The method proposed in this study was applied to customer classification data, and the proposed algorithm was trained using deterministic score models such as the Fisher score, the Laplacian score, and two semi-supervised algorithms. This method can also be trained using fewer samples, and stochastic algorithms can improve the performance of the algorithm [<xref ref-type="bibr" rid="CR12">12</xref>]. As mentioned above, feature selection is currently a topic of great research interest in the field of machine learning. The nature of the features and the degree to which they can be distinguished are not considered. The concept has been introduced and examined for benchmark datasets by Liu, et al. This method is appropriate for multimodal data types [<xref ref-type="bibr" rid="CR13">13</xref>].</p>
    </sec>
    <sec id="Sec5">
      <title>(iv) online-based</title>
      <p id="Par29">These methods select features using online user tips. In a related work, a feature cluster taxonomy feature selection (FCTFS) method has been introduced. The main goal of FCTFS is the selection of features based on a user-guided mode. The accuracy of this method is lower than that of the other methods [<xref ref-type="bibr" rid="CR14">14</xref>]. In a separate study, an online feature selection method based on the dependency on the k nearest neighbours (k-OFSD) has been proposed, and this is suitable for high-dimensional datasets. The main motivation for the abovementioned work is the selection of features with a higher ability to separate those for which the performance has been examined using unbalanced data [<xref ref-type="bibr" rid="CR15">15</xref>]. A library of online feature selection (LOFS) has also been developed using the state-of-art algorithms, for use with MATLAB and OCTAVE. Since the performance of LOFS has not been examined for a range of datasets, its performance has not been investigated [<xref ref-type="bibr" rid="CR16">16</xref>].</p>
    </sec>
    <sec id="Sec6">
      <title>(v) Hybrid-based</title>
      <p id="Par30">These methods are combination of four above categories. For example, some related works use two-step feature selection methods [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR18">18</xref>]. In these methods, a number of features are reduced by the first method, and the second method is then used for further reduction [<xref ref-type="bibr" rid="CR19">19</xref>]. While some works focus on only one of these categories, a hybrid two-step feature selection method, which combines the filter and wrapper methods, has been proposed for multi-word recognition. It is possible to remove the most discriminative features in the filter method, so that this method is solely dependent on the filter stage [<xref ref-type="bibr" rid="CR20">20</xref>]. DNA microarray datasets usually have a large size and a large number of features, and feature selection can reduce the size of this dataset, allowing a classifier to properly classify the data. For this purpose, a new hybrid algorithm has been suggested that combines the maximisation of mutual information with a genetic algorithm. Although the proposed method increases the accuracy, it appears that other state-of-the-art optimisation algorithms can improve accuracy to a greater extent than the genetic algorithm [<xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR23">23</xref>]. Defining a framework for the relationship between Bayesian error and mutual information [<xref ref-type="bibr" rid="CR24">24</xref>], and proposing a discrete optimisation algorithm based on opinion formation [<xref ref-type="bibr" rid="CR25">25</xref>] are other hybrid methods.</p>
      <p id="Par31">Other recent topics of study include review studies or feature selection in special area. A comprehensive and extensive review of over various relevant works was carried out by researchers. The scope, applications and restrictions of these works were also investigated [<xref ref-type="bibr" rid="CR26">26</xref>–<xref ref-type="bibr" rid="CR28">28</xref>]. Some other related works are as below: Unsupervised feature selection methods [<xref ref-type="bibr" rid="CR29">29</xref>–<xref ref-type="bibr" rid="CR31">31</xref>], feature selection using a variable number of features [<xref ref-type="bibr" rid="CR32">32</xref>], connecting data characteristics using feature selection [<xref ref-type="bibr" rid="CR33">33</xref>–<xref ref-type="bibr" rid="CR36">36</xref>], a new method for feature selection using feature self-representation and a low-rank representation [<xref ref-type="bibr" rid="CR36">36</xref>], integrating feature selection algorithms [<xref ref-type="bibr" rid="CR37">37</xref>], financial distress prediction using feature selection [<xref ref-type="bibr" rid="CR38">38</xref>], and feature selection based on a Morisita estimator for regression problems [<xref ref-type="bibr" rid="CR39">39</xref>]. Figure <xref rid="Fig1" ref-type="fig">1</xref> summarizes and describes the above categories in a graphical manner.<fig id="Fig1"><label>Fig. 1</label><caption><p>Classification of the related works. They have been categorized into five classes, including: (i) Filter method which scores features and then selects them. (ii) Wrapper method which scores a subset of features based on a learner performance. (iii) Embedded method which selects features based on the order that a learner selects them. (iv) Online method which is based online tools. (V) Hybrid method which combines different methods in order to acquire better results</p></caption><graphic xlink:href="12859_2019_2754_Fig1_HTML" id="MO1"/></fig></p>
      <p id="Par32">FeatureSelect is placed in the filter, wrapper, and hybrid categories. In the wrapper method, FeatureSelect scores a subset of features instead of scoring features separately. To this end, the optimization algorithms select a subset of features. Next, the selected subset is scored by a learner. In addition to the wrapper method, FeatureSelect includes 5 filter methods which can score features using Laplacian [<xref ref-type="bibr" rid="CR40">40</xref>], entropy [<xref ref-type="bibr" rid="CR41">41</xref>], Fisher [<xref ref-type="bibr" rid="CR42">42</xref>], Pearson-correlation [<xref ref-type="bibr" rid="CR43">43</xref>], and mutual information [<xref ref-type="bibr" rid="CR44">44</xref>] scores. After scoring, it selects features based on their scores. Furthermore, this software can be used in a hybrid manner. For example, a user can reduce the number of features using the filter method. Then, the reduced set can be used as input for the wrapper method in order to enhance the performance.</p>
    </sec>
  </sec>
  <sec id="Sec7">
    <title>Implementation</title>
    <p id="Par33">Data classification is a subject that has attracted a great deal of research interest in the domain of machine learning applications. An SVM can be used to construct a hyperplane between groups of data, and this approach can be applied to linear or multiclass classification and regression problems. The hyperplane has a suitable separation ability if it can maintain the largest distance from the points in either class; in other words, the high separation ability of the hyperplane is determined by a functional margin. The higher the value of a functional margin, the lower is the error in the value [<xref ref-type="bibr" rid="CR45">45</xref>]. Several modified versions of an SVM have also been proposed [<xref ref-type="bibr" rid="CR46">46</xref>].</p>
    <p id="Par34">Because SVM is a popular classifier in the area of machine learning, Chang and Lin have designed a library for support vector machine named LIBSVM [<xref ref-type="bibr" rid="CR47">47</xref>], which has several important properties, as follows:<list list-type="alpha-lower"><list-item><p id="Par35">It can easily be linked to different programing languages such as MATLAB, Java, Phyton, LISP, CLISP, WEKA, R, C#, PHP, Haskell, Perl and Ruby;</p></list-item><list-item><p id="Par36">Various SVM formulations and kernels are available;</p></list-item><list-item><p id="Par37">It provides a weighted SVM for unbalanced data;</p></list-item><list-item><p id="Par38">Cross-validation can be applied to the model selection.</p></list-item></list></p>
    <p id="Par39">In addition to SVM, ANN and DT are also available as learners in FeatureSelect. In the implementation of FeatureSelect, ANN has been implemented whereas SVM and DT have been added to it as a library. ANN, which includes some hidden layers and some neurons in them and can be applied to both classification and regression problems, has been inspired by neural system of living organisms [<xref ref-type="bibr" rid="CR48">48</xref>]. Like SVM and ANN, DT can also be used for both classification and regression issues. DT operates based on tree-like graph model and develops a tree step by step by adding new constraints which lead to desired consequences [<xref ref-type="bibr" rid="CR49">49</xref>].</p>
    <p id="Par40">The framework of FeatureSelect is depicted in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. The rectangles represent the interaction between FeatureSelect and the user, and the circles represent FeatureSelect processes.<fig id="Fig2"><label>Fig. 2</label><caption><p>Framework of FeatureSelect</p></caption><graphic xlink:href="12859_2019_2754_Fig2_HTML" id="MO2"/></fig></p>
    <p id="Par41">FeatureSelect consists of six main parts: (i) an input file is selected, and is then fuzzified or normalised if necessary, since this can enhance the learner’s functionality; (ii) using a suitable GUI, one of the learners is chosen for classification or regression purpose, and its parameters is adjusted; (iii) one of the two available methods, filter or wrapper method, is selected for feature selection, and then the selected method parameters are determined. In wrapper methods, the list of optimisation algorithms is available. We investigated the performance of 33 optimisation algorithms and have selected 11 state-of-the-art algorithms based on their different natures and performance (Table <xref rid="Tab1" ref-type="table">1</xref>).<table-wrap id="Tab1"><label>Table 1</label><caption><p>Implemented algorithms</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Algorithm name</th><th>Abrr.</th><th>Operations on population</th><th>Pub.</th><th>Ref</th></tr></thead><tbody><tr><td>World competitive contests</td><td>WCC</td><td>Attacking, shooting, passing, crossing</td><td>2016</td><td>[<xref ref-type="bibr" rid="CR61">61</xref>]</td></tr><tr><td>League championship algorithm</td><td>LCA</td><td>Playing, transfer</td><td>2014</td><td>[<xref ref-type="bibr" rid="CR62">62</xref>]</td></tr><tr><td>Genetic algorithm</td><td>GA</td><td>Crossover, mutation</td><td>1970</td><td>[<xref ref-type="bibr" rid="CR63">63</xref>]</td></tr><tr><td>Particle swarm optimisation</td><td>PSO</td><td>Social behavior</td><td>1995</td><td>[<xref ref-type="bibr" rid="CR64">64</xref>]</td></tr><tr><td>Ant colony optimisation</td><td>ACO</td><td>Edge selection, update pheromone</td><td>2006</td><td>[<xref ref-type="bibr" rid="CR65">65</xref>]</td></tr><tr><td>Imperialist competitive algorithm</td><td>ICA</td><td>Revolution, absorb, move</td><td>2007</td><td>[<xref ref-type="bibr" rid="CR66">66</xref>]</td></tr><tr><td>Learning automata</td><td>LA</td><td>Award, penalize</td><td>2003</td><td>[<xref ref-type="bibr" rid="CR67">67</xref>]</td></tr><tr><td>Heat transfer optimisation</td><td>HTS</td><td>Molecules conductions</td><td>2015</td><td>[<xref ref-type="bibr" rid="CR68">68</xref>]</td></tr><tr><td>Forest optimisation algorithm</td><td>FOA</td><td>Local seeding, global seeding</td><td>2014</td><td>[<xref ref-type="bibr" rid="CR69">69</xref>]</td></tr><tr><td>Discrete symbiotic organisms search</td><td>DSOS</td><td>Mutualism, commensalism, parasitism</td><td>2017</td><td>[<xref ref-type="bibr" rid="CR70">70</xref>]</td></tr><tr><td>Cuckoo optimisation algorithm</td><td>CUK</td><td>Eggs laying, eggs killing, eggs growing</td><td>2011</td><td>[<xref ref-type="bibr" rid="CR71">71</xref>]</td></tr></tbody></table></table-wrap></p>
    <p id="Par42">(iv) Selected features are evaluated by selected learner. For this purpose, three types of learner can be chosen and adjusted.</p>
    <p id="Par43">(v) <italic>FeatureSelect</italic> generates various types of results, based on the nature of the problem and selected method, and compares selected algorithms or methods with each other. The status of the executions and selected optimisation algorithms are available in the sixth section.</p>
    <p id="Par44">The relevant properties of FeatureSelect are described below:<list list-type="alpha-lower"><list-item><p id="Par45">Data fuzzification and data normalisation capabilities are available. Data are converted to the range [0,1] in both the fuzzification and normalisation stages. TXT, XLS and MAT formats are acceptable as formats for the input file. Data normalisation is carried out as shown in Eq. <xref rid="Equ1" ref-type="">1</xref>.</p></list-item></list></p>
    <p id="Par46">
      <disp-formula id="Equ1">
        <label>1</label>
        <alternatives>
          <tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\mathrm{v}}^{\hbox{'}}=\mathrm{low}+\frac{\left(v-v\mathit{\min}\right)\times \left( high- low\right)}{\left(v\max -v\min \right)} $$\end{document}</tex-math>
          <mml:math id="M2" display="block">
            <mml:msup>
              <mml:mi mathvariant="normal">v</mml:mi>
              <mml:mo>'</mml:mo>
            </mml:msup>
            <mml:mo>=</mml:mo>
            <mml:mi>low</mml:mi>
            <mml:mo>+</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mfenced close=")" open="(">
                  <mml:mrow>
                    <mml:mi>v</mml:mi>
                    <mml:mo>−</mml:mo>
                    <mml:mi>v</mml:mi>
                    <mml:mo mathvariant="italic">min</mml:mo>
                  </mml:mrow>
                </mml:mfenced>
                <mml:mo>×</mml:mo>
                <mml:mfenced close=")" open="(">
                  <mml:mrow>
                    <mml:mtext mathvariant="italic">high</mml:mtext>
                    <mml:mo>−</mml:mo>
                    <mml:mi mathvariant="italic">low</mml:mi>
                  </mml:mrow>
                </mml:mfenced>
              </mml:mrow>
              <mml:mfenced close=")" open="(">
                <mml:mrow>
                  <mml:mi>v</mml:mi>
                  <mml:mo>max</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mi>v</mml:mi>
                  <mml:mo>min</mml:mo>
                </mml:mrow>
              </mml:mfenced>
            </mml:mfrac>
          </mml:math>
          <graphic xlink:href="12859_2019_2754_Article_Equ1.gif" position="anchor"/>
        </alternatives>
      </disp-formula>
    </p>
    <p id="Par47">where v’, v, vmax, vmin, high and low are the normalised value, the current value to be normalised, the maximum and minimum values of the group, and the higher and the lower bounds of the range, respectively. High and low are configured to one and zero respectively in FeatureSelect. Fuzzification is the process that convert scalar values to fuzzy values [<xref ref-type="bibr" rid="CR50">50</xref>]. Figure <xref rid="Fig3" ref-type="fig">3</xref> illustrates the fuzzy membership function used in FeatureSelect.<list list-type="simple"><list-item><label>b)</label><p id="Par48">It provides a suitable graphical user interface for LIBSVM. For example, researchers can select LIBSVM’s learning parameters and apply them to their applications after selecting the input data (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). If a researcher is unfamiliar with the training and testing functions in LIBSVM, he/she can easily use LIBSVM by clicking on the corresponding buttons.</p></list-item><list-item><label>c)</label><p id="Par49">Optimisation algorithms, which are used for feature selection, have been tested and the correctness of them has been examined. Researchers can select one or more of these optimisation algorithms using the relevant box.</p></list-item><list-item><label>d)</label><p id="Par50">A user can select different types of learners and feature selection methods, and employee them as ensemble feature selection method. For example, a user can reduce the number of available features by filter methods, and then can use optimisation algorithms or other methods in order to acquire better results.</p></list-item><list-item><label>e)</label><p id="Par51">After executing a selected algorithm in a regression problem, FeatureSelect automatically generates useful diagrams and tables, such as the error convergence, error average convergence, error stability, correlation convergence, correlation average convergence and correlation stability diagrams for the selected algorithms in. In classification problems, results include: the accuracy convergence, the accuracy average convergence, the accuracy stability, the error convergence, the error average convergence and the error stability. For both regression and classification problems, an XLS file is generated consisting of a number of selected features, including standard deviation, <italic>P</italic>-value, confidence interval (CI) and the significance of the generated results, and a TXT file containing detailed information such as the indices of the selected features. For classification problems, certain statistical results such as accuracy, precision, false positive rate, and sensitivity are generated. Eqs. <xref rid="Equ2" ref-type="">2</xref> to <xref rid="Equ5" ref-type="">5</xref> express how these measures are computed in FeatureSelect, where ACC, PRE, FPR and SEN are abbreviations for accuracy, precision, false positive rate and sensitivity, respectively.</p></list-item></list><fig id="Fig3"><label>Fig. 3</label><caption><p>Fuzzy membership function</p></caption><graphic xlink:href="12859_2019_2754_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>Parameters for LIBSVM in FeatureSelect</p></caption><graphic xlink:href="12859_2019_2754_Fig4_HTML" id="MO4"/></fig></p>
    <p id="Par52">
      <disp-formula id="Equ2">
        <label>2</label>
        <alternatives>
          <tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{ACC}=\frac{\sum_{i=1}^n\left(\frac{TPi+ TNi}{TPi+ FNi+ FPi+ TNi}\right)\times Ci}{n} $$\end{document}</tex-math>
          <mml:math id="M4" display="block">
            <mml:mi>ACC</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:msubsup>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>n</mml:mi>
                </mml:msubsup>
                <mml:mfenced close=")" open="(">
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi mathvariant="italic">TPi</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="italic">TNi</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi mathvariant="italic">TPi</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="italic">FNi</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="italic">FPi</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="italic">TNi</mml:mi>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mfenced>
                <mml:mo>×</mml:mo>
                <mml:mi mathvariant="italic">Ci</mml:mi>
              </mml:mrow>
              <mml:mi>n</mml:mi>
            </mml:mfrac>
          </mml:math>
          <graphic xlink:href="12859_2019_2754_Article_Equ2.gif" position="anchor"/>
        </alternatives>
      </disp-formula>
      <disp-formula id="Equ3">
        <label>3</label>
        <alternatives>
          <tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{SEN}=\frac{\sum_{i=1}^n\left(\frac{TPi}{TPi+ FNi}\right)\times Ci}{n} $$\end{document}</tex-math>
          <mml:math id="M6" display="block">
            <mml:mi>SEN</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:msubsup>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>n</mml:mi>
                </mml:msubsup>
                <mml:mfenced close=")" open="(">
                  <mml:mfrac>
                    <mml:mi mathvariant="italic">TPi</mml:mi>
                    <mml:mrow>
                      <mml:mi mathvariant="italic">TPi</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="italic">FNi</mml:mi>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mfenced>
                <mml:mo>×</mml:mo>
                <mml:mi mathvariant="italic">Ci</mml:mi>
              </mml:mrow>
              <mml:mi>n</mml:mi>
            </mml:mfrac>
          </mml:math>
          <graphic xlink:href="12859_2019_2754_Article_Equ3.gif" position="anchor"/>
        </alternatives>
      </disp-formula>
      <disp-formula id="Equ4">
        <label>4</label>
        <alternatives>
          <tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{PRE}=\frac{\sum_{i=1}^n\left(\frac{TPi}{TPi+ FPi}\right)\times Ci}{n} $$\end{document}</tex-math>
          <mml:math id="M8" display="block">
            <mml:mi>PRE</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:msubsup>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>n</mml:mi>
                </mml:msubsup>
                <mml:mfenced close=")" open="(">
                  <mml:mfrac>
                    <mml:mi mathvariant="italic">TPi</mml:mi>
                    <mml:mrow>
                      <mml:mi mathvariant="italic">TPi</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="italic">FPi</mml:mi>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mfenced>
                <mml:mo>×</mml:mo>
                <mml:mi mathvariant="italic">Ci</mml:mi>
              </mml:mrow>
              <mml:mi>n</mml:mi>
            </mml:mfrac>
          </mml:math>
          <graphic xlink:href="12859_2019_2754_Article_Equ4.gif" position="anchor"/>
        </alternatives>
      </disp-formula>
      <disp-formula id="Equ5">
        <label>5</label>
        <alternatives>
          <tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{FPR}=\frac{\sum_{i=1}^n\left(\frac{FPi}{FPi+ TNi}\right)\times Ci}{n} $$\end{document}</tex-math>
          <mml:math id="M10" display="block">
            <mml:mi>FPR</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:msubsup>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>n</mml:mi>
                </mml:msubsup>
                <mml:mfenced close=")" open="(">
                  <mml:mfrac>
                    <mml:mi mathvariant="italic">FPi</mml:mi>
                    <mml:mrow>
                      <mml:mi mathvariant="italic">FPi</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi mathvariant="italic">TNi</mml:mi>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mfenced>
                <mml:mo>×</mml:mo>
                <mml:mi mathvariant="italic">Ci</mml:mi>
              </mml:mrow>
              <mml:mi>n</mml:mi>
            </mml:mfrac>
          </mml:math>
          <graphic xlink:href="12859_2019_2754_Article_Equ5.gif" position="anchor"/>
        </alternatives>
      </disp-formula>
    </p>
    <p id="Par53">FeatureSelect obtains results for the average state since it can be applied to both binary and multiple classes of classification problems. In Eqs. <xref rid="Equ2" ref-type="">2</xref> to <xref rid="Equ5" ref-type="">5</xref>, n, TP, TN, FP,,FN and C<sub>i</sub> represent the number of classes, true positive, true negative, false positive, false negative and number of samples in ith class, respectively.</p>
  </sec>
  <sec id="Sec8">
    <title>Results</title>
    <p id="Par54">FeatureSelect has been developed in the MATLAB programming language (Additional file <xref rid="MOESM1" ref-type="media">1</xref>), since this is widely used in many research fields such as computer science, biology, medicine and electrical engineering. FeatureSelect can be installed and executed on several operating systems including Windows, Linux and Mac. Moreover, MATLAB-based softwares are open-source, allowing future researchers to add new features to the source code of FeatureSelect.</p>
    <p id="Par55">In this section, we will evaluate the performance of FeatureSelect, and compare its algorithms using various datasets. The eight datasets shown in Table <xref rid="Tab2" ref-type="table">2</xref> were employed to evaluate the algorithms used in FeatureSelect. Table <xref rid="Tab2" ref-type="table">2</xref> shows the reference, name, area, number of features (NOF), number of samples (NOS) and number of dataset classes (NOC). Four datasets correspond to classification problems, while the other datasets correspond to regression problems. Using the GitHub link (<ext-link ext-link-type="uri" xlink:href="https://github.com/LBBSoft/FeatureSelect">https://github.com/LBBSoft/FeatureSelect</ext-link>), these datasets can be downloaded.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Name</th><th>Type</th><th>Area</th><th>NOF</th><th>NOS</th><th>NOC</th><th>Ref</th></tr></thead><tbody><tr><td>Social</td><td>Regression</td><td>Popularity prediction</td><td>59</td><td>200</td><td>–</td><td>[<xref ref-type="bibr" rid="CR72">72</xref>]</td></tr><tr><td>DRUG</td><td>Regression</td><td>Drug design</td><td>221</td><td>56</td><td>–</td><td>[<xref ref-type="bibr" rid="CR73">73</xref>]</td></tr><tr><td>AIR</td><td>Regression</td><td>Responses to gas multi sensors</td><td>15</td><td>9358</td><td>–</td><td>[<xref ref-type="bibr" rid="CR74">74</xref>]</td></tr><tr><td>Energy</td><td>Regression</td><td>Energy use in low energy building</td><td>29</td><td>19,735</td><td>–</td><td>[<xref ref-type="bibr" rid="CR75">75</xref>]</td></tr><tr><td>CARCINOM</td><td>Classification</td><td>Biology</td><td>9182</td><td>174</td><td>11</td><td>[<xref ref-type="bibr" rid="CR76">76</xref>]</td></tr><tr><td>USPS</td><td>Classification</td><td>Hand written image data</td><td>256</td><td>9298</td><td>10</td><td>[<xref ref-type="bibr" rid="CR76">76</xref>]</td></tr><tr><td>BASEHOCK</td><td>Classification</td><td>Text data</td><td>1993</td><td>4862</td><td>2</td><td>[<xref ref-type="bibr" rid="CR76">76</xref>]</td></tr><tr><td>DRIVE</td><td>Classification</td><td>Driving in real scenario</td><td>606</td><td>6400</td><td>3</td><td>[<xref ref-type="bibr" rid="CR77">77</xref>]</td></tr></tbody></table></table-wrap></p>
    <p id="Par56">We ran FeatureSelect on a system with 12 GB of RAM, a COREi7 CPU and a 64-bit Windows 8.1 operating system. FeatureSelect automatically generates tables and diagrams for selected algorithms and methods. In this paper, we selected all algorithms and compared their operation. Each algorithm was run 30 individual times. Since optimisation algorithms operate randomly, it is advisable to evaluate them over at least 30 individual executions [<xref ref-type="bibr" rid="CR51">51</xref>]. All the algorithms were run under the same conditions, for example calling an identical number of score functions. Accuracy and root mean squared error (RMSE) [<xref ref-type="bibr" rid="CR52">52</xref>] were used as the score functions for classification and regression, respectively. The number of generations was set as 50 for all algorithms. We used WCC operators in LCA, since these improve the performance. The datasets (DS) and the name of the algorithm (AL) are shown in the first and second columns of Table <xref rid="Tab3" ref-type="table">3</xref> (classification datasets) and Table <xref rid="Tab4" ref-type="table">4</xref> (regression datasets). These tables, in which the best results of each column have been determined, represent certain statistical measures as ready reference for comparing the algorithms. These measures are as follows:<list list-type="alpha-lower"><list-item><p id="Par57">NOF: Although the NOF was not applied to score functions, it can be restricted to an upper bound as a maximum number of features or genes in FeatureSelect. The maximum number of features was set as 400, 20, 10, 5, 5, 40, 10, and 5 for the CARCINOMA, BASEHOCK, USPS, DRIVE, AIR, DRUG, SOCIAL, and ENERGY datasets, respectively.</p></list-item><list-item><p id="Par58">Elapsed time (ET): After all algorithms were run 30 times, the best results were selected for each. The ET shows how much time in seconds elapsed in the execution for which the best result was obtained for an algorithm. Algorithms have different ETs due to their various stages.</p></list-item><list-item><p id="Par59">AC: This is a measure that states the rate of correctly predicted samples, relative to all the samples. The difference between AC and ACC is that ACC is an average accuracy for all classes, whereas AC is the accuracy of a specific class. The higher the accuracy, the better the answer.</p></list-item><list-item><p id="Par60">Accuracy standard deviation (AC_STD): This indicates how far the results differ from the mean of the results. It is therefore desirable that AC_STD is a minimum.</p></list-item><list-item><p id="Par61">CI: This represents a range of values, and the results are expected to fall into this range with a maximum specific probability. CI_L and CI_H stand for the lower and higher bounds on the confidence interval.</p></list-item><list-item><p id="Par62"><italic>P</italic>-value of accuracy (AC_P): The <italic>p</italic>-value is a statistical measurement that expresses the extent to which the obtained results are similar to random values. An algorithm with a minimum p-value is more reliable than others.</p></list-item><list-item><p id="Par63">Accuracy test statistic (AC_TS): TS is generally used to reject or accept a null hypothesis. When the TS is a maximum, the p-value is a minimum.</p></list-item><list-item><p id="Par64">Root mean squared error (ER or RMSE): ER is calculated using Eq. <xref rid="Equ6" ref-type="">6</xref>, where n, y<sub>i</sub> and y’<sub>i</sub> are the number of samples, and the predicted and label values, respectively. This measurement expresses the average difference between predicted and label values.</p></list-item></list><table-wrap id="Tab3"><label>Table 3</label><caption><p>Results obtained for classification datasets using SVM</p></caption><table frame="hsides" rules="groups"><thead><tr><th>DS</th><th>AL</th><th>NOF</th><th>ET</th><th>AC</th><th>AC_STD</th><th>AC_CI_L</th><th>AC_CI_H</th><th>AC_P</th><th>AC_TS</th><th>ER</th><th>ER_STD</th><th>ER_CI_L</th><th>ER_CI_H</th><th>ER_P</th><th>ER_TS</th></tr></thead><tbody><tr><td rowspan="11">CARCINOM)40%, N)</td><td>WCC</td><td>319</td><td>108</td><td>27.35</td><td>0.28</td><td>27.15</td><td>27.37</td><td>4.33E-69</td><td>918.77</td><td>17.38</td><td>0.001</td><td>17.38</td><td>17.39</td><td>5.75E-94</td><td>18,272.5</td></tr><tr><td>LCA</td><td>270</td><td>117</td><td>27.35</td><td>0.37</td><td>27.26</td><td>27.39</td><td>1.38E-65</td><td>869</td><td>17.38</td><td>0.002</td><td>17.38</td><td>17.39</td><td>1.96E-91</td><td>13,823.5</td></tr><tr><td>GA</td><td>487</td><td>260</td><td>26.41</td><td>1.67</td><td>21.32</td><td>22.57</td><td>3.50E-34</td><td>71.6</td><td>17.42</td><td>0.06</td><td>17.57</td><td>17.62</td><td>6.57E-72</td><td>1435.54</td></tr><tr><td>PSO</td><td>492</td><td>52</td><td>27.35</td><td>2.27</td><td>25.15</td><td>26.85</td><td>1.78E-32</td><td>62.47</td><td>17.38</td><td>0.09</td><td>17.4</td><td>17.47</td><td>6.12E-68</td><td>1047.51</td></tr><tr><td>ACO</td><td>491</td><td>110</td><td>26.41</td><td>3.29</td><td>21.789</td><td>24.24</td><td>2.19E-26</td><td>38.29</td><td>17.42</td><td>0.13</td><td>17.51</td><td>17.6</td><td>2.13E-63</td><td>730.34</td></tr><tr><td>ICA</td><td>488</td><td>79</td><td>27.35</td><td>1.11</td><td>25.21</td><td>26.04</td><td>2.55E-41</td><td>126.43</td><td>17.38</td><td>0.04</td><td>17.43</td><td>17.47</td><td>5.17E-77</td><td>2152.86</td></tr><tr><td>LA</td><td>484</td><td>57</td><td>26.41</td><td>6.71</td><td>15.76</td><td>20.77</td><td>3.96E-15</td><td>14.9</td><td>17.42</td><td>0.26</td><td>17.65</td><td>17.85</td><td>1.47E-54</td><td>361.99</td></tr><tr><td>HTS</td><td>480</td><td>43</td><td>26.41</td><td>3.68</td><td>18.97</td><td>21.72</td><td>1.69E-23</td><td>30.27</td><td>17.42</td><td>0.14</td><td>17.61</td><td>17.72</td><td>4.52E-62</td><td>657.31</td></tr><tr><td>FOA</td><td>333</td><td>93</td><td>28.3</td><td>0.52</td><td>27.76</td><td>28.15</td><td>7.55E-52</td><td>291.89</td><td>17.42</td><td>0.07</td><td>17.36</td><td>17.41</td><td>1.11E-70</td><td>1301.99</td></tr><tr><td>DSOS</td><td>363</td><td>78</td><td>27.35</td><td>0.23</td><td>26.38</td><td>26.56</td><td>4.79E-61</td><td>605.92</td><td>17.38</td><td>0.009</td><td>17.41</td><td>17.42</td><td>2.58E-96</td><td>9967.13</td></tr><tr><td>CUK</td><td>408</td><td>111</td><td>27.35</td><td>0.53</td><td>26.78</td><td>27.17</td><td>3.06E-51</td><td>278.11</td><td>17.38</td><td>0.02</td><td>17.39</td><td>17.4</td><td>2.96E-86</td><td>4484.43</td></tr><tr><td rowspan="11">BASEHOCK(80%,O)</td><td>WCC</td><td>14</td><td>176</td><td>72</td><td>5.33</td><td>51.03</td><td>55.01</td><td>9.17E-31</td><td>54.48</td><td>0.18</td><td>0.05</td><td>0.45</td><td>0.49</td><td>2.93E-29</td><td>48.28</td></tr><tr><td>LCA</td><td>15</td><td>140</td><td>75.25</td><td>6.57</td><td>53.91</td><td>58.82</td><td>6.49E-29</td><td>46.96</td><td>0.25</td><td>0.07</td><td>0.41</td><td>0.46</td><td>9.64E-26</td><td>36.35</td></tr><tr><td>GA</td><td>20</td><td>327</td><td>48.75</td><td>0.87</td><td>46.18</td><td>46.82</td><td>6.60E-52</td><td>293.25</td><td>0.51</td><td>0.01</td><td>0.53</td><td>0.54</td><td>1.13E-53</td><td>337.4</td></tr><tr><td>PSO</td><td>20</td><td>121</td><td>50.25</td><td>1.57</td><td>45.33</td><td>46.5</td><td>2.72E-44</td><td>160.12</td><td>0.5</td><td>0.02</td><td>0.53</td><td>0.55</td><td>2.37E-46</td><td>188.6</td></tr><tr><td>ACO</td><td>20</td><td>140</td><td>47.75</td><td>1.1</td><td>45.01</td><td>45.83</td><td>1.09E-48</td><td>227.11</td><td>0.52</td><td>0.01</td><td>0.54</td><td>0.55</td><td>5.28E-51</td><td>272.95</td></tr><tr><td>ICA</td><td>20</td><td>165</td><td>51</td><td>1.07</td><td>48.34</td><td>49.14</td><td>6.71E-50</td><td>250.04</td><td>0.49</td><td>0.01</td><td>0.51</td><td>0.52</td><td>1.55E-50</td><td>262.95</td></tr><tr><td>LA</td><td>20</td><td>81</td><td>68.25</td><td>3.8</td><td>51.1</td><td>53.94</td><td>7.28E-35</td><td>75.61</td><td>0.32</td><td>0.04</td><td>0.46</td><td>0.49</td><td>1.33E-33</td><td>68.36</td></tr><tr><td>HTS</td><td>20</td><td>65</td><td>47.5</td><td>0.89</td><td>45.32</td><td>45.98</td><td>2.25E-51</td><td>281.07</td><td>0.53</td><td>0.01</td><td>0.54</td><td>0.55</td><td>1.43E-53</td><td>334.63</td></tr><tr><td>FOA</td><td>16</td><td>85</td><td>65.5</td><td>3.9</td><td>47</td><td>49.92</td><td>1.53E-33</td><td>68.02</td><td>0.35</td><td>0.04</td><td>0.5</td><td>0.53</td><td>2.59E-34</td><td>72.35</td></tr><tr><td>DSOS</td><td>15</td><td>118</td><td>46</td><td>0.81</td><td>43.25</td><td>43.86</td><td>6.68E-52</td><td>293.13</td><td>0.54</td><td>0.01</td><td>0.56</td><td>0.57</td><td>3.65E-55</td><td>379.83</td></tr><tr><td>CUK</td><td>18</td><td>138</td><td>66.25</td><td>3.04</td><td>51.37</td><td>53.64</td><td>1.16E-37</td><td>94.51</td><td>0.34</td><td>0.03</td><td>0.46</td><td>0.49</td><td>2.10E-36</td><td>85.48</td></tr><tr><td rowspan="11">USPS(80%, F)</td><td>WCC</td><td>10</td><td>13</td><td>85.15</td><td>0.19</td><td>84.93</td><td>85.39</td><td>4.60E-09</td><td>290.07</td><td>2.07</td><td>0.16</td><td>1.58</td><td>1.85</td><td>0.00001</td><td>28.5</td></tr><tr><td>LCA</td><td>10</td><td>12</td><td>85.15</td><td>0.83</td><td>82.93</td><td>84.99</td><td>5.27E-09</td><td>226.64</td><td>2.15</td><td>0.26</td><td>2.06</td><td>2.7</td><td>0.00003</td><td>20.56</td></tr><tr><td>GA</td><td>10</td><td>10</td><td>85.15</td><td>1.5</td><td>80.71</td><td>84.44</td><td>2.62E-08</td><td>122.97</td><td>2.56</td><td>0.38</td><td>2.1</td><td>3.05</td><td>0.00011</td><td>15.06</td></tr><tr><td>PSO</td><td>10</td><td>6</td><td>87.13</td><td>2.05</td><td>82.01</td><td>87.1</td><td>8.33E-08</td><td>92.09</td><td>2.17</td><td>0.29</td><td>1.88</td><td>2.59</td><td>0.00006</td><td>17.34</td></tr><tr><td>ACO</td><td>10</td><td>17</td><td>85.15</td><td>2.03</td><td>80.85</td><td>85.89</td><td>8.41E-08</td><td>91.87</td><td>2.91</td><td>0.48</td><td>1.57</td><td>2.77</td><td>0.00055</td><td>10.02</td></tr><tr><td>ICA</td><td>10</td><td>7</td><td>86.14</td><td>2.05</td><td>80.02</td><td>85.12</td><td>9.16E-08</td><td>89.93</td><td>2.68</td><td>0.29</td><td>2.58</td><td>3.31</td><td>0.00002</td><td>22.37</td></tr><tr><td>LA</td><td>10</td><td>16</td><td>89.11</td><td>2.89</td><td>83.54</td><td>90.71</td><td>2.88E-07</td><td>67.49</td><td>1.56</td><td>0.57</td><td>1.23</td><td>2.65</td><td>0.00161</td><td>7.59</td></tr><tr><td>HTS</td><td>10</td><td>8</td><td>81.19</td><td>1.63</td><td>77.39</td><td>81.43</td><td>4.22E-08</td><td>109.14</td><td>3.43</td><td>0.62</td><td>3.2</td><td>4.74</td><td>0.00013</td><td>14.33</td></tr><tr><td>FOA</td><td>10</td><td>9</td><td>83.17</td><td>1.29</td><td>80.38</td><td>83.58</td><td>1.47E-08</td><td>142</td><td>1.74</td><td>0.67</td><td>1.65</td><td>3.3</td><td>0.00113</td><td>8.33</td></tr><tr><td>DSOS</td><td>10</td><td>14</td><td>82.18</td><td>2.85</td><td>74.28</td><td>81.36</td><td>4.32E-07</td><td>61.01</td><td>3.41</td><td>0.59</td><td>2.37</td><td>3.85</td><td>0.00003</td><td>11.69</td></tr><tr><td>CUK</td><td>10</td><td>14</td><td>84.16</td><td>1.63</td><td>80.36</td><td>84.4</td><td>3.64E-08</td><td>113.22</td><td>2.1</td><td>0.68</td><td>1.46</td><td>3.16</td><td>0.001637</td><td>7.56</td></tr><tr><td rowspan="11">DRIVE)50%, N)</td><td>WCC</td><td>3</td><td>70</td><td>91.8</td><td>0.18</td><td>91.5</td><td>91.51</td><td>1.81E-76</td><td>2759</td><td>0.08</td><td>0.001</td><td>0.08</td><td>0.09</td><td>1.05E-45</td><td>185.46</td></tr><tr><td>LCA</td><td>3</td><td>69</td><td>91.8</td><td>0.26</td><td>91.34</td><td>91.54</td><td>1.62E-75</td><td>1911.5</td><td>0.08</td><td>0.002</td><td>0.08</td><td>0.09</td><td>1.09E-45</td><td>178.97</td></tr><tr><td>GA</td><td>3</td><td>16</td><td>91.8</td><td>0.33</td><td>90.95</td><td>91.2</td><td>1.67E-72</td><td>1505.2</td><td>0.08</td><td>0.002</td><td>0.09</td><td>0.09</td><td>2.93E-43</td><td>147.51</td></tr><tr><td>PSO</td><td>3</td><td>6</td><td>91.26</td><td>0.88</td><td>88.63</td><td>89.29</td><td>6.05E-60</td><td>555.22</td><td>0.09</td><td>0.01</td><td>0.11</td><td>0.11</td><td>1.06E-33</td><td>68.89</td></tr><tr><td>ACO</td><td>3</td><td>34</td><td>91.26</td><td>0.93</td><td>88.65</td><td>89.34</td><td>2.93E-59</td><td>525.82</td><td>0.09</td><td>0.01</td><td>0.11</td><td>0.11</td><td>5.69E-33</td><td>65</td></tr><tr><td>ICA</td><td>3</td><td>9</td><td>91.8</td><td>0.74</td><td>90.72</td><td>91.28</td><td>2.41E-62</td><td>671.77</td><td>0.08</td><td>0.01</td><td>0.09</td><td>0.09</td><td>3.05E-33</td><td>66.42</td></tr><tr><td>LA</td><td>3</td><td>18</td><td>91.26</td><td>1.26</td><td>89.04</td><td>89.98</td><td>1.92E-55</td><td>388.32</td><td>0.09</td><td>0.01</td><td>0.1</td><td>0.11</td><td>1.58E-28</td><td>45.52</td></tr><tr><td>HTS</td><td>3</td><td>26</td><td>90.71</td><td>0.65</td><td>88.55</td><td>89.04</td><td>1.24E-63</td><td>744.03</td><td>0.09</td><td>0.01</td><td>0.11</td><td>0.11</td><td>1.41E-37</td><td>93.86</td></tr><tr><td>FOA</td><td>2</td><td>41</td><td>91.26</td><td>0.78</td><td>88.54</td><td>89.13</td><td>2.21E-61</td><td>622.33</td><td>0.09</td><td>0.01</td><td>0.11</td><td>0.11</td><td>2.73E-35</td><td>78.22</td></tr><tr><td>DSOS</td><td>3</td><td>52</td><td>91.26</td><td>0.53</td><td>88.45</td><td>88.85</td><td>3.12E-66</td><td>914.72</td><td>0.09</td><td>0.01</td><td>0.11</td><td>0.12</td><td>2.35E-40</td><td>117.09</td></tr><tr><td>CUK</td><td>3</td><td>67</td><td>91.8</td><td>1.3</td><td>89.33</td><td>90.3</td><td>3.66E-55</td><td>379.77</td><td>0.08</td><td>0.01</td><td>0.1</td><td>0.11</td><td>7.78E-28</td><td>43.05</td></tr></tbody></table></table-wrap><table-wrap id="Tab4"><label>Table 4</label><caption><p>Results obtained for regression datasets using SVM</p></caption><table frame="hsides" rules="groups"><thead><tr><th>DS</th><th>AL</th><th>NOF</th><th>ET</th><th>ER</th><th>ER_STD</th><th>ER_CI_1</th><th>ER_CI_2</th><th>ER_P</th><th>ER_TS</th><th>CR</th><th>CR_STD</th><th>CR_CI_1</th><th>CR_CI_2</th><th>CR_P</th><th>CR_TS</th></tr></thead><tbody><tr><td rowspan="11">AIR(80%,O)</td><td>WCC</td><td>5</td><td>105</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>0</td><td>5.3E+ 15</td><td>0.60</td><td>0.00</td><td>0.60</td><td>0.60</td><td>0</td><td>1.0E+ 15</td></tr><tr><td>LCA</td><td>5</td><td>164</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>1.0E-70</td><td>1306</td><td>0.60</td><td>0.00</td><td>0.60</td><td>0.60</td><td>1.25E-76</td><td>2088.68</td></tr><tr><td>GA</td><td>5</td><td>73</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>1.3E-70</td><td>1295.2</td><td>0.60</td><td>0.01</td><td>0.59</td><td>0.60</td><td>1.08E-54</td><td>365.92</td></tr><tr><td>PSO</td><td>5</td><td>39</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>1.9E-55</td><td>387.94</td><td>0.60</td><td>0.02</td><td>0.58</td><td>0.60</td><td>2.18E-42</td><td>137.64</td></tr><tr><td>ACO</td><td>5</td><td>167</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>8.7E-54</td><td>340.36</td><td>0.60</td><td>0.04</td><td>0.57</td><td>0.60</td><td>2.68E-35</td><td>78.28</td></tr><tr><td>ICA</td><td>5</td><td>41</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>6.7E-61</td><td>598.97</td><td>0.60</td><td>0.00</td><td>0.60</td><td>0.60</td><td>2.37E-69</td><td>1171.79</td></tr><tr><td>LA</td><td>5</td><td>64</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>7.5E-60</td><td>551.02</td><td>0.60</td><td>0.04</td><td>0.57</td><td>0.60</td><td>2.27E-34</td><td>72.69</td></tr><tr><td>HTS</td><td>4</td><td>64</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>3.7E-59</td><td>521.16</td><td>0.60</td><td>0.03</td><td>0.60</td><td>0.63</td><td>2.9E-39</td><td>107.35</td></tr><tr><td>FOA</td><td>5</td><td>332</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>4.3E-62</td><td>658.04</td><td>0.60</td><td>0.02</td><td>0.59</td><td>0.60</td><td>4.85E-46</td><td>184.01</td></tr><tr><td>DSOS</td><td>5</td><td>139</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>7.1E-53</td><td>316.65</td><td>0.60</td><td>0.03</td><td>0.55</td><td>0.58</td><td>1.14E-37</td><td>94.57</td></tr><tr><td>CUK</td><td>5</td><td>173</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>2.1E-68</td><td>1086</td><td>0.60</td><td>0.00</td><td>0.60</td><td>0.60</td><td>2.6E-74</td><td>1737.29</td></tr><tr><td rowspan="11">DRUG(80%,N)</td><td>WCC</td><td>32</td><td>140</td><td>0.01</td><td>0.00</td><td>0.01</td><td>0.01</td><td>2.7E-26</td><td>38.01</td><td>0.97</td><td>0.01</td><td>0.96</td><td>0.96</td><td>1.61E-65</td><td>864.45</td></tr><tr><td>LCA</td><td>23</td><td>115</td><td>0.00</td><td>0.00</td><td>0.01</td><td>0.01</td><td>3.3E-25</td><td>34.80</td><td>0.97</td><td>0.00</td><td>0.96</td><td>0.97</td><td>4.33E-72</td><td>1456.43</td></tr><tr><td>GA</td><td>38</td><td>48</td><td>0.01</td><td>0.00</td><td>0.02</td><td>0.02</td><td>1.0E-31</td><td>58.83</td><td>0.95</td><td>0.01</td><td>0.94</td><td>0.95</td><td>1.67E-56</td><td>422.49</td></tr><tr><td>PSO</td><td>36</td><td>47</td><td>0.01</td><td>0.00</td><td>0.01</td><td>0.01</td><td>9.3E-24</td><td>30.92</td><td>0.96</td><td>0.01</td><td>0.96</td><td>0.96</td><td>3.15E-63</td><td>720.56</td></tr><tr><td>ACO</td><td>36</td><td>141</td><td>0.01</td><td>0.00</td><td>0.02</td><td>0.02</td><td>9.4E-24</td><td>30.91</td><td>0.97</td><td>0.01</td><td>0.95</td><td>0.96</td><td>1.16E-55</td><td>395.13</td></tr><tr><td>ICA</td><td>35</td><td>38</td><td>0.01</td><td>0.00</td><td>0.02</td><td>0.02</td><td>6.7E-30</td><td>50.81</td><td>0.96</td><td>0.01</td><td>0.95</td><td>0.96</td><td>5.35E-61</td><td>603.64</td></tr><tr><td>LA</td><td>30</td><td>95</td><td>0.00</td><td>0.00</td><td>0.00</td><td>0.00</td><td>4.1E-24</td><td>31.84</td><td>0.98</td><td>0.00</td><td>0.97</td><td>0.97</td><td>3.35E-71</td><td>1357.20</td></tr><tr><td>HTS</td><td>32</td><td>98</td><td>0.01</td><td>0.00</td><td>0.02</td><td>0.03</td><td>3.8E-25</td><td>34.63</td><td>0.95</td><td>0.01</td><td>0.94</td><td>0.95</td><td>4.88E-57</td><td>440.77</td></tr><tr><td>FOA</td><td>20</td><td>99</td><td>0.00</td><td>0.00</td><td>0.01</td><td>0.01</td><td>1.9E-18</td><td>19.88</td><td>0.97</td><td>0.01</td><td>0.96</td><td>0.96</td><td>6.19E-66</td><td>893.35</td></tr><tr><td>DSOS</td><td>18</td><td>119</td><td>0.01</td><td>0.00</td><td>0.02</td><td>0.02</td><td>7.1E-29</td><td>46.80</td><td>0.96</td><td>0.01</td><td>0.95</td><td>0.96</td><td>3.24E-63</td><td>719.88</td></tr><tr><td>CUK</td><td>24</td><td>152</td><td>0.01</td><td>0.00</td><td>0.01</td><td>0.01</td><td>1.8E-30</td><td>53.15</td><td>0.97</td><td>0.01</td><td>0.96</td><td>0.97</td><td>4.68E-65</td><td>833.19</td></tr><tr><td rowspan="11">SOCIAL (80%,F)</td><td>WCC</td><td>8</td><td>121</td><td>0.02</td><td>0.00</td><td>0.01</td><td>0.02</td><td>3.44E-08</td><td>229.53</td><td>0.51</td><td>0.07</td><td>0.30</td><td>0.64</td><td>0.006725</td><td>12.13</td></tr><tr><td>LCA</td><td>8</td><td>135</td><td>0.02</td><td>0.00</td><td>0.01</td><td>0.02</td><td>4.66E-05</td><td>146.54</td><td>0.54</td><td>0.02</td><td>0.48</td><td>0.56</td><td>0.00033</td><td>55.01</td></tr><tr><td>GA</td><td>10</td><td>68</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>0.000558</td><td>42.33</td><td>0.36</td><td>0.04</td><td>0.23</td><td>0.44</td><td>0.005372</td><td>13.59</td></tr><tr><td>PSO</td><td>10</td><td>91</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>8.69E-05</td><td>107.26</td><td>0.39</td><td>0.05</td><td>0.24</td><td>0.47</td><td>0.00549</td><td>13.44</td></tr><tr><td>ACO</td><td>10</td><td>153</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>0.000394</td><td>50.35</td><td>0.31</td><td>0.05</td><td>0.17</td><td>0.42</td><td>0.010204</td><td>9.82</td></tr><tr><td>ICA</td><td>9</td><td>76</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>0.00017</td><td>76.61</td><td>0.37</td><td>0.01</td><td>0.36</td><td>0.39</td><td>6.79E-05</td><td>121.39</td></tr><tr><td>LA</td><td>10</td><td>93</td><td>0.02</td><td>0.00</td><td>0.01</td><td>0.02</td><td>0.000485</td><td>45.39</td><td>0.53</td><td>0.02</td><td>0.45</td><td>0.57</td><td>0.000754</td><td>36.40</td></tr><tr><td>HTS</td><td>8</td><td>93</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>6.75E-05</td><td>121.73</td><td>0.36</td><td>0.03</td><td>0.23</td><td>0.41</td><td>0.003921</td><td>15.92</td></tr><tr><td>FOA</td><td>8</td><td>86</td><td>0.02</td><td>0.00</td><td>0.01</td><td>0.03</td><td>0.010557</td><td>9.66</td><td>0.45</td><td>0.16</td><td>0.10</td><td>0.70</td><td>0.083971</td><td>3.23</td></tr><tr><td>DSOS</td><td>8</td><td>122</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.03</td><td>0.001028</td><td>31.17</td><td>0.25</td><td>0.04</td><td>0.11</td><td>0.31</td><td>0.012132</td><td>9.00</td></tr><tr><td>CUK</td><td>8</td><td>93</td><td>0.02</td><td>0.00</td><td>0.02</td><td>0.02</td><td>0.000439</td><td>47.70</td><td>0.35</td><td>0.03</td><td>0.26</td><td>0.39</td><td>0.002276</td><td>20.93</td></tr><tr><td rowspan="11">ENERGY(60%,O)</td><td>WCC</td><td>5</td><td>64</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>6.03E-80</td><td>2717.4</td><td>0.5</td><td>0</td><td>0.4</td><td>0.4</td><td>1.19E-35</td><td>80.49</td></tr><tr><td>LCA</td><td>5</td><td>82</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>1.60E-83</td><td>3609.2</td><td>0.5</td><td>0</td><td>0.4</td><td>0.4</td><td>6.82E-33</td><td>64.59</td></tr><tr><td>GA</td><td>5</td><td>23</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>2.70E-75</td><td>1878.2</td><td>0.4</td><td>0</td><td>0.3</td><td>0.4</td><td>3.46E-29</td><td>48</td></tr><tr><td>PSO</td><td>5</td><td>25</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>7.82E-70</td><td>1217.4</td><td>0.3</td><td>0.1</td><td>0.3</td><td>0.3</td><td>3.16E-23</td><td>29.61</td></tr><tr><td>ACO</td><td>5</td><td>52</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>1.34E-63</td><td>742.04</td><td>0.4</td><td>0.1</td><td>0.2</td><td>0.3</td><td>1.54E-17</td><td>18.4</td></tr><tr><td>ICA</td><td>5</td><td>57</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>4.89E-79</td><td>2528.3</td><td>0.5</td><td>0</td><td>0.4</td><td>0.4</td><td>1.55E-31</td><td>57.95</td></tr><tr><td>LA</td><td>5</td><td>24</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>1.57E-73</td><td>1632.7</td><td>0.5</td><td>0</td><td>0.4</td><td>0.4</td><td>1.07E-29</td><td>49.99</td></tr><tr><td>HTS</td><td>4</td><td>27</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>1.08E-66</td><td>948.73</td><td>0.4</td><td>0.1</td><td>0.3</td><td>0.3</td><td>1.78E-18</td><td>19.94</td></tr><tr><td>FOA</td><td>5</td><td>30</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>2.20E-66</td><td>925.79</td><td>0.5</td><td>0.1</td><td>0.3</td><td>0.3</td><td>1.97E-20</td><td>23.51</td></tr><tr><td>DSOS</td><td>5</td><td>42</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>3.70E-66</td><td>909.35</td><td>0.4</td><td>0.1</td><td>0.3</td><td>0.3</td><td>6.59E-24</td><td>31.31</td></tr><tr><td>CUK</td><td>5</td><td>80</td><td>0.08</td><td>0.00</td><td>0.08</td><td>0.08</td><td>2.33E-80</td><td>2807.9</td><td>0.5</td><td>0</td><td>0.4</td><td>0.4</td><td>6.99E-32</td><td>59.58</td></tr></tbody></table></table-wrap></p>
    <p id="Par65">
      <disp-formula id="Equ6">
        <label>6</label>
        <alternatives>
          <tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{ER}=\sqrt{\frac{\left( yi-{y}^{\hbox{'}}i\right)}{n}} $$\end{document}</tex-math>
          <mml:math id="M12" display="block">
            <mml:mi>ER</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:msqrt>
              <mml:mfrac>
                <mml:mfenced close=")" open="(">
                  <mml:mrow>
                    <mml:mi mathvariant="italic">yi</mml:mi>
                    <mml:mo>−</mml:mo>
                    <mml:msup>
                      <mml:mi>y</mml:mi>
                      <mml:mo>'</mml:mo>
                    </mml:msup>
                    <mml:mi>i</mml:mi>
                  </mml:mrow>
                </mml:mfenced>
                <mml:mi>n</mml:mi>
              </mml:mfrac>
            </mml:msqrt>
          </mml:math>
          <graphic xlink:href="12859_2019_2754_Article_Equ6.gif" position="anchor"/>
        </alternatives>
      </disp-formula>
      <list list-type="simple">
        <list-item>
          <label>i)</label>
          <p id="Par66">Error standard deviation (ER_STD): In the same way as AC_STD, ER_STD indicates how far the RMSE differs from the average RMSE when 30 individual executions are performed. The lower the ER_STD, the closer the obtained results.</p>
        </list-item>
        <list-item>
          <label>j)</label>
          <p id="Par67">Squared correlation coefficient (CR): The correlation (R) determines the connectivity between the predicted values and label values. CR is calculated based on R<sup>2</sup>. We expect the CR to increase when the error decreases.</p>
        </list-item>
      </list>
    </p>
    <p id="Par68">The concepts between (ER_CI_L and CR_CI_L and AC_CI_L), between (ER_CI_H and CR_CI_H and AC_CI_H), between (ER_STD and CR_STD and AC_STD), between (AC_P and ER_P and CR_P), and finally between (AC_TS and ER_TS and CR_TS) are alike. In addition to the name of the dataset, the training data percentage and an input data type are specified. Three input data types were used: fuzzified (F), normalised, (N) and ordinary (O).</p>
    <p id="Par69">FeatureSelect generates diagrams for the ACC, average of the ACC and the stability of the ACC for classification datasets. In addition, it generates diagrams of the ER, average ER and stability of the ER for both classification and regression datasets.</p>
    <p id="Par70">The criteria used to evaluate the optimisation algorithms were convergence, average convergence and stability. These measures indicate whether or not the algorithms have been correctly implemented. Figures <xref rid="Fig5" ref-type="fig">5</xref> and <xref rid="Fig6" ref-type="fig">6</xref> illustrate instances of FeatureSelect outputs based on the mentioned criteria. The convergence mean is that the answers must be improved when the number of iterations or time dedicated to the algorithms is increased. For example, we observe that the ER decreases and the CR and ACC increase with a higher number of iterations. From convergence point of view, all of the algorithms increase the accuracy and correlation, and reduce the error. Although all of them have generated acceptable results, LA, LCA, WCC and GA are suitable than others. In addition to convergence, there is the concept of average convergence. The difference between the two is that the convergence is obtained by extracting the best answer at the end of each iteration, whereas average convergence is calculated based on the mean of potential solution scores at the end of each iteration. As it is observable, all of the potential answers generated by algorithms except GA and ICA are improving when the iteration is increased. In order to improve the performance of GA, we replace some of the worst results with randomly created answers at the end of each iteration. Also, absorb operator of ICA makes some countries worse or better than their previous status. Hence, the average convergence of GA and ICA may not have ascending or descending form. Stability diagrams indicate how the results fluctuate from a forward line in the individual executions. An algorithm can be said to be better than others if its results lie on the forward line and if the mean of its results is better than those of other algorithms. The results shown in Tables <xref rid="Tab3" ref-type="table">3</xref> and <xref rid="Tab4" ref-type="table">4</xref> have been calculated based on the stability results. FeatureSelect also generates several addition outputs for classification datasets, as follows:<list list-type="alpha-lower"><list-item><p id="Par71">Essential statistical measurements: These measures are shown in Eqs. <xref rid="Equ2" ref-type="">2</xref> to <xref rid="Equ5" ref-type="">5</xref>. Table <xref rid="Tab5" ref-type="table">5</xref> presents these statistical measures for all datasets.</p></list-item><list-item><p id="Par72">Receiver operating characteristic (ROC) curve: This is usually used for binary classification, but has been extended here to multi-class classification. The ROC is a graphical plot that indicates the diagnostic ability of a classifier. The horizontal axis is FPR (1-specificity) and the vertical axis is TPR (true positive rate or sensitivity) [<xref ref-type="bibr" rid="CR53">53</xref>]. The ROC curve and ROC space for the algorithms for the USPS dataset are shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref> as an example of FeatureSelect’s output for classification datasets.</p></list-item></list><fig id="Fig5"><label>Fig. 5</label><caption><p>Diagrams generated for the DRIVE dataset using SVM. These diagrams compare the algorithms performances against each other based on accuracy and error scores. For every score, convergence, average convergence, and stability diagrams have been shown. Given the results on the DRIVE dataset, the performances of WCC, GA, LCA, and LA are better than the others</p></caption><graphic xlink:href="12859_2019_2754_Fig5_HTML" id="MO5"/></fig><fig id="Fig6"><label>Fig. 6</label><caption><p>Diagrams generated for the ENERGY dataset using SVR. These diagrams compare the algorithms performances against each other based on RMSE and correlation scores. For every score, convergence, average convergence, and stability diagrams have been shown. Given the results on the ENERGY dataset, the performances of CUK, HTS, LCA, and LA are proper than the others</p></caption><graphic xlink:href="12859_2019_2754_Fig6_HTML" id="MO6"/></fig><table-wrap id="Tab5"><label>Table 5</label><caption><p>Essential statistical measurements for all classification datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th>DS</th><th>AL_NAME</th><th>SEN</th><th>PRE</th><th>FPR</th><th>ACC</th><th>DS</th><th>AL_NAME</th><th>SEN</th><th>PRE</th><th>FPR</th><th>ACC</th></tr></thead><tbody><tr><td rowspan="11">CARCINOM(80%,N)</td><td>WCC</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.76</td><td rowspan="11">USPS(80%,O)</td><td>WCC</td><td>0.82</td><td>0.86</td><td>0.02</td><td>0.85</td></tr><tr><td>LCA</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.76</td><td>LCA</td><td>0.82</td><td>0.83</td><td>0.02</td><td>0.85</td></tr><tr><td>GA</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.75</td><td>GA</td><td>0.83</td><td>0.86</td><td>0.02</td><td>0.85</td></tr><tr><td>PSO</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.76</td><td>PSO</td><td>0.87</td><td>0.88</td><td>0.02</td><td>0.87</td></tr><tr><td>ACO</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.75</td><td>ACO</td><td>0.85</td><td>0.85</td><td>0.02</td><td>0.85</td></tr><tr><td>ICA</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.76</td><td>ICA</td><td>0.81</td><td>0.89</td><td>0.02</td><td>0.86</td></tr><tr><td>LA</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.75</td><td>LA</td><td>0.89</td><td>0.89</td><td>0.01</td><td>0.89</td></tr><tr><td>HTS</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.58</td><td>HTS</td><td>0.79</td><td>0.82</td><td>0.03</td><td>0.81</td></tr><tr><td>FOA</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.77</td><td>FOA</td><td>0.81</td><td>0.84</td><td>0.02</td><td>0.83</td></tr><tr><td>DSOS</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.76</td><td>DSOS</td><td>0.80</td><td>0.80</td><td>0.02</td><td>0.82</td></tr><tr><td>CUK</td><td>0.68</td><td>0.60</td><td>0.02</td><td>0.76</td><td>CUK</td><td>0.82</td><td>0.84</td><td>0.02</td><td>0.84</td></tr><tr><td rowspan="11">BASEHOCK(80%,F)</td><td>WCC</td><td>0.66</td><td>0.89</td><td>0.33</td><td>0.72</td><td rowspan="11">DRIVE(80%,N)</td><td>WCC</td><td>0.56</td><td>0.81</td><td>0.24</td><td>0.92</td></tr><tr><td>LCA</td><td>0.70</td><td>0.83</td><td>0.30</td><td>0.75</td><td>LCA</td><td>0.56</td><td>0.81</td><td>0.24</td><td>0.92</td></tr><tr><td>GA</td><td>0.57</td><td>0.72</td><td>0.43</td><td>0.49</td><td>GA</td><td>0.56</td><td>0.81</td><td>0.24</td><td>0.92</td></tr><tr><td>PSO</td><td>0.58</td><td>0.71</td><td>0.42</td><td>0.50</td><td>PSO</td><td>0.52</td><td>0.80</td><td>0.25</td><td>0.91</td></tr><tr><td>ACO</td><td>0.56</td><td>0.72</td><td>0.44</td><td>0.48</td><td>ACO</td><td>0.52</td><td>0.80</td><td>0.25</td><td>0.91</td></tr><tr><td>ICA</td><td>0.58</td><td>0.72</td><td>0.42</td><td>0.51</td><td>ICA</td><td>0.56</td><td>0.81</td><td>0.24</td><td>0.92</td></tr><tr><td>LA</td><td>0.68</td><td>0.67</td><td>0.32</td><td>0.68</td><td>LA</td><td>0.52</td><td>0.80</td><td>0.25</td><td>0.91</td></tr><tr><td>HTS</td><td>0.53</td><td>0.71</td><td>0.47</td><td>0.44</td><td>HTS</td><td>0.33</td><td>0.63</td><td>0.33</td><td>0.89</td></tr><tr><td>FOA</td><td>0.58</td><td>0.75</td><td>0.42</td><td>0.66</td><td>FOA</td><td>0.52</td><td>0.80</td><td>0.25</td><td>0.91</td></tr><tr><td>DSOS</td><td>0.54</td><td>0.72</td><td>0.46</td><td>0.46</td><td>DSOS</td><td>0.52</td><td>0.80</td><td>0.25</td><td>0.91</td></tr><tr><td>CUK</td><td>0.66</td><td>0.66</td><td>0.34</td><td>0.66</td><td>CUK</td><td>0.56</td><td>0.81</td><td>0.24</td><td>0.92</td></tr></tbody></table></table-wrap><fig id="Fig7"><label>Fig. 7</label><caption><p>ROC curve and ROC space for the algorithms used based on SVM</p></caption><graphic xlink:href="12859_2019_2754_Fig7_HTML" id="MO7"/></fig></p>
    <p id="Par73">Like the ROC curve, the ROC space represents the trade-offs between TPR and FPR. A point that is closer to the left and the top represents an algorithm with better diagnostic ability; for example, LCA has the best diagnostic ability for the USPS dataset.</p>
    <p id="Par74">In overall evaluation, we compare the performance of the FeatureSelect algorithms. The values in Tables <xref rid="Tab6" ref-type="table">6</xref>, <xref rid="Tab7" ref-type="table">7</xref> and <xref rid="Tab8" ref-type="table">8</xref> are a summary of those in Tables <xref rid="Tab3" ref-type="table">3</xref>, <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref> respectively (the average for table), and allow an overall comparison of the algorithms used in FeatureSelect. LCA has selected 74.5 features in the average state on four classification datasets. Although the time orders are the same for all algorithms, the average elapsed time for four classification datasets is 35.5 for HTS. LCA and WCC show similar operation, but the accuracy of LCA is better than that of WCC. Its accuracy confidence interval is also more acceptable than that of the others. We show the AC_P and ER_P using three floating digits.<table-wrap id="Tab6"><label>Table 6</label><caption><p>Summary of results for all classification datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th>AL</th><th>NOF</th><th>ET</th><th>AC</th><th>AC_STD</th><th>AC_CI_L</th><th>AC_CI_H</th><th>AC_P</th><th>AC_TS</th><th>ER</th><th>ER_STD</th><th>ER_CI_L</th><th>ER_CI_H</th><th>ER_P</th><th>ER_TS</th></tr></thead><tbody><tr><td>WCC</td><td>86.50</td><td>91.75</td><td>69.08</td><td>1.50</td><td>63.65</td><td>64.82</td><td>0.000</td><td>1005.58</td><td>4.93</td><td>0.05</td><td>4.94</td><td>4.96</td><td>0.000</td><td>4633.69</td></tr><tr><td>LCA</td><td>74.50</td><td>84.50</td><td>69.89</td><td>2.01</td><td>63.86</td><td>65.69</td><td>0.000</td><td>763.53</td><td>4.97</td><td>0.08</td><td>4.98</td><td>5.16</td><td>0.000</td><td>3514.85</td></tr><tr><td>GA</td><td>130.00</td><td>153.25</td><td>63.03</td><td>1.09</td><td>59.79</td><td>61.26</td><td>0.000</td><td>498.26</td><td>5.14</td><td>0.11</td><td>5.07</td><td>5.33</td><td>0.000</td><td>483.88</td></tr><tr><td>PSO</td><td>131.25</td><td>46.25</td><td>64.00</td><td>1.69</td><td>60.28</td><td>62.44</td><td>0.000</td><td>217.48</td><td>5.04</td><td>0.10</td><td>4.98</td><td>5.18</td><td>0.000</td><td>330.59</td></tr><tr><td>ACO</td><td>131.00</td><td>75.25</td><td>62.64</td><td>1.84</td><td>59.07</td><td>61.33</td><td>0.000</td><td>220.77</td><td>5.24</td><td>0.16</td><td>4.93</td><td>5.26</td><td>0.000</td><td>269.58</td></tr><tr><td>ICA</td><td>130.25</td><td>65.00</td><td>64.07</td><td>1.24</td><td>61.07</td><td>62.90</td><td>0.000</td><td>284.54</td><td>5.16</td><td>0.09</td><td>5.15</td><td>5.35</td><td>0.000</td><td>626.15</td></tr><tr><td>LA</td><td>129.25</td><td>43.00</td><td>68.76</td><td>3.67</td><td>59.86</td><td>63.85</td><td>0.000</td><td>136.58</td><td>4.85</td><td>0.22</td><td>4.86</td><td>5.28</td><td>0.000</td><td>120.87</td></tr><tr><td>HTS</td><td>128.25</td><td>35.50</td><td>61.45</td><td>1.71</td><td>57.56</td><td>59.54</td><td>0.000</td><td>291.13</td><td>5.37</td><td>0.20</td><td>5.37</td><td>5.78</td><td>0.000</td><td>275.03</td></tr><tr><td>FOA</td><td>90.25</td><td>57.00</td><td>67.06</td><td>1.62</td><td>60.92</td><td>62.70</td><td>0.000</td><td>281.06</td><td>4.90</td><td>0.20</td><td>4.91</td><td>5.34</td><td>0.000</td><td>365.22</td></tr><tr><td>DSOS</td><td>97.75</td><td>65.50</td><td>61.70</td><td>1.11</td><td>58.09</td><td>60.16</td><td>0.000</td><td>468.70</td><td>5.36</td><td>0.15</td><td>5.11</td><td>5.49</td><td>0.000</td><td>2618.94</td></tr><tr><td>CUK</td><td>109.75</td><td>82.50</td><td>67.39</td><td>1.63</td><td>61.96</td><td>63.88</td><td>0.000</td><td>216.40</td><td>4.98</td><td>0.19</td><td>4.85</td><td>5.29</td><td>0.000</td><td>1155.13</td></tr></tbody></table></table-wrap><table-wrap id="Tab7"><label>Table 7</label><caption><p>Summary of results for all regression datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th>AL</th><th>NOF</th><th>ET</th><th>ER</th><th>ER_STD</th><th>ER_CI_1</th><th>ER_CI_2</th><th>ER_P</th><th>ER_TS</th><th>CR</th><th>CR_STD</th><th>CR_CI_1</th><th>CR_CI_2</th><th>CR_P</th><th>CR_TS</th></tr></thead><tbody><tr><td>WCC</td><td>12.5</td><td>107.5</td><td>0.033</td><td>0.000</td><td>0.030</td><td>0.033</td><td>0.000</td><td>1.3E+ 15</td><td>0.65</td><td>0.020</td><td>0.615</td><td>0.640</td><td>0.000</td><td>2.5E+ 14</td></tr><tr><td>LCA</td><td>10.25</td><td>124</td><td>0.030</td><td>0.000</td><td>0.030</td><td>0.033</td><td>0.000</td><td>1274.13</td><td>0.65</td><td>0.005</td><td>0.610</td><td>0.633</td><td>0.000</td><td>916.1775</td></tr><tr><td>GA</td><td>14.5</td><td>53</td><td>0.033</td><td>0.000</td><td>0.035</td><td>0.035</td><td>0.000</td><td>818.640</td><td>0.57</td><td>0.015</td><td>0.515</td><td>0.598</td><td>0.001</td><td>212.5</td></tr><tr><td>PSO</td><td>14.00</td><td>50.5</td><td>0.033</td><td>0.000</td><td>0.033</td><td>0.033</td><td>0.000</td><td>435.880</td><td>0.56</td><td>0.045</td><td>0.520</td><td>0.583</td><td>0.001</td><td>225.3125</td></tr><tr><td>ACO</td><td>14.00</td><td>128.25</td><td>0.033</td><td>0.000</td><td>0.035</td><td>0.035</td><td>0.000</td><td>290.915</td><td>0.57</td><td>0.050</td><td>0.473</td><td>0.570</td><td>0.003</td><td>125.4075</td></tr><tr><td>ICA</td><td>13.50</td><td>53</td><td>0.033</td><td>0.000</td><td>0.035</td><td>0.035</td><td>0.000</td><td>813.673</td><td>0.60</td><td>0.005</td><td>0.578</td><td>0.588</td><td>0.000</td><td>488.6925</td></tr><tr><td>LA</td><td>12.50</td><td>69</td><td>0.030</td><td>0.000</td><td>0.028</td><td>0.030</td><td>0.000</td><td>565.238</td><td>0.65</td><td>0.015</td><td>0.598</td><td>0.635</td><td>0.000</td><td>379.07</td></tr><tr><td>HTS</td><td>12.00</td><td>70.5</td><td>0.033</td><td>0.000</td><td>0.035</td><td>0.038</td><td>0.000</td><td>406.563</td><td>0.57</td><td>0.043</td><td>0.518</td><td>0.573</td><td>0.001</td><td>145.995</td></tr><tr><td>FOA</td><td>9.50</td><td>136.75</td><td>0.030</td><td>0.000</td><td>0.030</td><td>0.035</td><td>0.003</td><td>403.343</td><td>0.63</td><td>0.073</td><td>0.488</td><td>0.640</td><td>0.021</td><td>276.025</td></tr><tr><td>DSOS</td><td>9.00</td><td>105.5</td><td>0.033</td><td>0.000</td><td>0.035</td><td>0.038</td><td>0.000</td><td>325.99</td><td>0.55</td><td>0.045</td><td>0.478</td><td>0.538</td><td>0.003</td><td>213.69</td></tr><tr><td>CUK</td><td>10.50</td><td>124.5</td><td>0.033</td><td>0.000</td><td>0.033</td><td>0.033</td><td>0.000</td><td>998.68</td><td>0.60</td><td>0.010</td><td>0.555</td><td>0.590</td><td>0.001</td><td>662.7475</td></tr></tbody></table></table-wrap><table-wrap id="Tab8"><label>Table 8</label><caption><p>Summary of essential statistical criteria for all classification datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th>AL_NAME</th><th>SEN</th><th>PRE</th><th>FPR</th><th>ACC</th></tr></thead><tbody><tr><td>WCC</td><td>0.6800</td><td>0.7900</td><td>0.1525</td><td>0.8125</td></tr><tr><td>LCA</td><td>0.6900</td><td>0.7675</td><td>0.1450</td><td>0.8200</td></tr><tr><td>GA</td><td>0.6600</td><td>0.7475</td><td>0.1775</td><td>0.7525</td></tr><tr><td>PSO</td><td>0.6625</td><td>0.7475</td><td>0.1775</td><td>0.7600</td></tr><tr><td>ACO</td><td>0.6525</td><td>0.7425</td><td>0.1825</td><td>0.7475</td></tr><tr><td>ICA</td><td>0.6575</td><td>0.7550</td><td>0.1750</td><td>0.7625</td></tr><tr><td>LA</td><td>0.6925</td><td>0.7400</td><td>0.1500</td><td>0.8075</td></tr><tr><td>HTS</td><td>0.5825</td><td>0.6900</td><td>0.2125</td><td>0.6800</td></tr><tr><td>FOA</td><td>0.6475</td><td>0.7475</td><td>0.1775</td><td>0.7925</td></tr><tr><td>DSOS</td><td>0.6350</td><td>0.7300</td><td>0.1875</td><td>0.7375</td></tr><tr><td>CUK</td><td>0.6800</td><td>0.7275</td><td>0.1550</td><td>0.7950</td></tr></tbody></table></table-wrap></p>
    <p id="Par75">These values are identical for all algorithms, indicating that the performance of the algorithms is not random. For all classification datasets, FOA reaches a minimum value of ER. Therefore, it is proper than other algorithms in ER point of view. We also observe that WCC operates better than the other algorithms in terms of ER_TS, CR, CR_CI, CR_P and CR_TS.</p>
    <p id="Par76">The DSOS algorithm selects nine features in the average state for all regression datasets. The elapsed time for PSO in which the best answer has been obtained was lowest for this algorithm. LCA, LA and FOA are algorithms which their functional are the same and proper than other algorithms. It is also obvious that LA has the best confidence interval of all alternative approaches. Except for FOA, which has an ER_P value of 0.003, ER_P is identical for all algorithms to three decimal places. In the same way as CR_CI, CR_P and CR_TS for all regression datasets, the highest ER_TS value was achieved by WCC. WCC, LCA and LA achieved the maximum value of correlation (CR) for all regression datasets.</p>
    <p id="Par77">SEN, PRE, FPR, and ACC are the most important comparison criteria for classification problems. A summary of Table <xref rid="Tab5" ref-type="table">5</xref> is shown in Table <xref rid="Tab8" ref-type="table">8</xref>, which indicates that LCA obtains the best results in terms of FPR and ACC, and LA achieves the best result for SEN. WCC also acquires the best result for PRE on average.</p>
    <p id="Par78">In a comprehensive comparison, we evaluate the performance of all algorithms and methods on BSEHOCK dataset that is larger than others. Unlike previous experiments which are based on single objective (ACC) score; this one is based on multi objective score for wrapper methods. In Table <xref rid="Tab9" ref-type="table">9</xref> in which the best values of each column have been determined; the results are observable for SVM, ANN and DT learner. PCRR, LAP, ENT and MI are abbreviation for pearson correlation, laplacian, entropy and mutual information respectively in Table <xref rid="Tab9" ref-type="table">9</xref>. As it is observed, every classifier and every feature selection method have their own attitude toward the data. Therefore, a user can apply various methods and algorithms along with different learners, and then can select the features which satisfy his/hers requirements. Also, it is possible that a user employee ensemble.<table-wrap id="Tab9"><label>Table 9</label><caption><p>A comprehensive comparison of all methods</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">AL</th><th colspan="5">Learner = SVM</th><th colspan="5">Learner = ANN</th><th colspan="5">Learner = Decision tree</th></tr><tr><th>SEN</th><th>SPC</th><th>PRE</th><th>FPR</th><th>ACC</th><th>SEN</th><th>SPC</th><th>PRE</th><th>FPR</th><th>ACC</th><th>SEN</th><th>SPC</th><th>PRE</th><th>FPR</th><th>ACC</th></tr></thead><tbody><tr><td>WCC</td><td>0/92</td><td>0/25</td><td>0/43</td><td>0/75</td><td>0/51</td><td>0/94</td><td>0/21</td><td>0/63</td><td>0/79</td><td>0/63</td><td>0/45</td><td><bold>0/69</bold></td><td>0/34</td><td><bold>0/31</bold></td><td><bold>0/52</bold></td></tr><tr><td>LCA</td><td>0/92</td><td>0/25</td><td>0/43</td><td>0/75</td><td>0/51</td><td>0/85</td><td>0/24</td><td>0/70</td><td>0/76</td><td>0/70</td><td>0/46</td><td>0/67</td><td><bold>0/36</bold></td><td>0/33</td><td>0/50</td></tr><tr><td>GA</td><td>0/92</td><td>0/25</td><td>0/43</td><td>0/75</td><td>0/51</td><td>0/96</td><td>0/02</td><td>0/63</td><td>0/98</td><td>0/63</td><td>0/44</td><td>0/61</td><td>0/33</td><td>0/39</td><td>0/45</td></tr><tr><td>PSO</td><td>0/92</td><td>0/25</td><td>0/43</td><td>0/75</td><td>0/51</td><td><bold>1/00</bold></td><td>0/00</td><td>0/65</td><td>1/00</td><td>0/65</td><td>0/44</td><td>0/63</td><td>0/31</td><td>0/37</td><td>0/47</td></tr><tr><td>ACO</td><td>0/92</td><td>0/25</td><td>0/43</td><td>0/75</td><td>0/51</td><td>0/97</td><td>0/14</td><td>0/72</td><td>0/86</td><td>0/72</td><td>0/43</td><td>0/60</td><td>0/31</td><td>0/40</td><td>0/43</td></tr><tr><td>ICA</td><td>0/92</td><td>0/25</td><td>0/43</td><td>0/75</td><td>0/51</td><td><bold>1/00</bold></td><td>0/00</td><td>0/70</td><td>1/00</td><td>0/70</td><td>0/44</td><td>0/62</td><td>0/33</td><td>0/38</td><td>0/45</td></tr><tr><td>LA</td><td>0/92</td><td>0/25</td><td>0/43</td><td>0/75</td><td>0/51</td><td><bold>1/00</bold></td><td>0/00</td><td><bold>0/73</bold></td><td>1/00</td><td><bold>0/73</bold></td><td>0/45</td><td>0/63</td><td><bold>0/36</bold></td><td>0/37</td><td>0/42</td></tr><tr><td>HTS</td><td>0/93</td><td>0/21</td><td>0/42</td><td>0/79</td><td>0/49</td><td>0/90</td><td>0/33</td><td>0/55</td><td>0/67</td><td>0/55</td><td>0/43</td><td>0/57</td><td>0/31</td><td>0/43</td><td>0/41</td></tr><tr><td>FOA</td><td>0/90</td><td><bold>0/32</bold></td><td><bold>0/46</bold></td><td><bold>0/68</bold></td><td><bold>0/54</bold></td><td>0/94</td><td>0/22</td><td>0/67</td><td>0/78</td><td>0/67</td><td>0/44</td><td>0/63</td><td>0/34</td><td>0/37</td><td>0/46</td></tr><tr><td>DSOS</td><td>0/92</td><td>0/25</td><td>0/43</td><td>0/75</td><td>0/51</td><td>0/74</td><td><bold>0/51</bold></td><td>0/67</td><td><bold>0/49</bold></td><td>0/67</td><td>0/44</td><td>0/61</td><td>0/34</td><td>0/39</td><td>0/44</td></tr><tr><td>CUK</td><td>0/92</td><td>0/25</td><td>0/43</td><td>0/75</td><td>0/51</td><td>0/83</td><td>0/40</td><td>0/65</td><td>0/60</td><td>0/65</td><td>0/43</td><td>0/59</td><td>0/28</td><td>0/41</td><td>0/43</td></tr><tr><td>PCRR</td><td><bold>0/98</bold></td><td>0/04</td><td>0/36</td><td>0/96</td><td>0/43</td><td>0/96</td><td>0/02</td><td>0/67</td><td>0/98</td><td>0/67</td><td>0/43</td><td>0/28</td><td>0/15</td><td>0/72</td><td>0/17</td></tr><tr><td>LAP</td><td>0/94</td><td>0/17</td><td>0/40</td><td>0/83</td><td>0/48</td><td>0/77</td><td>0/35</td><td>0/67</td><td>0/65</td><td>0/67</td><td>0/44</td><td>0/39</td><td>0/18</td><td>0/61</td><td>0/27</td></tr><tr><td>ENT</td><td>0/94</td><td>0/17</td><td>0/40</td><td>0/83</td><td>0/48</td><td><bold>1/00</bold></td><td>0/00</td><td>0/67</td><td>1</td><td>0/67</td><td>0/43</td><td>0/61</td><td>0/30</td><td>0/39</td><td>0/45</td></tr><tr><td>MI</td><td>1/00</td><td>0/00</td><td>0/35</td><td>1/00</td><td>0/41</td><td><bold>1/00</bold></td><td>0/00</td><td>0/68</td><td>1</td><td>0/68</td><td><bold>0/50</bold></td><td>0/00</td><td>0/00</td><td>1/00</td><td>0/00</td></tr><tr><td>Fisher</td><td>1/00</td><td>0/00</td><td>0/35</td><td>1/00</td><td>0/41</td><td>0/98</td><td>0/06</td><td>0/67</td><td>0/94</td><td>0/67</td><td><bold>0/50</bold></td><td>0/00</td><td>0/00</td><td>1/00</td><td>0/00</td></tr></tbody></table><table-wrap-foot><p>Boldface values indicate the best-obtained results of each criterion for every learner</p></table-wrap-foot></table-wrap></p>
  </sec>
  <sec id="Sec9">
    <title>Discussion</title>
    <p id="Par79">Feature selection is one the most important steps in machine learning applications. For this purpose, many tools and methods have been introduced by researchers. For example, a feature weighting tool for unsupervised applications [<xref ref-type="bibr" rid="CR54">54</xref>] and Weka machine learning tool [<xref ref-type="bibr" rid="CR55">55</xref>] have been developed. However, the main limitation of these tools like mRMR [<xref ref-type="bibr" rid="CR56">56</xref>] and mRMD [<xref ref-type="bibr" rid="CR57">57</xref>] is that they are based on filter methods which only consider the relation among features and disregard interaction between feature selection algorithm and learner. As another example, we can mention a wrapper feature selection tool which is based on genetic algorithm [<xref ref-type="bibr" rid="CR58">58</xref>]. Although time complexity of wrapper methods are higher than filter ones, these methods can lead better results; and it is valuable to spend more time. In this paper, we proposed a machine learning software named FeatureSelect that includes three types of popular learners (SVM, ANN and DT). In addition, two types of feature selection method are available in it. First method is wrapper method that is based on optimisation algorithms. Eleven state-of-art optimisation algorithms have been selected based on their popularity, novelty and functionality, and then implemented in FeatureSelect. Second type is the filter method which is based on Pearson correlation, entropy, Laplacian, mutual information and fisher scores. A user can also combine existing methods and algorithms, and then use them as ensemble or hybrid method like hybrid feature selection methods [<xref ref-type="bibr" rid="CR59">59</xref>]. For example, a user can confine a number of features to specific threshold using filter methods. After it, the user can use wrapper methods along with an agile learner such as SVM or DT for acquiring an optimal subset of features, and finally engage and test ANN with enhancing a number of training iterations to obtain suitable model. There are also some other application-specific tools like iFeature [<xref ref-type="bibr" rid="CR60">60</xref>] which is used for extracting and selecting features from protein and peptide sequences. Although iFeature includes a web server besides a stand-alone tool, FeatureSelect is the general software and provides different capabilities like hybrid feature selection and ensemble learning based on various states of combining filter and wrapper methods. In order to show capabilities of FeatureSelect, we applied it on various datasets with different sizes in multiple areas. The results show that every algorithm and every learner has its attitude relative to data, and algorithms’ performances vary on different data. In another comprehensive experiment, we applied all of algorithms and learners of FeatureSelect on the BASEHOCK dataset with multi-objective score function. Although filter methods are quicker than wrapper methods, the acquired results present that wrapper methods’ performance are proper than the filter methods.</p>
  </sec>
  <sec id="Sec10">
    <title>Conclusions</title>
    <p id="Par80">In this paper, a new software application for feature selection is proposed. This software is called FeatureSelect, and can be used in fields such as biology, image processing, drug design and numerous other domains. FeatureSelect selects a subset of features using optimisation algorithms with considering different score functions and then transmits these to the learner. SVM, ANN and DT are used here as a learner that can be applied to classification and regression datasets. Since LIBSVM is a library for SVM and provides a wide range of options for classification and regression problems, we developed FeatureSelect based on this library. Researchers can apply FeatureSelect to any dataset using three types of learners and two types of feature selection methods and obtain various tables and diagrams based on the nature of the dataset. It is also possible to combine the methods and algorithms as ensemble method. FeatureSelect was applied to eight datasets with differing scope and size. We then compared the performance of the algorithms in FeatureSelect to these datasets and presented some examples of the outputs in the form of tables and diagrams. Although the algorithms and feature selection methods have different functionality for different datasets, WCC, LCA, LA and FOA are the algorithms having proper functionality than others, and wrapper methods lead better results than filter methods.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Additional file</title>
    <sec id="Sec11">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2019_2754_MOESM1_ESM.zip">
            <label>Additional file 1:</label>
            <caption>
              <p>The supplementary file. It consists of source codes. FeatureSelect has been implemented in MATLAB and is free open source software. Therefore, users can change or improve it. The modified versions of it will be uploaded to the GItHub repository. Also, three types of stand-alone versions of FeatureSelect, including WIN 64-bit, java, and python packages, are available. (ZIP 151 mb)</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>ACC</term>
        <def>
          <p id="Par4">Accuracy</p>
        </def>
      </def-item>
      <def-item>
        <term>ACO</term>
        <def>
          <p id="Par5">Ant Colony Optimization</p>
        </def>
      </def-item>
      <def-item>
        <term>ANN</term>
        <def>
          <p id="Par6">Artificial Neural Network</p>
        </def>
      </def-item>
      <def-item>
        <term>CUK</term>
        <def>
          <p id="Par7">Cuckoo algorithm</p>
        </def>
      </def-item>
      <def-item>
        <term>DSOS</term>
        <def>
          <p id="Par8">Discrete Symbiotic Optimization Search</p>
        </def>
      </def-item>
      <def-item>
        <term>ER</term>
        <def>
          <p id="Par9">Error</p>
        </def>
      </def-item>
      <def-item>
        <term>FOA</term>
        <def>
          <p id="Par10">Forest Optimization Algorithm</p>
        </def>
      </def-item>
      <def-item>
        <term>FPR</term>
        <def>
          <p id="Par11">False Positive Rate</p>
        </def>
      </def-item>
      <def-item>
        <term>FS</term>
        <def>
          <p id="Par12">Feature Selection</p>
        </def>
      </def-item>
      <def-item>
        <term>GA</term>
        <def>
          <p id="Par13">Genetic Algorithm</p>
        </def>
      </def-item>
      <def-item>
        <term>HTS</term>
        <def>
          <p id="Par14">Heat Transfer Optimization</p>
        </def>
      </def-item>
      <def-item>
        <term>ICA</term>
        <def>
          <p id="Par15">Imperialist Competitive Algorithm</p>
        </def>
      </def-item>
      <def-item>
        <term>LA</term>
        <def>
          <p id="Par16">Learning Automata</p>
        </def>
      </def-item>
      <def-item>
        <term>LCA</term>
        <def>
          <p id="Par17">League Championship Algorithm</p>
        </def>
      </def-item>
      <def-item>
        <term>PRE</term>
        <def>
          <p id="Par18">Precision</p>
        </def>
      </def-item>
      <def-item>
        <term>PSO</term>
        <def>
          <p id="Par19">Particle Swarm Optimization</p>
        </def>
      </def-item>
      <def-item>
        <term>SEN</term>
        <def>
          <p id="Par20">Sensitivity</p>
        </def>
      </def-item>
      <def-item>
        <term>SPC</term>
        <def>
          <p id="Par21">Specificity</p>
        </def>
      </def-item>
      <def-item>
        <term>SVM</term>
        <def>
          <p id="Par22">Support Vector Machine</p>
        </def>
      </def-item>
      <def-item>
        <term>WCC</term>
        <def>
          <p id="Par23">World Competitive Contest Algorithm</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
    <sec id="FPar1">
      <title>Availability and requirements</title>
      <p id="Par81">Project name: FeatureSelect. Project homepage: <ext-link ext-link-type="uri" xlink:href="https://github.com/LBBSoft/FeatureSelect">https://github.com/LBBSoft/FeatureSelect</ext-link>, Operating systems: Win 10, Linux, and Mac. Programing language: MATLAB. Requirements: MATLAB Runtime, SDK, python 2.7, 3.4, or 3.5 (if a user runs the FeatureSelect using the python package), and java version 1.8 (if a user runs the FeatureSelect using the java package). License: MIT. Any restrictions to use by non-academics: MIT license.</p>
    </sec>
    <sec id="FPar2">
      <title>Funding</title>
      <p id="Par82">No funding.</p>
    </sec>
    <sec id="FPar3" sec-type="data-availability">
      <title>Availability of data and materials</title>
      <p id="Par83">FeatureSelect has been implemented in MATLAB programing language and is available at (<ext-link ext-link-type="uri" xlink:href="https://github.com/LBBSoft/FeatureSelect"><italic>https://github.com/LBBSoft/FeatureSelect</italic></ext-link>). In addition to the code and datasets, three stand-alone versions including java-package, python package, and an exe file for win_64_bit are also accessible.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>YMS: Conceptualization, software programming, formal analysis, investigation, writing-manuscript. HMG: Software testing, validation, visualization writing-manuscript. AMN: Conceptualization, Supervision, Project administration, Editing the manuscript. All authors have read and approved the manuscript.</p>
  </notes>
  <notes notes-type="COI-statement">
    <sec id="FPar4">
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="FPar5">
      <title>Consent for publication</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="FPar6">
      <title>Competing interests</title>
      <p>The authors declare that they have no competing interests.</p>
    </sec>
    <sec id="FPar7">
      <title>Publisher’s Note</title>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </sec>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Miao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Niu</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>A survey on feature selection</article-title>
        <source>Procedia Computer Science</source>
        <year>2016</year>
        <volume>91</volume>
        <fpage>919</fpage>
        <lpage>926</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>MotieGhader</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Gharaghani</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Masoudi-Sobhanzadeh</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Masoudi-Nejad</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Sequential and mixed genetic algorithm and learning automata (SGALA, MGALA) for feature selection in QSAR</article-title>
        <source>Iranian Journal of Pharmaceutical Research</source>
        <year>2017</year>
        <volume>16</volume>
        <issue>2</issue>
        <fpage>533</fpage>
        <lpage>553</lpage>
        <?supplied-pmid 28979308?>
        <pub-id pub-id-type="pmid">28979308</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sheikhpour</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Sarram</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Gharaghani</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chahooki</surname>
            <given-names>MAZ</given-names>
          </name>
        </person-group>
        <article-title>A survey on semi-supervised feature selection methods</article-title>
        <source>Pattern Recogn</source>
        <year>2017</year>
        <volume>64</volume>
        <fpage>141</fpage>
        <lpage>158</lpage>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <mixed-citation publication-type="other">Ghaddar B, Naoum-Sawaya J. High dimensional data classification and feature selection using support vector machines. Eur J Oper Res. 2017.</mixed-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Fang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>Pse-in-one: a web server for generating various modes of pseudo components of DNA, RNA, and protein sequences</article-title>
        <source>Nucleic Acids Res</source>
        <year>2015</year>
        <volume>43</volume>
        <issue>W1</issue>
        <fpage>W65</fpage>
        <lpage>W71</lpage>
        <?supplied-pmid 25958395?>
        <pub-id pub-id-type="pmid">25958395</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Xiao</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Cao</surname>
            <given-names>D-S</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>M-F</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Q-S</given-names>
          </name>
        </person-group>
        <article-title>Protr/ProtrWeb: R package and web server for generating various numerical representation schemes of protein sequences</article-title>
        <source>Bioinformatics</source>
        <year>2015</year>
        <volume>31</volume>
        <issue>11</issue>
        <fpage>1857</fpage>
        <lpage>1859</lpage>
        <?supplied-pmid 25619996?>
        <pub-id pub-id-type="pmid">25619996</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <mixed-citation publication-type="other">Rahmaninia M, Moradi P. OSFSMI: online stream feature selection method based on mutual information. Appl Soft Comput. 2017.</mixed-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Che</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Bai</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Maximum relevance minimum common redundancy feature selection for nonlinear data</article-title>
        <source>Inf Sci</source>
        <year>2017</year>
        <volume>409</volume>
        <fpage>68</fpage>
        <lpage>86</lpage>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sanz</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Valim</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vegas</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Oller</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Reverter</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>SVM-RFE: selection and visualization of the most relevant features through non-linear kernels</article-title>
        <source>BMC bioinformatics</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>1</issue>
        <fpage>432</fpage>
        <?supplied-pmid 30453885?>
        <pub-id pub-id-type="pmid">30453885</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <mixed-citation publication-type="other">Viegas F, Rocha L, Gonçalves M, Mourão F, Sá G, Salles T, Andrade G, Sandin I. A genetic programming approach for feature selection in highly dimensional skewed data. Neurocomputing. 2017.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Izetta</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Verdes</surname>
            <given-names>PF</given-names>
          </name>
          <name>
            <surname>Granitto</surname>
            <given-names>PM</given-names>
          </name>
        </person-group>
        <article-title>Improved multiclass feature selection via list combination</article-title>
        <source>Expert Syst Appl</source>
        <year>2017</year>
        <volume>88</volume>
        <fpage>205</fpage>
        <lpage>216</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <mixed-citation publication-type="other">Xiao J, Cao H, Jiang X, Gu X, Xie L. GMDH-based semi-supervised feature selection for customer classification. Knowl-Based Syst. 2017.</mixed-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Feature selection based on quality of information</article-title>
        <source>Neurocomputing</source>
        <year>2017</year>
        <volume>225</volume>
        <fpage>11</fpage>
        <lpage>22</lpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goswami</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Das</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Chakrabarti</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chakraborty</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>A feature cluster taxonomy based feature selection technique</article-title>
        <source>Expert Syst Appl</source>
        <year>2017</year>
        <volume>79</volume>
        <fpage>76</fpage>
        <lpage>89</lpage>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Zhou P, Hu X, Li P, Wu X. Online feature selection for high-dimensional class-imbalanced data. Knowl-Based Syst. 2017.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Ding</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>LOFS: a library of online streaming feature selection</article-title>
        <source>Knowl-Based Syst</source>
        <year>2016</year>
        <volume>113</volume>
        <fpage>1</fpage>
        <lpage>3</lpage>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>JCDSA: a joint covariate detection tool for survival analysis on tumor expression profiles</article-title>
        <source>BMC bioinformatics</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>1</issue>
        <fpage>187</fpage>
        <?supplied-pmid 29843599?>
        <pub-id pub-id-type="pmid">29843599</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Yang R, Zhang C, Zhang L, Gao R. A two-step feature selection method to predict Cancerlectins by Multiview features and synthetic minority oversampling technique. Biomed Res Int. 2018;2018.</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ge</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Luo</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Meng</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Mai</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>McTwo: a two-step feature selection algorithm based on maximal information coefficient</article-title>
        <source>BMC bioinformatics</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>142</fpage>
        <?supplied-pmid 27006077?>
        <pub-id pub-id-type="pmid">27006077</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <mixed-citation publication-type="other">Metin SK. Feature selection in multiword expression recognition. Expert Syst Appl. 2017.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <mixed-citation publication-type="other">Lu H, Chen J, Yan K, Jin Q, Xue Y, Gao Z. A hybrid feature selection algorithm for gene expression data classification. Neurocomputing. 2017.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Maldonado</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lopez</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Synchronized feature selection for support vector machines with twin hyperplanes</article-title>
        <source>Knowl-Based Syst</source>
        <year>2017</year>
        <volume>132</volume>
        <fpage>119</fpage>
        <lpage>128</lpage>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ma</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Xia</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>A tribe competition-based genetic algorithm for feature selection in pattern classification</article-title>
        <source>Appl Soft Comput</source>
        <year>2017</year>
        <volume>58</volume>
        <fpage>328</fpage>
        <lpage>338</lpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Peng H, Fan Y: Feature selection by optimizing a lower bound of conditional mutual information. <italic>Information Sciences</italic> 2017, 418(Supplement C):652–667.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Hamedmoghadam-Rafati H, Jalili M, Yu X. An opinion formation based binary optimization approach for feature selection. Physica A: Statistical Mechanics and its Applications. 2017.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chandrashekar</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Sahin</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>A survey on feature selection methods</article-title>
        <source>Computers &amp; Electrical Engineering</source>
        <year>2014</year>
        <volume>40</volume>
        <issue>1</issue>
        <fpage>16</fpage>
        <lpage>28</lpage>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lazar</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Taminau</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Meganck</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Steenhoff</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Coletta</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Molter</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>de Schaetzen</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Duque</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Bersini</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Nowe</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>A survey on filter techniques for feature selection in gene expression microarray analysis</article-title>
        <source>IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)</source>
        <year>2012</year>
        <volume>9</volume>
        <issue>4</issue>
        <fpage>1106</fpage>
        <lpage>1119</lpage>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Lee PY, Loh WP, Chin JF. Feature selection in multimedia: the state-of-the-art review. Image Vis Comput. 2017.</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <mixed-citation publication-type="other">Panday D, Cordeiro de Amorim R, Lane P. Feature weighting as a tool for unsupervised feature selection. Inf Process Lett. 2017.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sadeghianpourhamami</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Ruyssinck</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Deschrijver</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Dhaene</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Develder</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Comprehensive feature selection for appliance classification in NILM</article-title>
        <source>Energy and Buildings</source>
        <year>2017</year>
        <volume>151</volume>
        <fpage>98</fpage>
        <lpage>106</lpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Du</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Robust unsupervised feature selection via matrix factorization</article-title>
        <source>Neurocomputing</source>
        <year>2017</year>
        <volume>241</volume>
        <fpage>115</fpage>
        <lpage>127</lpage>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Agnihotri</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Verma</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Tripathi</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Variable global feature selection scheme for automatic classification of text documents</article-title>
        <source>Expert Syst Appl</source>
        <year>2017</year>
        <volume>81</volume>
        <fpage>268</fpage>
        <lpage>281</lpage>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Oreski</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Oreski</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Klicek</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>Effects of dataset characteristics on the performance of feature selection techniques</article-title>
        <source>Appl Soft Comput</source>
        <year>2017</year>
        <volume>52</volume>
        <fpage>109</fpage>
        <lpage>119</lpage>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Feature selection with effective distance</article-title>
        <source>Neurocomputing</source>
        <year>2016</year>
        <volume>215</volume>
        <fpage>100</fpage>
        <lpage>109</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Das</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Goswami</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Chakrabarti</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Chakraborty</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>A new hybrid feature selection approach using feature association map for supervised and unsupervised classification</article-title>
        <source>Expert Syst Appl</source>
        <year>2017</year>
        <volume>88</volume>
        <fpage>81</fpage>
        <lpage>94</lpage>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Hu</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wen</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Feature self-representation based hypergraph unsupervised feature selection via low-rank representation</article-title>
        <source>Neurocomputing</source>
        <year>2017</year>
        <volume>253</volume>
        <fpage>127</fpage>
        <lpage>134</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Toward integrating feature selection algorithms for classification and clustering</article-title>
        <source>IEEE Trans Knowl Data Eng</source>
        <year>2005</year>
        <volume>17</volume>
        <issue>4</issue>
        <fpage>491</fpage>
        <lpage>502</lpage>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liang</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>C-F</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>H-T</given-names>
          </name>
        </person-group>
        <article-title>The effect of feature selection on financial distress prediction</article-title>
        <source>Knowl-Based Syst</source>
        <year>2015</year>
        <volume>73</volume>
        <fpage>289</fpage>
        <lpage>297</lpage>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Golay</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Leuenberger</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Kanevski</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Feature selection for regression problems based on the Morisita estimator of intrinsic dimension</article-title>
        <source>Pattern Recogn</source>
        <year>2017</year>
        <volume>70</volume>
        <fpage>126</fpage>
        <lpage>138</lpage>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yu</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Rough sets and Laplacian score based cost-sensitive feature selection</article-title>
        <source>PLoS One</source>
        <year>2018</year>
        <volume>13</volume>
        <issue>6</issue>
        <fpage>e0197564</fpage>
        <?supplied-pmid 29912884?>
        <pub-id pub-id-type="pmid">29912884</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Sui</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>A relative decision entropy-based feature selection approach</article-title>
        <source>Pattern Recogn</source>
        <year>2015</year>
        <volume>48</volume>
        <issue>7</issue>
        <fpage>2151</fpage>
        <lpage>2163</lpage>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Gu Q, Li Z, Han J: Generalized fisher score for feature selection. <italic>arXiv preprint arXiv:12023725</italic> 2012.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Hira ZM, Gillies DF. A review of feature selection and feature extraction methods applied on microarray data. Adv Bioinforma. 2015;2015.</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hancer</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Xue</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Differential evolution for filter feature selection based on information theory and feature ranking</article-title>
        <source>Knowl-Based Syst</source>
        <year>2018</year>
        <volume>140</volume>
        <fpage>103</fpage>
        <lpage>119</lpage>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cortes</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support-vector networks</article-title>
        <source>Mach Learn</source>
        <year>1995</year>
        <volume>20</volume>
        <issue>3</issue>
        <fpage>273</fpage>
        <lpage>297</lpage>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ben-Hur</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Horn</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Siegelmann</surname>
            <given-names>HT</given-names>
          </name>
          <name>
            <surname>Vapnik</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>Support vector clustering</article-title>
        <source>J Mach Learn Res</source>
        <year>2001</year>
        <volume>2</volume>
        <issue>Dec</issue>
        <fpage>125</fpage>
        <lpage>137</lpage>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chang</surname>
            <given-names>C-C</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>C-J</given-names>
          </name>
        </person-group>
        <article-title>LIBSVM: a library for support vector machines</article-title>
        <source>ACM transactions on intelligent systems and technology (TIST)</source>
        <year>2011</year>
        <volume>2</volume>
        <issue>3</issue>
        <fpage>27</fpage>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wei</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yao</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Incorporating knowledge into neural network for text representation</article-title>
        <source>Expert Syst Appl</source>
        <year>2018</year>
        <volume>96</volume>
        <fpage>103</fpage>
        <lpage>114</lpage>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Region compatibility based stability assessment for decision trees</article-title>
        <source>Expert Syst Appl</source>
        <year>2018</year>
        <volume>105</volume>
        <fpage>112</fpage>
        <lpage>128</lpage>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <mixed-citation publication-type="other">Diaz-Hermida F, Pereira-Fariña M, Vidal JC, Ramos-Soto A. Characterizing quantifier Fuzzification mechanisms: a behavioral guide for applications. Fuzzy Sets Syst. 2017.</mixed-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Črepinšek</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>S-H</given-names>
          </name>
          <name>
            <surname>Mernik</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Replication and comparison of computational experiments in applied evolutionary computing: common pitfalls and guidelines to avoid them</article-title>
        <source>Appl Soft Comput</source>
        <year>2014</year>
        <volume>19</volume>
        <fpage>161</fpage>
        <lpage>170</lpage>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Schubert A-L, Hagemann D, Voss A, Bergmann K: Evaluating the model fit of diffusion models with the root mean square error of approximation. <italic>Journal of Mathematical Psychology</italic> 2017, <bold>77</bold>(Supplement C):29–45.</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hanley</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>McNeil</surname>
            <given-names>BJ</given-names>
          </name>
        </person-group>
        <article-title>The meaning and use of the area under a receiver operating characteristic (ROC) curve</article-title>
        <source>Radiology</source>
        <year>1982</year>
        <volume>143</volume>
        <issue>1</issue>
        <fpage>29</fpage>
        <lpage>36</lpage>
        <?supplied-pmid 7063747?>
        <pub-id pub-id-type="pmid">7063747</pub-id>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Panday</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>de Amorim</surname>
            <given-names>RC</given-names>
          </name>
          <name>
            <surname>Lane</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Feature weighting as a tool for unsupervised feature selection</article-title>
        <source>Inf Process Lett</source>
        <year>2018</year>
        <volume>129</volume>
        <fpage>44</fpage>
        <lpage>52</lpage>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Witten</surname>
            <given-names>IH</given-names>
          </name>
          <name>
            <surname>Frank</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Trigg</surname>
            <given-names>LE</given-names>
          </name>
          <name>
            <surname>Hall</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Holmes</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Cunningham</surname>
            <given-names>SJ</given-names>
          </name>
        </person-group>
        <source>Weka: practical machine learning tools and techniques with Java implementations</source>
        <year>1999</year>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Peng</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Minimum redundancy feature selection from microarray gene expression data</article-title>
        <source>J Bioinforma Comput Biol</source>
        <year>2005</year>
        <volume>3</volume>
        <issue>02</issue>
        <fpage>185</fpage>
        <lpage>205</lpage>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wei</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Xing</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ji</surname>
            <given-names>Z-L</given-names>
          </name>
          <name>
            <surname>Zou</surname>
            <given-names>Q</given-names>
          </name>
        </person-group>
        <article-title>Fast prediction of protein methylation sites using a sequence-based feature selection technique</article-title>
        <source>IEEE/ACM Transactions on Computational Biology and Bioinformatics</source>
        <year>2017</year>
        <volume>1</volume>
        <fpage>1</fpage>
        <lpage>1</lpage>
      </element-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Soufan</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Kleftogiannis</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kalnis</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Bajic</surname>
            <given-names>VB</given-names>
          </name>
        </person-group>
        <article-title>DWFS: a wrapper feature selection tool based on a parallel genetic algorithm</article-title>
        <source>PLoS One</source>
        <year>2015</year>
        <volume>10</volume>
        <issue>2</issue>
        <fpage>e0117988</fpage>
        <?supplied-pmid 25719748?>
        <pub-id pub-id-type="pmid">25719748</pub-id>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Hybrid feature selection using component co-occurrence based feature relevance measurement</article-title>
        <source>Expert Syst Appl</source>
        <year>2018</year>
        <volume>102</volume>
        <fpage>83</fpage>
        <lpage>99</lpage>
      </element-citation>
    </ref>
    <ref id="CR60">
      <label>60.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Leier</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marquez-Lago</surname>
            <given-names>TT</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Webb</surname>
            <given-names>GI</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>AI</given-names>
          </name>
          <name>
            <surname>Daly</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Chou</surname>
            <given-names>K-C</given-names>
          </name>
        </person-group>
        <article-title>iFeature: a python package and web server for features extraction and selection from protein and peptide sequences</article-title>
        <source>Bioinformatics</source>
        <year>2018</year>
        <volume>1</volume>
        <fpage>4</fpage>
      </element-citation>
    </ref>
    <ref id="CR61">
      <label>61.</label>
      <mixed-citation publication-type="other">Masoudi-Sobhanzadeh Y, Motieghader H: World Competitive Contests (WCC) algorithm: A novel intelligent optimization algorithm for biological and non-biological problems. Informatics in Medicine Unlocked 2016, 3(Supplement C):15–28.</mixed-citation>
    </ref>
    <ref id="CR62">
      <label>62.</label>
      <mixed-citation publication-type="other">Husseinzadeh Kashan A: League Championship Algorithm (LCA): An algorithm for global optimization inspired by sport championships. <italic>Applied Soft Computing</italic> 2014, 16(Supplement C):171–200.</mixed-citation>
    </ref>
    <ref id="CR63">
      <label>63.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Holland</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <article-title>Searching nonlinear functions for high values</article-title>
        <source>Appl Math Comput</source>
        <year>1989</year>
        <volume>32</volume>
        <issue>2</issue>
        <fpage>255</fpage>
        <lpage>274</lpage>
      </element-citation>
    </ref>
    <ref id="CR64">
      <label>64.</label>
      <mixed-citation publication-type="other">Eberhart R, Kennedy J: A new optimizer using particle swarm theory. In: <italic>Micro Machine and Human Science, 1995 MHS'95, Proceedings of the Sixth International Symposium on:</italic> 1995. IEEE: 39–43.</mixed-citation>
    </ref>
    <ref id="CR65">
      <label>65.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dorigo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Birattari</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Stutzle</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Ant colony optimization</article-title>
        <source>IEEE Comput Intell Mag</source>
        <year>2006</year>
        <volume>1</volume>
        <issue>4</issue>
        <fpage>28</fpage>
        <lpage>39</lpage>
      </element-citation>
    </ref>
    <ref id="CR66">
      <label>66.</label>
      <mixed-citation publication-type="other">Atashpaz-Gargari E, Lucas C: Imperialist competitive algorithm: an algorithm for optimization inspired by imperialistic competition. In: <italic>Evolutionary computation,</italic> 2007 CEC 2007 IEEE congress on: 2007. IEEE: 4661–4667.</mixed-citation>
    </ref>
    <ref id="CR67">
      <label>67.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Meybodi</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Beigy</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>New learning automata based algorithms for adaptation of backpropagation algorithm parameters</article-title>
        <source>Int J Neural Syst</source>
        <year>2002</year>
        <volume>12</volume>
        <issue>01</issue>
        <fpage>45</fpage>
        <lpage>67</lpage>
        <?supplied-pmid 11852444?>
        <pub-id pub-id-type="pmid">11852444</pub-id>
      </element-citation>
    </ref>
    <ref id="CR68">
      <label>68.</label>
      <mixed-citation publication-type="other">Patel VK, Savsani VJ: Heat transfer search (HTS): a novel optimization algorithm. <italic>Information Sciences</italic> 2015, 324(Supplement C):217–246.</mixed-citation>
    </ref>
    <ref id="CR69">
      <label>69.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ghaemi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Feizi-Derakhshi</surname>
            <given-names>M-R</given-names>
          </name>
        </person-group>
        <article-title>Forest optimization algorithm</article-title>
        <source>Expert Syst Appl</source>
        <year>2014</year>
        <volume>41</volume>
        <issue>15</issue>
        <fpage>6676</fpage>
        <lpage>6687</lpage>
      </element-citation>
    </ref>
    <ref id="CR70">
      <label>70.</label>
      <mixed-citation publication-type="other">Ezugwu AE-S, Adewumi AO: Discrete symbiotic organisms search algorithm for travelling salesman problem. <italic>Expert Systems with Applications</italic> 2017, 87(Supplement C):70–78.</mixed-citation>
    </ref>
    <ref id="CR71">
      <label>71.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Rajabioun</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Cuckoo optimization algorithm</article-title>
        <source>Appl Soft Comput</source>
        <year>2011</year>
        <volume>11</volume>
        <issue>8</issue>
        <fpage>5508</fpage>
        <lpage>5518</lpage>
      </element-citation>
    </ref>
    <ref id="CR72">
      <label>72.</label>
      <mixed-citation publication-type="other">Fernandes K, Vinagre P, Cortez P: A proactive intelligent decision support system for predicting the popularity of online news. In: <italic>Portuguese Conference on Artificial Intelligence:</italic> 2015. Springer: 535–546.</mixed-citation>
    </ref>
    <ref id="CR73">
      <label>73.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Laufer</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Patel</surname>
            <given-names>NKB</given-names>
          </name>
          <name>
            <surname>Edwards</surname>
            <given-names>LG</given-names>
          </name>
          <name>
            <surname>Lang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S-W</given-names>
          </name>
          <name>
            <surname>Feher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Awrey</surname>
            <given-names>DE</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Discovery of inhibitors of the mitotic kinase TTK based on N-(3-(3-sulfamoylphenyl)-1H-indazol-5-yl)-acetamides and carboxamides</article-title>
        <source>Bioorg Med Chem</source>
        <year>2014</year>
        <volume>22</volume>
        <issue>17</issue>
        <fpage>4968</fpage>
        <lpage>4997</lpage>
        <?supplied-pmid 25043312?>
        <pub-id pub-id-type="pmid">25043312</pub-id>
      </element-citation>
    </ref>
    <ref id="CR74">
      <label>74.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>De Vito</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Massera</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Piga</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Martinotto</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Di Francia</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>On field calibration of an electronic nose for benzene estimation in an urban pollution monitoring scenario</article-title>
        <source>Sensors Actuators B Chem</source>
        <year>2008</year>
        <volume>129</volume>
        <issue>2</issue>
        <fpage>750</fpage>
        <lpage>757</lpage>
      </element-citation>
    </ref>
    <ref id="CR75">
      <label>75.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Candanedo</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Feldheim</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Deramaix</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Data driven prediction models of energy use of appliances in a low-energy house</article-title>
        <source>Energy and Buildings</source>
        <year>2017</year>
        <volume>140</volume>
        <fpage>81</fpage>
        <lpage>97</lpage>
      </element-citation>
    </ref>
    <ref id="CR76">
      <label>76.</label>
      <mixed-citation publication-type="other">Li J, Cheng K, Wang S, Morstatter F, Trevino RP, Tang J, Liu H: Feature selection: A data perspective. <italic>arXiv preprint arXiv:160107996</italic> 2016.</mixed-citation>
    </ref>
    <ref id="CR77">
      <label>77.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Diaz-Chito</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Hernández-Sabaté</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>López</surname>
            <given-names>AM</given-names>
          </name>
        </person-group>
        <article-title>A reduced feature set for driver head pose estimation</article-title>
        <source>Appl Soft Comput</source>
        <year>2016</year>
        <volume>45</volume>
        <fpage>98</fpage>
        <lpage>107</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
