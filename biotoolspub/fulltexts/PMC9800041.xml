<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Integr Bioinform</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Integr Bioinform</journal-id>
    <journal-id journal-id-type="publisher-id">jib</journal-id>
    <journal-id journal-id-type="doi">jib</journal-id>
    <journal-title-group>
      <journal-title>Journal of Integrative Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1613-4516</issn>
    <publisher>
      <publisher-name>De Gruyter</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9800041</article-id>
    <article-id pub-id-type="publisher-id">jib-2022-0018</article-id>
    <article-id pub-id-type="doi">10.1515/jib-2022-0018</article-id>
    <article-id pub-id-type="pmid">36017752</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>KaIDA: a modular tool for assisting image annotation in deep learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="false" contrib-id-type="orcid">https://orcid.org/0000-0001-7366-2134</contrib-id>
        <name>
          <surname>Schilling</surname>
          <given-names>Marcel P.</given-names>
        </name>
        <xref rid="j_jib-2022-0018_aff_001" ref-type="aff"/>
        <email xlink:href="mailto:marcel.schilling@kit.edu">marcel.schilling@kit.edu</email>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="false" contrib-id-type="orcid">https://orcid.org/0000-0003-4989-591X</contrib-id>
        <name>
          <surname>Schmelzer</surname>
          <given-names>Svenja</given-names>
        </name>
        <xref rid="j_jib-2022-0018_aff_001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Klinger</surname>
          <given-names>Lukas</given-names>
        </name>
        <xref rid="j_jib-2022-0018_aff_001" ref-type="aff"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Reischl</surname>
          <given-names>Markus</given-names>
        </name>
        <xref rid="j_jib-2022-0018_aff_001" ref-type="aff"/>
      </contrib>
      <aff id="j_jib-2022-0018_aff_001"><institution content-type="university">Institute for Automation and Applied Informatics, Karlsruhe Institute of Technology</institution>, <addr-line>D-76344</addr-line><city>Eggenstein-Leopoldshafen</city>, <country country="DE">Germany</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><bold>Corresponding author: Marcel P. Schilling</bold>, <institution content-type="university">Institute for Automation and Applied Informatics, Karlsruhe Institute of Technology</institution>, <addr-line>D-76344</addr-line><city>Eggenstein-Leopoldshafen</city>, <country country="DE">Germany</country>, E-mail: <email xlink:href="mailto:marcel.schilling@kit.edu">marcel.schilling@kit.edu</email></corresp>
    </author-notes>
    <pub-date date-type="pub" publication-format="electronic">
      <day>26</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date date-type="collection" publication-format="electronic">
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <volume>19</volume>
    <issue>4</issue>
    <elocation-id seq="6">20220018</elocation-id>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>3</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>6</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2022 the author(s), published by De Gruyter, Berlin/Boston</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>the author(s), published by De Gruyter, Berlin/Boston GmbH, Berlin/Boston</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This work is licensed under the Creative Commons Attribution 4.0 International License.</license-p>
      </license>
    </permissions>
    <abstract>
      <title>Abstract</title>
      <p>Deep learning models achieve high-quality results in image processing. However, to robustly optimize parameters of deep neural networks, large annotated datasets are needed. Image annotation is often performed manually by experts without a comprehensive tool for assistance which is time- consuming, burdensome, and not intuitive. Using the here presented modular Karlsruhe Image Data Annotation (KaIDA) tool, for the first time assisted annotation in various image processing tasks is possible to support users during this process. It aims to simplify annotation, increase user efficiency, enhance annotation quality, and provide additional useful annotation-related functionalities. KaIDA is available open-source at <ext-link xlink:href="https://git.scc.kit.edu/sc1357/kaida" ext-link-type="uri">https://git.scc.kit.edu/sc1357/kaida</ext-link>.</p>
    </abstract>
    <kwd-group>
      <title>Keywords</title>
      <kwd>data annotation</kwd>
      <kwd>deep learning</kwd>
      <kwd>deep neural networks</kwd>
      <kwd>high-throughput screening</kwd>
      <kwd>image processing</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant" id="award-grp1">
        <funding-source>KIT Future Fields II</funding-source>
        <award-id>Screening Platform for Personalized Oncology</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="13"/>
      <table-count count="2"/>
      <ref-count count="30"/>
      <page-count count="17"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="j_jib-2022-0018_s_999">
    <title>List of non-standard abbreviations</title>
    <p>
      <def-list>
        <def-item>
          <term>API</term>
          <def>
            <p>Application Programming Interface</p>
          </def>
        </def-item>
        <def-item>
          <term>DL</term>
          <def>
            <p>Deep Learning</p>
          </def>
        </def-item>
        <def-item>
          <term>DNN</term>
          <def>
            <p>Deep Neural Network</p>
          </def>
        </def-item>
        <def-item>
          <term>GUI</term>
          <def>
            <p>Graphical User Interface</p>
          </def>
        </def-item>
        <def-item>
          <term>KaIDA</term>
          <def>
            <p>Karlsruhe Image Data Annotation Tool</p>
          </def>
        </def-item>
        <def-item>
          <term>SPPO</term>
          <def>
            <p>Screening Platform for Personalized Oncology</p>
          </def>
        </def-item>
      </def-list>
    </p>
  </sec>
  <sec id="j_jib-2022-0018_s_001">
    <label>1</label>
    <title>Introduction</title>
    <p>Deep Neural Networks (DNNs) often outperform traditional image processing methods [<xref rid="j_jib-2022-0018_ref_001" ref-type="bibr">1</xref>] and are widely used in biomedical high-throughput screening applications, such as classification of tumors [<xref rid="j_jib-2022-0018_ref_002" ref-type="bibr">2</xref>], instance segmentation of stained cell nuclei [<xref rid="j_jib-2022-0018_ref_003" ref-type="bibr">3</xref>] to perform automated cell viability analysis as part of molecular biology experiments, semantic segmentation to analyze organs non-invasively [<xref rid="j_jib-2022-0018_ref_004" ref-type="bibr">4</xref>], or seed detection for biodiversity studies [<xref rid="j_jib-2022-0018_ref_005" ref-type="bibr">5</xref>]. The annotation of data by domain experts is crucial. However, it is time-consuming, expensive [<xref rid="j_jib-2022-0018_ref_001" ref-type="bibr">1</xref>, <xref rid="j_jib-2022-0018_ref_006" ref-type="bibr">6</xref>], and demotivating. Often, annotation quality decreases over time and is inconsistent between annotators, which leads to noisy datasets [<xref rid="j_jib-2022-0018_ref_007" ref-type="bibr">7</xref>].</p>
    <p>There are software solutions to assist image annotation, though, their focus is mainly on the perspective of data scientists which hampers the application for domain-experts [<xref rid="j_jib-2022-0018_ref_008" ref-type="bibr">8</xref>] such as biologists or clinicians (e.g. software/hardware requirements, source code instead of Graphical User Interfaces (GUIs)). Existing annotation tools [<xref rid="j_jib-2022-0018_ref_009" ref-type="bibr">9</xref>–<xref rid="j_jib-2022-0018_ref_014" ref-type="bibr">14</xref>] do not assist users during annotation at all, only provide partial assistance, or make it difficult to integrate methods for assistance. Thus, there is a lack of making annotation more enjoyable: Proposals for pre-annotation are missing to speed up the process, an automated selection of promising samples is not implemented, no automated annotation post-processing is available, there is no feedback regarding the quality of annotations made to keep annotators attentive, software is hard to handle without using up-to-date interfaces (GUI, touchscreen, pens etc.), and for each problem formulation (classification, detection, segmentation) different software exists. Further, although designing Deep Learning (DL) pipelines is an iterative process, concepts for versioning datasets are missing. Hence, neither a generic and comprehensive concept nor a corresponding open-source software implementation for assisted image annotation exists, although annotation impacts many projects since the potential of DL was discovered [<xref rid="j_jib-2022-0018_ref_015" ref-type="bibr">15</xref>]. Furthermore, the research community lacks criteria for evaluating annotation tools and a detailed comparison of state-of-the-art tools, respectively.</p>
    <p>Therefore, we contribute (i) a concept including an open-source software tool for assisted image annotation, (ii) introduce metrics for usability evaluation, and (iii) provide a scheme for comparing annotation tools. In addition, we demonstrate the application of our proposal, show its usability, and present the advantages compared to state-of-the-art tools.</p>
  </sec>
  <sec id="j_jib-2022-0018_s_002">
    <label>2</label>
    <title>Concept</title>
    <sec id="j_jib-2022-0018_s_002_s_001">
      <label>2.1</label>
      <title>Overview</title>
      <p>We present the tool Karlsruhe Image Data Annotation (KaIDA) enabling various ways of assistance for annotating images, solutions for smart data management, and broader ideas to support the application of DL (cf. <xref rid="j_jib-2022-0018_fig_001" ref-type="fig">Figure 1  i</xref>). The proposal is based on Label Assistant [<xref rid="j_jib-2022-0018_ref_018" ref-type="bibr">18</xref>], but extended to a generic and modular approach suited for various image processing tasks. The open-source software tool KaIDA supports the annotation of datasets, increases user efficiency, enhances annotation quality, manages dataset versions, and offers additional extensions useful in DL. Taking <xref rid="j_jib-2022-0018_fig_001" ref-type="fig">Figure 1  ii</xref> into account, we propose a practical setup of how KaIDA can be integrated into the workflows of future laboratories (cf. <xref rid="j_jib-2022-0018_s_007" ref-type="sec">Supplementary Information</xref>). In <xref rid="j_jib-2022-0018_fig_001" ref-type="fig">Figure 1  iii</xref>, exemplary results during assisted image annotation are illustrated.</p>
      <fig position="float" id="j_jib-2022-0018_fig_001" fig-type="figure">
        <label>Figure 1:</label>
        <caption>
          <p>Overview KaIDA. The interaction of the individual modules is shown via an overview diagram. (ii) Application. A domain expert uses the tool KaIDA in the proposed setup (cf. <xref rid="j_jib-2022-0018_s_007" ref-type="sec">Supplementary Information</xref>) for the annotation of images in the case of instance segmentation [<xref rid="j_jib-2022-0018_ref_003" ref-type="bibr">3</xref>, <xref rid="j_jib-2022-0018_ref_009" ref-type="bibr">9</xref>]. In additioPopova2019FacileOneStepn, KaIDA supports seed detection, classification, and semantic segmentation [<xref rid="j_jib-2022-0018_ref_002" ref-type="bibr">2</xref>, <xref rid="j_jib-2022-0018_ref_004" ref-type="bibr">4</xref>, <xref rid="j_jib-2022-0018_ref_005" ref-type="bibr">5</xref>]. (iii) Exemplary Results. Three reference images [<xref rid="j_jib-2022-0018_ref_003" ref-type="bibr">3</xref>, <xref rid="j_jib-2022-0018_ref_016" ref-type="bibr">16</xref>, <xref rid="j_jib-2022-0018_ref_017" ref-type="bibr">17</xref>] are utilized to present the possible methods of KaIDA. Via “Selection” heterogeneous samples are obtained, “Pre-Processing” crops/normalize the image, and “Pre-Annotation” provides an initial annotation, which is adapted by a user during “Image Annotation”. Errors may remain or are introduced unintendedly. Via “Post-Annotation-Process” small noisy elements are removed/holes are filled and “Annotation Inspection” warns that the prior of one segment per image, average segment area, or excentricity of a given segment is violated. Additionally, it is shown that not every module needs to be activated (None).</p>
        </caption>
        <graphic xlink:href="j_jib-2022-0018_fig_001" position="float"/>
      </fig>
    </sec>
    <sec id="j_jib-2022-0018_s_002_s_002">
      <label>2.2</label>
      <title>Modules</title>
      <p>In the following, the modules are explained in detail. First, a raw dataset (“Image Acquisition”, obtained, i.e., by a scanner, microscope, camera) is supplied with meta-information (“Create Project”, e.g. class information or problem categorization) essential for scientific data handling [<xref rid="j_jib-2022-0018_ref_019" ref-type="bibr">19</xref>].</p>
      <sec id="j_jib-2022-0018_s_002_s_002_s_001">
        <label>2.2.1</label>
        <title>Selection</title>
        <p>State-of-the-art open-source annotation tools do not support the idea of influencing the sampling order during annotation. To enable deep active learning [<xref rid="j_jib-2022-0018_ref_020" ref-type="bibr">20</xref>], which has been considered primarily from a theoretical perspective, “Selection” allows the user to affect the order of images presented to focus on the most promising images instead of the naive sampling, i.e., from the first to the last image of a dataset.</p>
        <p>KaIDA provides multiple selectors for affecting the order of images presented to users.</p>
        <p>The state-of-the-art approach of sampling is depicted in the method of “Sequential Selection”, which sorts images in alphabetic order based on their corresponding file names.</p>
        <p>The method “Cherry Picker” displays an additional user interface to allow a manual selection of samples that should be annotated first, e.g. a diverse subset of all images to train a generalizing DNN with less annotation effort [<xref rid="j_jib-2022-0018_ref_020" ref-type="bibr">20</xref>].</p>
        <p>In addition, using “Random Selection” favors increasing heterogeneity in datasets. The next sample is selected randomly, which is beneficial, especially, in sequential datasets including a large amount of similar images [<xref rid="j_jib-2022-0018_ref_018" ref-type="bibr">18</xref>]. In general, this method is a good compromise in terms of computational effort and generalization performance of a DNN per used annotation.</p>
        <p>Besides, we provide the method of “Heterogeneity Sampling” inspired by deep active learning [<xref rid="j_jib-2022-0018_ref_020" ref-type="bibr">20</xref>]. First, a ResNet [<xref rid="j_jib-2022-0018_ref_021" ref-type="bibr">21</xref>] decoder serves as a feature extractor to avoid the curse of dimensionality when comparing images. Due to using transfer learning on ImageNet, this method needs no additional training on the considered individual dataset, which is beneficial in terms of computational effort. Second, by using a similarity metric, i.e., cosine similarity or L2-norm, all images are compared pairwise. The sampling score solves an optimization problem to determine the most diverse sample given an already considered subset of images.</p>
        <p>However, due to the large computational cost of doing inference w.r.t. all samples of the not annotated dataset, the sampling is saved in an external file. This is advantageous in two aspects: (i) in the case of continuing the annotation process, duplicate computation of scores is avoided, and (ii) sampling can be done on a more powerful device and imported only be by domain experts.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_002_s_002_s_002">
        <label>2.2.2</label>
        <title>Pre-processing</title>
        <p>There are various scenarios where annotating raw images is challenging, i.e., sub-optimal imaging conditions, large images including irrelevant areas, or low pixel-to-object resolution. “Pre-Processing” is advantageous or even necessary for the annotation, but, ordinarily, separate tools such as Fiji/ImageJ [<xref rid="j_jib-2022-0018_ref_022" ref-type="bibr">22</xref>] are utilized. KaIDA integrates pre-processing directly in the annotation pipeline and no separate tools are needed. We provide conventional image processing functions such as image normalization, noise filters, resampling to change the image resolution, or the creation of crops to focus only a region of interest given in an image.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_002_s_002_s_003">
        <label>2.2.3</label>
        <title>Pre-annotation</title>
        <p>Frequently, prior knowledge w.r.t. image annotation exists, but is not used. Users start from scratch when annotating images. The concept of “Pre-Annotation” incorporates available algorithms to provide an initial annotation prediction [<xref rid="j_jib-2022-0018_ref_023" ref-type="bibr">23</xref>] and is integrated into KaIDA. Only a correction of predicted annotations is required. The selection of a suited pre-annotation algorithm depends on the data.</p>
        <p>For semantic segmentation, seed detection, and instance segmentation tasks, we provide traditional image processing algorithms such as thresholding based on Otsu, constant values, or percentiles in histograms of images. Additionally, it is possible to use the same annotation as for the previous image, which is especially useful for related/sequential image data (3D images or time series) when only small changes are assumed between images. In this case, using sequential selection is essential. Moreover, pre-trained DNNs are a method for pre-annotation. We provide the state-of-the-art method Cellpose [<xref rid="j_jib-2022-0018_ref_009" ref-type="bibr">9</xref>] and an implementation of a U-Net [<xref rid="j_jib-2022-0018_ref_024" ref-type="bibr">24</xref>]. Other custom architectures can be integrated into KaIDA due to our extendable concept (cf. <xref rid="j_jib-2022-0018_s_007" ref-type="sec">Supplementary Information</xref>).</p>
      </sec>
      <sec id="j_jib-2022-0018_s_002_s_002_s_004">
        <label>2.2.4</label>
        <title>Image annotation</title>
        <p>The way of “Image Annotation” is not task-agnostic (e.g. pixel-wise annotations vs. annotation per image) whereby software tools often support only a specific image processing task. Hence, KaIDA adapts the input window [<xref rid="j_jib-2022-0018_ref_009" ref-type="bibr">9</xref>, <xref rid="j_jib-2022-0018_ref_011" ref-type="bibr">11</xref>] for “User Input” considering the image processing task given in the meta-information to enable annotation in different image processing tasks.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_002_s_002_s_005">
        <label>2.2.5</label>
        <title>Post-annotation-processing</title>
        <p>Practical projects show that some errors are reoccurring (e.g. holes in segments or unintended small noisy segments) [<xref rid="j_jib-2022-0018_ref_007" ref-type="bibr">7</xref>, <xref rid="j_jib-2022-0018_ref_025" ref-type="bibr">25</xref>]. Post-processing of annotations is meaningful, but often not integrated into annotation tools. Using separate tools for post-processing is cumbersome and may be critical without a supervision by users. Hence, we integrate “Post-Annotation-Processing” to enhance the annotation quality. To ensure traceability, overlaying of raw and post-processed annotation is possible. Hence, the post-processing is monitored and unwanted changes are avoided. Morphological operators, a method to fill holes in segments, or to remove small elements are already implemented in KaIDA. Further, fuzzy annotation in the case of classification is conceivable.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_002_s_002_s_006">
        <label>2.2.6</label>
        <title>Annotation inspection</title>
        <p>The work of Karimi et al. [<xref rid="j_jib-2022-0018_ref_007" ref-type="bibr">7</xref>] addresses the issue of annotator variability leading to noisy annotations. There are methods to handle them [<xref rid="j_jib-2022-0018_ref_007" ref-type="bibr">7</xref>, <xref rid="j_jib-2022-0018_ref_025" ref-type="bibr">25</xref>], i.e., using DNNs for the inspection of annotations, but currently, they are detached from the annotation process. An integration into state-of-the-art annotation tools lacks. KaIDA integrates an “Annotation Inspection” step, which evaluates annotations depending on suited quality criteria. The user selects thresholds to regulate warning appearances. In case of a triggered warning, the annotator is alerted and decides whether to re-inspect or keep the annotation.</p>
        <p>Currently, region proposals can be used to inspect the quality of an annotation. For instance, annotators can use the criterion of area, the number of segments, or convexity based on prior knowledge. Furthermore, the idea of utilizing DNNs for inspection presented in [<xref rid="j_jib-2022-0018_ref_025" ref-type="bibr">25</xref>] could be a further method. Annotations and predictions of uncertainty-aware DNNs are compared. However, it must be noted that using the same DNN in pre-annotation and annotation inspection is not meaningful, since a short circuit is formed in this scenario.</p>
        <p>Finally, the annotation of the input image is obtained, the partially annotated dataset grows continuously, and investigations regarding supervised DL pipelines can be done.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_002_s_002_s_007">
        <label>2.2.7</label>
        <title>Version control</title>
        <p>Annotating datasets is usually an iterative process. The required scope of annotated data is not known <italic toggle="yes">a priori</italic> [<xref rid="j_jib-2022-0018_ref_026" ref-type="bibr">26</xref>] or a domain shift in images degrades DNN performance [<xref rid="j_jib-2022-0018_ref_027" ref-type="bibr">27</xref>] and requires newly annotated samples. Besides, a changed annotation policy or correcting erroneous annotations are scenarios where the dataset changes within a project. Hence, datasets may develop during designing a DL processing pipeline, but tracking dataset versions in annotation tools lacks. We incorporate the ability of dataset “Version Control” using the ideas of [<xref rid="j_jib-2022-0018_ref_028" ref-type="bibr">28</xref>] for handling large files via meta-files located in Git version control. Thus, KaIDA allows analyzing and tracking the history of datasets. Users can roll back to all available dataset versions. It is beneficial to track the changes over time, i.e., comparing DNN results using different dataset versions to check learning curves or the influence of a changing annotation policy. In addition, the dataset version control supports the usage of data servers, which is beneficial in several ways (transfer of data to computing clusters for DL training, data backup, or interaction of data scientists/domain experts).</p>
      </sec>
      <sec id="j_jib-2022-0018_s_002_s_002_s_008">
        <label>2.2.8</label>
        <title>Plugins</title>
        <p>Dealing with image annotation, additional features are helpful for users. Hence, we created an interface to hand over the data and functionalities of KaIDA to tool extensions referred to as “Plugins”.</p>
        <p>Dealing with large high-resolution images, i.e., thousands of cell nuclei [<xref rid="j_jib-2022-0018_ref_003" ref-type="bibr">3</xref>] or hundreds of insects, image cropping is meaningful in two aspects. First, annotating smaller images is more comfortable for users. Second, by using crops of images, the GPU memory requirements during training are reduced. However, the crop functionality is often done by a separate implementation and not integrated into state-of-the-art annotation tools. Hence, we offer an image crop plugin to create fragments of large images without the requirement of a separate solution.</p>
        <p>It is difficult for domain experts to directly use standard DNN architectures such as U-Net [<xref rid="j_jib-2022-0018_ref_024" ref-type="bibr">24</xref>] since they are only available as source code. An application plugin directly allows the usage of trained DNNs for individual projects to process the data of experimenters. Hence, the processing can be done by the domain experts themselves with no need for a request to data scientists regarding the processing. Though, DNN training or inference require high-performance computing resources, which are in the normal case not available for domain experts. By considering our proposal of the practical setup in <xref rid="j_jib-2022-0018_s_007" ref-type="sec">Supplementary Information</xref>, a solution to solve this limitation is available. To process large-scale data, we support the integration of REST API in order to separate processing requests and the execution on powerful devices. State-of-the-art DNNs can be containerized (e.g. by using Docker images) and deployed on a high-performance computing cluster for the application. In this case, data needs only to be transmitted, but computation can be outsourced.</p>
      </sec>
    </sec>
    <sec id="j_jib-2022-0018_s_002_s_003">
      <label>2.3</label>
      <title>Software development and implementation</title>
      <sec id="j_jib-2022-0018_s_002_s_003_s_001">
        <label>2.3.1</label>
        <title>General</title>
        <p>KaIDA is available open-source for download/installation at <ext-link xlink:href="https://git.scc.kit.edu/sc1357/kaida" ext-link-type="uri">https://git.scc.kit.edu/sc1357/kaida</ext-link>. State-of-the-art software tools are mainly suited for a special image processing and extensions are not intended. To overcome this issue, KaIDA has been developed on a modular level and is agnostic w.r.t. the underlying image processing task or the used methods in the modules. Since the forms of assistance are strongly related to the practical project, extensions or customization can be integrated into KaIDA due to its generic structure to ensure universal applicability. The current implementation supports the image processing tasks of (i) classification, (ii) semantic segmentation, (iii) instance segmentation, and (iv) seed detection, but an extension for other tasks is possible due to its generic software concept. Further information and details regarding the software development concept, the implementation, and its integration in future laboratories are given in <xref rid="j_jib-2022-0018_s_007" ref-type="sec">Supplementary Information</xref>.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_002_s_003_s_002">
        <label>2.3.2</label>
        <title>Tutorial</title>
        <p>Taking software tools into account, uncomplicated operability for users with different backgrounds or levels of experience is important. To simplify the usage of KaIDA, we provide tutorial datasets for each image processing task. The dummy datasets are available at <ext-link xlink:href="https://osf.io/5zcye/" ext-link-type="uri">https://osf.io/5zcye/</ext-link>and given in <xref rid="j_jib-2022-0018_fig_002" ref-type="fig">Figure 2</xref>. Hence, users can test KaIDA and its functionalities directly. In addition, a manual to present the basic usage of KaIDA is presented in the open-source repository in form of a README file. However, the explanation of a tool by means of a video is often more beneficial for users. Thus, in addition, we provide a video per image processing tasks using the tutorial datasets to further lower the hurdles for new users for using KaIDA to annotate images. Thus, the whole workflow from initializing a new project to annotating exemplary samples is shown. All videos can be downloaded at <ext-link xlink:href="https://osf.io/5zcye/" ext-link-type="uri">https://osf.io/5zcye/</ext-link>.</p>
        <fig position="float" id="j_jib-2022-0018_fig_002" fig-type="figure">
          <label>Figure 2:</label>
          <caption>
            <p>Overview tutorial datasets. Tutorial datasets are provided to facilitate the entry into the use of KaIDA. For instance, the classification (a) of not infected lungs (a, first row)/infected lungs (a, second row), the semantic segmentation (b) of organs (heart, lung, or kidney), and instance segmentation (c)/seed detection (d) of bees. The results of annotation regarding the tasks (b–d) are masks (second row). Whilst each color of the semantic mask represents a different class (b, second row), the colors of the mask resulting from instance segmentation represent a different instance of the same class (c, second row). In the case of seed detection, the intention is only to count instances and not to completely segment them. Hence, only the head of each bee is annotated (d, second row).</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_002" position="float"/>
        </fig>
      </sec>
      <sec id="j_jib-2022-0018_s_002_s_003_s_003">
        <label>2.3.3</label>
        <title>Usability</title>
        <p>For proposing a software tool, it is important to consider and evaluate usability. We refer to the criteria introduced in the high-throughput image processing tool Grid Screener [<xref rid="j_jib-2022-0018_ref_029" ref-type="bibr">29</xref>] to investigate the usability of KaIDA. The criteria are given in <xref rid="j_jib-2022-0018_fig_003" ref-type="fig">Figure 3</xref>. The authors name accessibility, and requirements regarding software and hardware as being relevant for the usability of a tool. Available user manuals and GUIs enhance the usage of tools. Further, modular expandability is beneficial for the application of tools in a broader sense, e.g. for other applications. An evaluation of KaIDA regarding those usability criteria is given in <xref rid="j_jib-2022-0018_s_003_s_002" ref-type="sec">Section 3.2</xref>.</p>
        <fig position="float" id="j_jib-2022-0018_fig_003" fig-type="figure">
          <label>Figure 3:</label>
          <caption>
            <p>Criteria usability study. The authors of Grid Screener [<xref rid="j_jib-2022-0018_ref_029" ref-type="bibr">29</xref>] enumerate accessibility, expandability, software/hardware requirements, an available GUI, and user manual as criteria for the evaluation of a software tool in terms of usability.</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_003" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="j_jib-2022-0018_s_002_s_004">
      <label>2.4</label>
      <title>Scheme for comparing annotation tools</title>
      <p>A comparison of annotation tools is helpful for researchers in selecting an appropriate software tool for their DL project. Evaluation criteria are required for comparison. We consider different aspects in terms of functionality, methods, and usability. For clarity, we have summarized the criteria related to usability (cf. <xref rid="j_jib-2022-0018_s_002_s_003_s_003" ref-type="sec">Section 2.3.3</xref>) to the ability towards customizing/extending a tool and the direct usability to start annotation with little effort when using a tool the first time. In addition, we consider whether the tool is non-commercial, supports different image processing tasks, provide a framework for training/inference step (cf. model application), and integrates data version control. Besides, the criteria that directly influence the process of image annotation are taken into account: functionality to influence the order of the presented samples (cf. selection), image pre-processing, pre-annotation, post-annotation-processing, and annotation inspection. <xref rid="j_jib-2022-0018_fig_004" ref-type="fig">Figure 4</xref> presents an overview of all introduced criteria.</p>
      <fig position="float" id="j_jib-2022-0018_fig_004" fig-type="figure">
        <label>Figure 4:</label>
        <caption>
          <p>Scheme for comparing annotation tools. For the comparison of state-of-the-art image annotation tools, different criteria are used to consider different aspects.</p>
        </caption>
        <graphic xlink:href="j_jib-2022-0018_fig_004" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec id="j_jib-2022-0018_s_003">
    <label>3</label>
    <title>Application</title>
    <sec id="j_jib-2022-0018_s_004_s_001">
      <label>3.1</label>
      <title>Modules</title>
      <p>To investigate our proposed concept, results of the introduced modules are investigated in the following. Since our contribution KaIDA is a software tool, this is done by using images from different application cases.</p>
      <sec id="j_jib-2022-0018_s_003_s_001_s_001">
        <label>3.1.1</label>
        <title>Selection</title>
        <p><xref rid="j_jib-2022-0018_fig_005" ref-type="fig">Figure 5</xref> illustrates the method “Cherry Picker” using the tutorial dataset. Users can select the most promising samples via a GUI. Those images would be presented to the annotator first during image annotation.</p>
        <fig position="float" id="j_jib-2022-0018_fig_005" fig-type="figure">
          <label>Figure 5:</label>
          <caption>
            <p>GUI cherry picker. KaIDA shows a GUI for cherry-picking images of the human brain atlas [<xref rid="j_jib-2022-0018_ref_030" ref-type="bibr">30</xref>] via user input. The selected images would appear first during the annotation process.</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_005" position="float"/>
        </fig>
        <p>Furthermore, the results of “Heterogeneity Sampling” are taken into consideration. <xref rid="j_jib-2022-0018_fig_006" ref-type="fig">Figure 6</xref> illustrates exemplary results of obtaining similar and dissimilar images given a reference image w.r.t. the ISIC 2017 Melanoma image segmentation dataset [<xref rid="j_jib-2022-0018_ref_016" ref-type="bibr">16</xref>]. A visual comparison shows that using ResNet [<xref rid="j_jib-2022-0018_ref_021" ref-type="bibr">21</xref>] for feature extraction is a feasible way to compare images regarding similarity.</p>
        <fig position="float" id="j_jib-2022-0018_fig_006" fig-type="figure">
          <label>Figure 6:</label>
          <caption>
            <p>Visual validation heterogeneity sampling. Exemplary results of the heterogeneity sampler are presented. Considering a reference image of the ISIC 2017 Melanoma image segmentation dataset [<xref rid="j_jib-2022-0018_ref_016" ref-type="bibr">16</xref>], the top-4 similar and top-4 dissimilar images are opposed. It is visible that the sampler is capable of distinguishing similar and dissimilar images using a pre-trained ResNet [<xref rid="j_jib-2022-0018_ref_021" ref-type="bibr">21</xref>] decoder for feature extraction.</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_006" position="float"/>
        </fig>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_001_s_002">
        <label>3.1.2</label>
        <title>Pre-processing</title>
        <p>To present the functionality of pre-processing, examples are given in <xref rid="j_jib-2022-0018_fig_007" ref-type="fig">Figure 7</xref>. It shows impressively the advantage of pre-processing. Using image normalization, noise filtering, or extracted crops simplifies the annotation for users.</p>
        <fig position="float" id="j_jib-2022-0018_fig_007" fig-type="figure">
          <label>Figure 7:</label>
          <caption>
            <p>Illustration of methods in pre-processing. Exemplary results of normalization, noise filter, or crop are presented.</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_007" position="float"/>
        </fig>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_001_s_003">
        <label>3.1.3</label>
        <title>Pre-annotation</title>
        <p><xref rid="j_jib-2022-0018_fig_008" ref-type="fig">Figure 8</xref> illustrates examples of different pre-annotation methods already implemented in KaIDA. The pre-annotations are not correct in total. Nevertheless, the advantage of pre-annotation becomes clear, as it is easier to correct annotations than to start from scratch, i.e., dealing with numerous cell instances.</p>
        <fig position="float" id="j_jib-2022-0018_fig_008" fig-type="figure">
          <label>Figure 8:</label>
          <caption>
            <p>Illustration of methods in pre-annotation. Exemplary [<xref rid="j_jib-2022-0018_ref_003" ref-type="bibr">3</xref>, <xref rid="j_jib-2022-0018_ref_005" ref-type="bibr">5</xref>, <xref rid="j_jib-2022-0018_ref_016" ref-type="bibr">16</xref>, <xref rid="j_jib-2022-0018_ref_017" ref-type="bibr">17</xref>] results of pre-annotations computed by Cellpose [<xref rid="j_jib-2022-0018_ref_009" ref-type="bibr">9</xref>], Otsu, and U-Net [<xref rid="j_jib-2022-0018_ref_024" ref-type="bibr">24</xref>]. Few errors remain in the predictions, but the visualization clearly shows that correcting annotations is easier than annotating from scratch.</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_008" position="float"/>
        </fig>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_001_s_004">
        <label>3.1.4</label>
        <title>Post-annotation-processing</title>
        <p>The corresponding GUI of “Post-Annotation-Processing” is depicted in <xref rid="j_jib-2022-0018_fig_009" ref-type="fig">Figure 9</xref>. A comparison between user annotation and post-processed annotation is displayed to ensure traceability. The user can choose which one to keep. We present an example of removing small objects in the case of instance segmentation (cf. <xref rid="j_jib-2022-0018_fig_009" ref-type="fig">Figure 9a</xref>). Further, fuzzy annotation in classification tasks is shown in <xref rid="j_jib-2022-0018_fig_009" ref-type="fig">Figure 9b</xref>.</p>
        <fig position="float" id="j_jib-2022-0018_fig_009" fig-type="figure">
          <label>Figure 9:</label>
          <caption>
            <p>Illustration of methods in post-annotation-processing. Exemplary results of post-annotation processing. (a) Instance Segmentation. The method of removing small objects is displayed. (b) Classification. The exemplary method for fuzzy annotation is shown for a synthetic image illustrating cell mitosis.The sharp label of telophase is transformed into a mixture of telophase and anaphase. Hence, fuzzy annotation can beused in ambiguous cases.</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_009" position="float"/>
        </fig>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_001_s_005">
        <label>3.1.5</label>
        <title>Annotation inspection</title>
        <p><xref rid="j_jib-2022-0018_fig_010" ref-type="fig">Figure 10</xref> shows a scenario in which an inspection warning is triggered considering melanoma segmentation. It is assumed that only a single segment is visible per image. Since there are two marked segments, the user can choose to keep the annotation or to re-label the sample.</p>
        <fig position="float" id="j_jib-2022-0018_fig_010" fig-type="figure">
          <label>Figure 10:</label>
          <caption>
            <p>Illustration of methods in annotation inspection. Exemplary results of using the number of segments as an annotation quality criterion. Assuming only one melanoma per image, a warning is shown because two segments are marked.</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_010" position="float"/>
        </fig>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_001_s_006">
        <label>3.1.6</label>
        <title>Version control</title>
        <p><xref rid="j_jib-2022-0018_fig_011" ref-type="fig">Figure 11</xref> illustrates the GUI in the context of “Version Control”. Users receive feedback in terms of changes between different dataset version and may check out them in the current working directory.</p>
        <fig position="float" id="j_jib-2022-0018_fig_011" fig-type="figure">
          <label>Figure 11:</label>
          <caption>
            <p>Feedback of data version control to users. Changes in the dataset are listed to give feedback to users regarding different dataset versions.</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_011" position="float"/>
        </fig>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_001_s_007">
        <label>3.1.7</label>
        <title>Plugins</title>
        <p>Taking plugins into account, the benefit of generating image crops is demonstrated in <xref rid="j_jib-2022-0018_fig_012" ref-type="fig">Figure 12</xref>. Comparing the original image (a) and the image crop (b), annotators benefit from increased clarity within a considered crop in the case of many visible instances.</p>
        <fig position="float" id="j_jib-2022-0018_fig_012" fig-type="figure">
          <label>Figure 12:</label>
          <caption>
            <p>Illustration of image cropping. A crop (b) of an original image (a) is generated [<xref rid="j_jib-2022-0018_ref_005" ref-type="bibr">5</xref>]. Using this example, it becomes clear that generating crops can simplify annotation in the case of many instances given within a sample. (a) Original Image. (b) Cropped Image.</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_012" position="float"/>
        </fig>
        <p>KaIDA supports the integration of requests via REST API. <xref rid="j_jib-2022-0018_fig_013" ref-type="fig">Figure 13</xref> illustrates an interface for the submission of a computation job via web requests nested in KaIDA. This allows the domain expert to apply trained DNNs/entire image processing pipelines directly and take the advantage of computing clusters. Thereby, it is only necessary to select the files intended for processing.</p>
        <fig position="float" id="j_jib-2022-0018_fig_013" fig-type="figure">
          <label>Figure 13:</label>
          <caption>
            <p>DNN application. User interface is shown for submitting a job to a computing server via REST API in order to apply image processing pipelines including a DNN.</p>
          </caption>
          <graphic xlink:href="j_jib-2022-0018_fig_013" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec id="j_jib-2022-0018_s_003_s_002">
      <label>3.2</label>
      <title>Usability</title>
      <p>The results of the usability study are shown in <xref rid="j_jib-2022-0018_tab_001" ref-type="table">Table 1</xref>. KaIDA is available as an open-source project in a public repository which leads to full target achievement w.r.t. accessibility. By providing a user manual in form of a README file, tutorial datasets, and corresponding tutorial videos, the criterion of an available user manual is fulfilled completely. Furthermore, we provide a GUI to enhance the usage of our proposed tool. Deploying the software in python as pip package and providing the corresponding software dependencies via a conda environment, software requirements are low and installation can be done comfortably by users. However, basic computer skills are required leading to a ranking of partial target achievement. To assist in less computing time, a GPU or high-performance CPU is advantageous. However, KaIDA can be used by a non-powerful CPU as well, yet, leading to longer computational time during assistance depending on the used methods. Thus, the criterion of low hardware requirements is not fulfilled completely. By using the comprehensive practical setup for future laboratories presented in <xref rid="j_jib-2022-0018_s_007" ref-type="sec">Supplementary Information</xref> or REST API approaches, the previously mentioned objections, which lead to no full target achievement, can be eliminated. Thus, full target achievement is possible. As depicted in <xref rid="j_jib-2022-0018_s_002_s_003" ref-type="sec">Section 2.3</xref>, KaIDA focuses on the objective of a modular and generic tool. Users can customize and extend the software tool since we provide the open-source repository including explanations regarding ways of extension. However, the users need basic coding knowledge in python. Therefore, no full target achievement in terms of expandability can be held.</p>
      <table-wrap position="float" id="j_jib-2022-0018_tab_001">
        <label>Table 1:</label>
        <caption>
          <p>Usability study. The introduced criteria in grid screener [<xref rid="j_jib-2022-0018_ref_029" ref-type="bibr">29</xref>] are evaluated. Thereby, “<italic toggle="yes">✓</italic>” indicates full, “(<italic toggle="yes">✓</italic>)” partial, and “<italic toggle="yes">x</italic>” no target achievement.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
            <col align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Criterion</th>
              <th rowspan="1" colspan="1">Evaluation result</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Accessibility</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">User manual</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">GUI</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Software requirements/Installation</td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Hardware requirements</td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Expandability</td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>In total, we can demonstrate the usability of KaIDA for researchers from various backgrounds. The hypothesis is supported by fully satisfying three criteria (five criteria using the proposal given in <xref rid="j_jib-2022-0018_s_007" ref-type="sec">Supplementary Information</xref> or computation via REST API) and partially satisfying three criteria (one criterion following the concept in <xref rid="j_jib-2022-0018_s_007" ref-type="sec">Supplementary Information</xref>/REST API approach).</p>
    </sec>
    <sec id="j_jib-2022-0018_s_003_s_003">
      <label>3.3</label>
      <title>Comparison of annotation tools</title>
      <p>Taking the introduced criteria in <xref rid="j_jib-2022-0018_s_002_s_004" ref-type="sec">Section 2.4</xref> into account, we provide a comprehensive comparison of the state-of-the art image annotation tools, i.e., Image Labeling Tool [<xref rid="j_jib-2022-0018_ref_011" ref-type="bibr">11</xref>], labelMe [<xref rid="j_jib-2022-0018_ref_013" ref-type="bibr">13</xref>], LabelImg [<xref rid="j_jib-2022-0018_ref_012" ref-type="bibr">12</xref>], Cellpose [<xref rid="j_jib-2022-0018_ref_009" ref-type="bibr">9</xref>], CVAT [<xref rid="j_jib-2022-0018_ref_010" ref-type="bibr">10</xref>], hasty.ai [<xref rid="j_jib-2022-0018_ref_014" ref-type="bibr">14</xref>], and our proposal KaIDA. The result are given in <xref rid="j_jib-2022-0018_tab_002" ref-type="table">Table 2</xref> and are presented in detail below.</p>
      <table-wrap position="float" id="j_jib-2022-0018_tab_002">
        <label>Table 2:</label>
        <caption>
          <p>Comparison of image annotation tools. A detailed comparison of different image annotation tools w.r.t. the introduced evaluation criteria is presented. Used notation: “<italic toggle="yes">✓</italic>” full, “(<italic toggle="yes">✓</italic>)” partial, and “<italic toggle="yes">x</italic>” no target attainment.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
            <col align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Criterion</th>
              <th colspan="7" rowspan="1">Tools</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1"><bold>Image Labeling Tool</bold> [<xref rid="j_jib-2022-0018_ref_011" ref-type="bibr">11</xref>]</td>
              <td rowspan="1" colspan="1"><bold>labelMe</bold> [<xref rid="j_jib-2022-0018_ref_013" ref-type="bibr">13</xref>]</td>
              <td rowspan="1" colspan="1"><bold>LabelImg</bold> [<xref rid="j_jib-2022-0018_ref_012" ref-type="bibr">12</xref>]</td>
              <td rowspan="1" colspan="1"><bold>Cellpose</bold> [<xref rid="j_jib-2022-0018_ref_009" ref-type="bibr">9</xref>]</td>
              <td rowspan="1" colspan="1"><bold>CVAT</bold> [<xref rid="j_jib-2022-0018_ref_010" ref-type="bibr">10</xref>]</td>
              <td rowspan="1" colspan="1"><bold>hasty</bold>.<bold>ai</bold> [<xref rid="j_jib-2022-0018_ref_014" ref-type="bibr">14</xref>]</td>
              <td rowspan="1" colspan="1"><bold>KaIDA</bold> (<bold>Ours</bold>)</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Selection</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pre-annotation</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Pre-processing</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Post-annotation-processing</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Annotation inspection</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Different tasks</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Customization &amp; expandability</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Data version control</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Non-commercial</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Model application</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">x</italic>
              </td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Direct usability</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">✓</italic>
              </td>
              <td rowspan="1" colspan="1">(<italic toggle="yes">✓</italic>)</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <sec id="j_jib-2022-0018_s_003_s_003_s_001">
        <label>3.3.1</label>
        <title>Selection</title>
        <p>KaIDA is the only tool that allows a change the order of sampling during annotation to focus on the most relevant samples given a limited time budget.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_003_s_002">
        <label>3.3.2</label>
        <title>Pre-annotation</title>
        <p>The lightweight tools Image Labeling Tool, labelMe, LabelImg do not support pre-annotation natively. In contrast, the other annotation tools (Cellpose, CVAT, hasty.ai, KaIDA) allow the integration of functions to provide users with inital annotations.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_003_s_003">
        <label>3.3.3</label>
        <title>Pre-processing</title>
        <p>CVAT offers an integration of OpenCV for the pre-processing of images. In parallel, KaIDA enables the pre-processing of raw images. The other tools take image pre-processing not into account.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_003_s_004">
        <label>3.3.4</label>
        <title>Post-annotation-processing</title>
        <p>The idea of directly post-processing annotation based on prior knowledge is a concept unique to KaiDA.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_003_s_005">
        <label>3.3.5</label>
        <title>Annotation inspection</title>
        <p>CVAT supports the idea of manually reviewing annotations. LabelImg integrates a functionality to mark inspected annotations with a flag. However, automated annotation inspection approaches are only given in hasty.ai and KaIDA.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_003_s_006">
        <label>3.3.6</label>
        <title>Different tasks</title>
        <p>Only Image Labeling Tool (semantic segmentation) and LabelImg (object detection) focus on a single task. Cellpose partially meets the criterion as it only considers instance segmentation but in 2D and 3D modality. The other tools support different image processing tasks.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_003_s_007">
        <label>3.3.7</label>
        <title>Customization &amp; expandability</title>
        <p>CVAT allows to integrate user-defined models for pre-annotation. Further, hasty.ai offers an Application Programming Interface (API) which allows to customize parts of the elements, i.e., switching to a local custom model for pre-annotation. However, KaIDA is the only tools which focus on customization and expandability of methods in every module leading to more flexibility.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_003_s_008">
        <label>3.3.8</label>
        <title>Data version control</title>
        <p>The concept of data version control is only available in KaIDA.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_003_s_009">
        <label>3.3.9</label>
        <title>Non-commercial</title>
        <p>Except hasty.ai, all other tools are non-commercial.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_003_s_010">
        <label>3.3.10</label>
        <title>Model application</title>
        <p>Lightweight tools such as Image Labeling Tool, labelMe, LabelImg are not designed to directly generate a model using the annotated dataset. Cellpose does not support the training of DNNs via a GUI, but provides an API to train and use DNNs. CVAT, hasty.ai, and KaIDA integrate a functionality for application of DL.</p>
      </sec>
      <sec id="j_jib-2022-0018_s_003_s_003_s_011">
        <label>3.3.11</label>
        <title>Direct usability</title>
        <p>Image Labeling Tool and labelMe provide executables for various operating systems. Partially target fulfillment can only be stated in the case of LabelImg (executable only for Windows, manual installation for others) and Cellpose/KaIDA (manual installation). CVAT and hasty.ai can be used directly thanks to a web interface.</p>
      </sec>
    </sec>
  </sec>
  <sec id="j_jib-2022-0018_s_004">
    <label>4</label>
    <title>Discussion</title>
    <p>The results given in <xref rid="j_jib-2022-0018_s_003" ref-type="sec">Section 3</xref> show that there are several ways to expand the state-of-the-art annotation process. We present the functionalities in terms of different example dataset. Improvement and benefits regarding efficiency and quality of annotations for annotators by using KaIDA are demonstrated in contrast to the state of the art “no support”. By using practical examples, the relevance of presented methods and their applicability for users is demonstrated.</p>
    <p>In addition, the results show that the support strongly depends on the particular dataset, which means that the methods cannot be used in every project. The ability to disable modules or extend them with custom implementations can address this issue. However, basic coding knowledge is required to integrate custom methods. Further, when using elaborate methods in the assisted annotation process, powerful computers are required to avoid long computation times if the proposed setup (cf. <xref rid="j_jib-2022-0018_s_007" ref-type="sec">Supporting Information</xref>) is not considered.</p>
    <p>By comparing KaIDA with state-of-the-art image annotation tools, we show that KaIDA bundles various aspects of annotation improvement. Most state-of-the-art tools only consider parts of the assistance portfolio and/or are commercial. However, a limitation of KaIDA is the lack of a web interface/executable to further reduce the installation effort if the suggested practical server setup (cf. <xref rid="j_jib-2022-0018_s_007" ref-type="sec">Supporting Information</xref>) is not used.</p>
  </sec>
  <sec id="j_jib-2022-0018_s_005">
    <label>5</label>
    <title>Conclusions</title>
    <p>With KaIDA, we contribute to the research community a ready-for-use software tool for assisted image annotation. The advantages show up in (i) efficient, simplified, and high-quality annotations, (ii) customization/expandability, and (iii) additional features. The enhancement of image annotation boosts supervised DL approaches in biomedical image processing, but is not limited to this case of application. Current research considers implementation of further methods per module in KaIDA as well as the integration towards 3D image annotation. Moreover, the creation of a web interface/executable is currently in preparation.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material position="float" content-type="local-data">
      <caption>
        <p>Supplementary Material Details</p>
      </caption>
      <media xlink:href="j_jib-2022-0018_suppl.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="j_jib-2022-0018_ack_001">
    <title>Acknowledgments</title>
    <p>We acknowledge Kristen Rottmann and Lorenz Wührl for their help in preparing images of the practical setup. Furthermore, we thank the authors of Cellpose [<xref rid="j_jib-2022-0018_ref_009" ref-type="bibr">9</xref>] and Image Labeling Tool [<xref rid="j_jib-2022-0018_ref_011" ref-type="bibr">11</xref>] for providing a backbone user interface which can be used in a modified way for user annotation in KaIDA. Moreover, we would like to thank the group “Scientific-technical Infrastructure”, in particular, Daniel Bacher and Jürgen Engelmann, for their support in setting up the proposed practical setup in terms of a processing server. We acknowledge support by the KIT-Publication Fund of the Karlsruhe Institute of Technology.</p>
  </ack>
  <fn-group>
    <fn fn-type="con" id="j_jib-2022-0018_fn_001">
      <p><bold>Author contributions:</bold> Marcel P. Schilling (MPS), Svenja Schmelzer (SS), Lukas Klinger (LK), and Markus Reischl (MR), Conceptualization: MPS, MR, Formal analysis: MPS, MR, Funding acquisition: MR, Investigation: MPS, SS, LK, Methodology: MPS, MR, SS, LK, Project administration: MR, Software: MPS, SS, LK, Supervision: MR, Writing – original draft: MPS, SS, MR, Writing – review and editing: MPS, MR, SS, LK.</p>
    </fn>
    <fn id="j_jib-2022-0018_fn_002" fn-type="financial-disclosure">
      <p><bold>Research funding:</bold> This work was funded by the KIT Future Fields II Project “Screening Platform for Personalized Oncology (SPPO)”. This work was supported in part by the HoreKa Supercomputer through the Ministry of Science, Research, and the Arts Baden-Württemberg, in part by the Federal Ministry of Education and Research, the Helmholtz Association Initiative and Networking Fund on the HAICORE@KIT partition, and in part by the KIT-Publication Fund of the Karlsruhe Institute of Technology.</p>
    </fn>
    <fn fn-type="other" id="j_jib-2022-0018_fn_003">
      <p><bold>Conflict of interest statement:</bold> Authors state no conflict of interest. All authors have read the journal’s Publication ethics and publication malpractice statement available at the journal’s website and hereby confirm that they comply with all its parts applicable to the present scientific work.</p>
    </fn>
  </fn-group>
  <ref-list id="j_jib-2022-0018_reflist_001">
    <title>References</title>
    <ref id="j_jib-2022-0018_ref_001">
      <label>1.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mahony</surname>
            <given-names>NO</given-names>
          </name>
          <name>
            <surname>Campbell</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Carvalho</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Harapanahalli</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Velasco-Hernández</surname>
            <given-names>GA</given-names>
          </name>
          <name>
            <surname>Krpalkova</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning vs. traditional computer vision</article-title>
        <source>Advances in computer vision</source>
        <publisher-loc>Cham</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <year>2019</year>
        <fpage>128</fpage>
        <lpage>44</lpage>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_002">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Böhland</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tharun</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Scherr</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Mikut</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Hagenmeyer</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thompson</surname>
            <given-names>LDR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Machine learning methods for automated classification of tumors with papillary thyroid carcinoma-like nuclei: a quantitative analysis</article-title>
        <source>PLoS One</source>
        <year>2021</year>
        <volume>16</volume>
        <fpage>1</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="doi">10.1371/journal.pone.0257635</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_003">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Caicedo</surname>
            <given-names>JC</given-names>
          </name>
          <name>
            <surname>Goodman</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Karhohs</surname>
            <given-names>KW</given-names>
          </name>
          <name>
            <surname>Cimini</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Ackerman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Haghighi</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Nucleus segmentation across imaging experiments: the 2018 data science bowl</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>1247</fpage>
        <lpage>53</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0612-7</pub-id>
        <pub-id pub-id-type="pmid">31636459</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_004">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schutera</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Just</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gierten</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Mikut</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Reischl</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pylatiuk</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Machine learning methods for automated quantification of ventricular dimensions</article-title>
        <source>Zebrafish</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>542</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1089/zeb.2019.1754</pub-id>
        <pub-id pub-id-type="pmid">31536467</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_005">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wührl</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Pylatiuk</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Giersch</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lapp</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>von Rintelen</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Balke</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Diversity scanner: robotic handling of small invertebrates with machine learning methods</article-title>
        <source>Mol Ecol Resour</source>
        <year>2021</year>
        <volume>00</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1111/1755-0998.13567</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_006">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chi</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Gu</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Deep learning-based medical image segmentation with limited labels</article-title>
        <source>Phys Med Biol</source>
        <year>2020</year>
        <volume>65</volume>
        <fpage>235001</fpage>
        <pub-id pub-id-type="doi">10.1088/1361-6560/abc363</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_007">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Karimi</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Dou</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Warfield</surname>
            <given-names>SK</given-names>
          </name>
          <name>
            <surname>Gholipour</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Deep learning with noisy labels: exploring techniques and remedies in medical image analysis</article-title>
        <source>Med Image Anal</source>
        <year>2020</year>
        <volume>65</volume>
        <fpage>101759</fpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2020.101759</pub-id>
        <pub-id pub-id-type="pmid">32623277</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_008">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gómez-de-Mariscal</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>García-López-de-Haro</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ouyang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Donati</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lundberg</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Unser</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepImageJ: a user-friendly environment to run deep learning models in ImageJ</article-title>
        <source>Nat Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>1192</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-021-01262-9</pub-id>
        <pub-id pub-id-type="pmid">34594030</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_009">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stringer</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Michaelos</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pachitariu</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title>
        <source>Nat Methods</source>
        <year>2021</year>
        <volume>18</volume>
        <fpage>100</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id>
        <pub-id pub-id-type="pmid">33318659</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_010">
      <label>10.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Sekachev</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Manovich</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Zhiltsov</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhavoronkov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kalinin</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Hoff</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <source>Computer vision annotation tool (CVAT)</source>
        <year>2020</year>
        <comment>Available from</comment>
        <ext-link xlink:href="https://github.com/openvinotoolkit/cvat" ext-link-type="uri">https://github.com/openvinotoolkit/cvat</ext-link>
        <comment>Accessed</comment>
        <date-in-citation content-type="access">25 Oct 2021</date-in-citation>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_011">
      <label>11.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Bartschat</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Image labeling tool</source>
        <year>2019</year>
        <comment>Available from</comment>
        <ext-link xlink:href="https://bitbucket.org/abartschat/imagelabelingtool" ext-link-type="uri">https://bitbucket.org/abartschat/imagelabelingtool</ext-link>
        <comment>Accessed</comment>
        <date-in-citation content-type="access">21 Oct 2021</date-in-citation>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_012">
      <label>12.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <source>LabelImg</source>
        <year>2015</year>
        <comment>Available from</comment>
        <ext-link xlink:href="https://github.com/tzutalin/labelImg" ext-link-type="uri">https://github.com/tzutalin/labelImg</ext-link>
        <comment>Accessed</comment>
        <date-in-citation content-type="access">18 May 2022</date-in-citation>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_013">
      <label>13.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Wada</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <source>labelme: image polygonal annotation with python</source>
        <year>2018</year>
        <comment>Available from</comment>
        <ext-link xlink:href="https://github.com/wkentaro/labelme" ext-link-type="uri">https://github.com/wkentaro/labelme</ext-link>
        <comment>Accessed</comment>
        <date-in-citation content-type="access">18 May 2022</date-in-citation>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_014">
      <label>14.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Rouillard</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Proskudin</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Wennman</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Hasty.ai</source>
        <year>2022</year>
        <comment>Available from</comment>
        <ext-link xlink:href="https://hasty.ai" ext-link-type="uri">https://hasty.ai</ext-link>
        <comment>Accessed</comment>
        <date-in-citation content-type="access">18 May 2022</date-in-citation>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_015">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Hofestädt</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Taubert</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Integrative bioinformatics: history and future</article-title>
        <source>J Integr Bioinform</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>20192001</fpage>
        <pub-id pub-id-type="doi">10.1515/jib-2019-2001</pub-id>
        <pub-id pub-id-type="pmid">31560650</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_016">
      <label>16.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Codella</surname>
            <given-names>NCF</given-names>
          </name>
          <name>
            <surname>Gutman</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Celebi</surname>
            <given-names>ME</given-names>
          </name>
          <name>
            <surname>Helba</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Marchetti</surname>
            <given-names>MA</given-names>
          </name>
          <name>
            <surname>Dusza</surname>
            <given-names>SW</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (ISBI)</article-title>
        <source>IEEE international symposium on biomedical imaging</source>
        <year>2018</year>
        <fpage>168</fpage>
        <lpage>72</lpage>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_017">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Popova</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Tronser</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Demir</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Haitz</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Kuodyte</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Starkuviene</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Facile one step formation and screening of tumor spheroids using droplet-microarray platform</article-title>
        <source>Small</source>
        <year>2019</year>
        <volume>15</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1002/smll.201901299</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_018">
      <label>18.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Schilling</surname>
            <given-names>MP</given-names>
          </name>
          <name>
            <surname>Rettenberger</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Münke</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Cui</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Popova</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Levkin</surname>
            <given-names>PA</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Label assistant: a workflow for assisted data annotation in image segmentation tasks</article-title>
        <source>Proceedings – 31. Workshop computational intelligence</source>
        <year>2021</year>
        <fpage>211</fpage>
        <lpage>34</lpage>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_019">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wilkinson</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Dumontier</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Aalbersberg</surname>
            <given-names>IJ</given-names>
          </name>
          <name>
            <surname>Appleton</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Axton</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Baak</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>The FAIR guiding principles for scientific data management and stewardship</article-title>
        <source>Sci Data</source>
        <year>2016</year>
        <volume>3</volume>
        <fpage>160018</fpage>
        <pub-id pub-id-type="doi">10.1038/sdata.2016.18</pub-id>
        <pub-id pub-id-type="pmid">26978244</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_020">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ren</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Xiao</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Chang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>PY</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>BB</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A survey of deep active learning</article-title>
        <source>ACM Comput Surv</source>
        <year>2021</year>
        <volume>54</volume>
        <fpage>1</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1145/3472291</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_021">
      <label>21.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>He</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Ren</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Deep residual learning for image recognition</article-title>
        <source>Conference on computer vision and pattern recognition</source>
        <year>2016</year>
        <fpage>770</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_022">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schindelin</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Arganda-Carreras</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Frise</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kaynig</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Longair</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Pietzsch</surname>
            <given-names>T</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fiji: an open-source Platform for biological-image analysis</article-title>
        <source>Nat Methods</source>
        <year>2012</year>
        <volume>9</volume>
        <fpage>676</fpage>
        <lpage>82</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id>
        <pub-id pub-id-type="pmid">22743772</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_023">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Berg</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kutra</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kroeger</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Straehle</surname>
            <given-names>CN</given-names>
          </name>
          <name>
            <surname>Kausler</surname>
            <given-names>BX</given-names>
          </name>
          <name>
            <surname>Haubold</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Ilastik: interactive machine learning for (bio)image analysis</article-title>
        <source>Nat Methods</source>
        <year>2019</year>
        <volume>16</volume>
        <fpage>1226</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1038/s41592-019-0582-9</pub-id>
        <pub-id pub-id-type="pmid">31570887</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_024">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ronneberger</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Fischer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Brox</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>U-net: convolutional networks for biomedical image segmentation</article-title>
        <source>Medical image computing and computer-assisted intervention</source>
        <year>2015</year>
        <volume>9351</volume>
        <fpage>234</fpage>
        <lpage>41</lpage>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_025">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schilling</surname>
            <given-names>MP</given-names>
          </name>
          <name>
            <surname>Scherr</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Münke</surname>
            <given-names>FR</given-names>
          </name>
          <name>
            <surname>Neumann</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Schutera</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mikut</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated annotator variability inspection for biomedical image segmentation</article-title>
        <source>IEEE Access</source>
        <year>2022</year>
        <volume>10</volume>
        <fpage>2753</fpage>
        <lpage>65</lpage>
        <pub-id pub-id-type="doi">10.1109/access.2022.3140378</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_026">
      <label>26.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hoiem</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Shlapentokh-Rothman</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Learning curves for analysis of deep networks</article-title>
        <person-group person-group-type="editor">
          <name>
            <surname>Meila</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <source>International conference on machine learning</source>
        <year>2021</year>
        <volume>139</volume>
        <fpage>4287</fpage>
        <lpage>96</lpage>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_027">
      <label>27.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Torralba</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Efros</surname>
            <given-names>AA</given-names>
          </name>
        </person-group>
        <article-title>Unbiased look at dataset bias</article-title>
        <source>Conference on computer vision and pattern recognition</source>
        <year>2011</year>
        <fpage>1521</fpage>
        <lpage>8</lpage>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_028">
      <label>28.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Petrov</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Shcheklein</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <source>Data version control – open-source version control system for machine learning projects</source>
        <year>2021</year>
        <comment>Available from</comment>
        <ext-link xlink:href="https://dvc.org/" ext-link-type="uri">https://dvc.org/</ext-link>
        <comment>Accessed</comment>
        <date-in-citation content-type="access">21 Oct 2021</date-in-citation>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_029">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Schilling</surname>
            <given-names>MP</given-names>
          </name>
          <name>
            <surname>Schmelzer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Gómez</surname>
            <given-names>JEU</given-names>
          </name>
          <name>
            <surname>Popova</surname>
            <given-names>AA</given-names>
          </name>
          <name>
            <surname>Levkin</surname>
            <given-names>PA</given-names>
          </name>
          <name>
            <surname>Reischl</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Grid screener: a tool for automated high-throughput screening on biochemical and biological analysis platforms</article-title>
        <source>IEEE Access</source>
        <year>2021</year>
        <volume>9</volume>
        <fpage>166027</fpage>
        <lpage>38</lpage>
        <pub-id pub-id-type="doi">10.1109/access.2021.3135709</pub-id>
      </element-citation>
    </ref>
    <ref id="j_jib-2022-0018_ref_030">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thul</surname>
            <given-names>PJ</given-names>
          </name>
          <name>
            <surname>Lindskog</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>The human protein atlas: a spatial map of the human proteome</article-title>
        <source>Protein Sci</source>
        <year>2018</year>
        <volume>27</volume>
        <fpage>233</fpage>
        <lpage>44</lpage>
        <pub-id pub-id-type="doi">10.1002/pro.3307</pub-id>
        <pub-id pub-id-type="pmid">28940711</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <notes notes-type="supplement" specific-use="print-only" id="j_jib-2022-0018_notes_001">
    <sec id="j_jib-2022-0018_s_007">
      <title>Supplementary Material</title>
      <p>The online version of this article offers supplementary material (<ext-link xlink:href="10.1515/jib-2022-0018" ext-link-type="doi">https://doi.org/10.1515/jib-2022-0018</ext-link>).</p>
    </sec>
  </notes>
</back>
