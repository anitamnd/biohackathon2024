<?DTDIdentifier.IdentifierValue -//ES//DTD journal article DTD version 5.6.0//EN//XML?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName art560.dtd?>
<?SourceDTD.Version 5.6.0?>
<?ConverterInfo.XSLTName elsevier2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<?origin publisher?>
<?FILEmeta_NSM109779 xml ?>
<?FILEmain xml ?>
<?FILEmain pdf ?>
<?FILEga1 jpg ?>
<?FILEgr1 jpg ?>
<?FILEgr2 jpg ?>
<?FILEgr3 jpg ?>
<?FILEgr4 jpg ?>
<?FILEgr5 jpg ?>
<?FILEsi0001 svg ?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">J Neurosci Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">J Neurosci Methods</journal-id>
    <journal-title-group>
      <journal-title>Journal of Neuroscience Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">0165-0270</issn>
    <issn pub-type="epub">1872-678X</issn>
    <publisher>
      <publisher-name>Elsevier/North-Holland Biomedical Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">10375831</article-id>
    <article-id pub-id-type="pii">S0165-0270(22)00306-5</article-id>
    <article-id pub-id-type="doi">10.1016/j.jneumeth.2022.109779</article-id>
    <article-id pub-id-type="publisher-id">109779</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Visiomode: An open-source platform for building rodent touchscreen-based behavioral assays</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" id="au0005">
        <name>
          <surname>Eleftheriou</surname>
          <given-names>Constantinos</given-names>
        </name>
        <email>Constantinos.Eleftheriou@ed.ac.uk</email>
        <xref rid="aff0005" ref-type="aff">a</xref>
        <xref rid="aff0010" ref-type="aff">b</xref>
        <xref rid="cor1" ref-type="corresp">⁎</xref>
      </contrib>
      <contrib contrib-type="author" id="au0010">
        <name>
          <surname>Clarke</surname>
          <given-names>Thomas</given-names>
        </name>
        <xref rid="aff0010" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au0015">
        <name>
          <surname>Poon</surname>
          <given-names>V.</given-names>
        </name>
        <xref rid="aff0010" ref-type="aff">b</xref>
      </contrib>
      <contrib contrib-type="author" id="au0020">
        <name>
          <surname>Zechner</surname>
          <given-names>Marie</given-names>
        </name>
        <xref rid="aff0015" ref-type="aff">c</xref>
      </contrib>
      <contrib contrib-type="author" id="au0025">
        <name>
          <surname>Duguid</surname>
          <given-names>Ian</given-names>
        </name>
        <email>Ian.Duguid@ed.ac.uk</email>
        <xref rid="aff0005" ref-type="aff">a</xref>
        <xref rid="aff0010" ref-type="aff">b</xref>
        <xref rid="cor2" ref-type="corresp">⁎⁎</xref>
        <xref rid="fn1" ref-type="fn">1</xref>
      </contrib>
      <aff id="aff0005"><label>a</label>Simons Initiative for the Developing Brain, University of Edinburgh, Edinburgh EH8 9XD, UK</aff>
      <aff id="aff0010"><label>b</label>Centre for Discovery Brain Sciences and Patrick Wild Centre, Edinburgh Medical School: Biomedical Sciences, University of Edinburgh, Edinburgh EH8 9XD, UK</aff>
      <aff id="aff0015"><label>c</label>The Roslin Institute, University of Edinburgh, Easter Bush, Midlothian EH25 9RG, Scotland, UK</aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>⁎</label>Corresponding author at: Simons Initiative for the Developing Brain, University of Edinburgh, Edinburgh EH8 9XD, UK. <email>Constantinos.Eleftheriou@ed.ac.uk</email></corresp>
      <corresp id="cor2"><label>⁎⁎</label>Correspondence to: Centre for Discovery Brain Sciences, University of Edinburgh, Edinburgh Medical School: Biomedical Sciences, Hugh Robson Building, George Square, Edinburgh EH8 9XD, UK. <email>Ian.Duguid@ed.ac.uk</email></corresp>
      <fn id="fn1">
        <label>1</label>
        <p id="ntp0005">Twitter: @Ian_Duguid.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="pmc-release">
      <day>15</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="ppub">.-->
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <volume>386</volume>
    <elocation-id>109779</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>7</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>22</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>23</day>
        <month>12</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 The Authors</copyright-statement>
      <copyright-year>2023</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).</license-p>
      </license>
    </permissions>
    <abstract id="ab0010">
      <sec>
        <title>Background</title>
        <p>Touchscreen-based behavioral assays provide a robust method for assessing cognitive behavior in rodents, offering great flexibility and translational potential. The development of touchscreen assays presents a significant programming and mechanical engineering challenge, where commercial solutions can be prohibitively expensive and open-source solutions are underdeveloped, with limited adaptability.</p>
      </sec>
      <sec>
        <title>New method</title>
        <p>Here, we present Visiomode (www.visiomode.org), an open-source platform for building rodent touchscreen-based behavioral tasks. Visiomode leverages the inherent flexibility of touchscreens to offer a simple yet adaptable software and hardware platform. The platform is built on the Raspberry Pi computer combining a web-based interface and powerful plug-in system with an operant chamber that can be adapted to generate a wide range of behavioral tasks.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>As a proof of concept, we use Visiomode to build both simple stimulus-response and more complex visual discrimination tasks, showing that mice display rapid sensorimotor learning including switching between different motor responses (i.e., nose poke versus reaching).</p>
      </sec>
      <sec>
        <title>Comparison with existing methods</title>
        <p>Commercial solutions are the ‘go to’ for rodent touchscreen behaviors, but the associated costs can be prohibitive, limiting their uptake by the wider neuroscience community. While several open-source solutions have been developed, efforts so far have focused on reducing the cost, rather than promoting ease of use and adaptability. Visiomode addresses these unmet needs providing a low-cost, extensible platform for creating touchscreen tasks.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>Developing an open-source, rapidly scalable and low-cost platform for building touchscreen-based behavioral assays should increase uptake across the science community and accelerate the investigation of cognition, decision-making and sensorimotor behaviors both in health and disease.</p>
      </sec>
    </abstract>
    <abstract abstract-type="graphical" id="ab0015">
      <title>Graphical Abstract</title>
      <p>
        <fig id="fig0030" position="anchor">
          <alt-text id="at0030">ga1</alt-text>
          <graphic xlink:href="ga1"/>
        </fig>
      </p>
    </abstract>
    <abstract abstract-type="author-highlights" id="ab0020">
      <title>Highlights</title>
      <p>
        <list list-type="simple" id="li0005">
          <list-item id="u0005">
            <label>•</label>
            <p id="p0005">Open-source platform for building rodent touchscreen behavioral tasks.</p>
          </list-item>
          <list-item id="u0010">
            <label>•</label>
            <p id="p0010">Combines a web-based interface and powerful USB plug-in system.</p>
          </list-item>
          <list-item id="u0015">
            <label>•</label>
            <p id="p0015">Plug-and-play design enables rapid adaptability and scaling of touchscreen tasks.</p>
          </list-item>
          <list-item id="u0020">
            <label>•</label>
            <p id="p0020">Novel touchscreen-based 2-AFC forelimb reaching task for rodents.</p>
          </list-item>
        </list>
      </p>
    </abstract>
    <kwd-group id="keys0005">
      <title>Keywords</title>
      <kwd>Touchscreen</kwd>
      <kwd>Open-source</kwd>
      <kwd>Behavior</kwd>
      <kwd>Sensorimotor</kwd>
      <kwd>Visiomode</kwd>
      <kwd>Rodent</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="sec0005">
    <label>1</label>
    <title>Introduction</title>
    <p id="p0030">Since their introduction to biomedical research, touchscreens have become an increasingly popular tool for assessing cognitive function in rodents (<xref rid="bib12" ref-type="bibr">Bussey et al., 1997</xref>, <xref rid="bib18" ref-type="bibr">Dumont et al., 2021</xref>, <xref rid="bib34" ref-type="bibr">Markham et al., 1996</xref>). Their appeal lies with their remarkable flexibility, supporting a vast array of visual stimuli coupled with quantifiable motor responses (<xref rid="bib44" ref-type="bibr">Seitz et al., 2021</xref>), both of which are necessary for designing tasks to investigate complex cognitive processes such as category learning (<xref rid="bib9" ref-type="bibr">Broschard et al., 2021</xref>, <xref rid="bib31" ref-type="bibr">Kim et al., 2018</xref>), spatial attention (<xref rid="bib25" ref-type="bibr">Haddad et al., 2021</xref>), cognitive flexibility (<xref rid="bib23" ref-type="bibr">Groman et al., 2012</xref>), and visual perception (<xref rid="bib34" ref-type="bibr">Markham et al., 1996</xref>). Their use has transformed studies of neurological disorders by providing a sensitive assay of sensorimotor behaviors (<xref rid="bib4" ref-type="bibr">Arulsamy et al., 2019</xref>, <xref rid="bib15" ref-type="bibr">Copping et al., 2017</xref>, <xref rid="bib32" ref-type="bibr">Leach and Crawley, 2018</xref>, <xref rid="bib33" ref-type="bibr">Leach et al., 2016</xref>, <xref rid="bib35" ref-type="bibr">Morton et al., 2006</xref>, <xref rid="bib37" ref-type="bibr">Norris et al., 2019</xref>, <xref rid="bib51" ref-type="bibr">Yang et al., 2015</xref>), revealing subtle phenotypes that were not detected by more conventional assays (<xref rid="bib50" ref-type="bibr">Van den Broeck et al., 2019</xref>, <xref rid="bib52" ref-type="bibr">Zeleznikow-Johnston et al., 2018</xref>). This sensitive readout of changes in behavior holds great translational promise (<xref rid="bib48" ref-type="bibr">Talpos and Steckler, 2013</xref>), where tasks designed for animals can be directly translated to human subjects (<xref rid="bib14" ref-type="bibr">Chow et al., 2020</xref>, <xref rid="bib28" ref-type="bibr">Hvoslef-Eide et al., 2015</xref>, <xref rid="bib36" ref-type="bibr">Nithianantharajah et al., 2015</xref>). Despite their increasing popularity, the use of touchscreen-based behaviors is somewhat limited in rodent research. Uptake has been hampered either by the prohibitive up-front costs of commercial systems or the considerable ‘in-house’ development required to create bespoke touchscreen-based behaviors (<xref rid="bib18" ref-type="bibr">Dumont et al., 2021</xref>).</p>
    <p id="p0035">Commercially available touchscreen behavioral arenas provide researchers with a simple turnkey solution requiring minimal setup time. These systems have dominated the touchscreen landscape in biomedicine over the past two decades (<xref rid="bib4" ref-type="bibr">Arulsamy et al., 2019</xref>, <xref rid="bib7" ref-type="bibr">Brasted et al., 2002</xref>, <xref rid="bib8" ref-type="bibr">Brigman et al., 2010</xref>, <xref rid="bib11" ref-type="bibr">Bussey et al., 1998</xref>, <xref rid="bib13" ref-type="bibr">Bussey et al., 2008</xref>, <xref rid="bib17" ref-type="bibr">Delotterie et al., 2014</xref>, <xref rid="bib22" ref-type="bibr">Glover et al., 2020</xref>, <xref rid="bib25" ref-type="bibr">Haddad et al., 2021</xref>, <xref rid="bib27" ref-type="bibr">Heath et al., 2019</xref>, <xref rid="bib38" ref-type="bibr">Odland et al., 2021</xref>, <xref rid="bib41" ref-type="bibr">Piantadosi et al., 2019</xref>, <xref rid="bib47" ref-type="bibr">Stirman et al., 2016</xref>, <xref rid="bib49" ref-type="bibr">Talpos et al., 2008</xref>), and have played an important role in popularizing their use in rodent research (<xref rid="bib18" ref-type="bibr">Dumont et al., 2021</xref>). However, the prohibitive costs associated with commercial systems (i.e., &gt; 10,000 USD) provides a rate limiting step for their widespread adoption (<xref rid="bib18" ref-type="bibr">Dumont et al., 2021</xref>). In contrast, developing touchscreen tasks ’in-house’ is a particularly challenging programming and engineering problem. While most traditional open-field (<xref rid="bib26" ref-type="bibr">Hall and Ballachey, 1932</xref>) or operant chamber (<xref rid="bib45" ref-type="bibr">Skinner, 1938</xref>) tasks can be implemented with a simple microcontroller device (<xref rid="bib2" ref-type="bibr">Akam et al., 2022</xref>), the introduction of a touchscreen interface requires complex hardware and software integration to control the generation and display of graphics, as well as registering behavioral interactions with the screen. Utilizing graphics libraries available on most Operating Systems (e.g., Microsoft Windows, Linux and MacOS) requires extensive programming knowledge (<xref rid="bib30" ref-type="bibr">Kessenich et al., 2016</xref>), and while open-source initiatives such as PsychoPy greatly simplify the task of generating visual stimuli (<xref rid="bib40" ref-type="bibr">Peirce, 2007</xref>), they still require significant development to be adapted for touchscreen tasks (<xref rid="bib44" ref-type="bibr">Seitz et al., 2021</xref>). This is further complicated by the choice of touchscreen hardware, where heterogeneity in compact touchscreen systems results in variable touch sensitivities, requiring the developer to test and validate a range of screens before final implementation (<xref rid="bib18" ref-type="bibr">Dumont et al., 2021</xref>).</p>
    <p id="p0040">In our view, an open-source, community-driven touchscreen solution would solve both problems by distributing the development effort across multiple research groups, while also reducing overall costs (<xref rid="bib19" ref-type="bibr">Fortunato and Galassi, 2021</xref>, <xref rid="bib20" ref-type="bibr">Freeman, 2015</xref>). Open-science initiatives have continued to grow in the past few years, with projects like MouseBytes (<xref rid="bib6" ref-type="bibr">Beraldo et al., 2019</xref>) and the advice sharing platform touchscreencognition.org (<xref rid="bib18" ref-type="bibr">Dumont et al., 2021</xref>). To date, no open-source community-based solution exists. While open-source touchscreen-based operant chambers have been developed (<xref rid="bib24" ref-type="bibr">Gurley, 2019</xref>, <xref rid="bib39" ref-type="bibr">O'Leary et al., 2018</xref>, <xref rid="bib42" ref-type="bibr">Pineno, 2014</xref>), this has not led to increased uptake due a lack of code availability and the primary focus being on reducing costs, rather than enhancing the user experience, scalability, adaptability, and ease of use.</p>
    <p id="p0045">To address these unmet needs, we have developed Visiomode (www.visiomode.org), a complete open-source software and hardware platform for developing touchscreen-based behavioral tasks for rodents. Visiomode combines a sophisticated web-based user interface (UI) and powerful plug-in system, with an affordable hardware configuration that can accommodate a wide variety of visuomotor tasks (<xref rid="fig0005" ref-type="fig">Fig. 1</xref>). Our solution has been designed to maximize adaptability, while providing a consistent, standardized user experience and output data format. While several visual stimuli and task structures have been provided out-of-the-box, including drifting gratings, symbols and natural images, users can upload or simply programmatically define their own visual stimuli. Using simple stimulus-response and more complex visual discrimination tasks as exemplars, we show that mice display rapid sensorimotor learning, switching between both nose poke and visually guided reaching depending on task requirements. In addition, we discuss Visiomode’s Application Programming Interface (API), how it can be scaled to parallelize data acquisition using a single personal device and the necessary build components.<fig id="fig0005"><label>Fig. 1</label><caption><p>Visiomode: a flexible, scalable platform for building touchscreen-based tasks. Left, Image of the mouse behavioral arena with interactive touchscreen controlled by Visiomode. Right, Schematic showing that Visiomode is a web-based interface that controls, via WiFi, a Raspberry Pi computer and any coupled USB devices (e.g. loudspeaker, USB microcontroller for reward delivery, touchscreen). Visual stimuli can be generated programmatically via Visiomode's API or loaded as images / animation files on-the-fly and presented via any touchscreen supported by Raspberry Pi, including integrated touchscreen displays such as the Pimoroni Hyperpixel. Visiomode also supports real-time analysis with data being exported in a variety of formats, including JSON, HDF5 and NWB.</p></caption><alt-text id="at0005">Fig. 1</alt-text><graphic xlink:href="gr1"/></fig></p>
  </sec>
  <sec id="sec0010">
    <label>2</label>
    <title>Methods</title>
    <sec id="sec0015">
      <label>2.1</label>
      <title>API design principles</title>
      <p id="p0050">We first designed Visiomode’s API which encapsulates all the software functionality required to design and run touchscreen behavioral tasks, including stimulus generation, trial structure definition, response recording and interfacing with external hardware via the USB. The API is written in Python, leveraging its popularity, wide availability of libraries and ease of use. The PyGame and PySDL libraries are used for handling graphics and task timing, while the Flask library is used for rendering Visiomode's web interface. In addition to the behavioral tasks provided out-of-the-box, the API can be used to implement additional task components such as protocols, stimuli or hardware integrations using user-defined task files.</p>
      <p id="p0055">The API is broadly divided into three parts: <italic>stimulus interface</italic>, <italic>protocol interface</italic> and <italic>device interface</italic>. Each constituent part represents a Python abstract base class (Hunt, 2019), which defines a programmatic interface which all user-defined stimuli, protocols and devices must be derived from. Each interface allows users to integrate custom HTML forms for dynamically setting component-specific parameters within Visiomode's web interface.</p>
      <p id="p0060">The <italic>stimulus interface</italic> allows for user-defined visual stimuli to be integrated into behavioral tasks, independently from task structure. For example, the same user-defined stimulus, such as a solid color or a drifting grating, can be reused across all available task protocols without having to be redefined. Each class derived from the <italic>stimulus interface</italic> inherits several functions that are core to the presentation of the visual stimuli within a protocol, such as functions that control the appearance and removal of stimuli, updating of stimulus position, or modification of stimuli between trials. Additionally, stimuli can integrate peripheral USB devices to yield multi-sensory stimuli (i.e., simultaneous presentation of an auditory tone and drifting grating).</p>
      <p id="p0065">Task structure definition classes are inherited from the <italic>protocol interface</italic>, which controls the timing of the presentation of stimuli and processes touchscreen and external device events during an experimental session. User-defined tasks must specify, as a minimum, a target stimulus parameter, which may be set dynamically via the web interface. The web interface will pass the session duration, inter-trial interval (ITI) and duration of stimulus presentation to every protocol-derived class, which by default will iterate through calls to a trial_block function until the session duration expires. The trial_block function monitors the application's touch event queue during the ITI and while a stimulus is present, and assigns correct, incorrect, uncued or miss responses accordingly. The <italic>protocol interface</italic> implements several functions corresponding to different trial outcome conditions, such as on_correct, on_incorrect and on_uncued, as well as trial epochs such as on_trial_start or on_stimulus_start, which can be overridden by users to build complex trial structures with fewer lines of code and without requiring an extensive understanding of Visiomode's core functionality.</p>
      <p id="p0070">While most behavioral protocols will be defined before each session, Visiomode can support dynamic, on-the-fly changes to stimulus and protocol settings depending on behavioral performance. For example, the contrast of a stimulus could be decreased following a user-defined number of correct responses within a single training session. This would be achieved by implementing a new <italic>protocol</italic> file, which can then be uploaded to Visiomode via the web interface. For more information, we encourage users to visit www.visiomode.org for the latest guidance on implementing new protocols.</p>
      <p id="p0075">While Visiomode’s web interface can be accessed via a browser, instances of Visiomode do not need an active internet connection as they contain all necessary code libraries for running tasks and monitoring performance, including stimulus generation and live plotting of behavioral data. While the recommended installation route requires an internet connection, we provide alternative means of installing the software on Raspberry Pis if internet connectivity is not possible.</p>
      <p id="p0080">Finally, in addition to the touchscreen itself, Visiomode can integrate external USB devices connected to the Raspberry Pi through the <italic>device interface</italic>. For example, a water reward mechanism driven by an Arduino microcontroller can be used to dispense rewards following correct task responses. The <italic>device interface</italic> is subdivided into Input and Output interfaces, supporting both sensors that can feed into a task structure as well as actuators providing additional sensory stimuli or dispensing rewards. While the Raspberry Pi's own GPIO ports could also be used to integrate sensors or actuators in Visiomode tasks, supporting USB devices can be advantageous. Touchscreen displays for the Raspberry Pi use all the available GPIO ports, which leads to a more compact design, but necessitates the use of USB-connected microcontrollers to integrate external hardware. Given the ubiquity of microcontroller devices in neuroscience, this plug-and-play approach allows the user to easily incorporate USB-connected devices from existing setups facilitating friction-free development of novel behavioral tasks.</p>
      <p id="p0085">While Visiomode has been designed as a flexible platform that can be readily extended programmatically, it offers several common experimental paradigms out-of-the-box providing utility for users with no prior programming experience. Single target, two-alternative forced choice (2AFC) and Go/NoGo protocols are included with every installation which support correction trials, pseudo-randomization of stimulus presentation, and any arbitrary external reward devices. Sinusoidal gratings with user-defined characteristics, as well as a range of symbols and natural scenes are also included. Visiomode also supports a range of external reward devices, such as reward spouts and food hoppers, with microcontroller code available at https://doi.org/10.5281/zenodo.6877795. For any USB connected devices for which the Rasberry Pi operating systems does not provide an out-of-the-box solution for integration/calibration, Visiomode supports the use of custom written “driver” code within its <italic>devices</italic> interface.</p>
      <p id="p0090">To facilitate synchronization of behavior and physiological recordings, Visiomode timestamps behavioral epochs using the system clock, which can be synched with a local Network Time Protocol server to provide sub-millisecond time synchronization between the Raspberry Pi computer running Visiomode and the computer acquiring physiological data. For physiological recordings that require higher temporal precision (i.e., electrophysiological recordings at 20 KHz), Visiomode can provide a time synchronization signal using the host Raspberry Pi's GPIO ports or a microcontroller device connected to the USB (<xref rid="bib2" ref-type="bibr">Akam et al., 2022</xref>).</p>
      <p id="p0095">Visiomode's source code is openly available and is publicly hosted at https://doi.org/10.5281/zenodo.6877795 under the terms of the MIT license.</p>
    </sec>
    <sec id="sec0020">
      <label>2.2</label>
      <title>Web interface</title>
      <p id="p0100">Each instance of Visiomode exposes a web interface that enables the user to set up, run and monitor touchscreen experiments (<xref rid="fig0010" ref-type="fig">Fig. 2</xref>), accessible via any web browser that is connected to the same local network as the Raspberry Pi. The web interface is designed to cater to both novice and expert users. An accessible user interface enables the uptake of Visiomode by users with no prior programming knowledge, while offering a fully customizable user interface for users that wish to extend Visiomode's capabilities.<fig id="fig0010"><label>Fig. 2</label><caption><p>Visiomode: web-based Graphical User Interface (GUI). Visiomode GUI which provides session information for an individual mouse (mouse552) and experiment (single drifting grating target, nose poke). The 'More Options' tab allows the user to define advanced stimulus parameter options such as frequency, contrast, inter-trial-interval duration, and separator size (pixels).</p></caption><alt-text id="at0010">Fig. 2</alt-text><graphic xlink:href="gr2"/></fig></p>
      <p id="p0105">The primary function of the web interface is to set up and run touchscreen behavioral tasks. Task, stimulus, and device parameters can be set on-the-fly, allowing users to control all aspects of the setup without the need to modify the code. For example, Visiomode provides out-of-the-box support for drifting gratings, where parameters such as the cycles per degree, contrast and drift frequency can be adjusted online. Each component, derived from the <italic>Protocol</italic>, <italic>Stimulus</italic> and <italic>Device</italic> interfaces described in the previous section, can optionally implement a webform via the form_path attribute that passes parameters from the web interface to the Visiomode API (for an example, see https://doi.org/10.5281/zenodo.6877795) (<xref rid="fig0010" ref-type="fig">Fig. 2</xref>). These webforms are loaded dynamically when the user selects a particular component and can interface with multiple components to build complex tasks (e.g., protocols can have multiple stimuli integrated with multiple input and/or output devices). The parameters set on the web interface are serialized and converted to asynchronous calls to the Visiomode API, which assembles the different components into a behavioral protocol that runs on the touchscreen.</p>
      <p id="p0110">In addition to task control, the Visiomode interface plots real-time task analytics that are protocol-specific and can be further customized or extended via an analytics_path attribute, see https://doi.org/10.5281/zenodo.6877795. Visiomode analytics use the Graphs.js Javascript library for plotting, however other popular Javascript graphics libraries such as D3 or Plotly can also be used. Visiomode’s web interface pools data from the API running the behavior at 4 s intervals, updating only the analytics components visible to the user.</p>
    </sec>
    <sec id="sec0025">
      <label>2.3</label>
      <title>A flexible, scalable platform for building touchscreen tasks</title>
      <p id="p0115">The web interface design ensures that each Visiomode setup is self-contained, and that there is virtually no upper limit to parallelization, provided all devices are connected to the same local network. Each instance of the web interface is designed to be asynchronous to the running of the task, such that if the web interface is disconnected (e.g., by an intermittent network issue, or accidental closing of the device’s browser) the Visiomode API running the task will be unaffected. Decoupling the web interface from the API running the experimental session yields a resilient and easily scalable core platform, which users can customize and extend to suit their individual experimental needs.</p>
      <p id="p0120">The web interface can also be used to export session data in a variety of different formats. By default, Visiomode session data are stored as human-readable Javascript Object Notation (JSON) files which include metadata relating to the animal, the protocol and stimulation parameters as well as information on the host device and any other USB peripherals. Session files can additionally be exported to comma-separated value (CSV), hierarchical data format (HDF5) as well as Neurodata Without Borders (NWB) files (<xref rid="bib43" ref-type="bibr">Rübel et al., 2019</xref>). The NWB format is a particularly useful tool in integrating behavioral and neurophysiological data in a standardized and widely accessible format. To ease the onboarding of users with Visiomode's behavioural data, we provide an example Jupyter notebook at https://doi.org/10.5281/zenodo.6877795, which can be used as a template.</p>
    </sec>
    <sec id="sec0030">
      <label>2.4</label>
      <title>Touchscreen behavioral arena</title>
      <p id="p0125">Next, we designed a behavioral arena that conforms to a rectangular design common across many operant chambers, measuring 200 mm (L) x 200 mm (W) x 400 mm (H). The walls of the arena were constructed from red transparent acrylic panels (ER Plastics, UK), allowing for easy monitoring of animal behavior, while minimizing confounding external visual stimuli. Individual panels were mounted on aluminum struts (RS Components, UK) creating the exterior shape of the arena and enabling rapid assembly and disassembly during cleaning. The touchscreen (Hyperpixel 4.0, 58 mm × 97 mm, Pimoroni, RS Components, UK) was positioned on one side of the arena, accessed via a 90 mm × 115 mm hole cut in the back wall of the arena and mounted vertically across the GPIO ports of a Raspberry Pi computer (Revision 4, RS Components). To limit access to the touchscreen, a clear, transparent acyclic panel (100 mm × 125 mm x 2 mm) was positioned directly in front of the screen using magnetic mounting strips attached to the arena wall. For nose poke experiments we used a panel with a 40 mm × 20 mm cut out, positioned 10 mm above the floor of the arena, while for forelimb reaching, we designed a divider with two 35 mm × 4 mm vertical slits positioned 15 mm apart (<xref rid="fig0015" ref-type="fig">Fig. 3</xref>). The narrow width of the slits and position of the screen 6 mm from the arena wall prevents screen touches using the snout or tongue.<fig id="fig0015"><label>Fig. 3</label><caption><p>Behavioral arena with touchscreen module. (a) 3D reconstruction of the behavioral arena and touchscreen module consisting of a Hyperpixel Display 4.0, servo controlled reward spout, servo motor, solenoid, water reservoir and transparent Perspex screen divider with either nose poke or reaching slit cutouts. The behavioral arena and touchscreen module can be custom designed to fit the needs of each individual experiment. (b) Schematic diagrams showing 2-AFC nose poke (left) and forelimb reaching (right) configurations. Note the reward spout retracts after each trial using a servomotor which rotates by 10 degrees.</p></caption><alt-text id="at0015">Fig. 3</alt-text><graphic xlink:href="gr3"/></fig></p>
      <p id="p0130">For mouse behavior, the size of the capacitance touchscreen is in general inversely proportional to the sensitivity, so care must be taken to select a touchscreen with a sensitivity range that is compatible with mouse touch pressures. A variety of off-the-shelf touchscreens are available for the Raspberry Pi, however, we found that the Hyperpixel display offers a good compromise between size and sensitivity. Visiomode interfaces with touchscreen devices connected to its Raspberry Pi host via the host’s operating system. This allows for Visiomode to work with a wide range of touchscreen devices with no additional configuration. Consequently, touchscreen calibration, which should not be required in most instances, would not be performed within the Visiomode interface but rather via the operating system’s own tools. To use touchscreen devices for which the Raspberry Pi operating system offers no out-of-the-box solution for calibration, users should, in the first instance, refer to the manufacturer’s specification for device drivers compatible with the Linux kernel. Alternatively, Visiomode supports the integration of custom-written hardware drivers through its <italic>devices</italic> interface, which allows for the custom integration of any external touchscreen device.</p>
      <p id="p0135">To record the behavior of the mouse in the arena (e.g., open field locomotion, rearing, grooming and task engagement) a webcam (Logitech, UK) was mounted on the wall of the arena opposite the touchscreen using a custom 3D printed support arm (https://doi.org/10.5281/zenodo.6877081).</p>
      <p id="p0140">Upon successful completion of a trial, we delivered a water reward via a 2 mm diameter clear acrylic spout mounted directly below the touchscreen. The reward spout was gravity fed via a water reservoir positioned 30 cm above the arena and dispensation was controlled by a 5 V solenoid valve (RS Components, UK) connected to an Arduino Nano microcontroller (RS Components, UK). Solenoid opening times and the height of the reservoir were adjusted to reproducibly release 10 µl of water per rewarded trial (measured using a calibrated pipette). To prevent mice from chewing or pulling the spout after reward delivery it was mounted to a 5 V servomotor which rotated the spout by 10 degrees after a 1 s delay effectively retracting the spout. While we used a solenoid valve to dispense water, the reward mechanisms could be replaced by a syringe pump allowing for finer and dynamic reward size control (<xref rid="bib3" ref-type="bibr">Amarante et al., 2019</xref>), or by an automated food hopper mounted to the side of the touchscreen module (<xref rid="bib1" ref-type="bibr">Acosta-Rodríguez et al., 2017</xref>). Full behavioral arena and touchscreen module build details can be found at https://doi.org/10.5281/zenodo.6877081.</p>
    </sec>
    <sec id="sec0035">
      <label>2.5</label>
      <title>Animals, habituation, and water control</title>
      <p id="p0145">For all behavioral experiments, male adult C57BL/6 J wild-type mice (8–10 weeks old, 20–30 g, 3–4 animals per cage) were maintained on a reversed 12:12 h light:dark cycle and provided ad libitum access to food and water as well as environmental enrichment. All experiments and procedures were approved by the University of Edinburgh local ethical review committee and performed under license from the UK Home Office in accordance with the Animal (Scientific Procedures) Act 1986. Mice were handled extensively for at least 5 days prior to any behavioral training and were trained once per day for 30 mins. To increase task engagement, mice were placed on a water control regime (1 ml / day) and weighed daily to ensure body weight remained above 80 % of baseline (<xref rid="bib16" ref-type="bibr">Dacre et al., 2021</xref>).</p>
    </sec>
    <sec id="sec0040">
      <label>2.6</label>
      <title>Data analysis &amp; statistics</title>
      <p id="p0150">Data were analyzed using custom scripts written in Python v3.7. Data are reported as mean ± 95 % bootstrapped confidence interval (95 % CI) (10,000 bootstrap samples, 50 replicates per sample) unless otherwise stated. To assess the ability of mice to discriminate between the two visual stimuli in the 2AFC task, we calculated a discriminability index (d’), defined as<disp-formula id="eqn0005"><mml:math id="M1" altimg="si0001.svg"><mml:mrow><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo linebreak="goodbreak">=</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>√</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mo linebreak="badbreak">−</mml:mo><mml:mi>Z</mml:mi><mml:mrow><mml:mfenced open="(" close=")"><mml:mrow><mml:mi mathvariant="italic">FA</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>where Z(x) is the inverse-normal transformation of x, and H and FA correspond to the hit and false alarm rates, respectively (<xref rid="bib46" ref-type="bibr">Stanislaw and Todorov, 1999</xref>). All analysis is available in the form of Jupyter notebooks at https://doi.org/10.5281/zenodo.6877795.</p>
    </sec>
  </sec>
  <sec id="sec0045">
    <label>3</label>
    <title>Results</title>
    <sec id="sec0050">
      <label>3.1</label>
      <title>Phase I: simple stimulus-response task</title>
      <p id="p0155">In the first phase of training mice had to learn a simple stimulus-response behavior. Each trial began with a fixed-length (3 s) inter-trial-interval (ITI), where the touchscreen was left blank (default is a solid black screen), before presentation of a 10 s drifting grating stimulus (100 % contrast, 1 Hz sinusoid at 30 cycles / degree). During this shaping phase, mice explored the behavioral arena and responded to the presentation of a drifting grating by nose poking the touchscreen to receive a 10 µl water reward. The touchscreen rests behind a transparent insert which restricts the touchable surface thus centralizing contact points. Any contact with the screen during the ITI was deemed a ‘uncued touch’ and resulted in a reset and commencement of a subsequent ITI. To allow time for reward consumption during successful trials we implemented a 1 s delay prior to the start of the next trial (<xref rid="fig0020" ref-type="fig">Fig. 4</xref>a-b). Mice rapidly learned the association between presentation of the target stimulus, response and reward displaying an increased number of successful trials and fewer miss trials (i.e., no touch response during stimulus) (<xref rid="fig0020" ref-type="fig">Fig. 4</xref>c-d). Miss trials and uncued touches were not punished. On average, mice required two behavioral sessions to reach our experimenter-defined threshold of &gt; 70 successful trials for 2 consecutive days, before they transferred to Phase II of the behavior (mean = 1.9 days [1.7, 2.1] 95 % CI, N = 9 mice) (<xref rid="fig0020" ref-type="fig">Fig. 4</xref>e).<fig id="fig0020"><label>Fig. 4</label><caption><p>Using Visiomode to shape stimulus-response associations and 2-alternative forced choice task learning. (a) Schematic showing a mouse engaged in a simple stimulus-response behavior (Phase I). (b) Training flowchart showing Phase I of the behavioral task (simple stimulus-response association): presentation of a target stimulus (moving grating) after a fixed-length inter-trial-interval (3 s, ITI). The task requires mice to nose poke the touchscreen to receive a water reward, failure to touch the screen results in the initiation of a subsequent ITI. (c) Number of successful trials per 30 min training session (blue lines, data from individual mice, N = 9 mice). Gray dashed line, threshold of &gt; 70 rewards / session. Mice progress to Phase II after achieving &gt; 70 rewards / session for two consecutive sessions. (d) Number of miss trials per 30 min training session (blue lines, data from individual mice, N = 9 mice). Note, decrease in the number of miss trials is reflected in the increase in the number of successful trials shown in (c). (e) Box-and-whisker plot showing median, interquartile range, and range of the number of sessions required to reach &gt; 70 rewards (N = 9 mice). (f) Schematic showing a mouse engaged in a 2-AFC nose-poke task (Phase II). (g) Training flowchart showing Phase II of the behavioral task (2-AFC): presentation of a pair of stimuli (target stimulus = moving grating, distractor stimulus = isoluminescent gray screen) after a pseudo-random inter-trial-interval (4–6 s, ITI). The task requires mice to nose poke the target area of the touchscreen to receive a water reward, failure to touch the screen results in the initiation of a subsequent ITI. Nose poking the distractor area of the touchscreen results in a correction trial, where the same pair or stimuli are presented until a correct response has been achieved. (h) Discriminability index (d') as a function of the number of training sessions. Blue line, average d' ± 95 % CI (N = 9 mice). Gray dashed line, d' threshold of 1.5. (i) Box-and-whisker plot showing median, interquartile range, and range of the number of training sessions required to reach d' &gt; 1.5 (N = 9 mice). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</p></caption><alt-text id="at0020">Fig. 4</alt-text><graphic xlink:href="gr4"/></fig></p>
    </sec>
    <sec id="sec0055">
      <label>3.2</label>
      <title>Phase II: 2-AFC visual discrimination nose poke task</title>
      <p id="p0160">After successful completion of behavioral shaping in Phase I, mice progressed to the 2-alternative forced choice (2-AFC) version of the task which incorporated a pseudo-randomized (4–6 s) ITI, with the target stimulus (drifting grating, 10 s) presented alongside an isoluminant gray distractor stimulus. The 10 mm ‘dead zone’ separating the stimulus and distractor was an area where touch events would not be registered (black vertical line, <xref rid="fig0020" ref-type="fig">Fig. 4</xref>f). The left versus right positioning of the target and distractor stimuli were pseudo-randomly ordered per trial. Mice learned to nose poke the target stimulus to receive a 10 µl water reward, while ignoring the distractor stimulus. If mice incorrectly chose the distractor stimulus, the subsequent trial was a correction trial, whereby the same stimulus placement was presented until the mouse correctly touched the target stimulus (<xref rid="fig0020" ref-type="fig">Fig. 4</xref>g). Mice rapidly learned to discriminate between target and distractor stimuli, with reliable discrimination (discrimination index d’ &gt; 1.5) after an average of 9 training sessions (mean = 8.7, [7.4, 10.0] 95 % CI, N = 9 mice) with peak discrimination reflecting very high performance (d’ mean = 2.4 [2.2, 2.6], N = 9 mice) (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>h-i). To ensure an unbiased measure of discriminability, correction trials were excluded from d’ calculations. In addition, task engagement (i.e., hit trials / total trials) was consistently high across behavioral sessions (bootstrap mean = 92.3 % [84.3, 100.0] 95 % CI response rate across trials, N = 9 mice) despite mice routinely receiving more than their daily allowance of water during the task (cumulative volume of rewards &gt; 1 ml per session) (data not shown). After ∼12 sessions, d’ became asymptotic and at 20 sessions mice were transferred to Phase III of the task.<fig id="fig0025"><label>Fig. 5</label><caption><p>A 2-AFC visual discrimination reaching task for freely moving mice. (a) Schematic showing a mouse engaged in a 2-AFC reaching task (Phase III). (b) Discriminability index (d') as a function of the number of training sessions. Blue line, average d' ± 95 % CI (N = 9 mice). Gray dashed line, d' threshold of 1.5. (c) Box-and-whisker plot showing median, interquartile range, and range of the number of training sessions required to reach d' &gt; 1.5 (N = 9 mice). (d) Schematic showing paw placement distributions during an early (<italic>left</italic>) and late (<italic>right</italic>) training session. Red dots depict individual paw placements within the open slit. (e) Pairwise distance between individual paw placements as a function of the number of training sessions. Blue line, average pairwise distance (mm) ± 95 % CI (N = 9 mice). (f) Box-and-whisker plot showing median, interquartile range, and range of the pairwise distance between individual paw placements during an early and late training session (N = 9 mice). Red cross denotes identified outlier. <italic>p</italic> = 2.5 × 10<sup>-182</sup>. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</p></caption><alt-text id="at0025">Fig. 5</alt-text><graphic xlink:href="gr5"/></fig></p>
      <sec id="sec0060">
        <label>3.2.1</label>
        <title>Phase III: 2-AFC visual discrimination reaching task</title>
        <p id="p0165">To explore the use of forelimb reaching as an alternative readout in our 2-AFC task, we replaced the transparent ‘nose poke’ insert with an insert containing two guide slits spaced 15 mm apart. We encouraged reaching by placing the touchscreen 8 mm from the front of the slit (2 mm deep insert + 6 mm space between insert and touchscreen) which restricted access such that mice could neither nose poke nor contact the screen using their tongue (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>a). Mice had to learn to reach through the slit corresponding to the target stimulus to gain a 10 µl water reward, while ignoring the slit associated with the distractor stimulus. This task setup allows investigation of sensory perception, decision-making and skilled motor control with quantifiable metrics for each component. Given the increase in complexity of the movement when switching from nose poke to reaching, mice inevitably made more mistakes resulting in a reduction in d’ immediately after the transition. This was not due to inactivity as mice displayed many cue-triggered reaches but with a high proportion of misses (mean = 168.8 reaches [160.9, 176.6] 95 % CI, N = 9 mice), comparable to the number of nose pokes in Phase II (mean = 180.5 nose pokes [169.3, 191.8] 95 % CI; median difference = 16.6 responses [−9.7, 34.6] 95 % CI, p = 0.14; N = 9 mice). Mice also explored using nose pokes and licking as a strategy before associating reaching with reward. The exploration phase lasted for only a short period of time with d’ recovering to &gt; 1.5 within 2 training sessions (mean = 2.3 days [2.0, 2.7] 95 % CI, N = 9 mice) (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>b-c). A hallmark of rodent motor learning is the development of reproducible, stereotyped reach trajectories (<xref rid="bib5" ref-type="bibr">Becker and Person, 2019</xref>, <xref rid="bib21" ref-type="bibr">Galinanes et al., 2018</xref>, <xref rid="bib29" ref-type="bibr">Kawai et al., 2015</xref>). By comparing the average pairwise distance between paw touch positions across learning, we could demonstrate the rapid decrease in pairwise distance across training, resulting in highly clustered touch positions after ∼10 training sessions (pairwise distance - early, mean = 6.3 mm [5.9, 6.7] 95 % CI; pairwise distance - late, mean = 2.6 [2.5, 2.8] 95 % CI; median difference = 4.1 mm [3.8, 4.4] 96 % CI, p = 2.5 ×10<sup>-128</sup>, N = 9 mice) (<xref rid="fig0025" ref-type="fig">Fig. 5</xref>d-f). Together, our results show that mice rapidly accommodate the switch in task structure, transferring from nose poke to visually guided reaching with minimal extra training. To facilitate uptake, we have generated a detailed, step-by-step protocol for behavioral training that can be found at https://dx.doi.org/10.17504/protocols.io.bumgnu3w.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec0065">
    <label>4</label>
    <title>Discussion</title>
    <p id="p0170">Here we have developed Visiomode (www.visiomode.org), a complete open-source software and hardware platform for building touchscreen-based behavioral tasks for rodents. As a key design principle, our aim was to develop a platform that was low cost and as close to turnkey as possible without sacrificing flexibility.</p>
    <p id="p0175">Visiomode’s goal is to empower users with little or no programming experience to run their own touchscreen tasks without the up-front cost of a commercial solution. After setting up a behavioral arena to the specifications we describe in this paper, the typical Visiomode user is four clicks away from their own battery of touchscreen-based behavioral tasks. First, a user would navigate to our website at www.visiomode.org, download and install the software with the instructions provided, choose from a wide selection of pre-programmed task paradigms, and click start. In contrast with currently available open-source solutions (<xref rid="bib24" ref-type="bibr">Gurley, 2019</xref>, <xref rid="bib39" ref-type="bibr">O'Leary et al., 2018</xref>, <xref rid="bib42" ref-type="bibr">Pineno, 2014</xref>, <xref rid="bib10" ref-type="bibr">Buscher et al., 2020</xref>), Visiomode is openly available online and with no programming experience required due to its web interface that encapsulates all the functionality required to design tasks, as well as acquire and export behavioural data. The platform can be parallelized with no additional effort; users can follow the same steps for adding additional arenas, all of which can then be controlled from the same web browser. Thus, Visiomode is a unique, turkey solution which addresses many of the shortcomings of currently available open-source touchscreen solutions (i.e. ease of use, paradigm flexibility, code accessibility), and addresses many of the unmet needs of the user community (i.e. easy parallelization, low cost, adaptable).</p>
    <p id="p0180">Visiomode has been designed to be a community-driven project. Project development takes place on GitHub with a transparent development roadmap (https://github.com/DuguidLab/visiomode). Members of the community can contribute plugins for new protocols, stimuli and external USB devices, report bugs or suggest improvements in the software, help with documenting Visiomode or contribute to the development of the core API. The project will follow the "fork and pull" model of open-source software development for contributions, whereby any user can obtain their own copy of the source code to make changes, avoiding the need for change-related permissions. Submitted changes will be audited by the host lab to ensure all code conforms to the core project style. The "fork and pull" model empowers any user to become a contributor by eliminating the need for individual contributions to be coordinated by the project's core team, while the review process ensures Visiomode's stability and compatibility across behaviors. Our vision is that community-based contributions (protocols, stimuli etc) will eventually generate a comprehensive ‘go-to’ solution for any form of touchscreen-based behavior.</p>
    <p id="p0185">The Visiomode platform can accommodate a wide range of hardware configurations, including the addition of multiple USB-connected peripherals acting as input or output devices and various sized touchscreen to suit both rat and mouse behavioral arenas (<xref rid="bib4" ref-type="bibr">Arulsamy et al., 2019</xref>, <xref rid="bib7" ref-type="bibr">Brasted et al., 2002</xref>, <xref rid="bib8" ref-type="bibr">Brigman et al., 2010</xref>, <xref rid="bib11" ref-type="bibr">Bussey et al., 1998</xref>, <xref rid="bib13" ref-type="bibr">Bussey et al., 2008</xref>, <xref rid="bib17" ref-type="bibr">Delotterie et al., 2014</xref>, <xref rid="bib22" ref-type="bibr">Glover et al., 2020</xref>, <xref rid="bib24" ref-type="bibr">Gurley, 2019</xref>, <xref rid="bib25" ref-type="bibr">Haddad et al., 2021</xref>, <xref rid="bib27" ref-type="bibr">Heath et al., 2019</xref>, <xref rid="bib39" ref-type="bibr">O'Leary et al., 2018</xref>, <xref rid="bib38" ref-type="bibr">Odland et al., 2021</xref>, <xref rid="bib41" ref-type="bibr">Piantadosi et al., 2019</xref>, <xref rid="bib42" ref-type="bibr">Pineno, 2014</xref>, <xref rid="bib47" ref-type="bibr">Stirman et al., 2016</xref>, <xref rid="bib49" ref-type="bibr">Talpos et al., 2008</xref>). In addition, it permits rapid scaling to parallelize multiple behavioral arenas, each hosting its own web interface that is accessible via any personal device web browser connected to the same network. Unlike parallelized behavioral setups that are controlled by a single GUI running via a centralized host, Visiomode's decentralized web interface design allows for multiple behavioral setups running separate tasks to be controlled in parallel. This parallelization model can facilitate high-throughput behavioral testing, enabling large-scale behavioral assays, efficient drug screening, or disease model phenotyping. This combined with the easy-to-use USB plug-and-play design permits rapid scaling up and scaling down of experiments on-the-fly depending on experimental requirements.</p>
    <p id="p0190">By developing an open-source, rapidly scalable and low-cost platform our aim is to increase uptake of touchscreen-based behavioral assays across our community, accelerating the investigation of cognition, decision-making and sensorimotor behaviors both in health and disease.</p>
  </sec>
  <sec id="sec0070">
    <title>CRediT authorship contribution statement</title>
    <p id="p0195">Conceptualization and design, <bold>C.E.</bold>, and <bold>I.D.</bold>; Methodology &amp; Investigation, <bold>C.E., V.P., T.C.</bold>, and <bold>I.D</bold>.; Resources, <bold>C.E., T.C</bold>.; graphic design, <bold>M.Z.</bold>; Writing – review &amp; editing, all authors.</p>
  </sec>
  <sec sec-type="COI-statement">
    <title>Declaration of Competing Interest</title>
    <p id="p0200">The authors declare no declarations of competing interest.</p>
  </sec>
</body>
<back>
  <ref-list id="bibliog0005">
    <title>References</title>
    <ref id="bib1">
      <element-citation publication-type="journal" id="sbref1">
        <person-group person-group-type="author">
          <name>
            <surname>Acosta-Rodríguez</surname>
            <given-names>V.A.</given-names>
          </name>
          <name>
            <surname>de Groot</surname>
            <given-names>M.H.</given-names>
          </name>
          <name>
            <surname>Rijo-Ferreira</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Green</surname>
            <given-names>C.B.</given-names>
          </name>
          <name>
            <surname>Takahashi</surname>
            <given-names>J.S.</given-names>
          </name>
        </person-group>
        <article-title>Mice under caloric restriction self-impose a temporal restriction of food intake as revealed by an automated feeder system</article-title>
        <source>Cell Metab.</source>
        <volume>26</volume>
        <year>2017</year>
        <fpage>267</fpage>
        <lpage>277</lpage>
        <comment>e262</comment>
        <pub-id pub-id-type="pmid">28683292</pub-id>
      </element-citation>
    </ref>
    <ref id="bib2">
      <element-citation publication-type="journal" id="sbref2">
        <person-group person-group-type="author">
          <name>
            <surname>Akam</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Lustig</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Rowland</surname>
            <given-names>J.M.</given-names>
          </name>
          <name>
            <surname>Kapanaiah</surname>
            <given-names>S.K.</given-names>
          </name>
          <name>
            <surname>Esteve-Agraz</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Panniello</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Marquez</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Kohl</surname>
            <given-names>M.M.</given-names>
          </name>
          <name>
            <surname>Katzel</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Costa</surname>
            <given-names>R.M.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Open-source, python-based, hardware and software for controlling behavioural neuroscience experiments</article-title>
        <source>eLife</source>
        <volume>11</volume>
        <year>2022</year>
      </element-citation>
    </ref>
    <ref id="bib3">
      <element-citation publication-type="journal" id="sbref3">
        <person-group person-group-type="author">
          <name>
            <surname>Amarante</surname>
            <given-names>L.M.</given-names>
          </name>
          <name>
            <surname>Newport</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Mitchell</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Laubach</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>An open source syringe pump controller for fluid delivery of multiple volumes</article-title>
        <source>eNeuro</source>
        <volume>6</volume>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="bib4">
      <element-citation publication-type="journal" id="sbref4">
        <person-group person-group-type="author">
          <name>
            <surname>Arulsamy</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Corrigan</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Collins-Praino</surname>
            <given-names>L.E.</given-names>
          </name>
        </person-group>
        <article-title>Age, but not severity of injury, mediates decline in executive function: validation of the rodent touchscreen paradigm for preclinical models of traumatic brain injury</article-title>
        <source>Behav. Brain Res.</source>
        <volume>368</volume>
        <year>2019</year>
        <object-id pub-id-type="publisher-id">111912</object-id>
      </element-citation>
    </ref>
    <ref id="bib5">
      <element-citation publication-type="journal" id="sbref5">
        <person-group person-group-type="author">
          <name>
            <surname>Becker</surname>
            <given-names>M.I.</given-names>
          </name>
          <name>
            <surname>Person</surname>
            <given-names>A.L.</given-names>
          </name>
        </person-group>
        <article-title>Cerebellar control of reach kinematics for endpoint precision</article-title>
        <source>Neuron</source>
        <volume>103</volume>
        <year>2019</year>
        <fpage>335</fpage>
        <lpage>348</lpage>
        <comment>e335</comment>
        <pub-id pub-id-type="pmid">31174960</pub-id>
      </element-citation>
    </ref>
    <ref id="bib6">
      <element-citation publication-type="journal" id="sbref6">
        <person-group person-group-type="author">
          <name>
            <surname>Beraldo</surname>
            <given-names>F.H.</given-names>
          </name>
          <name>
            <surname>Palmer</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Memar</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Wasserman</surname>
            <given-names>D.I.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>W.V.</given-names>
          </name>
          <name>
            <surname>Liang</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Creighton</surname>
            <given-names>S.D.</given-names>
          </name>
          <name>
            <surname>Kolisnyk</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Cowan</surname>
            <given-names>M.F.</given-names>
          </name>
          <name>
            <surname>Mels</surname>
            <given-names>J.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>MouseBytes, an open-access high-throughput pipeline and database for rodent touchscreen-based cognitive assessment</article-title>
        <source>eLife</source>
        <volume>8</volume>
        <year>2019</year>
      </element-citation>
    </ref>
    <ref id="bib7">
      <element-citation publication-type="journal" id="sbref7">
        <person-group person-group-type="author">
          <name>
            <surname>Brasted</surname>
            <given-names>P.J.</given-names>
          </name>
          <name>
            <surname>Bussey</surname>
            <given-names>T.J.</given-names>
          </name>
          <name>
            <surname>Murray</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Wise</surname>
            <given-names>S.P.</given-names>
          </name>
        </person-group>
        <article-title>Fornix transection impairs conditional visuomotor learning in tasks involving nonspatially differentiated responses</article-title>
        <source>J. Neurophysiol.</source>
        <volume>87</volume>
        <year>2002</year>
        <fpage>631</fpage>
        <lpage>633</lpage>
        <pub-id pub-id-type="pmid">11784778</pub-id>
      </element-citation>
    </ref>
    <ref id="bib8">
      <element-citation publication-type="journal" id="sbref8">
        <person-group person-group-type="author">
          <name>
            <surname>Brigman</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Graybeal</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Holmes</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Predictably irrational: assaying cognitive inflexibility in mouse models of schizophrenia</article-title>
        <source>Front. Neurosci.</source>
        <volume>4</volume>
        <year>2010</year>
      </element-citation>
    </ref>
    <ref id="bib9">
      <element-citation publication-type="journal" id="sbref9">
        <person-group person-group-type="author">
          <name>
            <surname>Broschard</surname>
            <given-names>M.B.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Love</surname>
            <given-names>B.C.</given-names>
          </name>
          <name>
            <surname>Freeman</surname>
            <given-names>J.H.</given-names>
          </name>
        </person-group>
        <article-title>Category learning in rodents using touchscreen-based tasks</article-title>
        <source>Genes Brain Behav.</source>
        <volume>20</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">e12665</object-id>
      </element-citation>
    </ref>
    <ref id="bib10">
      <element-citation publication-type="journal" id="sbref10">
        <person-group person-group-type="author">
          <name>
            <surname>Buscher</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Ojeda</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Francoeur</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hulyalkar</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Claros</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Tang</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Terry</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Gupta</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Fakhraei</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Ramanathan</surname>
            <given-names>D.S.</given-names>
          </name>
        </person-group>
        <article-title>Open-source raspberry Pi-based operant box for translational behavioral testing in rodents</article-title>
        <source>J. Neurosci. Methods</source>
        <volume>342</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">108761</object-id>
        <pub-id pub-id-type="doi">10.1016/j.jneumeth.2020.108761</pub-id>
      </element-citation>
    </ref>
    <ref id="bib11">
      <element-citation publication-type="journal" id="sbref11">
        <person-group person-group-type="author">
          <name>
            <surname>Bussey</surname>
            <given-names>T.J.</given-names>
          </name>
          <name>
            <surname>Clea Warburton</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Aggleton</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Muir</surname>
            <given-names>J.L.</given-names>
          </name>
        </person-group>
        <article-title>Fornix lesions can facilitate acquisition of the transverse patterning task: a challenge for "configural" theories of hippocampal function</article-title>
        <source>J. Neurosci.</source>
        <volume>18</volume>
        <year>1998</year>
        <fpage>1622</fpage>
        <lpage>1631</lpage>
        <pub-id pub-id-type="pmid">9454867</pub-id>
      </element-citation>
    </ref>
    <ref id="bib12">
      <element-citation publication-type="journal" id="sbref12">
        <person-group person-group-type="author">
          <name>
            <surname>Bussey</surname>
            <given-names>T.J.</given-names>
          </name>
          <name>
            <surname>Muir</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Everitt</surname>
            <given-names>B.J.</given-names>
          </name>
          <name>
            <surname>Robbins</surname>
            <given-names>T.W.</given-names>
          </name>
        </person-group>
        <article-title>Triple dissociation of anterior cingulate, posterior cingulate, and medial frontal cortices on visual discrimination tasks using a touchscreen testing procedure for the rat</article-title>
        <source>Behav. Neurosci.</source>
        <volume>111</volume>
        <year>1997</year>
        <fpage>920</fpage>
        <lpage>936</lpage>
        <pub-id pub-id-type="pmid">9383514</pub-id>
      </element-citation>
    </ref>
    <ref id="bib13">
      <element-citation publication-type="journal" id="sbref13">
        <person-group person-group-type="author">
          <name>
            <surname>Bussey</surname>
            <given-names>T.J.</given-names>
          </name>
          <name>
            <surname>Padain</surname>
            <given-names>T.L.</given-names>
          </name>
          <name>
            <surname>Skillings</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Winters</surname>
            <given-names>B.D.</given-names>
          </name>
          <name>
            <surname>Morton</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Saksida</surname>
            <given-names>L.M.</given-names>
          </name>
        </person-group>
        <article-title>The touchscreen cognitive testing method for rodents: how to get the best out of your rat</article-title>
        <source>Learn Mem.</source>
        <volume>15</volume>
        <year>2008</year>
        <fpage>516</fpage>
        <lpage>523</lpage>
        <pub-id pub-id-type="pmid">18612068</pub-id>
      </element-citation>
    </ref>
    <ref id="bib14">
      <element-citation publication-type="journal" id="sbref14">
        <person-group person-group-type="author">
          <name>
            <surname>Chow</surname>
            <given-names>W.Z.</given-names>
          </name>
          <name>
            <surname>Ong</surname>
            <given-names>L.K.</given-names>
          </name>
          <name>
            <surname>Kluge</surname>
            <given-names>M.G.</given-names>
          </name>
          <name>
            <surname>Gyawali</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Walker</surname>
            <given-names>F.R.</given-names>
          </name>
          <name>
            <surname>Nilsson</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Similar cognitive deficits in mice and humans in the chronic phase post-stroke identified using the touchscreen-based paired-associate learning task</article-title>
        <source>Sci. Rep.</source>
        <volume>10</volume>
        <year>2020</year>
        <fpage>19545</fpage>
        <pub-id pub-id-type="pmid">33177588</pub-id>
      </element-citation>
    </ref>
    <ref id="bib15">
      <element-citation publication-type="journal" id="sbref15">
        <person-group person-group-type="author">
          <name>
            <surname>Copping</surname>
            <given-names>N.A.</given-names>
          </name>
          <name>
            <surname>Berg</surname>
            <given-names>E.L.</given-names>
          </name>
          <name>
            <surname>Foley</surname>
            <given-names>G.M.</given-names>
          </name>
          <name>
            <surname>Schaffler</surname>
            <given-names>M.D.</given-names>
          </name>
          <name>
            <surname>Onaga</surname>
            <given-names>B.L.</given-names>
          </name>
          <name>
            <surname>Buscher</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Silverman</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Touchscreen learning deficits and normal social approach behavior in the Shank3B model of Phelan-McDermid Syndrome and autism</article-title>
        <source>Neuroscience</source>
        <volume>345</volume>
        <year>2017</year>
        <fpage>155</fpage>
        <lpage>165</lpage>
        <pub-id pub-id-type="pmid">27189882</pub-id>
      </element-citation>
    </ref>
    <ref id="bib16">
      <element-citation publication-type="journal" id="sbref16">
        <person-group person-group-type="author">
          <name>
            <surname>Dacre</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Colligan</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Clarke</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Ammer</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Schiemann</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Chamosa-Pino</surname>
            <given-names>V.</given-names>
          </name>
          <name>
            <surname>Claudi</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Harston</surname>
            <given-names>J.A.</given-names>
          </name>
          <name>
            <surname>Eleftheriou</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pakan</surname>
            <given-names>J.M.</given-names>
          </name>
        </person-group>
        <article-title>A cerebellar-thalamocortical pathway drives behavioral context-dependent movement initiation</article-title>
        <source>Neuron</source>
        <volume>109</volume>
        <year>2021</year>
        <fpage>2326</fpage>
        <lpage>2338</lpage>
        <comment>e2328</comment>
        <pub-id pub-id-type="pmid">34146469</pub-id>
      </element-citation>
    </ref>
    <ref id="bib17">
      <element-citation publication-type="journal" id="sbref17">
        <person-group person-group-type="author">
          <name>
            <surname>Delotterie</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Cassel</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Dorner-Ciossek</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Marti</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Optimization of touchscreen-based behavioral paradigms in mice: implications for building a battery of tasks taxing learning and memory functions</article-title>
        <source>PLoS One</source>
        <volume>9</volume>
        <year>2014</year>
        <object-id pub-id-type="publisher-id">e100817</object-id>
      </element-citation>
    </ref>
    <ref id="bib18">
      <element-citation publication-type="journal" id="sbref18">
        <person-group person-group-type="author">
          <name>
            <surname>Dumont</surname>
            <given-names>J.R.</given-names>
          </name>
          <name>
            <surname>Salewski</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Beraldo</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Critical mass: the rise of a touchscreen technology community for rodent cognitive testing</article-title>
        <source>Genes Brain Behav.</source>
        <volume>20</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">e12650</object-id>
      </element-citation>
    </ref>
    <ref id="bib19">
      <element-citation publication-type="journal" id="sbref19">
        <person-group person-group-type="author">
          <name>
            <surname>Fortunato</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Galassi</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>The case for free and open source software in research and scholarship</article-title>
        <source>Philos. Trans. A Math. Phys. Eng. Sci.</source>
        <volume>379</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">20200079</object-id>
      </element-citation>
    </ref>
    <ref id="bib20">
      <element-citation publication-type="journal" id="sbref20">
        <person-group person-group-type="author">
          <name>
            <surname>Freeman</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Open source tools for large-scale neuroscience</article-title>
        <source>Curr. Opin. Neurobiol.</source>
        <volume>32</volume>
        <year>2015</year>
        <fpage>156</fpage>
        <lpage>163</lpage>
        <pub-id pub-id-type="pmid">25982977</pub-id>
      </element-citation>
    </ref>
    <ref id="bib21">
      <element-citation publication-type="journal" id="sbref21">
        <person-group person-group-type="author">
          <name>
            <surname>Galinanes</surname>
            <given-names>G.L.</given-names>
          </name>
          <name>
            <surname>Bonardi</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Directional reaching for water as a cortex-dependent behavioral framework for mice</article-title>
        <source>Cell Rep.</source>
        <volume>22</volume>
        <year>2018</year>
        <fpage>2767</fpage>
        <lpage>2783</lpage>
        <pub-id pub-id-type="pmid">29514103</pub-id>
      </element-citation>
    </ref>
    <ref id="bib22">
      <element-citation publication-type="journal" id="sbref22">
        <person-group person-group-type="author">
          <name>
            <surname>Glover</surname>
            <given-names>L.R.</given-names>
          </name>
          <name>
            <surname>Postle</surname>
            <given-names>A.F.</given-names>
          </name>
          <name>
            <surname>Holmes</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Touchscreen-based assessment of risky-choice in mice</article-title>
        <source>Behav. Brain Res.</source>
        <volume>393</volume>
        <year>2020</year>
        <object-id pub-id-type="publisher-id">112748</object-id>
      </element-citation>
    </ref>
    <ref id="bib23">
      <element-citation publication-type="journal" id="sbref23">
        <person-group person-group-type="author">
          <name>
            <surname>Groman</surname>
            <given-names>S.M.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Seu</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>James</surname>
            <given-names>A.S.</given-names>
          </name>
          <name>
            <surname>Feiler</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Mandelkern</surname>
            <given-names>M.A.</given-names>
          </name>
          <name>
            <surname>London</surname>
            <given-names>E.D.</given-names>
          </name>
          <name>
            <surname>Jentsch</surname>
            <given-names>J.D.</given-names>
          </name>
        </person-group>
        <article-title>Dysregulation of D(2)-mediated dopamine transmission in monkeys after chronic escalating methamphetamine exposure</article-title>
        <source>J. Neurosci.</source>
        <volume>32</volume>
        <year>2012</year>
        <fpage>5843</fpage>
        <lpage>5852</lpage>
        <pub-id pub-id-type="pmid">22539846</pub-id>
      </element-citation>
    </ref>
    <ref id="bib24">
      <element-citation publication-type="journal" id="sbref24">
        <person-group person-group-type="author">
          <name>
            <surname>Gurley</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Two open source designs for a low-cost operant chamber using Raspberry Pi</article-title>
        <source>J. Exp. Anal. Behav.</source>
        <volume>111</volume>
        <year>2019</year>
        <fpage>508</fpage>
        <lpage>518</lpage>
        <pub-id pub-id-type="pmid">31038195</pub-id>
      </element-citation>
    </ref>
    <ref id="bib25">
      <element-citation publication-type="journal" id="sbref25">
        <person-group person-group-type="author">
          <name>
            <surname>Haddad</surname>
            <given-names>F.L.</given-names>
          </name>
          <name>
            <surname>Ghahremani</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>De Oliveira</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Doornaert</surname>
            <given-names>E.E.</given-names>
          </name>
          <name>
            <surname>Johnston</surname>
            <given-names>K.D.</given-names>
          </name>
          <name>
            <surname>Everling</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Schmid</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>A novel three-choice touchscreen task to examine spatial attention and orienting responses in rodents</article-title>
        <source>eNeuro</source>
        <volume>8</volume>
        <year>2021</year>
      </element-citation>
    </ref>
    <ref id="bib26">
      <mixed-citation publication-type="other" id="othref0005">Hall, C., Ballachey, E.L. , 1932. A study of the rat's behavior in a field. A Contribution to Method in Comparative Psychology, University of California Publications in Psychology.</mixed-citation>
    </ref>
    <ref id="bib27">
      <element-citation publication-type="journal" id="sbref26">
        <person-group person-group-type="author">
          <name>
            <surname>Heath</surname>
            <given-names>C.J.</given-names>
          </name>
          <name>
            <surname>O'Callaghan</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Mason</surname>
            <given-names>S.L.</given-names>
          </name>
          <name>
            <surname>Phillips</surname>
            <given-names>B.U.</given-names>
          </name>
          <name>
            <surname>Saksida</surname>
            <given-names>L.M.</given-names>
          </name>
          <name>
            <surname>Robbins</surname>
            <given-names>T.W.</given-names>
          </name>
          <name>
            <surname>Barker</surname>
            <given-names>R.A.</given-names>
          </name>
          <name>
            <surname>Bussey</surname>
            <given-names>T.J.</given-names>
          </name>
          <name>
            <surname>Sahakian</surname>
            <given-names>B.J.</given-names>
          </name>
        </person-group>
        <article-title>A touchscreen motivation assessment evaluated in huntington's disease patients and R6/1 model mice</article-title>
        <source>Front. Neurol.</source>
        <volume>10</volume>
        <year>2019</year>
        <fpage>858</fpage>
        <pub-id pub-id-type="pmid">31447770</pub-id>
      </element-citation>
    </ref>
    <ref id="bib28">
      <element-citation publication-type="journal" id="sbref27">
        <person-group person-group-type="author">
          <name>
            <surname>Hvoslef-Eide</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Nilsson</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Saksida</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Bussey</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Cognitive translation using the rodent touchscreen testing approach</article-title>
        <source>Transl. Neuropsychopharmacol.</source>
        <year>2015</year>
        <fpage>423</fpage>
        <lpage>447</lpage>
      </element-citation>
    </ref>
    <ref id="bib29">
      <element-citation publication-type="journal" id="sbref28">
        <person-group person-group-type="author">
          <name>
            <surname>Kawai</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Markman</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Poddar</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Ko</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Fantana</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Dhawale</surname>
            <given-names>A.K.</given-names>
          </name>
          <name>
            <surname>Kampff</surname>
            <given-names>A.R.</given-names>
          </name>
          <name>
            <surname>Olveczky</surname>
            <given-names>B.P.</given-names>
          </name>
        </person-group>
        <article-title>Motor cortex is required for learning but not for executing a motor skill</article-title>
        <source>Neuron</source>
        <volume>86</volume>
        <year>2015</year>
        <fpage>800</fpage>
        <lpage>812</lpage>
        <pub-id pub-id-type="pmid">25892304</pub-id>
      </element-citation>
    </ref>
    <ref id="bib30">
      <mixed-citation publication-type="other" id="othref0010">Kessenich, J., Sellers, G., Shreiner, D. , 2016. OpenGL Programming Guide: the Official Guide to Learning OpenGL, Version 4.5 with SPIR-V, Addison-Wesley Professional.</mixed-citation>
    </ref>
    <ref id="bib31">
      <element-citation publication-type="journal" id="sbref29">
        <person-group person-group-type="author">
          <name>
            <surname>Kim</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Castro</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Wasserman</surname>
            <given-names>E.A.</given-names>
          </name>
          <name>
            <surname>Freeman</surname>
            <given-names>J.H.</given-names>
          </name>
        </person-group>
        <article-title>Dorsal hippocampus is necessary for visual categorization in rats</article-title>
        <source>Hippocampus</source>
        <volume>28</volume>
        <year>2018</year>
        <fpage>392</fpage>
        <lpage>405</lpage>
        <pub-id pub-id-type="pmid">29473984</pub-id>
      </element-citation>
    </ref>
    <ref id="bib32">
      <element-citation publication-type="journal" id="sbref30">
        <person-group person-group-type="author">
          <name>
            <surname>Leach</surname>
            <given-names>P.T.</given-names>
          </name>
          <name>
            <surname>Crawley</surname>
            <given-names>J.N.</given-names>
          </name>
        </person-group>
        <article-title>Touchscreen learning deficits in Ube3a, Ts65Dn and Mecp2 mouse models of neurodevelopmental disorders with intellectual disabilities</article-title>
        <source>Genes Brain Behav.</source>
        <volume>17</volume>
        <year>2018</year>
        <object-id pub-id-type="publisher-id">e12452</object-id>
      </element-citation>
    </ref>
    <ref id="bib33">
      <element-citation publication-type="journal" id="sbref31">
        <person-group person-group-type="author">
          <name>
            <surname>Leach</surname>
            <given-names>P.T.</given-names>
          </name>
          <name>
            <surname>Hayes</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Pride</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Silverman</surname>
            <given-names>J.L.</given-names>
          </name>
          <name>
            <surname>Crawley</surname>
            <given-names>J.N.</given-names>
          </name>
        </person-group>
        <article-title>Normal performance of Fmr1 mice on a touchscreen delayed nonmatching to position working memory task</article-title>
        <source>eNeuro</source>
        <volume>3</volume>
        <year>2016</year>
      </element-citation>
    </ref>
    <ref id="bib34">
      <element-citation publication-type="journal" id="sbref32">
        <person-group person-group-type="author">
          <name>
            <surname>Markham</surname>
            <given-names>M.R.</given-names>
          </name>
          <name>
            <surname>Butt</surname>
            <given-names>A.E.</given-names>
          </name>
          <name>
            <surname>Dougher</surname>
            <given-names>M.J.</given-names>
          </name>
        </person-group>
        <article-title>A computer touch-screen apparatus for training visual discriminations in rats</article-title>
        <source>J. Exp. Anal. Behav.</source>
        <volume>65</volume>
        <year>1996</year>
        <fpage>173</fpage>
        <lpage>182</lpage>
        <pub-id pub-id-type="pmid">8583196</pub-id>
      </element-citation>
    </ref>
    <ref id="bib35">
      <element-citation publication-type="journal" id="sbref33">
        <person-group person-group-type="author">
          <name>
            <surname>Morton</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Skillings</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Bussey</surname>
            <given-names>T.J.</given-names>
          </name>
          <name>
            <surname>Saksida</surname>
            <given-names>L.M.</given-names>
          </name>
        </person-group>
        <article-title>Measuring cognitive deficits in disabled mice using an automated interactive touchscreen system</article-title>
        <source>Nat. Methods</source>
        <volume>3</volume>
        <year>2006</year>
        <fpage>767</fpage>
        <pub-id pub-id-type="pmid">16990806</pub-id>
      </element-citation>
    </ref>
    <ref id="bib36">
      <element-citation publication-type="journal" id="sbref34">
        <person-group person-group-type="author">
          <name>
            <surname>Nithianantharajah</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>McKechanie</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Stewart</surname>
            <given-names>T.J.</given-names>
          </name>
          <name>
            <surname>Johnstone</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Blackwood</surname>
            <given-names>D.H.</given-names>
            <suffix>St</suffix>
          </name>
          <name>
            <surname>Clair</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Grant</surname>
            <given-names>S.G.</given-names>
          </name>
          <name>
            <surname>Bussey</surname>
            <given-names>T.J.</given-names>
          </name>
          <name>
            <surname>Saksida</surname>
            <given-names>L.M.</given-names>
          </name>
        </person-group>
        <article-title>Bridging the translational divide: identical cognitive touchscreen testing in mice and humans carrying mutations in a disease-relevant homologous gene</article-title>
        <source>Sci. Rep.</source>
        <volume>5</volume>
        <year>2015</year>
        <fpage>14613</fpage>
        <pub-id pub-id-type="pmid">26423861</pub-id>
      </element-citation>
    </ref>
    <ref id="bib37">
      <element-citation publication-type="journal" id="sbref35">
        <person-group person-group-type="author">
          <name>
            <surname>Norris</surname>
            <given-names>R.H.C.</given-names>
          </name>
          <name>
            <surname>Churilov</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Hannan</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Nithianantharajah</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Mutations in neuroligin-3 in male mice impact behavioral flexibility but not relational memory in a touchscreen test of visual transitive inference</article-title>
        <source>Mol. Autism</source>
        <volume>10</volume>
        <year>2019</year>
        <fpage>42</fpage>
        <pub-id pub-id-type="pmid">31827744</pub-id>
      </element-citation>
    </ref>
    <ref id="bib38">
      <element-citation publication-type="journal" id="sbref36">
        <person-group person-group-type="author">
          <name>
            <surname>Odland</surname>
            <given-names>A.U.</given-names>
          </name>
          <name>
            <surname>Sandahl</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Andreasen</surname>
            <given-names>J.T.</given-names>
          </name>
        </person-group>
        <article-title>Sequential reversal learning: a new touchscreen schedule for assessing cognitive flexibility in mice</article-title>
        <source>Psychopharmacology</source>
        <volume>238</volume>
        <year>2021</year>
        <fpage>383</fpage>
        <lpage>397</lpage>
        <pub-id pub-id-type="pmid">33123820</pub-id>
      </element-citation>
    </ref>
    <ref id="bib39">
      <element-citation publication-type="journal" id="sbref37">
        <person-group person-group-type="author">
          <name>
            <surname>O'Leary</surname>
            <given-names>J.D.</given-names>
          </name>
          <name>
            <surname>O'Leary</surname>
            <given-names>O.F.</given-names>
          </name>
          <name>
            <surname>Cryan</surname>
            <given-names>J.F.</given-names>
          </name>
          <name>
            <surname>Nolan</surname>
            <given-names>Y.M.</given-names>
          </name>
        </person-group>
        <article-title>A low-cost touchscreen operant chamber using a Raspberry Pi</article-title>
        <source>Behav. Res. Methods</source>
        <volume>50</volume>
        <year>2018</year>
        <fpage>2523</fpage>
        <lpage>2530</lpage>
        <pub-id pub-id-type="pmid">29520633</pub-id>
      </element-citation>
    </ref>
    <ref id="bib40">
      <element-citation publication-type="journal" id="sbref38">
        <person-group person-group-type="author">
          <name>
            <surname>Peirce</surname>
            <given-names>J.W.</given-names>
          </name>
        </person-group>
        <article-title>PsychoPy--psychophysics software in Python</article-title>
        <source>J. Neurosci. Methods</source>
        <volume>162</volume>
        <year>2007</year>
        <fpage>8</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="pmid">17254636</pub-id>
      </element-citation>
    </ref>
    <ref id="bib41">
      <element-citation publication-type="journal" id="sbref39">
        <person-group person-group-type="author">
          <name>
            <surname>Piantadosi</surname>
            <given-names>P.T.</given-names>
          </name>
          <name>
            <surname>Lieberman</surname>
            <given-names>A.G.</given-names>
          </name>
          <name>
            <surname>Pickens</surname>
            <given-names>C.L.</given-names>
          </name>
          <name>
            <surname>Bergstrom</surname>
            <given-names>H.C.</given-names>
          </name>
          <name>
            <surname>Holmes</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>A novel multichoice touchscreen paradigm for assessing cognitive flexibility in mice</article-title>
        <source>Learn Mem.</source>
        <volume>26</volume>
        <year>2019</year>
        <fpage>24</fpage>
        <lpage>30</lpage>
        <pub-id pub-id-type="pmid">30559117</pub-id>
      </element-citation>
    </ref>
    <ref id="bib42">
      <element-citation publication-type="journal" id="sbref40">
        <person-group person-group-type="author">
          <name>
            <surname>Pineno</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>ArduiPod Box: a low-cost and open-source Skinner box using an iPod Touch and an Arduino microcontroller</article-title>
        <source>Behav. Res Methods</source>
        <volume>46</volume>
        <year>2014</year>
        <fpage>196</fpage>
        <lpage>205</lpage>
        <pub-id pub-id-type="pmid">23813238</pub-id>
      </element-citation>
    </ref>
    <ref id="bib43">
      <mixed-citation publication-type="other" id="othref0015">Rübel, O., Tritt, A., Dichter, B., Braun, T., Cain, N., Clack, N., Davidson, T., Dougherty, M., Fillion-Robin, J., Graddis, N. , 2019. NWB: N 2.0: an accessible data standard for neurophysiology. bioRxiv, 523035.</mixed-citation>
    </ref>
    <ref id="bib44">
      <element-citation publication-type="journal" id="sbref41">
        <person-group person-group-type="author">
          <name>
            <surname>Seitz</surname>
            <given-names>B.M.</given-names>
          </name>
          <name>
            <surname>McCune</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>MacPherson</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bergeron</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Blaisdell</surname>
            <given-names>A.P.</given-names>
          </name>
          <name>
            <surname>Logan</surname>
            <given-names>C.J.</given-names>
          </name>
        </person-group>
        <article-title>Using touchscreen equipped operant chambers to study animal cognition. Benefits, limitations, and advice</article-title>
        <source>PLoS One</source>
        <volume>16</volume>
        <year>2021</year>
        <object-id pub-id-type="publisher-id">e0246446</object-id>
      </element-citation>
    </ref>
    <ref id="bib45">
      <mixed-citation publication-type="other" id="othref0020">Skinner, B. , 1938. The Behavior of Organisms: an Experimental Analysis (New York: Appleton-Century, 1938). Department of Psychology Virginia Polytechnic Institute Blacksburg, Virginia 24061.</mixed-citation>
    </ref>
    <ref id="bib46">
      <element-citation publication-type="journal" id="sbref42">
        <person-group person-group-type="author">
          <name>
            <surname>Stanislaw</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Todorov</surname>
            <given-names>N.</given-names>
          </name>
        </person-group>
        <article-title>Calculation of signal detection theory measures</article-title>
        <source>Behav. Res. Methods, Instrum., Comput.</source>
        <volume>31</volume>
        <year>1999</year>
        <fpage>137</fpage>
        <lpage>149</lpage>
        <pub-id pub-id-type="pmid">10495845</pub-id>
      </element-citation>
    </ref>
    <ref id="bib47">
      <element-citation publication-type="journal" id="sbref43">
        <person-group person-group-type="author">
          <name>
            <surname>Stirman</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Townsend</surname>
            <given-names>L.B.</given-names>
          </name>
          <name>
            <surname>Smith</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>A touchscreen based global motion perception task for mice</article-title>
        <source>Vis. Res.</source>
        <volume>127</volume>
        <year>2016</year>
        <fpage>74</fpage>
        <lpage>83</lpage>
        <pub-id pub-id-type="pmid">27497283</pub-id>
      </element-citation>
    </ref>
    <ref id="bib48">
      <element-citation publication-type="journal" id="sbref44">
        <person-group person-group-type="author">
          <name>
            <surname>Talpos</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Steckler</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Touching on translation</article-title>
        <source>Cell Tissue Res.</source>
        <volume>354</volume>
        <year>2013</year>
        <fpage>297</fpage>
        <lpage>308</lpage>
        <pub-id pub-id-type="pmid">23949375</pub-id>
      </element-citation>
    </ref>
    <ref id="bib49">
      <element-citation publication-type="journal" id="sbref45">
        <person-group person-group-type="author">
          <name>
            <surname>Talpos</surname>
            <given-names>J.C.</given-names>
          </name>
          <name>
            <surname>Dias</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Bussey</surname>
            <given-names>T.J.</given-names>
          </name>
          <name>
            <surname>Saksida</surname>
            <given-names>L.M.</given-names>
          </name>
        </person-group>
        <article-title>Hippocampal lesions in rats impair learning and memory for locations on a touch-sensitive computer screen: the "ASAT" task</article-title>
        <source>Behav. Brain Res.</source>
        <volume>192</volume>
        <year>2008</year>
        <fpage>216</fpage>
        <lpage>225</lpage>
        <pub-id pub-id-type="pmid">18499279</pub-id>
      </element-citation>
    </ref>
    <ref id="bib50">
      <element-citation publication-type="journal" id="sbref46">
        <person-group person-group-type="author">
          <name>
            <surname>Van den Broeck</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Hansquine</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Callaerts-Vegh</surname>
            <given-names>Z.</given-names>
          </name>
          <name>
            <surname>D'Hooge</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Impaired reversal learning in APPPS1-21 mice in the touchscreen visual discrimination task</article-title>
        <source>Front. Behav. Neurosci.</source>
        <volume>13</volume>
        <year>2019</year>
        <fpage>92</fpage>
        <pub-id pub-id-type="pmid">31143103</pub-id>
      </element-citation>
    </ref>
    <ref id="bib51">
      <element-citation publication-type="journal" id="sbref47">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Lewis</surname>
            <given-names>F.C.</given-names>
          </name>
          <name>
            <surname>Sarvi</surname>
            <given-names>M.S.</given-names>
          </name>
          <name>
            <surname>Foley</surname>
            <given-names>G.M.</given-names>
          </name>
          <name>
            <surname>Crawley</surname>
            <given-names>J.N.</given-names>
          </name>
        </person-group>
        <article-title>16p11.2 Deletion mice display cognitive deficits in touchscreen learning and novelty recognition tasks</article-title>
        <source>Learn Mem.</source>
        <volume>22</volume>
        <year>2015</year>
        <fpage>622</fpage>
        <lpage>632</lpage>
        <pub-id pub-id-type="pmid">26572653</pub-id>
      </element-citation>
    </ref>
    <ref id="bib52">
      <element-citation publication-type="journal" id="sbref48">
        <person-group person-group-type="author">
          <name>
            <surname>Zeleznikow-Johnston</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Renoir</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Churilov</surname>
            <given-names>L.</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Burrows</surname>
            <given-names>E.L.</given-names>
          </name>
          <name>
            <surname>Hannan</surname>
            <given-names>A.J.</given-names>
          </name>
        </person-group>
        <article-title>Touchscreen testing reveals clinically relevant cognitive abnormalities in a mouse model of schizophrenia lacking metabotropic glutamate receptor 5</article-title>
        <source>Sci. Rep.</source>
        <volume>8</volume>
        <year>2018</year>
        <fpage>16412</fpage>
        <pub-id pub-id-type="pmid">30401923</pub-id>
      </element-citation>
    </ref>
  </ref-list>
  <sec sec-type="data-availability" id="da0005">
    <title>Data availability</title>
    <p id="p0025">Data will be made available on request.</p>
  </sec>
  <ack id="ack0005">
    <title>Acknowledgements</title>
    <p id="p0205">We are grateful to members of the Duguid lab for experimental discussions and comments on the manuscript. This study was supported by the <funding-source id="gs2">Simons Initiative for the Developing Brain Ph.D. Studentship</funding-source> (C.E.) and a <funding-source id="gs1">Wellcome Senior Research Fellowship</funding-source> (UK) (110131/Z/15/Z) to I.D.</p>
  </ack>
</back>
