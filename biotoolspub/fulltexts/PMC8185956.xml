<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Physiol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Physiol</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Physiol.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Physiology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1664-042X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8185956</article-id>
    <article-id pub-id-type="doi">10.3389/fphys.2021.662314</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Physiology</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>BayCANN: Streamlining Bayesian Calibration With Artificial Neural Network Metamodeling</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Jalal</surname>
          <given-names>Hawre</given-names>
        </name>
        <xref ref-type="aff" rid="aff1">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="c001">
          <sup>*</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1218635/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Trikalinos</surname>
          <given-names>Thomas A.</given-names>
        </name>
        <xref ref-type="aff" rid="aff2">
          <sup>2</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1330046/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Alarid-Escudero</surname>
          <given-names>Fernando</given-names>
        </name>
        <xref ref-type="aff" rid="aff3">
          <sup>3</sup>
        </xref>
        <uri xlink:type="simple" xlink:href="http://loop.frontiersin.org/people/1311573/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Department of Health Policy and Management, University of Pittsburgh, Graduate School of Public Health</institution>, <addr-line>Pittsburgh, PA</addr-line>, <country>United States</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Departments of Health Services, Policy &amp; Practice and Biostatistics, Brown University</institution>, <addr-line>Providence, RI</addr-line>, <country>United States</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Division of Public Administration, Center for Research and Teaching in Economics (CIDE)</institution>, <addr-line>Aguascalientes</addr-line>, <country>Mexico</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p>Edited by: Michael Döllinger, University Hospital Erlangen, Germany</p>
      </fn>
      <fn fn-type="edited-by">
        <p>Reviewed by: Koen Degeling, The University of Melbourne, Australia; Dominic G. Whittaker, University of Nottingham, United Kingdom</p>
      </fn>
      <corresp id="c001">*Correspondence: Hawre Jalal <email>hjalal@pitt.edu</email></corresp>
      <fn fn-type="other" id="fn001">
        <p>This article was submitted to Computational Physiology and Medicine, a section of the journal Frontiers in Physiology</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>25</day>
      <month>5</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>12</volume>
    <elocation-id>662314</elocation-id>
    <history>
      <date date-type="received">
        <day>01</day>
        <month>2</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>20</day>
        <month>4</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2021 Jalal, Trikalinos and Alarid-Escudero.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>Jalal, Trikalinos and Alarid-Escudero</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p><bold>Purpose:</bold> Bayesian calibration is generally superior to standard direct-search algorithms in that it estimates the full joint posterior distribution of the calibrated parameters. However, there are many barriers to using Bayesian calibration in health decision sciences stemming from the need to program complex models in probabilistic programming languages and the associated computational burden of applying Bayesian calibration. In this paper, we propose to use artificial neural networks (ANN) as one practical solution to these challenges.</p>
      <p><bold>Methods:</bold> Bayesian Calibration using Artificial Neural Networks (BayCANN) involves (1) training an ANN metamodel on a sample of model inputs and outputs, and (2) then calibrating the trained ANN metamodel instead of the full model in a probabilistic programming language to obtain the posterior joint distribution of the calibrated parameters. We illustrate BayCANN using a colorectal cancer natural history model. We conduct a confirmatory simulation analysis by first obtaining parameter estimates from the literature and then using them to generate adenoma prevalence and cancer incidence targets. We compare the performance of BayCANN in recovering these “true” parameter values against performing a Bayesian calibration directly on the simulation model using an incremental mixture importance sampling (IMIS) algorithm.</p>
      <p><bold>Results:</bold> We were able to apply BayCANN using only a dataset of the model inputs and outputs and minor modification of BayCANN's code. In this example, BayCANN was slightly more accurate in recovering the true posterior parameter estimates compared to IMIS. Obtaining the dataset of samples, and running BayCANN took 15 min compared to the IMIS which took 80 min. In applications involving computationally more expensive simulations (e.g., microsimulations), BayCANN may offer higher relative speed gains.</p>
      <p><bold>Conclusions:</bold> BayCANN only uses a dataset of model inputs and outputs to obtain the calibrated joint parameter distributions. Thus, it can be adapted to models of various levels of complexity with minor or no change to its structure. In addition, BayCANN's efficiency can be especially useful in computationally expensive models. To facilitate BayCANN's wider adoption, we provide BayCANN's open-source implementation in R and Stan.</p>
    </abstract>
    <kwd-group>
      <kwd>Bayesian calibration</kwd>
      <kwd>machine learning</kwd>
      <kwd>mechanistic models</kwd>
      <kwd>artificial neural networks</kwd>
      <kwd>emulators</kwd>
      <kwd>surrogate models</kwd>
      <kwd>metamodels</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">National Institute on Drug Abuse<named-content content-type="fundref-id">10.13039/100000026</named-content></funding-source>
        <award-id rid="cn001">K01DA048985</award-id>
      </award-group>
      <award-group>
        <funding-source id="cn002">National Cancer Institute<named-content content-type="fundref-id">10.13039/100000054</named-content></funding-source>
        <award-id rid="cn002">U01-CA-199335</award-id>
        <award-id rid="cn002">U01-CA-253913</award-id>
      </award-group>
    </funding-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="2"/>
      <equation-count count="5"/>
      <ref-count count="73"/>
      <page-count count="13"/>
      <word-count count="7972"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>1. Background</title>
    <p>Modelers and decision-makers often use mathematical simulation models to simplify real-life complexity and inform decisions, particularly those for which uncertainty is inherent. However, some of the model parameters might be either unobserved or unobservable due to various financial, practical or ethical reasons. For example, a model that simulates the natural history of cancer progression may lack an estimate for the rate at which an individual transitions from a pre-symptomatic cancer state to becoming symptomatic. Although this rate might not be directly observable, it may be estimated using a technique commonly referred to as calibration (Alarid-Escudero et al., <xref rid="B1" ref-type="bibr">2018</xref>; Vanni et al., <xref rid="B68" ref-type="bibr">2011</xref>; Rutter et al., <xref rid="B60" ref-type="bibr">2009</xref>). Thus, calibration involves modifying the model input parameters until the desired output is obtained.</p>
    <p>Calibration has the potential for improving model inference, and recent guidelines recommend that model calibration of unknown parameters should be performed where data on outputs exist (Weinstein et al., <xref rid="B70" ref-type="bibr">2003</xref>; Briggs et al., <xref rid="B9" ref-type="bibr">2012</xref>). Modelers are also encouraged to report the uncertainty around calibrated parameters and use these uncertainties in both deterministic and probabilistic sensitivity analyses (Briggs et al., <xref rid="B9" ref-type="bibr">2012</xref>).</p>
    <p>There are several calibration techniques with various levels of complexity. For example, Nelder-Mead is a direct-search algorithm commonly used to calibrate models in health and medicine. Nelder-Mead is a deterministic approach that searches the parameter space for good-fitting parameter values (Nelder and Mead, <xref rid="B47" ref-type="bibr">1965</xref>). Although Nelder-Mead is generally effective, it cannot estimate parameter distributions or directly inform on the correlations among the calibrated parameters. It is also not guaranteed to find a global optimal value because it might converge to a local optimum. Unlike the direct-search algorithms, Bayesian methods are naturally suited for calibration because they estimate the input parameter's posterior joint and marginal distributions (Menzies et al., <xref rid="B44" ref-type="bibr">2017</xref>). However, Bayesian methods are difficult to implement due to the complexity of the models used and the computational challenges of applying these methods. Bayesian calibration often requires tens or hundreds of thousands of simulation runs and a model written in a probabilistic programming language, such as Stan (Carpenter et al., <xref rid="B10" ref-type="bibr">2017</xref>) or Bayesian inference Using Gibbs Sampling (BUGS) (Lunn et al., <xref rid="B41" ref-type="bibr">2009</xref>). We argue that the complexity of these tasks and their potential computational demand have prevented a wider adoption of Bayesian calibration methods in health decision science models.</p>
    <p>In this manuscript, we use artificial neural network (ANN) metamodels as a practical approach to streamlining Bayesian calibration in complex simulation models. Metamodels have increasingly been used to overcome the computational burden of Bayesian calibration. A metamodel is a surrogate model that can be used to approximate the model's input-output relationship (Jalal et al., <xref rid="B25" ref-type="bibr">2013</xref>). Metamodels can provide an approximation to the simulation model in a fraction of the time. While ANN metamodels are not fully probabilistic, they are flexible functions that can map highly non-linear relationships in large data. We use an ANN metamodel as an emulator to substitute the simulation model in the Bayesian calibration analysis. Thus, the ANN acts as a low computational cost proxy of the simulation model. In addition, analysts do not need to program their simulation model in a probabilistic language because coding the ANN in probabilistic languages (e.g., Stan) is relatively straight-forward, and analysts can reuse the provided Stan code with little or no modification.</p>
    <p>We refer to our approach as Bayesian calibration via artificial neural networks, or BayCANN for short. We demonstrate BayCANN by calibrating a realistic model of the natural history of colorectal cancer (CRC). We compare this approach's results to an approximate Bayesian calibration of the original model using an incremental mixture importance sampling (IMIS) algorithm. We provide the code in R and Stan for our application that researchers can adapt to calibrate their models.</p>
  </sec>
  <sec sec-type="methods" id="s2">
    <title>2. Methods</title>
    <p>We start this exposition by reviewing elements of Bayesian calibration. We describe the computational burden of using Bayes theorem in most realistic models, and how deep ANNs can streamline Bayesian calibration methods to calibrate these models. We illustrate this approach by calibrating a natural history model of CRC. We also compare BayCANN's performance to a Bayesian calibration using IMIS directly on a simulation model.</p>
    <sec>
      <title>2.1. Bayesian Calibration</title>
      <p>The Bayes theorem states that
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>|</mml:mo><mml:mtext class="textrm" mathvariant="normal">data</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext class="textrm" mathvariant="normal">data</mml:mtext><mml:mo>|</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where θ is a set of model parameters, data is the observed data, and <italic>p</italic>(data|θ) is the same as the likelihood <italic>l</italic>(θ|data). Because the denominator is not a function of θ, we can rewrite Equation (1) as
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>|</mml:mo><mml:mtext class="textrm" mathvariant="normal">data</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>|</mml:mo><mml:mtext class="textrm" mathvariant="normal">data</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      <p><xref rid="T1" ref-type="table">Table 1</xref> shows how each term in Equation (2) can be mapped to a component in a calibration exercise. The prior distribution, <italic>p</italic>(θ), represents our uncertainty about the distribution of the model parameters before calibrating the model. Modelers often use various distributions to describe this uncertainty, including beta or logit-normal distribution for probabilities, gamma for rates, or log-normal distributions for rates or hazard ratios. Thus, we can think of a prior distribution as the uncertainty of the pre-calibrated model input parameters. For example, we can represent a vague distribution by a uniform distribution where all the values are equally likely within a defined range.</p>
      <table-wrap id="T1" position="float">
        <label>Table 1</label>
        <caption>
          <p>The Bayes formula in a calibration context.</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Term</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Bayesian context</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Calibration context</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>p</italic>(θ)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Prior distribution of the model input parameters θ</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Pre-calibrated model input parameters</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>p</italic>(θ|data)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Posterior distribution of the model parameters θ given observed data</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Calibrated model parameters to target data</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1"><italic>l</italic>(θ|data)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Probability of the data given model parameters θ (model likelihood)</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Objective function or goodness-of-fit measure; how well the model output fits the target data given a particular value of θ</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Bayesian calibration will update the prior distribution based on the observed target data. The term <italic>p</italic>(θ|<italic>data</italic>) is called the posterior distribution, representing the updated distribution of θ after observing some data. The posterior distribution is equivalent to the calibrated parameter distribution when the data are the calibration targets.</p>
      <p>The likelihood function, <italic>l</italic>(θ|<italic>data</italic>), denotes how likely the observed data arise from a given data generation mechanism with a parameter set values θ. From a simulation modeling perspective, <italic>l</italic>(θ|<italic>data</italic>) is equivalent to measuring the goodness of the model output fit to the calibration targets given a simulation model's input parameter set θ.</p>
      <p>Thus, we can map all components of Bayes theorem to calibration components and use Bayesian inference to obtain the calibrated parameter distributions (a.k.a. the posterior distributions).</p>
      <p>Bayesian calibration is often challenging to adopt in practice in health decision science models. The main challenge lies in the complexity of applying Equation (2). Specifically, an analytical solution for <italic>p</italic>(θ|data) is unlikely to exist for most realistic simulation models. Thus, specialized algorithms, such as Markov-Chain Monte-Carlo (MCMC) might be necessary at the expense of being practically challenging to implement for complex models and computationally expensive.</p>
    </sec>
    <sec>
      <title>2.2. Metamodels</title>
      <p>To overcome the computational and practical challenges of Bayesian calibration, we propose to use artificial neural network (ANN) metamodels. As described above, a metamodel is a surrogate model that approximates the relationship between the simulation model's inputs and outputs (i.e., a metamodel is a model of the model) (Blanning, <xref rid="B8" ref-type="bibr">1974</xref>; Kleijnen, <xref rid="B33" ref-type="bibr">1975</xref>; Kleijnen et al., <xref rid="B37" ref-type="bibr">2005</xref>; Kleijnen, <xref rid="B36" ref-type="bibr">2015</xref>). Metamodels range from simple models, such as linear regressions, to complex non-linear models, such as artificial neural networks (ANN). Although linear regression models are the most common form of metamodels (Barton and Meckesheimer, <xref rid="B7" ref-type="bibr">2006</xref>; Barton, <xref rid="B6" ref-type="bibr">2009</xref>; Sacks et al., <xref rid="B64" ref-type="bibr">1989</xref>; Fu, <xref rid="B18" ref-type="bibr">1994</xref>; Weiser Friedman, <xref rid="B71" ref-type="bibr">1996</xref>; Banks, <xref rid="B4" ref-type="bibr">1998</xref>; Kleijnen and Sargent, <xref rid="B34" ref-type="bibr">2000</xref>; Jalal et al., <xref rid="B25" ref-type="bibr">2013</xref>, <xref rid="B24" ref-type="bibr">2015</xref>), in this paper we focus on ANN because they are more flexible while still being relatively simple to implement in Stan or BUGS.</p>
      <p>Metamodels are often used because they generally offer a vast reduction in computation time (Kleijnen, <xref rid="B35" ref-type="bibr">1979</xref>; Friedman and Pressman, <xref rid="B17" ref-type="bibr">1988</xref>; Barton, <xref rid="B5" ref-type="bibr">1992</xref>; Weiser Friedman, <xref rid="B71" ref-type="bibr">1996</xref>; O'Hagan et al., <xref rid="B49" ref-type="bibr">1999</xref>; Barton and Meckesheimer, <xref rid="B7" ref-type="bibr">2006</xref>; Santos and Santos, <xref rid="B65" ref-type="bibr">2007</xref>; Reis dos Santos and Reis dos Santos, <xref rid="B58" ref-type="bibr">2009</xref>; Khuri and Mukhopadhyay, <xref rid="B29" ref-type="bibr">2010</xref>). For example, a model that takes several hours or even days to run can be approximated with a metamodel that may only take a few milliseconds. This feature has been an attractive attribute of metamodels for many decades in engineering and computer science. Examples of metamodels in health decision sciences involve revealing model uncertainty using linear regression mdetamodeling (Jalal et al., <xref rid="B25" ref-type="bibr">2013</xref>), and speeding up computationally expensive microsimulation models using Gaussian processes metamodeling (Stevenson et al., <xref rid="B67" ref-type="bibr">2004</xref>; de Carvalho et al., <xref rid="B12" ref-type="bibr">2019</xref>).</p>
      <p>An additional benefit of using metamodels for Bayesian calibration is that one can reuse the same metamodel structure to calibrate very different simulation models. The same BayCANN code can be adapted to other problems with no or minimal change.</p>
      <sec>
        <title>2.2.1. ANN Metamodels</title>
        <p>Artificial neural networks (ANNs) are networks of non-linear regressions that were initially developed to mimic the neural signal processing in the brain and to model how the nervous system processes complex information (Másson and Wang, <xref rid="B43" ref-type="bibr">1990</xref>; Michie et al., <xref rid="B45" ref-type="bibr">1994</xref>; Rojas, <xref rid="B59" ref-type="bibr">1996</xref>; Jain et al., <xref rid="B23" ref-type="bibr">1996</xref>; Olden et al., <xref rid="B50" ref-type="bibr">2008</xref>). ANNs have recently witnessed significant advances for applications in machine learning, artificial intelligence, and pattern recognition (Ravì et al., <xref rid="B57" ref-type="bibr">2016</xref>).</p>
        <p><xref ref-type="fig" rid="F1">Figure 1</xref> illustrates the basic structure of a four-layer neural network with two hidden layers with <italic>I</italic> neurons (nodes) in the input layer, <italic>J</italic> hidden nodes in the first hidden layer, <italic>K</italic> hidden nodes in the second hidden layer, and <italic>O</italic> output nodes in the output layer. The ANNs with more than one hidden layer are often referred to as deep ANNs. The following sets of equations represent the structure of this ANN
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>    </mml:mtext><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where θ is the simulation model inputs, <italic>Y</italic> is the model outputs to be compared to the calibrated targets, and (<italic>W, b</italic>) = (<italic>W</italic><sup>(1)</sup>, <italic>b</italic><sup>(1)</sup>, <italic>W</italic><sup>(2)</sup>, <italic>b</italic><sup>(2)</sup>, <italic>W</italic><sup>(3)</sup>, <italic>b</italic><sup>(3)</sup>) are the ANN coefficients. <italic>W</italic><sup>(1)</sup> are the weights connecting the inputs θ with the nodes in the first hidden layer, <italic>W</italic><sup>(2)</sup> are the weights connecting the nodes in the first and second hidden layers, and <italic>W</italic><sup>(3)</sup> are the weights connecting the nodes in the second hidden layer with the output <italic>Y</italic>. The terms <italic>b</italic><sup>(1)</sup>, <italic>b</italic><sup>(2)</sup> and <italic>b</italic><sup>(3)</sup> are corresponding bias (intercept) terms. <italic>f</italic><sup>(1)</sup> is the activation function, commonly, a sigmoid function such as a hyperbolic tangent function
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
The function <italic>f</italic><sup>(3)</sup> is called a transfer function that transforms the results from the last hidden layer's nodes into a working output. The transfer function can also be a sigmoid function or a simple linear function. Thus, the <italic>z</italic><sup>(1)</sup>, <italic>z</italic><sup>(2)</sup> and <italic>z</italic><sup>(3)</sup> are the weighted sum of inputs from the input layer and the first and second hidden layers, respectively. ANNs can be made more flexible by increasing the number of hidden layers and/or the number of nodes in these layers.</p>
        <fig id="F1" position="float">
          <label>Figure 1</label>
          <caption>
            <p>Diagram of general structure of a deep neural network with <italic>I</italic> inputs, two hidden layers with <italic>J</italic> and <italic>K</italic> hidden nodes and <italic>O</italic> outputs.</p>
          </caption>
          <graphic xlink:href="fphys-12-662314-g0001"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>2.3. BayCANN Algorithm</title>
      <p>We implement BayCANN with TensorFlow to fit the ANN and Stan to obtain the parameter's posterior distributions. We use the package <monospace>keras</monospace> in <monospace>R</monospace> to create ANN metamodels that approximate the relationship between our model's input parameters and outputs and estimate the coefficients <italic>b</italic> and <italic>W</italic> (R Core Team, <xref rid="B55" ref-type="bibr">2018</xref>; Jalal et al., <xref rid="B26" ref-type="bibr">2017</xref>). We built the ANN from a set of probabilistic samples using a Latin hypercube sampling (LHS) design of experiment (DoE) to efficiently sample the input parameter space. Once we obtain the ANN coefficients, we perform the Bayesian calibration using the ANN rather than the simulation model.</p>
      <p>We implemented the deep ANN in Stan (Carpenter et al., <xref rid="B10" ref-type="bibr">2017</xref>) which uses a guided MCMC using gradient descent, referred to as Hamiltonian Monte-Carlo. Similarly, the R package <monospace>rstan</monospace>.</p>
      <p>Both TensorFlow and Stan utilize multithreading; thus, it is essential to ensure sufficient memory is available for all threads to run efficiently.</p>
      <p>Below we outline the steps to conduct BayCANN.</p>
      <list list-type="order">
        <list-item>
          <p>Structure the simulation model such that it produces outputs corresponding to the calibration targets. For example, if calibration targets are disease incidence or prevalence, ensure the model generates these outputs.</p>
        </list-item>
        <list-item>
          <p>Generate two datasets of input parameter sets—one for training the ANN (training dataset) and the second for validating it (validation dataset). The analyst could use an LHS to efficiently sample the model inputs' prior distributions.</p>
        </list-item>
        <list-item>
          <p>Run the simulation model using both training and validation datasets to generate their corresponding simulation model outputs.</p>
        </list-item>
        <list-item>
          <p>Train an ANN using the training dataset, and validate it using the validation dataset. Obtaining a high-fidelity ANN is crucial to ensure getting accurate and reliable results from BayCANN (Degeling et al., <xref rid="B13" ref-type="bibr">2020</xref>). Adjust the ANN's structure to obtain an accurate metamodel before proceeding.</p>
        </list-item>
        <list-item>
          <p>Perform the Bayesian calibration by passing the ANN coefficients <italic>W</italic> and <italic>b</italic>, the prior input parameter samples, and the calibration targets to the programmed ANN framework in Stan. Stan then returns the joint posterior distribution of the calibrated parameters.</p>
        </list-item>
      </list>
      <p>The code for implementing BayCANN is available on GitHub at <ext-link ext-link-type="uri" xlink:href="https://github.com/hjalal/BayCANN">https://github.com/hjalal/BayCANN</ext-link>. In the case study below, we use BayCANN to calibrate a colorectal cancer natural history model.</p>
    </sec>
    <sec>
      <title>2.4. Case Study: Natural History Model of Colorectal Cancer</title>
      <p>We use BayCANN to calibrate a state-transition model (STM) of the natural history of colorectal cancer (CRC) implemented in <monospace>R</monospace> (Jalal et al., <xref rid="B26" ref-type="bibr">2017</xref>). We refer to our model as CRCModR. CRCModR is a discrete-time STM based on a model structure originally proposed by (Wu et al., <xref rid="B72" ref-type="bibr">2006</xref>) that has previously been used for testing other methods (Alarid-Escudero et al., <xref rid="B1" ref-type="bibr">2018</xref>; Heath et al., <xref rid="B21" ref-type="bibr">2020</xref>). Briefly, CRCModR has 9 different health states that include absence of the disease, small and large precancerous lesions (i.e., adenomatous polyps), and early and late preclinical and clinical cancer states by stage. <xref ref-type="fig" rid="F2">Figure 2</xref> shows the state-transition diagram of the model. The progression between health states follows a continuous-time age-dependent Markov process. There are two age-dependent transition intensities (i.e., transition rates), λ<sub>1</sub>(<italic>a</italic>) and μ(<italic>a</italic>), that govern the age of onset of adenomas and all-cause mortality, respectively. Following Wu's original specification (Wu et al., <xref rid="B72" ref-type="bibr">2006</xref>), we specify λ<sub>1</sub>(<italic>a</italic>) as a Weibull hazard such that
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mtable class="eqnarray" columnalign="right center left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mi>γ</mml:mi><mml:msup><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
where <italic>l</italic> and γ are the scale and shape parameters of the Weibull hazard model, respectively. The model simulates two adenoma categories: small (adenoma smaller than or equal to 1 cm in size) and large (an adenoma larger than 1 cm in size). All adenomas start small and can transition to the large size category at a constant annual rate λ<sub>2</sub>. Large adenomas may become preclinical CRC at a constant annual rate λ<sub>3</sub>. Both small and large adenomas may progress to preclinical CRC, although most will not in an individual's lifetime. Early preclinical cancers progress to late stages at a constant annual rate λ<sub>4</sub> and could become symptomatic at a constant annual rate λ<sub>5</sub>. Late preclinical cancer could become symptomatic at a constant annual rate λ<sub>6</sub>. After clinical detection, the model simulates the survival time to death from early and late CRC using time-homogeneous mortality rates, λ<sub>7</sub> and λ<sub>8</sub>, respectively. In total, the model has nine health states: normal, small adenoma, large adenoma, early preclinical CRC, late preclinical CRC, CRC death, and other causes of death. The state-transition diagram of the model is shown in <xref ref-type="fig" rid="F2">Figure 2</xref>. The model simulates the natural history of CRC of a hypothetical cohort of 50-year-old women in the U.S. over a lifetime. The cohort starts the simulation with a prevalence of adenoma of <italic>p</italic><sub><italic>adeno</italic></sub>. A proportion, <italic>p</italic><sub><italic>small</italic></sub>, corresponds to small adenomas and prevalence of preclinical early and late CRC of 0.12 and 0.08, respectively. The simulated cohort in any state is at risk of all-cause mortality μ(<italic>a</italic>) obtained from the U.S. life tables Arias (<xref rid="B2" ref-type="bibr">2014</xref>). Similar models to CRCmodR have been used to inform population-level screening guidelines in the U.S. (Knudsen et al., <xref rid="B38" ref-type="bibr">2016</xref>).</p>
      <fig id="F2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>State-transition diagram of the natural history model of colorectal cancer. Ovals represent health states and arrows represent transitions. All states can transition to death from causes other than CRC with rate μ(<italic>a</italic>). CRC, colorectal cancer.</p>
        </caption>
        <graphic xlink:href="fphys-12-662314-g0002"/>
      </fig>
      <p>CRCModR has 11 parameters summarized in <xref rid="T2" ref-type="table">Table 2</xref> (Alarid-Escudero et al., <xref rid="B1" ref-type="bibr">2018</xref>). Mortality rates from early and late stages of CRC (λ<sub>7</sub>, λ<sub>8</sub>]) could be obtained from cancer population registries (e.g., SEER in the U.S.). Thus, we calibrate the model to the remaining nine parameters (<italic>p</italic><sub><italic>adeno</italic></sub>, <italic>p</italic><sub><italic>small</italic></sub>, <italic>l</italic>,γ, λ<sub>2</sub>, λ<sub>3</sub>, λ<sub>4</sub>, λ<sub>5</sub> and λ<sub>6</sub>).</p>
      <table-wrap id="T2" position="float">
        <label>Table 2</label>
        <caption>
          <p>The parameters of the natural history model of colorectal cancer (CRC).</p>
        </caption>
        <table frame="hsides" rules="groups">
          <thead>
            <tr>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Parameter</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Description</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Base value</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Calibrate?</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Source</bold>
              </th>
              <th valign="top" align="left" rowspan="1" colspan="1">
                <bold>Prior range</bold>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>l</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Scale parameter of Weibull hazard</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.86e-06</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Wu et al., <xref rid="B72" ref-type="bibr">2006</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">[2 × 10<sup>−6</sup>, 2 × 10<sup>−5</sup>]</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>g</italic>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Shape parameter of Weibull hazard</td>
              <td valign="top" align="left" rowspan="1" colspan="1">2.78</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Wu et al., <xref rid="B72" ref-type="bibr">2006</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">[2.00, 4.00]</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">λ<sub>2</sub></td>
              <td valign="top" align="left" rowspan="1" colspan="1">Small adenoma to large adenoma</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.0346</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Wu et al., <xref rid="B72" ref-type="bibr">2006</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">[0.01, 0.10]</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">λ<sub>3</sub></td>
              <td valign="top" align="left" rowspan="1" colspan="1">Large adenoma to preclinical early CRC</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.0215</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Wu et al., <xref rid="B72" ref-type="bibr">2006</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">[0.01, 0.04]</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">λ<sub>4</sub></td>
              <td valign="top" align="left" rowspan="1" colspan="1">Preclinical early to preclinical late CRC</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.3697</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Wu et al., <xref rid="B72" ref-type="bibr">2006</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">[0.20, 0.50]</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">λ<sub>5</sub></td>
              <td valign="top" align="left" rowspan="1" colspan="1">Preclinical early to clinical early CRC</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.2382</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Wu et al., <xref rid="B72" ref-type="bibr">2006</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">[0.20, 0.30]</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">λ<sub>6</sub></td>
              <td valign="top" align="left" rowspan="1" colspan="1">Preclinical late to clinical late CRC</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.4852</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Wu et al., <xref rid="B72" ref-type="bibr">2006</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">[0.30, 0.70]</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">λ<sub>7</sub></td>
              <td valign="top" align="left" rowspan="1" colspan="1">CRC mortality in early stage</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.0302</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Wu et al., <xref rid="B72" ref-type="bibr">2006</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">-</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">λ<sub>8</sub></td>
              <td valign="top" align="left" rowspan="1" colspan="1">CRC mortality in late stage</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.2099</td>
              <td valign="top" align="left" rowspan="1" colspan="1">No</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Wu et al., <xref rid="B72" ref-type="bibr">2006</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">-</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>p</italic>
                <sub>
                  <italic>adeno</italic>
                </sub>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Prevalence of adenoma at age 50</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.27</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Rutter et al., <xref rid="B62" ref-type="bibr">2007</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">[0.25, 0.35]</td>
            </tr>
            <tr>
              <td valign="top" align="left" rowspan="1" colspan="1">
                <italic>p</italic>
                <sub>
                  <italic>small</italic>
                </sub>
              </td>
              <td valign="top" align="left" rowspan="1" colspan="1">Proportion of small adenomas at age 50</td>
              <td valign="top" align="left" rowspan="1" colspan="1">0.71</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Yes</td>
              <td valign="top" align="left" rowspan="1" colspan="1">Wu et al., <xref rid="B72" ref-type="bibr">2006</xref></td>
              <td valign="top" align="left" rowspan="1" colspan="1">[0.38, 0.95]</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <p><italic>The base values are used to generate the calibration targets and the ranges of the uniform distribution used as priors for the Bayesian calibration</italic>.</p>
        </table-wrap-foot>
      </table-wrap>
      <sec>
        <title>2.4.1. Confirmatory Analysis</title>
        <p>We conducted a confirmatory analysis to compare BayCANN vs. IMIS. To obtain the “truth” that we could compare BayCANN and IMIS against, we <italic>generated</italic> the synthetic targets using the base-case values in <xref rid="T2" ref-type="table">Table 2</xref>. We generated four age-specific targets, including adenoma prevalence, the proportion of small adenomas, and CRC incidence for early and late stages which represent commonly used calibration targets for this type of model (Kuntz et al., <xref rid="B40" ref-type="bibr">2011</xref>). To generate the calibration targets, we ran CRCModR as a microsimulation (Krijkamp et al., <xref rid="B39" ref-type="bibr">2018</xref>) 100 times to produce different adenoma-related and cancer incidence outputs. We then aggregated the results across all 100 outputs to compute their mean and standard errors (SE). Different calibration targets could have different levels of uncertainty given the amount of data to compute their summary measures. Therefore, to account for different variations in the amount of data on different calibration targets, we simulated different numbers of individuals for adenoma targets (<italic>N</italic> = 500) and cancer incidence targets (<italic>N</italic> = 100, 000). <xref ref-type="fig" rid="F3">Figure 3</xref> shows the generated adenoma-related and cancer incidence calibration targets aggregated over 100 different runs using the parameter set in <xref rid="T2" ref-type="table">Table 2</xref>.</p>
        <fig id="F3" position="float">
          <label>Figure 3</label>
          <caption>
            <p>Generated calibration targets and its 95% credible interval of a cohort of 500 and 100,000 simulated individuals for adenoma-related targets cancer incidence targets, respectively plotted against age in years on the x-axis. These distributions are from 100 different runs using the same parameter set values in each set of runs.</p>
          </caption>
          <graphic xlink:href="fphys-12-662314-g0003"/>
        </fig>
        <p>To create a deep ANN metamodel, we generated a DOE by sampling each of the nine parameters from the ranges of the uniform distributions using an LHS design as shown in <xref rid="T2" ref-type="table">Table 2</xref>. Specifically, we created two LHS input datasets of sizes 8,000 samples and 2,000 samples for training and validating the ANN, respectively. We then ran the natural history model and generated adenoma prevalence and CRC incidence for each parameter set.</p>
        <p>We define an ANN with two hidden layers and 100 nodes per hidden layer. Then, we evaluated the ANN's performance by validating the predicted values for the 36 outcomes against the observed values from the validation dataset. The likelihood function for BayCANN was constructed by assuming that the targets, <italic>y</italic><sub><italic>t</italic><sub><italic>i</italic></sub></sub>, are normally distributed with mean ϕ<sub><italic>t</italic><sub><italic>i</italic></sub></sub> and standard deviation σ<sub><italic>t</italic><sub><italic>i</italic></sub></sub>, where ϕ<sub><italic>t</italic><sub><italic>i</italic></sub></sub> = <italic>M</italic>[θ] is the model-predicted output for each type of target <italic>t</italic> and age group <italic>i</italic> at parameter set θ. We defined uniform prior distributions for all θ<sub><italic>u</italic></sub> based on previous knowledge or nature of the parameters (<xref rid="T2" ref-type="table">Table 2</xref>).</p>
        <p>We compare BayCANN against a full Bayesian calibration of the natural history model using the incremental mixture importance sampling (IMIS) algorithm. The IMIS algorithm has been described elsewhere (Raftery and Bao, <xref rid="B56" ref-type="bibr">2010</xref>), but briefly, this algorithm reduces the computational burden of Bayesian calibration by incrementally building a better importance sampling function based on Gaussian mixtures.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>3. Results</title>
    <p>We present the ANN's performance in approximating the output of the simulation model and compare the generated joint posterior distribution of the simulation model parameters produced from BayCANN against the full joint posterior from the IMIS approach. We compare both BayCANN and IMIS results recovering the “true” values—the parameter values we used to generate the calibration targets in the confirmatory analysis.</p>
    <sec>
      <title>3.1. Validation</title>
      <p><xref ref-type="fig" rid="F4">Figure 4</xref> illustrates the ANN's performance in predicting the model outputs using the validation dataset. Each plot represents one of the model outputs, where we compare the ANN's prediction on the y-axis against the model's output on the x-axis. Each red dot represents one of the 2,000 DOE validation samples not used to train the ANN. The ANN had a high prediction performance in approximating the model outputs (<italic>R</italic><sup>2</sup> &gt; 99.9%), indicating that the deep ANN is a high fidelity metamodel of the simulation model within the parameter ranges we evaluated.</p>
      <fig id="F4" position="float">
        <label>Figure 4</label>
        <caption>
          <p>Validation of the fitted ANN on the validation Latin hyper cube sample (LHS) dataset. The x and y axes represent the scaled model outputs and scaled ANN predictions, respectively.</p>
        </caption>
        <graphic xlink:href="fphys-12-662314-g0004"/>
      </fig>
    </sec>
    <sec>
      <title>3.2. Comparing BayCANN and IMIS</title>
      <p><xref ref-type="fig" rid="F5">Figure 5</xref> compares BayCANN against IMIS in recovering the true parameter values used to generate the targets. The 95% credible intervals (CrI) of each parameter distribution obtained from BayCANN cover all nine true parameters. For IMIS, the 95% CrI did not cover the true parameters for λ<sub>2</sub> and λ<sub>3</sub>. This figure also shows the maximum a posteriori (MAP) estimate for both BayCANN and IMIS. The MAP is the sample associated with the highest log-posterior and indicates the posterior parameter set that best fits the target data.</p>
      <fig id="F5" position="float">
        <label>Figure 5</label>
        <caption>
          <p>Prior and Marginal posterior distributions of the calibrated parameters from the IMIS and BayCANN methods. The vertical solid lines indicate the “true” parameter values (i.e., the value of the parameters used to generate the calibration targets in the confirmatory analysis). The vertical dashed lines represent the maximum a posteriori (MAP) for BayCANN and the incremental mixture importance sampling (IMIS).</p>
        </caption>
        <graphic xlink:href="fphys-12-662314-g0005"/>
      </fig>
      <p><xref ref-type="fig" rid="F6">Figure 6</xref> compares the results of BayCANN against all the calibration targets for the probability of developing multiple adenomas, the proportion of small adenomas, and early and late clinical CRC incidence. Upon visual inspection, BayCANN fits all calibration targets well, indicating that the joint posterior distribution from BayCANN can produce targets in the desired ranges. The results here represent the model-predictive mean and the credible interval of using 10,000 posterior samples from BayCANN. We also present the results of using BayCANN's MAP estimates which closely follow the model-predicted posterior mean from the 10,000 posterior samples.</p>
      <fig id="F6" position="float">
        <label>Figure 6</label>
        <caption>
          <p>BayCANN calibration results by age in years on the x-axis. The upper panels show adenoma targets and lower panels show cancer incidence targets by stage. Calibration targets with their 95% confidence intervals are shown in black. The colored curves show the posterior model-predicted mean, and the shaded area shows the corresponding 95% posterior model-predicted credible interval of the outcomes. The dashed-dotted lines represent the output using the maximum a posteriori (MAP) estimate from BayCANN.</p>
        </caption>
        <graphic xlink:href="fphys-12-662314-g0006"/>
      </fig>
      <p>In this example, BayCANN was five times faster than the IMIS. The IMIS algorithm took 80 min to run in a MacBook Pro Retina, 15-inch, Late 2013 with a 2.6 GHz Intel Core i7 processor with 4 cores and 16 gigabytes of RAM. BayCANN took only 15 min on the same computer; 5 min to produce 10,000 samples for both LHS DOE dataset generations and about 10 min to fit the ANN in TensorFlow and produce the joint posterior distributions in Stan. The computational gain of BayCANN was modest given that our case study model was efficient and deterministic.</p>
      <p><xref ref-type="fig" rid="F7">Figure 7</xref> presents the joint distribution of all pairwise parameters in BayCANN, and along the diagonal, the marginal distributions of each parameter. This figure reveals insightful information about this calibration exercise. In practice, many calibrated parameters are correlated as shown in this figure. The absolute value of these correlations range from 0.013 to 0.963. The strength of the correlation reflects the level of non-identifiability between that pair of parameters. The stronger the correlation the higher the non-identifiability and the greater need to add additional target data or modify the model structure to <italic>separate</italic> the parameters in question (Alarid-Escudero et al., <xref rid="B1" ref-type="bibr">2018</xref>).</p>
      <fig id="F7" position="float">
        <label>Figure 7</label>
        <caption>
          <p>Joint posterior distribution of the calibrated parameters of the case study using the ANN method.</p>
        </caption>
        <graphic xlink:href="fphys-12-662314-g0007"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>4. Discussion</title>
    <p>In this study, we propose BayCANN as a feasible and practical solution to Bayesian calibration challenges in complex health decision science models. The distinct advantage of using BayCANN is that it represents the model on a functional basis as an ANN. Then, the ANN can become a high-fidelity representation of the model. Thus, those interested in implementing BayCANN can do so without the need to code their models in a probabilistic programming language. Given the high computational efficiency of the ANN, BayCANN can also provide a computational advantages over other Bayesian calibration methods.</p>
    <p>BayCANN uses ANNs specifically to streamline Bayesian calibration. ANNs have also been used as metamodels of both stochastic and deterministic responses, mainly for their computational efficiency (Barton, <xref rid="B6" ref-type="bibr">2009</xref>; Badiru and Sieger, <xref rid="B3" ref-type="bibr">1998</xref>; Hurrion, <xref rid="B22" ref-type="bibr">1997</xref>; Chambers and Mount-Campbell, <xref rid="B11" ref-type="bibr">2002</xref>; Zobel and Keeling, <xref rid="B73" ref-type="bibr">2008</xref>). One of the first implementations of ANN as metamodels was in 1992 for a scheduling simulation model (Pierreval et al., <xref rid="B53" ref-type="bibr">1992</xref>; Pierreval and Huntsinger, <xref rid="B54" ref-type="bibr">1992</xref>). Since then, ANNs have been successfully implemented as emulators of all sorts of discrete-event and continuous simulation models in a wide variety of fields (Kilmer, <xref rid="B31" ref-type="bibr">1996</xref>; Sabuncuoglu and Touhami, <xref rid="B63" ref-type="bibr">2002</xref>; Fonseca et al., <xref rid="B16" ref-type="bibr">2003</xref>; El Tabach et al., <xref rid="B14" ref-type="bibr">2007</xref>). ANNs have also been proposed as proxies for non-linear and simulation models (Paiva et al., <xref rid="B51" ref-type="bibr">2010</xref>; Mareš and Kučerová, <xref rid="B42" ref-type="bibr">2012</xref>; Pichler et al., <xref rid="B52" ref-type="bibr">2003</xref>). An example of ANNs as metamodels is estimating the mean and variance of patient time in emergency department visits (Kilmer, <xref rid="B30" ref-type="bibr">1994</xref>; Kilmer et al., <xref rid="B32" ref-type="bibr">1997</xref>). Nowadays, ANNs are widely popular as machine learning tools in artificial intelligence (Schmidhuber, <xref rid="B66" ref-type="bibr">2015</xref>). Deep learning using ANNs are used for visual recognition in self-driving cars (Ndikumana et al., <xref rid="B46" ref-type="bibr">2020</xref>) and in classifying galaxies (Folkes et al., <xref rid="B15" ref-type="bibr">1996</xref>). ANNs have been used for calibration of computationally expensive models, such as general circulation and rainfall-runoff models in climate science (Khu et al., <xref rid="B28" ref-type="bibr">2004</xref>; Hauser et al., <xref rid="B20" ref-type="bibr">2012</xref>), and other complex global optimization techniques such as genetic algorithms (Wang, <xref rid="B69" ref-type="bibr">2005</xref>).</p>
    <p>The superior performance of BayCANN relative to IMIS may pertain to the bias of the ANN in BayCANN being relatively lower than that of the Bayesian approximation of IMIS. BayCANN uses ANNs as high-fidelity metamodels of the simulator and conducts full Bayesian calibration. However, IMIS is an approximation of Bayesian inference that directly uses the simulator itself. Thus, visual examination of the ANN's performance similar to <xref ref-type="fig" rid="F4">Figure 4</xref> is an important step to ensure obtaining high-fidelity ANN for BayCANN.</p>
    <p>Bayesian calibration provides other practical advantages over direct-search algorithms because the samples from the joint posterior distribution can be used directly as inputs to probabilistic sensitivity analyses (PSA) which are now required for cost-effectiveness analyses (Neumann et al., <xref rid="B48" ref-type="bibr">2016</xref>; Rutter et al., <xref rid="B61" ref-type="bibr">2019</xref>). This joint posterior distribution is also informative in non-identifiable calibration problems where calibration targets are not sufficient to provide a unique solution to the calibrated parameters. Non-identifiability is often overlooked using standard non-Bayesian calibration approaches (Alarid-Escudero et al., <xref rid="B1" ref-type="bibr">2018</xref>).</p>
    <p>In our case study, BayCANN was both faster and overall more accurate in recovering the true parameter values than the IMIS algorithm. We developed BayCANN to be generalizable to models of various complexities, and we provide the open-source implementation in R and Stan to facilitate its wider adoption.</p>
    <p>BayCANN may have an additional advantage for representing models with first-order Monte-Carlo noise from individual-based state-transition models (iSTM). Traditionally, calibrating these models has been challenging because of (1) the stochasticity of each simulation due to the simulator's output varying given the same set of input parameter values, and (2) the extra computational burden involved in calibrating iSTM. Because BayCANN averages over a set of simulations, it can account for the first-order Monte-Carlo noise. Further research is needed to study BayCANN's performance in stochastic models.</p>
    <p>We chose ANNs over other metamodeling techniques because of their flexibility, efficiency, and ability to accept a large number of inputs. The use of metamodels in Bayesian calibration has been mostly limited to Gaussian processes (GP) (Kennedy and O'Hagan, <xref rid="B27" ref-type="bibr">2001</xref>; Gramacy, <xref rid="B19" ref-type="bibr">2020</xref>). GPs are attractive because they can be specified fully as Bayesian models (Kennedy and O'Hagan, <xref rid="B27" ref-type="bibr">2001</xref>). However, GPs are not without limitations, the main one being that they are themselves relatively computationally expensive. In practice, computational challenges limit training GPs to datasets in the low thousands limiting their applicability to health decision sciences models (Gramacy, <xref rid="B19" ref-type="bibr">2020</xref>).</p>
    <p>Our approach has some limitations. First, ANNs are not fully probabilistic, thus, the joint posterior distribution produced from the Bayesian calibration is an approximation of the true distribution. Other metamodels, such as GPs are fully probabilistic and can produce the full joint posterior distribution (Gramacy, <xref rid="B19" ref-type="bibr">2020</xref>). However, applying GPs in complex models can be computationally infeasible (Gramacy, <xref rid="B19" ref-type="bibr">2020</xref>). Second, accuracy—Because ANNs (and GPs) are metamodels, they may rarely achieve 100% precision compared to using the simulation model itself. In our example, with a relatively simple ANN (only two hidden layers with 100 hidden nodes each), we were able to achieve 99.9% accuracy. However, for other application, the accuracy of the ANN might be lower especially if the model outputs are not continuous or smooth in certain region of the parameter space. In addition, over-fitting can be a serious problem with any metamodel especially when the purpose of the metamodel is as sensitive as calibration. To reduce the chance of over-fitting, we validated the model against a subset of simulation runs. We visually inspected the degree of fit for the simulation output against those predicted by the ANN (<xref ref-type="fig" rid="F4">Figure 4</xref>). Third, similar to any Bayesian model, the choice of priors can be important. Fortunately, in health decision sciences' models, analysts often make careful choices of their priors when designing their models and running PSA analyses. Additionally, the best-fitting parameters may be outside the simulated ranges. Notably, the joint posterior distribution can give insights into the parameter ranges. For example, if a parameter is skewed heavily without a clear peak, that may indicate that the parameter range needs to be shifted to cover values that may fit better. This process is usually iterative and may involve multiple steps or redefining the parameter ranges and recalibrating the model. Finally, there is no strict guideline for choosing the number of hidden ANN layers or the number of nodes per layer. In this study, we chose an ANN with two hidden layers and 100 nodes per layer. Adjusting these parameters and additional parameters of the Bayesian calibration process can improve the calibration results and can be easily changed in BayCANN. While determining these values apriori can be challenging, we recommend modelers who wish to use BayCANN to start with simple settings and gradually increase the complexity of the ANN to accommodate their particular needs. We provide flexible code in R and Stan to simplify these tasks.</p>
    <p>In summary, Bayesian calibration can reveal important insights into model parameter values and produce outcomes that match observed data. BayCANN is one effort to target the computational and technical challenges of Bayesian calibration for complex models.</p>
  </sec>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>The original contributions presented in the study are included in the article and the code for BayCANN is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/hjalal/BayCANN">https://github.com/hjalal/BayCANN</ext-link>, further inquiries can be directed to the corresponding author.</p>
  </sec>
  <sec id="s6">
    <title>Author Contributions</title>
    <p>HJ and FA-E conceived the study. HJ, FA-E, and TT conducted the analyses, contributed to interpreting the results, and writing the manuscript. All authors contributed to the article and approved the submitted version.</p>
  </sec>
  <sec sec-type="COI-statement" id="conf1">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn fn-type="financial-disclosure">
      <p><bold>Funding.</bold> HJ was supported by the Centers for Disease Control and Prevention Contract No. 34150, and a grant from the National Institute on Drug Abuse of the National Institute of Health under award no. K01DA048985. FA-E was supported by two grants from the National Cancer Institute (U01-CA-199335 and U01-CA-253913) as part of the Cancer Intervention and Surveillance Modeling Network (CISNET), the Gordon and Betty Moore Foundation, and Open Society Foundations (OSF). The funding agencies had no role in the design of the study, interpretation of results, or writing of the manuscript. The content is solely the responsibility of the authors and does not necessarily represent the official views of the Centers for Disease Control and Prevention, National Cancer Institute or National Institutes of Health.</p>
    </fn>
  </fn-group>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alarid-Escudero</surname><given-names>F.</given-names></name><name><surname>MacLehose</surname><given-names>R. F.</given-names></name><name><surname>Peralta</surname><given-names>Y.</given-names></name><name><surname>Kuntz</surname><given-names>K. M.</given-names></name><name><surname>Enns</surname><given-names>E. A.</given-names></name></person-group> (<year>2018</year>). <article-title>Nonidentifiability in model calibration and implications for medical decision making</article-title>. <source>Med. Decis. Mak</source>. <volume>38</volume>, <fpage>810</fpage>–<lpage>821</lpage>. <pub-id pub-id-type="doi">10.1177/0272989X18792283</pub-id><?supplied-pmid 30248276?><pub-id pub-id-type="pmid">30248276</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Arias</surname><given-names>E.</given-names></name></person-group> (<year>2014</year>). <source>United States Life Tables, 2014</source>. National Vital Statistics Reports.<?supplied-pmid 29155687?></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badiru</surname><given-names>A. B.</given-names></name><name><surname>Sieger</surname><given-names>D. B.</given-names></name></person-group> (<year>1998</year>). <article-title>Neural network as a simulation metamodel in economic analysis of risky projects</article-title>. <source>Eur. J. Operat. Res</source>. <volume>105</volume>, <fpage>130</fpage>–<lpage>142</lpage>. <pub-id pub-id-type="doi">10.1016/S0377-2217(97)00029-5</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Banks</surname><given-names>J.</given-names></name></person-group> (<year>1998</year>). <source>Handbook of Simulation: Principles, Methodology, Advances, Applications, and Practice</source>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley &amp; Sons, Inc</publisher-name>. <pub-id pub-id-type="doi">10.1002/9780470172445</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barton</surname><given-names>R. R.</given-names></name></person-group> (<year>1992</year>). <article-title>Metamodels for simulation input-output relations</article-title>, in <source>Winter Simulation Conference</source>, <volume>Vol. 9</volume>, eds <person-group person-group-type="editor"><name><surname>Swain</surname><given-names>J.</given-names></name><name><surname>Goldsman</surname><given-names>D.</given-names></name><name><surname>Crain</surname><given-names>R.</given-names></name><name><surname>Wilson</surname><given-names>J.</given-names></name></person-group> (<publisher-loc>Arlington, VA</publisher-loc>), <fpage>289</fpage>–<lpage>299</lpage>. <pub-id pub-id-type="doi">10.1145/167293.167352</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barton</surname><given-names>R. R.</given-names></name></person-group> (<year>2009</year>). <article-title>Simulation optimization using metamodels</article-title>, in <source>Winter Simulation Conference (WSC)</source> (<publisher-loc>Austin, TX</publisher-loc>), <fpage>230</fpage>–<lpage>238</lpage>. <pub-id pub-id-type="doi">10.1109/WSC.2009.5429328</pub-id></mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barton</surname><given-names>R. R.</given-names></name><name><surname>Meckesheimer</surname><given-names>M.</given-names></name></person-group> (<year>2006</year>). <article-title>Chapter 18: Metamodel-based simulation optimization</article-title>, in <source>Handbooks in Operations Research and Management Science</source>, eds <person-group person-group-type="editor"><name><surname>Henderson</surname><given-names>S. G.</given-names></name><name><surname>Nelson</surname><given-names>B. L.</given-names></name></person-group> (<publisher-loc>Amsterdam: North-Holland</publisher-loc>), <fpage>535</fpage>–<lpage>574</lpage>. <pub-id pub-id-type="doi">10.1016/S0927-0507(06)13018-2</pub-id></mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanning</surname><given-names>R. W.</given-names></name></person-group> (<year>1974</year>). <article-title>The sources and uses of sensitivity information</article-title>. <source>Interfaces</source>
<volume>4</volume>, <fpage>32</fpage>–<lpage>38</lpage>. <pub-id pub-id-type="doi">10.1287/inte.4.4.32</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Briggs</surname><given-names>A. H.</given-names></name><name><surname>Weinstein</surname><given-names>M. C.</given-names></name><name><surname>Fenwick</surname><given-names>E. A. L.</given-names></name><name><surname>Karnon</surname><given-names>J.</given-names></name><name><surname>Sculpher</surname><given-names>M. J.</given-names></name><name><surname>Paltiel</surname><given-names>A. D.</given-names></name></person-group> (<year>2012</year>). <article-title>Model parameter estimation and uncertainty analysis: a report of the ISPOR-SMDM modeling good research practices task force working group-6</article-title>. <source>Med. Decis. Mak</source>. <volume>32</volume>, <fpage>722</fpage>–<lpage>732</lpage>. <pub-id pub-id-type="doi">10.1177/0272989X12458348</pub-id><?supplied-pmid 22990087?><pub-id pub-id-type="pmid">22990087</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>B.</given-names></name><name><surname>Gelman</surname><given-names>A.</given-names></name><name><surname>Hoffman</surname><given-names>M. D.</given-names></name><name><surname>Lee</surname><given-names>D.</given-names></name><name><surname>Goodrich</surname><given-names>B.</given-names></name><name><surname>Betancourt</surname><given-names>M.</given-names></name><etal/></person-group>. (<year>2017</year>). <article-title>Stan: a probabilistic programming language</article-title>. <source>J. Stat. Softw</source>. <volume>76</volume>, <fpage>1</fpage>–<lpage>32</lpage>. <pub-id pub-id-type="doi">10.18637/jss.v076.i01</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>M.</given-names></name><name><surname>Mount-Campbell</surname><given-names>C. A.</given-names></name></person-group> (<year>2002</year>). <article-title>Process optimization via neural network metamodeling</article-title>. <source>Int. J. Prod. Econ</source>. <volume>79</volume>, <fpage>93</fpage>–<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1016/S0925-5273(00)00188-2</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Carvalho</surname><given-names>T. M.</given-names></name><name><surname>Heijnsdijk</surname><given-names>E. A.</given-names></name><name><surname>Coffeng</surname><given-names>L.</given-names></name><name><surname>de Koning</surname><given-names>H. J.</given-names></name></person-group> (<year>2019</year>). <article-title>Evaluating parameter uncertainty in a simulation model of cancer using emulators</article-title>. <source>Med. Decis. Mak</source>. <volume>39</volume>, <fpage>405</fpage>–<lpage>413</lpage>. <pub-id pub-id-type="doi">10.1177/0272989X19837631</pub-id><?supplied-pmid 31804156?><pub-id pub-id-type="pmid">31804156</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Degeling</surname><given-names>K.</given-names></name><name><surname>IJzerman</surname><given-names>M. J.</given-names></name><name><surname>Lavieri</surname><given-names>M. S.</given-names></name><name><surname>Strong</surname><given-names>M.</given-names></name><name><surname>Koffijberg</surname><given-names>H.</given-names></name></person-group> (<year>2020</year>). <article-title>Introduction to metamodeling for reducing computational burden of advanced analyses with health economic models: a structured overview of metamodeling methods in a 6-step application process</article-title>. <source>Med. Decis. Mak</source>. <volume>40</volume>, <fpage>348</fpage>–<lpage>363</lpage>. <pub-id pub-id-type="doi">10.1177/0272989X20912233</pub-id><?supplied-pmid 32428428?><pub-id pub-id-type="pmid">32428428</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El Tabach</surname><given-names>E.</given-names></name><name><surname>Lancelot</surname><given-names>L.</given-names></name><name><surname>Shahrour</surname><given-names>I.</given-names></name><name><surname>Najjar</surname><given-names>Y.</given-names></name></person-group> (<year>2007</year>). <article-title>Use of artificial neural network simulation metamodelling to assess groundwater contamination in a road project</article-title>. <source>Math. Comput. Modell</source>. <volume>45</volume>, <fpage>766</fpage>–<lpage>776</lpage>. <pub-id pub-id-type="doi">10.1016/j.mcm.2006.07.020</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Folkes</surname><given-names>S.</given-names></name><name><surname>Lahav</surname><given-names>O.</given-names></name><name><surname>Maddox</surname><given-names>S.</given-names></name></person-group> (<year>1996</year>). <article-title>An artificial neural network approach to the classification of galaxy spectra</article-title>. <source>Mnthly Notices R. Astron. Soc</source>. <volume>283</volume>, <fpage>651</fpage>–<lpage>665</lpage>. <pub-id pub-id-type="doi">10.1093/mnras/283.2.651</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonseca</surname><given-names>D.</given-names></name><name><surname>Navaresse</surname><given-names>D.</given-names></name><name><surname>Moynihan</surname><given-names>G.</given-names></name></person-group> (<year>2003</year>). <article-title>Simulation metamodeling through artificial neural networks</article-title>. <source>Eng. Appl. Artif. Intell</source>. <volume>16</volume>, <fpage>177</fpage>–<lpage>183</lpage>. <pub-id pub-id-type="doi">10.1016/S0952-1976(03)00043-5</pub-id></mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>L. W.</given-names></name><name><surname>Pressman</surname><given-names>I.</given-names></name></person-group> (<year>1988</year>). <article-title>The metamodel in simulation analysis: can it be trusted?</article-title>
<source>J. Operat. Res. Soc</source>. <volume>39</volume>, <fpage>939</fpage>–<lpage>948</lpage>. <pub-id pub-id-type="doi">10.1057/jors.1988.160</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>M. C.</given-names></name></person-group> (<year>1994</year>). <article-title>A tutorial review of techniques for simulation optimization</article-title>, in <source>Proceedings of the 1994 Winter Simulation Conference</source>, eds <person-group person-group-type="editor"><name><surname>Tew</surname><given-names>J. D.</given-names></name><name><surname>Manivannan</surname><given-names>S.</given-names></name><name><surname>Sadowski</surname><given-names>D. A.</given-names></name><name><surname>Seila</surname><given-names>A. F.</given-names></name></person-group> (<publisher-loc>Lake Buena Vista, FL</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>8</fpage>. <pub-id pub-id-type="doi">10.1109/WSC.1994.717096</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gramacy</surname><given-names>R. B.</given-names></name></person-group> (<year>2020</year>). <source>Surrogates: Gaussian Process Modeling, Design, and Optimization for the Applied Sciences</source>. <publisher-loc>Boca Raton, FL</publisher-loc>: <publisher-name>CRC Press</publisher-name>, <fpage>333</fpage>–<lpage>376</lpage>.</mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauser</surname><given-names>T.</given-names></name><name><surname>Keats</surname><given-names>A.</given-names></name><name><surname>Tarasov</surname><given-names>L.</given-names></name></person-group> (<year>2012</year>). <article-title>Artificial neural network assisted Bayesian calibration of climate models</article-title>. <source>Clim. Dyn</source>. <volume>39</volume>, <fpage>137</fpage>–<lpage>154</lpage>. <pub-id pub-id-type="doi">10.1007/s00382-011-1168-0</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heath</surname><given-names>A.</given-names></name><name><surname>Kunst</surname><given-names>N.</given-names></name><name><surname>Jackson</surname><given-names>C.</given-names></name><name><surname>Strong</surname><given-names>M.</given-names></name><name><surname>Alarid-Escudero</surname><given-names>F.</given-names></name><name><surname>Goldhaber-Fiebert</surname><given-names>J. D.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>Calculating the expected value of sample information in practice: considerations from 3 case studies</article-title>. <source>Med. Decis. Mak</source>. <volume>40</volume>, <fpage>314</fpage>–<lpage>326</lpage>. <pub-id pub-id-type="doi">10.1177/0272989X20912402</pub-id><?supplied-pmid 32297840?><pub-id pub-id-type="pmid">32297840</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hurrion</surname><given-names>R.</given-names></name></person-group> (<year>1997</year>). <article-title>An example of simulation optimisation using a neural network metamodel: Finding the optimum number of kanbans in a manufacturing system</article-title>. <source>J. Operat. Res. Soc</source>. <volume>48</volume>, <fpage>1105</fpage>–<lpage>1112</lpage>. <pub-id pub-id-type="doi">10.1038/sj.jors.2600468</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>A. K.</given-names></name><name><surname>Mao</surname><given-names>J.</given-names></name><name><surname>Mohiuddin</surname><given-names>K.</given-names></name></person-group> (<year>1996</year>). <article-title>Artifical neural networks: a tutorial</article-title>. <source>Computer</source>
<volume>29</volume>, <fpage>31</fpage>–<lpage>44</lpage>. <pub-id pub-id-type="doi">10.1109/2.485891</pub-id></mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jalal</surname><given-names>H.</given-names></name><name><surname>Boudreaux</surname><given-names>M.</given-names></name><name><surname>Dowd</surname><given-names>B.</given-names></name><name><surname>Kuntz</surname><given-names>K. M.</given-names></name></person-group> (<year>2015</year>). <article-title>Measuring decision sensitivity with Monte Carlo simulation and multinomial logistic regression metamodeling</article-title>, in <source>The Society for Medical Decision Making Conference</source> (<publisher-loc>St. Louis, MO</publisher-loc>).</mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jalal</surname><given-names>H.</given-names></name><name><surname>Dowd</surname><given-names>B.</given-names></name><name><surname>Sainfort</surname><given-names>F.</given-names></name><name><surname>Kuntz</surname><given-names>K. M.</given-names></name></person-group> (<year>2013</year>). <article-title>Linear regression metamodeling as a tool to summarize and present simulation model results</article-title>. <source>Med. Decis. Mak</source>. <volume>33</volume>, <fpage>880</fpage>–<lpage>890</lpage>. <pub-id pub-id-type="doi">10.1177/0272989X13492014</pub-id><?supplied-pmid 23811758?><pub-id pub-id-type="pmid">23811758</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jalal</surname><given-names>H.</given-names></name><name><surname>Pechlivanoglou</surname><given-names>P.</given-names></name><name><surname>Krijkamp</surname><given-names>E.</given-names></name><name><surname>Alarid-Escudero</surname><given-names>F.</given-names></name><name><surname>Enns</surname><given-names>E. A.</given-names></name><name><surname>Hunink</surname><given-names>M. G. M.</given-names></name></person-group> (<year>2017</year>). <article-title>An overview of R in health decision sciences</article-title>. <source>Med. Decis. Mak</source>. <volume>37</volume>, <fpage>735</fpage>–<lpage>746</lpage>. <pub-id pub-id-type="doi">10.1177/0272989X16686559</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennedy</surname><given-names>M. C.</given-names></name><name><surname>O'Hagan</surname><given-names>A.</given-names></name></person-group> (<year>2001</year>). <article-title>Bayesian calibration of computer models</article-title>. <source>J. R. Stat. Soc. Ser. B Stat. Methodol</source>. <volume>63</volume>, <fpage>425</fpage>–<lpage>464</lpage>. <pub-id pub-id-type="doi">10.1111/1467-9868.00294</pub-id></mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Khu</surname><given-names>S.-T.</given-names></name><name><surname>Savic</surname><given-names>D.</given-names></name><name><surname>Liu</surname><given-names>Y.</given-names></name><name><surname>Madsen</surname><given-names>H.</given-names></name><name><surname>Science</surname><given-names>C.</given-names></name></person-group> (<year>2004</year>). <article-title>A fast evolutionary-based meta-modelling approach for the calibration of a rainfall-runoff model</article-title>, in <source>Trans. 2nd Biennial Meeting of the International Environmental Modelling and Software Society, iEMSs</source> (<publisher-loc>Osnabruck</publisher-loc>), <fpage>1</fpage>–<lpage>6</lpage>.</mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khuri</surname><given-names>A. I.</given-names></name><name><surname>Mukhopadhyay</surname><given-names>S.</given-names></name></person-group> (<year>2010</year>). <article-title>Response surface methodology</article-title>. <source>Wiley Interdiscipl. Rev. Comput. Stat</source>. <volume>2</volume>, <fpage>128</fpage>–<lpage>149</lpage>. <pub-id pub-id-type="doi">10.1002/wics.73</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kilmer</surname><given-names>R. A.</given-names></name></person-group> (<year>1994</year>). <source>Artificial neural network metamodels of stochastic computer simulations</source> (Ph.D. thesis). Pittsburgh University, <publisher-loc>Pittsburgh, PA, United States</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kilmer</surname><given-names>R. A.</given-names></name></person-group> (<year>1996</year>). <article-title>Applications of artificial neural networks to combat simulations</article-title>. <source>Math. Comput. Modell</source>. <volume>23</volume>, <fpage>91</fpage>–<lpage>99</lpage>. <pub-id pub-id-type="doi">10.1016/0895-7177(95)00220-0</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kilmer</surname><given-names>R. A.</given-names></name><name><surname>Smith</surname><given-names>A. E.</given-names></name><name><surname>Shuman</surname><given-names>L. J.</given-names></name></person-group> (<year>1997</year>). <article-title>An emergency department simulation and a neural network metamodel</article-title>. <source>J. Soc. Health Syst</source>. <volume>5</volume>, <fpage>63</fpage>–<lpage>79</lpage>. <?supplied-pmid 9035024?><pub-id pub-id-type="pmid">9035024</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleijnen</surname><given-names>J.</given-names></name></person-group> (<year>1975</year>). <article-title>A comment on Blanning's “metamodel for sensitivity analysis: the regression metamodel in simulation”</article-title>. <source>Interfaces</source>
<volume>5</volume>, <fpage>21</fpage>–<lpage>23</lpage>. <pub-id pub-id-type="doi">10.1287/inte.5.3.21</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleijnen</surname><given-names>J. P.</given-names></name><name><surname>Sargent</surname><given-names>R. G.</given-names></name></person-group> (<year>2000</year>). <article-title>A methodology for fitting and validating metamodels in simulation</article-title>. <source>Eur. J. Operat. Res</source>. <volume>120</volume>, <fpage>14</fpage>–<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1016/S0377-2217(98)00392-0</pub-id></mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleijnen</surname><given-names>J. P. C.</given-names></name></person-group> (<year>1979</year>). <article-title>Regression metamodels for generalizing simulation results</article-title>. <source>IEEE Trans. Syst. Man Cybernet</source>. <volume>9</volume>, <fpage>93</fpage>–<lpage>96</lpage>. <pub-id pub-id-type="doi">10.1109/TSMC.1979.4310155</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kleijnen</surname><given-names>J. P. C.</given-names></name></person-group> (<year>2015</year>). <source>Design and Analysis of Simulation Experiments, 2nd Edn</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Springer US</publisher-name>. <pub-id pub-id-type="doi">10.1007/978-3-319-18087-8</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleijnen</surname><given-names>J. P. C.</given-names></name><name><surname>Sanchez</surname><given-names>S. M.</given-names></name><name><surname>Lucas</surname><given-names>T. W.</given-names></name><name><surname>Cioppa</surname><given-names>T. M.</given-names></name></person-group> (<year>2005</year>). <article-title>State-of-the-art review: a user's guide to the brave new world of designing simulation experiments</article-title>. <source>INFORMS J. Comput</source>. <volume>17</volume>, <fpage>263</fpage>–<lpage>289</lpage>. <pub-id pub-id-type="doi">10.1287/ijoc.1050.0136</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knudsen</surname><given-names>A. B.</given-names></name><name><surname>Zauber</surname><given-names>A. G.</given-names></name><name><surname>Rutter</surname><given-names>C. M.</given-names></name><name><surname>Naber</surname><given-names>S. K.</given-names></name><name><surname>Doria-Rose</surname><given-names>V. P.</given-names></name><name><surname>Pabiniak</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Estimation of benefits, burden, and harms of colorectal cancer screening strategies: modeling study for the US preventive services task force</article-title>. <source>JAMA</source><volume>10017</volume>, <fpage>2595</fpage>–<lpage>2609</lpage>. <pub-id pub-id-type="doi">10.1001/jama.2016.6828</pub-id><?supplied-pmid 27305518?><pub-id pub-id-type="pmid">27305518</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krijkamp</surname><given-names>E. M.</given-names></name><name><surname>Alarid-Escudero</surname><given-names>F.</given-names></name><name><surname>Enns</surname><given-names>E. A.</given-names></name><name><surname>Jalal</surname><given-names>H. J.</given-names></name><name><surname>Hunink</surname><given-names>M. M.</given-names></name><name><surname>Pechlivanoglou</surname><given-names>P.</given-names></name></person-group> (<year>2018</year>). <article-title>Microsimulation modeling for health decision sciences using R: a tutorial</article-title>. <source>Med. Decis. Mak</source>. <volume>38</volume>, <fpage>400</fpage>–<lpage>422</lpage>. <pub-id pub-id-type="doi">10.1177/0272989X18754513</pub-id><?supplied-pmid 29587047?><pub-id pub-id-type="pmid">29587047</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuntz</surname><given-names>K. M.</given-names></name><name><surname>Lansdorp-Vogelaar</surname><given-names>I.</given-names></name><name><surname>Rutter</surname><given-names>C. M.</given-names></name><name><surname>Knudsen</surname><given-names>A. B.</given-names></name><name><surname>van Ballegooijen</surname><given-names>M.</given-names></name><name><surname>Savarino</surname><given-names>J. E.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>A systematic comparison of microsimulation models of colorectal cancer: the role of assumptions about adenoma progression</article-title>. <source>Med. Decis. Mak</source>. <volume>31</volume>, <fpage>530</fpage>–<lpage>539</lpage>. <pub-id pub-id-type="doi">10.1177/0272989X11408730</pub-id><?supplied-pmid 21673186?><pub-id pub-id-type="pmid">21673186</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lunn</surname><given-names>D.</given-names></name><name><surname>Spiegelhalter</surname><given-names>D.</given-names></name><name><surname>Thomas</surname><given-names>A.</given-names></name><name><surname>Best</surname><given-names>N.</given-names></name></person-group> (<year>2009</year>). <article-title>The bugs project: evolution, critique and future directions</article-title>. <source>Stat. Med</source>. <volume>28</volume>, <fpage>3049</fpage>–<lpage>3067</lpage>. <pub-id pub-id-type="doi">10.1002/sim.3680</pub-id><?supplied-pmid 19630097?><pub-id pub-id-type="pmid">19630097</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mareš</surname><given-names>T.</given-names></name><name><surname>Kučerová</surname><given-names>A.</given-names></name></person-group> (<year>2012</year>). <article-title>Artificial neural networks in calibration of nonlinear models</article-title>, in <source>Life-Cycle and Sustainability of Civil Infrastructure Systems-Proceedings of the Third International Symposium on Life-Cycle Civil Engineering (IALCCE'12)</source> (<publisher-loc>Vienna</publisher-loc>: <publisher-name>CRC Press Stirlingshire</publisher-name>), <fpage>2225</fpage>–<lpage>2232</lpage>.</mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Másson</surname><given-names>E.</given-names></name><name><surname>Wang</surname><given-names>Y.-J.</given-names></name></person-group> (<year>1990</year>). <article-title>Introduction to computation and learning in artificial neural networks</article-title>. <source>Eur. J. Operat. Res</source>. <volume>47</volume>, <fpage>1</fpage>–<lpage>28</lpage>. <pub-id pub-id-type="doi">10.1016/0377-2217(90)90085-P</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menzies</surname><given-names>N. A.</given-names></name><name><surname>Soeteman</surname><given-names>D. I.</given-names></name><name><surname>Pandya</surname><given-names>A.</given-names></name><name><surname>Kim</surname><given-names>J. J.</given-names></name></person-group> (<year>2017</year>). <article-title>Bayesian methods for calibrating health policy models: a tutorial</article-title>. <source>PharmacoEconomics</source>
<volume>35</volume>, <fpage>613</fpage>–<lpage>624</lpage>. <pub-id pub-id-type="doi">10.1007/s40273-017-0494-4</pub-id><?supplied-pmid 28247184?><pub-id pub-id-type="pmid">28247184</pub-id></mixed-citation>
    </ref>
    <ref id="B45">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Michie</surname><given-names>E. D.</given-names></name><name><surname>Spiegelhalter</surname><given-names>D. J.</given-names></name><name><surname>Taylor</surname><given-names>C. C.</given-names></name></person-group> (<year>1994</year>). <source>Machine Learning, Neural and Statistical Classification</source>. <publisher-loc>River, NJ</publisher-loc>: <publisher-name>Ellis Horwood</publisher-name>, <fpage>84</fpage>–<lpage>106</lpage>.</mixed-citation>
    </ref>
    <ref id="B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ndikumana</surname><given-names>A.</given-names></name><name><surname>Tran</surname><given-names>N. H.</given-names></name><name><surname>Kim</surname><given-names>K. T.</given-names></name><name><surname>Hong</surname><given-names>C. S.</given-names></name></person-group> (<year>2020</year>). <article-title>Deep learning based caching for self-driving cars in multi-access edge computing</article-title>. <source>IEEE Trans. Intell. Transport. Syst</source>. <pub-id pub-id-type="doi">10.1109/TITS.2020.2976572</pub-id></mixed-citation>
    </ref>
    <ref id="B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelder</surname><given-names>J.</given-names></name><name><surname>Mead</surname><given-names>R.</given-names></name></person-group> (<year>1965</year>). <article-title>A simplex method for function minimization</article-title>. <source>Computer J</source>. <volume>7</volume>, <fpage>308</fpage>–<lpage>313</lpage>. <pub-id pub-id-type="doi">10.1093/comjnl/7.4.308</pub-id></mixed-citation>
    </ref>
    <ref id="B48">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Neumann</surname><given-names>P. J.</given-names></name><name><surname>Sanders</surname><given-names>G. D.</given-names></name><name><surname>Russell</surname><given-names>L. B.</given-names></name><name><surname>Siegel</surname><given-names>J. E.</given-names></name><name><surname>Ganiats</surname><given-names>T. G.</given-names></name></person-group> (<year>2016</year>). <source>Cost-Effectiveness in Health and Medicine</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>. <pub-id pub-id-type="doi">10.1093/acprof:oso/9780190492939.001.0001</pub-id></mixed-citation>
    </ref>
    <ref id="B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Hagan</surname><given-names>A.</given-names></name><name><surname>Kennedy</surname><given-names>M. C.</given-names></name><name><surname>Oakley</surname><given-names>J. E.</given-names></name></person-group> (<year>1999</year>). <article-title>Uncertainty analysis and other inference tools for complex computer codes</article-title>. <source>Bayesian Staist</source>. <volume>6</volume>, <fpage>503</fpage>–<lpage>524</lpage>.</mixed-citation>
    </ref>
    <ref id="B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olden</surname><given-names>J. D.</given-names></name><name><surname>Lawler</surname><given-names>J. J.</given-names></name><name><surname>Poff</surname><given-names>N. L.</given-names></name></person-group> (<year>2008</year>). <article-title>Machine learning methods without tears: a primer for ecologists</article-title>. <source>Q. Rev. Biol</source>. <volume>83</volume>, <fpage>171</fpage>–<lpage>193</lpage>. <pub-id pub-id-type="doi">10.1086/587826</pub-id><?supplied-pmid 18605534?><pub-id pub-id-type="pmid">18605534</pub-id></mixed-citation>
    </ref>
    <ref id="B51">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paiva</surname><given-names>R. M. D.</given-names></name><name><surname>Carvalho</surname><given-names>A. R.</given-names></name><name><surname>Crawford</surname><given-names>C.</given-names></name><name><surname>Suleman</surname><given-names>A.</given-names></name></person-group> (<year>2010</year>). <article-title>Comparison of surrogate models in a multidisciplinary optimization framework for wing design</article-title>. <source>AIAA J</source>. <volume>48</volume>, <fpage>995</fpage>–<lpage>1006</lpage>. <pub-id pub-id-type="doi">10.2514/1.45790</pub-id></mixed-citation>
    </ref>
    <ref id="B52">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichler</surname><given-names>B.</given-names></name><name><surname>Lackner</surname><given-names>R.</given-names></name><name><surname>Mang</surname><given-names>H. a</given-names></name></person-group>. (<year>2003</year>). <article-title>Back analysis of model parameters in geotechnical engineering by means of soft computing</article-title>. <source>Int. J. Num. Methods Eng</source>. <volume>57</volume>, <fpage>1943</fpage>–<lpage>1978</lpage>. <pub-id pub-id-type="doi">10.1002/nme.740</pub-id></mixed-citation>
    </ref>
    <ref id="B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pierreval</surname><given-names>H.</given-names></name><name><surname>Bernard</surname><given-names>U. C.</given-names></name><name><surname>Novembre</surname><given-names>B.</given-names></name><name><surname>Cedex</surname><given-names>V.</given-names></name></person-group> (<year>1992</year>). <article-title>Training a neural network by simulation for dispatching problems</article-title>. <source>Proc. Third Int. Conf. Comput. Integr. Manufact</source>. <volume>1992</volume>, <fpage>332</fpage>–<lpage>336</lpage>. <pub-id pub-id-type="doi">10.1109/CIM.1992.639120</pub-id></mixed-citation>
    </ref>
    <ref id="B54">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pierreval</surname><given-names>H.</given-names></name><name><surname>Huntsinger</surname><given-names>R. C.</given-names></name></person-group> (<year>1992</year>). <article-title>An investigation on neural network capabilities as simulation metamodels</article-title>, in <source>Proceedings of the 1992 Summer Computer Simulation Conference</source> (<publisher-loc>Troy, NY</publisher-loc>), <fpage>413</fpage>–<lpage>417</lpage>.</mixed-citation>
    </ref>
    <ref id="B55">
      <mixed-citation publication-type="webpage"><person-group person-group-type="author"><collab>R Core Team</collab></person-group> (<year>2018</year>). <source>R: A Language and Environment for Statistical Computing. Vienna: R Foundation for Statistical Computing</source>. Available online at: <ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link></mixed-citation>
    </ref>
    <ref id="B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raftery</surname><given-names>A. E.</given-names></name><name><surname>Bao</surname><given-names>L.</given-names></name></person-group> (<year>2010</year>). <article-title>Estimating and projecting trends in HIV/AIDS generalized epidemics using incremental mixture importance sampling</article-title>. <source>Biometrics</source>
<volume>66</volume>, <fpage>1162</fpage>–<lpage>1173</lpage>. <pub-id pub-id-type="doi">10.1111/j.1541-0420.2010.01399.x</pub-id><?supplied-pmid 20222935?><pub-id pub-id-type="pmid">20222935</pub-id></mixed-citation>
    </ref>
    <ref id="B57">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravì</surname><given-names>D.</given-names></name><name><surname>Wong</surname><given-names>C.</given-names></name><name><surname>Deligianni</surname><given-names>F.</given-names></name><name><surname>Berthelot</surname><given-names>M.</given-names></name><name><surname>Andreu-Perez</surname><given-names>J.</given-names></name><name><surname>Lo</surname><given-names>B.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>Deep learning for health informatics</article-title>. <source>IEEE J. Biomed. Health Inform</source>. <volume>21</volume>, <fpage>4</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1109/JBHI.2016.2636665</pub-id><?supplied-pmid 28055930?><pub-id pub-id-type="pmid">28055930</pub-id></mixed-citation>
    </ref>
    <ref id="B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reis dos Santos</surname><given-names>P. M.</given-names></name><name><surname>Reis dos Santos</surname><given-names>M. I.</given-names></name></person-group> (<year>2009</year>). <article-title>Using subsystem linear regression metamodels in stochastic simulation</article-title>. <source>Eur. J. Operat. Res</source>. <volume>196</volume>, <fpage>1031</fpage>–<lpage>1040</lpage>. <pub-id pub-id-type="doi">10.1016/j.ejor.2008.05.005</pub-id></mixed-citation>
    </ref>
    <ref id="B59">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rojas</surname><given-names>R.</given-names></name></person-group> (<year>1996</year>). <article-title>Statistics and neural networks</article-title>, in <source>Neural Networks</source> (<publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer</publisher-name>) <fpage>229</fpage>–<lpage>264</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-642-61068-4_9</pub-id></mixed-citation>
    </ref>
    <ref id="B60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutter</surname><given-names>C. M.</given-names></name><name><surname>Miglioretti</surname><given-names>D. L.</given-names></name><name><surname>Savarino</surname><given-names>J. E.</given-names></name></person-group> (<year>2009</year>). <article-title>Bayesian calibration of microsimulation models</article-title>. <source>J. Am. Stat. Assoc</source>. <volume>104</volume>, <fpage>1338</fpage>–<lpage>1350</lpage>. <pub-id pub-id-type="doi">10.1198/jasa.2009.ap07466</pub-id><?supplied-pmid 20076767?><pub-id pub-id-type="pmid">20076767</pub-id></mixed-citation>
    </ref>
    <ref id="B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutter</surname><given-names>C. M.</given-names></name><name><surname>Ozik</surname><given-names>J.</given-names></name><name><surname>DeYoreo</surname><given-names>M.</given-names></name><name><surname>Collier</surname><given-names>N.</given-names></name></person-group> (<year>2019</year>). <article-title>Microsimulation model calibration using incremental mixture approximate bayesian computation</article-title>. <source>Ann. Appl. Stat</source>. <volume>13</volume>, <fpage>2189</fpage>–<lpage>2212</lpage>. <pub-id pub-id-type="doi">10.1214/19-AOAS1279</pub-id></mixed-citation>
    </ref>
    <ref id="B62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutter</surname><given-names>C. M.</given-names></name><name><surname>Yu</surname><given-names>O.</given-names></name><name><surname>Miglioretti</surname><given-names>D. L.</given-names></name></person-group> (<year>2007</year>). <article-title>A hierarchical non-homogenous Poisson model for meta-analysis of adenoma counts</article-title>. <source>Stat. Med</source>. <volume>26</volume>, <fpage>98</fpage>–<lpage>109</lpage>. <pub-id pub-id-type="doi">10.1002/sim.2460</pub-id><?supplied-pmid 16372387?><pub-id pub-id-type="pmid">16372387</pub-id></mixed-citation>
    </ref>
    <ref id="B63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sabuncuoglu</surname><given-names>I.</given-names></name><name><surname>Touhami</surname><given-names>S.</given-names></name></person-group> (<year>2002</year>). <article-title>Simulation metamodelling with neural networks: an experimental investigation</article-title>. <source>Int. J. Product. Res</source>. <volume>40</volume>, <fpage>2483</fpage>–<lpage>2505</lpage>. <pub-id pub-id-type="doi">10.1080/00207540210135596</pub-id></mixed-citation>
    </ref>
    <ref id="B64">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sacks</surname><given-names>J.</given-names></name><name><surname>Welch</surname><given-names>W. J.</given-names></name><name><surname>Mitchell</surname><given-names>T. J.</given-names></name><name><surname>Wynn</surname><given-names>H. P.</given-names></name></person-group> (<year>1989</year>). <article-title>Design and analysis of computer experiments</article-title>. <source>Stat. Sci</source>. <volume>4</volume>, <fpage>409</fpage>–<lpage>423</lpage>. <pub-id pub-id-type="doi">10.1214/ss/1177012420</pub-id></mixed-citation>
    </ref>
    <ref id="B65">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Santos</surname><given-names>I. R.</given-names></name><name><surname>Santos</surname><given-names>P. R.</given-names></name></person-group> (<year>2007</year>). <article-title>Simulation metamodels for modeling output distribution parameters</article-title>, in <source>Winter Simulation Conference</source> (<publisher-loc>Washington, DC</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>910</fpage>–<lpage>918</lpage>. <pub-id pub-id-type="doi">10.1109/WSC.2007.4419687</pub-id></mixed-citation>
    </ref>
    <ref id="B66">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidhuber</surname><given-names>J.</given-names></name></person-group> (<year>2015</year>). <article-title>Deep learning in neural networks: an overview</article-title>. <source>Neural Netw</source>. <volume>61</volume>, <fpage>85</fpage>–<lpage>117</lpage>. <pub-id pub-id-type="doi">10.1016/j.neunet.2014.09.003</pub-id><?supplied-pmid 25462637?><pub-id pub-id-type="pmid">25462637</pub-id></mixed-citation>
    </ref>
    <ref id="B67">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevenson</surname><given-names>M. D.</given-names></name><name><surname>Oakley</surname><given-names>J.</given-names></name><name><surname>Chilcott</surname><given-names>J. B.</given-names></name></person-group> (<year>2004</year>). <article-title>Gaussian process modeling in conjunction with individual patient simulation modeling: a case study describing the calculation of cost-effectiveness ratios for the treatment of established osteoporosis</article-title>. <source>Med. Decis. Mak</source>. <volume>24</volume>, <fpage>89</fpage>–<lpage>100</lpage>. <pub-id pub-id-type="doi">10.1177/0272989X03261561</pub-id><?supplied-pmid 15005958?><pub-id pub-id-type="pmid">15005958</pub-id></mixed-citation>
    </ref>
    <ref id="B68">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanni</surname><given-names>T.</given-names></name><name><surname>Karnon</surname><given-names>J.</given-names></name><name><surname>Madan</surname><given-names>J.</given-names></name><name><surname>White</surname><given-names>R. G.</given-names></name><name><surname>Edmunds</surname><given-names>W. J.</given-names></name><name><surname>Foss</surname><given-names>A. M.</given-names></name><etal/></person-group>. (<year>2011</year>). <article-title>Calibrating models in economic evaluation: a seven-step approach</article-title>. <source>PharmacoEconomics</source><volume>29</volume>, <fpage>35</fpage>–<lpage>49</lpage>. <pub-id pub-id-type="doi">10.2165/11584600-000000000-00000</pub-id><?supplied-pmid 21142277?><pub-id pub-id-type="pmid">21142277</pub-id></mixed-citation>
    </ref>
    <ref id="B69">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L.</given-names></name></person-group> (<year>2005</year>). <article-title>A hybrid genetic algorithm-neural network strategy for simulation optimization</article-title>. <source>Appl. Math. Comput</source>. <volume>170</volume>, <fpage>1329</fpage>–<lpage>1343</lpage>. <pub-id pub-id-type="doi">10.1016/j.amc.2005.01.024</pub-id></mixed-citation>
    </ref>
    <ref id="B70">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinstein</surname><given-names>M. C.</given-names></name><name><surname>O'Brien</surname><given-names>B.</given-names></name><name><surname>Hornberger</surname><given-names>J.</given-names></name><name><surname>Jackson</surname><given-names>J.</given-names></name><name><surname>Johannesson</surname><given-names>M.</given-names></name><name><surname>McCabe</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2003</year>). <article-title>Principles of good practice for decision analytic modeling in health-care evaluation: report of the ISPOR Task Force on Good Research Practices-Modeling Studies</article-title>. <source>Value Health</source><volume>6</volume>, <fpage>9</fpage>–<lpage>17</lpage>. <pub-id pub-id-type="doi">10.1046/j.1524-4733.2003.00234.x</pub-id><?supplied-pmid 12535234?><pub-id pub-id-type="pmid">12535234</pub-id></mixed-citation>
    </ref>
    <ref id="B71">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Weiser Friedman</surname><given-names>L.</given-names></name></person-group> (<year>1996</year>). <source>The Simulation Metamodel</source>. <publisher-loc>Norwell, MA</publisher-loc>: <publisher-name>Kluwer Academic Publishers</publisher-name>. <pub-id pub-id-type="doi">10.1007/978-1-4613-1299-4</pub-id></mixed-citation>
    </ref>
    <ref id="B72">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>G. H.-M.</given-names></name><name><surname>Wang</surname><given-names>Y.-M.</given-names></name><name><surname>Yen</surname><given-names>A. M.-F.</given-names></name><name><surname>Wong</surname><given-names>J.-M.</given-names></name><name><surname>Lai</surname><given-names>H.-C.</given-names></name><name><surname>Warwick</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2006</year>). <article-title>Cost-effectiveness analysis of colorectal cancer screening with stool DNA testing in intermediate-incidence countries</article-title>. <source>BMC Cancer</source><volume>6</volume>:<fpage>136</fpage>. <pub-id pub-id-type="doi">10.1186/1471-2407-6-136</pub-id><?supplied-pmid 16723013?><pub-id pub-id-type="pmid">16723013</pub-id></mixed-citation>
    </ref>
    <ref id="B73">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zobel</surname><given-names>C. W.</given-names></name><name><surname>Keeling</surname><given-names>K. B.</given-names></name></person-group> (<year>2008</year>). <article-title>Neural network-based simulation metamodels for predicting probability distributions</article-title>. <source>Comput. Indus. Eng</source>. <volume>54</volume>, <fpage>879</fpage>–<lpage>888</lpage>. <pub-id pub-id-type="doi">10.1016/j.cie.2007.08.012</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
