<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5860029</article-id>
    <article-id pub-id-type="pmid">28582478</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btx325</article-id>
    <article-id pub-id-type="publisher-id">btx325</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Papers</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Gene Expression</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Model-based branching point detection in single-cell data by K-branches clustering</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chlis</surname>
          <given-names>Nikolaos K</given-names>
        </name>
        <xref ref-type="aff" rid="btx325-aff1">1</xref>
        <xref ref-type="aff" rid="btx325-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wolf</surname>
          <given-names>F Alexander</given-names>
        </name>
        <xref ref-type="aff" rid="btx325-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Theis</surname>
          <given-names>Fabian J</given-names>
        </name>
        <xref ref-type="aff" rid="btx325-aff1">1</xref>
        <xref ref-type="aff" rid="btx325-aff2">2</xref>
        <xref ref-type="aff" rid="btx325-aff3">3</xref>
        <xref ref-type="corresp" rid="btx325-cor1"/>
        <!--<email>fabian.theis@helmholtz-muenchen.de</email>-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Bar-Joseph</surname>
          <given-names>Ziv</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <aff id="btx325-aff1"><label>1</label>Institute of Computational Biology, Helmholtz Zentrum München, Neuherberg, Germany</aff>
    <aff id="btx325-aff2"><label>2</label>School of Life Sciences Weihenstephan, Technical University of Munich, Freising, Germany</aff>
    <aff id="btx325-aff3"><label>3</label>Department of Mathematics, Technical University of Munich, Garching, Germany</aff>
    <author-notes>
      <corresp id="btx325-cor1">To whom correspondence should be addressed. Email: <email>fabian.theis@helmholtz-muenchen.de</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>15</day>
      <month>10</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2017-06-05">
      <day>05</day>
      <month>6</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>6</month>
      <year>2017</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>33</volume>
    <issue>20</issue>
    <fpage>3211</fpage>
    <lpage>3219</lpage>
    <history>
      <date date-type="received">
        <day>16</day>
        <month>12</month>
        <year>2016</year>
      </date>
      <date date-type="rev-recd">
        <day>11</day>
        <month>4</month>
        <year>2017</year>
      </date>
      <date date-type="accepted">
        <day>31</day>
        <month>5</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author 2017. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2017</copyright-year>
      <license xlink:href="http://creativecommons.org/licenses/by-nc/4.0/" license-type="cc-by-nc">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btx325.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="SA1">
        <title>Motivation</title>
        <p>The identification of heterogeneities in cell populations by utilizing single-cell technologies such as single-cell RNA-Seq, enables inference of cellular development and lineage trees. Several methods have been proposed for such inference from high-dimensional single-cell data. They typically assign each cell to a branch in a differentiation trajectory. However, they commonly assume specific geometries such as tree-like developmental hierarchies and lack statistically sound methods to decide on the number of branching events.</p>
      </sec>
      <sec id="SA2">
        <title>Results</title>
        <p>We present K-Branches, a solution to the above problem by locally fitting half-lines to single-cell data, introducing a clustering algorithm similar to K-Means. These halflines are proxies for branches in the differentiation trajectory of cells. We propose a modified version of the GAP statistic for model selection, in order to decide on the number of lines that best describe the data locally. In this manner, we identify the location and number of subgroups of cells that are associated with branching events and full differentiation, respectively. We evaluate the performance of our method on single-cell RNA-Seq data describing the differentiation of myeloid progenitors during hematopoiesis, single-cell qPCR data of mouse blastocyst development, single-cell qPCR data of human myeloid monocytic leukemia and artificial data.</p>
      </sec>
      <sec id="SA3">
        <title>Availability and implementation</title>
        <p>An R implementation of K-Branches is freely available at <ext-link ext-link-type="uri" xlink:href="https://github.com/theislab/kbranches">https://github.com/theislab/kbranches</ext-link>.</p>
      </sec>
      <sec id="SA4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Recent advances in single-cell technologies have led to the discovery and characterization of novel cell types in multicellular organisms. Studying diverse cell populations that differ in morphology and function can pinpoint distinct cell types in different stages of regulatory processes, such as cellular development. For example, single-cell methods have led to new discoveries related to hematopoietic stem cells (<xref rid="btx325-B15" ref-type="bibr">Moignard <italic>et al.</italic>, 2015</xref>; <xref rid="btx325-B16" ref-type="bibr">Paul <italic>et al.</italic>, 2015</xref>), as well as the immune system (<xref rid="btx325-B9" ref-type="bibr">Jaitin <italic>et al.</italic>, 2014</xref>; <xref rid="btx325-B13" ref-type="bibr">Mahata <italic>et al.</italic>, 2014</xref>; <xref rid="btx325-B17" ref-type="bibr">Proserpio <italic>et al.</italic>, 2016</xref>).</p>
    <p>The development of novel computational techniques for the analysis of single-cell data is an active research topic in the field of bioinformatics (<xref rid="btx325-B3" ref-type="bibr">de Vargas Roditi and Claassen, 2015</xref>; <xref rid="btx325-B4" ref-type="bibr">Grün and van Oudenaarden, 2015</xref>; <xref rid="btx325-B20" ref-type="bibr">Stegle <italic>et al.</italic>, 2015</xref>). The key idea of the Waddington epigenetic landscape (<xref rid="btx325-B24" ref-type="bibr">Waddington, 1942</xref>, <xref rid="btx325-B25" ref-type="bibr">1957</xref>) is that individual cells can be mapped from a high-dimensional space to a low-dimensional manifold of trajectories that reflect the continuous regulatory processes. As a result, a number of methods have been proposed that can reconstruct differentiation trajectories, given snapshot data of individual cells in different stages of the differentiation process, such as Monocle (<xref rid="btx325-B23" ref-type="bibr">Trapnell <italic>et al.</italic>, 2014</xref>), Wishbone (<xref rid="btx325-B19" ref-type="bibr">Setty <italic>et al.</italic>, 2016</xref>), Diffusion Pseudotime (DPT) (<xref rid="btx325-B7" ref-type="bibr">Haghverdi <italic>et al.</italic>, 2016</xref>), SLICER (<xref rid="btx325-B26" ref-type="bibr">Welch <italic>et al.</italic>, 2016</xref>) and TSCAN (<xref rid="btx325-B10" ref-type="bibr">Ji and Ji, 2016</xref>). Given a ‘root’ cell as a starting point, most of these methods can also calculate an ordering of the cells (pseudotime) based on the stage each cell is in the differentiation process. However, with the exception of DPT, while these methods are successful in assigning cells to discrete differentiation trajectories (branches) they do not tackle the problem of identifying the local dimensionality around each cell. That is, identifying branching regions of cells not yet strongly associated to any branch, intermediate regions along a branch and tip regions of fully differentiated cells. Moreover, all the above methods lack a sound statistical model to identify the existence and number of cell subgroups associated to branching events. Finally, while TSCAN employs model selection to decide on the number of cell-clusters, it does not aim to identify branching and tip regions.</p>
    <p>In this study, we propose a data driven, model-based clustering method that identifies the exact number of ‘branching regions’, as well as the exact number of fully differentiated ‘tip regions’ in the lineage tree. The method then proceeds to assign each cell to a branching, intermediate or tip region. The proposed methodology does not aim to infer a pseudotemporal ordering of the cells and as such no ‘root’ cell needs to be defined. Moreover, since characterization of each cell is based on local information in the differentiation trajectory, the method can successfully identify cells belonging to the aforementioned regions of interest in trajectories of arbitrary geometry.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Problem formulation</title>
      <p>Given a center <bold>c</bold> and direction <bold>v</bold>, a halfline <italic>L</italic> is defined as the set of points satisfying <inline-formula id="IE1"><mml:math id="IM1"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mo>·</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>}</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mtext>with</mml:mtext><mml:mo> </mml:mo><mml:mi mathvariant="bold">l</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">v</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. We aim to find <italic>K</italic> halflines <inline-formula id="IE2"><mml:math id="IM2"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with a common center <bold>c</bold> and <italic>K</italic> distinct direction vectors <inline-formula id="IE3"><mml:math id="IM3"><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In this case, each halfline <italic>L<sub>k</sub></italic> corresponds one cluster <italic>C<sub>k</sub></italic>. As a prerequisite to defining a cost function, note that the Euclidean distance of a given point <bold>x</bold> to a halfline <italic>L<sub>k</sub></italic> reads:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">{</mml:mo><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mtext>if</mml:mtext><mml:mo> </mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow><mml:mo> </mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
Additionally, one may also use other distance metrics (<xref rid="btx325-B11" ref-type="bibr">Kiselev <italic>et al.</italic>, 2017</xref>).</p>
      <p>The clustering method aims to assign each of the given data points (cells) into its closest halfline, while minimizing the total cost. In other words, the goal is to identify the center <bold>c</bold>, as well as the direction vectors <inline-formula id="IE4"><mml:math id="IM4"><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of unit length that minimize the overall clustering cost. To this end, we define the cost function <italic>J</italic> to describe the total dispersion, which corresponds to the sum of dispersions over the <italic>K</italic> clusters and reads:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mi>J</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>d</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo>|</mml:mo><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE5"><mml:math id="IM5"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup><mml:mo>∪</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> corresponds to all elements in cluster <italic>k</italic> and <inline-formula id="IE6"><mml:math id="IM6"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> correspond to the sets of elements in cluster <italic>k</italic> with negative and positive dot product to all vectors in the direction of <italic>L<sub>k</sub></italic>, respectively.</p>
      <p>The main idea of the proposed methodology is to perform local clustering in single-cell trajectories, by fitting <italic>K</italic> halflines (branches) that share a common center. Then, model selection is applied to identify the number of <italic>K</italic> branches best fitting the local neighborhood around each cell. Thus, the local structure of single-cell trajectories is identified and each cell is assigned to a tip, intermediate or branching region, as illustrated in <xref ref-type="fig" rid="btx325-F1">Figure 1</xref>.
</p>
      <fig id="btx325-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p>Local application of K-Branches clustering reveals tip, intermediate and branching regions in single-cell trajectories. <bold>(A)</bold> Each cell is used as the center of the branches (halflines) and local clustering is performed in its neighborhood. Then, by using model selection the center cell is either characterized as a tip cell, a cell belonging to an intermediate region or a cell belonging to a branching region depending on which of the three models (<inline-formula id="IE7"><mml:math id="IM7"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mtext>or</mml:mtext><mml:mo> </mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> branches) best describes the structure of the neighborhood. <bold>(B)</bold> After local clustering is performed on the dataset, cells belonging to three tips (T1, T2, T3) and one branching region (B1) have been identified, while the rest of the cells are considered to belong to intermediate regions. The exact number of tip and branching regions is inferred from the data and does not need to be specified by the user</p>
        </caption>
        <graphic xlink:href="btx325f1"/>
      </fig>
      <sec>
        <title>2.1.1 The K-Branches clustering method</title>
        <p>In order to calculate the model parameters, after random initialization we follow an EM-like iterative optimization procedure similar to that of K-Means (<xref rid="btx325-B8" ref-type="bibr">Hastie <italic>et al.</italic>, 2009</xref>). Namely, we iteratively (i) assign data points to their closest cluster and (ii) update the estimates of <bold>c</bold> and <inline-formula id="IE8"><mml:math id="IM8"><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> while minimizing <italic>J</italic> in each step, until convergence. Since the method might converge to a local optimum of the cost function, multiple executions using different initializations have to be carried out. The method is randomly initialized by assigning one random data point as the center <bold>c</bold> and <italic>K</italic> other random data points as the direction vectors <inline-formula id="IE9"><mml:math id="IM9"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>v</mml:mi></mml:mstyle></mml:msub><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>v</mml:mi></mml:mstyle></mml:msub><mml:msub><mml:mrow/><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In the following subsections we present the update equations for the center and directions, respectively.</p>
      </sec>
      <sec>
        <title>2.1.2 Estimating the center of the halflines</title>
        <p>First, we optimize the cost function <italic>J</italic> with respect to the center of the halflines <bold>c</bold>. Therefore, we have to calculate the gradient <inline-formula id="IE10"><mml:math id="IM10"><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:msub><mml:mi>J</mml:mi></mml:mrow></mml:math></inline-formula>, as follows:
<disp-formula id="E3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>c</mml:mi></mml:mstyle></mml:msub><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>c</mml:mi></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>A</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msub><mml:msup><mml:mrow/><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>c</mml:mi></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where the matrix <inline-formula id="IE11"><mml:math id="IM11"><mml:mrow><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is defined as:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
with <inline-formula id="IE12"><mml:math id="IM12"><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p>
        <p>The equation <inline-formula id="IE13"><mml:math id="IM13"><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi mathvariant="bold">c</mml:mi></mml:msub><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> can be solved in closed form, and the optimal <bold>c</bold> reads:
<disp-formula id="E5"><label>(5)</label><mml:math id="M5"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi mathvariant="bold">c</mml:mi><mml:mrow><mml:mtext>opt</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mrow><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup><mml:mo>|</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE14"><mml:math id="IM14"><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>±</mml:mo></mml:msubsup><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> refers to the size of the set <inline-formula id="IE15"><mml:math id="IM15"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>±</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. In the case <italic>K</italic> = 1 the right part of <xref ref-type="disp-formula" rid="E5">Equation (5)</xref> simplifies to <inline-formula id="IE16"><mml:math id="IM16"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup><mml:mo>|</mml:mo><mml:mi mathvariant="bold">I</mml:mi><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">A</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, which is not full rank and therefore not invertible when <inline-formula id="IE17"><mml:math id="IM17"><mml:mrow><mml:mo>|</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>−</mml:mo></mml:msubsup><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Although the method for local clustering introduced in a subsequent section is also performed with <italic>K</italic> = 1, it uses a fixed center <bold>c</bold>, rendering the above limitation irrelevant.</p>
      </sec>
      <sec>
        <title>2.1.3 Estimating the directions of the halflines</title>
        <p>To optimize the cost function <italic>J</italic> with respect to the direction vector of unit length <inline-formula id="IE18"><mml:math id="IM18"><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we have to calculate the gradient <inline-formula id="IE19"><mml:math id="IM19"><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>J</mml:mi></mml:mrow></mml:math></inline-formula>, as follows:
<disp-formula id="E6"><label>(6)</label><mml:math id="M6"><mml:mrow><mml:mtable><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>J</mml:mi></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"/><mml:mtd columnalign="left"><mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula>
Assuming that <inline-formula id="IE20"><mml:math id="IM20"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is as matrix whose <italic>i</italic>th row corresponds to <inline-formula id="IE21"><mml:math id="IM21"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mi>C</mml:mi><mml:mi>k</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula>, then setting <inline-formula id="IE22"><mml:math id="IM22"><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>J</mml:mi></mml:mrow></mml:math></inline-formula> to zero is equivalent to computing the first eigenvector of <inline-formula id="IE23"><mml:math id="IM23"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
        <p>The pseudocode for the K-Branches algorithm is presented in <bold>Algorithm 1</bold>, while a comparison between K-Branches and K-Means is illustrated in <xref ref-type="fig" rid="btx325-F2">Figure 2</xref>.
<boxed-text id="btx325-BOX1" position="float" orientation="portrait"><label>Algorithm 1 </label><caption><p>K-Branches clustering</p></caption><p>1: <bold>Inputs:</bold> K: number of clusters, <inline-formula id="IE24"><mml:math id="IM24"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: data points</p><p>2: Random initialization of <inline-formula id="IE25"><mml:math id="IM25"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p><p>3: <bold>for</bold><italic>n</italic> in 1:N <bold>do</bold>     ▹ N: number of all data points</p><p>4:   assign <inline-formula id="IE26"><mml:math id="IM26"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to nearest <italic>L<sub>k</sub></italic>, according to <xref ref-type="disp-formula" rid="E1">Equation (1)</xref></p><p>5: <bold>end for</bold></p><p>6: <bold>repeat</bold></p><p>7:   update the center <bold>c</bold>   ▹ according to <xref ref-type="disp-formula" rid="E5">Equation (5)</xref></p><p>8:   update the direction vectors <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p><p>9:   <bold>for</bold> n in 1:N <bold>do</bold></p><p>10:    assign <inline-formula id="IE28"><mml:math id="IM28"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to nearest <italic>L<sub>k</sub></italic>, according to <xref ref-type="disp-formula" rid="E1">Equation (1)</xref></p><p>11:  <bold>end for</bold></p><p>12: <bold>until</bold> no change in cluster assignments</p></boxed-text><boxed-text id="btx325-BOX2" position="float" orientation="portrait"><label>Algorithm 2 </label><caption><p>K-Branches clustering, medoid version</p></caption><p>1: <bold>Inputs:</bold> K: number of clusters, <inline-formula id="IE29"><mml:math id="IM29"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>N</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>: data points</p><p>2: <bold>Define:</bold><inline-formula id="IE30"><mml:math id="IM30"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> ▹ medoid indices <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:mo>⊆</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula></p><p>3: Random initialization of <inline-formula id="IE32"><mml:math id="IM32"><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> ▹ to random indices</p><p>4: <bold>for</bold> n in 1:N <bold>do</bold>  ▹ N: number of all data points</p><p>5:  assign <inline-formula id="IE33"><mml:math id="IM33"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to nearest <italic>L<sub>k</sub></italic>, according to <xref ref-type="disp-formula" rid="E1">Equation (1)</xref></p><p>6: <bold>end for</bold></p><p>7: <bold>while</bold> total cost <italic>J</italic> decreases <bold>do</bold> ▹ Repeat until convergence</p><p>8:  <inline-formula id="IE34"><mml:math id="IM34"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∉</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>    ▹ update the center</p><p>9:  <bold>for</bold> k in 1:K <bold>do</bold>     ▹ iterate over K directions</p><p>10:   <inline-formula id="IE35"><mml:math id="IM35"><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mtext>argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>∉</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>J</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> ▹ update the directions</p><p>11: <bold>end for</bold></p><p>12: <bold>for</bold><italic>n</italic> in 1:N <bold>do</bold></p><p>13:     assign <inline-formula id="IE36"><mml:math id="IM36"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to nearest <italic>L<sub>k</sub></italic>, according to <xref ref-type="disp-formula" rid="E1">Equation (1)</xref></p><p>14: <bold>end for</bold></p><p>15: <bold>end while</bold></p></boxed-text></p>
        <fig id="btx325-F2" orientation="portrait" position="float">
          <label>Fig. 2.</label>
          <caption>
            <p>Illustration of K-Branches clustering on artificial data and comparison to K-Means. <bold>(A)</bold> Original data <bold>(B)</bold> In the case artificial data, K-Branches successfully clusters the three halflines. The center of the halflines as well as the lines corresponding to the direction of each cluster are plotted on top of the data points. The medoids version yields almost identical results for the same data. <bold>(C)</bold> Unlike K-Branches, K-Means (also with <italic>K</italic> = 3) merges part of the bottom halfline into the middle cluster. Since K-Means clusters points in spherical clusters, it is clearly not suitable for clustering data points which belong to distinct differentiation trajectories</p>
          </caption>
          <graphic xlink:href="btx325f2"/>
        </fig>
      </sec>
      <sec>
        <title>2.1.4 <italic>Medoid</italic> version of K-Branches</title>
        <p>As in K-Means, the K-Branches method described above determines a ‘centroid’ <inline-formula id="IE37"><mml:math id="IM37"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> per cluster, which depends on arbitrary vectors <inline-formula id="IE38"><mml:math id="IM38"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi mathvariant="bold">k</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. We can easily modify this to use data points, as in K-Medoids (<xref rid="btx325-B8" ref-type="bibr">Hastie <italic>et al.</italic>, 2009</xref>; <xref rid="btx325-B21" ref-type="bibr">Theodoridis and Koutroumbas, 2008</xref>). The goal of the <italic>Medoid</italic> version of K-Branches is to identify one data point as the center <italic>medoid</italic><inline-formula id="IE39"><mml:math id="IM39"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <italic>K</italic> data points as the direction <italic>medoids</italic><inline-formula id="IE40"><mml:math id="IM40"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>v</mml:mi></mml:mstyle></mml:msub><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>v</mml:mi></mml:mstyle></mml:msub><mml:msub><mml:mrow/><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. That is, the model parameters now correspond to <italic>K</italic> + 1 data points, instead of <italic>K</italic> + 1 points in <inline-formula id="IE41"><mml:math id="IM41"><mml:mrow><mml:msup><mml:mo>ℝ</mml:mo><mml:mi>P</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, where <italic>P</italic> the number of dimensions. Similar to K-Medoids, the proposed algorithm searches over all data points during each iteration in a greedy manner and identifies the data points that minimize the cost function <italic>J</italic> given by <xref ref-type="disp-formula" rid="E2">Equation (2)</xref>. All medoids are reassigned during each iteration of the algorithm, until a local minimum for <italic>J</italic> is reached and the total clustering cost cannot be further decreased. At this point, the algorithm converges to a solution where one of the data points is the center medoid <inline-formula id="IE42"><mml:math id="IM42"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> of the halflines and <italic>K</italic> data points correspond to the direction medoids <inline-formula id="IE43"><mml:math id="IM43"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>v</mml:mi></mml:mstyle></mml:msub><mml:msub><mml:mrow/><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>v</mml:mi></mml:mstyle></mml:msub><mml:msub><mml:mrow/><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The relationship between the original and the medoid version is similar to that of K-Means and K-Medoids. That is, the medoid version is more robust in selecting the center of the halflines with respect to non-global optima and usually even only one random initialization is sufficient in practice. In the original algorithm, calculating the parameters <inline-formula id="IE44"><mml:math id="IM44"><mml:mrow><mml:mi mathvariant="bold">c</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">v</mml:mi><mml:mi>K</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> requires time proportional to the number of data points <italic>O</italic>(<italic>N</italic>). A speedup of the medoid version is possible by computing the distance matrix <bold>D</bold> only once, where <inline-formula id="IE45"><mml:math id="IM45"><mml:mrow><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula>. Then, the distance of a data point <inline-formula id="IE46"><mml:math id="IM46"><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> to a halfline <inline-formula id="IE47"><mml:math id="IM47"><mml:mrow><mml:mi>L</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">v</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> can be computed in O(1) time from <xref ref-type="disp-formula" rid="E7">Equation (8)</xref>. However, for every one of the <inline-formula id="IE48"><mml:math id="IM48"><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula><italic>candidate</italic> medoids, the distance to every other data point is taken into consideration to calculate the overall clustering cost. As a result, <inline-formula id="IE49"><mml:math id="IM49"><mml:mrow><mml:mi>O</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> time is required to update the medoids during every iteration.
<disp-formula id="E7"><label>(8)</label><mml:math id="M7"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">v</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">c</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi mathvariant="bold">D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi mathvariant="bold">D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="bold">D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>v</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi mathvariant="bold">D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
To summarize, in cases where robustness in the identification of the center of the halflines is crucial, the medoid version might be preferable. In applications where robust identification of the center of the halflines is not as crucial, especially in larger datasets, the original version of the algorithm could be preferable. Last, in cases where the center of the halflines is known (or held fixed), such as the case of local clustering presented later in the methods section, there is no advantage to using the medoid over the original version, since both are equally robust in identifying the directions of the halflines.</p>
      </sec>
    </sec>
    <sec>
      <title>2.2 Identifying branching and tip regions through local clustering</title>
      <sec>
        <title>2.2.1 Local clustering</title>
        <p>In this section we derive a method for the identification of” regions of interest” in single-cell data, in particular, the identification of branching regions and tips of branches in lineage trees of differentiating single cells. The main idea is to center the previous model on each data point and adopt a local perspective by examining only the neighborhood of <italic>S</italic> nearest neighbors to the center. We will show that by fixing the center of the halflines on a given data point and fitting the previous model of <italic>K</italic> halflines using a neighborhood size of <italic>S</italic> data points, one can infer whether the center data point itself belongs to branching, intermediate or tip region, through appropriate model selection.</p>
      </sec>
      <sec>
        <title>2.2.2 Selection of the neighborhood size <italic>S</italic></title>
        <p>The proposed method utilizes a number of <italic>S</italic> nearest neighbors to extract the neighborhood of the center data point that is being examined. The size of the neighborhood must be sufficiently large to reflect the local structure of the data, without capturing irrelevant global information. The proposed method is able to automatically suggest a value for <italic>S</italic> using a threshold on <inline-formula id="IE50"><mml:math id="IM50"><mml:mrow><mml:mo>δ</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, which ensures that the average cumulative squared distance <italic>δ</italic> of each data point to all other data points in the dataset is kept at a constant value. Moreover, the accompanying software package provides the option of visualization and manual fine tuning of <italic>S</italic> through a graphical user interface. <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S1</xref> demonstrates the effect of neighborhood size in the overall performance of the method on a toy model of differentiation (<xref rid="btx325-B6" ref-type="bibr">Haghverdi <italic>et al.</italic>, 2015</xref>).</p>
      </sec>
      <sec>
        <title>2.2.3 Neighborhood scaling</title>
        <p>Another challenging aspect is related to datasets showing strong variation in the density of data points along the differentiation trajectories. For example, in the dataset of <xref rid="btx325-B5" ref-type="bibr">Guo <italic>et al.</italic>, 2010</xref>), there are sparse and dense regions. Variability of data point density might reflect an artifact of the data acquisition process, or could be a result of the underlying biological system. In the datasets examined so far, regions of very low density do not pose a threat to the performance of the method, since efficient selection of <italic>S</italic> will expand the neighborhood size accordingly. On the other hand, the fixed number of <italic>S</italic> neighbors may drastically shrink the size of the neighborhood in regions of very high density. To compensate for this effect, an appropriate heuristic rule was implemented. To be precise, for a given number of <italic>S</italic> neighbors, we calculate the median neighborhood radius <inline-formula id="IE51"><mml:math id="IM51"><mml:mrow><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> over all neighborhoods of size <italic>S</italic>. The neighborhood scaling scheme is as follows: prior to performing local clustering for the <italic>i</italic>th data point, its neighborhood radius <italic>ρ<sub>i</sub></italic> (which corresponds to its distance to the furthest point in the neighborhood) is calculated and the condition <inline-formula id="IE52"><mml:math id="IM52"><mml:mrow><mml:msub><mml:mo>ρ</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> is assessed. If it is true, clustering is performed as usual. Otherwise, the neighborhood size (<italic>S</italic>) of the <italic>i</italic>th data point is increased until <inline-formula id="IE53"><mml:math id="IM53"><mml:mrow><mml:msub><mml:mo>ρ</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:math></inline-formula> holds.</p>
      </sec>
      <sec>
        <title>2.2.4 Local model selection</title>
        <p>The goal is to infer whether each data point belongs to a tip, intermediate or branching region of a differentiation trajectory, based on local clustering. That is, using a given data point as the fixed center <bold>c</bold> of the halflines, three different models are fit using <italic>K</italic> = 1, 2 and 3 halflines. The aim of the model selection step in the problem at hand is to identify the clustering model, i.e. the value of <italic>K</italic>, that best fits the data of the local neighborhood centred around the data point in question. If one halfline best fits the neighborhood, then the central data point belongs to a branch tip. If two halflines provide the best fit, then the central data point belongs to an intermediate region. If three halflines best fit the local neighborhood, then the central data point belongs to a branching region. Although values of <italic>K</italic> &gt; 3 could in theory be considered for local clustering and model selection, we have observed that <italic>K</italic> = 3 is sufficient in practice for the identification of branching regions. Therefore, the computational overhead of assessing additional values of <italic>K</italic> can be safely avoided.</p>
        <p>The GAP statistic (<xref rid="btx325-B22" ref-type="bibr">Tibshirani <italic>et al.</italic>, 2001</xref>) is a popular method for identifying the number of clusters that best fit some given data. It depends on the sum of pairwise distances of points in each cluster. If the Euclidean distance is used as the distance measure, it corresponds to the dispersion around the cluster means (clustering cost). The GAP statistic compares the decrease in the clustering cost of the original data with the decrease in clustering cost of data drawn from a null distribution where no natural cluster structure exists. In theory, the dispersion in the data sampled from the null distribution decreases monotonically as <italic>K</italic> increases, while the dispersion in the original data drops rapidly for the value of <italic>K</italic> that best fits the dataset. Thus, the GAP statistic is maximized when the best value of <italic>K</italic> is used for clustering. Assuming that the Euclidean distance is used as the distance measure, the total within-cluster-dispersion <italic>W<sub>K</sub></italic> (<xref rid="btx325-B22" ref-type="bibr">Tibshirani <italic>et al.</italic>, 2001</xref>) is:
<disp-formula id="E8"><label>(9)</label><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mo>μ</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE54"><mml:math id="IM54"><mml:mrow><mml:msub><mml:mi mathvariant="bold">μ</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the mean cluster of <italic>k</italic>. Then, the equation for the GAP statistic for a given number of clusters <italic>K</italic> reads:
<disp-formula id="E9"><label>(10)</label><mml:math id="M9"><mml:mrow><mml:msub><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo>{</mml:mo><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>K</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:mo>}</mml:mo><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE55"><mml:math id="IM55"><mml:mrow><mml:mi>E</mml:mi><mml:mo>{</mml:mo><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>K</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:mo>}</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mrow><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the dispersions <inline-formula id="IE56"><mml:math id="IM56"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> are calculated by applying <xref ref-type="disp-formula" rid="E8">Equation (9)</xref> after performing clustering on each of the <inline-formula id="IE57"><mml:math id="IM57"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula> bootstrap datasets (of the same size as the original dataset) drawn from the null reference distribution.</p>
        <p>In the case of local K-Branches clustering, we introduce a modification of the GAP statistic that calculates the dispersion around halflines, as follows:
<disp-formula id="E10"><label>(11)</label><mml:math id="M10"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
where <inline-formula id="IE58"><mml:math id="IM58"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:math></inline-formula> is given by <xref ref-type="disp-formula" rid="E1">Equation (1)</xref>. Moreover, in contrast to the original GAP we do not take the logarithm of the dispersion, since it has been reported to overestimate the number of clusters in some cases (<xref rid="btx325-B14" ref-type="bibr">Mohajer <italic>et al.</italic>, 2010</xref>). Finally, the modified GAP statistic is given by:
<disp-formula id="E11"><label>(12)</label><mml:math id="M11"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>B</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>B</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
The dispersions <inline-formula id="IE59"><mml:math id="IM59"><mml:mrow><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mi>W</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>*</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> are calculated by applying <xref ref-type="disp-formula" rid="E10">Equation (11)</xref> after performing clustering on each of the <inline-formula id="IE60"><mml:math id="IM60"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula> bootstrap datasets (of the same size as the original dataset) drawn from the null reference distribution.</p>
        <p>To summarize, given a data point as the center of the halflines, local clustering is performed. Then, if <inline-formula id="IE61"><mml:math id="IM61"><mml:mrow><mml:msub><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, it belongs to a branch tip. Otherwise, if the data point does not belong to a tip and <inline-formula id="IE62"><mml:math id="IM62"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> holds, it belongs to an intermediate region. Finally, if the data point does not belong to a tip and <inline-formula id="IE63"><mml:math id="IM63"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, it belongs to a branching region. Both the original and modified versions of the GAP statistic are necessary for model selection and are complementary to each other. That is, <italic>GAP</italic> can identify tip cells (<xref ref-type="fig" rid="btx325-F3">Fig. 3C</xref>) but is not suitable for separating intermediate from branching cells (<xref ref-type="fig" rid="btx325-F3">Fig. 3D</xref>). On the other hand, <inline-formula id="IE64"><mml:math id="IM64"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> can separate intermediate and branching cells (<xref ref-type="fig" rid="btx325-F3">Fig. 3E</xref>), but it not suitable for identifying tip cells, since it would falsely identify a large number of branching cells as tip cells (<xref ref-type="fig" rid="btx325-F3">Fig. 3F</xref>). The performance comparison of the different GAP statistics is illustrated in <xref ref-type="fig" rid="btx325-F3">Figure 3</xref>. Moreover, the behavior of the GAP statistic when additional noise is added is illustrated in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S2</xref>. After all data points have been assigned to tip, intermediate and branching regions, an optional filtering of each cell’s label (tip, branching, or intermediate) based of the values of a few (e.g. 5) nearest neighbors can be performed to aid in smoothing out any random false positives caused by the inherent stochasticity of the GAP statistic. As a final step, K-Means clustering is performed on the subset of the data belonging to tips, using the original GAP statistic for model selection. In this manner, the exact number of tips is identified and each data point that has been characterized as belonging to a tip region is uniquely assigned to a specific tip. The same process is applied to cells belonging in branching regions in order to identify the exact number of branching events and assign branching region cells to their corresponding branching event.
</p>
        <fig id="btx325-F3" orientation="portrait" position="float">
          <label>Fig. 3.</label>
          <caption>
            <p>The original, as well as the modified versions of the GAP statistic are necessary for the identification of regions of interest in single-cell differentiation trajectories. <bold>(A)</bold> Toy data were generated by uniform sampling of data-points along three line segments. Two of the segments are held fixed and one of them is positioned at an angle <italic>θ</italic>. A new dataset is sampled for each value of <italic>θ</italic> and zero-mean Gaussian noise of standard deviation <italic>σ</italic> is added. Then, local clustering is performed on centred on three cells, each being in a distinct region: Tip, Intermediate and Branching. The <italic>S</italic> = 31 (as selected by the proposed heuristic) nearest neighbors of each center-cell define the region used for local clustering. <bold>(B)</bold> Example of toy data generated for <inline-formula id="IE65"><mml:math id="IM65"><mml:mrow><mml:mo>θ</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>30</mml:mn></mml:mrow><mml:mo>°</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE66"><mml:math id="IM66"><mml:mrow><mml:mo>σ</mml:mo><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>. The centers of the three regions are highlighted. (C) The original <italic>GAP</italic> successfully identifies the tip region cell, since <inline-formula id="IE67"><mml:math id="IM67"><mml:mrow><mml:msub><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> holds for a wide variety of angles. <bold>(D)</bold><italic>GAP</italic> cannot be used to identify branching region cells, since in many cases <inline-formula id="IE68"><mml:math id="IM68"><mml:mrow><mml:msub><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> holds in the <italic>intermediate</italic> region. As a result, a large number of intermediate region cells would be false positive branching region cells. <bold>(E)</bold><inline-formula id="IE69"><mml:math id="IM69"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> successfully identifies the branching region cell, since <inline-formula id="IE70"><mml:math id="IM70"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> holds for a wide variety of angles. <bold>(F)</bold><inline-formula id="IE71"><mml:math id="IM71"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> cannot be used to identify tip cells. In many cases there is strong overlap between the confidence intervals for <inline-formula id="IE72"><mml:math id="IM72"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE73"><mml:math id="IM73"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which in practice would lead to a large number of branching cells being falsely identified as tip cells. All error bars in the plot correspond to 95% CIs generated using 500 bootstrap datasets</p>
          </caption>
          <graphic xlink:href="btx325f3"/>
        </fig>
      </sec>
      <sec>
        <title>2.2.5 Dimension reduction precedes model selection</title>
        <p>In this section we focus on the selection of the null reference distribution. Uniform sampling of features over a box aligned with the principal components of the data is suggested in (<xref rid="btx325-B22" ref-type="bibr">Tibshirani <italic>et al.</italic>, 2001</xref>). Alternatively, uniform sampling over the range of every feature in the original dimensions of the data is suggested for simplicity. Although the K-Branches clustering method performs well in the original space, model selection does not. This follows from the ‘curse of dimensionality’ (<xref rid="btx325-B8" ref-type="bibr">Hastie <italic>et al.</italic>, 2009</xref>), since it becomes exponentially hard to estimate the null distribution in high dimensions. As a result, dimensionality reduction is a necessity if model selection is to be performed. Diffusion maps (<xref rid="btx325-B2" ref-type="bibr">Coifman <italic>et al.</italic>, 2005</xref>) are a non-linear dimensionality reduction method which are known to successfully identify differentiation trajectories (<xref rid="btx325-B6" ref-type="bibr">Haghverdi <italic>et al.</italic>, 2015</xref>), outperforming traditional dimensionality reduction methods such as principal component analysis (PCA) (<xref rid="btx325-B8" ref-type="bibr">Hastie <italic>et al.</italic>, 2009</xref>) and Locally Linear Embedding (LLE) (<xref rid="btx325-B18" ref-type="bibr">Roweis and Saul, 2000</xref>). As a result, the dataset is first processed by diffusion maps and the first few diffusion components (DCs) are selected. Then, local clustering is performed for each data point in the space of the selected DCs. Finally, the reference distribution is calculated by uniform sampling over a box aligned with the same DCs, resulting in the computation of the GAP and <inline-formula id="IE74"><mml:math id="IM74"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mtext>GAP</mml:mtext></mml:mrow><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> statistics used for model selection.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 Datasets</title>
      <p>The performance of local K-Branches is evaluated using three publicly available datasets, as well as one synthetic dataset. The first dataset corresponds to single-cell RNA-seq data describing the differentiation of myeloid progenitors during hematopoiesis (<xref rid="btx325-B16" ref-type="bibr">Paul <italic>et al.</italic>, 2015</xref>); Accession Number GSE72857) and consists of measurements of 2730 cells and 8716 genes. The second dataset consists of single-cell qPCR data related to mouse blastocyst development (<xref rid="btx325-B5" ref-type="bibr">Guo <italic>et al.</italic>, 2010</xref>); Accession Number J:140465) and includes measurements of 428 cells and 48 genes. The third dataset corresponds to a single-cell qPCR dataset of multiple time points where THP-1 human myeloid monocytic leukemia cells undergo differentiation into macrophages (<xref rid="btx325-B12" ref-type="bibr">Kouno <italic>et al.</italic>, 2013</xref>); Data available in the supplement of the original publication) and include measurements of 960 cells and 45 genes. The last dataset corresponds to an artificial dataset used as proof of concept and includes measurements of 2 synthetic genes and 244 cells that differentiate into three branches but the differentiation process includes a loop. Such a dataset could for example correspond to cellular reprogramming, or cells exiting the cell cycle, as also suggested by (<xref rid="btx325-B26" ref-type="bibr">Welch <italic>et al.</italic>, 2016</xref>).</p>
    </sec>
    <sec>
      <title>3.2 Comparison to other methods</title>
      <p>The purpose of local K-Branches is to identify branching and tip regions, while current popular methods assign cells to distinct branches. Local K-Branches is compared with DPT (<xref rid="btx325-B7" ref-type="bibr">Haghverdi <italic>et al.</italic>, 2016</xref>) which in addition to assigning cells to distinct branches, also identifies tip cells and undecided cells in branching regions. One difference between DPT and the proposed method is that DPT only identifies one cell of each branch as the tip, while the proposed method typically identifies a region of tip cells. Although TSCAN does not directly identify branching and tip regions, it does construct a minimum spanning tree that connects the cluster centers. As a result, one could consider as tip, intermediate and branching clusters those clusters that are connected to one, two and more than two clusters in the minimum spanning tree. Monocle is similar to TSCAN. However, it connects single cells instead of cell-clusters on the minimum spanning tree. Consequently, extending monocle to identify tip and branching regions in a similar manner is not straightforward or statistically motivated. As such, Monocle (<xref rid="btx325-B23" ref-type="bibr">Trapnell <italic>et al.</italic>, 2014</xref>) and SLICER (<xref rid="btx325-B26" ref-type="bibr">Welch <italic>et al.</italic>, 2016</xref>) are only <italic>indirectly</italic> compared with the proposed method, in terms of estimating correct branching in the data. The results of applying the above methods on all datasets are presented in <xref ref-type="fig" rid="btx325-F4">Figure 4</xref>. The proposed method was either performed on the first two or three DCs, depending on the morphology of the dataset. On the other hand, DPT always takes all available DCs into account. All other methods perform dimensionality reduction as part of their pre-processing and they are <italic>only visualized</italic> using diffusion maps. Additionally, the performance of local K-Branches when LLE (<xref rid="btx325-B18" ref-type="bibr">Roweis and Saul, 2000</xref>) is used for dimensionality reduction is presented in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S3</xref>. Finally, in the datasets where ground truth for the identification of tip cells is available, quantitative comparison of local K-Branches, DPT and TSCAN was performed, assessing their capability to identify tips cells in terms of precision and recall. Precision calculates the fraction of cells identified as tip-cells that actually correspond to true tip-cells. Recall calculates the fraction of true tip-cells selected by the method, over the total number of true tip-cells present in the dataset. Both scores range from zero to one, with one corresponding to a perfect score. Another quantitative comparison is performed on the basis of correct identification of the number of branching events present in the dataset. Quantitative results are summarized in <xref rid="btx325-T1" ref-type="table">Table 1</xref>.
<table-wrap id="btx325-T1" orientation="portrait" position="float"><label>Table 1.</label><caption><p>Quantitative comparison of methods</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col valign="top" align="left" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/><col valign="top" align="center" span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1">Dataset</th><th rowspan="1" colspan="1">Score</th><th rowspan="1" colspan="1">Local K-Branches</th><th rowspan="1" colspan="1">DPT</th><th rowspan="1" colspan="1">TSCAN</th></tr></thead><tbody><tr><td rowspan="3" colspan="1">Paul <italic>et al.</italic><xref ref-type="table-fn" rid="tblfn1"><sup>a</sup></xref></td><td rowspan="1" colspan="1">precision</td><td rowspan="1" colspan="1">0.77</td><td rowspan="1" colspan="1">0.67</td><td rowspan="1" colspan="1">0.61</td></tr><tr><td rowspan="1" colspan="1">recall</td><td rowspan="1" colspan="1">0.04</td><td rowspan="1" colspan="1">0.001</td><td rowspan="1" colspan="1">0.24</td></tr><tr><td rowspan="1" colspan="1">correct branching<xref ref-type="table-fn" rid="tblfn2"><sup>b</sup></xref></td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td></tr><tr><td rowspan="3" colspan="1">Guo <italic>et al.</italic></td><td rowspan="1" colspan="1">precision</td><td rowspan="1" colspan="1">0.96</td><td rowspan="1" colspan="1">1.0</td><td rowspan="1" colspan="1">0.6</td></tr><tr><td rowspan="1" colspan="1">recall</td><td rowspan="1" colspan="1">0.36</td><td rowspan="1" colspan="1">0.02</td><td rowspan="1" colspan="1">0.89</td></tr><tr><td rowspan="1" colspan="1">correct branching<xref ref-type="table-fn" rid="tblfn2"><sup>b</sup></xref></td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td></tr><tr><td rowspan="3" colspan="1">Kouno <italic>et al.</italic></td><td rowspan="1" colspan="1">precision</td><td rowspan="1" colspan="1">0.77</td><td rowspan="1" colspan="1">1.0</td><td rowspan="1" colspan="1">0.47</td></tr><tr><td rowspan="1" colspan="1">recall</td><td rowspan="1" colspan="1">0.4</td><td rowspan="1" colspan="1">0.012</td><td rowspan="1" colspan="1">0.43</td></tr><tr><td rowspan="1" colspan="1">correct branching<xref ref-type="table-fn" rid="tblfn2"><sup>b</sup></xref></td><td rowspan="1" colspan="1">Yes</td><td rowspan="1" colspan="1">No</td><td rowspan="1" colspan="1">No</td></tr></tbody></table><table-wrap-foot><fn id="tblfn1"><label>a</label><p>Quantifying tip recall is problematic since ground truth is based on thresholding of FACS markers and hence recalls too many cells.</p></fn><fn id="tblfn2"><label>b</label><p>The number of branching events was identified correctly.</p></fn></table-wrap-foot></table-wrap></p>
      <fig id="btx325-F4" orientation="portrait" position="float">
        <label>Fig. 4.</label>
        <caption>
          <p>Results local K-Branches, DPT, TSCAN, Monocle and SLICER on all datasets. <bold>(A)</bold> Single-cell RNA-Seq data of myeloid progenitors (<xref rid="btx325-B16" ref-type="bibr">Paul <italic>et al.</italic>, 2015</xref>). Common myeloid progenitor (CMP) cells branch into MEPs and GMPs. <bold>(B)</bold> Single-cell qPCR data of mouse blastocyst development (<xref rid="btx325-B5" ref-type="bibr">Guo <italic>et al.</italic>, 2010</xref>), where the initial population of 2-cell blastocyst differentiates into three different groups of 64-cell blastocysts, undergoing two distinct branching events. <bold>(C)</bold> Single-cell qPCR data of human monocytic leukemia (<xref rid="btx325-B12" ref-type="bibr">Kouno <italic>et al.</italic>, 2013</xref>). In this dataset, THP-1 human myeloid monocytic leukemia cells differentate into macrophages and no branching event is present. <bold>(D)</bold> Artificial data of arbitrary geometry where three distinct tips are connected by a loop</p>
        </caption>
        <graphic xlink:href="btx325f4"/>
      </fig>
      <sec>
        <title>3.2.1 Single-cell RNA-seq data of myeloid progenitors</title>
        <p>When applied to the first two DCs of the single-cell RNA-Seq dataset of (<xref rid="btx325-B16" ref-type="bibr">Paul <italic>et al.</italic>, 2015</xref>), the proposed method identifies three branch tips of fully differentiated cells, as well as one branching region. The regions identified by K-Branches are illustrated with respect to Fluorescence Activated Cell Sorting (FACS) labels in <xref ref-type="fig" rid="btx325-F5">Figure 5</xref>. In order to perform a quantitative comparison, true tip-cells were considered cells belonging in the granulocyte/macrophage progenitor (GMP) and megakaryocyte/erythrocyte progenitor (MEP) gates of <xref ref-type="fig" rid="btx325-F5">Figure 5</xref>. However, selecting tip cells in this manner is only approximately accurate. The results of DPT on the same data agree with the findings of local K-Branches. Two of the three tips identified by DPT are in the tip regions of local K-Branches, while the third tip of DPT is not inside but in the vicinity of the local K-Branches tip region. When comparing the branching region, the undecided cells of DPT are either inside or in close proximity to the branching region identified by local K-Branches. However, considerably fewer cells are considered as undecided by DPT. Additionally, TSCAN finds no branching and identifies two tip regions. Finally, Monocle overestimates, while SLICER underestimates the overall branching. According to the results in <xref rid="btx325-T1" ref-type="table">Table 1</xref>, local K-Branches is the most precise method, while TSCAN achieves better recall but fails to identify the branching event.
</p>
        <fig id="btx325-F5" orientation="portrait" position="float">
          <label>Fig. 5.</label>
          <caption>
            <p>Cells plotted according to FACS-measured FcgR3 and CD34 protein expression values (<xref rid="btx325-B16" ref-type="bibr">Paul <italic>et al.</italic>, 2015</xref>). The cells corresponding to regions B1, T1, T2, T3, as identified by local K-Branches, are highlighted. The MEP, granulocyte/macrophage progenitors and CMPs gates are also plotted. Pie-charts correspond to the distribution of T1, T2 and T3 cells in the MEP and GMP gates. The cells of branching region B1 are enriched only in the CMP gate (Fisher’s exact test <italic>P</italic>-value &lt; <inline-formula id="IE75"><mml:math id="IM75"><mml:mrow><mml:msup><mml:mrow><mml:mn>2.2</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). The cells of tip T1 correspond to MEP, while the cells of tip T2 correspond to GMP. The cells of tip T3 correspond to outlier groups of dendritic cells and natural killer cells (lymphocytes)</p>
          </caption>
          <graphic xlink:href="btx325f5"/>
        </fig>
      </sec>
      <sec>
        <title>3.2.2 Single-cell qPCR data of mouse blastocyst development</title>
        <p>The proposed method was applied to the first three DCs of the single-cell qPCR data which contains two distinct branching events (<xref rid="btx325-B5" ref-type="bibr">Guo <italic>et al.</italic>, 2010</xref>). Once more there is close agreement between the results of local K-Branches and DPT. Both methods identify four branch tips and the tip cells of DPT are in the tip regions of the proposed method. One key difference is that the proposed method automatically identified four branch tips and two branching regions, while DPT had to be manually executed twice on the data: First, three branches were identified, then DPT was performed on one of the branches, identifying the second branching region and new branch tips. On the other hand, TSCAN identifies two tips, one of which corresponds to a tip in the diffusion map trajectory, while it identifies no branching regions in the data. In order to perform quantitative comparisons, cells belonging to the 2- and 64-cell blastocysts were considered tip-cells. DPT is the most precise method achieving precision of 1, with local K-Branches being a close second with 0.96 precision. On the other hand, TSCAN achieves only 0.6 precision and fails to identify the branching events. However, it performs better in terms of recall, even though it does not identify any cells of the 2-cell stage tip (T1 of local K-Branches and DPT). Finally, Monocle identifes 5, while SLICER finds 3 clusters in the data.</p>
      </sec>
      <sec>
        <title>3.2.3 Single-cell qPCR data of human monocytic leukemia cells</title>
        <p>The third dataset contains measurements of 960 THP-1 human myeloid monocytic leukemia cells which undergo differentiation into macrophages and includes measurements along eight distinct timepoints (<xref rid="btx325-B12" ref-type="bibr">Kouno <italic>et al.</italic>, 2013</xref>). In order to perform a quantitative comparison, we considered the cells belonging to the first and last timepoints as the two tip populations. In terms of pre-processing, one of the genes (KLF10) was removed, since it was only strongly expressed during the second timepoint and hindered the average performance of all methods as shown in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S4</xref>. Local K-Branches was performed on the first two DCs and identified two tips and no branching event. On the other hand, DPT and TSCAN identified three tips and a branching event. As such, local K-Branches is the only method that successfully does not identify branching in the data. On the other hand, all three tip-cells of DPT lie in tip regions and it achieves the highest precision of 1, followed by local K-Branches with 0.77 and TSCAN with 0.47. Finally, TSCAN achieves the greatest recall score of 0.43, followed closely by local K-Branches with 0.4 while DPT only achieves recall of 0.012. Monocle finds 23, while TSCAN identifies 5 clusters in the data.</p>
      </sec>
      <sec>
        <title>3.2.4 Artificial data of arbitrary geometry</title>
        <p>The final dataset highlights an important advantage of the proposed methodology. Namely, the identification branch tips and branching regions in datasets of arbitrary geometry. In this case, the dataset was manually generated to consist of three branches with a loop among them and the first two DCs retain the same geometry as the original dataset. Even though it could be directly applied to the original two-dimensional data, the proposed method was performed on the first two DCs. This was done for two reasons: First, for real data of high dimensions clustering and model selection will be performed on the DCs and we assume that dimensionality reduction through diffusion maps will also retain the loop structure of real data. Second, by using the DCs there are direct comparison to the performance of DPT. Despite the challenging geometry of the dataset, the proposed method correctly identifies the three regions corresponding to the branch tips, as well as the three branching regions. On the other hand, DPT correctly identifies the three tip cells but fails in identifying the branching regions. To be precise, it identifies one branching region correctly, but then it fails to find the other two and considers one irrelevant part of the loop as a branching region. Monocle underestimates the number of branching events, probably since it always assumes that the differentiation trajectory corresponds to a tree-like structure. Finally, SLICER overestimates the overall branching in the data, while TSCAN identifies two tips mostly lying in an intermediate region. An illustration of the performance of K-Branches on the same dataset for different levels of added noise is presented in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Figure S5</xref>.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion and discussion</title>
    <p>In this study, a model based clustering approach was introduced for the identification of regions of interest in single-cell data. First, a novel clustering method called K-Branches was introduced, which clusters data points into a set of K halflines with a common center. Subsequently, this clustering method was applied locally to the neighborhood of each cell and a modified version of the GAP statistic was developed to perform model selection. The goal of model selection is to identify the <italic>local dimensionality</italic> of the data. That is, identify fully differentiated tip cells and cells belonging to branching regions. In this manner, all branching events, as well as all end-points (tips) in differentiation trajectories can be identified. As demonstrated, this local view of the data allows the method to be successfully applied to challenging datasets that include sparsity and complex geometries.</p>
    <p>The main idea of the proposed methodology is different from that of commonly used methods such as DPT, Monocle, Wishbone, SLICER or TSCAN. To be precise, these methods aim to assign each cell to a distinct branch in the differentiation process and also calculate pseudotime: an ordering of the cells, relevant to their distance from a starting root cell, which reflects how far they have progressed in the differentiation process. As such, K-Branches cannot be directly compared with most of these methods, perhaps with the exceptions of DPT and TSCAN. To be precise, DPT also identifies tip cells and branching regions of undecided cells, while TSCAN can be extended to search for tip, intermediate and branching clusters. The performance of the proposed method was compared with that of DPT and TSCAN in three single-cell datasets, as well as an artificial dataset. Local K-Branches achieved high precision and correctly identified the number (or absence) of branching events in all three single-cell datasets, while performing better than DPT in terms of recall. DPT was very precise and found the correct number of branching events in two of the three datasets, but since it only selects one cell per tip, it is poor in terms of recall. TSCAN was the least precise of all methods and did not identify the correct number of branching events in any dataset. However, it performed better than all other methods in terms of recall, in part since it selects a large number of cells. Moreover, in the dataset which consists of three branches with a loop in between, the local approach of the proposed methodology successfully identifies all tip and branching regions, while DPT only identifies the branch tips and TSCAN finds two tips in an intermediate region. Although this difference was observed on a synthetic dataset, real datasets containing loops could in theory correspond to cells exiting cell cycle, cells resulting in the same state through different differentiation trajectories, or cellular reprogramming (<xref rid="btx325-B1" ref-type="bibr">Bendall <italic>et al.</italic>, 2014</xref>). One advantage of DPT is faster execution time since the entire dataset is typically processed in a few minutes. On the other hand, local K-Branches requires a few seconds <italic>per data point</italic>. However, in the case of local K-Branches each data point can be processed completely in parallel. TSCAN is also faster than local K-Branches but was less precise in the identification of tip-cells. To be fair, it was designed to solve a different problem and uses PCA for dimensionality reduction. PCA can be sufficient when the goal is to identify distinct cell-clusters, but has limited capabilities when it comes to learning continuous manifolds of differentiation trajectories which appear to be a necessity for the accurate identification of branching and tip regions. Finally, TSCAN utilizes a model-based approach to decide on the global number of clusters. In contrast, local K-Branches utilizes model selection to identify the dimensionality of the data in a local context.</p>
    <p>In terms of future work, it would be interesting to extend the method to support explicit identification of the branches that lie between the branching and tip regions, which are currently only characterized as intermediate regions. Although clustering works in the original dimensions, model selection using the GAP statistic does not. As such, the proposed method utilizes diffusion maps for dimensionality reduction. Although LLE achieved similar results, it required tedious fine-tuning to produce satisfactory trajectories. Moreover, developing a different model selection method, other than the GAP statistic, that would allow the methodology to be directly applied in the original dimensions could be an additional topic of future work.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>Supplementary Data</label>
      <media xlink:href="btx325_supplementary_materials.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <title>Acknowledgements</title>
    <p>We would like to acknowledge L. Haghverdi for her helpful advice. We would like to thank M. Büttner for her comments and support on drawing biological conclusions. Finally, we would like to thank P. Angerer and D. S. Fischer for their comments on the R package and article, respectively.</p>
    <sec>
      <title>Funding</title>
      <p>N.K.C. is supported by a DFG Fellowship through the Graduate School of Quantitative Biosciences Munich (QBM). F.A.W. acknowledges support by the ‘Helmholtz Postdoc Programme’, Initiative and Networking Fund of the Helmholtz Association. F.J.T. acknowledges financial support by the German Science Foundation (SFB 1243 and Graduate School QBM) as well as by the Bavarian government (BioSysNet).</p>
      <p><italic>Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="btx325-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bendall</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Single-cell trajectory detection uncovers progression and regulatory coordination in human b cell development</article-title>. <source>Cell</source>, <volume>157</volume>, <fpage>714</fpage>–<lpage>725</lpage>.<pub-id pub-id-type="pmid">24766814</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Coifman</surname><given-names>R.R.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>) 
<article-title>Geometric diffusions as a tool for harmonic analysis and structure definition of data: diffusion maps</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>102</volume>, <fpage>7426</fpage>–<lpage>7431</lpage>.<pub-id pub-id-type="pmid">15899970</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>de Vargas Roditi</surname><given-names>L.</given-names></name>, <name name-style="western"><surname>Claassen</surname><given-names>M.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Computational and experimental single cell biology techniques for the definition of cell type heterogeneity, interplay and intracellular dynamics</article-title>. <source>Curr. Opin. Biotechnol</source>., <volume>34</volume>, <fpage>9</fpage>–<lpage>15</lpage>. Systems biology Nanobiotechnology.<pub-id pub-id-type="pmid">25461506</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Grün</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>van Oudenaarden</surname><given-names>A.</given-names></name></person-group> (<year>2015</year>) 
<article-title>Design and analysis of single-cell sequencing experiments</article-title>. <source>Cell</source>, <volume>163</volume>, <fpage>799</fpage>–<lpage>810</lpage>.<pub-id pub-id-type="pmid">26544934</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Resolution of cell fate decisions revealed by single-cell gene expression analysis from zygote to blastocyst</article-title>. <source>Develop. Cell</source>, <volume>18</volume>, <fpage>675</fpage>–<lpage>685</lpage>.</mixed-citation>
    </ref>
    <ref id="btx325-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Haghverdi</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Diffusion maps for high-dimensional single-cell analysis of differentiation data</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>2989</fpage>–<lpage>2998</lpage>.<pub-id pub-id-type="pmid">26002886</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Haghverdi</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Diffusion pseudotime robustly reconstructs lineage branching</article-title>. <source>Nat. Methods</source>, <volume>13</volume>, <fpage>845</fpage>–<lpage>848</lpage>.<pub-id pub-id-type="pmid">27571553</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B8">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hastie</surname><given-names>T.J.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) <source>The Elements of Statistical Learning: data Mining, Inference, and Prediction</source>. 
<publisher-name>Springer series in statistics, Springer</publisher-name>, 
<publisher-loc>New York</publisher-loc>. Autres impressions: 2011 (corr), 2013 (7e corr).</mixed-citation>
    </ref>
    <ref id="btx325-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jaitin</surname><given-names>D.A.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Massively parallel single-cell rna-seq for marker-free decomposition of tissues into cell types</article-title>. <source>Science</source>, <volume>343</volume>, <fpage>776</fpage>–<lpage>779</lpage>.<pub-id pub-id-type="pmid">24531970</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ji</surname><given-names>Z.</given-names></name>, <name name-style="western"><surname>Ji</surname><given-names>H.</given-names></name></person-group> (<year>2016</year>) 
<article-title>Tscan: pseudo-time reconstruction and evaluation in single-cell rna-seq analysis</article-title>. <source>Nucleic Acids Res</source>., <volume>44</volume>, <fpage>e117.</fpage><pub-id pub-id-type="pmid">27179027</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kiselev</surname><given-names>V.Y.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Sc3 - consensus clustering of single-cell rna-seq data</article-title>. <source>Nat. Meth.</source>, <volume>14</volume>, <fpage>483</fpage>–<lpage>486</lpage>.</mixed-citation>
    </ref>
    <ref id="btx325-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kouno</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Temporal dynamics and transcriptional control using single-cell gene expression analysis</article-title>. <source>Genome Biol</source>., <volume>14</volume>, <fpage>R118.</fpage><pub-id pub-id-type="pmid">24156252</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mahata</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Single-cell rna sequencing reveals t helper cells synthesizing steroids de novo to contribute to immune homeostasis</article-title>. <source>Cell Rep</source>., <volume>7</volume>, <fpage>1130</fpage>–<lpage>1142</lpage>.<pub-id pub-id-type="pmid">24813893</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B14">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Mohajer</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) A comparison of gap statistic definitions with and without logarithm function. <ext-link ext-link-type="uri" xlink:href="http://dbm.neuro.uni-jena.de/vbm">https://epub.ub.uni-muenchen.de/11920/</ext-link>.</mixed-citation>
    </ref>
    <ref id="btx325-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moignard</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Decoding the regulatory network of early blood development from single-cell gene expression measurements</article-title>. <source>Nat. Biotech</source>., <volume>33</volume>, <fpage>269</fpage>–<lpage>276</lpage>.</mixed-citation>
    </ref>
    <ref id="btx325-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paul</surname><given-names>F.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Transcriptional heterogeneity and lineage commitment in myeloid progenitors</article-title>. <source>Cell</source>, <volume>163</volume>, <fpage>1663</fpage>–<lpage>1677</lpage>.<pub-id pub-id-type="pmid">26627738</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Proserpio</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Single-cell analysis of cd4+ t-cell differentiation reveals three major cell states and progressive acceleration of proliferation</article-title>. <source>Genome Biol</source>., <volume>17</volume>, <fpage>1</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">26753840</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Roweis</surname><given-names>S.T.</given-names></name>, <name name-style="western"><surname>Saul</surname><given-names>L.K.</given-names></name></person-group> (<year>2000</year>) 
<article-title>Nonlinear dimensionality reduction by locally linear embedding</article-title>. <source>Science</source>, <volume>290</volume>, <fpage>2323</fpage>–<lpage>2326</lpage>.<pub-id pub-id-type="pmid">11125150</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Setty</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Wishbone identifies bifurcating developmental trajectories from single-cell data</article-title>. <source>Nat. Biotech</source>., <volume>34</volume>, <fpage>637</fpage>–<lpage>645</lpage>.</mixed-citation>
    </ref>
    <ref id="btx325-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Stegle</surname><given-names>O.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Computational and analytical challenges in single-cell transcriptomics</article-title>. <source>Nat. Rev. Genet</source>., <volume>16</volume>, <fpage>133</fpage>–<lpage>145</lpage>.<pub-id pub-id-type="pmid">25628217</pub-id></mixed-citation>
    </ref>
    <ref id="btx325-B21">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Theodoridis</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Koutroumbas</surname><given-names>K.</given-names></name></person-group> (<year>2008</year>) <source>Pattern Recognition</source>, <edition>4th edn.</edition>, 
<publisher-name>Academic Press, Burlington, MA</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btx325-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tibshirani</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2001</year>) 
<article-title>Estimating the number of clusters in a data set via the gap statistic</article-title>. <source>J. R Stat. Soc. Ser. B (Stat. Methodol.)</source>, <volume>63</volume>, <fpage>411</fpage>–<lpage>423</lpage>.</mixed-citation>
    </ref>
    <ref id="btx325-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Trapnell</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells</article-title>. <source>Nat. Biotech</source>., <volume>32</volume>, <fpage>381</fpage>–<lpage>386</lpage>. Research.</mixed-citation>
    </ref>
    <ref id="btx325-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Waddington</surname><given-names>C.H.</given-names></name></person-group> (<year>1942</year>) 
<article-title>Canalization of development and the inheritance of acquired characters</article-title>. <source>Nature</source>, <volume>150</volume>, <fpage>563</fpage>–<lpage>565</lpage>.</mixed-citation>
    </ref>
    <ref id="btx325-B25">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Waddington</surname><given-names>C.H.</given-names></name></person-group> (<year>1957</year>) <source>The Strategy of the Genes. A Discussion of Some Aspects of Theoretical Biology</source>. With an appendix by <person-group person-group-type="editor"><name name-style="western"><surname>Kacser</surname><given-names>H.</given-names></name></person-group>
<publisher-name>Allen &amp; Unwin</publisher-name>, 
<publisher-loc>London</publisher-loc>.</mixed-citation>
    </ref>
    <ref id="btx325-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Welch</surname><given-names>J.D.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Slicer: inferring branched, nonlinear cellular trajectories from single cell rna-seq data</article-title>. <source>Genome Biol</source>., <volume>17</volume>, <fpage>106.</fpage><pub-id pub-id-type="pmid">27215581</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
