<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6499410</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/bty855</article-id>
    <article-id pub-id-type="publisher-id">bty855</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Applications Notes</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Bioimage Informatics</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>CANDI: an R package and Shiny app for annotating radiographs and evaluating computer-aided diagnosis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0001-8064-9050</contrib-id>
        <name>
          <surname>Badgeley</surname>
          <given-names>Marcus A</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff1">1</xref>
        <xref ref-type="aff" rid="bty855-aff2">2</xref>
        <xref ref-type="aff" rid="bty855-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Manway</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Glicksberg</surname>
          <given-names>Benjamin S</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff4">4</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shervey</surname>
          <given-names>Mark</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff1">1</xref>
        <xref ref-type="aff" rid="bty855-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zech</surname>
          <given-names>John</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff5">5</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Shameer</surname>
          <given-names>Khader</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff6">6</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lehar</surname>
          <given-names>Joseph</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff7">7</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Oermann</surname>
          <given-names>Eric K</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff8">8</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>McConnell</surname>
          <given-names>Michael V</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff3">3</xref>
        <xref ref-type="aff" rid="bty855-aff9">9</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Snyder</surname>
          <given-names>Thomas M</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Dudley</surname>
          <given-names>Joel T</given-names>
        </name>
        <xref ref-type="aff" rid="bty855-aff1">1</xref>
        <xref ref-type="aff" rid="bty855-aff2">2</xref>
        <xref ref-type="corresp" rid="bty855-cor1"/>
        <!--<email>joel.dudley@mssm.edu</email>-->
      </contrib>
    </contrib-group>
    <aff id="bty855-aff1"><label>1</label>Department of Genetics and Genomic Sciences, Icahn School of Medicine at Mount Sinai, New York, NY, USA</aff>
    <aff id="bty855-aff2"><label>2</label>Institute for Next Generation Healthcare, Icahn School of Medicine at Mount Sinai, New York, NY, USA</aff>
    <aff id="bty855-aff3"><label>3</label>Verily Life Sciences LLC, South San Francisco, CA, USA</aff>
    <aff id="bty855-aff4"><label>4</label>Institute for Computational Health Sciences, University of California, San Francisco, CA, USA</aff>
    <aff id="bty855-aff5"><label>5</label>Department of Radiology, Icahn School of Medicine at Mount Sinai, New York, NY, USA</aff>
    <aff id="bty855-aff6"><label>6</label>Department of Medical Informatics, Northwell Health, Centre for Research Informatics and Innovation, New Hyde Park, NY, USA</aff>
    <aff id="bty855-aff7"><label>7</label>Department of Bioinformatics, Boston University, Boston, MA, USA</aff>
    <aff id="bty855-aff8"><label>8</label>Department of Neurological Surgery, Icahn School of Medicine at Mount Sinai, New York, NY, USA</aff>
    <aff id="bty855-aff9"><label>9</label>Division of Cardiovascular Medicine, Stanford School of Medicine, Stanford, CA, USA</aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Murphy</surname>
          <given-names>Robert</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="bty855-cor1">To whom correspondence should be addressed. E-mail: <email>joel.dudley@mssm.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="ppub">
      <day>01</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2018-10-10">
      <day>10</day>
      <month>10</month>
      <year>2018</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>10</day>
      <month>10</month>
      <year>2018</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>9</issue>
    <fpage>1610</fpage>
    <lpage>1612</lpage>
    <history>
      <date date-type="received">
        <day>10</day>
        <month>6</month>
        <year>2018</year>
      </date>
      <date date-type="rev-recd">
        <day>29</day>
        <month>8</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>09</day>
        <month>10</month>
        <year>2018</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2018. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2018</copyright-year>
      <license license-type="cc-by" xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="bty855.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Radiologists have used algorithms for Computer-Aided Diagnosis (CAD) for decades. These algorithms use machine learning with engineered features, and there have been mixed findings on whether they improve radiologists’ interpretations. Deep learning offers superior performance but requires more training data and has not been evaluated in joint algorithm-radiologist decision systems.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We developed the Computer-Aided Note and Diagnosis Interface (CANDI) for collaboratively annotating radiographs and evaluating how algorithms alter human interpretation. The annotation app collects classification, segmentation, and image captioning training data, and the evaluation app randomizes the availability of CAD tools to facilitate clinical trials on radiologist enhancement.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Demonstrations and source code are hosted at (<ext-link ext-link-type="uri" xlink:href="https://candi.nextgenhealthcare.org">https://candi.nextgenhealthcare.org</ext-link>), and (<ext-link ext-link-type="uri" xlink:href="https://github.com/mbadge/candi">https://github.com/mbadge/candi</ext-link>), respectively, under GPL-3 license.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary material</xref> is available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">Verily Life Sciences</named-content>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">LLC</named-content>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">Verily Academic Partnership</named-content>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">Icahn School of Medicine at Mount Sinai</named-content>
          <named-content content-type="funder-identifier">10.13039/100007277</named-content>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Institutes of Health</named-content>
          <named-content content-type="funder-identifier">10.13039/100000002</named-content>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Center for Advancing Translational Sciences</named-content>
          <named-content content-type="funder-identifier">10.13039/100006108</named-content>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">NCATS</named-content>
          <named-content content-type="funder-identifier">10.13039/100006108</named-content>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">Clinical and Translational Science Award</named-content>
        </funding-source>
        <award-id>UL1TR001433-01</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="3"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction </title>
    <p>Computer vision algorithms have demonstrated success in many fields including medical radiology. Convolutional neural networks (CNNs) are a type of deep learning (DL) model that automatically learns image features and can be applied to several image-recognition tasks. Successful models are trained on the order of 100 000 training images acquired through multi-site efforts (<xref rid="bty855-B6" ref-type="bibr">Gulshan <italic>et al.</italic>, 2016</xref>; <xref rid="bty855-B12" ref-type="bibr">Ting <italic>et al.</italic>, 2017</xref>). In medicine, data collection and crowdsourcing are complicated by privacy and specialized training requirements.</p>
    <p>Web-based medical image annotation tools have been described but kept proprietary to an institution (<xref rid="bty855-B9" ref-type="bibr">Mata <italic>et al.</italic>, 2016</xref>, <xref rid="bty855-B151" ref-type="bibr">2017</xref>) and to specific crowdsourced projects (<xref rid="bty855-B152" ref-type="bibr">Cheplygina <italic>et al.</italic>, 2017</xref>; <xref rid="bty855-B8" ref-type="bibr">Maier-Hein <italic>et al.</italic>, 2014</xref>). LabelMe is a fully featured online tool designed for everyday images, but does not support sensitive data (<xref rid="bty855-B11" ref-type="bibr">Russell <italic>et al.</italic>, 2008</xref>). In clinical practice radiologists interpret images in the context of a patient’s previous image studies and non-image medical record data. There is a lack of annotation tools that provide multimodal patient data interfaces and can be deployed for collaborative work on sensitive data.</p>
    <p>Algorithms designed for Computer-Aided Diagnosis (CAD) are frequently only evaluated in isolation, and studies evaluating human performance with and without CAD have had inconsistent results. Retrospective studies on engineered feature (not DL) CAD in clinical practice have found accuracy benefit (<xref rid="bty855-B7" ref-type="bibr">Kasai <italic>et al.</italic>, 2008</xref>), no accuracy benefit (<xref rid="bty855-B2" ref-type="bibr">Benedikt <italic>et al.</italic>, 2017</xref>), or a negative effect (<xref rid="bty855-B5" ref-type="bibr">Gilbert <italic>et al.</italic>, 2008</xref>). CAD enhancement of human interpretation has been studied in disparate experimental designs. Commercially available CAD tools have been tested in fully randomized studies (<xref rid="bty855-B5" ref-type="bibr">Gilbert <italic>et al.</italic>, 2008</xref>) and observational studies (<xref rid="bty855-B4" ref-type="bibr">Fenton <italic>et al.</italic>, 2007</xref>). Experimental algorithms have been tested in only one mode (see RCT Case Study below) (<xref rid="bty855-B7" ref-type="bibr">Kasai <italic>et al.</italic>, 2008</xref>), or over multiple sessions (double-crossover design) where one day a radiologist interprets images with CAD and several months later she interprets images without CAD (or vice versa, by randomization) (<xref rid="bty855-B2" ref-type="bibr">Benedikt <italic>et al.</italic>, 2017</xref>). RCTs are graded as stronger evidence than pseudorandomized or observational studies, but RCTs have only been done with commercially available CAD systems.</p>
    <p>This manuscript introduces two open access computer-aided note and diagnosis interface (CANDI) web applications for collaboratively addressing the annotation and evaluation barriers to translating DL. The CANDI radiograph annotation dashboard (CANDI-RAD) app provides multimodal patient and image data to obtain training and testing data, and the CANDI-CAD evaluation app facilitates randomized controlled trials (RCTs) on human enhancement with algorithms.</p>
  </sec>
  <sec>
    <title>2 Implementation</title>
    <p>CANDI is distributed as an R package with web interfaces implemented as Shiny applications and modules which generate html and javascript browser-based dashboards. CANDI’s modules handle user input and render an image or all the images from a selected case, along with patient metadata. Additional modules for annotation graphically summarize a user’s entry records, and evaluation modules support CAD utilities (e.g. searching for similar images) and queue randomization. The package includes metadata from the public OpenI chest X-ray database (<xref rid="bty855-B3" ref-type="bibr">Demner-Fushman <italic>et al.</italic>, 2016</xref>) to demonstrate multimodal dashboards [images are separately available from the CC-NC-ND licensed openI database (<ext-link ext-link-type="uri" xlink:href="https://openi.nlm.nih.gov/">https://openi.nlm.nih.gov/</ext-link>)].</p>
    <p>We use third-party packages to support data input and output. The European Bioinformatics Institute package EBImage reads and renders standard biomedical image formats from disk or URL. The googlesheets package saves user input to the cloud for de-identified annotation storage. CANDI builds on these individual packages by providing Shiny modules so users can compile an interface suited for their study context (<xref rid="bty855-B1" ref-type="bibr">Badgeley <italic>et al.</italic>, 2016</xref>).</p>
    <p>Demonstration apps and user instructions are available at candi.nextgenhealthcare.org, which is hosted by a Nginx cloud server running Ubuntu. The CAD utilities were generated with several variations of Convolutional Neural Networks (CNNs) to predict disease status and localization and similar image search (further discussed in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>). The similar search module uses CNN image embeddings to compute the Euclidean distance between a test radiograph and all designated historical radiographs.</p>
  </sec>
  <sec>
    <title>3 Case studies</title>
    <sec>
      <title>3.1 Annotation</title>
      <p>The CANDI training data generation app (candi.nextgenhealthcare.org/rad_institution) collects annotations for three supervised learning problems: (i) disease classification, (ii) image segmentation and (iii) image captioning. Each of these can be used to train a different implementation of a CNN (see <xref ref-type="fig" rid="bty855-F1">Fig. 1</xref>). To adjudicate the gold-standard disease status, radiologists should use the multimodal app (candi.nextgenhealthcare.org/rad_case) to benefit from contemporaneous images and patients’ clinical data.
</p>
      <fig id="bty855-F1" orientation="portrait" position="float">
        <label>Fig. 1.</label>
        <caption>
          <p><bold>Annotation modalities and distinct uses.</bold> (<bold>A</bold>) The CANDI radiograph annotation (RAD) and computer-aided diagnosis (CAD) applications provide human-algorithm interfaces to generate training annotations and evaluate the subsequent models. Different annotation data modalities provide training data for distinct deep learning model utilities. We use convolutional neural networks (CNNs) to generate predictions in CANDI-CAD. (<bold>B</bold>) Various input/output systems are set up that conform to the security needs of different types of users </p>
        </caption>
        <graphic xlink:href="bty855f1"/>
      </fig>
    </sec>
    <sec>
      <title>3.2 Evaluation randomized control trial</title>
      <p>We implement CANDI-CAD to measure how users interpret radiographs under different assistance modes: concurrent and second-reader. In concurrent mode, the user receives algorithm support during the entire case interpretation, whereas in second-reader mode, algorithm support is only provided after the user formulates an initial unaided impression.</p>
      <p>Rigorous evaluation of new algorithms requires CAD software to be integrated into image database systems (<xref rid="bty855-B10" ref-type="bibr">Matsumoto <italic>et al.</italic>, 2013</xref>). CANDI-CAD enables experimental algorithms to be incorporated into image interpretation dashboards with randomized availability of CAD utilities. The demonstration at (candi.nextgenhealthcare.org/cad) uses three DL utilities: (i) Image similarity search, (ii) whole image classification and (iii) image bounding-box localization (see <xref ref-type="fig" rid="bty855-F1">Fig. 1</xref>). Image queue order and CAD mode are fully randomized to facilitate a 2-arm RCT in one session.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Conclusion</title>
    <p>CANDI aims to ease the translation of CAD algorithms to medical imaging by facilitating collaborative image annotation and randomized clinical evaluation. CANDI-RAD facilitates distributed annotation with a multimodal interface for patient context, which reflects clinical practice and allows radiologists to produce gold-standard data. CANDI-CAD facilitates randomized clinical trials to rigorously evaluate CAD augmentation of radiologists’ performance. Different data input/output interfaces can be used to apply CANDI to sensitive or public medical image data.</p>
  </sec>
  <sec>
    <title>Funding </title>
    <p>This work was supported by Verily Life Sciences, LLC as part of the Verily Academic Partnership with Icahn School of Medicine at Mount Sinai and by the National Institutes of Health, National Center for Advancing Translational Sciences (NCATS), Clinical and Translational Science Award [UL1TR001433-01] to J.T.D.</p>
    <p><italic>Conflict of Interest:</italic> JTD has received consulting fees or honoraria from Janssen Pharmaceuticals, GlaxoSmithKline, AstraZeneca and Hoffman-La Roche. JTD is a scientific advisor to LAM Therapeutics and holds equity in NuMedii, Ayasdi and Ontomics. JL currently works for Merck in addition to his adjunct professor role at Boston University.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>Supplementary Data</label>
      <media xlink:href="bty855_online_supplement.docx">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="REF1">
    <title>References</title>
    <ref id="bty855-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Badgeley</surname><given-names>M.A.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>). 
<article-title>EHDViz: clinical dashboard development using open-source technologies</article-title>. <source>BMJ Open</source>, <volume>6</volume>(<issue>3</issue>).</mixed-citation>
    </ref>
    <ref id="bty855-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Benedikt</surname><given-names>R.A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Concurrent computer-aided detection improves reading time of digital breast tomosynthesis and maintains interpretation performance in a multireader multicase study</article-title>. <source>Am. J. Roentgenol</source>., <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="bty855-B152">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cheplygina</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Early experiences with crowdsourcing airway annotations in chest CT</article-title>. <source>arXiv [cs.CV]</source>. <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1706.02055">http://arxiv.org/abs/1706.02055</ext-link>.</mixed-citation>
    </ref>
    <ref id="bty855-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Demner-Fushman</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Preparing a collection of radiology examinations for distribution and retrieval</article-title>. <source>J. Am. Med. Inform. Assn</source>., <volume>23</volume>, <fpage>304</fpage>–<lpage>310</lpage>.</mixed-citation>
    </ref>
    <ref id="bty855-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fenton</surname><given-names>J.J.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Influence of computer-aided detection on performance of screening mammography</article-title>. <source>N. Engl. J. Med</source>., <volume>356</volume>, <fpage>1399</fpage>–<lpage>1409</lpage>.<pub-id pub-id-type="pmid">17409321</pub-id></mixed-citation>
    </ref>
    <ref id="bty855-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gilbert</surname><given-names>F.J.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Single reading with computer-aided detection for screening mammography</article-title>. <source>N. Engl. J. Med</source>., <volume>359</volume>, <fpage>1675</fpage>–<lpage>1684</lpage>.<pub-id pub-id-type="pmid">18832239</pub-id></mixed-citation>
    </ref>
    <ref id="bty855-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gulshan</surname><given-names>V.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</article-title>. <source>J. Am Med. Assoc</source>., <volume>316</volume>, <fpage>2402</fpage>–<lpage>2410</lpage>.</mixed-citation>
    </ref>
    <ref id="bty855-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kasai</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Usefulness of computer-aided diagnosis schemes for vertebral fractures and lung nodules on chest radiographs</article-title>. <source>Am. J. Roentgenol</source>., <volume>191</volume>, <fpage>260</fpage>–<lpage>265</lpage>.<pub-id pub-id-type="pmid">18562756</pub-id></mixed-citation>
    </ref>
    <ref id="bty855-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maier-Hein</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Crowdsourcing for reference correspondence generation in endoscopic images</article-title>. <source>Lect. Notes Comput. Sci</source>., <volume>349</volume>, <fpage>356</fpage>.</mixed-citation>
    </ref>
    <ref id="bty855-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mata</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>ProstateAnalyzer: web-based medical application for the management of prostate cancer using multiparametric MR imaging</article-title>. <source>Inform Health Soc Ca</source>, <volume>41</volume>, <fpage>1</fpage>–<lpage>306</lpage>.</mixed-citation>
    </ref>
    <ref id="bty855-B151">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mata</surname><given-names>C.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Semi-automated labelling of medical images: benefits of a collaborative work in the evaluation of prostate cancer in MRI</article-title>. <source>arXiv [physics.med-ph]</source>, <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1708.08698">http://arxiv.org/abs/1708.08698</ext-link>.</mixed-citation>
    </ref>
    <ref id="bty855-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Matsumoto</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Computer-aided detection of lung nodules on multidetector CT in concurrent-reader and second-reader modes: a comparative study</article-title>. <source>Eur. J. Radiol</source>., <volume>82</volume>, <fpage>1332</fpage>–<lpage>1337</lpage>.<pub-id pub-id-type="pmid">23480965</pub-id></mixed-citation>
    </ref>
    <ref id="bty855-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Russell</surname><given-names>B.C.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>LabelMe: a database and web-based tool for image annotation</article-title>. <source>Int. J. Comput. Vision</source>, <volume>77</volume>, <fpage>157</fpage>–<lpage>173</lpage>.</mixed-citation>
    </ref>
    <ref id="bty855-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ting</surname><given-names>D.S.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes</article-title>. <source>J. Am. Med. Assoc</source>., <volume>318</volume>, <fpage>2211</fpage>–<lpage>2223</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
