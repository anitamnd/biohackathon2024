<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">5314482</article-id>
    <article-id pub-id-type="publisher-id">1510</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-017-1510-6</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Software</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>repo: an R package for data-centered management of bioinformatic pipelines</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7782-8506</contrib-id>
        <name>
          <surname>Napolitano</surname>
          <given-names>Francesco</given-names>
        </name>
        <address>
          <email>franapoli@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1763 4683</institution-id><institution-id institution-id-type="GRID">grid.11492.3f</institution-id><institution/><institution>Telethon Institute of Genetics and Medicine (TIGEM), </institution></institution-wrap>Via Campi Flegrei 34, Pozzuoli (NA), 80078 Italy </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>16</day>
      <month>2</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>16</day>
      <month>2</month>
      <year>2017</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2017</year>
    </pub-date>
    <volume>18</volume>
    <elocation-id>112</elocation-id>
    <history>
      <date date-type="received">
        <day>18</day>
        <month>7</month>
        <year>2016</year>
      </date>
      <date date-type="accepted">
        <day>31</day>
        <month>1</month>
        <year>2017</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2017</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p>Reproducibility in Data Analysis research has long been a significant concern, particularly in the areas of Bioinformatics and Computational Biology. Towards the aim of developing reproducible and reusable processes, Data Analysis management tools can help giving structure and coherence to complex data flows. Nonetheless, improved software quality comes at the cost of additional design and planning effort, which may become impractical in rapidly changing development environments. I propose that an adjustment of focus from processes to data in the management of Bioinformatic pipelines may help improving reproducibility with minimal impact on preexisting development practices.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p>In this paper I introduce the repo
<italic>R</italic> package for bioinformatic analysis management. The tool supports a data-centered philosophy that aims at improving analysis reproducibility and reusability with minimal design overhead. The core of repo lies in its support for easy data storage, retrieval, distribution and annotation. In repo the data analysis flow is derived a posteriori from dependency annotations.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p>The repo package constitutes an unobtrusive data and flow management extension of the <italic>R</italic> statistical language. Its adoption, together with good development practices, can help improving data analysis management, sharing and reproducibility, especially in the fields of Bioinformatics and Computational Biology.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Reproducible research</kwd>
      <kwd>R language</kwd>
      <kwd>Data flows</kwd>
      <kwd>Data pipelines</kwd>
    </kwd-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2017</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>Reproducibility has been often pointed out in literature as a fundamental point in Data Analysis research. Nonetheless it has not yet received due attention in practice, particularly in the areas of Bioinformatics and Computational Biology [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. The complexity of bioinformatic data and processes and the rapidly changing environments in which they are often dealt with tend to have a negative impact on best programming practices [<xref ref-type="bibr" rid="CR4">4</xref>], which dictate careful planning, accurate design and detailed documentation.</p>
    <p>Data flow management is an important part of Data Analysis with respect to both reusability and reproducibility [<xref ref-type="bibr" rid="CR3">3</xref>]. Once a number of recurrent procedures are established, each of them can be encapsulated into a module. Different analysis pipelines can then be designed by properly interconnecting predefined modules. This approach elegantly fits a number of data analysis contexts in which standard procedures are combined together to build complex pipelines [<xref ref-type="bibr" rid="CR5">5</xref>]. In addition, although with varying degrees of flexibility [<xref ref-type="bibr" rid="CR6">6</xref>], pipeline management tools often provide the possibility of modifying existing modules or defining new ones. Besides pipeline modules, resulting data may be reused as input to other analyses, thus also requiring proper management.</p>
    <p>Many data flow management tools have been developed with diverse features and approaches, ranging from simple command line scripting tools like Bpipe [<xref ref-type="bibr" rid="CR7">7</xref>] to highly visual and interactive software like Galaxy [<xref ref-type="bibr" rid="CR8">8</xref>]. Other tools are designed to add pipeline management support to specific programming languages, like Ruffus [<xref ref-type="bibr" rid="CR9">9</xref>] and Pyleaf [<xref ref-type="bibr" rid="CR4">4</xref>] for Python. See [<xref ref-type="bibr" rid="CR6">6</xref>] for a recent review encompassing the whole spectrum of pipeline management tools.</p>
    <p>The support for formalization of an analysis pipeline design is of course a precious resource in order to foster reproducibility in Bioinformatics and Computational Biology. However, it is not always rigorously applicable in practice. When developing innovative methods for a specific application, new knowledge gathered from partial results may induce a feedback loop between data and processes, with the latter being modified as a consequence of the former. In Software Engineering, similar concepts are formalized in the context of prototype-based development [<xref ref-type="bibr" rid="CR10">10</xref>]. I and colleagues previously pointed out that an incremental development approach cycling between results and processes is often implicitly or explicitly adopted in bioinformatic research [<xref ref-type="bibr" rid="CR4">4</xref>]. In such cases, the use of process-focused management tools may introduce unjustified overhead.</p>
    <p>Figure <xref rid="Fig1" ref-type="fig">1</xref> shows an ideal comparison between pure Process-Centered and Data-Centered pipeline development Approaches (PCA and DCA respectively) as defined in this article. PCA focuses on the selection or adaptation of well defined, existing processes for each stage of the pipeline. DCA relies on results obtained through prototypical methods in order to refine the processes themselves. While PCA is desirable, DCA is necessary when well established processes are not available. Of course pipeline development may proceed through hybrid PC/DC approaches in practice.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Process-centered and Data-centered approaches <bold>a</bold> In a process-centered approach (PCA) to the development of an analysis pipeline a well established process <italic>P</italic>
<sub>1</sub> is selected to build a new resource from input data. In the data-centered approach (DCA), the resource <italic>R</italic>
<sub>1</sub> has been created by prototypical code which needs to be properly structured and polished. <bold>b</bold> In the PCA the resource <italic>R</italic>
<sub>1</sub> has been produced and a well established procedure <italic>P</italic>
<sub>2</sub> has been selected to further process it. In the DCA the process <italic>P</italic>
<sub>1</sub> is now properly structured and the resource <italic>R</italic>
<sub>2</sub> has been created using prototypical code. <bold>c</bold> Both approaches finally yield equivalent pipelines and annotations</p></caption><graphic xlink:href="12859_2017_1510_Fig1_HTML" id="MO1"/></fig>
</p>
    <p>In this paper a DCA is embraced. Under this paradigm, the analysis pipeline is not seen as a well defined chain of processes to run data through, but rather as an <italic>a posteriori</italic> reconstruction of how data was processed. A pipeline is thus mainly conceived as a documentation tool meant to improve manageability and reproducibility of results. Its level of detail and completeness is the developer’s choice, ranging from a flat description of resources to a fully structured data flow. DCA, however, primarily focuses on proper storage, retrieval, annotation and distribution of data produced by each stage of the pipeline.</p>
    <p>To the best of my knowledge, the <italic>R</italic> language currently misses extensions supporting pipeline management (either data- or flow-centered). The language does feature a range of reproducibility tools, although not dealing with pipeline management. Support for Literate Programming [<xref ref-type="bibr" rid="CR11">11</xref>] is provided by packages such as the popular Sweave [<xref ref-type="bibr" rid="CR12">12</xref>], allowing to mix together documentation and code in order to produce self-documenting processes. However, data itself is not part of the output. The <italic>R</italic> package rctrack [<xref ref-type="bibr" rid="CR13">13</xref>] was developed to fill this gap. The tool can automatically track files accessed by <italic>R</italic> processes and archive them for reproducibility. This approach is certainly valuable, although it focuses on making a process reproducible, without explicit support for structuring it into a pipeline or managing the produced resources for reusability.</p>
    <p>In the following I introduce the repo extension of the <italic>R</italic> statistical language. repo implements the previously described data-centered approach to pipeline management. It is publicly available from the CRAN repository [<xref ref-type="bibr" rid="CR14">14</xref>], while more up-to-date versions are maintained at GitHub [<xref ref-type="bibr" rid="CR15">15</xref>]. The next Section introduces the general design of the tool. A more detailed description through usage examples is presented in the “<xref rid="Sec3" ref-type="sec">Results</xref>” Section.</p>
  </sec>
  <sec id="Sec2">
    <title>Implementation</title>
    <p>The <italic>R</italic> package repo has been developed with the aim of supporting a data-centered pipeline management philosophy. The tool mainly focuses on storage, retrieval and rich annotation of data. The definition of the data flow itself is part of the data annotation. The design of repo assumes centrality of data and high variability of processes.</p>
    <p>In order to foster reproducibility, repo implements a data repository layer which takes care of managing permanent storage of both data and annotations. Basic mandatory annotations for each stored item include a name, a textual description, and a set of tags. Additional annotations include inter-item relations and generic external attachments like rich-text documents or images.</p>
    <p>The repo interface replaces the standard save and load
<italic>R</italic> functions for permanent storage and retrieval. The user passes objects and corresponding annotations to repo, which transparently stores them to the file system. All items and annotations for the same repository are stored within a single directory. The inclusion of data descriptors and tags overcomes the need for directory structure since repository items are retrieved basing on annotation, as opposed to location. In particular, tags are used as a generalization of the directory tree model, as they identify possibly overlapping sets of items.</p>
    <p>Repositories in repo are self-contained by design, so that an entire repository can be easily shared. Moreover, inside the repository directory all metadata are contained within a single file, i.e. the <italic>index</italic>. In fact, the index file alone can be conveniently shared. It allows to browse through all items and annotations of a repository taking advantage of all repo features not dealing with actual data, such as data analysis flow visualizations. Support for remote download can be exploited to selectively obtain data of interest.</p>
    <p>In repo the data pipeline is actually reverse-engineered from relational annotations. For example, the user may store source code file as a repository item and annotate other items as being generated by it. Special comments in the source code can be added to associate a specific code section with the production of a resource. Dependency between items can also be annotated. The tool is aware of the data flow implicitly defined by annotations and supports batch actions on interrelated items. In repo the data flow definition is thus optional as any other annotation.</p>
    <p>
repo is an <italic>R</italic> language extension developed using the Reference Class paradigm [<xref ref-type="bibr" rid="CR16">16</xref>]. In the <italic>R</italic> environment the user creates an object of class repo associated with a file system directory and controls the corresponding repository through the object methods (see Table <xref rid="Tab1" ref-type="table">1</xref> for a summary of the available methods). In the next Section a more detailed view of the tool is provided through direct examples.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>A summary of commands available in the latest development version of the repo package</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Command</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">
attach
</td><td align="justify">Store a generic file into the repository.</td></tr><tr><td align="left">
attr
</td><td align="justify">Retrieves item attributes.</td></tr><tr><td align="left">
build
</td><td align="justify">Runs code chunk associated with an item and dependant items if needed.</td></tr><tr><td align="left">
bulkedit
</td><td align="justify">Saves repository meta data to a text file for offline editing or loads the file after editing.</td></tr><tr><td align="left">
check
</td><td align="justify">Checks MD5-consistency of stored items.</td></tr><tr><td align="left">
chunk
</td><td align="justify">Displays the code chunk associated with an item.</td></tr><tr><td align="left">
copy
</td><td align="justify">Copies items between repositories.</td></tr><tr><td align="left">
cpanel
</td><td align="justify">Runs visual interface.</td></tr><tr><td align="left">
dependencies
</td><td align="justify">Returns and/or plots item dependencies.</td></tr><tr><td align="left">
export
</td><td align="justify">Saves the contents of a repository item to a file in RDS format.</td></tr><tr><td align="left">
find
</td><td align="justify">Searches all metadata for a partial string match.</td></tr><tr><td align="left">
get
</td><td align="justify">Loads an item into the current workspace.</td></tr><tr><td align="left">
handlers
</td><td align="justify">Returns a list of functions to be used as an alternative interface to the repository.</td></tr><tr><td align="left">
has
</td><td align="justify">Checks wether an item is present in the repository.</td></tr><tr><td align="left">
info
</td><td align="justify">Displays a summary of information about a regular item, a project item, or the repository.</td></tr><tr><td align="left">
lazydo
</td><td align="justify">Evaluates specified code caching results in the repository. Loads results if already cached.</td></tr><tr><td align="left">
options
</td><td align="justify">Sets default parameters to be used by subsequent calls to the put command.</td></tr><tr><td align="left">
pies
</td><td align="justify">Shows statistics about disk space used by each item in the repository.</td></tr><tr><td align="left">
print
</td><td align="justify">Summarizes information about items.</td></tr><tr><td align="left">
project
</td><td align="justify">Creates a special “project” item.</td></tr><tr><td align="left">
pull
</td><td align="justify">Overwrites item contents by downloading data from the associated URL.</td></tr><tr><td align="left">
put
</td><td align="justify">Stores new data into the repository.</td></tr><tr><td align="left">
related
</td><td align="justify">Lists items related to a given item according to dependencies.</td></tr><tr><td align="left">
rm
</td><td align="justify">Removes items from the repository.</td></tr><tr><td align="left">
root
</td><td align="justify">Returns repository root position on the file system.</td></tr><tr><td align="left">
set
</td><td align="justify">Updates an existing item.</td></tr><tr><td align="left">
stash
</td><td align="justify">Stores an item with unspecified meta information.</td></tr><tr><td align="left">
stashclear
</td><td align="justify">Removes stash-ed items.</td></tr><tr><td align="left">
sys
</td><td align="justify">Runs a system command on a given item.</td></tr><tr><td align="left">
tag
</td><td align="justify">Set tags for an item.</td></tr><tr><td align="left">
tags
</td><td align="justify">Retrieves tags for an item.</td></tr><tr><td align="left">
untag
</td><td align="justify">Removes specified tags from an item.</td></tr></tbody></table></table-wrap>
</p>
  </sec>
  <sec id="Sec3" sec-type="results">
    <title>Results</title>
    <p>This Section illustrates the main features of the repo package and its philosophy through an application example. The example involves the creation and population of a repository, its exploration, manipulation and distribution.</p>
    <sec id="Sec4">
      <title>Repository creation and population</title>
      <p>In repo all the data and annotations for a single repository completely reside under a specified file system position. One repository can store resources produced by different analyses. The choice between the creation of a single central repository or multiple project-specific repositories is up to the user. The following code creates a new, empty repository in a temporary directory:</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figa_HTML.gif" id="MO2"/>
      </p>
      <p>The example code reported in this Section is contained in a file named article.Rnw. The next code block stores the source code as a repository item. The attach function stores generic files (as opposed to <italic>R</italic> objects) in the repository. An item description and a list of tags are also specified. The project command creates a special repository item containing pipeline-wise information. The options commands sets the default source file and the default project to associate items with.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figb_HTML.gif" id="MO3"/>
      </p>
      <p>This example uses the “Mice Protein Expression Data Set” from the UCI repository [<xref ref-type="bibr" rid="CR17">17</xref>]. In the following block the data is downloaded and a copy is stored in the repository, specifying the download URL. The URL field is useful to trace the provenance of the data, but can also be used to download the item contents through the pull function. The variable xls.name which contains the name of the downloaded file, is also used to set the identifier of the newly created object in the repository.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figc_HTML.gif" id="MO4"/>
      </p>
      <p>The stored data is not in <italic>R</italic> format. The following code imports it into the variable data and permanently stores the variable in the repository through the put function. In this case two relations are annotated for the newly created item: the generating source code, set as the file article.Rnw; and a dependency from the downloaded file (xls.name variable). Note that Mice Cortex is annotated as being dependent on the appropriate repository item, which contains both necessary and sufficient data to build the newly created resource. However, the actual code loads the data from the downloaded file and uses a variable defined elsewhere (xls.name). These inconsistencies with the process will be fixed later in accordance to the data-centered paradigm (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>).</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figd_HTML.gif" id="MO5"/>
      </p>
      <p>The dataset includes missing values and non-real variables. As a preprocessing step, all incomplete samples are removed and a reduced version of the dataset is stored. Dependence of the reduced set from the full set (just stored as Mice Cortex) is also annotated.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Fige_HTML.gif" id="MO6"/>
      </p>
      <p>Suppose that a change is decided about the data preprocessing step. One may want to overwrite the current Mice Cortex notNA item, but keeping the previous one as a possible alternative. repo implements a simple versioning system to accomplish this task. The following code creates a scaled version of the dataset and overwrites the previously created Mice Cortex notNA item. However, since the parameter replace is set to addversion, the old item is preserved with a new name, as shown by the print output.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figf_HTML.gif" id="MO7"/>
      </p>
      <p>The attach function can be exploited to store visualizations in the repository and link them to the data they represent. The following code plots a 2-dimensional visualization of the Mice Cortex data to a PDF file and attach-es it to the item containing the corresponding data (using the to parameter).</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figg_HTML.gif" id="MO8"/>
      </p>
      <p>The accuracy of the 2D plot is bound to the amount of variance explained by the first two Principal Components of the reduced dataset. The following code creates a plot of the variance explained by each Principal Component and attaches it to the previous plot.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figh_HTML.gif" id="MO9"/>
      </p>
    </sec>
    <sec id="Sec5">
      <title>Repository exploration</title>
      <p>
repo supports a few commands to visualize information about a repository or a set of items. Global information can be visualized through the info command as follows.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figi_HTML.gif" id="MO10"/>
      </p>
      <p>It is also possible to visualize the composition of the repository in terms of memory usage through the pies function (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>).
<fig id="Fig2"><label>Fig. 2</label><caption><p>Example of repository statistics <italic>Pie</italic> chart visualization of the repository items according to their memory usage on the disk, as produced by the pies function</p></caption><graphic xlink:href="12859_2017_1510_Fig2_HTML" id="MO11"/></fig>
</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figj_HTML.gif" id="MO12"/>
      </p>
      <p>Other details about single items can be visualized using the print function. Some items (like attachments) are <italic>hidden</italic> by default. The code below lists all the items in the repository, including hidden ones.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figk_HTML.gif" id="MO13"/>
      </p>
      <p>Three types of relations between items are supported in repo: <italic>attached to</italic>, <italic>depends on</italic>, <italic>generated by</italic>. Such relations can be represented through a directed graph. The dependencies function creates the corresponding visualization (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>). When items are properly annotated, such visualization defines the analysis <italic>data flow</italic>.
<fig id="Fig3"><label>Fig. 3</label><caption><p>The dependency graph summarizing relations between items in the repository. Three types of relations are supported by repo: <italic>attached to</italic>, <italic>depends on</italic>, <italic>generated by</italic>. When items are properly annotated, this visualization also represents the analysis data flow</p></caption><graphic xlink:href="12859_2017_1510_Fig3_HTML" id="MO14"/></fig>
</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figl_HTML.gif" id="MO15"/>
      </p>
      <p>As a repository grows, it may contain a large number of items from multiple projects. In order to properly identify item subgroups, <italic>tags</italic> can be exploited as filters. Tags are supported by many repo functions and can be combined using different logic operators. In the next code block the plot items (associated with the tag “visualization”) are excluded from the dependency graph (see Fig. <xref rid="Fig4" ref-type="fig">4</xref>).
<fig id="Fig4"><label>Fig. 4</label><caption><p>Selective plot of dependencies within the repository. In this case all the items annotated with the tag “visualization” are excluded</p></caption><graphic xlink:href="12859_2017_1510_Fig4_HTML" id="MO16"/></fig>
</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figm_HTML.gif" id="MO17"/>
      </p>
      <p>The repo package also includes a preliminary visual interface (see Fig. <xref rid="Fig5" ref-type="fig">5</xref>). The current version allows to browse repository items and load them into the current workspace.
<fig id="Fig5"><label>Fig. 5</label><caption><p>The repository control panel. It is constituted by a Shiny [<xref ref-type="bibr" rid="CR20">20</xref>] application running in an Internet browser. The user can browse through repository items and load them into the current workspace</p></caption><graphic xlink:href="12859_2017_1510_Fig5_HTML" id="MO18"/></fig>
</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Fign_HTML.gif" id="MO19"/>
      </p>
    </sec>
    <sec id="Sec6">
      <title>Items access</title>
      <p>The most used command in repo is get. get loads an item from the permanent storage basing on its name.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figo_HTML.gif" id="MO20"/>
      </p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figp_HTML.gif" id="MO21"/>
      </p>
      <p>On the other hand, all the details stored for a single item are reported by the info function. The summary also reports the dimensions of the data, its creation date, the storage space used, the relative file system path to the file containing the data, and an <italic>MD5 checksum</italic>.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figq_HTML.gif" id="MO22"/>
      </p>
      <p>If the exact identifier is unknown the find function can be used to perform a string matching against all item details.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figr_HTML.gif" id="MO23"/>
      </p>
    </sec>
    <sec id="Sec7">
      <title>Analysis reproducibility</title>
      <p>While repo focuses on data, it also supports features directly dealing with processes. Such features make the tool able to reproduce resources basing on the code they were annotated to. Reproducibility is also supported by the special project items, which collect information about an entire analysis, including the list of resources involved, R version used and necessary libraries. The info command implements a special behaviour for project items, as shown in the following:</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figs_HTML.gif" id="MO24"/>
      </p>
      <p>Items in the example repository have dependencies set, thus enabling to trace back which data were used to build each resource. This may provide significant help in reproducing an analysis or reuse produced items in other analyses. However, the exact process building each resource is not described, as a generic source file is associated with all of them. Following the data-centered approach (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>), once the analysis is well assessed, source code can be cleaned up and single processes assigned to each item. Although the code used for this example is rather simple, the following is a refinement of the block related to the Mice Cortex resource:</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figt_HTML.gif" id="MO25"/>
      </p>
      <p>Note that the xls.name variable is not used anymore, and the downloaded data set is loaded from within the repository. This code is now both necessary <italic>end</italic> sufficient to build the Mice Cortex resource if its dependencies are satisfied. The comments starting with <italic>“## chunk”</italic> will be used by repo to associate the Mice Cortex resource with the actual instructions that are necessary to build it. The following lines update the source code of the project by resetting its content and show the newly defined code chunk:</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figu_HTML.gif" id="MO26"/>
      </p>
      <p>The build command runs the code associated with a resource. By default, if the resource has dependencies not already present in the repository, their associated code is run first, recursively. Otherwise their code chunks are skipped. It is also possible to set a session-wise option to determine other behaviours. For example, the following code can be used to download the latest version of the file “Data_Cortex_Nuclear.xls” and build the corresponding Mice Cortex object, without overwriting the respective previous versions. Annotation of the Data_Cortex_Nuclear.xls code chunk, as shown above for the Mice Cortex chunk, is assumed.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figv_HTML.gif" id="MO27"/>
      </p>
      <p>As previously explained, when new versions of existing items are created, the latter are renamed by adding an incremental version number. Note that, thanks to the mechanism of code chunk annotation, repo supports reentrancy [<xref ref-type="bibr" rid="CR6">6</xref>] at each properly defined pipeline stage.</p>
    </sec>
    <sec id="Sec8">
      <title>Data exchange</title>
      <p>The repo system stores data and metadata into subfolders of the repository root in the <italic>R</italic> standard <italic>RDS</italic> format. Internally, all references to stored files are relative to the root directory, implying that each repository is completely self-contained and can be easily cloned or moved. Dedicated support for data exchange is described in this Subsection.</p>
      <p>The tool can handle multiple repositories and copy items from one repository to another. For example, the code below creates a new repository and copies two items to it:</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figw_HTML.gif" id="MO28"/>
      </p>
      <p>The related function returns the names of all items that are directly or indirectly linked to a given item, thus allowing to select an independent set of items. In the following such a set is saved to the standard <italic>R</italic> data format <italic>RDS</italic> (or their original format for <italic>attachments</italic>) using the export function.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figx_HTML.gif" id="MO29"/>
      </p>
      <p>An interesting application of the URL annotation regards the distribution of repositories. The buildURL parameter of the set function can be used to assign a base URL to all items. The code below copies the previously selected set of items to the repository rp2 and sets a base URL for all items (except Data_Cortex_Nuclear.xls).</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figy_HTML.gif" id="MO30"/>
      </p>
      <p>Once the repository directory is copied to a public website, its index (i.e. the file R_repo.RDS in the repository root) can be distributed. Users can then selectively download items of interest using the pull
repo function. The check command can be used to run an integrity check on all repository items.</p>
      <p>
        <graphic xlink:href="12859_2017_1510_Figz_HTML.gif" id="MO31"/>
      </p>
    </sec>
  </sec>
  <sec id="Sec9" sec-type="discussion">
    <title>Discussion</title>
    <p>The “<xref rid="Sec3" ref-type="sec">Results</xref>” Section shows how the repo package can be used in the usual context of <italic>R</italic> development by replacing the common actions of storing and retrieving processed data with feature-reach calls to a data abstraction layer. A summary of the described commands together with other currently supported commands is reported in Table <xref rid="Tab1" ref-type="table">1</xref>. Dependency annotations are used by the tool to reconstruct the data flow, and exploit such implicit structure both for data management and documentation purposes. The tool does not require any particular structuring of the code into modules or any coding conventions in general, allowing the developer to use his preferred programming paradigm and framework. However, resources can be easily associated with any portion of consecutive lines of code in order to define the exact process associated with a pipeline stage. repo features for data management and annotations are now well established and included in the stable version available on CRAN [<xref ref-type="bibr" rid="CR14">14</xref>]. The complementary process management features, such as the chunk and build commands, instead, are included in the latest version of the package [<xref ref-type="bibr" rid="CR15">15</xref>] and constitute the current development focus of repo. Proper storage of resources and processes can greatly help in making data pipelines manageable and reproducible, within the same lab or across different labs. However, repo currently misses support for standard data exchange formats, which limits reproducibility of data flows across platforms [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>], posing a stimulating priority for further development of the tool. Finally, the support of most repo features through its visual interface will improve its overall usability, particularly for inexperienced users.</p>
  </sec>
  <sec id="Sec10" sec-type="conclusion">
    <title>Conclusions</title>
    <p>Data Analysis management tools can greatly help in making computational research manageable and reproducible. However, in rapidly changing development environments the implied overhead may constitute a significant obstacle. I developed the repo
<italic>R</italic> package for data-centered pipeline management with the aim of supporting reproducible analysis while keeping design and documentation overhead at a minimum. This is achieved by supporting the management of data and metadata storage and retrieval within the <italic>R</italic> environment. Future developments of repo include the support for data exchange formats and coverage of most features through the visual interface. The tool is publicly available from the CRAN repository [<xref ref-type="bibr" rid="CR14">14</xref>]. More up-to-date versions are maintained on the GitHub web site [<xref ref-type="bibr" rid="CR15">15</xref>].</p>
  </sec>
  <sec id="Sec11">
    <title>Availability and requirements</title>
    <p><bold>Project name</bold>: repo</p>
    <p><bold>Project home page</bold>: <ext-link ext-link-type="uri" xlink:href="https://github.com/franapoli/repo">https://github.com/franapoli/repo</ext-link></p>
    <p><bold>Archived version</bold>: 10.5281/zenodo.159584</p>
    <p><bold>Operating system(s)</bold>: Platform independent</p>
    <p><bold>Programming language</bold>: R</p>
    <p><bold>Other requirements</bold>: R environment including digest and tools packages. Tested on R version 3.2.3.</p>
    <p><bold>License</bold>: GNU GPL</p>
    <p><bold>Any restrictions to use by non-academics</bold>: no restrictions</p>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>CRAN</term>
        <def>
          <p>Comprehensive R archive network</p>
        </def>
      </def-item>
      <def-item>
        <term>DCA</term>
        <def>
          <p>Data centered pipeline development approach</p>
        </def>
      </def-item>
      <def-item>
        <term>PCA</term>
        <def>
          <p>Process centered pipeline development approach</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <ack>
    <title>Acknowledgements</title>
    <p>The author would like to thank the Telethon Foundation.</p>
    <sec id="d29e1499">
      <title>Funding</title>
      <p>No funding was received for this study.</p>
    </sec>
    <sec id="d29e1504">
      <title>Author’s contributions</title>
      <p>FN conceived the idea of the study, implemented the repo tool and wrote the manuscript. The author read and approved the final manuscript.</p>
    </sec>
    <sec id="d29e1512">
      <title>Competing interests</title>
      <p>The author declares that he has no competing interests.</p>
    </sec>
    <sec id="d29e1517">
      <title>Consent for publication</title>
      <p>Not applicable.</p>
    </sec>
    <sec id="d29e1522">
      <title>Ethics approval and consent to participate</title>
      <p>Not applicable.</p>
    </sec>
  </ack>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ince</surname>
            <given-names>DC</given-names>
          </name>
          <name>
            <surname>Hatton</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Graham-Cumming</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>The case for open computer programs</article-title>
        <source>Nature</source>
        <year>2012</year>
        <volume>482</volume>
        <issue>7386</issue>
        <fpage>485</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1038/nature10836</pub-id>
        <?supplied-pmid 22358837?>
        <pub-id pub-id-type="pmid">22358837</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>RD</given-names>
          </name>
        </person-group>
        <article-title>Reproducible research in computational science</article-title>
        <source>Science</source>
        <year>2011</year>
        <volume>334</volume>
        <issue>6060</issue>
        <fpage>1226</fpage>
        <lpage>7</lpage>
        <pub-id pub-id-type="doi">10.1126/science.1213847</pub-id>
        <?supplied-pmid 22144613?>
        <pub-id pub-id-type="pmid">22144613</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
        </person-group>
        <article-title>Ten simple rules for reducing overoptimistic reporting in methodological computational research</article-title>
        <source>PLoS Comput Biol</source>
        <year>2015</year>
        <volume>11</volume>
        <issue>4</issue>
        <fpage>1004191</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004191</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Napolitano</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Mariani-Costantini</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Tagliaferri</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Bioinformatic pipelines in Python with Leaf</article-title>
        <source>BMC Bioinforma</source>
        <year>2013</year>
        <volume>14</volume>
        <issue>1</issue>
        <fpage>201</fpage>
        <pub-id pub-id-type="doi">10.1186/1471-2105-14-201</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Reich</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Liefeld</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Gould</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Lerner</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Tamayo</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Mesirov</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>Genepattern 2.0</article-title>
        <source>Nat Genet</source>
        <year>2006</year>
        <volume>38</volume>
        <issue>5</issue>
        <fpage>500</fpage>
        <lpage>1</lpage>
        <pub-id pub-id-type="doi">10.1038/ng0506-500</pub-id>
        <?supplied-pmid 16642009?>
        <pub-id pub-id-type="pmid">16642009</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <mixed-citation publication-type="other">Leipzig J. A review of bioinformatic pipeline frameworks. Brief Bioinform. 2016. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1093/bib/bbw020">10.1093/bib/bbw020</ext-link>, <ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/bib/articlelookup/doi/10.1093/bib/bbw020">https://academic.oup.com/bib/articlelookup/doi/10.1093/bib/bbw020</ext-link>. Accessed 25 Jan 2017.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sadedin</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Pope</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Oshlack</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Bpipe: a tool for running and managing bioinformatics pipelines</article-title>
        <source>Bioinformatics</source>
        <year>2012</year>
        <volume>28</volume>
        <issue>11</issue>
        <fpage>1525</fpage>
        <lpage>6</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/bts167</pub-id>
        <?supplied-pmid 22500002?>
        <pub-id pub-id-type="pmid">22500002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goecks</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Nekrutenko</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Taylor</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences</article-title>
        <source>Genome Biol</source>
        <year>2010</year>
        <volume>11</volume>
        <issue>8</issue>
        <fpage>86</fpage>
        <pub-id pub-id-type="doi">10.1186/gb-2010-11-8-r86</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goodstadt</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Ruffus: a lightweight python library for computational pipelines</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <issue>21</issue>
        <fpage>2778</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btq524</pub-id>
        <?supplied-pmid 20847218?>
        <pub-id pub-id-type="pmid">20847218</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <mixed-citation publication-type="other">Bruegge B, Dutoit AH. Object-Oriented Software Engineering: Using UML, Patterns and Java, Second Edition. Upper Saddle River: Prentice-Hall, Inc.</mixed-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Knuth</surname>
            <given-names>DE</given-names>
          </name>
        </person-group>
        <article-title>Literate programming</article-title>
        <source>Comput J</source>
        <year>1984</year>
        <volume>27</volume>
        <issue>2</issue>
        <fpage>97</fpage>
        <lpage>111</lpage>
        <pub-id pub-id-type="doi">10.1093/comjnl/27.2.97</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Leisch</surname>
            <given-names>F</given-names>
          </name>
        </person-group>
        <article-title>Sweave: Dynamic generation of statistical reports using literate data analysis</article-title>
        <source>Compstat</source>
        <year>2002</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer-Verlag</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Pounds</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>An r package that automatically collects and archives details for reproducible computing</article-title>
        <source>BMC Bioinforma</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1093/bib/bbs075</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <mixed-citation publication-type="other">Napolitano F. repo: A Data-Centered Data Flow Manager. 2016. <italic>R</italic> package version 2.0.2. <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=repo">http://CRAN.R-project.org/package=repo</ext-link>. Accessed 25 Jan 2017.</mixed-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <mixed-citation publication-type="other">Napolitano F. repo: A Data-Centered Data Flow Manager. 2016. <italic>R</italic> package version 2.0.4.4. <ext-link ext-link-type="uri" xlink:href="https://github.com/franapoli/repo">https://github.com/franapoli/repo</ext-link>. Accessed 25 Jan 2017.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <mixed-citation publication-type="other">Wickham H. Advanced, R, 1st ed. Boca Raton: Chapman and Hall/CRC.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <mixed-citation publication-type="other">Lichman M. UCI Machine Learning Repository. 2013. <ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</ext-link>. Accessed 25 Jan 2017.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Waltemath</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Wolkenhauer</surname>
            <given-names>O</given-names>
          </name>
        </person-group>
        <article-title>How modeling standards, software, and initiatives support reproducibility in systems biology and systems medicine</article-title>
        <source>IEEE Trans Biomed Eng</source>
        <year>2016</year>
        <volume>63</volume>
        <issue>10</issue>
        <fpage>1999</fpage>
        <lpage>2006</lpage>
        <pub-id pub-id-type="doi">10.1109/TBME.2016.2555481</pub-id>
        <?supplied-pmid 27295645?>
        <pub-id pub-id-type="pmid">27295645</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <mixed-citation publication-type="other">González-Beltrán A, Li P, Zhao J, Avila-Garcia MS, Roos M, Thompson M, Horst Evd, Kaliyaperumal R, Luo R, Lee TL, Lam T-w, Edmunds SC, Sansone SA, Rocca-Serra P. From peer-reviewed to peer-reproduced in scholarly publishing: The complementary roles of data models and workflows in bioinformatics; 10(7):0127612. doi:<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.1371/journal.pone.0127612">10.1371/journal.pone.0127612</ext-link>. Accessed 05 Oct 2016</mixed-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Chang W, et al. shiny: Web Application Framework for R. 2016. <italic>R</italic> package version 0.13.2. <ext-link ext-link-type="uri" xlink:href="http://CRAN.R-project.org/package=shiny">http://CRAN.R-project.org/package=shiny</ext-link>. Accessed 25 Jan 2017.</mixed-citation>
    </ref>
  </ref-list>
</back>
