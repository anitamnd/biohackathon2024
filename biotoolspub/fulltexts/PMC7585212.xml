<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7585212</article-id>
    <article-id pub-id-type="publisher-id">3641</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-020-03641-z</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>A rank-based marker selection method for high throughput scRNA-seq data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5930-8841</contrib-id>
        <name>
          <surname>Vargo</surname>
          <given-names>Alexander H. S.</given-names>
        </name>
        <address>
          <email>ahsvargo@umich.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Gilbert</surname>
          <given-names>Anna C.</given-names>
        </name>
        <address>
          <email>anna.gilbert@yale.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.214458.e</institution-id><institution-id institution-id-type="ISNI">0000000086837370</institution-id><institution>Department of Mathematics, </institution><institution>University of Michigan, </institution></institution-wrap>530 Church Street, Ann Arbor, 48109 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.47100.32</institution-id><institution-id institution-id-type="ISNI">0000000419368710</institution-id><institution>Department of Mathematics, Yale University, </institution></institution-wrap>10 Hillhouse Ave, New Haven, 06511 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>10</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>477</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>10</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>2</day>
        <month>7</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">High throughput microfluidic protocols in single cell RNA sequencing (scRNA-seq) collect mRNA counts from up to one million individual cells in a single experiment; this enables high resolution studies of rare cell types and cell development pathways. Determining small sets of genetic markers that can identify specific cell populations is thus one of the major objectives of computational analysis of mRNA counts data. Many tools have been developed for marker selection on single cell data; most of them, however, are based on complex statistical models and handle the multi-class case in an ad-hoc manner.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">We introduce <sc>RankCorr</sc>, a fast method with strong mathematical underpinnings that performs multi-class marker selection in an informed manner. <sc>RankCorr</sc> proceeds by ranking the mRNA counts data before linearly separating the ranked data using a small number of genes. The step of ranking is intuitively natural for scRNA-seq data and provides a non-parametric method for analyzing count data. In addition, we present several performance measures for evaluating the quality of a set of markers when there is no known ground truth. Using these metrics, we compare the performance of <sc>RankCorr</sc> to a variety of other marker selection methods on an assortment of experimental and synthetic data sets that range in size from several thousand to one million cells.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">According to the metrics introduced in this work, <sc>RankCorr</sc> is consistently one of most optimal marker selection methods on scRNA-seq data. Most methods show similar overall performance, however; thus, the speed of the algorithm is the most important consideration for large data sets (and comparing the markers selected by several methods can be fruitful). <sc>RankCorr</sc> is fast enough to easily handle the largest data sets and, as such, it is a useful tool to add into computational pipelines when dealing with high throughput scRNA-seq data. <sc>RankCorr</sc> software is available for download at <ext-link ext-link-type="uri" xlink:href="https://github.com/ahsv/RankCorr">https://github.com/ahsv/RankCorr</ext-link>with extensive documentation.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Single cell RNA-seq</kwd>
      <kwd>Marker selection</kwd>
      <kwd>Machine learning</kwd>
      <kwd>Data analysis</kwd>
      <kwd>Algorithms</kwd>
      <kwd>Benchmarking</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>The Michigan Institute for Data Science</institution>
        </funding-source>
        <award-id>Not available</award-id>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Chan Zuckerberg Initiative</institution>
        </funding-source>
        <award-id>Not available</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p>In recent years, single cell RNA sequencing (scRNA-seq) has made it possible to characterize cellular diversity by determining detailed gene expression profiles of specific cell types and states ([<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]). Furthermore, mRNA data can now be collected from more than one million cells in one experiment due to the development of high throughput microfluidic sequencing protocols [<xref ref-type="bibr" rid="CR3">3</xref>]. The incorporation of unique molecular identifier (UMI) technology additionally makes it possible to process these raw sequencing data into integer valued read counts (instead of the “counts per million fragments” types of rates that were used in bulk sequencing [<xref ref-type="bibr" rid="CR1">1</xref>]). Thus, modern scRNA-seq experiments produce massive amounts of integer valued counts data.</p>
    <p>These scRNA-seq data exhibit high variance and are sparse (often, approximately 90% of the reads are 0 [<xref ref-type="bibr" rid="CR4">4</xref>]) for both biological (e.g. transcriptional bursting) and technical (e.g. 3’ bias in UMI based sequencing protocols) reasons. Those characteristics, in combination with the integer valued quality of the counts and the high dimensionality of the data (often, 20,000 genes show nonzero expression levels in an experiment), are such that scRNA-seq data do not match many of the models that underlie common data analysis techniques. For this reason, many specialized tools have been developed to answer biological questions with scRNA-seq data.</p>
    <p>One such biological question that has generated a significant amount of study in the scRNA-seq literature is the problem of finding <italic>marker genes</italic>. From a biological perspective, we loosely define marker genes as genes that can be used to identify a given group of cells and distinguish those cells from all other cells or from other specific groups of cells. Usually, these are genes that show higher (or lower) levels of expression in the group of interest compared to the rest of the cell population; this provides simple ways to visualize the cell types and to test for the given cell types in experiments. In practice, certain genes are more desirable markers than others; for example, marker genes that encode surface proteins allow for the physical isolation of cell types via fluorescence-activated cell sorting (FACS).</p>
    <p>A multitude of tools for finding marker genes are present in the (sc)RNA-seq literature. These tools often inherently define marker genes to be genes that are <italic>differentially expressed</italic> between two cell populations. That is, in order to find the genes that are useful for separating two populations of cells, a statistical test is applied to each gene in the data set to determine if the distributions of gene expression are different between the two populations: the genes with the most significance are selected as marker genes. The commonly-used analysis tools scanpy [<xref ref-type="bibr" rid="CR5">5</xref>] and Seurat [<xref ref-type="bibr" rid="CR6">6</xref>] implement differential expression methods as their default marker selection techniques; see also [<xref ref-type="bibr" rid="CR7">7</xref>] for a survey of differential expression methods.</p>
    <p>Marker selection has also received extensive study in the computer science literature, where it is known as <italic>feature selection</italic>. Given a data set, the goal of feature selection is to determine a (small) subset of the variables (genes) in that data set that are the most “relevant.” In this case, the relevance of a set of variables is defined by some external evaluation function - different feature selection algorithms use a variety of approaches to optimize different relevance functions.</p>
    <p>There are generally two main classes of feature selection algorithms: greedy algorithms that select features one-by-one, computing a score at each step to determine the next marker to select (for example, forward- or backward-stepwise selection, see Section 3.3 of [<xref ref-type="bibr" rid="CR8">8</xref>]; mutual information based methods, see e.g. [<xref ref-type="bibr" rid="CR9">9</xref>]; and other greedy methods e.g. [<xref ref-type="bibr" rid="CR10">10</xref>]), and slower algorithms that are based on solving some regularized convex optimization problem (for example LASSO [<xref ref-type="bibr" rid="CR11">11</xref>], Elastic Nets [<xref ref-type="bibr" rid="CR12">12</xref>], and other related methods [<xref ref-type="bibr" rid="CR13">13</xref>]).</p>
    <p>A major drawback of many existing feature selection and differential expression algorithms is that they are not designed to handle data that contain more than two cell types. Using a differential expression method, for example, one strategy is to pick a fixed number (e.g., 10) of the statistically most significant genes for each cell type; there may be overlap in the genes selected for different cell types. This strategy does not take into account the fact that some cell types are more difficult to characterize than others, however: one cell type may require more than 10 markers to separate from the other cells, while a different cell type may be separated with only one marker. Setting a significance threshold for the statistical test does not solve this problem; a cell type that is easy to separate from other cells will often exhibit several high significance markers, while a cell type that is difficult to separate might not exhibit any high significance markers.</p>
    <p>In this work, we introduce <sc>RANKCORR</sc>, a feature selection algorithm that addresses the problem of multi-class marker selection on massive data sets in a novel manner<xref ref-type="fn" rid="Fn1">1</xref>. <sc>RANKCORR</sc> is motivated by the algorithm introduced in [<xref ref-type="bibr" rid="CR14">14</xref>]; here, we present a fast method for solving the optimization problem from [<xref ref-type="bibr" rid="CR15">15</xref>] that is at the core of the algorithm from [<xref ref-type="bibr" rid="CR14">14</xref>]. As a result, <sc>RANKCORR</sc> runs quickly: it uses computational resources commensurate with several fast and light simple statistical techniques and it can run on data sets that contain over one million cells. In addition, a key step of the <sc>RANKCORR</sc> method is ranking the scRNA-seq data: this provides a non-parametric way of considering the counts and eliminates the need to normalize the data. Moreover, <sc>RANKCORR</sc> is a one-vs-all method that selects markers for each cluster based on one input parameter. Instead of providing a score for every gene in each cluster and requiring for the user to manually trim these lists down, <sc>RANKCORR</sc> selects an informative number of markers for each cluster. In the general case, different numbers of markers will be selected for different clusters. The union of the markers selected for all clusters provides a set of markers that is informative about the entire clustering. Unlike the method of choosing a significance threshold with a differential expression method, cell types that are more difficult to separate from others will generally contribute more markers to the final set. Thus, <sc>RANKCORR</sc> represents a step towards principled multi-class marker selection when compared to the procedures that are common in most existing methods.</p>
    <p>We test the performance of <sc>RANKCORR</sc> when it is applied to a collection of four experimental UMI counts data sets and an ensemble of synthetic data sets. These data sets contain up to one million cells and include examples of well-differentiated cell types as well as cell differentiation trajectories. Moreover, each data set comes equipped with a cell type classification; we consider classifications that are biologically motivated as well as clusters that are algorithmically created. We refer to the original data source references for their detailed descriptions of clustering and labeling procedures. See Table <xref rid="Tab1" ref-type="table">1</xref> for a summary of the data sets; full descriptions of the experimental data sets and synthetic data sets can be found in the <xref rid="Sec23" ref-type="sec">Methods</xref>.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Data sets considered in this work. The ’genes’ column lists the number of non-zero genes detected in the data set. The peripheral blood mononuclear cell (PBMC) data set from [<xref ref-type="bibr" rid="CR2">2</xref>] appears twice in this table: <sc>ZHENGFILT</sc> contains a subset of the full data set <sc>ZHENGFULL</sc>. See experimental data for more information. <sc>ZHENGSIM</sc> is a collection of simulated data sets created with the Splatter R package; see the generating synthetic data in the <xref rid="Sec23" ref-type="sec">Methods</xref> sections for more information</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Data set</th><th align="left">Cells</th><th align="left">Genes</th><th align="left">Description</th><th align="left">Ground truth clusters</th><th align="left">Ref.</th></tr></thead><tbody><tr><td align="left"><sc>Zeisel</sc></td><td align="left">3005</td><td align="left">4999</td><td align="left">mouse neurons</td><td align="left">9 (well-separated clusters)</td><td align="left">[<xref ref-type="bibr" rid="CR24">24</xref>]</td></tr><tr><td align="left"><sc>Paul</sc></td><td align="left">2730</td><td align="left">3451</td><td align="left">mouse myeloid progenitor cells</td><td align="left">19 (differentiation trajectory)</td><td align="left">[<xref ref-type="bibr" rid="CR25">25</xref>]</td></tr><tr><td align="left"><sc>ZhengFull</sc></td><td align="left">68579</td><td align="left">20387</td><td align="left">human PBMCs</td><td align="left">11 (some clusters overlap)</td><td align="left">[<xref ref-type="bibr" rid="CR2">2</xref>]</td></tr><tr><td align="left"><sc>ZhengFilt</sc></td><td align="left"/><td align="left">5000</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"><sc>10 x Mouse</sc></td><td align="left">1.3 million</td><td align="left">24015</td><td align="left">mouse neurons</td><td align="left">39 (algorithmically generated)</td><td align="left">[<xref ref-type="bibr" rid="CR3">3</xref>]</td></tr><tr><td align="left"><sc>ZhengSim</sc></td><td align="left">5000</td><td align="left">varies</td><td align="left">simulated from human CD19+ B cells</td><td align="left">2</td><td align="left">using [<xref ref-type="bibr" rid="CR26">26</xref>]</td></tr></tbody></table></table-wrap></p>
    <p>Using these data sets, we compare <sc>RANKCORR</sc> to a diverse set of feature selection methods. We consider feature selection algorithms from the computer science literature, the statistical tests used by default in the in the Seurat [<xref ref-type="bibr" rid="CR6">6</xref>] and scanpy [<xref ref-type="bibr" rid="CR5">5</xref>] packages, and several more complex statistical differential expression methods from the scRNA-seq literature. Refer to Table <xref rid="Tab2" ref-type="table">2</xref> and the marker selection methods section for a detailed list.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>Differential expression methods tested in this paper. The “Data sets” column lists the data sets that each method is tested on in this work. The top block contains the methods that are presented in this work; implementations of these methods can be found in the repository linked in the data availability disclosure. The second block of methods consists of general statistical tests. The third block consists of methods that were designed specifically for scRNA-seq data. The fourth block consists of standard machine learning methods; Log. Reg. stands for logistic regression. We also consider selecting markers randomly without replacement. See the marker selection methods description in the <xref rid="Sec23" ref-type="sec">Methods</xref> for more information</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Method</th><th align="left">Data sets</th><th align="left">Package</th><th align="left">Version</th><th align="left">Ref</th></tr></thead><tbody><tr><td align="left"><sc>RankCorr</sc></td><td align="left">All</td><td align="left">custom</td><td align="left"/><td align="left"/></tr><tr><td align="left"><sc>Spa</sc></td><td align="left"><sc>Zeisel</sc>, <sc>Paul</sc></td><td align="left">implementation</td><td align="left"/><td align="left">[<xref ref-type="bibr" rid="CR14">14</xref>]</td></tr><tr><td align="left">t-test</td><td align="left">All</td><td align="left">scanpy</td><td align="left">1.3.7; see text</td><td align="left"/></tr><tr><td align="left">Wilcoxon</td><td align="left">All</td><td align="left"/><td align="left">1.3.7; see text</td><td align="left"/></tr><tr><td align="left">edgeR</td><td align="left"><sc>Zeisel</sc>, <sc>Paul</sc>, <sc>ZhengFilt</sc></td><td align="left">edgeR, rpy2 v2.9.4</td><td align="left">3.24.1</td><td align="left">[<xref ref-type="bibr" rid="CR27">27</xref>]</td></tr><tr><td align="left">MAST</td><td align="left"><sc>Zeisel</sc>, <sc>Paul</sc>, <sc>ZhengFilt</sc></td><td align="left">MAST, rpy2 v2.9.4</td><td align="left">1.8.1</td><td align="left">[<xref ref-type="bibr" rid="CR28">28</xref>]</td></tr><tr><td align="left">scVI</td><td align="left"><sc>Zeisel</sc>, <sc>Paul</sc></td><td align="left">Source from GitHub</td><td align="left">0.2.4</td><td align="left">[<xref ref-type="bibr" rid="CR29">29</xref>]</td></tr><tr><td align="left">Elastic Nets</td><td align="left"><sc>Zeisel</sc>, <sc>Paul</sc></td><td align="left">scikit-learn [<xref ref-type="bibr" rid="CR30">30</xref>]</td><td align="left">0.20.0</td><td align="left">[<xref ref-type="bibr" rid="CR12">12</xref>]</td></tr><tr><td align="left">Log. Reg.</td><td align="left">All</td><td align="left">scanpy</td><td align="left">1.3.7; see text</td><td align="left">[<xref ref-type="bibr" rid="CR31">31</xref>]</td></tr><tr><td align="left">Random selection</td><td align="left"><sc>Zeisel</sc>, <sc>Paul</sc>, <sc>ZhengFilt</sc>,</td><td align="left"/><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"><sc>ZhengFull</sc></td><td align="left"/><td align="left"/><td align="left"/></tr></tbody></table></table-wrap></p>
    <p>There is currently no definitive ground truth set of markers for any experimental scRNA-seq data set. Known markers for cell types have usually been determined from bulk samples, and treating these as ground truth markers neglects the individual cell resolution of single cell sequencing. Moreover, we argue that the set of known markers is incomplete and that other genes could be used as effectively as (or more effectively than) known markers for many cell types. Indeed, finding new, better markers for rare cell types is one of the coveted promises of single cell sequencing.</p>
    <p>Since one goal of marker selection is to discover heretofore unknown markers, we cannot easily evaluate the efficacy of a marker selection algorithm by testing to see if the algorithm recovers a set of previously known markers on experimental scRNA-seq data sets. For this reason we evaluate the quality of the selected markers by measuring how much information the selected markers provide about the given clustering. In this work, we propose several metrics that attempt to quantify this idea.</p>
    <p>According to these evaluation metrics, all of the algorithms considered in this manuscript produce reasonable markers, in the sense that they all perform significantly better than choosing genes uniformly at random. In addition, <sc>RANKCORR</sc> tends to perform well in comparison to the other methods, especially when selecting small numbers of markers. That said, there are generally only small differences between the different marker selection algorithms, and the “best” marker selection method depends on the data set being examined and the evaluation metric in question. It is thus impossible to conclude that any method <italic>always</italic> selects better markers than any of the others.</p>
    <p>The major factors that differentiate the methods examined in this work are the computational resources (both physical and temporal) that the methods require. Since the algorithms show similar overall quality, researchers should prefer marker selection methods that are fast and light. This suggests that fast marker selection methods should be preferred over high complexity algorithms. <sc>RANKCORR</sc> is one of the fastest and lightest algorithms considered in this text, competitive with simple statistical tests. Thus, as a fast and efficient marker selection method that takes a further step towards understanding the multi-class case, <sc>RANKCORR</sc> is a useful tool to add into computational toolboxes.</p>
    <sec id="Sec2">
      <title>Related work: towards a precise definition of marker genes</title>
      <p>In the preceding discussion, we implicitly defined three types of “marker genes”:
<list list-type="bullet"><list-item><p>biological markers, i.e. genes whose expression can be used in a laboratory setting to distinguish the cells in one population from the other cells (or from other cell subpopulations);</p></list-item><list-item><p>genes that are differentially expressed between one cell population and the other cells (or another cell subpopulation);</p></list-item><list-item><p>and genes that are chosen according to a feature selection algorithm that statistically/mathematically characterizes the relevance of genes to the cell populations in some way (e.g. by minimizing a loss function).</p></list-item></list></p>
      <p>Although we will use these ideas fairly interchangeably throughout this manuscript (referring, for example, to “the markers selected by the algorithm”), it is important to keep in mind the differences between them. For instance, a differentially expressed gene that shows low expression is not a particularly useful biological marker. Indeed, it would be difficult to use a low expression gene to visualize the differences between cell types and inefficient to purify cell populations based on a low expression gene with a FACS sorter.</p>
      <p>Several recent marker selection tools start to bridge the gap between differentially expressed genes and biological markers. For example, [<xref ref-type="bibr" rid="CR16">16</xref>] incorporates a high expression requirement in a heuristic mathematical definition of marker genes, and [<xref ref-type="bibr" rid="CR17">17</xref>] utilizes a test for differential expression that is robust to small differences between population means. For a given cell type and candidate marker gene, the test used in [<xref ref-type="bibr" rid="CR17">17</xref>] also incorporates both a lower bound on the number of cells that must express the candidate marker within the cell type and an upper bound on the number of cells that can express a marker outside of the cell type. See the discussion of marker selection methods in the <xref rid="Sec23" ref-type="sec">Methods</xref> for some further information.</p>
      <p>In any case, it is worthwhile to establish a more precise biological definition of a marker gene in order to provide a solid theoretical framework for marker selection. For example, one biological definition of markers requires the cells to be grouped before markers can be determined; this assumes that markers are inherently associated with known cell types or states. This approach is influenced by the computational pipeline that many researchers are currently following (clustering followed by marker selection, see e.g. [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]) and is the approach we consider in this manuscript. An alternative is to define markers as genes that naturally separate the cells into groups in some nice way; the discovered groups would then be classified into different cell types (i.e. allow marker selection to guide clustering). Another recent method [<xref ref-type="bibr" rid="CR20">20</xref>] defines markers in terms of their overall importance to a clustering, eschewing the notion of markers for specific cell types. Their framework also incorporates finding markers for hierarchical cell type classifications (instead of “flat” clustering). We leave full considerations of rigorous definitions for future work.</p>
    </sec>
    <sec id="Sec3">
      <title>Notation and definitions</title>
      <p>Let <bold><italic>R</italic></bold> denote the set of real numbers, <bold><italic>Z</italic></bold> denote the set of integers, and <bold><italic>N</italic></bold> denote the set of natural numbers.</p>
      <p>Consider an scRNA-seq experiment that collects gene expression information from <italic>n</italic> cells, and assume that <italic>p</italic> different mRNAs are detected during the experiment. After processing, for each cell that is sequenced, a vector <italic>x</italic>∈<bold><italic>R</italic></bold><sup><italic>p</italic></sup> is obtained: <italic>x</italic><sub><italic>j</italic></sub> represents the number of copies of a specific mRNA that was observed during the sequencing procedure. When all <italic>n</italic> cells are sequenced, this results in <italic>n</italic> vectors in <bold><italic>R</italic></bold><sup><italic>p</italic></sup>, which we arrange into a data matrix <italic>X</italic>∈<bold><italic>R</italic></bold><sup><italic>n</italic>×<italic>p</italic></sup>. The entry <italic>X</italic><sub><italic>i</italic>,<italic>j</italic></sub> represents the number of counts of gene <italic>j</italic> in cell <italic>i</italic>. Note that this is the <italic>transpose</italic> of the data matrix that is common in the scRNA-seq literature.</p>
      <p>Let [ <italic>n</italic>]={1,…,<italic>n</italic>}. For a matrix <italic>X</italic>, let <italic>X</italic><sub><italic>i</italic></sub> denote column <italic>i</italic> of <italic>X</italic>. Given a vector <italic>x</italic>, let <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\mu (x) = \bar {x}$\end{document}</tex-math><mml:math id="M2"><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq1.gif"/></alternatives></inline-formula> denote the average of the elements of <italic>x</italic> and let <italic>σ</italic>(<italic>x</italic>) represent the standard deviation of the elements in <italic>x</italic>; that is, <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\sigma (x) = \sqrt {\tfrac 1n {\sum \nolimits }_{i=1}^{n} (x_{i} - \mu (x))^{2}}.$\end{document}</tex-math><mml:math id="M4"><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt><mml:mi>.</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq2.gif"/></alternatives></inline-formula> We use the notation ∥<italic>x</italic>∥<sub><italic>p</italic></sub> to represent the <italic>p</italic>-norm of the vector <italic>x</italic>. For example, ∥<italic>x</italic>∥<sub>2</sub> is the standard Euclidean norm of <italic>x</italic> and <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left \|x\right \|_{1} = {\sum \nolimits }_{i=1}^{n} |x_{i}|$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq3.gif"/></alternatives></inline-formula>. The notation ∥<italic>x</italic>∥<sub>0</sub> represents the number of nonzero elements in <italic>x</italic>.</p>
    </sec>
    <sec id="Sec4">
      <title>Ranking scRNA-seq data</title>
      <p>The first step of <sc>RANKCORR</sc> is to <italic>rank</italic> the entries of an scRNA-seq counts matrix <italic>X</italic>. In this section, we make the notion of ranking precise, and we establish some intuition as to why the rank transformation produces intelligible results on scRNA-seq UMI counts data. We can do much more formal analysis in regards to the behavior of the rank transformation on scRNA-seq data; this analysis will appear in an upcoming work.</p>
      <p>Consider a vector <italic>x</italic>∈<bold><italic>R</italic></bold><sup><italic>n</italic></sup>. For a given index <italic>i</italic> with 1≤<italic>i</italic>≤<italic>n</italic>, let <italic>S</italic><sub><italic>i</italic></sub>(<italic>x</italic>)={<italic>ℓ</italic>∈[ <italic>n</italic>]:<italic>x</italic><sub><italic>ℓ</italic></sub>&lt;<italic>x</italic><sub><italic>i</italic></sub>} and <italic>E</italic><sub><italic>i</italic></sub>(<italic>x</italic>)={<italic>ℓ</italic>∈[ <italic>n</italic>]:<italic>x</italic><sub><italic>ℓ</italic></sub>=<italic>x</italic><sub><italic>i</italic></sub>} (note that <italic>i</italic>∈<italic>E</italic><sub><italic>i</italic></sub>(<italic>x</italic>)). We have that |<italic>S</italic><sub><italic>i</italic></sub>(<italic>x</italic>)| is the number of elements of <italic>x</italic> that are strictly smaller than <italic>x</italic><sub><italic>i</italic></sub> and |<italic>E</italic><sub><italic>i</italic></sub>(<italic>x</italic>)| is the number of elements of <italic>x</italic> that are equal to <italic>x</italic><sub><italic>i</italic></sub> (including <italic>x</italic><sub><italic>i</italic></sub> itself).</p>
      <sec id="d30e1480">
        <title>
          <bold>Definition 1</bold>
        </title>
        <p>The <italic>rank transformation</italic>
<italic>Φ</italic>:<bold><italic>R</italic></bold><sup><italic>n</italic></sup>→<bold><italic>R</italic></bold><sup><italic>n</italic></sup> is defined by
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \Phi(x)_{i} = \left|S_{i}(x)\right| + \frac{\left|E_{i}(x)\right| + 1}{2}.  $$ \end{document}</tex-math><mml:math id="M8"><mml:mi>Φ</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mi>.</mml:mi></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>Note that <italic>Φ</italic>(<italic>x</italic>)<sub><italic>i</italic></sub> is the index of <italic>x</italic><sub><italic>i</italic></sub> in an ordered version of <italic>x</italic> (i.e. it is the <italic>rank</italic> of <italic>x</italic><sub><italic>i</italic></sub> in <italic>x</italic>). If multiple elements in <italic>x</italic> are equal, we assign their ranks to be the average of the ranks that would be assigned to those elements (that is, for fixed <italic>i</italic>, all elements <italic>x</italic><sub><italic>j</italic></sub> for <italic>j</italic>∈<italic>E</italic><sub><italic>i</italic></sub>(<italic>x</italic>) will be assigned the same rank).</p>
      </sec>
      <sec id="d30e1631">
        <title>
          <bold>Example 1</bold>
        </title>
        <p>Let <italic>n</italic>=5, and consider the point <italic>x</italic>=(17,17,4,308,17). Then <italic>Φ</italic>(<italic>x</italic>)=(3,3,1,5,3). This value will be the same as the rank transformation applied to any point in <italic>x</italic>∈<bold><italic>R</italic></bold><sup>5</sup> with <italic>x</italic><sub>3</sub>&lt;<italic>x</italic><sub>1</sub>=<italic>x</italic><sub>2</sub>=<italic>x</italic><sub>5</sub>&lt;<italic>x</italic><sub>4</sub>.</p>
        <p>Ranking is commonly used in non-parametric statistical tests - ranking scRNA-seq data allows for statistical tests to be performed on the data without specific assumptions about the underlying distribution for the counts. This is important, since models for the counts distribution are continually evolving. As the measurement technology develops, different statistical models become more (or less) appropriate.</p>
        <p>In addition, the rank transformation seems to be especially suited to UMI counts data, which is sparse and has a high dynamic range. When analyzing the expression of a fixed gene <italic>g</italic> across a population of cells, it is intuitive to separate the cells that are observed to express <italic>g</italic> from the cells that are not. Among the cells that express <italic>g</italic>, it is important to distinguish between low expression of <italic>g</italic> and high expression of <italic>g</italic>. The actual counts of <italic>g</italic> in cells with high expression (say a count of 500 vs a count of 1000) are often not especially important. Under the rank transformation, the largest count will be brought adjacent to the second-largest - no gap will be preserved. On the other hand, since there are many entries that are 0, the gap between no expression (a count of 0) and some expression will be significantly expanded (in Eq. (<xref rid="Equ1" ref-type="">1</xref>), the set <italic>E</italic><sub><italic>i</italic></sub>(<italic>x</italic>) will be large for any <italic>i</italic> such that <italic>x</italic><sub><italic>i</italic></sub>=0). See Fig. <xref rid="Fig1" ref-type="fig">1</xref> for a visualization of these ideas on experimental scRNA-seq data.
<fig id="Fig1"><label>Fig. 1</label><caption><p>Counts of gene PRTN3 in bone marrow cells in the <sc>PAUL</sc> data set (See the <xref rid="Sec23" ref-type="sec">Methods</xref>). Each point corresponds to a cell; the horizontal axis shows the number of reads and the vertical axis shows the number of cells with a fixed number of reads. No library size or cell size normalization has been carried out in these pictures. Note that the tail of the log transformed data is subjectively longer, while the gap between zero counts and nonzero counts appears larger in the rank transformed data</p></caption><graphic xlink:href="12859_2020_3641_Fig1_HTML" id="MO1"/></fig></p>
        <p>Stratifying gene expression in this way is intuitively useful for determining the genes that are important in identifying cell types: a gene that shows expression in many cells of a given cell type can be used to separate that cell type from all of the others and thus is a useful marker gene. Thus, by enforcing a large separation between expression and no expression (when compared to the separation between low expression and high expression), it will be easier to identify markers. For these reasons, and since the rank transformation has shown promise in other scRNA-seq tools (for example NODES [<xref ref-type="bibr" rid="CR21">21</xref>]) we use the rank transform in the <sc>RANKCORR</sc> marker selection algorithm.</p>
        <p>A final note is that a connection can be made between the rank transformation and the log normalization that is commonly performed in the scRNA-seq literature. Often, the counts matrix <italic>X</italic> is normalized by taking <italic>X</italic><sub><italic>ij</italic></sub>↦ log(<italic>X</italic><sub><italic>ij</italic></sub>+1). This is a nonlinear transformation that helps to reduce the gaps between the largest entries of <italic>X</italic> while leaving the entries that were originally 0 unchanged (and preserving much of the gap between “no expression” and “some expression”). With this in mind, the rank transformation can be viewed as a more aggressive log transformation.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec5" sec-type="results">
    <title>Results</title>
    <p>Our results are of two types: (i) algorithmic performance guarantees for <sc>RANKCORR</sc> and (ii) empirical performance of <sc>RANKCORR</sc> and its comparison algorithms on scRNA-seq data sets (both benchmark and simulated).</p>
    <sec id="Sec6">
      <title><sc>RANKCORR</sc>: algorithmic performance guarantees</title>
      <p><sc>RANKCORR</sc> is based on the ideas presented in [<xref ref-type="bibr" rid="CR14">14</xref>]. It is a fast algorithm that chooses an informative number of genes for each cluster by first ranking the scRNA-seq data, and then splitting the clusters in the ranked data with sparse separating hyperplanes that pass through the origin.</p>
      <p>A full description of the RankCorr algorithm is found in the <xref rid="Sec23" ref-type="sec">Methods</xref>. We provide an outline here so as to explain why it is such an efficient algorithm as compared to alternatives. <sc>RANKCORR</sc> requires three inputs:
<list list-type="bullet"><list-item><p>An scRNA-seq counts matrix <italic>X</italic>∈<bold><italic>R</italic></bold><sup><italic>n</italic>×<italic>p</italic></sup> (<italic>n</italic> cells, <italic>p</italic> genes). The rank transformation provides a non-parametric normalization of the counts data, and thus there is no need to normalize the counts data before starting marker selection.</p></list-item><list-item><p>A vector of labels <italic>y</italic>∈<bold><italic>Z</italic></bold><sup><italic>n</italic></sup> that defines a grouping of the cells (<italic>y</italic><sub><italic>i</italic></sub>=<italic>k</italic> means that cell <italic>i</italic> belongs to group <italic>k</italic>). We think of <italic>y</italic> as separating the cells into distinct cell types or cell states, but in general the groups defined by <italic>y</italic> could consist of any arbitrary (non-overlapping) subsets of cells.</p></list-item><list-item><p>A parameter <italic>s</italic> that indirectly controls the number of markers to select. For a fixed input <italic>X</italic>, increasing <italic>s</italic> will produce more markers.</p></list-item></list></p>
      <p>Before selecting markers, <sc>RANKCORR</sc> ranks and standardizes the input data <italic>X</italic> to create the matrix <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M10"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq4.gif"/></alternatives></inline-formula> defined by
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \overline{X}_{j} = \frac{\Phi\left(X_{j}\right) - \mu\left(\Phi\left(X_{j}\right)\right)}{\sigma\left(\Phi\left(X_{j}\right)\right)}  $$ \end{document}</tex-math><mml:math id="M12"><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>Φ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>Φ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>Φ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <italic>Φ</italic> is the rank transformation, as defined in (<xref rid="Equ1" ref-type="">1</xref>), and <italic>X</italic><sub><italic>j</italic></sub> denotes the <italic>j</italic>-th column of <italic>X</italic><xref ref-type="fn" rid="Fn2">2</xref>. Markers are then selected for the clustering defined by <italic>y</italic> in a one-vs-all manner. For a single group <italic>k</italic>∈<italic>y</italic>, define <italic>τ</italic>∈{±1}<sup><italic>n</italic></sup> such that <italic>τ</italic><sub><italic>i</italic></sub>=+1 if <italic>y</italic><sub><italic>i</italic></sub>=<italic>k</italic> (that is, if cell <italic>i</italic> is in group <italic>k</italic>) and <italic>τ</italic><sub><italic>i</italic></sub>=−1 otherwise; we refer to <italic>τ</italic> as the <italic>cluster indicator vector</italic> for the group <italic>k</italic>. To select markers for group <italic>k</italic>, <sc>RANKCORR</sc> constructs the vector <inline-formula id="IEq5"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {\tau } = \Phi (\tau) - \mu \left (\Phi \left (\tau \right)\right)$\end{document}</tex-math><mml:math id="M14"><mml:mover accent="false"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>Φ</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>Φ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq5.gif"/></alternatives></inline-formula>. Following this, using the input parameter <italic>s</italic>, the algorithm determines the vector <inline-formula id="IEq6"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M16"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq6.gif"/></alternatives></inline-formula> as the solution to the optimization problem (<xref rid="Equ3" ref-type="">3</xref>):
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} \hat{\omega} = \underset{\omega}{\arg\max} &amp; \sum\limits_{i=1}^{n} \overline{\tau}_{i} \left\langle \overline{x}_{i}, \omega \right\rangle\\ \mathrm{subject\ to\ } &amp; \left\|\omega\right\|_{2} \leq 1, \left\|\omega\right\|_{1} \leq \sqrt{s} \end{aligned}  $$ \end{document}</tex-math><mml:math id="M18"><mml:mtable><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>arg</mml:mo><mml:mo>max</mml:mo></mml:mrow><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:munder></mml:mtd><mml:mtd><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mfenced close="〉" open="〈"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">subject</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">to</mml:mtext><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msqrt><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msqrt></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>where <inline-formula id="IEq7"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {x}_{i}$\end{document}</tex-math><mml:math id="M20"><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq7.gif"/></alternatives></inline-formula> denotes the <italic>i</italic>-th row of <inline-formula id="IEq8"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M22"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq8.gif"/></alternatives></inline-formula>. <sc>RANKCORR</sc> returns the genes that have nonzero support in <inline-formula id="IEq9"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M24"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq9.gif"/></alternatives></inline-formula> as the markers for group <italic>k</italic>.</p>
      <p>Note that the output <inline-formula id="IEq10"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M26"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq10.gif"/></alternatives></inline-formula> to (<xref rid="Equ3" ref-type="">3</xref>) can be viewed as the normal vector to a hyperplane that passes through the origin and attempts to split the cells in group <italic>k</italic> from the cells that aren’t in group <italic>k</italic>. For example, if cell <italic>i</italic> is in group <italic>k</italic> (i.e. <italic>y</italic><sub><italic>i</italic></sub>=<italic>k</italic>), then the term <inline-formula id="IEq11"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {\tau }_{i} \langle \overline {x}_{i}, \omega \rangle $\end{document}</tex-math><mml:math id="M28"><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo>〉</mml:mo></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq11.gif"/></alternatives></inline-formula> is positive exactly when <inline-formula id="IEq12"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\langle \overline {x}_{i}, \omega \rangle &gt;0$\end{document}</tex-math><mml:math id="M30"><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo>〉</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq12.gif"/></alternatives></inline-formula>. Thus, the objective function in (<xref rid="Equ3" ref-type="">3</xref>) increases when more cells from group <italic>k</italic> are on the same side of the hyperplane with normal vector <inline-formula id="IEq13"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M32"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq13.gif"/></alternatives></inline-formula>.</p>
      <p>The optimization (<xref rid="Equ3" ref-type="">3</xref>) was originally introduced in [<xref ref-type="bibr" rid="CR15">15</xref>] in the context of sparse signal recovery and was adapted to the context of feature selection in a biological setting in [<xref ref-type="bibr" rid="CR14">14</xref>]. The speed of <sc>RANKCORR</sc> is due to a fast algorithm (presented in the <xref rid="Sec23" ref-type="sec">Methods</xref>) that allows us to quickly jump to the solution of the optimization (<xref rid="Equ3" ref-type="">3</xref>) without the use of specialized optimization software.</p>
      <sec id="Sec7">
        <title><sc>RANKCORR</sc> handles multi-class marker selection in a non-trivial one-vs-all manner</title>
        <p>To extend <sc>RANKCORR</sc> to the multi-class scenario, we select markers for each group of cells defined by <italic>y</italic>. In particular, the value of the parameter <italic>s</italic> is fixed and the optimization (<xref rid="Equ3" ref-type="">3</xref>) is run for each group of cells using the fixed value of <italic>s</italic>. To obtain a collection of markers for the clustering defined by <italic>y</italic> as a whole, <sc>RANKCORR</sc> returns the union of the selected markers from all of the groups. Alternatively, the markers can be kept separate to provide genes that can identify the individual groups.</p>
        <p>The effect of fixing <italic>s</italic> across groups is complex. In the full description of RankCorr, we show that, for a fixed cluster with cluster indicator vector <italic>τ</italic>, the markers that <sc>RANKCORR</sc> selects are the genes that have the highest (in magnitude) Spearman correlation with <italic>τ</italic>. That is, similar to a differential expression method, <sc>RANKCORR</sc> can be thought of as generating lists of gene scores (one for each group) that are used to select the proper markers - instead of <italic>p</italic>-values, the scores considered by <sc>RANKCORR</sc> are magnitudes of specific Spearman correlations.</p>
        <p>The parameter <italic>s</italic> does not directly control the number of high-correlation markers that are selected, however, and fixing <italic>s</italic> results in different numbers of markers for each cluster. Thus, the set of markers selected by <sc>RANKCORR</sc> is different from the set obtained by choosing a constant number of top scoring genes for each cluster. Additionally, fixing <italic>s</italic> is not equivalent to picking a correlation threshold <italic>ρ</italic> and selecting all genes that exhibit a Spearman correlation greater than <italic>ρ</italic> with any cluster indicator vector. Determining a precise characterization of the numbers of markers selected for each cluster is left for future work.</p>
        <p>A key result is that <sc>RANKCORR</sc> contains a new method of merging lists of scores that is based on an algorithm with known performance guarantees [<xref ref-type="bibr" rid="CR15">15</xref>]. Whether or not this one-vs-all method is an improvement over common heuristics for merging lists requires further exploration. There is some evidence, collected using simple synthetic test data, that selecting a constant number genes with the top Spearman correlation scores for each cluster results in comparable performance to <sc>RANKCORR</sc>. This has not been studied in the context of experimental data, however, and does not lead to any appreciable time savings over <sc>RANKCORR</sc>. It is also possible that the merging method used by <sc>RANKCORR</sc> could be adapted to work with <italic>p</italic>-values and provide an alternative method of merging the lists that are produced by differential expression methods. Thus we focus on <sc>RANKCORR</sc> as it is currently presented.</p>
      </sec>
    </sec>
    <sec id="Sec8">
      <title>Empirical performance of <sc>RANKCORR</sc></title>
      <p>In the remainder of this <xref rid="Sec5" ref-type="sec">Results</xref>, we present evidence that <sc>RANKCORR</sc> selects markers that are generally similar in quality to (or better than) the markers that are selected by other commonly used marker selection methods. Moreover, <sc>RANKCORR</sc> runs quickly, and only requires computational resources comparable to those required to run simple statistical tests. Thus, <sc>RANKCORR</sc> is a useful marker selection tool for researchers to add to their computational libraries.</p>
      <p>We evaluated the performance of <sc>RANKCORR</sc> on four experimental data sets and a collection of synthetic data sets. These data sets are listed in Table <xref rid="Tab1" ref-type="table">1</xref>; see the experimental data and synthetic data descriptions in the <xref rid="Sec23" ref-type="sec">Methods</xref> for more information (including further details about the ground truth clusters considered in this analysis). We compare <sc>RANKCORR</sc> to the marker selection methods listed in the leftmost column of Table <xref rid="Tab2" ref-type="table">2</xref>. The Wilcoxon method is the default marker selection technique in the Seurat [<xref ref-type="bibr" rid="CR6">6</xref>] package, while the scanpy [<xref ref-type="bibr" rid="CR5">5</xref>] package defaults to the version of the t-test that we include here. See the marker selection methods section for more implementation details.</p>
      <sec id="Sec9">
        <title>Evaluation of marker sets when ground truth markers are not known</title>
        <p>To interpret, evaluate, and simply to present our results, we must quantify how much information a set of selected markers provides about a given clustering when ground truth markers are not known for certain (e.g., when selecting markers on an experimental data set). We propose two general procedures to accomplish this and present results using these:
<list list-type="bullet"><list-item><p>Supervised classification: train a classifier on the data contained in the selected markers using the ground truth clustering as the target output.</p></list-item><list-item><p>Unsupervised clustering: cluster the cells using the information in the set of selected markers without reference to the ground truth clustering.</p></list-item></list></p>
        <p>In this study, we implemented algorithms that accomplish each general procedure. For each algorithm, we considered several different evaluation metrics. A summary of these algorithms and marker set evaluation metrics is found in Table <xref rid="Tab3" ref-type="table">3</xref>, along with the abbreviations that we will use to refer to the metrics.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Evaluation metrics for marker sets on experimental data. The “Average precision” metric is a weighted average of precision over the clusters. The Matthews correlation coefficient is a summary statistic that incorporates all information from the confusion matrix. See [<xref ref-type="bibr" rid="CR22">22</xref>] for more information about the classification metrics and [<xref ref-type="bibr" rid="CR23">23</xref>] for more information about the clustering metrics</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">General procedure</th><th align="left">Methods</th><th align="left">Metrics</th></tr></thead><tbody><tr><td align="left">Supervised classification</td><td align="left">Nearest centroid classifier (NCC)Random forests classifier (RFC)</td><td align="left">Classification error (1 - accuracy)</td></tr><tr><td align="left"/><td align="left"/><td align="left">Average precision</td></tr><tr><td align="left"/><td align="left"/><td align="left">Matthews correlation coefficient</td></tr><tr><td align="left">Unsupervised clustering</td><td align="left">Louvain clustering</td><td align="left">Adjusted rand index (ARI)</td></tr><tr><td align="left"/><td align="left"/><td align="left">Adjusted mutual information (AMI)</td></tr><tr><td align="left"/><td align="left"/><td align="left">Fowlkes-Mallows score (FMS)</td></tr></tbody></table></table-wrap></p>
        <p>The three supervised classification metrics (error rate, precision, and Matthews correlation coefficient) generally provide similar information. Thus, for most selected marker sets, we present results from five of the metrics (NCC classification error, RFC classification error, ARI, AMI, and FMS). The precision and Matthews correlation coefficient data can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref>. See the marker evaluation metrics in the <xref rid="Sec23" ref-type="sec">Methods</xref> for further details about these metrics.</p>
        <p>It is important to note that these metrics represent some summary statistical information about the selected markers - they do not capture the full information contained in a set of genes. Results on synthetic data suggest that the metrics are informative but not fine-grained enough to capture all differences between methods. Therefore, we would advise considering these metrics as “tests” for marker selection methods; that is, these metrics should mostly be used to identify marker selection methods that don’t perform well.</p>
        <p>We compute each of the metrics using 5-fold cross-validation in order to reduce overfitting (see Section 7.10 of [<xref ref-type="bibr" rid="CR8">8</xref>]). The timing information reported in the following sections represents the time needed to select markers on one fold.</p>
      </sec>
      <sec id="Sec10">
        <title>Evaluating <sc>RANKCORR</sc> on experimental data</title>
        <p>Performance summaries of the of the methods on the <sc>ZEISEL</sc>, <sc>PAUL</sc>, and <sc>ZHENGFILT</sc> data sets are presented in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. In this figure, the colors of the boxes indicate relative performance: a blue box indicates performance that is better than the majority of the other methods, a yellow box indicates median performance, and an orange box indicates performance that is worse than the other methods. The coloring in the figures is based on Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref>, <xref rid="Fig6" ref-type="fig">6</xref>, <xref rid="Fig7" ref-type="fig">7</xref>, <xref rid="Fig8" ref-type="fig">8</xref>, <xref rid="Fig9" ref-type="fig">9</xref> and <xref rid="Fig10" ref-type="fig">10</xref>; the numbers of markers in the bins (in the top row) are selected to emphasize features found in these plots. The first row for each method represents the classification metrics and the second row represents the clustering metrics.
<fig id="Fig2"><label>Fig. 2</label><caption><p>Performance of the marker selection methods on the (<bold>a</bold>) <sc>ZEISEL</sc>, (<bold>b</bold>) (<sc>PAUL</sc>), and (<bold>c</bold>) <sc>ZHENGFILT</sc> data sets as the number of selected markers is varied. There are two rows for each method; the first row for each method represents the classification metrics and the second row represents the clustering metrics. Blue indicates better performance than the other methods; orange indicates notably worse performance than the other methods. The marker bins are chosen to emphasize certain features in Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref>, <xref rid="Fig6" ref-type="fig">6</xref>, <xref rid="Fig7" ref-type="fig">7</xref>, <xref rid="Fig8" ref-type="fig">8</xref>, <xref rid="Fig9" ref-type="fig">9</xref> and <xref rid="Fig10" ref-type="fig">10</xref>; these figures present the values of the evaluation metrics for the different data sets. The values in the boxes correspond to a ranking of the methods, with 1 being the best method in the marker range. The classification and clustering results are ranked separately. Further notes: (a) All of the methods perform well on the <sc>ZEISEL</sc> data set - an orange box here does not indicate poor performance, but rather that other methods outperformed the orange one. (b) Many of the methods showed nearly identical performance according to the classification metrics; thus, this table contains many yellow boxes</p></caption><graphic xlink:href="12859_2020_3641_Fig2_HTML" id="MO2"/></fig></p>
        <p>The values in the columns in Fig. <xref rid="Fig2" ref-type="fig">2</xref> correspond to a heuristic ranking of the methods, with 1 the optimal method (on average) in the indicated range of markers; see the <xref rid="Sec23" ref-type="sec">Methods</xref> for a full description of how the ranking is calculated. The classification metrics and clustering metrics are ranked separately (so that each column contains two full rankings of the methods; e.g. in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b, every column contains the numbers 1 to 9 twice). Since these numbers are ranks, they do not capture the magnitude of the gaps in performance between methods. For example, if two methods differ by one rank (e.g. the method ranked 2 vs the method ranked 3), there could be a large gap in performance between the two methods. The colors of the cells are meant to capture the larger differences between (tiers of) methods, and methods with the same color in a column perform comparably (regardless of the difference in rank). For example, in some cases, the top four methods (ranked 1 through 4) are similar to each other and clearly better than the others, so they will be colored blue. In other cases, no method will appear significantly better than the others, so none of the boxes will be colored blue.</p>
        <p>The <sc>ZHENGFULL</sc> and <sc>10XMOUSE</sc> data sets were too large for the majority of the methods to handle; thus, data were only collected for the <sc>RANKCORR</sc>, t-test, logistic regression, and Wilcoxon methods (the fastest methods) on these data sets. We include the <sc>10XMOUSE</sc> data specifically as a stress test to determine the methods that can handle the largest data sets. It is impressive that these methods are able to run on such a large data set in a reasonable amount of time. The performance characteristics of the methods on these data sets are found later in this section.</p>
        <p>Overall, the different methods select sets of markers that are of similar quality: the performance of any “optimal” method is usually not much better than several of its competitors. For example, the true differences in performance between the yellow and blue boxes in Fig. <xref rid="Fig2" ref-type="fig">2</xref> are often quite small. In addition, there is no method that consistently selects the best markers. The optimal method depends on the choice of data set, the evaluation metric, and the number of markers that are selected.</p>
        <p>That said, the <sc>RANKCORR</sc> method tends to perform well on these data sets: it is generally competitive with the best methods in terms of performance. In particular, it especially excels when selecting less than 100 markers on all three data sets according to both the clustering and classification metrics.</p>
        <p>Since the algorithms generally exhibit similar performances under the metrics considered in this work, efficient algorithms have a significant advantage. The computational resources (total computer time and memory) required to select markers on the experimental data sets are presented in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. The fastest and lightest methods are <sc>RANKCORR</sc>, the t-test, and Wilcoxon: notably, <sc>RANKCORR</sc> is nearly as fast and light as the two very simple statistical methods (the t-test and Wilcoxon). It is thus these three methods that show a clear advantage over the other methods for working with experimental data. Logistic regression also runs quickly on the smaller data sets but does not scale as well as the three methods mentioned above and significantly slows down on the larger data sets. In addition, logistic regression shows inconsistent performance and is often one of the worst performers when selecting small numbers of markers. The other methods are significantly slower or require large computational resources compared to the size of the data set: see Fig. <xref rid="Fig3" ref-type="fig">3</xref> for further discussion.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Computational resources used by the marker selection methods. In both figures, the data set size is the number of entries in the data matrix <italic>X</italic>: it is given by <italic>n</italic>×<italic>p</italic>, the number of cells times the number of genes. The data sets that we consider in this work are indicated in the figures. The total computation time required to select markers on one fold (CPU time, calculated as number of processors used multiplied by the time taken for marker selection) is shown in (<bold>a</bold>); the total memory required during these trials is shown in (<bold>b</bold>). Elastic nets scales poorly in (<bold>a</bold>), so it is only run on <sc>PAUL</sc> and <sc>ZEISEL</sc>. Both edgeR and MAST are limited by memory on <sc>ZHENGFILT</sc> (see (<bold>b</bold>)); this prevents their application to the larger data sets. scVI also requires a GPU while it is running; this prevents us from testing it on the larger data sets. <sc>RANKCORR</sc>, the t-test, Wilcoxon, and logistic regression all use 8 GB to run on <sc>ZHENGFULL</sc> and 80 GB to run on <sc>10XMOUSE</sc>. See the marker selection methods for more details</p></caption><graphic xlink:href="12859_2020_3641_Fig3_HTML" id="MO3"/></fig><fig id="Fig4"><label>Fig. 4</label><caption><p>Error rate of both the nearest centroids classifier (NCC; (<bold>a</bold>) and (<bold>b</bold>)) and the random forests classifier (RFC; (<bold>c</bold>) and (<bold>d</bold>)) on the Zeisel data set. Figure (<bold>b</bold>) (respectively (<bold>d</bold>)) is a detailed image of the error rate of the different methods using the NCC (respectively RFC) when smaller numbers of markers are selected</p></caption><graphic xlink:href="12859_2020_3641_Fig4_HTML" id="MO9"/></fig><fig id="Fig5"><label>Fig. 5</label><caption><p>Clustering performance metrics vs total number of markers selected for marker selection methods on the <sc>ZEISEL</sc> data set. The ARI score is shown in (<bold>a</bold>), the AMI score is shown in (<bold>b</bold>), and the Fowlkes-Mallows score is shown in (<bold>c</bold>). The clustering is carried out using 5-fold cross validation and scores are averaged across folds</p></caption><graphic xlink:href="12859_2020_3641_Fig5_HTML" id="MO10"/></fig><fig id="Fig6"><label>Fig. 6</label><caption><p>Error rates of both the nearest centroids classifier (NCC; (<bold>a</bold>) and (<bold>b</bold>)) and the random forests classifier (RFC; (<bold>c</bold>) and (<bold>d</bold>)) on the Paul data set. Figure (<bold>b</bold>) (respectively (<bold>d</bold>)) is a detailed image of the error rate of the different methods using the NCC (respectively RFC) when smaller numbers of markers are selected. Figure (<bold>b</bold>) details up to 220 total markers to make clear how similar the methods perform when small numbers of markers are selected. Figure (<bold>d</bold>) examines up to 350 total markers to detail the performance of the methods when small numbers of markers are selected as well as get an idea for the increasing behavior and noisy nature of the curves</p></caption><graphic xlink:href="12859_2020_3641_Fig6_HTML" id="MO4"/></fig><fig id="Fig7"><label>Fig. 7</label><caption><p>Clustering performance metrics vs total number of markers selected for marker selection methods on the <sc>PAUL</sc> data set. The ARI score is shown in (<bold>a</bold>), the AMI score is shown in (<bold>b</bold>), and the Fowlkes-Mallows score is shown in (<bold>c</bold>). The clustering is carried out using 5-fold cross validation and scores are averaged across folds</p></caption><graphic xlink:href="12859_2020_3641_Fig7_HTML" id="MO5"/></fig><fig id="Fig8"><label>Fig. 8</label><caption><p>Accuracy and precision of the nearest centroids classifier on the <sc>ZHENG</sc> data sets using the bulk labels. The top row corresponds to the <sc>ZHENGFILT</sc> data set and the bottom row corresponds to the <sc>ZHENGFULL</sc> data set</p></caption><graphic xlink:href="12859_2020_3641_Fig8_HTML" id="MO6"/></fig><fig id="Fig9"><label>Fig. 9</label><caption><p>Accuracy and precision of the random forests classifier on the <sc>ZHENG</sc> data sets using the bulk labels. The top row corresponds to the <sc>ZHENGFILT</sc> data set and the bottom row corresponds to the <sc>ZHENGFULL</sc> data set</p></caption><graphic xlink:href="12859_2020_3641_Fig9_HTML" id="MO7"/></fig><fig id="Fig10"><label>Fig. 10</label><caption><p>Clustering metrics on the <sc>ZHENGFILT</sc> data set</p></caption><graphic xlink:href="12859_2020_3641_Fig10_HTML" id="MO8"/></fig></p>
        <p>These results support the idea that <sc>RANKCORR</sc> is a worthwhile marker selection method to consider (along side other fast methods) when analyzing massive UMI data sets. In the rest of this section, we give performance results on each specific data set.</p>
      </sec>
      <sec id="Sec11">
        <title>The marker selection methods perform well on the <sc>ZEISEL</sc> data set</title>
        <p>The classification error rates of the nearest centroid and random forests classifiers on the <sc>ZEISEL</sc> data set are presented in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. The error rates are very low: it requires only 100 markers (an average of 11 markers per cluster) to reach an error rate lower than 5% for most methods using the RFC. The ARI, AMI, and FM scores, reported in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, are also high (good) for all methods. Only a small number of markers were selected by elastic nets on the <sc>ZEISEL</sc> data set; thus, the elastic nets curves end before the others. Figure <xref rid="Fig2" ref-type="fig">2</xref>a contains a summary of the data presented in Figs. <xref rid="Fig4" ref-type="fig">4</xref> and <xref rid="Fig5" ref-type="fig">5</xref>. The computational resources required by the methods are presented in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, where it is clear that the <sc>RANKCORR</sc>, t-test, Wilcoxon, and logistic regression methods all run quickly on the <sc>ZEISEL</sc> data set and require few resources in comparison to the other methods.</p>
        <p>The ground truth clustering that we consider on the <sc>ZEISEL</sc> data set is biologically motivated and contains nine clusters that are generally well separated (they represent distinct cell types) [<xref ref-type="bibr" rid="CR24">24</xref>]. Most of the methods tested here produce markers that provide a significant amount of information about this ground truth clustering; these results thus represent a biological verification of the marker selection methods. That is, in this ideal biological scenario (a data set with highly discrete cell types) the (mathematically or statistically defined) markers that are chosen by the methods are biologically informative and can be used as real (biological) markers.</p>
        <p>This suggests that, when selecting markers on a data set that is well clustered, it is useful to examine several marker selection algorithms to get different perspectives on which genes are most important. Marker selection algorithms that can run using only small amounts of resources, such as <sc>RANKCORR</sc> (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>), thus have an advantage over the other methods.</p>
        <p>In addition to this, <sc>RANKCORR</sc> is the only method that shows high performance when selecting fewer than 100 markers in both the clustering and classification metrics. Most researchers will be looking for small numbers of markers for their data sets; thus <sc>RANKCORR</sc> stands out as a promising method on the <sc>ZEISEL</sc> data set. Note also that <sc>RANKCORR</sc> generally outperforms <sc>SPA</sc> in the clustering metrics and is competitive with <sc>SPA</sc> in the classification metrics: <sc>RANKCORR</sc> is both faster than <sc>SPA</sc> and selects a generally more informative set of markers than <sc>SPA</sc> on the <sc>ZEISEL</sc> data set. Therefore, the performance on the <sc>ZEISEL</sc> data set is evidence for the fact that <sc>RANKCORR</sc> is a useful adaptation of <sc>SPA</sc> [<xref ref-type="bibr" rid="CR14">14</xref>] for sparse UMI counts scRNA-seq data.</p>
        <p>Finally, these data illustrate how the different evaluation metrics provide different statistical snapshots into the information contained in a set of markers. For example, logistic regression performs significantly worse than all of the other methods when large numbers of markers are selected according to the ARI and FMS plots. In the supervised classification trials, however, the logistic regression method performs competitively with the other methods. When no information about the ground truth clustering is provided, the performance of logistic regression on the <sc>ZEISEL</sc> data sets drops considerably. Despite the good results in the supervised clustering plots (that could be due to quickly selecting a small number of useful markers) it is reasonable to conclude that logistic regression selects many uninformative genes (in comparison to the other methods) as more markers are selected. It is best to think of the metrics as tests that can identify the marker selection methods that don’t perform well.</p>
      </sec>
      <sec id="Sec12">
        <title>Marker selection algorithms struggle with the cell types defined along the cell differentiation trajectory in the <sc>PAUL</sc> data set</title>
        <p>The <sc>PAUL</sc> data set consists of bone marrow cells and contains 19 clusters [<xref ref-type="bibr" rid="CR25">25</xref>]. The clusters lie along a cell differentiation trajectory; therefore, it is reasonable that it would be difficult to separate the clusters or to accurately reproduce the clustering into discrete cell types. The Paul data set thus represents an adversarial example for these marker selection algorithms.</p>
        <p>Figure <xref rid="Fig6" ref-type="fig">6</xref> shows the performance of marker selection algorithms on the <sc>PAUL</sc> data set as evaluated by the supervised classification metrics. It is not surprising to see relatively high clustering error rates: the rates are always larger than 30% for the NCC and reach a minimum of around 27% with the RFC. Since there are 19 clusters, this is still much better then classifying the cells at random. The dependence of the ARI, AMI, and FM clustering scores on the number of markers selected is plotted for the different marker selection algorithms in Fig. <xref rid="Fig7" ref-type="fig">7</xref>. The values of the scores are all in low to medium ranges for all marker selection algorithms.</p>
        <p>All of the scores produced by all of the methods on the <sc>PAUL</sc> data set are significantly worse than the metrics on the <sc>ZEISEL</sc> data set; however, the methods perform considerably better than markers selected uniformly at random (see Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figures 5–7 for this comparison). This is sensible, since it should intuitively be difficult to reproduce a discrete clustering that has been assigned along a continuous path. The notion of discrete cell types does not fit well with a cell differentiation trajectory; the poor score levels reflect the necessity to come up with a better mathematical description of a trajectory for the purposes of marker selection.</p>
        <p>The ARI values are especially low on the <sc>PAUL</sc> data set, and the methods consistently produce lower ARI values than AMI values. This is a change from the <sc>ZEISEL</sc> data set, where the ARI scores were higher than the AMI scores (and the FMSs were the highest of all). The aspects of the data sets that change the relative ordering of the metrics are unclear; it must be the data sets that influence this change, however, since the change persists across the marker selection algorithms. Designing a metric for benchmarking marker selection algorithms is itself a difficult task, and the optimal metric to consider could depend on the data set in question.</p>
        <p>A summary of the relative performance of the marker selection algorithms on the Paul data set is presented in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b. All of the marker selection methods perform quite similarly on the <sc>PAUL</sc> data set. The <sc>RANKCORR</sc> algorithm is one of only three methods that always performs nearly optimally under every metric examined here; the others are the t-test and MAST. In addition, <sc>RANKCORR</sc> always performs well when selecting small numbers of markers, and shows exceptional performance in this regime under the Fowlkes-Mallows clustering metric. Combined with the facts that <sc>RANKCORR</sc> is fast to run and requires low computational resources, this shows that <sc>RANKCORR</sc> is a useful marker selection method to add to computational pipelines.</p>
      </sec>
      <sec id="Sec13">
        <title>Results on the <sc>ZHENGFULL</sc> and <sc>ZHENGFILT</sc> data sets</title>
        <p>Here, we examine the data set consisting of 68k peripheral blood mononuclear cells (PBMCs) from [<xref ref-type="bibr" rid="CR2">2</xref>]; it contains data from more than 30 times the number of cells in either the <sc>PAUL</sc> or <sc>ZEISEL</sc> data sets. This is more representative of the sizes of the data sets that we are interested in working with. The ground truth clustering that we consider is the labeling obtained in [<xref ref-type="bibr" rid="CR2">2</xref>] by correlation with bulk profiles (biologically motivated “bulk labels”). There are 11 cell types in this clustering. See the section describing the data sets (in the <xref rid="Sec23" ref-type="sec">Methods</xref>) for more information.</p>
        <p>The <sc>ZHENG</sc> data sets contain some distinct clusters (e.g. B cells), as well as some clusters that are highly overlapping (e.g. different types of T cells). There are no specific cell differentiation trajectories (that we are aware of), but the overlapping clusters provide a challenge for the marker selection methods. Thus, we expect to see performance benchmarks between those of <sc>PAUL</sc> and <sc>ZEISEL</sc>.</p>
        <p>We mostly focus on <sc>ZHENGFILT</sc>, a version of the data set that is filtered to only include the information from the top 5000 most variable genes. We also consider the performance of the most efficient algorithms (<sc>RANKCORR</sc>, logistic regression, Wilcoxon, and the t-test) on <sc>ZHENGFULL</sc>, the data set containing all of the genes, to check for any differences. Extrapolating from Fig. <xref rid="Fig3" ref-type="fig">3</xref>, it would be infeasible to run the other methods on <sc>ZHENGFULL</sc>. Here we also begin to see that logistic regression scales worse than the other methods: it is already becoming slow and computationally heavy on “only” 68 thousand cells.</p>
        <p>Figure <xref rid="Fig8" ref-type="fig">8</xref> focuses on the performance of the methods when the NCC is used for classification; corresponding data using the RFC is found in Fig. <xref rid="Fig9" ref-type="fig">9</xref>. Unlike the <sc>PAUL</sc> and <sc>ZEISEL</sc> data sets, the precision curves are slightly different in some occasions, and thus they are presented here. In particular, the precision of these methods is significantly higher than their accuracy. Neither the classification accuracy nor the precision changes by very much when we filter from the full gene set (Figs. <xref rid="Fig8" ref-type="fig">8</xref>c,d and <xref rid="Fig9" ref-type="fig">9</xref>c,d) to the 5000 most variable genes (Figs. <xref rid="Fig8" ref-type="fig">8</xref>a,b and <xref rid="Fig9" ref-type="fig">9</xref>a,b). In general, this filtering very slightly increases both the accuracy and precision of the t-test, Wilcoxon, and <sc>RANKCORR</sc> methods, while it worsens the performance of the logistic regression method. This suggests that enough marker genes are kept by this variable gene filtering process to maintain accurate marker selection.</p>
        <p>Overall, the classification error rates according to the NCC for these data are quite high, and don’t level off (to a minimum value of approximately 40%) until around 200 markers are selected; this corresponds to an average of around 18 unique markers per cluster. For very small numbers of markers selected, the classification error rates obtained from the NCC are quite high (around 55%).</p>
        <p>The error rates using the RFC are decreased significantly compared to the NCC, and level off to approximately 22% when large numbers of markers are selected. The error again does not completely level off until around 200 total markers are selected, but there is a steeper initial descent. This steep initial descent in error rates could appear as the large groups of cells are separated from each other (e.g. B cells from T cells) and the slower improvement from 100 to 200 of total markers selected could be the methods fine-tuning the more difficult clusters (e.g. Regulatory T from Helper T). The error rates are between those observed in <sc>PAUL</sc> and <sc>ZEISEL</sc>. On the other hand, the error rates for the NCC classifier are much higher than expected.</p>
        <p>We focus on the <sc>ZHENGFILT</sc> data set for the clustering metrics. This is due to the fact that the classification metrics are changed only slightly between <sc>ZHENGFILT</sc> and <sc>ZHENGFULL</sc> as well as the fact that Louvain clustering on the large <sc>ZHENG</sc> data set is itself time and resource intensive. The clustering metrics on the <sc>ZHENGFILT</sc> data set are presented in Fig. <xref rid="Fig10" ref-type="fig">10</xref>. All three scores are generally quite low, though they are again mostly much higher than random marker selection. The performance of random marker selection can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 12.</p>
        <p>A summary of the performance of the marker selection algorithms on the <sc>ZHENGFILT</sc> data set is presented in Fig. <xref rid="Fig2" ref-type="fig">2</xref>c. Apart from the t-test, the methods show inconsistent performance when comparing the clustering metrics to the classification methods. For example, the edgeR method exhibits the top performance on the <sc>ZHENGFILT</sc> data set after more than 50-100 unique markers are selected according to the classification metrics. The classification metrics show edgeR as one of the worst methods when choosing less than 50 unique markers, however. This is in direct contradiction to the clustering metrics, where edgeR is always the best method for the smallest (∼20) total numbers of markers selected, and it then shows performance in the middle of the other methods as larger numbers of markers are selected.</p>
        <p>It is possible that changing the number of nearest neighbours considered in the Louvain clustering would produce more consistent data. Although the clustering metrics did not appear to change significantly when altering the number of nearest neighbours on the previous data sets (see the Louvain parameter selection information in the <xref rid="Sec23" ref-type="sec">Methods</xref>), the <sc>ZHENGFILT</sc> data set is much larger than those previous data sets; the larger number of cells may necessitate the use of information from more nearest neighbors to recreate the full clustering structure.</p>
        <p>It is also possible that the bulk labels that are used for the ground truth are difficult to reproduce through the Louvain algorithm. We generated a clustering that visually looked like the bulk labels via the Louvain algorithm (it appears in Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 18); the ARI, AMI, and FMS values for the generated Louvain clustering compared to the bulk labels are in the ranges produced by the Wilcoxon and t-test methods (not larger than the scores here). In addition, the top ARI and AMI scores (produced by the Wilcoxon and the t-test methods) are comparable to (or only slightly better than) the scores on the <sc>PAUL</sc> data set (Fig. <xref rid="Fig7" ref-type="fig">7</xref>). This runs counter to our expectations: the <sc>PAUL</sc> data set contains a cell differentiation trajectory, with no real clusters that are easy to separate out, while the <sc>ZHENG</sc> data sets contain several clusters that are well separated. It is possible that the bulk labels produce clusters that are more mixed than it appears in a UMAP plot.</p>
        <p>In any case, the disparity between the different types of scores further emphasizes the fact that the classification and clustering metrics provide different ways of looking at the information contained in a selected set of markers. Methods that perform well according to both types of metrics should be preferred.</p>
        <p>Following this logic, the t-test produces the overall best results on the <sc>ZHENGFILT</sc> data set. It performs well under the classification metrics, especially for small numbers of total markers selected. In addition, it is consistently competitive with the best method (Wilcoxon) according to the clustering metrics.</p>
        <p>Nonetheless, on the whole, <sc>RANKCORR</sc> performs approximately as well as the t-test, especially when selecting smaller numbers of markers. In particular, <sc>RANKCORR</sc> shows nearly optimal performance on the <sc>ZHENGFILT</sc> data set under the classification metrics. It performs poorly according to the clustering metrics when selecting more than 120 total markers, however, though it is still competitive with logistic regression in this domain. Still, the good performance when selecting less than 120 markers supports the notion that <sc>RANKCORR</sc> is a useful analytical resource for researchers to consider.</p>
      </sec>
      <sec id="Sec14">
        <title>Marker selection on the 1 million cell <sc>10XMOUSE</sc> data set</title>
        <p>We consider the <sc>10XMOUSE</sc> data set: it consists of 1.3 million mouse neurons generated using 10x protocols [<xref ref-type="bibr" rid="CR3">3</xref>]. The “ground truth” clustering that we examine in this case was algorithmically generated without any biological verification or interpretation (see the section about data sets in the <xref rid="Sec23" ref-type="sec">Methods</xref>). We include this data set as a stress test for the methods and therefore we do not perform any variable gene selection before running the marker selection algorithms (to keep the data set as large as possible). We also only consider the four fastest and lightest methods (<sc>RANKCORR</sc>, the t-test, Wilcoxon, and logistic regression) as these are the only methods considered in this work that could possibly produce results in a reasonable amount of time on this data set.</p>
        <p>The classification error rates of the four methods according to the NCC classifier show behavior similar to the other data sets that we have examined in this manuscript: starting out relatively high when selecting a small number of markers, then rapidly decreasing for a short period until becoming nearly constant as a larger number of markers are selected. See Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 13 for the NCC curves.</p>
        <p>The error rates of the methods approach approximately 25% as the number of markers selected increases. There are 39 clusters in the “ground truth” clustering that we examine here - thus, the error rates produced by all of the methods are much lower than the error rate expected from random classification. In Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 13, we see that the logistic regression method performs the best overall, and that <sc>RANKCORR</sc> consistently shows the highest error rate. The largest difference between the <sc>RANKCORR</sc> curve and the logistic regression curve is only around 3%, however. In addition, as mentioned above, logistic regression is the slowest method by far on this data set - extra accuracy is not worth much if the method is not able to finish running.</p>
        <p>Because a biologically motivated or interpreted clustering may be quite different from the clustering used here and because the classification error rate does not capture the full information in a set of markers (and thus similar error rates are not necessarily an accurate indication of the relative performance of methods), it is only possible to conclude that all four methods examined here show similar performance on the <sc>10XMOUSE</sc> dataset. The <sc>RANKCORR</sc> method produces useful markers, runs in a competitive amount of time, and takes a step towards selecting a smart set of markers for each cluster (rather than the same number of markers per cluster). It is impressive that these methods are able to run on such a massive data set.</p>
        <p>The implementation of the RFC in scikit-learn was quite slow on the large <sc>10XMOUSE</sc> data set, and thus we do not compare the methods via the RFC. From the smaller data sets, we might expect that the random forest classifier produces curves that are shaped similarly to the ones in Figure 13 in Additional file <xref rid="MOESM1" ref-type="media">1</xref> but are shifted down to a lower error rate. This is indeed what we see for the <sc>RANKCORR</sc> method: a comparison of the RFC and the NCC is shown in Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 14. Each point on this RFC curve took over 3 hours on 10 processors to generate; the largest point took over 15 hours. The Louvain clustering method was also too slow to compute any clustering error rates for the markers selected here. This is a situation where the marker selection algorithms are faster than almost all of the evaluation metrics (emphasizing the continued need for good marker set evaluation metrics).</p>
      </sec>
    </sec>
    <sec id="Sec15">
      <title>Comparison of marker selection methods on synthetic data</title>
      <p>We have evaluated <sc>RANKCORR</sc> on synthetic data sets that are designed to look like experimental scRNA-seq data. In each synthetic data set that we consider, there is a known ground truth set of markers, and all genes that are not markers are statistically identical across the cell populations. Thus, we can present the actual precision of the marker selection methods as well as ROC curves. Precision is an especially important metric for marker selection - it is desirable for an algorithm to select genes that truly separate the two data sets (rather than genes that are statistically identical across the two populations). The values of precision, TPR, and FPR are computed without cross-validation, since the entire set (of genes) in each data set is test data - there is no training to be done. We additionally examine the classification error metric that was introduced in Table <xref rid="Tab3" ref-type="table">3</xref>. We still use 5-fold cross-validation to compute this metric.</p>
      <p>Since the speed of a marker selection algorithm has been observed as an important factor for use on experimental data, we compare <sc>RANKCORR</sc> only to the fastest methods: the t-test, Wilcoxon, and logistic regression.</p>
      <p>See the synthetic data generation subsection in the <xref rid="Sec23" ref-type="sec">Methods</xref> for a full description of the data generation process. See also Fig. <xref rid="Fig11" ref-type="fig">11</xref> for an outline of the design. In short, we generate 20 different synthetic data sets; each simulated data set consists of 5000 cells that are split into two groups, and 10% of the genes are differentially expressed between these groups.
<fig id="Fig11"><label>Fig. 11</label><caption><p>Set up of the simulated data. We consider 3 conditions: all genes used for simulation, filtering after simulation, and filtering before simulation. On the left side of this diagramme, we produce 10 data sets by using all genes in simulation, and 10 more by filtering down to the 5000 most variable genes after simulation. These “filtering after simulation” data sets contain a subset of the information from the “all genes used for simulation” data sets. On the right hand side, we produce 10 data sets by filtering down to the 5000 most variable before simulation</p></caption><graphic xlink:href="12859_2020_3641_Fig11_HTML" id="MO11"/></fig></p>
      <p>For the purposes of computational efficiency, many data analysis pipelines reduce input data to a subset of the most variable genes before selecting markers. Thus, we examine synthetic data sets that are filtered down to the 5000 most variable genes in addition to unfiltered data sets. In 10 samples, we filter before simulating (and simulate 5000 genes); in the other 10 samples, we simulate without filtering (and simulate as many nonzero genes as there were in the input data, usually around 12000 genes). From each data set that was simulated without filtering, we produce another data set by filtering down to the 5000 most variable genes. This results in three simulation conditions (all genes used for simulation, filtering before simulation, and filtering after simulation) and a total of 30 data sets. See Fig. <xref rid="Fig11" ref-type="fig">11</xref>.</p>
      <p>Apart from the t-test data, each curve presented in this section represents the average across all 10 simulated data sets that are relevant to the curve. For the t-test, one of the trials in each simulation condition produced genes with tied <italic>p</italic>-values. This resulted in situations where it was impossible to select the top <italic>k</italic> genes in a stable manner; thus, these data sets were ignored and the t-test precision, TPR, and FPR curves each represent the average of the 9 data sets that are relevant to the curve.</p>
      <p>The differentially expressed genes are chosen randomly; thus many of them show low expression levels (often expressed in less than 10 cells) and are difficult to detect. In general, marker selection methods should not select genes with very low expression levels (since these genes are not particularly useful as markers when all cell types have large enough populations). Thus, we do not present information about the recall here.</p>
      <sec id="Sec16">
        <title>Simulated data illuminates the precise performance characteristics of marker selection methods</title>
        <p>In Fig. <xref rid="Fig12" ref-type="fig">12</xref>, we examine the precision of the marker selection algorithms for the first 400 unique genes selected. It is promising to see that <sc>RANKCORR</sc> produces the highest precision in marker selection across all of the simulation methods. The t-test is second, the Wilcoxon method is third, and logistic regression consistently exhibits the lowest precision.
<fig id="Fig12"><label>Fig. 12</label><caption><p>Precision of the marker selection methods versus the number of markers selected for the first 400 markers selected. Each sub-figure corresponds to a simulation method and the four lines correspond to the different marker selection algorithms. The <sc>RANKCORR</sc> method consistently shows the highest precision across all three simulation methods</p></caption><graphic xlink:href="12859_2020_3641_Fig12_HTML" id="MO12"/></fig></p>
        <p>Examining Fig. <xref rid="Fig12" ref-type="fig">12</xref> more closely, we see that the methods generally start off with high precision that decreases as more markers are selected (each data set contains more than 400 differentially expressed genes). In both of the filtered simulation conditions, all of the methods get close to a precision of 0.1 or 0.2 when 400 markers are selected, and all of the curves are still decreasing at this point (a precision of 0.1 corresponds to random gene selection on these data sets). There are around 2000 differentially expressed genes in the un-filtered simulation condition, so the fact that the precision drops significantly when selecting up to 400 markers indicates proportionally similar behavior to the filtered data sets.</p>
        <p>The ROC curves in Fig. <xref rid="Fig13" ref-type="fig">13</xref> also reflect this behavior: the curves increase (above the diagonal) quite rapidly for a short period of time, but then remain close to the diagonal overall.
<fig id="Fig13"><label>Fig. 13</label><caption><p>ROC curves. Each sub-figure corresponds to a simulation method and the four lines correspond to the different marker selection algorithms. The solid (purple) line is the diagonal TPR = FPR</p></caption><graphic xlink:href="12859_2020_3641_Fig13_HTML" id="MO13"/></fig></p>
        <p>These data are somewhat expected: in the simulations that come from all of the genes, many of the “differentially expressed” genes show low levels of expression. Thus, we would expect that the ROC curves should end up close to the diagonal as intermediate to large numbers of total markers are selected (since finding these low expression markers should be close to random selection). The filtered data sets could have solved this problem; however, the filtering method used here (see the full synthetic data description in the <xref rid="Sec23" ref-type="sec">Methods</xref>) preserves the relative proportions of low- and high-expression genes and (possibly for this reason) do not affect the ROC curves very much.</p>
        <p>Another explanation for these difficulties could be the differential expression parameters used in the Splat simulation. With these default parameters, the gene mean for some of the “differentially expressed” genes are only slightly different between the two clusters (for specifics, see the synthetic data discussion in the <xref rid="Sec23" ref-type="sec">Methods</xref>). Thus, although the simulation may label these genes as differentially expressed, detecting the differential expression by any method will be very difficult. This underscores the differences between biological markers and differentially expressed genes: these differentially expressed genes would not be good practical biological markers, as it would be very difficult to tell two clusters apart based on the expression levels of these types of genes without collecting a lot of data.</p>
        <p>Regardless, both of these plots support the notion that the methods are able to easily identify a small set of differentially expressed genes from the synthetic data but then rapidly start to have difficulties as more genes are selected. In addition, the <sc>RANKCORR</sc> method consistently shows the highest value of precision and TPR.</p>
      </sec>
      <sec id="Sec17">
        <title>Inconsistent results are obtained when these simulated genes are filtered by dispersion</title>
        <p>Comparing Fig. <xref rid="Fig12" ref-type="fig">12</xref>a-c across the simulation conditions, we see that the highest precision for each of the marker selection methods is obtained by using all genes for simulation, without any filtering. It is tough to explain why filtering genes by dispersion (the filtering method that we use here; see the synthetic data discussion information in the <xref rid="Sec23" ref-type="sec">Methods</xref>) after simulating produces lower precision scores than not filtering. Since the t-test (for example) works by choosing genes based on a <italic>p</italic>-value score, and the genetic information is not changed by the filtering process (<italic>p</italic>-values would be the same in both the unfiltered and filtered after simulation data sets), it must be the case that many of the differentially expressed genes are removed from the data set when we filter after simulation. The highly variable genes selected by the filtering method used here are not required to have high expression; thus, there is no obvious reason that many differentially expressed genes should be filtered out.</p>
        <p>Note that a similar effect is not observed in the <sc>ZHENG</sc> data sets (see Figs. <xref rid="Fig8" ref-type="fig">8</xref> and <xref rid="Fig9" ref-type="fig">9</xref>), suggesting that this inconsistency is an artifact of the simulation methods used here. Simulating scRNA-seq data is itself a difficult task; see also [<xref ref-type="bibr" rid="CR32">32</xref>] for a further discussion of the difficulties involved in simulating scRNA-seq data (and a tool that can help to expose these types of issues). Nonetheless, filtering genes is quite a heuristic process, and there is still more work to be done in fully understanding how this filtering impacts real scRNA-seq data<xref ref-type="fn" rid="Fn3">3</xref>. At the very least, it is clear that the process of filtering genes by dispersion does not commute with the simulation methods used here, since filtering before simulation shows higher precision than filtering after simulation.</p>
      </sec>
      <sec id="Sec18">
        <title>The classification error rate is an informative but coarse metric</title>
        <p>Finally, we examine the classification error rate of the methods applied to the synthetic data in Fig. <xref rid="Fig14" ref-type="fig">14</xref>. It is interesting to note that, with only two clusters, we still misclassify a minimum of around 10% of cells. This suggests that the simulated data are not well separated - the differential expression introduced in the synthetic data is not strong enough to easily separate the two clusters. Moreover, apart from the curves corresponding to the logistic regression method, all of the curves look to be fairly constant after a small number of markers have been selected (approximately 50 for the simulations based on all genes and approximately 30 for the simulations based on filtered data). This further supports the discussion from above - the methods start by quickly choosing a small number of good markers; after this, the genes that are selected do not provide significantly more information about the clustering.
<fig id="Fig14"><label>Fig. 14</label><caption><p>Clustering error rates using the Random Forest classifier for the first 500 markers chosen by each method. The sub-figures correspond to different simulation conditions. The <sc>RANKCORR</sc> algorithm consistently produces the smallest values of the clustering error rate</p></caption><graphic xlink:href="12859_2020_3641_Fig14_HTML" id="MO14"/></fig></p>
        <p>Note that the methods that show higher precision in Fig. <xref rid="Fig12" ref-type="fig">12</xref> also show a lower classification error in Fig. <xref rid="Fig14" ref-type="fig">14</xref>. On the other hand, logistic regression shows poor precision levels on the filtered data sets and also appears significantly worse than the other methods in the classification error rate curves. Thus, according to these experiments, the classification error rate seems to be a coarse but reasonable measure of how well a set of markers describes the data set. In this example, if one methods performs worse than another method according to the classification error rate curves (Fig. <xref rid="Fig14" ref-type="fig">14</xref>), then the same relationship holds in the precision curves (Fig. <xref rid="Fig12" ref-type="fig">12</xref>). Some large differences in precision are eliminated in the classification error rate curves, however, and thus the classification error rate should be considered with a grain of salt. That is, the classification error rate is informative, but it does not provide a full statistical picture of how the methods are actually performing.</p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec19" sec-type="discussion">
    <title>Discussion</title>
    <sec id="Sec20">
      <title>The difficulties of benchmarking and the importance of simulated data</title>
      <p>Benchmarking marker selection algorithms on scRNA-seq data is inherently a difficult task. The lack of a ground truth set of markers requires for us to devise performance evaluation metrics that will illuminate the information contained in a selected set of genes. We have examined several natural evaluation metrics in this work; these metrics sometimes produce conflicting results, however. Our experiments make it clear that these metrics provide different ways to view the information contained in a set of genes rather than capturing the full picture provided by of a set of markers.</p>
      <p>Having a ground truth set of markers available makes the evaluation of marker selection algorithms much more explicit. In the synthetic data here, for example, it becomes apparent that the methods rapidly select a set of markers that provide a lot of information about the clustering, then essentially start picking things by chance. This type of behavior can only be revealed by a study with a known ground truth.</p>
      <p>On the other hand, simulating scRNA-seq data is itself a difficult problem. The simulated data that we consider in this work behaves strangely when we filter it by selecting highly variable genes. In particular, the filtering process considered here seems to remove many of the useful differentially expressed genes in the simulated data. This type of behavior was not observed in the <sc>ZHENGFILT</sc> experimental data set, where working only with high variance genes had little impact on the marker set evaluation metrics. Better simulation methods, and mathematical results formalizing the quality of simulated data, are extremely important future projects. See [<xref ref-type="bibr" rid="CR32">32</xref>, <xref ref-type="bibr" rid="CR33">33</xref>] for some work towards these goals.</p>
    </sec>
    <sec id="Sec21">
      <title>The relationship between marker selection and the process of defining cell types</title>
      <p>The marker selection framework considered in this work is quite narrow. It is focused on discrete cell types, and (as shown in the <sc>PAUL</sc> data set) does not handle cell differentiation trajectory patterns very well. Moreover, we assume that the genetic information that we supply to a marker selection algorithm consists of cells that are already partitioned into cell types. This is consistent with the data processing pipeline that many researchers currently follow (cluster the scRNA-seq data with an algorithm, then find markers for the clusters that are produced [<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR19">19</xref>]); it seems more reasonable to allow for marker selection to help guide the process of finding and defining cell types, however.</p>
      <p>For example, future marker selection methods could find markers that are useful for identifying certain regions of the transcriptome space (in an unsupervised or semi-supervised manner). This would allow for clarity along a cell differentiation pathway - at any point on the trajectory, a researcher could view the markers that identify the nearby area, and to what degree each marker identifies the area. Thus, cell types (or differentiation pathways) could be suggested based on marker genes. These cell types might themselves reveal more informative markers, creating an iterative process: let the markers guide the clustering and vice versa. Such a method is known as an <italic>embedded</italic> feature selection method in the computer science literature; adapting an embedded feature selection method to scRNA-seq data is left for future consideration.</p>
    </sec>
  </sec>
  <sec id="Sec22" sec-type="conclusion">
    <title>Conclusions</title>
    <p>Across a wide variety of data sets (large and small; data sets containing cell differentiation trajectories; datasets with well separated clusters; biologically defined clusters; algorithmically defined clusters) and looking at many different performance metrics, it is impossible (and even inappropriate) to say that any of the methods tested selects better markers than all of the others. Indeed, the marker selection method that was “best” depended on the data set as well as the evaluation metric in question, and the difference in performance between the “best” marker selection algorithm and the “worst” was often quite small.</p>
    <p>Thus, the major factors that differentiate the methods examined in this work are the computational resources (both physical and temporal) that the methods require. Since the algorithms show similar overall quality, researchers should prefer marker selection methods that are fast and light.</p>
    <p>In addition to this, as technology advances, the trend is towards the generation of larger and larger data sets. High throughput sequencing protocols are becoming more efficient and cheaper, and other statistical and computational methods are improved when many samples are collected. Through imputation and smoothing methods (see e.g. [<xref ref-type="bibr" rid="CR34">34</xref>–<xref ref-type="bibr" rid="CR36">36</xref>]), a detailed description of the transcriptome space can be revealed even when low numbers of reads are collected in individual cells. Thus, the speed of a marker selection algorithm will only become more important.</p>
    <p>The <sc>RANKCORR</sc>, Wilcoxon, t-test, and logistic regression methods run the fastest of all of the methods considered in this work. They run considerably faster and/or lighter than any of the complex statistical methods that have been designed specifically for scRNA-seq data. Logistic regression does not scale particularly well with the data set size, however, and it requires an amount of resources that is not competitive with the other three methods on the largest data sets. Moreover, logistic regression exhibits poor performance on several of the data sets considered in this work, especially when selecting small numbers of markers. Thus, as a general guideline, <sc>RANKCORR</sc>, Wilcoxon, and the t-test are the optimal marker selection algorithms examined in this work for the analysis of large, sparse UMI counts data. This recommendation is further bolstered by the fact that these three algorithms tend to perform well in the experiments that we have considered here, especially when selecting lower numbers of markers.</p>
    <p>The <sc>RANKCORR</sc> algorithm, introduced in this work, is the slowest of the three recommended algorithms. Nonetheless, <sc>RANKCORR</sc> outperformed the other fast algorithms in our synthetic tests. In addition, it provides some interpretability in the multi-class marker selection scenario. Specifically, <sc>RANKCORR</sc> attempts to select an informative number of markers for each cluster (rather than just a fixed number for each cluster), generally selecting more markers for clusters that we are less certain about. The work of properly selecting sets of markers in a multi-class scenario has not been completed, however, and <sc>RANKCORR</sc> only proposes one step. Overall, as a fast and efficient marker selection algorithm, <sc>RANKCORR</sc> is a valuable addition to the set of scRNA-seq analysis tools.</p>
    <p><sc>RANKCORR</sc> also involves taking a rank transform of scRNA-seq counts data. The rank transformation has other uses in scRNA-seq; it is thus useful to understand the further properties of the rank transformation. These properties will be explored in upcoming work.</p>
    <p>Finally, in the way that data processing pipelines are currently set up, researchers will often be forced to select markers without the knowledge of a ground truth set of markers. Thus, it may be valuable to consider metrics such as the ones discussed in this work when performing marker selection. Combining the values of several of the metrics may help to aid researchers in deciding when they have selected enough markers to adequately describe their cell types (so that they are not considering genes that were chosen at random), for example. The question of how to stop selecting markers is another important consideration for future work.</p>
  </sec>
  <sec id="Sec23">
    <title>Methods</title>
    <sec id="Sec24">
      <title>Details of the <sc>RANKCORR</sc> algorithm</title>
      <p>Given a vector <italic>x</italic>∈<bold><italic>R</italic></bold><sup><italic>p</italic></sup> and a parameter <italic>β</italic>∈<bold><italic>R</italic></bold>, we define the soft-thresholding operator <italic>T</italic><sub><italic>β</italic></sub>(<italic>x</italic>):<bold><italic>R</italic></bold><sup><italic>p</italic></sup>→<bold><italic>R</italic></bold><sup><italic>p</italic></sup> by
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ T_{\beta}(x)_{j} = \left\{\begin{array}{lll} \text{sign}\left(x_{j}\right)\left|x_{j} - \beta \right| &amp;\colon &amp; \left|x_{j}\right| &gt; \beta\\ 0 &amp; \colon &amp; \text{otherwise} \end{array}\right.  $$ \end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable class="array" columnalign="left"><mml:mtr><mml:mtd><mml:mtext>sign</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfenced></mml:mtd><mml:mtd><mml:mo>:</mml:mo></mml:mtd><mml:mtd><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>&gt;</mml:mo><mml:mi>β</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>:</mml:mo></mml:mtd><mml:mtd><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>We say that <italic>T</italic><sub><italic>β</italic></sub>(<italic>x</italic>) is a soft-thresholding of the vector <italic>x</italic>.</p>
      <sec id="Sec25">
        <title>Setup</title>
        <p>Recall the notation from the <xref rid="Sec5" ref-type="sec">Results</xref>: let <italic>X</italic>∈<bold><italic>R</italic></bold><sup><italic>n</italic>×<italic>p</italic></sup> be a scRNA-seq count matrix (<italic>n</italic> cells, <italic>p</italic> genes). Label the cells with the numbers in [ <italic>n</italic>]. Given a subset <italic>S</italic>⊂[ <italic>n</italic>] of cells, define <italic>τ</italic>∈{±1}<sup><italic>n</italic></sup> such that <italic>τ</italic><sub><italic>i</italic></sub>=+1 if cell <italic>i</italic> is in the subset (that is, if <italic>i</italic> is in <italic>S</italic>) and <italic>τ</italic><sub><italic>i</italic></sub>=−1 otherwise. We refer to <italic>τ</italic> as the <italic>cluster indicator vector</italic> for the set <italic>S</italic>.</p>
        <p>To find markers for <italic>S</italic>, we desire a vector <italic>ω</italic>∈<bold><italic>R</italic></bold><sup><italic>p</italic></sup> such that
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \tau = \text{sign}\left(\overline{X} \omega\right)  $$ \end{document}</tex-math><mml:math id="M36"><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mtext>sign</mml:mtext><mml:mfenced close=")" open="("><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover><mml:mi>ω</mml:mi></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <inline-formula id="IEq14"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M38"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq14.gif"/></alternatives></inline-formula> denotes a transformed version of <italic>X</italic> (we use the specific transformation (<xref rid="Equ2" ref-type="">2</xref>) for <sc>RANKCORR</sc>). Note that, if cell <italic>i</italic> is in <italic>S</italic>, then <inline-formula id="IEq15"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left \langle \overline {x}_{i}, \omega \right \rangle &gt;0$\end{document}</tex-math><mml:math id="M40"><mml:mfenced close="〉" open="〈"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:mfenced><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq15.gif"/></alternatives></inline-formula>; otherwise, <inline-formula id="IEq16"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\langle \overline {x}_{i}, \omega \rangle &lt; 0$\end{document}</tex-math><mml:math id="M42"><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo>〉</mml:mo><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq16.gif"/></alternatives></inline-formula>. Thus, <italic>ω</italic> is the normal vector to a hyperplane passing through the origin that separates the cells that are in <italic>S</italic> from all other cells. In this framework, the nonzero entries of <italic>ω</italic> are marker genes for the subset <italic>S</italic> - they are the features that separate the given cell type from the other cells. To obtain a small number of markers, we desire a sparse solution <italic>ω</italic>; that is, a solution <italic>ω</italic> with few nonzero entries.</p>
        <p>Unfortunately, it is computationally infeasible to find the sparsest vector <italic>ω</italic><sup>∗</sup> that satisfies (<xref rid="Equ5" ref-type="">5</xref>) (see [<xref ref-type="bibr" rid="CR37">37</xref>]). In addition, for noisy experimental data, there is probably no vector <italic>ω</italic> that will perfectly satisfy (<xref rid="Equ5" ref-type="">5</xref>).</p>
        <p>In [<xref ref-type="bibr" rid="CR15">15</xref>], the authors circumvent these issues by assuming that there is a vector <italic>ω</italic> (and a value <italic>t</italic> such that ∥<italic>ω</italic>∥<sub>0</sub>≤<italic>t</italic>) that <italic>mostly</italic> satisfies (<xref rid="Equ5" ref-type="">5</xref>) (i.e. the vector equality does not need to hold in all coordinates). They present the convex optimization (<xref rid="Equ3" ref-type="">3</xref>) that uses <inline-formula id="IEq17"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}, \tau $\end{document}</tex-math><mml:math id="M44"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq17.gif"/></alternatives></inline-formula> and an input sparsity parameter <italic>s</italic> to produce an approximate solution <inline-formula id="IEq18"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M46"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq18.gif"/></alternatives></inline-formula> that is “close” (in a technical sense) to this true sparse <italic>ω</italic>. We refer to <italic>s</italic> as a sparsity parameter due to the fact that it influences the number of zeros in the approximation <inline-formula id="IEq19"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M48"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq19.gif"/></alternatives></inline-formula>. Specifically, <italic>s</italic> controls the size of the set that the approximation <inline-formula id="IEq20"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M50"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq20.gif"/></alternatives></inline-formula> will be chosen from: when <italic>s</italic>≥<italic>t</italic> (so that the true signal <italic>ω</italic> has ∥<italic>ω</italic>∥<sub>0</sub>≤<italic>s</italic>), then <italic>ω</italic> will be in the feasible region of the optimization. For convenience, the optimization is reproduced below.
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} \hat{\omega} =\underset{\omega} {\arg\min} &amp; \sum\limits_{i=1}^{n} \tau_{i} \langle \overline{x}_{i}, \omega \rangle\\ \mathrm{subject\ to\ } &amp; \left\|\omega\right\|_{2} \leq 1, \left\|\omega\right\|_{1} \leq \sqrt{s} \end{aligned}  $$ \end{document}</tex-math><mml:math id="M52"><mml:mtable><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mo>arg</mml:mo><mml:mo>min</mml:mo></mml:mrow><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:munder></mml:mtd><mml:mtd><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>ω</mml:mi><mml:mo>〉</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle mathvariant="normal"><mml:mtext mathvariant="italic">subject</mml:mtext><mml:mspace width="1em"/><mml:mtext mathvariant="italic">to</mml:mtext><mml:mspace width="1em"/></mml:mstyle></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msqrt><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msqrt></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>In the optimization (<xref rid="Equ3" ref-type="">3</xref>), <inline-formula id="IEq21"><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {x}_{i}$\end{document}</tex-math><mml:math id="M54"><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq21.gif"/></alternatives></inline-formula> denotes the <italic>i</italic>-th row of <inline-formula id="IEq22"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M56"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq22.gif"/></alternatives></inline-formula>.</p>
        <p>A couple of technical points deserve mention here: first, there are other efficient methods for obtaining an approximate solution <inline-formula id="IEq23"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M58"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq23.gif"/></alternatives></inline-formula> to (<xref rid="Equ5" ref-type="">5</xref>). As mentioned in the overview of <sc>RANKCORR</sc>, however, the optimization (<xref rid="Equ3" ref-type="">3</xref>) has previously [<xref ref-type="bibr" rid="CR14">14</xref>] been developed into <sc>SPA</sc>, a feature selection algorithm for use with sparse biological data (specifically, mass spectrometry data in proteomics). We thus focus on the optimization (<xref rid="Equ3" ref-type="">3</xref>) to solve (<xref rid="Equ5" ref-type="">5</xref>) for this work.</p>
        <p>Moreover, there are algorithms for finding general sparse separating hyperplanes (e.g. sparse support vector machines, see Section 4.5 of [<xref ref-type="bibr" rid="CR8">8</xref>]). These methods don’t assume that the hyperplane passes through the origin, but they are somewhat slow, and it would not be reasonable to use them with the massive scRNA-seq data sets that are considered in this paper. Thus, to develop a fast marker selection method, we keep the additional assumption that the separating hyperplanes pass through the origin.</p>
        <p>Finally, the fact that we are searching for a hyperplane passing through the origin necessitates a good choice of the transformation that yields <inline-formula id="IEq24"><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M60"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq24.gif"/></alternatives></inline-formula> from <italic>X</italic>. For example, if <italic>X</italic> is left unchanged, then all of the cells lie in the first orthant of <bold><italic>R</italic></bold><sup><italic>p</italic></sup>. In this case there is almost certainly no <italic>ω</italic> that satisfies (<xref rid="Equ5" ref-type="">5</xref>); many hyperplanes, for example, do not even pass through the first orthant.</p>
        <p>In <sc>SPA</sc>, the authors of [<xref ref-type="bibr" rid="CR14">14</xref>] construct the input <inline-formula id="IEq25"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M62"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq25.gif"/></alternatives></inline-formula> by “quasi-standardizing” the columns of the data matrix <italic>X</italic>: the column <inline-formula id="IEq26"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}_{j}$\end{document}</tex-math><mml:math id="M64"><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq26.gif"/></alternatives></inline-formula> is a linear combination of the centered version of <italic>X</italic><sub><italic>j</italic></sub> and the standardized version of <italic>X</italic><sub><italic>j</italic></sub>; that is,
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \overline{X}_{j} = \alpha^{2\left(1 - \left|\rho_{j}\right|\right)}\cdot\lambda \cdot \left(X_{j} - \mu\left(X_{j}\right)\right) + \left(1-\alpha^{2\left(1-|\rho_{j}|\right)}\right) \cdot\left(\frac{X_{j} - \mu\left(X_{j}\right)}{\sigma\left(X_{j}\right)}\right)  $$ \end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mrow><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfenced close="|" open="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>·</mml:mo><mml:mi>λ</mml:mi><mml:mo>·</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>·</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <italic>α</italic> and <italic>λ</italic> are hyperparameters and <italic>ρ</italic><sub><italic>j</italic></sub> is the empirical correlation of <italic>X</italic><sub><italic>j</italic></sub> with <italic>τ</italic>.</p>
        <p>The goal of the quasi-standardization in <sc>SPA</sc> is to provide more weight to the genes that are highly correlated with the labels <italic>τ</italic> (regardless of their expression levels) while also downweighting high expression genes that are not well correlated with the labels <italic>τ</italic> (that is, <italic>λ</italic> should be quite large). Similar behavior is accomplished in <sc>RANKCORR</sc> through the use of the rank transformation, see the intuitive description of the rank transformation in the <xref rid="Sec1" ref-type="sec">Background</xref> for more information. Thus, <sc>RANKCORR</sc> has the benefit that there are no hyperparameters to tune. In experiments, we see that <sc>RANKCORR</sc> runs much more quickly than the method <sc>SPA</sc> from [<xref ref-type="bibr" rid="CR14">14</xref>] and generally produces better results on scRNA-seq data.</p>
      </sec>
      <sec id="Sec26">
        <title>A fast algorithm for solving the optimization (3)</title>
        <p>Given a matrix <inline-formula id="IEq27"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M68"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq27.gif"/></alternatives></inline-formula> and a signal <italic>ω</italic> (with ∥<italic>ω</italic>∥<sub>0</sub>≤<italic>s</italic>), let <inline-formula id="IEq28"><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tau = \text {sign}(\overline {X}\omega)$\end{document}</tex-math><mml:math id="M70"><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mtext>sign</mml:mtext><mml:mo>(</mml:mo><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover><mml:mi>ω</mml:mi><mml:mo>)</mml:mo></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq28.gif"/></alternatives></inline-formula>. Recall that the optimization (<xref rid="Equ3" ref-type="">3</xref>) uses <inline-formula id="IEq29"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}, \tau $\end{document}</tex-math><mml:math id="M72"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>τ</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq29.gif"/></alternatives></inline-formula>, and <italic>s</italic> to provide an approximation <inline-formula id="IEq30"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M74"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq30.gif"/></alternatives></inline-formula> that is “close” to <italic>ω</italic>.</p>
        <p>In both [<xref ref-type="bibr" rid="CR38">38</xref>] and [<xref ref-type="bibr" rid="CR39">39</xref>], the authors show that the solution <inline-formula id="IEq31"><alternatives><tex-math id="M75">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M76"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq31.gif"/></alternatives></inline-formula> to (<xref rid="Equ3" ref-type="">3</xref>) is given by a normalized soft thresholding of the vector
<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M77">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ v = \sum\limits_{i=1}^{n} \tau_{i} x_{i},  $$ \end{document}</tex-math><mml:math id="M78"><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <italic>x</italic><sub><italic>i</italic></sub> represents the <italic>i</italic>-th row of <inline-formula id="IEq32"><alternatives><tex-math id="M79">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M80"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq32.gif"/></alternatives></inline-formula>. That is,
<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M81">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \hat{\omega} = T_{\beta}(v)/\left\|T_{\beta}(v)\right\|_{2},  $$ \end{document}</tex-math><mml:math id="M82"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>where <italic>β</italic> is a parameter that depends on <italic>s</italic> and the cluster indicator vector <italic>τ</italic> in a non-trivial manner. For feature selection, we are interested only in the support of <inline-formula id="IEq33"><alternatives><tex-math id="M83">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M84"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq33.gif"/></alternatives></inline-formula>. Thus, the optimization (<xref rid="Equ3" ref-type="">3</xref>) can be solved simply by soft-thresholding at each coordinate of <italic>v</italic>. Algorithm <sc>SELECT</sc> implements this idea to quickly find the support of <inline-formula id="IEq34"><alternatives><tex-math id="M85">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {\omega }$\end{document}</tex-math><mml:math id="M86"><mml:mover accent="true"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq34.gif"/></alternatives></inline-formula>. It is defined in Algorithm 1.</p>
        <p>
          <graphic position="anchor" xlink:href="12859_2020_3641_Figa_HTML" id="MO15"/>
        </p>
        <p>Intuitively, Algorithm 1 works in the following manner. For <italic>s</italic>&gt;1, note that the feasible set <inline-formula id="IEq35"><alternatives><tex-math id="M87">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left \{x \in \boldsymbol {\mathrm {R}}^{p} \colon \left \|x\right \|_{1} \leq \sqrt {s}, \left \|x\right \|_{2} \leq 1\right \}$\end{document}</tex-math><mml:math id="M88"><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="normal"><mml:mi>R</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msqrt><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msqrt><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq35.gif"/></alternatives></inline-formula> looks like the set <inline-formula id="IEq36"><alternatives><tex-math id="M89">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left \{x \in \boldsymbol {\mathrm {R}}^{p} \colon \left \|x\right \|_{1}\leq \sqrt {s}\right \}$\end{document}</tex-math><mml:math id="M90"><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi mathvariant="normal">R</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msqrt><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq36.gif"/></alternatives></inline-formula> with the corners chopped off and rounded. Thus, we start by creating <inline-formula id="IEq37"><alternatives><tex-math id="M91">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {v}$\end{document}</tex-math><mml:math id="M92"><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq37.gif"/></alternatives></inline-formula>, a normalized non-zero soft-thresholding of <italic>v</italic> that has as few nonzero entries as possible<xref ref-type="fn" rid="Fn4">4</xref>. We then soft-threshold <italic>v</italic> by smaller and smaller values so that <inline-formula id="IEq39"><alternatives><tex-math id="M93">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {v}$\end{document}</tex-math><mml:math id="M94"><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq39.gif"/></alternatives></inline-formula> gains more non-zero coordinates and thus points further away from a coordinate axis. Since <inline-formula id="IEq40"><alternatives><tex-math id="M95">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {v}$\end{document}</tex-math><mml:math id="M96"><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq40.gif"/></alternatives></inline-formula> is always normalized, it is on the 2-sphere {<italic>x</italic>:∥<italic>x</italic>∥=1}. Thus, we can stop when <inline-formula id="IEq41"><alternatives><tex-math id="M97">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {v}$\end{document}</tex-math><mml:math id="M98"><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq41.gif"/></alternatives></inline-formula> is also on the 1-sphere <inline-formula id="IEq42"><alternatives><tex-math id="M99">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\left \{x \colon \left \|x\right \|_{1} = \sqrt {s}\right \}$\end{document}</tex-math><mml:math id="M100"><mml:mfenced close="}" open="{"><mml:mrow><mml:mi>x</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq42.gif"/></alternatives></inline-formula>. Then, <inline-formula id="IEq43"><alternatives><tex-math id="M101">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {v}$\end{document}</tex-math><mml:math id="M102"><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq43.gif"/></alternatives></inline-formula> is a point where the 1-sphere intersects the 2-sphere, and the support of this <inline-formula id="IEq44"><alternatives><tex-math id="M103">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {v}$\end{document}</tex-math><mml:math id="M104"><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq44.gif"/></alternatives></inline-formula> are the features that we are interested in selecting.</p>
        <p>There is also a faster algorithm for solving a problem that is equivalent to (<xref rid="Equ3" ref-type="">3</xref>) presented in [<xref ref-type="bibr" rid="CR39">39</xref>]. This algorithm does not allow for an interesting generalization to the multi-class problem, however.</p>
        <p>We use <sc>SELECT</sc> in our implementations of both <sc>SPA</sc> and <sc>RANKCORR</sc>; this also means that our implementation of <sc>SPA</sc> is faster than it would have appeared in past work, including [<xref ref-type="bibr" rid="CR14">14</xref>] (where <sc>SPA</sc> is introduced).</p>
      </sec>
      <sec id="Sec27">
        <title>Applying <sc>SELECT</sc> to rank transformed data</title>
        <p>This section contains an algorithm <sc>RANKBIN</sc> (defined in Algorithm 2) that uses <sc>SELECT</sc> along with the rank transformation to select markers for a fixed cell type in a way that is motivated by <sc>SPA</sc>. The inputs are a UMI counts matrix <italic>X</italic>∈<bold><italic>R</italic></bold><sup><italic>n</italic>×<italic>p</italic></sup>, a vector <italic>τ</italic>∈{±1}<sup><italic>n</italic></sup>, and a sparsity parameter <italic>s</italic>. Note that this is still a binary marker selection method, since the entries of <italic>τ</italic> are either +1 or −1. The extension to the multi-class case is described in the next section.</p>
        <p>
          <graphic position="anchor" xlink:href="12859_2020_3641_Figb_HTML" id="MO16"/>
        </p>
        <p>In Algorithm 2, the construction of <inline-formula id="IEq45"><alternatives><tex-math id="M105">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M106"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq45.gif"/></alternatives></inline-formula> is motivated by the quasi-standardization (<xref rid="Equ7" ref-type="">7</xref>) of the data matrix <italic>X</italic> in <sc>SPA</sc>. In <sc>RANKBIN</sc>, the columns of the data matrix <italic>X</italic> are standardized <italic>after</italic> they are rank transformed. Moreover, motivated by the work in [<xref ref-type="bibr" rid="CR14">14</xref>] and [<xref ref-type="bibr" rid="CR38">38</xref>], the vector <italic>τ</italic> is replaced with <italic>Φ</italic>(<italic>τ</italic>)−<italic>μ</italic>(<italic>Φ</italic>(<italic>τ</italic>)) in <sc>RANKBIN</sc>. That is, the rank transformation is applied both to the data matrix <italic>X</italic> and the class indicator <italic>τ</italic>. In this case, the vector <italic>v</italic> that we soft threshold when we call <sc>SELECT</sc> (see (<xref rid="Equ8" ref-type="">8</xref>), above) has entries given by
<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ v_{j} = \sum\limits_{i=1}^{n} \left(\Phi(\tau)_{i} - \mu(\Phi(\tau))\right) \frac{\Phi\left(X_{j}\right)_{i} - \mu\left(\Phi\left(X_{j}\right)\right)}{\sigma\left(\Phi\left(X_{j}\right)\right)}.  $$ \end{document}</tex-math><mml:math id="M108"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mfenced close=")" open="("><mml:mrow><mml:mi>Φ</mml:mi><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo>(</mml:mo><mml:mi>Φ</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfenced><mml:mfrac><mml:mrow><mml:mi>Φ</mml:mi><mml:msub><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>Φ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mi>Φ</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mi>.</mml:mi></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula></p>
        <p>That is, entry <italic>j</italic> of <italic>v</italic> is (proportional to) the Spearman rank correlation between gene <italic>j</italic> and the vector <italic>τ</italic>. Thus, <sc>RANKBIN</sc> will select the genes that have the highest (absolute) Spearman correlation with the vector of class labels. It is possible to show that replacing <italic>τ</italic> with <italic>Φ</italic>(<italic>τ</italic>)−<italic>μ</italic>(<italic>Φ</italic>(<italic>τ</italic>)) has no effect on the markers that are selected by the algorithm; algorithm <sc>RANKBIN</sc> is written with <inline-formula id="IEq46"><alternatives><tex-math id="M109">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {\Phi (\tau)}$\end{document}</tex-math><mml:math id="M110"><mml:mover accent="false"><mml:mrow><mml:mi>Φ</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq46.gif"/></alternatives></inline-formula> instead of <italic>τ</italic> to emphasize the connection with the Spearman rank correlation. (Similar calculations show that the markers selected by <sc>SPA</sc> are generally those that have high correlation with the cell type labels.)</p>
        <p>Tangentially, note that the rows of the rank transformed and standardized data matrix <inline-formula id="IEq47"><alternatives><tex-math id="M111">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M112"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq47.gif"/></alternatives></inline-formula> in Algorithm 2 (the <sc>RANKBIN</sc> algorithm) come from a bounded - and thus sub-Gaussian - distribution with mean 0 and variance 1. Thus, this matrix <inline-formula id="IEq48"><alternatives><tex-math id="M113">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M114"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq48.gif"/></alternatives></inline-formula> matches many of the hypotheses of the theoretical guarantees about the solution to (<xref rid="Equ3" ref-type="">3</xref>) that are presented in [<xref ref-type="bibr" rid="CR40">40</xref>] (the rows of <inline-formula id="IEq49"><alternatives><tex-math id="M115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M116"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq49.gif"/></alternatives></inline-formula> are not independent, however).</p>
      </sec>
      <sec id="Sec28">
        <title><sc>RANKCORR</sc>: multi-class marker selection</title>
        <p><sc>RANKCORR</sc>, defined in Algorithm 3 works by fixing a parameter <italic>s</italic> and applying <sc>RANKBIN</sc> to each of the cell types in the data set. Specifically, fix a sparsity parameter <italic>s</italic>; this parameter will be the same for all of the cell types. For cell type <italic>j</italic>, construct the vector <italic>τ</italic><sup><italic>j</italic></sup> with <inline-formula id="IEq50"><alternatives><tex-math id="M117">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tau _{i}^{j} = 1$\end{document}</tex-math><mml:math id="M118"><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq50.gif"/></alternatives></inline-formula> if cell <italic>i</italic> is in cell type <italic>j</italic> and <inline-formula id="IEq51"><alternatives><tex-math id="M119">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tau _{i}^{j} = -1$\end{document}</tex-math><mml:math id="M120"><mml:msubsup><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq51.gif"/></alternatives></inline-formula> otherwise. Then run <sc>RANKBIN</sc> on the data matrix <italic>X</italic>, <italic>τ</italic><sup><italic>j</italic></sup>, and the fixed sparsity parameter <italic>s</italic> to get the markers for cell type <italic>j</italic>. This will usually result in a different (informative) number of markers selected for each cell type.</p>
        <p>
          <graphic position="anchor" xlink:href="12859_2020_3641_Figc_HTML" id="MO17"/>
        </p>
        <p>Note that the computation of <inline-formula id="IEq52"><alternatives><tex-math id="M121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M122"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq52.gif"/></alternatives></inline-formula> does not depend on the group <italic>j</italic> that we are selecting markers for; thus, <inline-formula id="IEq53"><alternatives><tex-math id="M123">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\overline {X}$\end{document}</tex-math><mml:math id="M124"><mml:mover accent="false"><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq53.gif"/></alternatives></inline-formula> can be computed one time at the start of marker selection. To explicitly avoid this extra computation, we write <sc>RANKCORR</sc> without calling <sc>RANKBIN</sc>; the ideas behind the steps in the loop are provided in our discussion of <sc>RANKBIN</sc>, however.</p>
        <p>In <sc>RANKCORR</sc>, we take the union of all the markers selected for each cluster to get a set of markers that will represent all of the given cell types. This step is to allow for easier collection of benchmarking statistics - we would like to capture how well a selected set of markers informs us about an entire clustering. In practice, the sets of markers could be kept separate to give information about individual cell types. Note that there could still be duplicate markers in these sets - here, we do not address the problem of dealing with duplicates in a smart way.</p>
        <p>As a final note, when we are not interested in a full set of markers, we can quickly compare two genes to determine the gene that is favored by <sc>RANKCORR</sc> for a fixed cluster <italic>q</italic>: gene <italic>j</italic> will be chosen as a marker more favorably than gene <italic>k</italic> if the norm of the Spearman correlation between <italic>τ</italic><sup><italic>q</italic></sup> and <italic>X</italic><sub><italic>j</italic></sub> is larger than the norm of the Spearman correlation between <italic>τ</italic><sup><italic>q</italic></sup> and <italic>X</italic><sub><italic>k</italic></sub>.</p>
      </sec>
    </sec>
    <sec id="Sec29">
      <title>Marker evaluation methods for experimental data</title>
      <p>Below, we discuss two general procedures for the evaluation of a set of markers: supervised classification (that incorporates the given ground truth clustering as prior information) and unsupervised clustering (that does not).</p>
      <p>Assuming that the data set contains <italic>n</italic> points in <italic>k</italic> clusters, the result obtained by either classifying or clustering the data is a vector of predicted cell type labels <inline-formula id="IEq54"><alternatives><tex-math id="M125">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {y} \in \boldsymbol {\mathrm {Z}}^{n}$\end{document}</tex-math><mml:math id="M126"><mml:mi>ŷ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mstyle mathvariant="bold-italic"><mml:mi mathvariant="normal">Z</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq54.gif"/></alternatives></inline-formula>. We would like to compare this to the “ground truth” cluster label vector <italic>y</italic>∈[<italic>k</italic>]<sup><italic>n</italic></sup>. The full information about the similarity between <italic>y</italic> and <inline-formula id="IEq55"><alternatives><tex-math id="M127">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\hat {y}$\end{document}</tex-math><mml:math id="M128"><mml:mi>ŷ</mml:mi></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq55.gif"/></alternatives></inline-formula> can be presented in terms of a confusion matrix; this is unwieldy when many such comparisons are required, however. For this reason, many summary statistics have been developed in the machine learning literature for the classification [<xref ref-type="bibr" rid="CR22">22</xref>] and clustering [<xref ref-type="bibr" rid="CR23">23</xref>] settings. We choose to examine several of these metrics in this work; the full list is summarized in Table <xref rid="Tab3" ref-type="table">3</xref>.</p>
      <sec id="Sec30">
        <title>Cross validation</title>
        <p>In order to avoid overfitting, we perform all marker selection, classification, and clustering using 5-fold cross validation. See Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 1 for a summary of this procedure. Specifically, we split the cells into five groups (called “folds”). For each fold, we combine the other four folds into one data set, find the markers on the dataset containing four folds, and train the classifier using the selected markers on the dataset containing four folds as the training data. We then apply the trained classifier to the initial (held-out) fold and perform clustering on the initial fold using the markers that were selected on the other four folds. In this way, the initial fold is “test data” for the classifier/clustering metrics.</p>
        <p>Repeating this process for all five folds creates a classification for the entire data set. On the other hand, we get a separate clustering for each fold, and these clustering solutions may be incompatible (they may contain different numbers of clusters, for example). See the section on clustering evaluation metrics for how we reconcile this.</p>
      </sec>
      <sec id="Sec31">
        <title>Supervised classification</title>
        <p>In order to incorporate information about the ground truth clustering into an evaluation metric, we train a multi-class classifier on the scRNA-seq data using the cluster labels as the target output. In order to evaluate the selected marker genes, we train the classifier using only the marker genes as the input data. When applied to a vector of counts (e.g. the counts of the markers in a cell), the classifier outputs a prediction of the cluster that the vector belongs to.</p>
        <p><bold>Training a classifier.</bold> The specifics of how a classifier is trained are presented in Algorithm 4.</p>
        <p>
          <graphic position="anchor" xlink:href="12859_2020_3641_Figd_HTML" id="MO18"/>
        </p>
        <p>In line 2 of Algorithm 4, we normalize the matrix <italic>X</italic>. It is possible to use any normalization for this step; for the purposes of our analysis we use a log normalization procedure that is commonly found in the scRNA-seq literature. Specifically, we perform a library-size normalization so that the sum of the entries in each row of <italic>X</italic> is 10,000 and follow this by taking the base 2 logarithm of (1 plus) each entry of <italic>X</italic> to create a “log normalized” counts matrix.</p>
        <p>Library size normalization was introduced in [<xref ref-type="bibr" rid="CR41">41</xref>] to account for differences in capture efficiency between cells and taking a logarithm has it roots in bulk RNA-seq where it is used to attenuate technical variance (see [<xref ref-type="bibr" rid="CR42">42</xref>]). Since log normalization of this type is often applied when clustering scRNA-seq counts data in a data processing pipeline, we apply log normalization when attempting to recover the information in the given clusters. It is important to note that the marker selection algorithms that we examine in this work do not assume that the input counts data are normalized (apart from when noted in their descriptions).</p>
        <p><bold>Classification evaluation metrics.</bold> We select markers and classify the cells using 5-fold cross-validation (see also Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 1). Once we have classified all cells in the data set, we examine how well the vector of classification labels matches the vector of ground truth cluster labels. Since we are in a classification framework, we use multi-class classification evaluation metrics for this purpose. In particular, we examine the classification error (1- accuracy) and precision of the classification compared to the known ground truth. For precision in a multi-class setting, we compute the precision for each class (as in a binary classification setting) and then take a weighted average of the per-class precision values, weighted by the class sizes. Finally, we also examine the Matthews correlation coefficient, which is a summary statistic that incorporates information about the entire confusion matrix. See [<xref ref-type="bibr" rid="CR22">22</xref>] for more information about these statistics.</p>
        <p>In all of the tests that we perform in this work, the precision and Matthews correlation coefficient curves look subjectively similar to each other (though the actual values of the statistics do differ), while the classification error appears very similar to the other curves except it is flipped upside down. It is not clear why these summary statistics look as similar as they do. In any case, we generally only present the classification error rate in this document; the precision and Matthews correlation coefficient pictures can be found in Additional file <xref rid="MOESM1" ref-type="media">1</xref> (Figures 2 and 3 for <sc>ZEISEL</sc>; Figures 5 and 6 for <sc>PAUL</sc>; Figures 8–11 for <sc>ZHENGFILT</sc> and <sc>ZHENGFULL</sc>; and Figure 13 for <sc>10XMOUSE</sc>).</p>
        <p><bold>Classifiers</bold> We examine two classifiers to evaluate the marker sets (so that we are computing two classifications for each selected set of markers, and looking at all three metrics for both classifications).</p>
        <p>The first is a simple (and fast) nearest centroids method that uses information about the original clustering to determine the locations of the cluster centroids. We refer to this as the Nearest Centroids Classifier (NCC). See the end of this section for a full description of the NCC. In the second, we use the Random Forest Classifier (RFC) that is implemented in the Python package scikit-learn ([<xref ref-type="bibr" rid="CR30">30</xref>]), version 0.20.0, with nestimators = 100.</p>
        <p>The summary statistics of the classifications produced when using the RFC are always better (more optimal) than the statistics that are produced when using the NCC. In addition, the overall shape of the curves produced using the RFC mostly mirror the curves produced using the NCC. The RFC is too slow to run on the largest data sets that we examine for testing. Since the RFC and NFC curves look similar for the smaller data sets, we are not concerned that we are missing information here.</p>
        <p>Also note that, even with nestimators = 100, there is a significant amount of variability in the classification results obtained through the RFC. That is, running the RFC multiple times with the same set of markers will produce different classification results. See Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 15 for a visualization of the differences in error rate that can be obtained when running the RFC twice on the same sets of markers (this example is created using the <sc>PAUL</sc> data set; see the discussion of experimental data sets).</p>
        <p><bold>The nearest centroids classifier (NCC).</bold> The NearestCentroids.train method is presented in Algorithm 5.</p>
        <p>
          <graphic position="anchor" xlink:href="12859_2020_3641_Fige_HTML" id="MO19"/>
        </p>
      </sec>
      <sec id="Sec32">
        <title>Unsupervised clustering</title>
        <p>Another natural way to measure the information in a selected set of markers is to cluster the data using only the selected coordinates in an unsupervised manner and compare this new clustering to the original clustering. Clustering scRNA-seq is itself a complicated problem that has inspired a great deal of study; here we restrict ourselves to Louvain clustering as implemented in the scanpy (version 1.3.7) package. Louvain clustering was introduced for use with scRNA-seq experiments in [<xref ref-type="bibr" rid="CR43">43</xref>] and it is currently the recommended method for clustering scRNA-seq data in several commonly-used software packages including scanpy [<xref ref-type="bibr" rid="CR5">5</xref>] and Seurat [<xref ref-type="bibr" rid="CR6">6</xref>].</p>
        <p><bold>The clustering procedure.</bold> A method for performing Louvain clustering on scRNA-seq data is presented in Algorithm 6. Note that we do not perform any dimensionality reduction (e.g. PCA) before finding nearest neighbors or performing the clustering. This is due to the fact that we project the data onto the selected markers. These markers are meant to capture the important dimensions in the data - they are the features that have the most information about the clustering according to a marker selection algorithm. Thus, we work in the space spanned by these markers without performing any additional dimensionality reduction.</p>
        <p>
          <graphic position="anchor" xlink:href="12859_2020_3641_Figf_HTML" id="MO20"/>
        </p>
        <p>In line 2, of Algorithm 6, we normalize the counts matrix <italic>X</italic>. As in the case of the supervised classification metrics, we apply log-normalization for this step.</p>
        <p><bold>Clustering evaluation metrics.</bold> The unsupervised clustering is compared to the ground truth clustering using three metrics from the machine learning literature: the Adjusted Rand Index (ARI), Adjusted Mutual Information (AMI), and the Fowlkes-Mallows score (FMS). All three of these scores attempt to capture the amount of similarity between two groupings of one data set (e.g. the unsupervised clustering produced using a selected marker set and the ground truth clustering). They are also normalized scores: values near zero indicate that the cluster labels are close to random, while positive values indicate better performance. All of the scores have a maximum value of +1. Moreover, all three of these metrics do not make any assumption about the number of clusters: the unsupervised clustering can have a different number of clusters from the ground truth clustering and these indices can still be computed. See [<xref ref-type="bibr" rid="CR23">23</xref>] for more information about these metrics.</p>
        <p>We again use 5-fold cross-validation to compute the clustering performance metrics. Note that the clustering solutions for the different folds may be incompatible: for example, the number of clusters in the Louvain cluster solution for the first fold may be different from the number of clusters in the Louvain cluster solution for the second fold, and there may be no obvious way to relate the clusters in the first fold to the clusters in the second fold. For this reason, we compute the clustering performance metrics separately on each fold, comparing the Louvain cluster solution to the ground truth clustering restricted to the fold. The scores that we report are averaged over all of the folds (and when we optimize over the resolution parameter <italic>r</italic>, discussed below, we find the optimal value of the average over the folds).</p>
        <p>Note that some of the fine structure from the ground truth clustering may not be maintained in a specific fold and thus it is impossible to capture this structure when performing Louvain clustering on the fold. This means that the actual values of these metrics are not particularly informative - it is more useful to compare the different methods along a metric. In addition, in all of the Louvain clusterings for a specific data set, we fix the value of <italic>k</italic>, the number of nearest neighbours that we consider. Thus, small differences between the scores are not particularly informative, as they could disappear if <italic>k</italic> was selected perfectly for each method. Nonetheless, it is useful to get an idea as to how well the markers selected by different algorithms could be used in an unsupervised manner to recover a given clustering.</p>
        <p><bold>Choice of Louvain clustering parameters.</bold> Louvain clustering requires the input of a number <italic>k</italic> of nearest neighbors and a resolution parameter <italic>r</italic>. It would be ideal to optimize both <italic>k</italic> and <italic>r</italic> for each set of markers on each data set for each clustering comparison metric; then we would be comparing the “optimal” performances of the marker selection algorithms under each metric. This is not computationally realistic for all of the data sets in consideration here, however.</p>
        <p>Thus, for a given data set, we fix the value of <italic>k</italic>. For each set of markers on the data set, we compute the <italic>k</italic> nearest neighbors (once), and then quickly optimize over the resolution parameter <italic>r</italic>. To optimize <italic>r</italic>, we examine a grid from <italic>r</italic>=0.1 to <italic>r</italic>=3.0 with a step size of 0.1. This allows us to compute approximately optimal values of each of the metrics for each set of selected markers in a computationally efficient manner. Importantly, the resolution parameter is optimized for each metric using each marker selection algorithm.</p>
        <p><bold>Choosing</bold><bold><italic>k</italic></bold><bold>.</bold> On the <sc>PAUL</sc> and <sc>ZEISEL</sc> data sets, we examined values of <italic>k</italic> varying from 15 to 30 (with a step size of 5). For each value of <italic>k</italic>, we used the <sc>RANKCORR</sc> algorithm to optimize over <italic>r</italic> (varying from <italic>r</italic>=0.1 to <italic>r</italic>=3.0 with a step size of 0.1), and we varied the number of markers selected to get a picture of the entire parameter space. The curves that are produced by this process are quite similar for different values of <italic>k</italic> (see Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figures 16 and 17). The value of <italic>k</italic> is chosen to be the one that subjectively appears to optimize the performance of the majority of the metrics.</p>
        <p>On the <sc>ZEISEL</sc> dataset, it appears that <italic>k</italic>=15 nearest neighbors does not capture quite enough of the cluster structure, while <italic>k</italic>=30 nearest neighbors results in lower scores than <italic>k</italic>=25. We thus fix <italic>k</italic> at 25 for the unsupervised clustering evaluation on the <sc>ZEISEL</sc> data set. See Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 16 for the data that were used for this determination.</p>
        <p>For the <sc>PAUL</sc> data set, we observed that changing the number of nearest neighbors used in the Louvain clustering has little effect on the ARI, AMI, or FM scores. It appeared that the scores were slightly improved for <italic>k</italic>=30 when small numbers of markers were selected, thus we fixed <italic>k</italic> at 30 for the <sc>PAUL</sc> data set. See Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 17.</p>
        <p>The <sc>ZHENGFULL</sc> and <sc>ZHENGFILT</sc> data sets are large, and thus we focus on the <sc>ZHENGFILT</sc> data set when considering the unsupervised clustering metric. To estimate a value of <italic>k</italic>, the fixed number of nearest neighbours that we use for all of the clusterings, we computed a Louvain clustering that looks quite similar to the bulk labels in a UMAP plot. This clustering used 25 nearest neighbors (and used the top 50 PCs); thus we fix <italic>k</italic> at 25 for the <sc>ZHENG</sc> data sets. See Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 18 to see a comparison of the bulk labels and the generated Louvain clustering in UMAP space.</p>
      </sec>
    </sec>
    <sec id="Sec33">
      <title>Experimental data sets</title>
      <p>We examine four publicly available experimental scRNA-seq data sets in this work. We focus on data sets that have been clustered, with clusters that have been biologically verified in some way. In addition, we mostly examine data sets that were collected using microfluidic protocols (Drop-seq, 10X) with UMIs. This is due to the fact that these protocols tend to collect a smaller number of reads in a larger number of cells (producing large amounts of sparse data). These data sets are summarized in Table <xref rid="Tab1" ref-type="table">1</xref>. We discuss them further below. See the statement on data availability for how to obtain these data and for more information about the scripts used for pre-processing.</p>
      <p><sc>ZEISEL</sc>. We work with one well-known reference fluidigm data set. This is <sc>ZEISEL</sc>, a data set consisting of mouse neuron cells that was introduced in [<xref ref-type="bibr" rid="CR24">24</xref>]. Neuron cells are generally well-differentiated, and thus this data set contains distinct clusters that should be quite easy to separate. In [<xref ref-type="bibr" rid="CR24">24</xref>], the authors have additionally used in-depth analysis with known markers in combination with a biclustering method of their own design to painstakingly label each cell with a specific cell type. This labeling is the closest to an actual ground truth clustering of a dataset in the scRNA-seq literature - this fact makes <sc>ZEISEL</sc> a valuable data set for our benchmarking purposes.</p>
      <p>For our ground truth clustering, we consider only the nine major classes that the authors define in [<xref ref-type="bibr" rid="CR24">24</xref>]. In addition, we pre-process the data set using the cell_ranger flavor of the filter_genes_dispersion function in the scanpy Python package after library size normalization. We ask for the top 5000 most variable genes; the filter_genes_dispersion function only returns 4999 genes, however. We perform this pre-processing to speed up the marker selection process for the slower methods.</p>
      <p><sc>PAUL</sc>. The smallest data set that we examine is <sc>PAUL</sc>, a data set consisting of 2730 mouse bone marrow cells that was introduced in [<xref ref-type="bibr" rid="CR25">25</xref>] and collected using the MARS-seq protocol. As opposed to <sc>ZEISEL</sc>, bone marrow cells consist of progenitor cells that are in the process of differentiating. Thus, there are no well separated cell types in the <sc>PAUL</sc> data - the data appear in a continuous trajectory. The authors of [<xref ref-type="bibr" rid="CR25">25</xref>] define discrete cell types along this trajectory based on known markers, however: we consider this clustering from [<xref ref-type="bibr" rid="CR25">25</xref>] to be the ground truth for our analysis in this manuscript.</p>
      <p><sc>ZHENG</sc><bold>data sets.</bold> We perform an analysis of the data set introduced in [<xref ref-type="bibr" rid="CR2">2</xref>] that consists of around 68 thousand human PBMCs from a single donor; we refer to this full data set as <sc>ZHENGFULL</sc>. These data were collected using 10x protocols.</p>
      <p>The ground truth clustering for the <sc>ZHENG</sc> data set that we examine in this manuscript contains 11 clusters. It was constructed in [<xref ref-type="bibr" rid="CR2">2</xref>] by maximizing correlation with purified reference cell populations. This clustering contains some cell types that should be easy to separate (e.g. B cells vs T cells), as well as some cell types that have mostly overlapping profiles (e.g. several types of T cells are included as different clusters). The cluster labels can be found on the scanpy_usage GitHub repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/theislab/scanpy_usage/blob/master/170503_zheng17/data/zheng17_bulk_lables.txt">https://github.com/theislab/scanpy_usage/blob/master/170503_zheng17/data/zheng17_bulk_lables.txt</ext-link>(we use commit 54607f0).</p>
      <p>To be more precise, these clusters were determined in the following manner: first, the full data set was clustered (using k-means), and the clusters were assigned biological types based on known markers. The authors of [<xref ref-type="bibr" rid="CR2">2</xref>] then took more cells (from the same donor) and isolated a set cells of each cell type that they found in their clustering of <sc>ZHENGFULL</sc>. They then sequenced the cells from the individual types. Finally, they used these pure samples to cluster the <sc>ZHENGFULL</sc> data set again: each cell is assigned to the type whose (average) profile correlates most strongly with the cell’s profile..</p>
      <p>We additionally generate a data set <sc>ZHENGFILT</sc> from <sc>ZHENGFULL</sc> by restricting to the top 5000 most variable genes. We select the 5000 most variable genes by performing a library size normalization on the <sc>ZHENGFULL</sc> data set and then using the cell_ranger flavor of the filter_genes_dispersion function in the scanpy Python package. We can run the slower methods on <sc>ZHENGFILT</sc>.</p>
      <p><bold>1.3 million mouse neurons.</bold> Finally, we examine <sc>10XMOUSE</sc>, a data set consisting of 1.3 million mouse neurons generated using 10x protocols [<xref ref-type="bibr" rid="CR3">3</xref>]. As noted above, neurons are well-differentiated into cell types, so this data set should contain well-separated clusters. The “ground truth” clustering that we consider for this data set is a graph-based (Louvain) clustering performed on the full <sc>10XMOUSE</sc> dataset by the team behind scanpy. It can be found from the scanpy_usage GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells">https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells</ext-link>; we consider commit ba6eb85) As far as we know, this clustering has not been verified in any biological manner.</p>
    </sec>
    <sec id="Sec34">
      <title>Marker selection methods</title>
      <p>A summary of the marker selection methods that we consider in this work is found in Table <xref rid="Tab2" ref-type="table">2</xref>. In addition, we also implemented SCDE and D<sup>3</sup>E, but found these two methods to be too slow. We discuss our precise implementation details below. We use Python version 3.7 and R version 3.5.0 unless otherwise noted.</p>
      <p><bold>Wilcoxon and the t-test.</bold> The t-test and the Wilcoxon rank sum are general statistical methods that aren’t specifically designed for RNA-seq data, but they are still often used for the purposes of differential expression testing in the scRNA-seq literature. We use the Python scanpy package (version 1.3.7) implementation to find Wilcoxon rank sum and t-test <italic>p</italic>-values with some editing to the file _rank_genes_groups.py to fix several bugs (that are now fixed in the main release). See the data availability disclosure for how to find this edited file.</p>
      <p>Both of these methods produce a score for each gene: when choosing the markers for the clusters, we use the absolute value of this score (so we would chose markers that have a large negative score as well). This is for more direct comparison to the <sc>RANKCORR</sc> method, where we choose markers by the absolute value of their coefficients. In addition, both of these methods correct the <italic>p</italic>-values that they produce using Benjamini-Hochberg correction. Finally, we use the version of the t-test in scanpy that overestimates the variance of the data.</p>
      <p><bold>edgeR and MAST.</bold> The methods edgeR and MAST were originally implemented in R. In order to run them with our existing framework, we use the rpy2 (version 2.9.4) package to access the methods through Python.</p>
      <p>Based on the results and scripts from [<xref ref-type="bibr" rid="CR7">7</xref>], edgeR (version 3.24.1) was run using the quasi-likelihood approach (QLF method) on the un-normalized scRNA-seq counts matrix <italic>X</italic>. For MAST (version 1.8.1), the data matrix <italic>X</italic> was normalized: the rows of <italic>X</italic> were scaled so that each row summed to 1 million (to approximate something that looks like “transcripts per million”) to create a scaled matrix <italic>X</italic><sup><italic>s</italic></sup> and then each entry <inline-formula id="IEq56"><alternatives><tex-math id="M129">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$X^{s}_{ij}$\end{document}</tex-math><mml:math id="M130"><mml:msubsup><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ij</mml:mtext></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq56.gif"/></alternatives></inline-formula> of <italic>X</italic><sup><italic>s</italic></sup> was replaced by <inline-formula id="IEq57"><alternatives><tex-math id="M131">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\log \left (X^{s}_{ij} + 1\right)$\end{document}</tex-math><mml:math id="M132"><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:munderover><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mtext mathvariant="italic">ij</mml:mtext></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:munderover><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq57.gif"/></alternatives></inline-formula>.</p>
      <p>Again following [<xref ref-type="bibr" rid="CR7">7</xref>], we ran both edgeR and MAST in two ways on the <sc>PAUL</sc> and <sc>ZEISEL</sc> data sets. In the first way, we only consider the cluster label when fitting the statistical model; these results are presented in the <xref rid="Sec5" ref-type="sec">Results</xref> section above. For the second way, we additionally include the fraction of genes that are detected in each cell (“detection rate”) as a covariate. We refer to edgeR and MAST run the second way by edgeRdet and MASTdet respectively. According to the marker set evaluation metrics, edgeRdet and MASTdet perform similarly to the other methods considered in this work. In addition, edgeRdet (MASTdet) requires slightly more computational resources than edgeR (MAST). For this reason, we choose not to include the edgeRdet or MASTdet results in this manuscript; see Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figures 2–7 for that information.</p>
      <p><bold>scVI.</bold> scVI (version 0.2.4) is implemented in Python and utilizes GPUs for faster training. Although the authors provide evidence that their code can handle a data set with one million cells (scVI is tested on the <sc>10XMOUSE</sc> data set in [<xref ref-type="bibr" rid="CR29">29</xref>]), scVI requires steep computational resources - around 75 GB of RAM to go with one core and one GPU. We were unable to obtain this large amount of memory and a GPU at the same time, so we have been unable to reproduce their results here. One issue is that scVI does not work with sparse data structures (or it makes them dense after loading them); thus, it has been computationally infeasible for us to run scVI on the larger data sets like <sc>10XMOUSE</sc>.</p>
      <p>Another issue with scVI is that the differential expression methods included in the package are themselves computationally demanding (even after the model has been trained). As far as we can tell, requesting information about differentially expressed genes from a trained scVI instance produces a matrix of size larger than (10·<italic>n</italic>)×<italic>p</italic>, where <italic>n</italic> is number of cells and <italic>p</italic> is the number of genes in the original data set. Even restricting to the top 3000 variable genes in the <sc>10XMOUSE</sc> data set, this matrix would require around 250 GB of memory to load into storage - in addition to the storage required for the (dense) <sc>10XMOUSE</sc> dataset itself. Thus, although it may be possible to train the model on the <sc>10XMOUSE</sc> data set, it will be nearly impossible with our computational resources to actually acquire the differential expression information from the trained model.</p>
      <p>(An example of the extreme memory used by scVI: the <sc>ZEISEL</sc> dataset takes approximately 5 MB to store in a dense format. The matrix produced during the differential expression computation method requires 4.1 GB. The actual computation of the Bayes factors - the generalization of a <italic>p</italic>-value produced by scVI - uses a peak of 15-16GB of memory during processing. This high memory usage does not appear in the data presented in the Fig. <xref rid="Fig3" ref-type="fig">3</xref> (in the <xref rid="Sec5" ref-type="sec">Results</xref> section) since it is only required for post processing - actually training the scVI model does not require this memory.)</p>
      <p><sc>SPA</sc>. We examine the performance of the method <sc>SPA</sc> introduced in [<xref ref-type="bibr" rid="CR14">14</xref>] and analyzed further in [<xref ref-type="bibr" rid="CR38">38</xref>]. As discussed in the full description of RankCorr, <sc>SPA</sc> was the inspiration for this work, and selects markers based on a sparsity parameter <italic>s</italic>. <sc>SPA</sc> also has two hyperparameters that we are required to optimize over, and this causes <sc>SPA</sc> to take considerably longer than <sc>RANKCORR</sc> to run for a fixed value of <italic>s</italic>. Moreover, in a situation with no known ground truth, it is unclear what metric we would like to optimize when selecting these hyperparameters. For the current evaluation, we have minimized the classification error rate using the NCC (see information about the marker set evaluation metrics), but it is not clear that this would be the best metric to optimize in general. We choose the NCC classifier since the RFC exhibits some variance (see Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 15) - thus, optimizing the classification error rate according to the RFC classifier would produce an unstable set of markers (performing the optimization again would result in a different set of markers). We choose to optimize the supervised classification error (rather than one of the unsupervised clustering metrics) for the sake of speed - optimizing a slower evaluation metric would increase the computation time required for the <sc>SPA</sc> marker selection method.</p>
      <p>Another inconvenience of the <sc>SPA</sc> method is that the hyperparameters affect the number of markers that are selected for a fixed value of <italic>s</italic>. This makes the number of markers selected by <sc>SPA</sc> method more inconsistent and unpredictable. For example, it has occurred that the “optimal” (in terms of minimizing the classification error rate using the NCC, as discussed above) choice of hyperparameters for sparsity parameters <italic>s</italic><sub>1</sub>&gt;<italic>s</italic><sub>2</sub> has resulted in a smaller number of markers selected for <italic>s</italic><sub>1</sub> than the “optimal” choice of hyperparameters for <italic>s</italic><sub>2</sub>. That is, increasing <italic>s</italic> can lead to selecting smaller numbers of markers.</p>
      <p><bold>Elastic nets.</bold> The <sc>SPA</sc> method introduced in [<xref ref-type="bibr" rid="CR15">15</xref>] is similar to an <italic>L</italic><sub>1</sub>- and <italic>L</italic><sub>2</sub>- regularized SVM without an offset (i.e. it finds a sparse separating hyperplane that is assumed to pass though the origin, the instinct for this is given near Equation (<xref rid="Equ5" ref-type="">5</xref>) in the <xref rid="Sec23" ref-type="sec">Methods</xref>). Thus, we also compare the performance of <sc>RANKCORR</sc> to that of the Elastic Nets version of LASSO: a least squares method with both <italic>L</italic><sub>1</sub> and <italic>L</italic><sub>2</sub> regularization [<xref ref-type="bibr" rid="CR12">12</xref>]. Elastic Nets has two regularization parameters that need to be tweaked in order to find the optimal set of features; this requires extra cross-validation and therefore we are only able to run on the smaller <sc>PAUL</sc> and <sc>ZEISEL</sc> data sets. Although the scikit-learn (version 0.20.0) package contains a method for finding the regularization parameters by cross-validation, it still takes a significant manual effort in order to find a range of the regularization parameters that capture the full possible behavior of the system but will also allow for the objective function to converge (in a reasonable number of iterations) the majority of the time. The timing information presented in Fig. <xref rid="Fig3" ref-type="fig">3</xref> only represents the run time of the method, and does not take into account this (time consuming) process of manipulating the data.</p>
      <p>Another feature to note about the cross-validated elastic nets method is that it is (intentionally) a sparse method. Thus, scores are only generated for a small number of genes in each cluster - the genes that are specifically deemed “markers” for that cluster. It is not possible to compare the relative utilities of the genes that are not considered markers - each of those genes are given a score of 0. Thus, beyond a certain number of genes, it is not possible to get any more information from the markers selected by the elastic nets method. (You cannot, for example, request a “bad” marker in order to combine it with the information from other “good” markers).</p>
      <p><bold>Logistic regression.</bold> Logistic regression is proposed as a method for marker selection in [<xref ref-type="bibr" rid="CR31">31</xref>]. Specifically, a regression is performed on each gene using the cluster label as the response variable. This is translated into a <italic>p</italic>-value via a likelihood ratio (comparing to the null model of logistic regression). The scanpy (version 1.3.7) package includes this method, and thus we are able to run it on sparse data. We again have made some updates to the file _rank_genes_groups.py in the scanpy package to fix some slight errors (that are now fixed in the main release); see the data availability disclosure for where to find this edited file.</p>
      <p><sc>RANKCORR</sc>. The <sc>RANKCORR</sc> algorithm is introduced in this paper; the precise procedure is discussed full description of RankCorr. It is important to note that the implementation of <sc>RANKCORR</sc> that we use here has not been fully optimized. Note that the major step (2) of the <sc>SELECT</sc> algorithm (Algorithm 1) essentially consists of computing the dot product of each column of a data matrix with the cluster labels <italic>τ</italic>. The only other time consuming portion of <sc>SELECT</sc> is computing the <italic>ℓ</italic><sub>2</sub> norm of a vector. These types of linear algebraic computation have fast implementations that are accessible from Python (e.g. numba). We have not yet optimized the method to take advantage of all possible speed ups since <sc>RANKCORR</sc> runs quickly enough in our trials.</p>
      <p><bold>Random marker selection.</bold> As a sanity check, we select markers by choosing genes uniformly at random (the same number of markers for each cluster). All of the other methods presented in this work outperform random marker selection by significant margins. For the sake of clarity, the performance of random marker selection is relegated to Additional file <xref rid="MOESM1" ref-type="media">1</xref> (Figures 2–12).</p>
      <p><bold>Seurat</bold> The commonly-used Seurat data analysis package [<xref ref-type="bibr" rid="CR6">6</xref>] contains implementations of several methods that we consider here, including the Wilcoxon method, the t-test, logistic regression, and MAST. The default method for selecting markers in Seurat is the Wilcoxon method combined with some gene pre-filtering that is designed to speed up the computations. Potentially, the Seurat method could perform better than the standard Wilcoxon rank-sum test (considered in the manuscript) if the researcher is careful to manually tune the different gene filtering thresholds to eliminate uninteresting genes that have high Wilcoxon scores. The Seurat documentation warns that this type of filtering may also eliminate marker genes with weaker differential expression signals, however. We thus do not examine tuning these parameters in this manuscript. See [<xref ref-type="bibr" rid="CR6">6</xref>] or the Seurat website (<ext-link ext-link-type="uri" xlink:href="https://satijalab.org/seurat/">https://satijalab.org/seurat/</ext-link>) for further information.</p>
      <p><bold>SCDE.</bold> The SCDE package (we examined version 2.6.0) is implemented in R. Our testing found that it was too slow to be used on real scRNA-seq data sets: it was taking approximately one minute per cell to fit the model (on one core). Since we are performing 5-fold cross-validation, we would need to fit the model approximately 5 times. On one of the smaller data sets (<sc>PAUL</sc> or <sc>ZEISEL</sc>), this would require approximately 250 hours of computer time; it would be infeasible to train on the larger data sets. Since we are specifically developing methods for use with the large data sets that are appearing more often, we have excluded SCDE from our final analysis.</p>
      <p><bold>D</bold><sup><bold>3</bold></sup><bold>E.</bold> D<sup>3</sup>E is also implemented in Python (version 2.7), but it has no support for sparse data structures; thus, running on the <sc>10XMOUSE</sc> data set would require a very large amount of memory. Although the method allows for splitting the data into smaller segments (to allow for parallel computation), the full data set needs to be loaded into memory when initializing the process. In addition, when running on the <sc>PAUL</sc> data set using the faster method-of-moments mode, D<sup>3</sup>E took about 25 minutes running on 10 cores (about 4 hours and 10 minutes total processor time) to find markers for one cluster (vs the rest of the population). Since we need the <italic>p</italic>-values for all (19) clusters for all 5 folds, this method would require approximately 40 hours on 10 cores. Although this is faster than SCDE, this would still be too slow to run on the larger data sets, and thus we exclude D<sup>3</sup>E from our final analysis as well. All of our testing was carried out using the D<sup>3</sup>E source on GitHub (commit efe21d1).</p>
      <p><bold>COMET.</bold> The COMET method [<xref ref-type="bibr" rid="CR17">17</xref>] is a (streamlined) brute force approach for examining the predictive power of “gene panels” (sets containing up to four genes). COMET inherently has different goals than the majority of the marker selection methods discussed here, and it can not (currently) select more than four genes for a given cell type. Restricting to a very small number of selected markers is useful when the cell types are well-known and a researcher wishes to perform further experimental analysis; COMET is indeed used to guide FACS sorting in [<xref ref-type="bibr" rid="CR17">17</xref>]. The brute force computational costs do not make COMET a useful tool for data exploration or for providing feedback towards the veracity of a potential clustering, however. In fact, when tested on the <sc>PAUL</sc> data set, the COMET method took an average of eight minutes to rank pairs of markers for a fixed cluster (resulting in around 2.5 hours of total runtime for all 19 clusters; this would be approximately 12.5 hours to run on all five folds). In addition to this, as of this writing, COMET requires all data to be input as text files or 10x expression files - these are not (especially) sparse formats, and this further limits the data sets that can be studied with this tool (for example, the 10x data files for the full <sc>ZHENG</sc> dataset require around 600MB of disk space, while the sparse version requires about 100 MB). We thus do not include the full COMET method in our comparisons in this manuscript.</p>
      <p>As mentioned in the <xref rid="Sec1" ref-type="sec">Background</xref> section, however, COMET is based on a statistical test (the XL-mHG test, see [<xref ref-type="bibr" rid="CR44">44</xref>]) that has some desirable properties for scRNA-seq data. In addition to considering combinations of genes, COMET also ranks individual genes by the average of their XL-mHG <italic>p</italic>-values and (the logarithm of) their fold-changes. These ranks (or the related <italic>p</italic>-values) could be used in a similar fashion to the other differential expression methods considered in this manuscript to select markers for the data sets (e.g. select the top five genes for each cluster). We have elected not to examine this method, however, since we are working with several other differential expression methods based on statistical tests (e.g. Wilcoxon, the t-test).</p>
      <p><bold>scGeneFit.</bold> The scGeneFit method is introduced in the preprint [<xref ref-type="bibr" rid="CR20">20</xref>]. As mentioned in the <xref rid="Sec1" ref-type="sec">Background</xref> section, this method attempts to discover markers that are informative about a clustering as a whole rather than determining markers for individual cell types. Specifically, it finds a set of genes <italic>M</italic> such that the projection of the data onto <italic>M</italic> is “optimal.” In this case, optimality means that the distance (after projecting) between different clusters is larger than an input parameter (as much as possible). This optimal projection is defined by a linear program: the variables correspond to genes, and the constraints enforce separation between clusters (there is one constraint for each pair of cells in different clusters). For the sake of efficiency, the current implementation of scGeneFit (<ext-link ext-link-type="uri" xlink:href="https://github.com/solevillar/scGeneFit-python">https://github.com/solevillar/scGeneFit-python</ext-link>, commit 32dd6a1) considers a random subsample of the constraints; thus, scGeneFit can be applied to data sets of arbitrary size if selecting lower quality markers is acceptable. The trade-offs between efficiency and marker quality are not yet fully explored in the preprint [<xref ref-type="bibr" rid="CR20">20</xref>], however (for example, the number of constraints required for quality should probably be related to the number of clusters; this method of random sampling also seems to deemphasize rare cell types).</p>
      <p>We ran scGeneFit using parameters suggested on the scGeneFit GitHub page and altering the number of constraints so that the method ran in a reasonable amount of time on the <sc>PAUL</sc> and <sc>ZEISEL</sc> data sets (approximately 10 minutes to run on one fold). The results that we obtained were somewhat suboptimal, however, especially on the <sc>PAUL</sc> data set. Additionally, the supercomputer architecture used for our analysis changed before we collected data using scGeneFit; thus, comparisons of the timing of scGeneFit to the other methods would not be valid. Therefore, further considering that scGeneFit [<xref ref-type="bibr" rid="CR20">20</xref>] is presented in a preprint (and is thus subject to significant future change), we report our scGeneFit results in Additional file <xref rid="MOESM1" ref-type="media">1</xref> (Figures 2 and 3 for <sc>ZEISEL</sc>; Figures 5 and 6 for <sc>PAUL</sc>) rather than the main manuscript.</p>
    </sec>
    <sec id="Sec35">
      <title>Generating marker sets of different sizes from algorithms other than <sc>RANKCORR</sc></title>
      <p>For a fixed data set, we need to select markers for a given clustering - not just markers for a single cluster. Here we describe how we select a specific number of markers and how we merge lists of markers for individual clusters to make a marker list for the entire clustering.</p>
      <p>For a differential expression method, we proceed in a one-vs-all fashion: letting <italic>C</italic> denote the number of clusters in the given clustering, we use the differential expression methods to find <italic>C</italic> vectors of <italic>p</italic>-values; the <italic>i</italic>-th vector corresponds to the comparison between cluster <italic>i</italic> and all of the other cells. For the sake of simplicity, we then include an equal number of markers for each cluster to create a set of markers for the clustering.</p>
      <p>For example, we consider the classification error rate when the marker list consists of the three genes with the smallest <italic>p</italic>-values from each cluster (with duplicates removed). As previously mentioned, this is a vast oversimplification of a tough problem - how to merge these lists of <italic>p</italic>-values in an optimal way, making sure that we have good representation of each cluster - but it allows for us to quickly and easily compare the methods that we present here. (Note that we would probably want to choose more markers for a cluster in which all <italic>p</italic>-values were large - we probably need more coordinates to distinguish this cluster from all of the others, even if those coordinates are not particularly informative. Thus, setting a <italic>p</italic>-value threshold could potentially perform worse than the method outlined here, as we may not select any markers for a certain cluster with a thresholding method.) The process of merging lists of <italic>p</italic>-values is left for future work.</p>
      <p>For the elastic nets method, which selects one list of markers as optimal (without giving a score for all of the markers), we apply a similar strategy to approximate selecting a small number of markers. In particular, we choose an equal number of markers with the highest score for each cluster until we run out of markers to select. For example, when attempting to select 20 markers per cluster, we may include the top 20 markers for one cluster and all 18 of the markers that are selected for a different cluster.</p>
    </sec>
    <sec id="Sec36">
      <title>Ranking the performance of the methods in Fig. 2</title>
      <p>The numbers in the cells of the tables in Fig. <xref rid="Fig2" ref-type="fig">2</xref> are meant to provide an approximate ranking of the marker selection methods as different numbers of markers are selected. The supervised classification metrics are ranked separately from the unsupervised clustering metrics; that is, each column in a table in Fig. <xref rid="Fig2" ref-type="fig">2</xref> contains <italic>two</italic> rankings of the marker selection methods. A rank of 1 corresponds to the best method; larger rank values indicate worse performance.</p>
      <p>Each rank is meant to capture the results of multiple evaluation metrics when the methods select a range of markers (six metrics for the classification table cells and three metrics for the clustering table cells - see Table <xref rid="Tab3" ref-type="table">3</xref>; the ranges of numbers of markers appear in the first row of each table in Fig. <xref rid="Fig2" ref-type="fig">2</xref>). We thus first calculate a score summarizing the metrics in a range of selected markers. Here, will call these the AoM scores, so called because they are based on an Average of Medians. Briefly, for a fixed method and a fixed range of markers, the AoM score is computed as an average across the relevant evaluation metrics. Each quantity in the average is the median (computed over the fixed range of markers) of the performance of the method according to one of the relevant evaluation metrics. That is, for a fixed method and a fixed range [<italic>a</italic>,<italic>b</italic>] of markers, we define
<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="M133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \text{AoM} = \text{Average}_{m\in \text{metrics}}\left\{ \text{median}_{[a,b]}\{ m(\text{method})\} \right\}  $$ \end{document}</tex-math><mml:math id="M134"><mml:mtext>AoM</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mtext>Average</mml:mtext></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>∈</mml:mo><mml:mtext>metrics</mml:mtext></mml:mrow></mml:msub><mml:mfenced close="}" open="{"><mml:mrow><mml:msub><mml:mrow><mml:mtext>median</mml:mtext></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:msub><mml:mo>{</mml:mo><mml:mi>m</mml:mi><mml:mo>(</mml:mo><mml:mtext>method</mml:mtext><mml:mo>)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3641_Article_Equ11.gif" position="anchor"/></alternatives></disp-formula></p>
      <p>Some considerations about the definitions of the AoM scores is discussed in more detail in the following paragraphs. The ranks are then determined from the relative AoM scores of the different methods.</p>
      <p>We calculate the AoM scores from the data that appear in Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref>, <xref rid="Fig6" ref-type="fig">6</xref>, <xref rid="Fig7" ref-type="fig">7</xref>, <xref rid="Fig8" ref-type="fig">8</xref>, <xref rid="Fig9" ref-type="fig">9</xref> and <xref rid="Fig10" ref-type="fig">10</xref> without any extrapolation (that is, the curves themselves are not considered in these calculations, only the points). For this reason, the AoM scores are quite sensitive to the marker ranges (the bins in the top row of the tables in Fig. <xref rid="Fig2" ref-type="fig">2</xref>): since the performance of the methods improve as more markers are selected, methods that have a data point towards the right edge of one of the marker ranges will generally be rated more favorably by summary calculations. This effect will be especially pronounced when analyzing the leftmost portions of Figs. <xref rid="Fig4" ref-type="fig">4</xref>, <xref rid="Fig5" ref-type="fig">5</xref>, <xref rid="Fig6" ref-type="fig">6</xref>, <xref rid="Fig7" ref-type="fig">7</xref>, <xref rid="Fig8" ref-type="fig">8</xref>, <xref rid="Fig9" ref-type="fig">9</xref> and <xref rid="Fig10" ref-type="fig">10</xref> (that is, when considering small sets of markers), where the curves are generally improving rapidly. To partially account for this issue, the AoM scores are based on the median performance value produced by the methods in a range of markers.</p>
      <p>We choose to average these medians as a way to summarize them in our computation of the AoM scores. This is mainly due to the fact that we want each evaluation metric to contribute equally to the summary score. For example, for a fixed method, if one evaluation metric produces poor results while the other metrics are okay, we still want to include information about this outlier in our AoM score. As discussed previously, the evaluation metrics all produce different ways of examining the information contained in a set of markers; it is thus a warning sign if even one metric is poor. The evaluation metrics are on the same scale (all produce values from 0 to 1, with larger values indicating better performance), so standardization is not needed.</p>
      <p>We report ranks in Fig. <xref rid="Fig2" ref-type="fig">2</xref> (rather than the AoM scores) since the values of these AoM scores are relatively uninformative and difficult to read (for example, a difference of 0.02 between two AoM scores is quite large - it represents a 2% difference between the two methods). The actual AoM values can be found in the data in the GitHub repository related to this paper (<ext-link ext-link-type="uri" xlink:href="https://github.com/ahsv/marker-selection-code">https://github.com/ahsv/marker-selection-code</ext-link>).</p>
    </sec>
    <sec id="Sec37">
      <title>Generating synthetic data based on scRNA-seq data</title>
      <p>In order to generate synthetic data that is made to look like an experimental droplet-based scRNA-seq data set, we use the Splat method from the R Splatter package (version 1.6.1) [<xref ref-type="bibr" rid="CR26">26</xref>] in R version 3.5.0.</p>
      <p>We use the data set consisting of purified (CD19+) B cells from [<xref ref-type="bibr" rid="CR2">2</xref>] in order to estimate the Splat simulation parameters. In [<xref ref-type="bibr" rid="CR2">2</xref>], the authors analyzed this dataset and saw only one cluster, suggesting that it consists mostly of one cell type. We have also combined it with the full <sc>ZHENGFULL</sc> dataset from [<xref ref-type="bibr" rid="CR2">2</xref>] (see the descriptions of the experimental data sets) and observed good overlap with the cluster that the authors identified as B cells in <sc>ZHENGFULL</sc> when looking at a two dimensional UMAP visualization. This overlap appears in Additional file <xref rid="MOESM1" ref-type="media">1</xref>, Figure 18.</p>
      <p>Testing with Splatter showed that including dropout in the Splat simulation resulted in a simulated data set with a higher fraction of entries that are 0 than the original dataset. On the other hand, not including dropout resulted in similar fractions of entries that are 0 in the simulated and original datasets. Taking into account the fact that the Splat dropout randomly sets entries to 0 regardless of the size of those entries (a practice that we would argue is an unrealistic representation of actual dropout), we do not include additional dropout in our Splat simulations.</p>
      <p>In the Splat method, differential expression is simulated by generating a multiplicative factor for each gene that is applied to the gene mean before cell counts are created - a factor of 1 means that the gene is not differentially expressed. These multiplicative factors come from a lognormal distribution with location 0.1 and scale 0.4 - the default values in the Splatter package. We have not attempted to tweak these default parameters in this work. Using the default parameters, many of the “differentially expressed” genes have a differential expression multiplier that is between 0.9 and 1.1; for these genes, the gene mean is barely different between the two clusters. This creates a significant number of differentially expressed genes that are difficult to detect.</p>
      <p>In our synthetic data sets, we ask for Spatter to simulate two groups: 10% of the genes in the first group are differentially expressed (i.e. have a differential expression multiplier not equal to 1) and none of the genes in the second group are differentially expressed. In this way, all differentially expressed genes can be considered to be marker genes for the first group - there are no overlaps between markers for the first and second groups. The direction of differential expression is randomly determined for each gene. Since the differentially expressed genes are chosen at random, this means that many of the genes that are labeled as differentially expressed in the output data show low expression levels (often they are expressed in less than 10 cells).</p>
      <p>As discussed in the simulated data results, we create 20 different simulated data sets from the CD19+ B cells dataset. See Fig. <xref rid="Fig11" ref-type="fig">11</xref> for a diagramme of the set-up. For all 20 simulated data sets, we simulate 5000 cells and the same number of genes that we input. The first 10 data sets are created by using the full (unfiltered) information from 10 random samples of 5000 cells from the B cell data set. This procedure results in synthetic data sets that consist of 5000 cells and about 12000 genes (the number of nonzero genes depends on the subsample). We label our results on these data sets under the heading “all genes used for simulation.”</p>
      <p>To attempt to mitigate the issue of extremely similar gene means between the two clusters in some of the “differentially expressed” genes, we filter the genes of the simulated data via the method introduced in [<xref ref-type="bibr" rid="CR1">1</xref>]: namely, place the genes in 20 bins based on their mean expression levels and select the genes with the highest dispersion from each bin. Using this method, we select the top 5000 most variable genes from the simulated data and we then use only these genes for marker selection. In the figures, we report these data under the heading “filtering after simulation.”</p>
      <p>This type of gene filtering is also common in the literature, and we thought the affect of filtering on marker selection deserved further consideration. Thus, the second 10 simulated data sets are created by using only the top 5000 most variable genes in the original data as the input to Splatter. In this way, we are forcing the differentially expressed genes to look like genes that were originally highly variable. The results on these data are labeled “filtering before simulation.”</p>
      <p>We again use the cell_ranger flavor of the filter_genes_dispersion function in the scanpy Python package for all variable gene selection. Occasionally, this results in only 4999 genes selected; in those cases, we consider 4999 genes (rather than 5000) in the filtered data sets. See the scripts and notebooks on our GitHub repository (link in the data availability statement) for precise information about when this occurred.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary information</title>
    <sec id="Sec38">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="12859_2020_3641_MOESM1_ESM.pdf">
            <caption>
              <p><bold>Additional file 1</bold> Supplementary figures. This file (available in pdf format) contains figures that are supplementary to the data presented in this manuscript. These figures include:</p>
              <p>Plots of all supervised clustering metrics (see Table <xref rid="Tab1" ref-type="table">1</xref>) for all methods (including edgeRdet, MASTdet, and random marker selection when the relevant data was collected) on all four experimental data sets generated using both the NCC and RFC.</p>
              <p>Plots of the unsupervised clustering metrics (see Table <xref rid="Tab1" ref-type="table">1</xref>) for all methods (including edgeRdet, MASTdet, and random marker selection when the relevant data was collected) on the <sc>Zeisel</sc>, <sc>Paul</sc>, and <sc>ZhengFilt</sc> data sets.</p>
              <p>A visualization of the variance in the classification error rate when using the random forests classifier (RFC).</p>
              <p>Plots of the data that were used to choose the value of <italic>k</italic> (see the discussion on the selection of Louvain parameters) that was used to compute the unsupervised clustering metrics on the <sc>Zeisel</sc> and <sc>Paul</sc> data sets.</p>
              <p>A comparison of UMAP plots of the <sc>ZhengFull</sc> data set when labeled by (a) the biologically motivated bulk labels that were used as the “ground truth” cell types for marker selection in this manuscript, and (b) a Louvain clustering that was generated for this work. The Louvain clustering in (b) was used to guide the selection of <italic>k</italic> (see the discussion on the selection of Louvain parameters) to compute the unsupervised clustering metrics on the <sc>ZhengFilt</sc> data set.</p>
              <p>A UMAP plot of the purified CD19+ B cell data set that was used to generate the <xref rid="Sec16" ref-type="sec">Simulated data illuminates the precise performance characteristics of marker selection methods</xref> in this work combined with the <sc>ZhengFull</sc> data set.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>scRNA-seq</term>
        <def>
          <p>Single-cell RNA sequencing</p>
        </def>
      </def-item>
      <def-item>
        <term>UMI</term>
        <def>
          <p>Unique molecular identifier</p>
        </def>
      </def-item>
      <def-item>
        <term>FACS</term>
        <def>
          <p>Fluorescence-activated cell sorting</p>
        </def>
      </def-item>
      <def-item>
        <term>PCA</term>
        <def>
          <p>Principal component analysis (for dimensionality reduction)</p>
        </def>
      </def-item>
      <def-item>
        <term>UMAP</term>
        <def>
          <p>Uniform manifold approximation and projection (for dimensionality reduction)</p>
        </def>
      </def-item>
      <def-item>
        <term>Log. Reg</term>
        <def>
          <p>Logistic regression. <italic>Marker selection evaluation measures</italic></p>
        </def>
      </def-item>
      <def-item>
        <term>NCC</term>
        <def>
          <p>Nearest centroids classifier</p>
        </def>
      </def-item>
      <def-item>
        <term>RFC</term>
        <def>
          <p>Random forest classifier</p>
        </def>
      </def-item>
      <def-item>
        <term>MCC</term>
        <def>
          <p>Matthews correlation coefficient</p>
        </def>
      </def-item>
      <def-item>
        <term>ARI</term>
        <def>
          <p>Adjusted Rand index</p>
        </def>
      </def-item>
      <def-item>
        <term>AMI</term>
        <def>
          <p>Adjusted mutual information</p>
        </def>
      </def-item>
      <def-item>
        <term>FMS</term>
        <def>
          <p>Fowlkes-Mallows score</p>
        </def>
      </def-item>
      <def-item>
        <term>TPR</term>
        <def>
          <p>True positive rate</p>
        </def>
      </def-item>
      <def-item>
        <term>FPR</term>
        <def>
          <p>False positive rate</p>
        </def>
      </def-item>
      <def-item>
        <term>ROC curve</term>
        <def>
          <p>Receiver operating characteristic curve</p>
        </def>
      </def-item>
      <def-item>
        <term>AoM</term>
        <def>
          <p>Average of medians</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p>Software is available for download at <ext-link ext-link-type="uri" xlink:href="https://github.com/ahsv/RankCorr">https://github.com/ahsv/RankCorr</ext-link>.</p>
    </fn>
    <fn id="Fn2">
      <label>2</label>
      <p>Recall that <italic>X</italic><sub><italic>j</italic></sub> represents the counts of gene <italic>j</italic> across the entire population of cells. Since the genes are ranked separately, a count of 0 will be given a different rank in each gene.</p>
    </fn>
    <fn id="Fn3">
      <label>3</label>
      <p>It may be possible to filter so as to retain only the interesting genes while reducing the size of the data set (this is the goal, for example, of the different filtering and preprocessing schemes for marker selection in the Seurat package; see the description of Seurat in the marker selection algorithms); it is important to carefully consider the effects of these preprocessing choices on the final results.</p>
    </fn>
    <fn id="Fn4">
      <label>4</label>
      <p>In the usual case with noisy experimental data, <italic>v</italic> has a unique largest entry, and thus <inline-formula id="IEq38"><alternatives><tex-math id="M135">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\tilde {v}$\end{document}</tex-math><mml:math id="M136"><mml:mover accent="true"><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2020_3641_Article_IEq38.gif"/></alternatives></inline-formula> will have one nonzero entry so that it is pointing along one of the coordinate axes.</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Alexander H.S. Vargo and Anna C. Gilbert contributed equally to this work.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary information</title>
    <p><bold>Supplementary information</bold> accompanies this paper at 10.1186/s12859-020-03641-z.</p>
  </sec>
  <ack>
    <p>The authors wish to thank Umang Varma for contributing his valuable programming experience and his mathematical insights. Thanks also to the members of the Michigan Center for Single-Cell Genomic Data Analytics for their advice and feedback. Special thanks to Jun Li for his ideas, biological insights, and comments during the preparation of this manuscript. Also thanks to Justin Colacino for his biology knowledge and ideas, as well as to Xiang Zhou and his students Lulu Shang and Shiquan Sun for their comments on a previous draft of this work. This research was supported in part through computational resources and services provided by Advanced Research Computing at the University of Michigan, Ann Arbor.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>AV and ACG conceived of this research. AV carried out the computational analysis and mathematical research, supervised and guided by ACG. AV wrote and revised the manuscript. ACG edited and provided revisions for the manuscript. All author(s) approved the manuscript for publication.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>ACG and AV were supported by the Michigan Institute for Data Science and the Chan Zuckerberg Initiative.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The experimental data sets analysed during the current study are publicly available. They can be found in the following locations:</p>
    <p>• <sc>Zeisel</sc> is found on the website of the authors of [<xref ref-type="bibr" rid="CR24">24</xref>]: <ext-link ext-link-type="uri" xlink:href="http://linnarssonlab.org/cortex/">http://linnarssonlab.org/cortex/</ext-link>. The data are also available on the GEO (GSE60361).</p>
    <p>• <sc>Paul</sc> is found in the scanpy Python package - we consider the version obtained by calling the scanpy.api.datasets.paul15() function. The clustering is included in the resulting Anndata object under the heading paul15_clusters. The data are also available on the GEO (GSE72857).</p>
    <p>• <sc>ZhengFull</sc> and <sc>ZhengFilt</sc> are (subsets) of the data sets introduced in [<xref ref-type="bibr" rid="CR2">2</xref>]. The full data set can be found on the 10x website (<ext-link ext-link-type="uri" xlink:href="https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/fresh_68k_pbmc_donor_a">https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/fresh_68k_pbmc_donor_a</ext-link>) as well as on the SRA (SRP073767). The biologically motivated bulk labels can be found on the scanpy_usage GitHub repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/theislab/scanpy_usage/blob/master/170503_zheng17/data/zheng17_bulk_lables.txt">https://github.com/theislab/scanpy_usage/blob/master/170503_zheng17/data/zheng17_bulk_lables.txt</ext-link>(we use commit 54607f0).</p>
    <p>• <sc>10xMouse</sc> is available for download on the 10x website (<ext-link ext-link-type="uri" xlink:href="https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.3.0/1M_neurons">https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.3.0/1M_neurons</ext-link>). The clustering analysed in this manuscript can be found on the scanpy_usage GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells">https://github.com/theislab/scanpy_usage/tree/master/170522_visualizing_one_million_cells</ext-link>; we consider commit ba6eb85)</p>
    <p>The synthetic data analysed in this manuscript is based on the CD19+ B cell data set from [<xref ref-type="bibr" rid="CR2">2</xref>]. This B cell data set can be found on the 10x website at <ext-link ext-link-type="uri" xlink:href="https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/b_cells">https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/b_cells</ext-link>. The synthetic data sets themselves are available from the author on request.</p>
    <p>All scripts that were used for marker selection and data processing (including implementations of <sc>Spa</sc> and <sc>RankCorr</sc>) can be found at the GitHub repository located at <ext-link ext-link-type="uri" xlink:href="https://github.com/ahsv/marker-selection-code">https://github.com/ahsv/marker-selection-code</ext-link>. These scripts also include Jupyter notebooks that produce interactive versions of the figures in this manuscript (allowing for the user to zoom in, remove some of the curves, and more). A streamlined implementation of <sc>RankCorr</sc> (with documentation) can additionally be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/ahsv/RankCorr">https://github.com/ahsv/RankCorr</ext-link>.</p>
  </notes>
  <notes id="FPar1">
    <title>Ethics approval and consent to participate</title>
    <p>Not applicable.</p>
  </notes>
  <notes id="FPar2">
    <title>Consent for publication</title>
    <p>Not applicable.</p>
  </notes>
  <notes id="FPar3" notes-type="COI-statement">
    <title>Competing interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Macosko</surname>
            <given-names>EZ</given-names>
          </name>
          <name>
            <surname>Basu</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Satija</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Nemesh</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shekhar</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Goldman</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Tirosh</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Bialas</surname>
            <given-names>AR</given-names>
          </name>
          <name>
            <surname>Kamitaki</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Martersteck</surname>
            <given-names>EM</given-names>
          </name>
          <name>
            <surname>Trombetta</surname>
            <given-names>JJ</given-names>
          </name>
          <name>
            <surname>Weitz</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Sanes</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Shalek</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Regev</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>McCarroll</surname>
            <given-names>SA</given-names>
          </name>
        </person-group>
        <article-title>Highly parallel genome-wide expression profiling of individual cells using nanoliter droplets</article-title>
        <source>Cell</source>
        <year>2015</year>
        <volume>161</volume>
        <issue>5</issue>
        <fpage>1202</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2015.05.002</pub-id>
        <pub-id pub-id-type="pmid">26000488</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zheng</surname>
            <given-names>GXY</given-names>
          </name>
          <name>
            <surname>Terry</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Belgrader</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Ryvkin</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Bent</surname>
            <given-names>ZW</given-names>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Ziraldo</surname>
            <given-names>SB</given-names>
          </name>
          <name>
            <surname>Wheeler</surname>
            <given-names>TD</given-names>
          </name>
          <name>
            <surname>McDermott</surname>
            <given-names>GP</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gregory</surname>
            <given-names>MT</given-names>
          </name>
          <name>
            <surname>Shuga</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Montesclaros</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Underwood</surname>
            <given-names>JG</given-names>
          </name>
          <name>
            <surname>Masquelier</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Nishimura</surname>
            <given-names>SY</given-names>
          </name>
          <name>
            <surname>Schnall-Levin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wyatt</surname>
            <given-names>PW</given-names>
          </name>
          <name>
            <surname>Hindson</surname>
            <given-names>CM</given-names>
          </name>
          <name>
            <surname>Bharadwaj</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Wong</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ness</surname>
            <given-names>KD</given-names>
          </name>
          <name>
            <surname>Beppu</surname>
            <given-names>LW</given-names>
          </name>
          <name>
            <surname>Deeg</surname>
            <given-names>HJ</given-names>
          </name>
          <name>
            <surname>McFarland</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Loeb</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Valente</surname>
            <given-names>WJ</given-names>
          </name>
          <name>
            <surname>Ericson</surname>
            <given-names>NG</given-names>
          </name>
          <name>
            <surname>Stevens</surname>
            <given-names>EA</given-names>
          </name>
          <name>
            <surname>Radich</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Mikkelsen</surname>
            <given-names>TS</given-names>
          </name>
          <name>
            <surname>Hindson</surname>
            <given-names>BJ</given-names>
          </name>
          <name>
            <surname>Bielas</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <article-title>Massively parallel digital transcriptional profiling of single cells</article-title>
        <source>Nat Commun</source>
        <year>2017</year>
        <volume>8</volume>
        <fpage>14049</fpage>
        <pub-id pub-id-type="doi">10.1038/ncomms14049</pub-id>
        <pub-id pub-id-type="pmid">28091601</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3</label>
      <mixed-citation publication-type="other">10x Genomics. 1.3 Million Brain Cells from E18 Mice. <ext-link ext-link-type="uri" xlink:href="https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.3.0/1M_neurons">https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.3.0/1M_neurons</ext-link>. Accessed 22 Sept 2018.</mixed-citation>
    </ref>
    <ref id="CR4">
      <label>4</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kharchenko</surname>
            <given-names>PV</given-names>
          </name>
          <name>
            <surname>Silberstein</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Scadden</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>Bayesian approach to single-cell differential expression analysis</article-title>
        <source>Nat Methods</source>
        <year>2014</year>
        <volume>11</volume>
        <issue>7</issue>
        <fpage>740</fpage>
        <lpage>2</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2967</pub-id>
        <pub-id pub-id-type="pmid">24836921</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wolf</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Angerer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>SCANPY: large-scale single-cell gene expression data analysis</article-title>
        <source>Genome Biol</source>
        <year>2018</year>
        <volume>19</volume>
        <fpage>15</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-017-1382-0</pub-id>
        <pub-id pub-id-type="pmid">29409532</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Butler</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Hoffman</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Smibert</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Papalexi</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Satija</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Integrating single-cell transcriptomic data across different conditions, technologies, and species</article-title>
        <source>Nat Biotechnol</source>
        <year>2018</year>
        <volume>36</volume>
        <issue>5</issue>
        <fpage>411</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1038/nbt.4096</pub-id>
        <pub-id pub-id-type="pmid">29608179</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Soneson</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Robinson</surname>
            <given-names>MD</given-names>
          </name>
        </person-group>
        <article-title>Bias, robustness and scalability in single-cell differential expression analysis</article-title>
        <source>Nat Methods</source>
        <year>2018</year>
        <volume>15</volume>
        <issue>4</issue>
        <fpage>255</fpage>
        <lpage>61</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.4612</pub-id>
        <pub-id pub-id-type="pmid">29481549</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Friedman</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <source>The elements of statistical learning</source>
        <year>2009</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer-Verlag</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Brown</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Pocock</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>M-J</given-names>
          </name>
          <name>
            <surname>Luján</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Conditional likelihood maximisation: A unifying framework for information theoretic feature selection</article-title>
        <source>J Mach Learn Res</source>
        <year>2012</year>
        <volume>13</volume>
        <fpage>27</fpage>
        <lpage>66</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Das</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Kempe</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection</article-title>
        <source>Proceedings of the 28th International Conference on International Conference on Machine Learning. ICML’11</source>
        <year>2011</year>
        <publisher-loc>USA</publisher-loc>
        <publisher-name>Omnipress</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>The Lasso Method for variable selection in the Cox model</article-title>
        <source>Stat Med</source>
        <year>1997</year>
        <volume>16</volume>
        <issue>4</issue>
        <fpage>385</fpage>
        <lpage>95</lpage>
        <pub-id pub-id-type="doi">10.1002/(SICI)1097-0258(19970228)16:4&lt;385::AID-SIM380&gt;3.0.CO;2-3</pub-id>
        <pub-id pub-id-type="pmid">9044528</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zou</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Regularization and variable selection via the elastic net</article-title>
        <source>J R Stat Soc Ser B Stat Methodol</source>
        <year>2005</year>
        <volume>67</volume>
        <issue>2</issue>
        <fpage>301</fpage>
        <lpage>20</lpage>
        <pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00503.x</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Mark</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Raskutti</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Willett</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Graph-based regularization for regression problems with highly-correlated designs</article-title>
        <source>2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)</source>
        <year>2018</year>
        <publisher-loc>Piscataway</publisher-loc>
        <publisher-name>IEEE</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Conrad</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Genzel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Cvetkovic</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Wulkow</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Leichtle</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Vybiral</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Kutyniok</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Schütte</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>Sparse proteomics analysis – a compressed sensing-based approach for feature selection and classification of high-dimensional proteomics mass spectrometry data</article-title>
        <source>BMC Bioinforma</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>160</fpage>
        <pub-id pub-id-type="doi">10.1186/s12859-017-1565-4</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Plan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Vershynin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach</article-title>
        <source>IEEE Trans Inf Theory</source>
        <year>2013</year>
        <volume>59</volume>
        <issue>1</issue>
        <fpage>482</fpage>
        <lpage>94</lpage>
        <pub-id pub-id-type="doi">10.1109/TIT.2012.2207945</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16</label>
      <mixed-citation publication-type="other">Ibrahim MM, Kramann R. genesorter: Feature ranking in clustered single cell data. bioRxiv. 2019;:676379. https://doi.org/10.1101/676379. {https://www.biorxiv.org/content/early/2019/09/02/676379.full.pdf}.</mixed-citation>
    </ref>
    <ref id="CR17">
      <label>17</label>
      <mixed-citation publication-type="other">Delaney C, Schnell A, Cammarata LV, Yao-Smith A, Regev A, Kuchroo VK, Singer M. Combinatorial prediction of marker panels from single-cell transcriptomic data. Mol Syst Biol. 2019; 15(10):9005. <pub-id pub-id-type="doi">10.15252/msb.20199005</pub-id>. <ext-link ext-link-type="uri" xlink:href="https://www.embopress.org/doi/pdf/10.15252/msb.20199005">https://www.embopress.org/doi/pdf/10.15252/msb.20199005</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR18">
      <label>18</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Potter</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Whitsett</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Xu</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>SINCERA: A pipeline for single-cell RNA-seq profiling analysis</article-title>
        <source>PLOS Comput Biol</source>
        <year>2015</year>
        <volume>11</volume>
        <issue>11</issue>
        <fpage>1004575</fpage>
        <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004575</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Wolfgruber</surname>
            <given-names>TK</given-names>
          </name>
          <name>
            <surname>Tasato</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Arisdakessian</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Garmire</surname>
            <given-names>DG</given-names>
          </name>
          <name>
            <surname>Garmire</surname>
            <given-names>LX</given-names>
          </name>
        </person-group>
        <article-title>Granatum: a graphical single-cell RNA-seq analysis pipeline for genomics scientists</article-title>
        <source>Genome Med</source>
        <year>2017</year>
        <volume>9</volume>
        <fpage>108</fpage>
        <pub-id pub-id-type="doi">10.1186/s13073-017-0492-3</pub-id>
        <pub-id pub-id-type="pmid">29202807</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20</label>
      <mixed-citation publication-type="other">Dumitrascu B, Villar S, Mixon DG, Engelhardt BE. Optimal marker gene selection for cell type discrimination in single cell analyses. bioRxiv. 2019:599654. https://doi.org/10.1101/599654. {https://www.biorxiv.org/content/early/2019/04/04/599654.full.pdf}.</mixed-citation>
    </ref>
    <ref id="CR21">
      <label>21</label>
      <mixed-citation publication-type="other">Sengupta D, Rayan NA, Lim M, Lim B, Prabhakar S. Fast, scalable and accurate differential expression analysis for single cells. bioRxiv. 2016:049734. https://doi.org/10.1101/049734. {https://www.biorxiv.org/content/early/2016/04/22/049734.full.pdf}.</mixed-citation>
    </ref>
    <ref id="CR22">
      <label>22</label>
      <mixed-citation publication-type="other">scikit-learn developers. Model evaluation: quantifying the quality of predictions. 2019. <ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/modules/model_evaluation.html">https://scikit-learn.org/stable/modules/model_evaluation.html</ext-link>. Accessed 9 May 2019.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23</label>
      <mixed-citation publication-type="other">scikit-learn developers. Clustering performance evaluation. 2019. <ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation">https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation</ext-link>. Accessed 9 May 2019.</mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24</label>
      <mixed-citation publication-type="other">Zeisel A, Muñoz-Manchado AB, Codeluppi S, Lönnerberg P, La Manno G, Juréus A, Marques S, Munguba H, He L, Betsholtz C, Rolny C, Castelo-Branco G, Hjerling-Leffler J, Linnarsson S. Cell types in the mouse cortex and hippocampus revealed by single-cell rna-seq. Science. 2015; 347(6226):1138–42. <pub-id pub-id-type="doi">10.1126/science.aaa1934</pub-id>. <ext-link ext-link-type="uri" xlink:href="http://science.sciencemag.org/content/347/6226/1138.full.pdf">http://science.sciencemag.org/content/347/6226/1138.full.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paul</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Arkin</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Giladi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Jaitin</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Kenigsberg</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Keren-Shaul</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Winter</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Lara-Astiaso</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Gury</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Weiner</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>David</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Cohen</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Lauridsen</surname>
            <given-names>FKB</given-names>
          </name>
          <name>
            <surname>Haas</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Schlitzer</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mildner</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Ginhoux</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Jung</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Trumpp</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Porse</surname>
            <given-names>BT</given-names>
          </name>
          <name>
            <surname>Tanay</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Amit</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Transcriptional heterogeneity and lineage commitment in myeloid progenitors</article-title>
        <source>Cell</source>
        <year>2015</year>
        <volume>163</volume>
        <issue>7</issue>
        <fpage>1663</fpage>
        <lpage>77</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2015.11.013</pub-id>
        <pub-id pub-id-type="pmid">26627738</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zappia</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Phipson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Oshlack</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Splatter: simulation of single-cell RNA sequencing data</article-title>
        <source>Genome Biol</source>
        <year>2017</year>
        <volume>18</volume>
        <fpage>174</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-017-1305-0</pub-id>
        <pub-id pub-id-type="pmid">28899397</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Robinson</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>McCarthy</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Smyth</surname>
            <given-names>GK</given-names>
          </name>
        </person-group>
        <article-title>edgeR: a bioconductor package for differential expression analysis of digital gene expression data</article-title>
        <source>Bioinformatics</source>
        <year>2009</year>
        <volume>26</volume>
        <issue>1</issue>
        <fpage>139</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1093/bioinformatics/btp616</pub-id>
        <pub-id pub-id-type="pmid">19910308</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Finak</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>McDavid</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Yajima</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Deng</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Gersuk</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Shalek</surname>
            <given-names>AK</given-names>
          </name>
          <name>
            <surname>Slichter</surname>
            <given-names>CK</given-names>
          </name>
          <name>
            <surname>Miller</surname>
            <given-names>HW</given-names>
          </name>
          <name>
            <surname>McElrath</surname>
            <given-names>MJ</given-names>
          </name>
          <name>
            <surname>Prlic</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Linsley</surname>
            <given-names>PS</given-names>
          </name>
          <name>
            <surname>Gottardo</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>MAST: a flexible statistical framework for assessing transcriptional changes and characterizing heterogeneity in single-cell RNA sequencing data</article-title>
        <source>Genome Biol</source>
        <year>2015</year>
        <volume>16</volume>
        <fpage>278</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-015-0844-5</pub-id>
        <pub-id pub-id-type="pmid">26653891</pub-id>
      </element-citation>
    </ref>
    <ref id="CR29">
      <label>29</label>
      <mixed-citation publication-type="other">Lopez R, Regier J, Cole MB, Jordan M, Yosef N. Bayesian inference for a generative model of transcriptome profiles from single-cell rna sequencing. bioRxiv. 2018:292037. https://doi.org/10.1101/292037. {https://www.biorxiv.org/content/early/2018/03/30/292037.full.pdf}.</mixed-citation>
    </ref>
    <ref id="CR30">
      <label>30</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Pedregosa</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Varoquaux</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Gramfort</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Michel</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Thirion</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Grisel</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Blondel</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Prettenhofer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Weiss</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Dubourg</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Vanderplas</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Passos</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Cournapeau</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Brucher</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Perrot</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Duchesnay</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Scikit-learn: Machine learning in Python</article-title>
        <source>J Mach Learn Res</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>2825</fpage>
        <lpage>30</lpage>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31</label>
      <mixed-citation publication-type="other">Ntranos V, Yi L, Melsted P, Pachter L. Identification of transcriptional signatures for cell types from single-cell rna-seq. bioRxiv. 2018:258566. https://doi.org/10.1101/258566. https://www.biorxiv.org/content/early/2018/02/14/258566.full.pdf.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32</label>
      <mixed-citation publication-type="other">Soneson C, Robinson MD. Towards unified quality verification of synthetic count data with countsimQC. Bioinformatics. 2017; 34(4):691–2. <pub-id pub-id-type="doi">10.1093/bioinformatics/btx631</pub-id>. <ext-link ext-link-type="uri" xlink:href="http://oup.prod.sis.lan/bioinformatics/article-pdf/34/4/691/25117229/btx631.pdf">http://oup.prod.sis.lan/bioinformatics/article-pdf/34/4/691/25117229/btx631.pdf</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33</label>
      <mixed-citation publication-type="other">Crowell HL, Soneson C, Germain P-L, Calini D, Collin L, Raposo C, Malhotra D, Robinson MD. On the discovery of population-specific state transitions from multi-sample multi-condition single-cell rna sequencing data. bioRxiv. 2019:713412. https://doi.org/10.1101/713412. {https://www.biorxiv.org/content/early/2019/07/26/713412.full.pdf}.</mixed-citation>
    </ref>
    <ref id="CR34">
      <label>34</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>WV</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>An accurate and robust imputation method scImpute for single-cell RNA-seq data</article-title>
        <source>Nat Commun</source>
        <year>2018</year>
        <volume>9</volume>
        <fpage>997</fpage>
        <pub-id pub-id-type="doi">10.1038/s41467-018-03405-7</pub-id>
        <pub-id pub-id-type="pmid">29520097</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>van Dijk</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Nainys</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yim</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kathail</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Carr</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Burdziak</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Moon</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Chaffer</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Pattabiraman</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bierie</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Mazutis</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Wolf</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Krishnaswamy</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Pe’er</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Recovering gene interactions from single-cell data using data diffusion</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>174</volume>
        <issue>3</issue>
        <fpage>716</fpage>
        <lpage>72927</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2018.05.061</pub-id>
        <pub-id pub-id-type="pmid">29961576</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36</label>
      <mixed-citation publication-type="other">Wagner F, Yan Y, Yanai I. K-nearest neighbor smoothing for high-throughput single-cell rna-seq data. bioRxiv. 2019:217737. https://doi.org/10.1101/217737. {https://www.biorxiv.org/content/early/2018/04/09/217737.full.pdf}.</mixed-citation>
    </ref>
    <ref id="CR37">
      <label>37</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Amaldi</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Kann</surname>
            <given-names>V</given-names>
          </name>
        </person-group>
        <article-title>On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems</article-title>
        <source>Theor Comput Sci</source>
        <year>1998</year>
        <volume>209</volume>
        <issue>1</issue>
        <fpage>237</fpage>
        <lpage>60</lpage>
        <pub-id pub-id-type="doi">10.1016/S0304-3975(97)00115-1</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38</label>
      <mixed-citation publication-type="other">Genzel M. Sparse proteomics analysis: Toward a mathematical foundation of feature selection and disease classification. Master’s thesis, Technische Universität Berlin, Berlin, Germany. 2015.</mixed-citation>
    </ref>
    <ref id="CR39">
      <label>39</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Yi</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Jin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Xing</surname>
            <given-names>EP</given-names>
          </name>
          <name>
            <surname>Jebara</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Efficient algorithms for robust one-bit compressive sensing</article-title>
        <source>Proceedings of the 31st International Conference on Machine Learning. Proceedings of Machine Learning Research</source>
        <year>2014</year>
        <publisher-loc>Bejing, China</publisher-loc>
        <publisher-name>PMLR</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ai</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Lapanowski</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Plan</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Vershynin</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>One-bit compressed sensing with non-gaussian measurements</article-title>
        <source>Linear Algebra Appl</source>
        <year>2014</year>
        <volume>441</volume>
        <fpage>222</fpage>
        <lpage>39</lpage>
        <pub-id pub-id-type="doi">10.1016/j.laa.2013.04.002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grün</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Kester</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>van Oudenaarden</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Validation of noise models for single-cell transcriptomics</article-title>
        <source>Nat Methods</source>
        <year>2014</year>
        <volume>11</volume>
        <issue>6</issue>
        <fpage>637</fpage>
        <lpage>40</lpage>
        <pub-id pub-id-type="doi">10.1038/nmeth.2930</pub-id>
        <pub-id pub-id-type="pmid">24747814</pub-id>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Love</surname>
            <given-names>MI</given-names>
          </name>
          <name>
            <surname>Huber</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Anders</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2</article-title>
        <source>Genome Biol</source>
        <year>2014</year>
        <volume>15</volume>
        <issue>12</issue>
        <fpage>550</fpage>
        <pub-id pub-id-type="doi">10.1186/s13059-014-0550-8</pub-id>
        <pub-id pub-id-type="pmid">25516281</pub-id>
      </element-citation>
    </ref>
    <ref id="CR43">
      <label>43</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Levine</surname>
            <given-names>JH</given-names>
          </name>
          <name>
            <surname>Simonds</surname>
            <given-names>EF</given-names>
          </name>
          <name>
            <surname>Bendall</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>KL</given-names>
          </name>
          <name>
            <surname>Amir</surname>
            <given-names>E. -a. D.</given-names>
          </name>
          <name>
            <surname>Tadmor</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Litvin</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Fienberg</surname>
            <given-names>HG</given-names>
          </name>
          <name>
            <surname>Jager</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Zunder</surname>
            <given-names>ER</given-names>
          </name>
          <name>
            <surname>Finck</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gedman</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Radtke</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Downing</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Pe’er</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Nolan</surname>
            <given-names>GP</given-names>
          </name>
        </person-group>
        <article-title>Data-driven phenotypic dissection of AML reveals progenitor-like cells that correlate with prognosis</article-title>
        <source>Cell</source>
        <year>2015</year>
        <volume>162</volume>
        <issue>1</issue>
        <fpage>184</fpage>
        <lpage>97</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cell.2015.05.047</pub-id>
        <pub-id pub-id-type="pmid">26095251</pub-id>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44</label>
      <mixed-citation publication-type="other">Wagner F. The XL-mHG test for gene set enrichment. 2017. 10.7287/peerj.preprints.1962v3. Accessed 07 May 2020.</mixed-citation>
    </ref>
  </ref-list>
</back>
