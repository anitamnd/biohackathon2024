<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Behav Res Methods</journal-id>
    <journal-id journal-id-type="iso-abbrev">Behav Res Methods</journal-id>
    <journal-title-group>
      <journal-title>Behavior Research Methods</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1554-351X</issn>
    <issn pub-type="epub">1554-3528</issn>
    <publisher>
      <publisher-name>Springer US</publisher-name>
      <publisher-loc>New York</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7406525</article-id>
    <article-id pub-id-type="publisher-id">1350</article-id>
    <article-id pub-id-type="doi">10.3758/s13428-020-01350-2</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Speeding up the detection of non-iconic and iconic gestures (SPUDNIG): A toolkit for the automatic detection of hand movements and gestures in video data</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Ripperda</surname>
          <given-names>Jordy</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes" equal-contrib="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9154-7033</contrib-id>
        <name>
          <surname>Drijvers</surname>
          <given-names>Linda</given-names>
        </name>
        <address>
          <email>linda.drijvers@mpi.nl</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Holler</surname>
          <given-names>Judith</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.5590.9</institution-id><institution-id institution-id-type="ISNI">0000000122931605</institution-id><institution>Donders Institute for Brain, Cognition, and Behaviour, </institution><institution>Radboud University, </institution></institution-wrap>Montessorilaan 3, 6525 HR Nijmegen, The Netherlands </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.419550.c</institution-id><institution-id institution-id-type="ISNI">0000 0004 0501 3839</institution-id><institution>Max Planck Institute for Psycholinguistics, </institution></institution-wrap>Wundtlaan 1, 6525 XD Nijmegen, The Netherlands </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>23</day>
      <month>1</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <year>2020</year>
    </pub-date>
    <volume>52</volume>
    <issue>4</issue>
    <fpage>1783</fpage>
    <lpage>1794</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">In human face-to-face communication, speech is frequently accompanied by visual signals, especially communicative hand gestures. Analyzing these visual signals requires detailed manual annotation of video data, which is often a labor-intensive and time-consuming process. To facilitate this process, we here present SPUDNIG (SPeeding Up the Detection of Non-iconic and Iconic Gestures), a tool to automatize the detection and annotation of hand movements in video data. We provide a detailed description of how SPUDNIG detects hand movement initiation and termination, as well as open-source code and a short tutorial on an easy-to-use graphical user interface (GUI) of our tool. We then provide a proof-of-principle and validation of our method by comparing SPUDNIG’s output to manual annotations of gestures by a human coder. While the tool does not entirely eliminate the need of a human coder (e.g., for false positives detection), our results demonstrate that SPUDNIG can detect both iconic and non-iconic gestures with very high accuracy, and could successfully detect all iconic gestures in our validation dataset. Importantly, SPUDNIG’s output can directly be imported into commonly used annotation tools such as ELAN and ANVIL. We therefore believe that SPUDNIG will be highly relevant for researchers studying multimodal communication due to its annotations significantly accelerating the analysis of large video corpora.</p>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>Automatic movement detection</kwd>
      <kwd>Gesture</kwd>
      <kwd>Openpose</kwd>
      <kwd>Annotation</kwd>
      <kwd>Hand</kwd>
      <kwd>Methodology</kwd>
      <kwd>Motion tracking</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>ERC Consolidator</institution>
        </funding-source>
        <award-id>773079</award-id>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Psychonomic Society, Inc. 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Spoken language is mostly used in multimodal, face-to-face contexts. In addition to speech, face-to-face communication involves a plethora of visual articulators, such as manual gestures. Due to their close relation to speech, manual gestures (henceforth: gestures) have often been the focus of multimodal research in the domains of psychology, anthropology, linguistics and neuroscience (Goldin-Meadow, <xref ref-type="bibr" rid="CR12">2005</xref>; Kendon, <xref ref-type="bibr" rid="CR22">2004</xref>; McNeill, <xref ref-type="bibr" rid="CR28">1992</xref>). For example, gestures can be used to refer to objects, events, locations and ideas, and can convey semantic information integral to a speaker’s message (Holler &amp; Beattie, <xref ref-type="bibr" rid="CR15">2003</xref>; Holler &amp; Wilkin, <xref ref-type="bibr" rid="CR17">2009</xref>; Hostetter, <xref ref-type="bibr" rid="CR18">2011</xref>; McNeill, <xref ref-type="bibr" rid="CR28">1992</xref>). Previous work has demonstrated that this information is integrated with the speech signal, processed by the listener, and that it facilitates language comprehension (Drijvers &amp; Özyürek, <xref ref-type="bibr" rid="CR10">2017</xref>; Holler, Shovelton, &amp; Beattie, <xref ref-type="bibr" rid="CR16">2009</xref>; Kelly, Barr, Church, &amp; Lynch, <xref ref-type="bibr" rid="CR19">1999</xref>; Kelly, Kravitz, &amp; Hopkins, <xref ref-type="bibr" rid="CR20">2004</xref>; Kelly et al., <xref ref-type="bibr" rid="CR21">2010</xref>; Ozyurek, <xref ref-type="bibr" rid="CR30">2014</xref>).</p>
    <p id="Par3">One of the main challenges for studies on human communication in face-to-face contexts is the labor-intensive and time-consuming manual annotation of the occurrence and timing of such gestures. These manual analyses are often performed on the basis of pre-defined coding schemes (e.g., Dael, Mortillaro, &amp; Scherer, <xref ref-type="bibr" rid="CR9">2012</xref>; McNeill, <xref ref-type="bibr" rid="CR28">1992</xref>; Zhao &amp; Badler, <xref ref-type="bibr" rid="CR36">2001</xref>), and using annotation tools such as ANVIL (Kipp, <xref ref-type="bibr" rid="CR23">2001</xref>) or ELAN (Wittenburg, Brugman, Russel, Klassmann &amp; Sloetjes, <xref ref-type="bibr" rid="CR34">2006</xref>). These annotation tools log manually made annotation entries time-stamped in relation to the video – an extremely useful step – but they do not speed up or automatize the annotation process itself. Gestures still need to be detected based on the human eye and their begin and end points need to be entered manually. Moreover, annotators need to be trained to perform this procedure, and multiple coders need to be involved to establish inter-rater reliability. This results in an extremely time-consuming process even for individual experiments, and especially so for more large-scale, corpora-based research projects. Facilitating the gesture annotation process through the development of automated techniques thus would advance multimodal communication research significantly.</p>
    <p id="Par4">Recent technological advances have opened up exciting possibilities for the automatic analyses of movement parameters (such as space, motion trajectories, size, distance, and velocity). These automatic analyses are often performed on the basis of device-based optic marker or markerless motion tracking systems, such as Polhemus Liberty (Vermont, USA; <ext-link ext-link-type="uri" xlink:href="http://polhemus.com">http://polhemus.com</ext-link>; Liberty Latus Brochure, 2012), Optotrak (Northern Digital, Waterloo, Canada), Microsoft Kinect (Zhang, <xref ref-type="bibr" rid="CR35">2012</xref>), and Leap Motion (San Francisco, USA; <ext-link ext-link-type="uri" xlink:href="http://leapmotion.com">http://leapmotion.com</ext-link>).</p>
    <p id="Par5">Alternatively, when motion capture systems are unavailable, video-based tracking systems can be used to automatically track movements. These video-based analysis methods include pixel differentiation methods (e.g., Paxton &amp; Dale, <xref ref-type="bibr" rid="CR31">2013</xref>), computer-vision methods relying on deep learning, (e.g., OpenPose; Cao, Hidalgo, Simon, Wei, &amp; Sheikh, <xref ref-type="bibr" rid="CR6">2018</xref>; Cao, Simon, Wei, &amp; Sheikh, <xref ref-type="bibr" rid="CR7">2016</xref>) and Deeplabcut (Mathis et al., <xref ref-type="bibr" rid="CR27">2018</xref>); for an overview and discussion of the different methods see Pouw, Trujillo, &amp; Dixon, <xref ref-type="bibr" rid="CR32">2018</xref>). Recent work has validated that video-based tracking systems perform equally well in estimating gesture characteristics (such as movement peaks) as device-based motion tracking systems (Pouw et al., <xref ref-type="bibr" rid="CR32">2018</xref>).</p>
    <p id="Par6">The decision on what type of motion tracking method is most suitable for a user’s research question is often dependent on the user’s access to such methods. Access can be limited by the cost level of motion capture systems or specific computer-based hardware requirements (e.g., access to a graphics processing unit (GPU), but also due to the requirement of technical expertise to apply existent video-based motion tracking methods (first and foremost, expertise in specific programming languages). Moreover, existing toolkits on (semi-)automatic gesture and movement analyses still require, as a first step, the <italic>manual identification</italic> of the gesture/movement events that need to be analyzed (e.g., Pouw et al., <xref ref-type="bibr" rid="CR32">2018</xref>; Trujillo, Vaitonyte, Simanova, &amp; Özyürek, <xref ref-type="bibr" rid="CR33">2019</xref>), require motion capture data as input for detection of gesture initiation and termination (e.g., by using EPAA, see Hassemer, <xref ref-type="bibr" rid="CR13">2015</xref>), or are not suited to track fine-grained finger movements (e.g., Beugher, Brône, &amp; Goedemé, <xref ref-type="bibr" rid="CR3">2018</xref>).</p>
    <p id="Par7">To overcome these limitations, we here present SPUDNIG (<bold>SP</bold>eeding <bold>U</bold>p the <bold>D</bold>etection of <bold>N</bold>on-iconic and <bold>I</bold>conic <bold>G</bold>estures), a newly developed open-source toolkit including an easy-to-use graphical user interface (GUI) for the automatic detection of hand movements and gestures. Similar to De Beugher et al., (<xref ref-type="bibr" rid="CR3">2018</xref>), we define ‘detection’ as segmenting movement sequences from non-movement sequences (note that <italic>recognition</italic> would involve distinguishing which of these movements are gestures and which of these movements are not gestures). SPUDNIG uses OpenPose as input for continuous, video-based motion tracking of movements and gestures, and subsequently uses observed changes in x/y coordinates of user-defined key points in the body to automatically detect movement initiation and termination. SPUDNIG does not require the use of motion capture hardware devices, nor does it require in-depth technical knowledge to adapt parameters used for gesture detection, or expertise in programming languages. In what follows, we provide a proof-of-principle and validation of our “off-the-shelf” gesture detection method by comparing the output of SPUDNIG to manually annotated gesture data in the ELAN annotation tool. We will test its performance for both non-iconic and iconic gestures. Please note that SPUDNIG, in its current form, was not designed to entirely eliminate the need of a human coder. Its current version is not able to <italic>recognize</italic> gestural from non-gestural movement, but is able to <italic>detect</italic> movement initiation and termination. Our aim here is to evaluate its overlap with gestures identified by a trained human coder, and to provide a toolkit that significantly reduces the need of a human coder, limiting it to the removal of false positives (i.e., non-gestural movements) at the end of the machine-based annotation process.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>SPUDNIG: speeding up the detection of non-iconic and iconic gestures</title>
      <sec id="Sec4">
        <title>OpenPose</title>
        <p id="Par8">SPUDNIG is partially based on output created by OpenPose (<ext-link ext-link-type="uri" xlink:href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">https://github.com/CMU-Perceptual-Computing-Lab/openpose</ext-link>). OpenPose is a video-based motion tracking method that uses computer-vision methods to estimate body parts from 2D frames (e.g., body, face, and hands, see: Cao et al., <xref ref-type="bibr" rid="CR7">2016</xref>, <xref ref-type="bibr" rid="CR6">2018</xref>). Specifically, it uses convolutional neural networks as a deep learning method to predict the location of body parts and the occurrence of body part motion. This makes OpenPose more robust to background noise than other video-based tracking methods, such as pixel differentiation, and therefore perhaps more suited to the study of human communication in face-to-face contexts. Note that other methods, such as AlphaPose (Fang et al., <xref ref-type="bibr" rid="CR11">2017</xref>), have demonstrated to outperform OpenPose at the task of pose estimation. However, AlphaPose can only estimate body pose, and cannot detect key points in the hands. As SPUDNIG requires the detection of fine-grained hand and finger movements, OpenPose was chosen as the basis for SPUDNIG.</p>
      </sec>
      <sec id="Sec5">
        <title>Movement/no-movement detection</title>
        <p id="Par9">Per frame, OpenPose provides x- and y-coordinates of 25 key points divided over the body, and 21 key points per hand. SPUDNIG then extracts these coordinates per frame and outputs them in three .csv files (body key points / left-hand key points / right-hand key points). Out of these files, it selects a set of eight (default) key points to estimate the initiation and termination of hand movements. The key points that are taken into account from the body are both wrists, and the key point between the wrist and elbow for each arm. Coordinate changes in those points mainly reflect large hand movements. The key points that are taken into account from the left- and right-hand are the tip of the index finger and the tip of the thumb of each hand. The selection of these key points, as well as the number of key points, were the result of a careful piloting phase in which we investigated the trade-off between false positives and false negatives. Adding more key points resulted in more false positives, and removing key points resulted in more false negatives. However, note that the user can edit the number and selection of the key points for their own research purposes to alter this weighting.</p>
        <p id="Par10">We have visualized our algorithm in a flow chart in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. For each frame, SPUDNIG calculates per key point whether a movement is taking place or not. First, it checks whether the reliability of the OpenPose output is above a certain threshold (default = 0.3, which can be altered by the user). If the reliability of a key point in a frame is below the reliability threshold, the script stops checking for potential movement and indicates that no movement is detected in the respective frame, before continuing to the next frame. If the reliability is above the threshold, the script continues and determines whether the key point in question is part of a rest state or part of a movement.<fig id="Fig1"><label>Fig. 1</label><caption><p>Flowchart of the SPUDNIG algorithm</p></caption><graphic xlink:href="13428_2020_1350_Fig1_HTML" id="MO1"/></fig></p>
        <p id="Par11">Rest states (i.e., the hands not being engaged in movement) are determined by checking the x/y-coordinates of a certain manual key point over a span of 15 frames (corresponding to 600 ms when using 25 frames per second (fps), as one frame corresponds to 40 ms (1000 ms/25 frames)), with the current frame (<italic>k</italic>) being the midpoint (i.e., frame <italic>k</italic>-7 until frame <italic>k</italic>+7 are checked). If the x/y-coordinates of these frames differ less than ten pixels from the coordinates of the current frame <italic>k</italic>, with an overall certainty threshold of 0.7 (i.e., 70% of the frames show less than ten pixels difference), SPUDNIG indicates that frame <italic>k</italic> is part of a rest state and that no movement is occurring. It then continues to the next frame. If the certainty threshold of 0.7 is not met, SPUDNIG continues to assess whether frame <italic>k</italic> is part of a movement. It then first checks whether frame <italic>k</italic> differs more than five pixels from the previous frame. If it does, it evaluates whether out of the five frames following frame <italic>k</italic>, minimally three frames differ more than five pixels from frame <italic>k</italic>. This extra check of the 5+ pixel criterion is performed to ensure that a movement is not falsely recognized due to slight shifts in the x/y-coordinates of a certain key point. Again, the particular criterion chosen was based on minimizing false negatives and false positives during our piloting, but can be altered in the code.</p>
        <p id="Par12">If movement is detected, SPUDNIG searches for a rest state in the upcoming 300 frames (~12 s, when using 25 fps), according to the method described above. If a new rest state cannot be found, SPUDNIG repeats this procedure with a slightly larger range in order to increase the chance of finding a rest state.</p>
        <p id="Par13">The above-mentioned process is repeated for all default key points and results in a list that indicates per frame, per key point, whether movement was detected or not. The resulting lists are then merged for all key points to ensure all manual movements are captured, even when reliability for one of the default key points is low. This approach minimized false negatives, and ensured that when one or some of the key points had lower reliability, movements could still be detected.</p>
      </sec>
      <sec id="Sec6">
        <title>Post-processing</title>
        <p id="Par14">After SPUDNIG has determined which frames contain movements, it removes movements smaller than four frames to diminish the number of false positives. Additionally, movements that occurred within four frames from each other are merged into one to account for small pauses or holds in movements. These settings are based on an extensive trial phase with videos different from the ones we used to validate our toolkit, with the aim to arrive at settings that would keep both the number of false positives and false negatives at a minimum. To most optimally determine the threshold for movement detection, the parameters were set by, as a primary criterion, keeping the number of false negatives as low as possible, combined with a secondary criterion that reduced the number of false positives as much as possible, given criterion one. Note that the user might want to consider changing these settings, depending on their research goal or type of video. These specific parameters can, if needed, easily be altered in the source code.</p>
        <p id="Par15">Based on the fps, the timing of each movement is calculated by converting the frame number to hh:mm:ss.ms format. This is used to create a .csv file containing all start and end times of annotations, which is compatible with commonly used annotation tools, such as ANVIL (Kipp, <xref ref-type="bibr" rid="CR23">2001</xref>) and ELAN (Wittenburg et al., <xref ref-type="bibr" rid="CR34">2006</xref>).</p>
      </sec>
      <sec id="Sec7">
        <title>Graphical user interface</title>
        <p id="Par16">SPUDNIG is implemented in an easy-to-use GUI, for which no programming, specific technological knowledge, or GPU hardware is needed. Below, we provide a short tutorial on how to use the SPUDNIG GUI to analyze video data.</p>
      </sec>
      <sec id="Sec8">
        <title>Brief SPUDNIG tutorial</title>
        <sec id="FPar1">
          <title>Step 1 – Loading the video file</title>
          <p id="Par17">After starting the application, an .avi video file can be selected for analysis (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>). A frame of the selected video file will appear on the screen.</p>
        </sec>
        <sec id="FPar2">
          <title>Step 2 – Video analysis &amp; selecting analysis parameters</title>
          <p id="Par18">The ‘Analyze’ button at the bottom of the screen will turn to green, indicating that the video is ready to be analyzed (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>). Pressing the green ‘Analyze’ button will display a parameter selection window (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>). After pressing the ‘Analyze’ button, a ‘Settings’ screen appears (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>). Here, the frames per second of the video need to be defined (default: 25), as well as the desired reliability threshold (default: 0.3, see above for explanation), and whether the left, right or both hands will be analyzed.<fig id="Fig2"><label>Fig. 2</label><caption><p>SPUDNIG's start screen. The user can select and load a file by clicking 'File --&gt; Open'. The 'Analyze' button will turn green when a file has been loaded in the application. After clicking the green 'Analyze' button, the user will be prompted by a 'Settings' screen. The default settings are already filled out</p></caption><graphic xlink:href="13428_2020_1350_Fig2_HTML" id="MO2"/></fig></p>
        </sec>
        <sec id="FPar3">
          <title>Step 3 – Launching analysis and exporting the data</title>
          <p id="Par19">After specifying the desired settings, pressing the green ‘Analyze’ button again will initiate the analysis. Progress of the analysis can be monitored through the green bar, which will fill up during the analysis (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>). When finished, the resulting .csv file can be exported by pressing the blue ‘Export’ button, which prompts a window in which the desired save location can be selected (see Fig. <xref rid="Fig3" ref-type="fig">3</xref>).<fig id="Fig3"><label>Fig. 3</label><caption><p>Final screen after the video is analyzed. Users can follow the progress of the analysis in the green bar, and export the resulting .csv file by pressing the blue button</p></caption><graphic xlink:href="13428_2020_1350_Fig3_HTML" id="MO3"/></fig></p>
        </sec>
      </sec>
      <sec id="Sec9">
        <title>Usage notes</title>
        <p id="Par20">
          <list list-type="order">
            <list-item>
              <p id="Par21">In addition to our GUI, the underlying source code of SPUDNIG is available on <ext-link ext-link-type="uri" xlink:href="https://github.com/jorrip/SPUDNIG">https://github.com/jorrip/SPUDNIG</ext-link>. The parameters described above can, if the user desires, be altered in the source code. Currently, implementation of these features in the GUI is underway (e.g., selection of key points). Additionally, we are planning to add a function that outputs velocity plots per movement (following existing methods, such as for Kinect data, as described by Trujillo et al., <xref ref-type="bibr" rid="CR33">2019</xref>).</p>
            </list-item>
            <list-item>
              <p id="Par22">It should be noted that OpenPose performs best when .avi files are used. Other, more complex formats, such as mpeg, might cause OpenPose to skip some frames at the start of videos. This will perturb the timing of the frames and output x/y-coordinates and thus the timing of the detected gestures. Therefore, SPUDNIG currently only takes .avi files as input. Alternative formats can be converted by using free video converters (see our GitHub page for suggested links).</p>
            </list-item>
            <list-item>
              <p id="Par23">Although a GPU is not needed to run SPUDNIG, it can speed up its analysis time. We therefore have a GPU compatible version of our GUI that is available on our Open Science Framework (OSF) repository.</p>
            </list-item>
            <list-item>
              <p id="Par24">Currently, SPUDNIG is designed to only output gestures from a single individual. However, this could easily be extended to analyses of multiple individuals that are visible in a video, as OpenPose can track multiple people.</p>
            </list-item>
          </list>
        </p>
      </sec>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Validation analyses</title>
    <p id="Par25">We tested and validated SPUDNIG by comparing its automated annotations of hand movements to manual annotations by a trained human coder. The focus of these analyses was on the detection of gestures. Second, we examined how accurately SPUDNIG could detect both non-iconic and iconic gestures. Finally, we tested whether and how much SPUDNIG accelerates the annotation process by comparing the time it takes to manually annotate videos versus how long it takes to remove false positives that are created by SPUDNIG and adjust, where necessary, the timing of the annotations that are generated by SPUDNIG.</p>
    <sec id="Sec11">
      <title>Corpus</title>
      <p id="Par26">As a test case for our validation analyses, we used 20 videos that form part of a multimodal communication corpus (CoAct corpus, ERC project #773079). All videos consisted of two acquainted native Dutch speakers that were engaged in dyadic, casual conversations. Participants were recorded while they were having a 1-h conversation. Out of these 1-h conversations, we randomly selected three, 2-min segments per video. These 2-min segments were used to (1) comparing SPUDNIG’s automated annotations of hand movements to manual annotations by a trained human coder to determine how many gestures are detected by SPUDNIG, (2) to test whether SPUDNIG indeed speeds up the annotation process, by comparing how much time untrained and trained human coders take to manually annotate data compared to adjust annotations or remove annotations by SPUDNIG (see below for more detail). All participants were filmed from a frontal perspective while seated on a chair (see Fig. <xref rid="Fig4" ref-type="fig">4</xref>).<fig id="Fig4"><label>Fig. 4</label><caption><p><italic>Upper panels</italic>: Frontal camera view used for data analysis in the SPUDNIG validation test. <italic>Lower panel</italic>: overview of set-up and distance between participants in the corpus used</p></caption><graphic xlink:href="13428_2020_1350_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par27">Note that SPUDNIG was not created by using these frontal videos, that is, we did not use these frontal videos to optimize our detection algorithm. Instead, we used other videos from the same video corpus that included recordings of the speakers seated at a ~ 45° angle (see Fig. <xref rid="Fig5" ref-type="fig">5</xref>). Note that the frontal angle in the videos used for our validation analyses thus provides a more optimal perspective for gesture detection, as the reliability of the key points are likely to be higher in this orientation, and movements can more easily be detected. Overlap with annotations by our human coder might therefore be higher.<fig id="Fig5"><label>Fig. 5</label><caption><p>Camera view used while designing and optimizing SPUDNIG. Note that here the speaker’s hands and arms are less visible than from a frontal perspective, and therefore overlap with a human coder might be lower, as some of the key points might be less visible. This might result in less accurate movement detection</p></caption><graphic xlink:href="13428_2020_1350_Fig5_HTML" id="MO5"/></fig></p>
    </sec>
    <sec id="Sec12">
      <title>Gesture annotation</title>
      <p id="Par28">We first compared the automated annotations that SPUDNIG made in these videos to manual annotations that were made by a trained, human coder that was blind to the purpose of the coding exercise. This coder was asked to manually annotate the occurrence of gestural movements in the videos, and was asked to include all manual movements that carried some form of meaning. This included movements such as deictic gestures, iconic and metaphoric gestures (depicting aspects of actions, objects space, or abstract concepts), pragmatic gestures (including beats), and interactive gestures (e.g., a palm-up, open-hand gesture that relates to the addressee) (Bavelas, Chovil, Coates, &amp; Roe, <xref ref-type="bibr" rid="CR1">1995</xref>; Bavelas, Chovil, Lawrie, &amp; Wade, <xref ref-type="bibr" rid="CR2">1992</xref>; McNeill, <xref ref-type="bibr" rid="CR28">1992</xref>). The annotations our coder made did not differentiate between these gesture types.</p>
      <p id="Par29">We asked our human coder to annotate all gesture strokes (Kita, van Gijn, &amp; van der Hulst, <xref ref-type="bibr" rid="CR24">1998</xref>), including holds and superimposed beats, and to define the begin point of a gesture as the first frame of when the hand left its rest state and its end point as the last frame of the gesture retraction after which the hand remained in rest state (or in the case of successive gestures without retractions, the last frame of the gesture stroke).</p>
      <p id="Par30">Gestures were further annotated in two different ways, one including form-based coding, and one including meaning-based coding. In the form-based coding, every stroke of a gesture was counted as a separate gesture annotation, regardless of whether repeated strokes depicted the same semantic meaning they were part of the same semantic gesture or not (thus, a swimming gesture with multiple successive strokes depicting someone swimming would result in multiple stroke annotations, for example). In the meaning-based coding, individual strokes are combined into one gesture annotation if they clearly depict the same semantic concept and are carried out without perceptible pauses or changes in form (in this case, repeated strokes depicting someone swimming would be annotated as one gesture, for example). The rationale was that both coding approaches seem to be used by gesture researchers and we aimed to make our machine-human coder comparison applicable to both.</p>
      <p id="Par31">For all gestures that were coded in the meaning-based coding tier, we also determined whether the gesture was iconic or non-iconic. This was done to test whether iconic gestures might be better or more easily detected by SPUDNIG than non-iconic gestures. For example, iconic gestures might include fewer holds, which would result in easier detection (because holds cannot be distinguished from non-gesture-embedded rest states by SPUDNIG), or iconic gestures might be larger in size or space, which would result in more clear x/y-coordinate changes than for non-iconic gestures.</p>
    </sec>
    <sec id="Sec13">
      <title>SPUDNIG-human coder reliability analyses</title>
      <p id="Par32">Because SPUDNIG annotates all hand movements in the videos and not just gestures, the question is how this compares to a human coding for gestures. To establish this, we calculated the overlap between all hand movements that SPUDNIG detected and all gestures that were detected by our human coder, for both form-based and meaning-based annotations, as well as distinguishing between iconic gesture and non-iconic gesture annotations for the latter. We calculated a modified Cohen’s kappa between SPUDNIG and our human coder by using EasyDIAg (Holle &amp; Rein, <xref ref-type="bibr" rid="CR14">2015</xref>), a standard method for establishing reliability between two human coders. EasyDIAg calculates a modified Cohen’s kappa by taking into account the temporal overlap of the annotations, the categorization of values, and the segmentation of behavior (e.g., when an annotation starts and ends). As recommended, we used an overlap criterion of 60%, indicating that there should be a temporal overlap of 60% between events (i.e., indicating the overlap in movements detected by SPUDNIG and gestures detected by a human).</p>
    </sec>
    <sec id="Sec14">
      <title>Gesture detection accuracy</title>
      <p id="Par33">In addition to trying to identify how many of the gestures the human identified would also be captured by the machine’s movement annotations, we also aimed to identify how much movement detected by the machine was not gestural. Thus, as a second step, we manually compared all raw output from SPUDNIG (i.e., annotations containing all movements in the video) to the output from our human coder to investigate how many movements that were detected by SPUDNIG did not overlap with a gesture annotation. This approach indicated how much of SPUDNIG’s output would have to be filtered out to obtain annotations that solely overlap with gestures, and also indicated how many gestures were not recognized by SPUDNIG, but were recognized by our human coder.</p>
    </sec>
    <sec id="Sec15">
      <title>Does SPUDNIG accelerate the annotation process?</title>
      <p id="Par34">As SPUDNIG annotates all hand movements in the videos and not just gestures, we asked four additional human coders to compare the time it takes to manually annotate the data compared to checking SPUDNIG’s output for whether something is indeed a gesture, and if so, whether the onset and offset of SPUDNIG’s annotation needed to be altered. The human coders were presented with forty, 2-min snippets of video data. Out of these, they were asked to annotate 20 videos manually (i.e., not relying on any pre-annotated data). For the remaining 20 videos, the human coders were asked to use the SPUDNIG output as a basis. A crucial element of this is that SPUDNIG’s algorithm and set threshold result in a highly reliable detection rate of all movements that may constitute gestures. This means that human coders can by-pass the step of checking all frames of a video for possible gestural movement. Instead, with the SPUDNIG output, coders can jump to the already annotated parts and check them for accuracy and then remove false positives and adjust potentially misaligned onsets and offsets.</p>
    </sec>
  </sec>
  <sec id="Sec16">
    <title>Results</title>
    <sec id="Sec17">
      <title>Form-based annotations of gestures</title>
      <p id="Par35">We first compared the overlap between SPUDNIG’s movement annotations and our human coder’s annotations to the form-based gesture annotations by our human coder, and observed 87% raw agreement and a modified Cohen’s kappa maximum value of .86, indicating very high agreement (Cohen, <xref ref-type="bibr" rid="CR8">1960</xref>; Landis &amp; Koch, <xref ref-type="bibr" rid="CR26">1977</xref>).</p>
      <p id="Par36">Our manual analysis revealed that there were 207 gestures that were identified by our human coder, and 206 by SPUDNIG. Note however that this analysis does not take the amount of overlap between SPUDNIG and the human coder into account, whereas the modified Cohen’s Kappa value does (Holle &amp; Rein, <xref ref-type="bibr" rid="CR14">2015</xref>).</p>
    </sec>
    <sec id="Sec18">
      <title>Meaning-based annotations of gestures</title>
      <p id="Par37">We then compared the overlap between SPUDNIG’s movement annotations and our human coder’s annotations to the meaning-based gesture annotations by our coder. We observed 86% raw agreement, and a modified Cohen’s kappa maximum value of .77, indicating a high level of agreement (Cohen, <xref ref-type="bibr" rid="CR8">1960</xref>; Landis &amp; Koch, <xref ref-type="bibr" rid="CR26">1977</xref>).</p>
      <p id="Par38">Our manual analysis revealed that out of the 185 gestures that were detected by our human coder, 184 were identified by SPUDNIG.</p>
    </sec>
    <sec id="Sec19">
      <title>Iconic gestures</title>
      <p id="Par39">As a next step, we investigated how many of the meaning-based annotations of gestures were iconic gestures. Out of 185 gestures, 45 gestures were iconic. We then compared the overlap between SPUDNIG’s movement annotations and the iconic gesture annotations. Here, we observed 93% raw agreement, and a modified Cohen’s kappa maximum value of 1, indicating almost perfect agreement (Cohen, <xref ref-type="bibr" rid="CR8">1960</xref>; Landis &amp; Koch, <xref ref-type="bibr" rid="CR26">1977</xref>).</p>
    </sec>
    <sec id="Sec20">
      <title>Non-iconic gestures</title>
      <p id="Par40">Out of the 185 meaning-based annotations of gestures, 140 gestures were non-iconic gestures. We compared the overlap between SPUDNIG’s movement annotations and the non-iconic gesture annotations to the non-iconic gesture annotations. We observed 84% raw agreement, and a modified Cohen’s kappa maximum value of .74, indicating a high level of agreement (Cohen, <xref ref-type="bibr" rid="CR8">1960</xref>; Landis &amp; Koch, <xref ref-type="bibr" rid="CR26">1977</xref>).</p>
    </sec>
    <sec id="Sec21">
      <title>Movement detection/gesture detection</title>
      <p id="Par41">While SPUDNIG seems to detect movements that constitute gesture highly reliably, it cannot currently recognize gestural from non-gestural movements, leading to considerably more annotations than those that result from a human coding for gestures: SPUDNIG annotated 311 hand movements in the videos, of which 217 were not part of a gesture (= 70%).</p>
    </sec>
    <sec id="Sec22">
      <title>SPUDNIG accelerates the manual annotation process</title>
      <p id="Par42">The crucial test of whether SPUDNIG accelerates the manual annotation process for gestures is based on this high detection rate of movements that potentially constitute gestures. This prerequisite means that our human coders could by-pass the step of checking the videos for false negatives (i.e., any movements SPUDNIG may have overlooked). Instead, they could focus on all extant SPUDNIG annotations to check these for accuracy. Doing so, and correcting the output by removing false positives and adjusting movement onsets and offsets where necessary (in order to make them correspond to gesture on and offsets, or the on and offsets of gesture sequences), significantly sped up our human coders: On average, they were almost twice as quick when using SPUDNIG (mean = 19.3, SD = 12.6, median = 17.8, IQR = 18.4) as compared to manually annotating the data (mean = 35.4, SD = 25.9, median = 33.2, IQR = 24.3), <italic>W</italic> = 235, <italic>p =</italic> 0.02.<xref ref-type="fn" rid="Fn1">1</xref></p>
    </sec>
  </sec>
  <sec id="Sec23">
    <title>Discussion</title>
    <p id="Par44">We presented SPUDNIG: SPeeding Up the Detection of Non-iconic and Iconic Gestures, a toolkit for the automatic detection of hand movements and gestures in video data. We provided proof-of-principle of our toolkit, and introduced an easy-to-use graphical user interface that researchers can use to automatically annotate hand movements and gestures in their video data.</p>
    <p id="Par45">To validate our method, we used video data containing natural dyadic conversations, and compared SPUDNIG’s output to form-based and meaning-based annotations of gestures, as identified by a trained, human coder, as well as iconic and non-iconic annotations of gestures. The results of our validation analyses demonstrated that our toolkit can very accurately annotate the occurrence of movements that match onto gestures in video data (&gt; 99%) (compared to human generated based on both form and meaning), and irrespective of whether the gesture is iconic or non-iconic. We also note that SPUDNIG leads to a ‘surplus’ of annotations (based on non-gestural movements), meaning that if one is not only interested in annotating human movement per se but in identifying communicative gestures, a human coder is still needed for removing these false positives. However, we have demonstrated that SPUDNIG advances current methods considerably by speeding up the detection of movement that constitutes gestures, the most laborious part of the process, requiring a human coder only primarily for the removal of false positives. Removing such ‘surplus’ annotations is a comparatively much easier and faster process than identifying gestures in a video from scratch. Below we will discuss the performance of SPUDNIG, its implications, and limitations.</p>
    <sec id="Sec24">
      <title>Performance</title>
      <p id="Par46">For all types of gesture coding, SPUDNIG achieved high to very high raw overlap and good to very good modified Cohen’s kappa values (Cohen, <xref ref-type="bibr" rid="CR8">1960</xref>; Holle &amp; Rein, <xref ref-type="bibr" rid="CR14">2015</xref>; Landis &amp; Koch, <xref ref-type="bibr" rid="CR26">1977</xref>). When comparing its performance for iconic and non-iconic gestures, we observed higher raw overlap for iconic gestures than for non-iconic gestures. A possible explanation for this is that iconic gestures might be larger in space or size, and therefore have more easily detectable changes in x/y-coordinates than non-iconic gestures. Non-iconic gestures, for example, might be closer to the threshold for x/y-coordinate changes, and might therefore have less temporal overlap with the annotations of our human coder.</p>
      <p id="Par47">In this regard, it should be noted that SPUDNIG detects all hand movements in the videos, and not only gestures (similar to other semi-automatic gesture detection algorithms, such as Beugher et al., <xref ref-type="bibr" rid="CR3">2018</xref>). This means that many of the movements that are part of SPUDNIG’s output need to be manually removed. In all 20 videos that were used for our validation analyses, a total of 311 movements were annotated by SPUDNIG, of which 217 were not part of a gesture (= 70%). These annotations include non-gestural hand and arm movements, such as fidgeting and self-grooming. This percentage is expected to be higher in videos containing natural conversations than in more stimulus-induced or task-based multimodal behavior. Although a substantial percentage of annotations thus needs to be removed by the user, SPUDNIG demonstrated very high gesture annotation overlap, meaning it can be used to save time to identify gestures. By giving the human coder highly reliable annotations of movements that are potentially gestures it can significantly speed up the video data annotation process. This was confirmed in our validation analyses that compared the time it takes human coders to manually annotate a dataset from scratch compared to removing and altering annotations made by SPUDNIG.</p>
    </sec>
    <sec id="Sec25">
      <title>Which gesture did SPUDNIG not detect, and why?</title>
      <p id="Par48">In general, 185 gestures (meaning-based coding, 207 in form-based coding) were detected by our human coder, and 184 gestures (meaning-based coding, 206 in form-based coding) were detected by SPUDNIG. One non-iconic gesture was not annotated by SPUDNIG, but was annotated by our human coder (see Fig. <xref rid="Fig6" ref-type="fig">6</xref>). As can be observed from the three video stills at different moments in time, the missed gesture remains in approximately the same position from the moment that SPUDNIG detects a movement to the moment that our human coder detects a second gesture. Closer inspection of this gesture, both in the generated .csv files by SPUDNIG and by looking at the video data, revealed that the x/y-coordinates of the key points do not differ much over the span of the two identified gestures. Most probably, SPUDNIG therefore does not detect the second gesture as a separate event, as it consists of a very small change in x/y-coordinates.<fig id="Fig6"><label>Fig. 6</label><caption><p><italic>Upper three panels</italic>: video stills at different moments in time. On the lower panel, the <italic>three vertical colored lines</italic> correspond to the colored squares around the video. The top two tiers include manual annotations from our human coder, indicating the occurrence of gestures. The lowest tier represents movements recognized by SPUDNIG. The <italic>light-purple shaded area</italic> covering the tiers represents the gesture that SPUDNIG missed</p></caption><graphic xlink:href="13428_2020_1350_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par49">Importantly, this example illustrates that SPUDNIG might not be optimal for detecting holds in gestures. In the above-mentioned video, the speaker holds her hand in the same position for a prolonged time. This will be recognized as a rest state by SPUDNIG, and would only be recognized as movement when the pixel change threshold would be lowered. This, on the other hand, would result in an increase of false positives.</p>
      <p id="Par50">Second, SPUDNIG is not optimized for segmenting gestures, as it merges annotations that are separated by four or less frames, and it removes movements shorter than four frames to reduce false positives and negatives. However, this threshold can, if desired, be altered in the source code.</p>
      <p id="Par51">Third, Fig. <xref rid="Fig6" ref-type="fig">6</xref> also shows that the timing of some annotations might need to be adjusted to match the full length of the gesture. As SPUDNIG does not include small movements (i.e., very small changes in x-y coordinates) or very short movements, the onset and offset of annotations might need adjustments by a human coder.</p>
    </sec>
    <sec id="Sec26">
      <title>Implications</title>
      <p id="Par52">In addition to SPUDNIG’s annotations capturing gestures annotated by a human extremely well, SPUDNIG’s main strengths are that using the tool requires no knowledge of programming, no use of motion capture systems, no GPU hardware, and that it comes with an easy-to-use GUI. This makes this tool highly accessible for a large community of users. Moreover, it has the potential to be developed further (based on open source code), with the ultimate aim being a tool that can distinguish gestural from non-gestural movements. While this still requires a big computational leap, the current state of the tool provides the non-programming user with a possibility to significantly speed up current gesture annotation practices.</p>
      <p id="Par53">Finally, the community of users could also be extended to researchers that investigate sign language. As SPUDNIG uses OpenPose as input for its analyses, it would be possible to study finer-grained hand movements and finger movements. These finer-grained movements are usually not detectable by other (semi-)automatic gesture detection systems that use other computer-vision methods (see for a discussion: Beugher et al., <xref ref-type="bibr" rid="CR3">2018</xref>). Moreover, SPUDNIG does not require pre-defined regions of interest to look for resting positions or movement. We investigated whether adding user-defined areas of interest for detecting rest states or movements improved detection performance, but we observed that this increased the chance of false negatives.</p>
    </sec>
    <sec id="Sec27">
      <title>Limitations</title>
      <p id="Par54">Although SPUDNIG is highly accurate in detecting hand movements and gestures, it cannot <italic>recognize</italic> different gesture types or forms. Future work could use SPUDNIG’s open source code to add functionalities to recognize gestures. A good starting point would for example be to train SPUDNIG on recurrent gestures (see, for example, Bressem &amp; Müller, <xref ref-type="bibr" rid="CR4">2014</xref>, <xref ref-type="bibr" rid="CR5">2017</xref>; Ladewig, <xref ref-type="bibr" rid="CR25">2011</xref>; Müller, <xref ref-type="bibr" rid="CR29">2017</xref>), by which it recognizes certain regular changes in x/y-coordinates over time. However, the recognition of gestures lies beyond the scope of the current manuscript and toolkit.</p>
      <p id="Par55">Second, SPUDNIG’s performance highly depends on how well OpenPose performs on the video data. If the (hands of the) speaker in videos are often occluded (e.g., the speaker sits with their arms folded over each other, or sits on their hands), or the video quality is too low, or the recording angle extreme, it might be difficult to find enough reliable data points to recognize movements. To some extent, these factors can be addressed by altering the threshold for movement detection, but the user should be aware that this has implications for the false positives and negatives in the output.</p>
      <p id="Par56">Third, SPUDNIG can be used on 2D data, but does not provide information on movements in three-dimensional space. It therefore seems less suited to study complex movement dynamics related to directionality or space (see for example Trujillo et al., <xref ref-type="bibr" rid="CR33">2019</xref>, where 3D information is used to study detailed kinematic features).</p>
      <p id="Par57">Finally, SPUDNIG uses pixel coordinates as thresholds for detecting movements. An alternative way to analyze the OpenPose output is by using millimeters as thresholds, which can be achieved by using the camera calibration procedure that is already part of OpenPose. This could result in a more generic threshold when two different cameras capture the same movements, and the resolution of these two cameras differs. This option is currently not supported in SPUDNIG.</p>
    </sec>
  </sec>
  <sec id="Sec28">
    <title>Conclusions</title>
    <p id="Par58">We demonstrated that SPUDNIG highly accurately detects iconic and non-iconic gestures in video data. SPUDNIG aims to accelerate and facilitate the annotation of hand movements in video data, and provides an easy-to-use and quick alternative to the labor-intensive and time-consuming manual annotation of gestures.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn id="Fn1">
      <label>1</label>
      <p id="Par43">As a comparison, we also asked our coders to use the SPUDNIG output (with a different set of 20 video snippets) to check all non-annotated frames for potential false negatives, in addition to making corrections for false positives and on and offsets. With such a procedure, we still see a benefit in terms of average annotation time, but one that is less pronounced (with SPUDNIG output: mean = 76.8 ms, SD = 83.4 vs. manually annotating the data without SPUDNIG output: mean = 81.2 ms, SD = 77.06). This means that, even when coders do not make full use of the tool by mistrusting SPUDNIG to detect potential gestural movements with high accuracy, we observe a benefit in terms of annotation time. However, note that the full benefit results from the tool’s ability to maximize human coders’ efforts by focusing them on to-be-checked pre-annotated segments.</p>
    </fn>
    <fn>
      <p>
        <bold>Publisher’s note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>Jordy Ripperda and Linda Drijvers contributed equally to this work.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>This work was supported by an ERC Consolidator grant (#773079, awarded to JH). We thank the Max Planck Gesellschaft for additional support. We thank Katharina Menn, Marlijn ter Bekke, Naomi Nota, Mareike Geiger, and Chloe Berry for assistance with reliability coding.</p>
    <sec id="FPar4">
      <title>Open Practices Statement</title>
      <p id="Par59">All data and materials are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/jorrip/SPUDNIG">https://github.com/jorrip/SPUDNIG</ext-link></p>
    </sec>
  </ack>
  <notes notes-type="funding-information">
    <title>Funding Information</title>
    <p>Open Access funding provided by Projekt DEAL.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bavelas</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>Chovil</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Coates</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Roe</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Gestures Specialized for Dialogue</article-title>
        <source>Personality and Social Psychology Bulletin</source>
        <year>1995</year>
        <volume>21</volume>
        <issue>4</issue>
        <fpage>394</fpage>
        <lpage>405</lpage>
        <pub-id pub-id-type="doi">10.1177/0146167295214010</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bavelas</surname>
            <given-names>JB</given-names>
          </name>
          <name>
            <surname>Chovil</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Lawrie</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Wade</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Interactive gestures</article-title>
        <source>Discourse Processes</source>
        <year>1992</year>
        <volume>15</volume>
        <issue>4</issue>
        <fpage>469</fpage>
        <lpage>489</lpage>
        <pub-id pub-id-type="doi">10.1080/01638539209544823</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Beugher</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Brône</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Goedemé</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>A semi-automatic annotation tool for unobtrusive gesture analysis</article-title>
        <source>Language Resources and Evaluation</source>
        <year>2018</year>
        <volume>52</volume>
        <issue>2</issue>
        <fpage>433</fpage>
        <lpage>460</lpage>
        <pub-id pub-id-type="doi">10.1007/s10579-017-9404-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <mixed-citation publication-type="other">Bressem, J., &amp; Müller, C. (2014). The family of away gestures: Negation, refusal, and negative assessment. In <italic>2</italic>. <italic>Body–language–communication: An international handbook on multimodality in human interaction. An International Handbook on Multimodality in Human Interaction (Handbooks of Linguistics and Communication Science 38.2).</italic> (pp. 1592–1604). Berlin, Boston: De Gruyter Mouton.</mixed-citation>
    </ref>
    <ref id="CR5">
      <mixed-citation publication-type="other">Bressem, J., &amp; Müller, C. (2017). The “Negative-Assessment-Construction”–A multimodal pattern based on a recurrent gesture? <italic>Linguistics Vanguard</italic>, <italic>3</italic>(s1).</mixed-citation>
    </ref>
    <ref id="CR6">
      <mixed-citation publication-type="other">Cao, Z., Hidalgo, G., Simon, T., Wei, S.-E., &amp; Sheikh, Y. (2018). OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. <italic>ArXiv:1812.08008 [Cs]</italic>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1812.08008">http://arxiv.org/abs/1812.08008</ext-link></mixed-citation>
    </ref>
    <ref id="CR7">
      <mixed-citation publication-type="other">Cao, Z., Simon, T., Wei, S.-E., &amp; Sheikh, Y. (2016). Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. <italic>ArXiv:1611.08050 [Cs]</italic>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1611.08050">http://arxiv.org/abs/1611.08050</ext-link></mixed-citation>
    </ref>
    <ref id="CR8">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cohen</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>A coefficient of agreement for nominal scales</article-title>
        <source>Educational and Psychological Measurement</source>
        <year>1960</year>
        <volume>20</volume>
        <fpage>37</fpage>
        <lpage>46</lpage>
        <pub-id pub-id-type="doi">10.1177/001316446002000104</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dael</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Mortillaro</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Scherer</surname>
            <given-names>KR</given-names>
          </name>
        </person-group>
        <article-title>The Body Action and Posture Coding System (BAP): Development and Reliability</article-title>
        <source>Journal of Nonverbal Behavior</source>
        <year>2012</year>
        <volume>36</volume>
        <issue>2</issue>
        <fpage>97</fpage>
        <lpage>121</lpage>
        <pub-id pub-id-type="doi">10.1007/s10919-012-0130-0</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Drijvers</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Özyürek</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Visual Context Enhanced: The Joint Contribution of Iconic Gestures and Visible Speech to Degraded Speech Comprehension</article-title>
        <source>Journal of Speech, Language, and Hearing Research</source>
        <year>2017</year>
        <volume>60</volume>
        <issue>1</issue>
        <fpage>212</fpage>
        <lpage>222</lpage>
        <pub-id pub-id-type="doi">10.1044/2016_JSLHR-H-16-0101</pub-id>
        <?supplied-pmid 27960196?>
        <pub-id pub-id-type="pmid">27960196</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <mixed-citation publication-type="other">Fang, H. S., Xie, S., Tai, Y. W., &amp; Lu, C. (2017). Rmpe: Regional multi-person pose estimation. In Proceedings of the IEEE International Conference on Computer Vision pp. 2334–2343.</mixed-citation>
    </ref>
    <ref id="CR12">
      <mixed-citation publication-type="other">Goldin-Meadow, S. (2005). <italic>Hearing Gesture: How Our Hands Help Us Think</italic>. Harvard University Press.</mixed-citation>
    </ref>
    <ref id="CR13">
      <mixed-citation publication-type="other">Hassemer, J. (2015). <italic>Towards a theory of Gesture Form Analysis. Imaginary forms as part of gesture conceptualisation, with empirical support from motion-capture data</italic>. RWTH Aachen University, Aachen.</mixed-citation>
    </ref>
    <ref id="CR14">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Holle</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Rein</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>EasyDIAg: A tool for easy determination of interrater agreement</article-title>
        <source>Behavior Research Methods</source>
        <year>2015</year>
        <volume>47</volume>
        <issue>3</issue>
        <fpage>837</fpage>
        <lpage>847</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-014-0506-7</pub-id>
        <?supplied-pmid 25106813?>
        <pub-id pub-id-type="pmid">25106813</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <mixed-citation publication-type="other">Holler, J., &amp; Beattie, G. (2003). How iconic gestures and speech interact in the representation of meaning: Are both aspects really integral to the process? <italic>Semiotica</italic>, <italic>2003</italic>(146). 10.1515/semi.2003.083</mixed-citation>
    </ref>
    <ref id="CR16">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Holler</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shovelton</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Beattie</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Do Iconic Hand Gestures Really Contribute to the Communication of Semantic Information in a Face-to-Face Context?</article-title>
        <source>Journal of Nonverbal Behavior</source>
        <year>2009</year>
        <volume>33</volume>
        <issue>2</issue>
        <fpage>73</fpage>
        <lpage>88</lpage>
        <pub-id pub-id-type="doi">10.1007/s10919-008-0063-9</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Holler</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Wilkin</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Communicating common ground: How mutually shared knowledge influences speech and gesture in a narrative task</article-title>
        <source>Language and Cognitive Processes</source>
        <year>2009</year>
        <volume>24</volume>
        <issue>2</issue>
        <fpage>267</fpage>
        <lpage>289</lpage>
        <pub-id pub-id-type="doi">10.1080/01690960802095545</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hostetter</surname>
            <given-names>AB</given-names>
          </name>
        </person-group>
        <article-title>When do gestures communicate? A meta-analysis</article-title>
        <source>Psychological Bulletin</source>
        <year>2011</year>
        <volume>137</volume>
        <issue>2</issue>
        <fpage>297</fpage>
        <lpage>315</lpage>
        <pub-id pub-id-type="doi">10.1037/a0022128</pub-id>
        <?supplied-pmid 21355631?>
        <pub-id pub-id-type="pmid">21355631</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kelly</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Barr</surname>
            <given-names>DJ</given-names>
          </name>
          <name>
            <surname>Church</surname>
            <given-names>RB</given-names>
          </name>
          <name>
            <surname>Lynch</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Offering a Hand to Pragmatic Understanding: The Role of Speech and Gesture in Comprehension and Memory</article-title>
        <source>Journal of Memory and Language</source>
        <year>1999</year>
        <volume>40</volume>
        <issue>4</issue>
        <fpage>577</fpage>
        <lpage>592</lpage>
        <pub-id pub-id-type="doi">10.1006/jmla.1999.2634</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kelly</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Kravitz</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Hopkins</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Neural correlates of bimodal speech and gesture comprehension</article-title>
        <source>Brain and Language</source>
        <year>2004</year>
        <volume>89</volume>
        <issue>1</issue>
        <fpage>253</fpage>
        <lpage>260</lpage>
        <pub-id pub-id-type="doi">10.1016/s0093-934x(03)00335-3</pub-id>
        <?supplied-pmid 15010257?>
        <pub-id pub-id-type="pmid">15010257</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kelly</surname>
            <given-names>SD</given-names>
          </name>
          <name>
            <surname>Ozyürek</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Maris</surname>
            <given-names>E</given-names>
          </name>
        </person-group>
        <article-title>Two sides of the same coin: speech and gesture mutually interact to enhance comprehension</article-title>
        <source>Psychological Science</source>
        <year>2010</year>
        <volume>21</volume>
        <issue>2</issue>
        <fpage>260</fpage>
        <lpage>267</lpage>
        <pub-id pub-id-type="doi">10.1177/0956797609357327</pub-id>
        <?supplied-pmid 20424055?>
        <pub-id pub-id-type="pmid">20424055</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <mixed-citation publication-type="other">Kendon, A. (2004). <italic>Gesture: Visible Action as Utterance</italic>. Cambridge University Press.</mixed-citation>
    </ref>
    <ref id="CR23">
      <mixed-citation publication-type="other">Kipp, M. (2001). ANVIL - a generic annotation tool for multimodal dialogue. <italic>INTERSPEECH</italic>.</mixed-citation>
    </ref>
    <ref id="CR24">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kita</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>van Gijn</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>van der Hulst</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <person-group person-group-type="editor">
          <name>
            <surname>Wachsmuth</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Fröhlich</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Movement phases in signs and co-speech gestures, and their transcription by human coders</article-title>
        <source>
          <italic>Gesture and Sign Language in Human-Computer Interaction</italic>
        </source>
        <year>1998</year>
        <publisher-loc>Berlin Heidelberg</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>23</fpage>
        <lpage>35</lpage>
      </element-citation>
    </ref>
    <ref id="CR25">
      <mixed-citation publication-type="other">Ladewig, S. H. (2011). Putting the cyclic gesture on a cognitive basis. <italic>CogniTextes. Revue de l’Association Française de Linguistique Cognitive</italic>, (Volume 6). 10.4000/cognitextes.406</mixed-citation>
    </ref>
    <ref id="CR26">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Landis</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Koch</surname>
            <given-names>GG</given-names>
          </name>
        </person-group>
        <article-title>The measurement of observer agreement for categorical data</article-title>
        <source>Biometrics</source>
        <year>1977</year>
        <volume>33</volume>
        <issue>1</issue>
        <fpage>159</fpage>
        <lpage>174</lpage>
        <pub-id pub-id-type="doi">10.2307/2529310</pub-id>
        <pub-id pub-id-type="pmid">843571</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Mathis</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Mamidanna</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Cury</surname>
            <given-names>KM</given-names>
          </name>
          <name>
            <surname>Abe</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Murthy</surname>
            <given-names>VN</given-names>
          </name>
          <name>
            <surname>Mathis</surname>
            <given-names>MW</given-names>
          </name>
          <name>
            <surname>Bethge</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title>
        <source>Nature Neuroscience</source>
        <year>2018</year>
        <volume>21</volume>
        <issue>9</issue>
        <fpage>1281</fpage>
        <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
        <?supplied-pmid 30127430?>
        <pub-id pub-id-type="pmid">30127430</pub-id>
      </element-citation>
    </ref>
    <ref id="CR28">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>McNeill</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <source>
          <italic>Hand and Mind: What Gestures Reveal About Thought</italic>
        </source>
        <year>1992</year>
        <publisher-loc>Chicago</publisher-loc>
        <publisher-name>University of Chicago press</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR29">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Müller</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>How recurrent gestures mean</article-title>
        <source>Gesture</source>
        <year>2017</year>
        <volume>16</volume>
        <issue>2</issue>
        <fpage>277</fpage>
        <lpage>304</lpage>
        <pub-id pub-id-type="doi">10.1075/gest.16.2.05mul</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ozyurek</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Hearing and seeing meaning in speech and gesture: insights from brain and behaviour</article-title>
        <source>Philosophical Transactions of the Royal Society B: Biological Sciences</source>
        <year>2014</year>
        <volume>369</volume>
        <issue>1651</issue>
        <fpage>20130296</fpage>
        <lpage>20130296</lpage>
        <pub-id pub-id-type="doi">10.1098/rstb.2013.0296</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Paxton</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dale</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Frame-differencing methods for measuring bodily synchrony in conversation</article-title>
        <source>Behavior Research Methods</source>
        <year>2013</year>
        <volume>45</volume>
        <issue>2</issue>
        <fpage>329</fpage>
        <lpage>343</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-012-0249-2</pub-id>
        <?supplied-pmid 23055158?>
        <pub-id pub-id-type="pmid">23055158</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <mixed-citation publication-type="other">Pouw, W., Trujillo, J., &amp; Dixon, J. A. (2018). <italic>The Quantification of Gesture-speech Synchrony: A Tutorial and Validation of Multi-modal Data Acquisition Using Device-based and Video-based Motion Tracking</italic> [Preprint]. 10.31234/osf.io/jm3hk</mixed-citation>
    </ref>
    <ref id="CR33">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Trujillo</surname>
            <given-names>JP</given-names>
          </name>
          <name>
            <surname>Vaitonyte</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Simanova</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Özyürek</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Toward the markerless and automatic analysis of kinematic features: A toolkit for gesture and movement research</article-title>
        <source>Behavior Research Methods</source>
        <year>2019</year>
        <volume>51</volume>
        <issue>2</issue>
        <fpage>769</fpage>
        <lpage>777</lpage>
        <pub-id pub-id-type="doi">10.3758/s13428-018-1086-8</pub-id>
        <?supplied-pmid 30143970?>
        <pub-id pub-id-type="pmid">30143970</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <mixed-citation publication-type="other">Wittenburg, P., Brugman, H., Russel, A., Klassmann, A., &amp; Sloetjes, H. (2006). <italic>ELAN: a professional framework for multimodality research</italic>. 1556–1559. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_60436">https://pure.mpg.de/pubman/faces/ViewItemOverviewPage.jsp?itemId=item_60436</ext-link></mixed-citation>
    </ref>
    <ref id="CR35">
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>Microsoft Kinect Sensor and Its Effect</article-title>
        <source>IEEE MultiMedia</source>
        <year>2012</year>
        <volume>19</volume>
        <issue>2</issue>
        <fpage>4</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1109/MMUL.2012.24</pub-id>
      </element-citation>
    </ref>
    <ref id="CR36">
      <mixed-citation publication-type="other">Zhao, L., &amp; Badler, N. (2001). Synthesis and Acquisition of Laban Movement Analysis Qualitative Parameters for Communicative Gestures. <italic>Technical Reports (CIS)</italic>. Retrieved from <ext-link ext-link-type="uri" xlink:href="https://repository.upenn.edu/cis_reports/116">https://repository.upenn.edu/cis_reports/116</ext-link></mixed-citation>
    </ref>
  </ref-list>
</back>
