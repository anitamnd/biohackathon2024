<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8382278</article-id>
    <article-id pub-id-type="pmid">34252966</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btab272</article-id>
    <article-id pub-id-type="publisher-id">btab272</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Macromolecular Sequence, Structure, and Function</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EnHiC: learning fine-resolution Hi-C contact maps using a generative adversarial framework</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Hu</surname>
          <given-names>Yangyang</given-names>
        </name>
        <aff><institution>Department of Computer Science and Engineering</institution>, Riverside, CA 92521, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-4097-1621</contrib-id>
        <name>
          <surname>Ma</surname>
          <given-names>Wenxiu</given-names>
        </name>
        <aff><institution>Department of Statistics, University of California Riverside</institution>, Riverside, CA 92521, <country country="US">USA</country></aff>
        <xref rid="btab272-cor1" ref-type="corresp"/>
        <!--wenxiu.ma@ucr.edu-->
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btab272-cor1">To whom correspondence should be addressed. <email>wenxiu.ma@ucr.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2021-07-12">
      <day>12</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>12</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <volume>37</volume>
    <issue>Suppl 1</issue>
    <issue-title>ISMB/ECCB 2021 Proceedings</issue-title>
    <fpage>i272</fpage>
    <lpage>i279</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btab272.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The high-throughput chromosome conformation capture (Hi-C) technique has enabled genome-wide mapping of chromatin interactions. However, high-resolution Hi-C data requires costly, deep sequencing; therefore, it has only been achieved for a limited number of cell types. Machine learning models based on neural networks have been developed as a remedy to this problem.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>In this work, we propose a novel method, EnHiC, for predicting high-resolution Hi-C matrices from low-resolution input data based on a generative adversarial network (GAN) framework. Inspired by non-negative matrix factorization, our model fully exploits the unique properties of Hi-C matrices and extracts rank-1 features from multi-scale low-resolution matrices to enhance the resolution. Using three human Hi-C datasets, we demonstrated that EnHiC accurately and reliably enhanced the resolution of Hi-C matrices and outperformed other GAN-based models. Moreover, EnHiC-predicted high-resolution matrices facilitated the accurate detection of topologically associated domains and fine-scale chromatin interactions.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>EnHiC is publicly available at <ext-link xlink:href="https://github.com/wmalab/EnHiC" ext-link-type="uri">https://github.com/wmalab/EnHiC</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>U.S. National Institute of Health</institution>
          </institution-wrap>
          <!-- oupReleaseDelayRemoved from OA Article (00|0) -->
        </funding-source>
        <award-id>R35GM133678</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="8"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Recent developments of the high-throughput chromosome conformation capture (Hi-C) techniques (<xref rid="btab272-B5" ref-type="bibr">Duan <italic toggle="yes">et al.</italic>, 2010</xref>; <xref rid="btab272-B12" ref-type="bibr">Lieberman-Aiden <italic toggle="yes">et al.</italic>, 2009</xref>; <xref rid="btab272-B17" ref-type="bibr">Rao <italic toggle="yes">et al.</italic>, 2014</xref>) have enabled us to detect genome-wide chromatin interactions and investigate the organizational principles of the three-dimensional (3D) genome. Studies of Hi-C data have revealed the multi-scale organization of the 3D genome, including active/inactive chromosomal compartments (<xref rid="btab272-B12" ref-type="bibr">Lieberman-Aiden <italic toggle="yes">et al.</italic>, 2009</xref>), topologically associated domains (TADs) (<xref rid="btab272-B4" ref-type="bibr">Dixon <italic toggle="yes">et al.</italic>, 2012</xref>) and fine-scale chromatin loops (<xref rid="btab272-B17" ref-type="bibr">Rao <italic toggle="yes">et al.</italic>, 2014</xref>; <xref rid="btab272-B15" ref-type="bibr">Ma <italic toggle="yes">et al.</italic>, 2015</xref>). Large-scale chromatin structures, such as compartments and TADs, can be identified from relatively low-resolution (50 kb to 1 Mb) Hi-C contact matrices. However, detecting fine-scale chromatin loops often requires high-resolution (i.e. 10 kb or finer) contact matrices. Moreover, fine-resolution Hi-C data are more compatible with other genomic and epigenomic data, and could therefore facilitate the interrogation of genome regulation and function.</p>
    <p>However, high-resolution chromatin contact maps require costly, deep sequencing, and have been achieved in only a limited number of cell lines. For instance, a kilobase-resolution Hi-C map of human lymphoblastoid GM12878 cells required five billion chromatin contacts (<xref rid="btab272-B17" ref-type="bibr">Rao <italic toggle="yes">et al.</italic>, 2014</xref>). Without sufficient sequencing depth, the observed Hi-C contact maps are often sparse and noisy, which imposes great computational challenges on the identification of chromatin loops between distal regulatory elements and their target genes. Therefore, computational approaches to enhance the resolution of Hi-C contact maps would greatly facilitate the investigation of the 3D genome at a finer scale, and are therefore in great demand.</p>
    <p>Several pioneering works on predicting higher-resolution contact frequency matrices from low-resolution Hi-C data have emerged since 2018. The HiCPlus method (<xref rid="btab272-B26" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic>, 2018a</xref>) was the first attempt to enhance Hi-C data resolution with a convolutional neural network (CNN) by minimizing the L2 mean square error (MSE) loss function. Similar to the image super-resolution approach (<xref rid="btab272-B27" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic>, 2018b</xref>), HiCPlus extracts hidden features from high-resolution Hi-C matrices in the training process and then predicts high-resolution Hi-C matrices from low-resolution input data. Later, <xref rid="btab272-B14" ref-type="bibr">Liu and Wang (2019)</xref> proposed the HiCNN model, which employs a more complex (with more than 14 layers) and efficient CNN model with residual learning by utilizing skip connections. However, both HiCPlus and HiCNN use the MSE loss; therefore, they are sensitive to outliers and would result in blurred output when the input Hi-C matrix is sparse.</p>
    <p>More recently, several generative adversarial network (GAN) models, such as hicGAN (<xref rid="btab272-B13" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2019</xref>), Deephic (<xref rid="btab272-B7" ref-type="bibr">Hong <italic toggle="yes">et al.</italic>, 2020</xref>) and HiCSR (<xref rid="btab272-B3" ref-type="bibr">Dimmick <italic toggle="yes">et al.</italic>, 2020</xref>), have been proposed to enhance Hi-C matrix resolution. The general GAN framework consists of two neural networks: a generator and a discriminator that contest with each other. In the training step, the generator learns to create a candidate to deceive the discriminator, while the discriminator learns to distinguish the generated candidate from the true data. First, hicGAN (<xref rid="btab272-B13" ref-type="bibr">Liu <italic toggle="yes">et al.</italic>, 2019</xref>) adopts the SRGAN model (<xref rid="btab272-B11" ref-type="bibr">Ledig <italic toggle="yes">et al.</italic>, 2017</xref>) in image super-resolution to enhance resolution of Hi-C matrices. The hicGAN model uses a skip-connection network as the generator and replaces the traditional pixel-wise MSE loss with a purely adversarial loss. As a result of minimizing the adversarial loss, hicGAN often misses fine-scale image details. Later, <xref rid="btab272-B7" ref-type="bibr">Hong <italic toggle="yes">et al.</italic> (2020)</xref> proposed Deephic, a model similar to hicGAN. To recover fine-scale image details, Deephic uses a mixture loss function that consists of the MSE loss, total variation loss, perceptual loss and adversarial loss. The perceptual loss component was derived from the VGG-type model (<xref rid="btab272-B20" ref-type="bibr">Simonyan and Zisserman, 2014</xref>). However, this perceptual loss causes unwanted natural image textures in the output. Lastly, the HiCSR model (<xref rid="btab272-B3" ref-type="bibr">Dimmick <italic toggle="yes">et al.</italic>, 2020</xref>) uses a skip-connection network as the generator and a CNN as the discriminator. Their loss function consists of the L1 mean absolute error (MAE) loss, feature loss and adversarial loss. The feature loss was derived from a pre-trained model, which is a denoising autoencoder modified from an image restoration architecture (<xref rid="btab272-B16" ref-type="bibr">Mao <italic toggle="yes">et al.</italic>, 2016</xref>).</p>
    <p>The previously proposed models, hicGAN, Deephic and HiCSR, have demonstrated the power of the GAN framework in predicting high-resolution Hi-C matrices. However, these models treat the Hi-C matrix as a one-channel image and their GAN networks are primarily built on image super-resolution models. As a result, their predictions often contain image artifacts and, therefore, do not accurately represent the underlying chromatin interaction features of the Hi-C data.</p>
    <p>To tackle this problem, we developed a new GAN-based model, EnHiC, to enhance the resolution of Hi-C contact frequency matrices. Specifically, we propose a novel convolutional layer (the <italic toggle="yes">Decomposition &amp; Reconstruction Block</italic>, see Methods) that accounts for the non-negative and symmetric properties of Hi-C matrices. In our GAN framework, the generator extracts rank-1 matrix features from multiple scales of low-resolution matrices and predicts the high-resolution matrix via a series of sub-pixel CNN layers (<xref rid="btab272-B19" ref-type="bibr">Shi <italic toggle="yes">et al.</italic>, 2016</xref>). Accordingly, the discriminator decomposes a high-resolution Hi-C matrix into multiple lower-resolution matrices and extracts the corresponding rank-1 matrix features to determine whether the high-resolution matrix is derived from the generator or the true data.</p>
    <p>We evaluated the performance of our EnHiC model using published Hi-C datasets in three human cell lines: GM12878 (lymphoblastoid cells), IMR90 (lung fibroblast cells) and K562 (leukemia cells) (<xref rid="btab272-B17" ref-type="bibr">Rao <italic toggle="yes">et al.</italic>, 2014</xref>). We demonstrated that EnHiC accurately enhanced the resolution of Hi-C data and achieved high similarity scores with respect to the true high-resolution data, outperforming previously proposed GAN-based models. Using the model trained in one cell type, EnHiC effectively enhanced the resolution of insufficient sequenced Hi-C data in other cell types. In addition, using the EnHiC-enhanced data, we successfully recovered Hi-C-specific features, such as TADs and significant chromatin interactions.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Hi-C contact frequency matrix</title>
      <p>First, we introduce a few notations regarding the Hi-C contact frequency matrix. A bulk Hi-C experiment characterizes an ensemble of chromatin contacts from thousands or millions of cell nuclei. The raw data generated from the Hi-C experiment can be presented as a non-negative symmetric matrix <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, namely the contact frequency matrix, where <italic toggle="yes">N</italic> is the number of fixed-size non-overlapping bins in the genome. Each matrix element <italic toggle="yes">C<sub>ij</sub></italic> is the observed contact frequency between the genomic loci pair <italic toggle="yes">i</italic> and <italic toggle="yes">j</italic>. A higher contact frequency indicates a smaller spatial distance between a pair of genomic loci in cell nuclei. In short, we refer to the bulk Hi-C contact frequency matrix as the Hi-C matrix.</p>
      <p>In our method, we aim to predict high-resolution Hi-C matrices from low-resolution input data. Here, high resolution indicates more chromatin interaction details (i.e. more valid pairs of sequencing reads), rather than a higher dimension of the Hi-C matrix. Given a Hi-C input dataset, it can processed into a matrix of any arbitrary bin size. Therefore, a high-dimensional Hi-C matrix is not always of high resolution. In this work, we refer to the dimension of the Hi-C matrix as its scale. A lower-scale Hi-C matrix has a smaller number of rows and columns.</p>
    </sec>
    <sec>
      <title>2.2 Overview of the model</title>
      <p>In this section, we describe the framework of the EnHiC model. More details of the model are provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>. EnHiC is based on a GAN framework that contains a generator and a discriminator. Through competition between them, the generator learns to predict high-resolution Hi-C matrices from low-resolution input matrices, while the discriminator distinguishes the generator-predicted high-resolution matrices from real data.</p>
      <p>The main difference between our model and other GAN-based approaches is that EnHiC exploits the unique properties of the Hi-C matrix and treats it as a multi-scale interaction contact map instead of a pure image. Specifically, EnHiC extracts rank-1 matrix features from low-resolution input data at multiple scales and learns to enhance the matrix resolution using these estimated rank-1 features. The overview of the EnHiC framework is illustrated in <xref rid="btab272-F1" ref-type="fig">Figure 1</xref>.</p>
      <fig position="float" id="btab272-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>The framework of the EnHiC model. The details of the <italic toggle="yes">Downsampling Block</italic>, <italic toggle="yes">Upsampling Block</italic>, <italic toggle="yes">Combination Block</italic>, <italic toggle="yes">Normalization Block</italic>, <italic toggle="yes">Rank-1 Estimation Block</italic> and <italic toggle="yes">Decomposition &amp; Reconstruction Block</italic> are illustrated in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref></p>
        </caption>
        <graphic xlink:href="btab272f1" position="float"/>
      </fig>
      <sec>
        <label>2.2.1</label>
        <title>Decomposition &amp; reconstruction block</title>
        <p>A key component in our model is the <italic toggle="yes">Decomposition &amp; Reconstruction Block</italic>, as illustrated in <xref rid="btab272-F1" ref-type="fig">Figure 1</xref> and <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref>.</p>
        <p>In our model, we represent a Hi-C matrix as a multi-channel image (i.e. a tensor). Let <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> be the number of input and output channels, respectively. The input and output tensors are denoted by <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">N</italic> is the dimension of the Hi-C matrix. The <italic toggle="yes">Decomposition &amp; Reconstruction Block</italic> contains three layers:
</p>
        <list list-type="bullet">
          <list-item>
            <p>The decomposition layer, which passes <bold>X</bold> into a convolutional layer with kernel <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. In contrast to the traditional convolutional layer, the kernel is a vector rather than a square matrix. The length of the kernel vector is the same as the height/width of the input tensor. Hence, the kernel only moves in one direction, and the number of shared parameters for this convolutional layer is <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>in</mml:mtext></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The resulting tensor is denoted by <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">U</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">u</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, which represents the rank-1 features of the input data.</p>
          </list-item>
          <list-item>
            <p>The weighting layer, which scales the feature tensor as <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="bold">U</mml:mi><mml:mi mathvariant="bold">w</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">w</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, where the weight vector <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> is a learnable parameter.</p>
          </list-item>
          <list-item>
            <p>The reconstruction layer, which constructs the output tensor <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> using the weighted rank-1 features. For each channel <italic toggle="yes">k</italic>, we have a rank-1 matrix <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mo>:</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mi>k</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mtext>out</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>.</p>
          </list-item>
        </list>
      </sec>
      <sec>
        <label>2.2.2</label>
        <title>Generator</title>
        <p>The generator consists of two parts: (i) extracting rank-1 matrix features from low-resolution input matrices at multiple scales and (ii) enhancing Hi-C matrix resolution using the multi-scale features learned in the first part. The overview of the generator framework (<italic toggle="yes">G</italic><sub>1</sub> in the orange dashed box and <italic toggle="yes">G</italic><sub>2</sub> in the blue dashed box) is shown in <xref rid="btab272-F1" ref-type="fig">Figure 1</xref>.</p>
        <p>Because the low-resolution input matrix is often sparse, we first downscale the matrix to enhance its signal. The downscaling operation is achieved by shrinking the size of the matrix by an average-pooling layer. In our experiments, we aim to enhance the resolution of the Hi-C matrix by a factor of 16, which is equivalent to scaling up the matrix by a factor of 4 (i.e. multiplying both the height and width of the matrix by 4). Therefore, in our model, we generate two downscaled matrices by factors of 2 and 4 [denoted as LR(<inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mo>↓</mml:mo></mml:mrow></mml:math></inline-formula>) and LR(<inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo>↓</mml:mo></mml:mrow></mml:math></inline-formula>), respectively]. We use LR(<inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mo>↓</mml:mo></mml:mrow></mml:math></inline-formula>) and LR(<inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo>↓</mml:mo></mml:mrow></mml:math></inline-formula>) as the ground truth to assist in the estimation of the rank-1 matrix features at the corresponding scales. Note that in our EnHiC framework, the number of downscaling operations can be adjusted for different applications. For instance, if we aim to enhance the Hi-C resolution by a factor of 100, it is recommended to include additional levels of downscaled matrices (and accordingly, more <italic toggle="yes">Decomposition &amp; Reconstruction Blocks</italic>) to facilitate a better estimation of matrix features.</p>
        <p>The first part of the generator (<italic toggle="yes">G</italic><sub>1</sub>) extracts multi-scale rank-1 features from the low-resolution input matrix. First, it transforms the input matrix (<italic toggle="yes">N </italic>×<italic toggle="yes"> N</italic>) into a tensor (<inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:mfrac><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>) using a space-to-depth layer (TensorFlow built-in function). The space-to-depth layer permutes the spatial blocks of the input matrix into the depth dimension without any loss of information. Then, a multi-channel image (tensor) is subsequently processed through the <italic toggle="yes">Decomposition &amp; Reconstruction Block</italic> and its rank-1 features are extracted. Note that the input Hi-C matrix is symmetric and non-negative, and our rank-1 approximations retain the symmetric and non-negative properties of the data. In our framework, we extract the rank-1 features for two different scales (<inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula>2 and 4, respectively), and the two estimation matrices, denoted as EHiC(<inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mo>×</mml:mo><mml:mn>2</mml:mn><mml:mo>↓</mml:mo></mml:mrow></mml:math></inline-formula>) and EHiC(<inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo>↓</mml:mo></mml:mrow></mml:math></inline-formula>), are compared against the true data, as shown in <xref rid="btab272-F1" ref-type="fig">Figure 1</xref>.</p>
        <p>The second part of the generator (<italic toggle="yes">G</italic><sub>2</sub>) recombines the rank-1 features from multiple scales and enhances the matrix resolution through a series of <italic toggle="yes">Upsampling Blocks</italic>. The <italic toggle="yes">Upsampling Block</italic> contains a sub-pixel convolutional layer (<xref rid="btab272-B19" ref-type="bibr">Shi <italic toggle="yes">et al.</italic>, 2016</xref>) that upscales the previously learned features in low-resolution space to a high-resolution output. The upscaled tensor is subsequently averaged with its transpose to reinforce the symmetric property of the output matrix. In concert with the two <italic toggle="yes">Decomposition &amp; Reconstruction Blocks</italic> in the first part, we have two <italic toggle="yes">Upsampling Blocks</italic>, each of which upscales the matrix dimension by a factor of 2 (i.e. enhancing the data resolution by a factor of 4). Therefore, the final output matrix has an enhanced resolution by a factor of 16 compared to the low-resolution input matrix. Details of the <italic toggle="yes">Upsampling Block</italic> are illustrated in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S1</xref>.</p>
      </sec>
      <sec>
        <label>2.2.3</label>
        <title>Loss functions of the generator</title>
        <p>The objective of the generator is to estimate the rank-1 features at multiple scales and to enhance resolution of the input matrix. Therefore, we design two loss functions for these two tasks separately. Although the extraction of rank-1 features can be obtained using a pre-trained model, we combine it in the generator network so that we can reuse the intermediate rank-1 feature data in the training process. Therefore, the generator has two loss functions and two back-propagation steps to update their associated parameters separately.</p>
        <p><bold><italic toggle="yes">Loss function for low-resolution approximation (rank-1 matrix features)</italic></bold> Inspired by NMF, the approximate low-resolution Hi-C matrix is calculated as a combination of rank-1 matrices. To estimate these rank-1 matrices, we include both pixel-wise MSE loss and structural dissimilarity (DSSIM) measures in the loss function. The DSSIM metric is derived from the structural similarity (SSIM) metric (<xref rid="btab272-B22" ref-type="bibr">Wang <italic toggle="yes">et al.</italic>, 2004</xref>) to quantify the perceptual differences between two images. Specifically, <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:mtext>DSSIM</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>SSIM</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. As described above, the generator may involve more than one downscaled representation of the low-resolution input, so we denote the factor set as <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">f</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and the corresponding weights for the downscaled matrices as <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="bold">w</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>K</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>f</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:msubsup><mml:mi>f</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:math></inline-formula>. The loss function of rank-1 feature extraction is:
<disp-formula id="E1"><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>MSE</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mtext>DSSIM</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
        <p>In our application, we downscale the low-resolution input matrix by two different factors. Hence, <italic toggle="yes">K </italic>=<italic toggle="yes"> </italic>2, <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>.</p>
        <p><bold><italic toggle="yes">Loss function for high-resolution enhancement</italic></bold> In the second part of the generator, we feed the rank-1 matrix features extracted from multiple downscaled low-resolution data into several sub-pixel layers to enhance matrix resolution. The loss function for the prediction of a high-resolution matrix consists of the pixel-wise MSE loss and the adversarial loss:
<disp-formula id="E2"><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>SR</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>HR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>MSE</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>SR</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>HR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>adv</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the <italic toggle="yes">α</italic><sub>0</sub> and <italic toggle="yes">α</italic><sub>1</sub> are hyperparameters.</p>
        <p>The adversarial loss <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>adv</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a crucial part of the GAN framework that connects the generator and discriminator networks. For the generator, minimizing the loss is equivalent to minimizing the binary cross-entropy loss between the true label (<bold>y</bold>) and the prediction (<bold>x</bold>) of generated Hi-C matrices by the discriminator. That is, <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>bce</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>·</mml:mo><mml:mo> </mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. To disorient the discriminator, all labels of the predicted matrices are set to true. More details on the adversarial loss are discussed in Section 2.2.5.
<disp-formula id="E3"><mml:math id="M3" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>adv</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>bce</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>LR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>SR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      </sec>
      <sec>
        <label>2.2.4</label>
        <title>Discriminator</title>
        <p>The discriminator aims to differentiate between high-resolution predictions from the generator and real high-resolution data. In our EnHiC model, the discriminator shares the same strategy of the multi-scale rank-1 approximation as the generator, as illustrated in <xref rid="btab272-F1" ref-type="fig">Figure 1</xref>. First, the input matrix is converted to multiple downscaled tensors by space-to-depth layers (in the <italic toggle="yes">Downsampling Block</italic>) and the rank-1 matrix features are subsequently extracted from each of the downscaled tensors (in the <italic toggle="yes">Decomposition &amp; Reconstruction Block</italic>). In our design, we extract rank-1 features from the original matrix as well as three downscaled matrices (by a factor of 2, 4 and 8, respectively). Second, these rank-1 matrix features are passed into a cascade of <italic toggle="yes">Convolutional Blocks</italic> to detect latent features at multiple resolutions. As shown in <xref rid="btab272-F1" ref-type="fig">Figure 1</xref>, each <italic toggle="yes">Convolutional Block</italic> includes a Leaky ReLU layer, a max-pooling layer and a 2D convolution layer. After pooling and convolution, the dimensions of rank-1 matrix features are reduced by a factor of 2. These higher-resolution features are then concatenated with lower-resolution features and passed into the subsequent <italic toggle="yes">Convolutional Block</italic>. Finally, after a fully connected layer, the discriminator outputs the probability that the input is real, that is, the true high-resolution data rather than a prediction from the generator.</p>
      </sec>
      <sec>
        <label>2.2.5</label>
        <title>Loss function of the discriminator</title>
        <p>In the training process, the generator and discriminator compete with each other and are connected by a MinMax loss. The generator tries to minimize the following function while the discriminator attempts to maximize it:
<disp-formula id="E4"><mml:math id="M4" display="block" overflow="scroll"><mml:msub><mml:munder><mml:mi>min</mml:mi><mml:mi>G</mml:mi></mml:munder><mml:mrow/></mml:msub><mml:msub><mml:munder><mml:mi>max</mml:mi><mml:mi>D</mml:mi></mml:munder><mml:mrow/></mml:msub><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mtext>HR</mml:mtext></mml:msub></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mtext>LR</mml:mtext></mml:msub></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mtext>LR</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula>where <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>·</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the estimated probability by the discriminator. <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>HR</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the expected value over all true instances. <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>LR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the generator’s output when fed with the low-resolution Hi-C matrix <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>LR</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, which is also called the super-resolution Hi-C matrix <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>SR</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>LR</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the expected value over all generated instances.</p>
        <p>The GAN framework has two adversarial loss functions: one for generator training (as discussed in Section 2.2.3) and one for discriminator training. The discriminator aims to maximize <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mtext>HR</mml:mtext></mml:msub></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mtext>LR</mml:mtext></mml:msub></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mtext>log</mml:mtext><mml:mo> </mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mtext>LR</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula>. Thus, the adversarial loss of the discriminator can be expressed as a combination of two binary cross-entropy losses:
<disp-formula id="E5"><mml:math id="M5" display="block" overflow="scroll"><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mi mathvariant="normal">D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>bce</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>HR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow><mml:mrow><mml:mtext>bce</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>LR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>HR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mtext>SR</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 EnHic accurately predicts high-resolution Hi-C matrices</title>
      <p>First, we sought to evaluate the enhancement capability of our EnHiC model against two other GAN-based models, Deephic and HiCSR. It has been shown that Deephic and HiCSR outperformed previously proposed models, including HiCPlus, HiCNN and hicGAN. Therefore, these models were not included in our evaluation. All three models, EnHiC, Deephic and HiCSR, were trained to predict a high-resolution (10 kb) Hi-C matrix from a low-resolution (40 kb) Hi-C matrix. In other words, the desired resolution enhancement factor was 16.</p>
      <sec>
        <label>3.1.1</label>
        <title>Data preprocessing</title>
        <p>In our validation experiments, we used three published Hi-C datasets in different human cell lines: GM12878 (lymphoblastoid cells), IMR90 (lung fibroblast cells) and K562 (leukemia cells) (<xref rid="btab272-B17" ref-type="bibr">Rao <italic toggle="yes">et al.</italic>, 2014</xref>). Among them, the GM12878 dataset has the highest number of chromatin contacts (2.88 billion), followed by IMR90 (0.76 billion) and K562 (0.62 billion) (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>). High-resolution (10 kb) Hi-C matrices were obtained from the cooler database (<xref rid="btab272-B1" ref-type="bibr">Abdennur and Mirny, 2020</xref>). Low-resolution Hi-C matrices were generated using a random downsampling procedure. Here we used the default downsampling ratio of 16. In other words, the sequencing depth in the resulting low-resolution matrices was 1/16 of the high-resolution data.</p>
        <p>First, we trained the three models (EnHiC, Deephic and HiCSR) on the most deeply sequenced Hi-C data generated from GM12878 cells. We used chromosomes 1-16 for training, chromosomes 17 and 18 for hyperparameter tuning, and chromosomes 19–22 and X for evaluation. After model training in the GM12878 data, we applied the three methods to the IMR90 and K562 data to investigate the enhancement performance across different cell types.</p>
        <p>The raw Hi-C matrix contains various types of technical and biological biases. Therefore, normalization is an essential step in Hi-C data analysis. Many normalization methods based on matrix-balancing approaches have been proposed (<xref rid="btab272-B8" ref-type="bibr">Imakaev <italic toggle="yes">et al.</italic>, 2012</xref>; <xref rid="btab272-B9" ref-type="bibr">Knight and Ruiz, 2013</xref>; <xref rid="btab272-B10" ref-type="bibr">Kumar <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab272-B18" ref-type="bibr">Servant <italic toggle="yes">et al.</italic>, 2015</xref>). In the EnHiC model, we employ the Sequential Component Normalization (SCN) method (<xref rid="btab272-B18" ref-type="bibr">Servant <italic toggle="yes">et al.</italic>, 2015</xref>) to normalize the input Hi-C matrix. The Deephic and HiCSR models do not require Hi-C-specific normalization of the input matrix. Instead, Deephic uses the min-max normalization to scale the input data. HiCSR first conducts a log1p transformation (i.e. <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:mtext>log</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>) and then a min-max normalization of the input data.</p>
        <p>After normalization, the intra-chromosomal Hi-C matrices were divided into small pieces (submatrices of size <italic toggle="yes">n </italic>×<italic toggle="yes"> n</italic>) for both training and testing. Here, we set <italic toggle="yes">n </italic>=<italic toggle="yes"> </italic>400. Specifically, EnHiC first divides the Hi-C matrix into non-overlapping submatrices of size <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula> and then combines two diagonal submatrices with their off-diagonal interacting submatrix to form an <italic toggle="yes">n </italic>×<italic toggle="yes"> n</italic> matrix. This operation ensures that the resulting submatrices are symmetric. Deephic divides the Hi-C matrix into non-overlapping submatrices of size 40 × 40. HiCSR divides the Hi-C matrix into partially overlapping submatrices of size 40 × 40 with a step size of 28 × 28. Therefore, the input submatrices are of size 40 × 40 and the output submatrices are of size 28 × 28. Because the average TAD size is less than 1 Mb and most of the significant interactions are located inside TADs, we omitted submatrices with the genomic distances greater than 2 Mb.</p>
      </sec>
      <sec>
        <label>3.1.2</label>
        <title>Training and prediction</title>
        <p>The EnHiC model was implemented in Python 3 with TensorFlow2; and the source code is available at <ext-link xlink:href="https://github.com/wmalab/EnHiC" ext-link-type="uri">https://github.com/wmalab/EnHiC</ext-link>. Both the training and prediction processes of the three assessed models were conducted on Intel Haswell CPU and NVIDIA Tesla K80 GPU with 128 GB of memory. For EnHiC, the number of epochs for training was set to 300 with parameters <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>. The runtime of the training process was approximately 85 hours (17 min per epoch). More training details, including the configuration and visualization generated by TensorBoard, are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Information</xref>. The runtimes for HiCSR (500 epochs) and Deephic (800 epochs) were approximately 2 to 4 days.</p>
      </sec>
      <sec>
        <label>3.1.3</label>
        <title>Model validation and evaluations in GM12878 data</title>
        <p>After the training step, we first applied the three models (EnHiC, Deephic and HiCSR) to the evaluation set (chromosomes 19-22 and X) in human GM12878 data to enhance the resolution of low-resolution Hi-C matrices (downsampled from high-resolution Hi-C matrices by a factor of 16). We denote the 10 kb high-resolution Hi-C matrices obtained from the cooler database as the ground truth.</p>
        <p>For each chromosome, we assembled the predicted submatrices into one intra-chromosomal matrix. Because different models use different normalization procedures, it is necessary to reverse the normalizations to facilitate a fair comparison with the same ground truth. Denote the model output as <bold>X</bold>, and de-normalized result as <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>.
</p>
        <list list-type="bullet">
          <list-item>
            <p>Deephic uses the min-max normalization. Hence, the reversion is <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mi mathvariant="bold">X</mml:mi><mml:mo>+</mml:mo><mml:mi>min</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:math></inline-formula> are maximal and minimal values in the ground truth, respectively.</p>
          </list-item>
          <list-item>
            <p>HiCSR uses both the log1p transformation and the min-max normalization. Therefore, the reversion is <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>max</mml:mi><mml:mi mathvariant="bold">X</mml:mi><mml:mo>+</mml:mo><mml:mi>min</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:math></inline-formula> are the maximal and minimal log1p values in the ground truth.</p>
          </list-item>
          <list-item>
            <p>EnHiC uses the SCN normalization, therefore the reversion is <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">X</mml:mi><mml:mo stretchy="false">˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi mathvariant="bold">X</mml:mi><mml:mo>⊘</mml:mo><mml:mi mathvariant="bold">b</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, where <bold>b</bold> is the bias vector estimated from the ground truth using the SCN method and <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mo>⊘</mml:mo></mml:math></inline-formula> is the element-wise division. In the form of each element, we have <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>.</p>
          </list-item>
        </list>
        <p>After reverse normalization, we evaluated the prediction results of the three models with the ground truth using four metrics: two classic pixel-wise numeric errors (MAE and MSE) and two Hi-C-specific similarity metrics: HiCRep (<xref rid="btab272-B25" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2017</xref>) and GenomeDISCO (<xref rid="btab272-B21" ref-type="bibr">Ursu <italic toggle="yes">et al.</italic>, 2018</xref>). <xref rid="sup1" ref-type="supplementary-material">Supplementary Table S2</xref> summarizes the MAE and MSE measurements of the EnHiC, Deephic and HiCSR predictions. Overall, EnHiC achieved the best performance with the lowest MAE and MSE errors. We noticed that MAE and MSE errors were inflated in Deephic and HiCSR predictions. This is likely due to the reverse normalization procedure, where the MAE and MSE errors were amplified by the <italic toggle="yes">max</italic> value and exponential operation. Therefore, the MAE and MSE metrics were not effective in assessing the performance of the Hi-C enhancement. We present the results for reference because MAE is a component of the loss function in the HiCSR model, and MSE is included in the loss functions in both EnHiC and Deephic.</p>
        <p>In addition to the MAE and MSE metrics, we also considered two popular similarity measurements specifically designed for assessing reproducibility of Hi-C matrices, HiCRep (<xref rid="btab272-B25" ref-type="bibr">Yang <italic toggle="yes">et al.</italic>, 2017</xref>) and GenomeDISCO (<xref rid="btab272-B21" ref-type="bibr">Ursu <italic toggle="yes">et al.</italic>, 2018</xref>). HiCRep calculates a stratum-adjusted correlation coefficient (SCC) between two Hi-C matrices. The resulting SCC values range from –1 to 1, where a larger SCC value indicates a higher similarity between the two matrices. GenomeDISCO treats the Hi-C matrix as a network; it applies random walks on the network to smooth the data and then calculates a reproducibility score at multiple scales. Similar to HiCRep, GenomeDISCO scores also range from –1 to 1, where higher scores representing the higher reproducibility. Besides HiCRep and GenomeDISCO, HiC-Spector (<xref rid="btab272-B24" ref-type="bibr">Yan <italic toggle="yes">et al.</italic>, 2017</xref>) is another Hi-C reproducibility metric. HiC-Spector applies the adjacency matrix to impute missing values and then calculates a similarity score between two full matrices. In our experiments, since we only predicted a strip of data in the full matrix (i.e. submatrices with genomic distances shorter than 2 Mb), HiC-Spector is not applicable in our evaluation.</p>
        <p><xref rid="btab272-T1" ref-type="table">Table 1</xref> summarizes the HiCRep and GenomeDISCO evaluation results of EnHiC, Deephic and HiCSR. As shown in <xref rid="btab272-T1" ref-type="table">Table 1</xref>, The HiCRep SCC scores were greater than 0.94 for all three methods, indicating that their high-resolution predictions are very similar to the ground truth. Among them, our EnHiC model achieved the highest HiCRep SCC values and GenomeDISCO scores for all five test chromosomes. These results demonstrated that EnHiC can accurately and robustly enhance the resolution of Hi-C matrices and outperformed existing GAN-based models.</p>
        <table-wrap position="float" id="btab272-T1">
          <label>Table 1.</label>
          <caption>
            <p>Evaluation of high-resolution Hi-C matrices predicted by EnHiC, Deephic and HiCSR</p>
          </caption>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col valign="top" align="left" span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
              <col valign="top" align="char" char="." span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th rowspan="1" colspan="1"/>
                <th colspan="3" rowspan="1">HiCRep<hr/></th>
                <th colspan="3" rowspan="1">GenomeDISCO<hr/></th>
              </tr>
              <tr>
                <th rowspan="1" colspan="1">Chromosome</th>
                <th rowspan="1" colspan="1">EnHiC</th>
                <th rowspan="1" colspan="1">Deephic</th>
                <th rowspan="1" colspan="1">HiCSR</th>
                <th rowspan="1" colspan="1">EnHiC</th>
                <th rowspan="1" colspan="1">Deephic</th>
                <th rowspan="1" colspan="1">HiCSR</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td rowspan="1" colspan="1">19</td>
                <td rowspan="1" colspan="1">
                  <bold>0.972</bold>
                </td>
                <td rowspan="1" colspan="1">0.942</td>
                <td rowspan="1" colspan="1">0.970</td>
                <td rowspan="1" colspan="1">
                  <bold>0.83</bold>
                </td>
                <td rowspan="1" colspan="1">0.768</td>
                <td rowspan="1" colspan="1">0.677</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">20</td>
                <td rowspan="1" colspan="1">
                  <bold>0.972</bold>
                </td>
                <td rowspan="1" colspan="1">0.941</td>
                <td rowspan="1" colspan="1">0.967</td>
                <td rowspan="1" colspan="1">
                  <bold>0.837</bold>
                </td>
                <td rowspan="1" colspan="1">0.777</td>
                <td rowspan="1" colspan="1">0.65</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">21</td>
                <td rowspan="1" colspan="1">
                  <bold>0.973</bold>
                </td>
                <td rowspan="1" colspan="1">0.966</td>
                <td rowspan="1" colspan="1">0.968</td>
                <td rowspan="1" colspan="1">
                  <bold>0.816</bold>
                </td>
                <td rowspan="1" colspan="1">0.771</td>
                <td rowspan="1" colspan="1">0.636</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">22</td>
                <td rowspan="1" colspan="1">
                  <bold>0.978</bold>
                </td>
                <td rowspan="1" colspan="1">0.974</td>
                <td rowspan="1" colspan="1">0.973</td>
                <td rowspan="1" colspan="1">
                  <bold>0.844</bold>
                </td>
                <td rowspan="1" colspan="1">0.786</td>
                <td rowspan="1" colspan="1">0.716</td>
              </tr>
              <tr>
                <td rowspan="1" colspan="1">X</td>
                <td rowspan="1" colspan="1">
                  <bold>0.949</bold>
                </td>
                <td rowspan="1" colspan="1">0.930</td>
                <td rowspan="1" colspan="1">0.945</td>
                <td rowspan="1" colspan="1">
                  <bold>0.781</bold>
                </td>
                <td rowspan="1" colspan="1">0.743</td>
                <td rowspan="1" colspan="1">0.639</td>
              </tr>
            </tbody>
          </table>
          <table-wrap-foot>
            <fn id="tblfn1">
              <p><italic toggle="yes">Note</italic>: Three models are evaluated on chrosomomes 19-22 and X in human GM12878 Hi-C data. Each model prediction result is compared against the ground truth, and the HiCRep and GenomeDISCO scores are calculated. The highest HiCRep and GenomeDISCO scores are highlighted in bold.</p>
            </fn>
          </table-wrap-foot>
        </table-wrap>
      </sec>
      <sec>
        <label>3.1.4</label>
        <title>Performance on IMR90 and K562 data</title>
        <p>In the previous section, we have demonstrated the capability of EnHiC in recovering high-resolution Hi-C matrices from low-resolution input data. We then asked whether EnHiC can enhance Hi-C matrix resolution across different cell types. Toward this goal, we applied three models (EnHiC, Deephic and HiCSR) that were previously trained on the deeply sequenced GM12878 (lymphoblastoid cells) dataset to two other less-sequenced Hi-C datasets: IMR90 (lung fibroblast cells), and K562 (leukemia cells). The same data preprocessing was performed in each cell type; and HiCRep and GenomeDISCO similarity scores were calculated to evaluate the model predictions.</p>
        <p><xref rid="btab272-F2" ref-type="fig">Figure 2</xref> illustrates the cross-cell-type performance of EnHiC, Deephic and HiCSR. Overall, EnHiC outperformed both Deephic and HiCSR with the highest HiCRep and GenomeDISCO scores in both IMR90 and K562 datasets. We observed that the HiCRep and GenomeDISCO similarity scores were relatively lower than the ones previously obtained from GM12878 data, but they were significantly higher that the baseline (low-resolution input data). In addition, the performance of all three models were slightly better in IMR90 than K562. This is likely due to the relatively higher sequencing depth in the IMR90 data (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>). Taken together, these results indicated that EnHiC can effectively recover high-resolution matrices from insufficiently sequenced Hi-C data across cell types.</p>
        <fig position="float" id="btab272-F2">
          <label>Fig. 2.</label>
          <caption>
            <p>Evaluation of high-resolution Hi-C matrix predictions by EnHiC, Deephic and HiCSR on human IMR90 and K562 Hi-C data (23 chromosomes). The models are first trained on GM12878 data and then applied to the other cell types. Each prediction result is compared against the ground truth, and the HiCRep and GenomeDISCO similarity scores are reported. Each box represents similarity scores of 23 chromosomes (1–22 and X). Low-resolution (LR) input data are included as the baseline</p>
          </caption>
          <graphic xlink:href="btab272f2" position="float"/>
        </fig>
      </sec>
      <sec>
        <label>3.1.5</label>
        <title>Performance on different downsampling ratios</title>
        <p>In the training process, we generated low-resolution Hi-C matrices that were <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:mn>16</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> downsampled from high-resolution ground truth, i.e. the sequencing depth of the low-resolution input data was 1/16 of the high-resolution data. We set the downsampling ratio at 16 to facilitate a fair comparison with previously published methods (Deephic and HiCSR). Although being trained by <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:mn>16</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> downsampled data, our EnHiC model is flexible and can be applied to low-resolution data with much less sequencing depth. Next, we sought to investigate the performance of our model using low-resolution input data generated with different downsampling ratios.</p>
        <p>In this experiment, we generated low-resolution input data at six different downsampling ratios (4, 8, 16, 32, 48 and 64). We trained three models (EnHiC, Deephic, HiCSR) on the human GM12878 data using the same training set (chromosomes 1–16) and validation set (chromosomes 17–18) at <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:mn>16</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> downsampled ratio as previously described. We then evaluated the model performance using all 23 chromosomes at six different downsampled ratios, except for the <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mrow><mml:mn>16</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></inline-formula> downsampled data where the 18 training and validation chromosomes were excluded.</p>
        <p>As shown in <xref rid="btab272-F3" ref-type="fig">Figure 3</xref>, the HiCRep and GenomeDISCO similarity scores of low-resolution input baseline decreased sharply as the downsampling ratio increased. Notably, our EnHiC model robustly and stably recovered high-resolution Hi-C matrices from low-resolution input data with large downsampled ratios. Moreover, EnHiC achieved higher HiCRep and GenomeDISCO scores than DeepHiC and HiCSR at almost all downsampled ratios. Although HiCSR performed slightly better than EnHiC by the HiCRep metric when the downsampling ratio was 4, its performance dropped sharply when the downsampling ratio increased. This is probably due to the pre-trained denoise model used in the loss function of HiCSR. Collectively, these results demonstrated that EnHiC can successfully predict high-resolution Hi-C matrices from insufficiently sequenced low-resolution data.</p>
        <fig position="float" id="btab272-F3">
          <label>Fig. 3.</label>
          <caption>
            <p>Performance of high-resolution Hi-C matrix predictions by EnHiC, Deephic and HiCSR on GM12878 data at various downsampling ratios (4, 8, 16, 32, 48 and 64). Each prediction result is compared against the ground truth; and the HiCRep and GenomeDISCO reproducibility scores are reported. The mean values and error bars are calculated using scores from 23 chromosomes (1-22 and X). Low-resolution (LR) input data are included as the baseline</p>
          </caption>
          <graphic xlink:href="btab272f3" position="float"/>
        </fig>
      </sec>
    </sec>
    <sec>
      <title>3.2 EnHiC facilitates accurate detection of TADs</title>
      <p>TADs are functional units of chromatin, where chromatin interactions are observed more frequently within TADs than outside TADs. TAD boundaries are largely conserved across cell types and are enriched with CTCF and other chromatin-binding proteins (<xref rid="btab272-B4" ref-type="bibr">Dixon <italic toggle="yes">et al.</italic>, 2012</xref>). To investigate whether high-resolution enhancing methods promote TAD detection, we compared the TADs identified from high-resolution predictions by EnHiC, Deephic and HiCSR, with the TADs identified from the true high-resolution data.</p>
      <p>Several computational methods exist for detecting TADs in Hi-C contact maps. Here, we used the hicFindTADs method in the HiCExplorer package (<xref rid="btab272-B23" ref-type="bibr">Wolff <italic toggle="yes">et al.</italic>, 2018</xref>). We calculated Jaccard scores to assess the consistency between TADs detected from model predictions and TADs detected from true high-resolution (HR) data. The Jaccard score measures the similarity between two sets and is defined as the ratio of the intersection size over the union size. Jaccard score has been commonly used to quantify similarities of TAD and chromatin loop detections (<xref rid="btab272-B6" ref-type="bibr">Forcato <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btab272-B7" ref-type="bibr">Hong <italic toggle="yes">et al.</italic>, 2020</xref>). Here we calculated Jaccard scores of TAD boundaries and allowed the boundaries to be shifted within 5 bins between the two sets.
<disp-formula id="E6"><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:mtext>Jaccard</mml:mtext><mml:mo> </mml:mo><mml:mtext>score</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>TAD</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mtext>HR</mml:mtext></mml:mrow></mml:msub><mml:mo>∩</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>TAD</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mtext>prediction</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mtext>TAD</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mtext>HR</mml:mtext></mml:mrow></mml:msub><mml:mo>∪</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mtext>TAD</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mtext>prediction</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p>
      <p><xref rid="btab272-F4" ref-type="fig">Figure 4</xref> illustrates the Jaccard score evaluation of various methods in the validation dataset (chromosomes 17 and 18) and the test dataset (chromosomes 19–22 and X). The TADs detected from low-resolution input matrices were also included as baselines. Overall, EnHiC promoted accurate TAD detection; and the identified TADs were highly consistent with the ones identified from the true high-resolution data. In most cases, except for chromosome 21, high-resolution predictions from GAN-based models resulted in more accurate TAD detection than low-resolution input matrices (<xref rid="btab272-F5" ref-type="fig">Fig. 5</xref>). Overall, EnHiC yielded the highest Jaccard scores for five out of seven chromosomes, and outperformed both Deephic and HiCSR.</p>
      <fig position="float" id="btab272-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>The Jaccard scores of TADs. TADs detected from high-resolution predictions by EnHiC, Deephic and HiCSR were compared with TADs detected from real high-resolution (10 kb) Hi-C data, for chromosomes 17–22 and X. TAD detection results from low-resolution (LR) input data were also included</p>
        </caption>
        <graphic xlink:href="btab272f4" position="float"/>
      </fig>
      <fig position="float" id="btab272-F5">
        <label>Fig. 5.</label>
        <caption>
          <p>Numbers of TADs detected by each model. The results of seven chromosomes (17–22 and X) are summed. The red bars represent common TADs in both the true high-resolution (HR) matrices and model predictions. The blue (yellow) bars represent unique TADs detected only in the HR (predicted) matrices</p>
        </caption>
        <graphic xlink:href="btab272f5" position="float"/>
      </fig>
      <p>We also characterized the ChIP-seq profiles of several chromatin structural proteins and histone marks at the detected TAD boundaries in EnHiC-predicted matrices (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S10</xref>). Consistent with the previous findings (<xref rid="btab272-B4" ref-type="bibr">Dixon <italic toggle="yes">et al.</italic>, 2012</xref>), we observed that CTCF, members of the cohesin complex (SMC3 and RAD21), RNA polymerase PolII binding and H3K4me3 and H3K27me3 histone modifications were enriched at TAD boundaries, whereas H3K9me3 was depleted at such boundaries.</p>
      <p>We further examined TAD detection results in two local regions (chr17:72–74Mbp and chr19:14–16Mbp), as illustrated in <xref rid="btab272-F6" ref-type="fig">Figure 6</xref>. The low-resolution input matrices are sparse and noisy; therefore, the detected TADs are often merged or split. Our EnHiC model accurately predicted high-resolution matrices from low-resolution input data. As a result, the TADs detected from EnHiC predictions were in agreement with the TADs from the true high-resolution data in both examples. We observed that both Deephic and HiCSR predictions overinflated the contact frequencies and Deephic predictions contained unwanted image textures, thereby resulting in inaccurate TAD detection.</p>
      <fig position="float" id="btab272-F6">
        <label>Fig. 6.</label>
        <caption>
          <p>Examples of TAD detection results. (<bold>a</bold>) Chromosome 17 from 72Mbp to 74Mbp, (<bold>b</bold>) Chromosome 19 from 14Mbp to 16Mbp. TADs were identified using HiCExplorer. From top to bottom: true high-resolution (10 kb) Hi-C data, CTCF ChIP-seq signal, low-resolution (40 kb) input Hi-C data and high-resolution predictions from EnHiC, Deephic and HiCSR. For each Hi-C matrix, the heatmap of close-to-diagonal region is displayed with the color key from low (blue) to high (red) interaction frequencies. TADs are identified using HiCExplorer, and marked as black triangles</p>
        </caption>
        <graphic xlink:href="btab272f6" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>3.3 EnHiC-predicted high-resolution matrices promote precise identifications of significant chromatin interactions</title>
      <p>Next, we investigated whether the EnHiC-predicted high-resolution Hi-C data could facilitate the identification of fine-scale chromatin loops. We applied Fit-Hi-C (<xref rid="btab272-B2" ref-type="bibr">Ay <italic toggle="yes">et al.</italic>, 2014</xref>) to identify significant interactions within 1 Mb genomic distances and compared the overlaps between the real and predicted Hi-C matrices. The Jaccard score was used to assess consistency between the significant interactions in the two matrices.</p>
      <p>As shown in <xref rid="btab272-F7" ref-type="fig">Figure 7</xref>, EnHiC evidently outperformed the other two GAN-based prediction models with significantly higher Jaccard scores (<italic toggle="yes">t</italic>-tests, <italic toggle="yes">P</italic>-values: <inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mrow><mml:mn>2.57</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (EnHiC versus Deephic), <inline-formula id="IE56"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mrow><mml:mn>1.23</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (versus HiCSR) and <inline-formula id="IE57"><mml:math id="IM57" display="inline" overflow="scroll"><mml:mrow><mml:mn>4.98</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (versus LR)). The low-resolution Hi-C input matrices lack sufficient sequencing depth; therefore, they are not suitable for the identification of fine-scale chromatin interactions, especially when the genomic distance increases (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S11</xref>).</p>
      <fig position="float" id="btab272-F7">
        <label>Fig. 7.</label>
        <caption>
          <p>The Jaccard scores of significant interactions between the true high-resolution Hi-C and model predictions. The results from low-resolution (LR) input data were included as baseline. Each box depicts the Jaccard scores of seven chromosomes (17–22 and X)</p>
        </caption>
        <graphic xlink:href="btab272f7" position="float"/>
      </fig>
      <p>We further looked at two example regions, chromosome 17:32–34Mbp (<xref rid="btab272-F8" ref-type="fig">Fig. 8</xref>) and chromosome 19:14–16Mbp (<xref rid="btab272-F9" ref-type="fig">Fig. 9</xref>). As demonstrated in both regions, EnHiC successfully recovered the high-resolution matrices and produced highly similar chromatin loop identifications as those identified from real high-resolution data. As previously observed, Deephic and HiCSR tended to overinflate the contact matrix, thereby leading to a large number of false discoveries of significant interactions. The high false discovery rate is likely due to the preprocessing procedures or loss functions in these models. For example, HiCSR uses a log1p transformation in its preprocessing step, which may inflate low contact frequencies. In addition, Deephic uses a perceptual loss; as a result, its predictions contained unwanted image textual artifacts. Our EnHiC model is specifically designed to account for the unique data properties in the Hi-C matrix; therefore, the EnHiC-predicted matrices faithfully present high-resolution details in the Hi-C matrix.</p>
      <fig position="float" id="btab272-F8">
        <label>Fig. 8.</label>
        <caption>
          <p>Significant chromatin interactions identified in chromosome 17 from 32Mbp to 34Mbp. (<bold>a</bold>) High resolution (HR) Hi-C at 10 kb, (<bold>b</bold>) EnHiC prediction, (<bold>c</bold>) HiCSR prediction, and (<bold>d</bold>) Deephic prediction. Significant interactions were identified using FitHiC and are highlighted in green. Hi-C matrices are plotted on a log1p scale</p>
        </caption>
        <graphic xlink:href="btab272f8" position="float"/>
      </fig>
      <fig position="float" id="btab272-F9">
        <label>Fig. 9.</label>
        <caption>
          <p>Significant chromatin interactions identified in chromosome 19 from 14Mbp to 16Mbp. (<bold>a</bold>) High resolution (HR) Hi-C at 10 kb, (<bold>b</bold>) EnHiC prediction, (<bold>c</bold>) HiCSR prediction, and (<bold>d</bold>) Deephic prediction. Significant interactions were identified using FitHiC and are highlighted in green. Hi-C matrices are plotted on a log1p scale</p>
        </caption>
        <graphic xlink:href="btab272f9" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion and conclusions</title>
    <p>In this study, we proposed a generative adversarial framework, EnHiC, for predicting high-resolution Hi-C matrices from low-resolution input data. Specifically, high-resolution enhancement is achieved through the extraction of rank-1 matrix features from multi-scale low-resolution input samples and subsequent upsampling processes via sub-pixel CNN layers.</p>
    <p>Existing resolution-enhancement models, such as Deephic and HiCSR, treat Hi-C matrices as single-channel images, and leverage on the established neural networks of image super-resolution models. Although such models can produce super-resolution Hi-C matrices, their predictions often overinflate the Hi-C matrix features and sometimes contain unwanted natural image artifacts. Unlike other models, our EnHiC model utilizes the unique properties of Hi-C data.</p>
    <p>Inspired by NMF, our EnHiC model uses similar notions of rank-1 features and matrix factorization. However, our model is different from NMF in the following aspects. First, our model attempts to decompose a set of submatrices, instead of a full matrix. In the decomposition step, it searches for a rank-1 solution that fits all submatrices. Here we limit the rank to 1 to bypass the problem of picking the appropriate number of ranks in a low-rank solution. Second, our model optimizes the rank-1 matrix decomposition via the <italic toggle="yes">Decomposition &amp; Reconstruction Block</italic> in the GAN framework. The difference between the input Hi-C matrix and its rank-1 approximation is characterized by a loss function consisting of the L2 MSE loss and structural dissimilarity.</p>
    <p>We demonstrated the performance of our EnHiC model using Hi-C datasets on three human cell lines. Overall, our EnHiC model evidently outperformed two other GAN-based methods, Deephic and HiCSR, achieving low prediction errors and high reproducibility scores when compared with the true high-resolution data. Moreover, EnHiC model is capable of recovering high-resolution Hi-C matrices across different cell types and from insufficiently sequenced input data. Additionally, we demonstrated that EnHiC-predicted matrices facilitated more accurate and precise detection of TADs and fine-scale chromatin interactions.</p>
    <p>We envision a few possible extensions and future directions based on this work. First, EnHiC uses SCN normalization in the pre-processing step. The SCN normalization helps to reduce systematic biases in Hi-C data, and rescales the intensity values to real numbers between [0,1]. It is possible to add alternative options of other Hi-C normalization methods in the implementation. And we do not expect the choice of normalization methods to have a major impact on the model performance. Second, EnHiC requires the input matrices to be symmetric. In our experiments, when dividing the entire Hi-C matrix into small submatrices, we merged two on-diagonal submatrices with one off-diagonal matrix to generate a symmetric matrix. This divide-and-merge strategy may cause artifacts at the edges of the submatrices. One possible future extension is to build a paired layer that simultaneously estimates the row and column vectors to relax the symmetry requirement. Third, to effectively extract multi-scale rank-1 features, large input matrices are recommended. In the current setting, we used 400 × 400 submatrices to achieve the desired enhancement factor of 16. Increasing the dimension of the input matrices would require more memory allocation and result in a heavier computation load. One possible future extension is to build a distributed implementation to mitigate the burden on each node.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btab272_Supplementary_Data</label>
      <media xlink:href="btab272_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>The authors thank Wei Wu for valuable discussions, and the reviewers for their constructive and valuable suggestions. The method performance tests of this study were performed in the High-Performance Computing Center at UC Riverside.</p>
    <sec>
      <title>Funding</title>
      <p>This work was supported by the U.S. National Institute of Health [R35GM133678].</p>
      <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btab272-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Abdennur</surname><given-names>N.</given-names></string-name>, <string-name><surname>Mirny</surname><given-names>L.A.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Cooler: scalable storage for hi-c data and other genomically labeled arrays</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>311</fpage>–<lpage>316</lpage>.<pub-id pub-id-type="pmid">31290943</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ay</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) <article-title>Statistical confidence estimation for hi-c data reveals regulatory chromatin contacts</article-title>. <source>Genome Res</source>., <volume>24</volume>, <fpage>999</fpage>–<lpage>1011</lpage>.<pub-id pub-id-type="pmid">24501021</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dimmick</surname><given-names>M.C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>HiCSR: a Hi-C super-resolution framework for producing highly realistic contact maps</article-title>. bioRxiv preprint, <source>https://doi.org/10.1101/2020.02.24.961714</source>.</mixed-citation>
    </ref>
    <ref id="btab272-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dixon</surname><given-names>J.R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) <article-title>Topological domains in mammalian genomes identified by analysis of chromatin interactions</article-title>. <source>Nature</source>, <volume>485</volume>, <fpage>376</fpage>–<lpage>380</lpage>.<pub-id pub-id-type="pmid">22495300</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Duan</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2010</year>) <article-title>A three-dimensional model of the yeast genome</article-title>. <source>Nature</source>, <volume>465</volume>, <fpage>363</fpage>–<lpage>367</lpage>.<pub-id pub-id-type="pmid">20436457</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Forcato</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Comparison of computational methods for hi-c data analysis</article-title>. <source>Nat. Methods</source>, <volume>14</volume>, <fpage>679</fpage>–<lpage>685</lpage>.<pub-id pub-id-type="pmid">28604721</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hong</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>DeepHiC: a generative adversarial network for enhancing Hi-C data resolution</article-title>. <source>PLoS Comput. Biol</source>., <volume>16</volume>, <fpage>e1007287</fpage>.<pub-id pub-id-type="pmid">32084131</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Imakaev</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) <article-title>Iterative correction of hi-c data reveals hallmarks of chromosome organization</article-title>. <source>Nat. Methods</source>, <volume>9</volume>, <fpage>999</fpage>–<lpage>1003</lpage>.<pub-id pub-id-type="pmid">22941365</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knight</surname><given-names>P.A.</given-names></string-name>, <string-name><surname>Ruiz</surname><given-names>D.</given-names></string-name></person-group> (<year>2013</year>) <article-title>A fast algorithm for matrix balancing</article-title>. <source>IMA J. Numer. Anal</source>., <volume>33</volume>, <fpage>1029</fpage>–<lpage>1047</lpage>.</mixed-citation>
    </ref>
    <ref id="btab272-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kumar</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Genome contact map explorer: a platform for the comparison, interactive visualization and analysis of genome contact maps</article-title>. <source>Nucleic Acids Res</source>., <volume>45</volume>, <fpage>e152</fpage>.<pub-id pub-id-type="pmid">28973466</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B11">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ledig</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <part-title>Photo-realistic single image super-resolution using a generative adversarial network</part-title>. In <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, pp. <fpage>4681</fpage>–<lpage>4690</lpage>, Honolulu, HI, USA.</mixed-citation>
    </ref>
    <ref id="btab272-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lieberman-Aiden</surname><given-names>E.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) <article-title>Comprehensive mapping of long-range interactions reveals folding principles of the human genome</article-title>. <source>Science</source>, <volume>326</volume>, <fpage>289</fpage>–<lpage>293</lpage>.<pub-id pub-id-type="pmid">19815776</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>Q.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>hicgan infers super resolution hi-c data with generative adversarial networks</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>i99</fpage>–<lpage>i107</lpage>.<pub-id pub-id-type="pmid">31510693</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Liu</surname><given-names>T.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>Z.</given-names></string-name></person-group> (<year>2019</year>) <article-title>HiCNN: a very deep convolutional neural network to better enhance the resolution of Hi-C data</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>4222</fpage>–<lpage>4228</lpage>.<pub-id pub-id-type="pmid">31056636</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ma</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <article-title>Fine-scale chromatin interaction maps reveal the cis-regulatory landscape of human lincRNA genes</article-title>. <source>Nat. Methods</source>, <volume>12</volume>, <fpage>71</fpage>–<lpage>78</lpage>.<pub-id pub-id-type="pmid">25437436</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Mao</surname><given-names>X.-J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Image restoration using convolutional auto-encoders with symmetric skip connections</article-title>. In Proceedings of Advances in Neural Information Processing Systems, pp. 2802–2810, Barcelona, Spain.</mixed-citation>
    </ref>
    <ref id="btab272-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rao</surname><given-names>S.S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2014</year>) <article-title>A 3D map of the human genome at kilobase resolution reveals principles of chromatin looping</article-title>. <source>Cell</source>, <volume>159</volume>, <fpage>1665</fpage>–<lpage>1680</lpage>.<pub-id pub-id-type="pmid">25497547</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Servant</surname><given-names>N.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2015</year>) <article-title>Hic-pro: an optimized and flexible pipeline for Hi-C data processing</article-title>. <source>Genome Biol</source>., <volume>16</volume>, <fpage>259</fpage>.<pub-id pub-id-type="pmid">26619908</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B19">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Shi</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <part-title>Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</part-title>. In <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1874-1883, Las Vegas, NV, USA.</source></mixed-citation>
    </ref>
    <ref id="btab272-B20">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Simonyan</surname><given-names>K.</given-names></string-name>, <string-name><surname>Zisserman</surname><given-names>A.</given-names></string-name></person-group> (<year>2014</year>) <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. <italic toggle="yes">arXiv preprint, arXiv:1409.1556.</italic></mixed-citation>
    </ref>
    <ref id="btab272-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ursu</surname><given-names>O.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Genomedisco: a concordance score for chromosome conformation capture experiments using random walks on contact map graphs</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>2701</fpage>–<lpage>2707</lpage>.<pub-id pub-id-type="pmid">29554289</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wang</surname><given-names>Z.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2004</year>) <article-title>Image quality assessment: from error visibility to structural similarity</article-title>. <source>IEEE Trans. Image Process</source>., <volume>13</volume>, <fpage>600</fpage>–<lpage>612</lpage>.<pub-id pub-id-type="pmid">15376593</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wolff</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Galaxy hicexplorer: a web server for reproducible Hi-C data analysis, quality control and visualization</article-title>. <source>Nucleic Acids Res</source>., <volume>46</volume>, <fpage>W11</fpage>–<lpage>W16</lpage>.<pub-id pub-id-type="pmid">29901812</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yan</surname><given-names>K.-K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Hic-spector: a matrix library for spectral and reproducibility analysis of hi-c contact maps</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>2199</fpage>–<lpage>2201</lpage>.<pub-id pub-id-type="pmid">28369339</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>T.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>HiCREP: assessing the reproducibility of Hi-C data using a stratum-adjusted correlation coefficient</article-title>. <source>Genome Res</source>., <volume>27</volume>, <fpage>1939</fpage>–<lpage>1949</lpage>.<pub-id pub-id-type="pmid">28855260</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018a</year>) <article-title>Enhancing Hi-C data resolution with deep convolutional neural network HiCPlus</article-title>. <source>Nat. Commun</source>., <volume>9</volume>, <fpage>1</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">29317637</pub-id></mixed-citation>
    </ref>
    <ref id="btab272-B27">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>Y.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018b</year>) <part-title>Residual dense network for image super-resolution</part-title>. In <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>, pp. <fpage>2472</fpage>–<lpage>2481</lpage>, Salt Lake City, UT, USA.</mixed-citation>
    </ref>
  </ref-list>
</back>
