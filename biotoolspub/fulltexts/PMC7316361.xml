<?properties open_access?>
<?subarticle pcbi.1007973.r001?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS Comput Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS Comput. Biol</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">ploscomp</journal-id>
    <journal-title-group>
      <journal-title>PLoS Computational Biology</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1553-734X</issn>
    <issn pub-type="epub">1553-7358</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7316361</article-id>
    <article-id pub-id-type="pmid">32542056</article-id>
    <article-id pub-id-type="publisher-id">PCOMPBIOL-D-19-01642</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1007973</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Cell Biology</subject>
          <subj-group>
            <subject>Cellular Types</subject>
            <subj-group>
              <subject>Animal Cells</subject>
              <subj-group>
                <subject>Neurons</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Cellular Neuroscience</subject>
            <subj-group>
              <subject>Neurons</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
              <subj-group>
                <subject>Genetic Algorithms</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
            <subj-group>
              <subject>Genetic Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Optimization</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Applied Mathematics</subject>
            <subj-group>
              <subject>Algorithms</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Simulation and Modeling</subject>
          <subj-group>
            <subject>Algorithms</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Sensory Perception</subject>
            <subj-group>
              <subject>Vision</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Sensory Perception</subject>
            <subj-group>
              <subject>Vision</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Social Sciences</subject>
        <subj-group>
          <subject>Psychology</subject>
          <subj-group>
            <subject>Sensory Perception</subject>
            <subj-group>
              <subject>Vision</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neuronal Tuning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Neural Networks</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neural Networks</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>XDream: Finding preferred stimuli for visual neurons using generative networks and gradient-free optimization</article-title>
      <alt-title alt-title-type="running-head">XDream: Finding preferred visual stimuli via optimization</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-5555-3217</contrib-id>
        <name>
          <surname>Xiao</surname>
          <given-names>Will</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Data curation</role>
        <role content-type="https://casrai.org/credit/">Formal analysis</role>
        <role content-type="https://casrai.org/credit/">Investigation</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Software</role>
        <role content-type="https://casrai.org/credit/">Visualization</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3505-8475</contrib-id>
        <name>
          <surname>Kreiman</surname>
          <given-names>Gabriel</given-names>
        </name>
        <role content-type="https://casrai.org/credit/">Conceptualization</role>
        <role content-type="https://casrai.org/credit/">Funding acquisition</role>
        <role content-type="https://casrai.org/credit/">Methodology</role>
        <role content-type="https://casrai.org/credit/">Project administration</role>
        <role content-type="https://casrai.org/credit/">Resources</role>
        <role content-type="https://casrai.org/credit/">Supervision</role>
        <role content-type="https://casrai.org/credit/">Visualization</role>
        <role content-type="https://casrai.org/credit/">Writing – original draft</role>
        <role content-type="https://casrai.org/credit/">Writing – review &amp; editing</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
        <xref ref-type="aff" rid="aff003">
          <sup>3</sup>
        </xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Department of Molecular and Cellular Biology, Harvard University, Cambridge, Massachusetts, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Center for Brains, Minds, and Machines, Boston, Massachusetts, United States of America</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Department of Ophthalmology, Boston Children’s Hospital, Boston, Massachusetts, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Fyshe</surname>
          <given-names>Alona</given-names>
        </name>
        <role>Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>University of Alberta, CANADA</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>xiaow@fas.harvard.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>15</day>
      <month>6</month>
      <year>2020</year>
    </pub-date>
    <volume>16</volume>
    <issue>6</issue>
    <elocation-id>e1007973</elocation-id>
    <history>
      <date date-type="received">
        <day>24</day>
        <month>9</month>
        <year>2019</year>
      </date>
      <date date-type="accepted">
        <day>21</day>
        <month>5</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2020 Xiao, Kreiman</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Xiao, Kreiman</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pcbi.1007973.pdf"/>
    <abstract>
      <p>A longstanding question in sensory neuroscience is what types of stimuli drive neurons to fire. The characterization of effective stimuli has traditionally been based on a combination of intuition, insights from previous studies, and luck. A new method termed XDream (E<bold>X</bold>tending <bold>D</bold>eepDream with <bold>r</bold>eal-time <bold>e</bold>volution for <bold>a</bold>ctivation <bold>m</bold>aximization) combined a generative neural network and a genetic algorithm in a closed loop to create strong stimuli for neurons in the macaque visual cortex. Here we extensively and systematically evaluate the performance of XDream. We use ConvNet units as <italic>in silico</italic> models of neurons, enabling experiments that would be prohibitive with biological neurons. We evaluated how the method compares to brute-force search, and how well the method generalizes to different neurons and processing stages. We also explored design and parameter choices. XDream can efficiently find preferred features for visual units without any prior knowledge about them. XDream extrapolates to different layers, architectures, and developmental regimes, performing better than brute-force search, and often better than exhaustive sampling of &gt;1 million images. Furthermore, XDream is robust to choices of multiple image generators, optimization algorithms, and hyperparameters, suggesting that its performance is locally near-optimal. Lastly, we found no significant advantage to problem-specific parameter tuning. These results establish expectations and provide practical recommendations for using XDream to investigate neural coding in biological preparations. Overall, XDream is an efficient, general, and robust algorithm for uncovering neuronal tuning preferences using a vast and diverse stimulus space. XDream is implemented in Python, released under the MIT License, and works on Linux, Windows, and MacOS.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000002</institution-id>
            <institution>National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>R01EY026025</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3505-8475</contrib-id>
          <name>
            <surname>Kreiman</surname>
            <given-names>Gabriel</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>STC CCF-1231216</award-id>
      </award-group>
      <funding-statement>W.X. and G.K. are supported by the Center for Brains, Minds and Machines funded by NSF528STC award CCF-1231216 and also by NIH R01EY026025. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. <ext-link ext-link-type="uri" xlink:href="https://www.nsf.gov">https://www.nsf.gov</ext-link><ext-link ext-link-type="uri" xlink:href="https://www.nih.gov">https://www.nih.gov</ext-link>.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="1"/>
      <page-count count="15"/>
    </counts>
    <custom-meta-group>
      <custom-meta>
        <meta-name>PLOS Publication Stage</meta-name>
        <meta-value>vor-update-to-uncorrected-proof</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>Publication Update</meta-name>
        <meta-value>2020-06-25</meta-value>
      </custom-meta>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>All data underlying the findings described in their manuscript have been made available in the publicly available GitHub repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/willwx/XDream">https://github.com/willwx/XDream</ext-link>.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>All data underlying the findings described in their manuscript have been made available in the publicly available GitHub repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/willwx/XDream">https://github.com/willwx/XDream</ext-link>.</p>
  </notes>
</front>
<body>
  <disp-quote>
    <p>This is a <italic>PLOS Computational Biology</italic> Software paper.</p>
  </disp-quote>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>What stimuli excite a neuron, and how can we find them? Consider vision as a paradigmatic example, the selection of stimuli to probe neural activity has shaped the understanding of how visual neurons represent information. It is practically impossible to exhaustively evaluate neuronal responses to images, due to the combinatorially large number of possible images. Instead, investigators have traditionally selected stimuli guided by natural image statistics, behavioral relevance, theoretical postulates about internal representations, intuitions from previous studies, and serendipitous findings. Stimuli selected in this way underlie our current understandings of how circular center-surround receptive fields [<xref rid="pcbi.1007973.ref001" ref-type="bibr">1</xref>] give rise to orientation tuning [<xref rid="pcbi.1007973.ref002" ref-type="bibr">2</xref>], then to encoding of more complex shapes such as curvatures [<xref rid="pcbi.1007973.ref003" ref-type="bibr">3</xref>, <xref rid="pcbi.1007973.ref004" ref-type="bibr">4</xref>], and further to selective responses to complex objects such as faces [<xref rid="pcbi.1007973.ref005" ref-type="bibr">5</xref>–<xref rid="pcbi.1007973.ref007" ref-type="bibr">7</xref>].</p>
    <p>Despite the progress made in understanding visual cortex by testing limited sets of hand-chosen stimuli, these experiments could be missing the true feature preferences of neurons. In other words, there could be other images that drive visual neurons better than those found so far. Such images could lead us to revisit our current descriptions of feature tuning in visual cortex.</p>
    <p>A recently introduced method shows promise to begin bridging the gap. Named XDream (e<bold>X</bold>tending <bold>D</bold>eepDream with <bold>r</bold>eal-time <bold>e</bold>volution for <bold>a</bold>ctivation <bold>m</bold>aximization), this method combines a genetic algorithm and a deep generative neural network [<xref rid="pcbi.1007973.ref008" ref-type="bibr">8</xref>]—both inspired by previous work [<xref rid="pcbi.1007973.ref009" ref-type="bibr">9</xref>–<xref rid="pcbi.1007973.ref012" ref-type="bibr">12</xref>]—to evolve images that trigger high activation in neurons [<xref rid="pcbi.1007973.ref013" ref-type="bibr">13</xref>]. XDream can generate strong stimuli for neurons in macaque inferior temporal (IT) and primary visual cortex (V1).</p>
    <p>The performance and design options of XDream have not been thoroughly evaluated, due to the time-intensiveness of neuronal recordings and the difficulty to fully control experimental variables. To overcome these challenges, here we test the performance of XDream using state-of-the-art <italic>in silico</italic> models of visual neurons in lieu of real neurons, in the same spirit of [<xref rid="pcbi.1007973.ref014" ref-type="bibr">14</xref>]. Specifically, we use convolutional neural networks (ConvNets) pre-trained on visual recognition tasks as an approximation to the computations performed along ventral visual cortex [<xref rid="pcbi.1007973.ref015" ref-type="bibr">15</xref>–<xref rid="pcbi.1007973.ref017" ref-type="bibr">17</xref>]. Using these models as a proxy for real neurons allows us to compare synthetic stimuli with a large set of reference images, to evaluate XDream’s performance across processing stages, model architectures, and training regimes, to empirically optimize algorithm design and parameter choices in a systematic fashion, and to disentangle the effects of neuronal response stochasticity.</p>
    <p>Although there is a rich literature in computer science on feature visualization [<xref rid="pcbi.1007973.ref018" ref-type="bibr">18</xref>–<xref rid="pcbi.1007973.ref021" ref-type="bibr">21</xref>], we focus on the more biologically relevant scenario where there is no information about the architecture and weights of the target model, and where we only have access to a few, potentially stochastic, activation values from the neurons. These conditions reflect those prevailing in neuronal recordings and are fundamentally different from the assumptions made in computer science studies.</p>
    <p>Under these realistic constraints, we show that XDream still reliably and efficiently uncovers preferred features of units with a wide range of response properties, generalizing to different processing stages within a network, different network architectures, and different training datasets. Furthermore, XDream performed equally well with a wide range of algorithmic and parameter choices. Based on these results, we suggest parameters to use and results that can be expected when using XDream to investigate neuronal tuning properties. Our findings suggest that XDream is a general and robust method for investigating neuronal preferences in visual cortex.</p>
    <sec id="sec002">
      <title>Design and implementation</title>
      <sec id="sec003">
        <title>Overview</title>
        <p>XDream combines an image generator (e.g., the generator in a generative adversarial network), a target neuron (e.g., a unit in a ConvNet), and a non-gradient-based optimization algorithm (e.g., a genetic algorithm) in a closed loop. In each iteration, the optimization algorithm proposes a set of codes, the image generator synthesizes the codes into images, the images are evaluated by the target neuron to produce one scalar score per image, and the scores are used by the optimization algorithm to propose a new set of codes (<xref ref-type="fig" rid="pcbi.1007973.g001">Fig 1</xref>). Importantly, no optimization gradient is needed from the neuron.</p>
        <fig id="pcbi.1007973.g001" orientation="portrait" position="float">
          <object-id pub-id-type="doi">10.1371/journal.pcbi.1007973.g001</object-id>
          <label>Fig 1</label>
          <caption>
            <title>Overview of the XDream method.</title>
            <p><bold>a)</bold>, XDream combines an image generator, a target neuron, and a non-gradient-based optimization algorithm. <bold>b,c)</bold>, An example experiment targeting CaffeNet layer fc8, unit 1. <bold>b)</bold>, mean activation achieved over 500 generations, 20 images per generation (10,000 total image presentations). <bold>c)</bold>, Images obtained at a few example generations indicated by minor x-ticks in <bold>b)</bold>. The activation to each image is labeled above the image and indicated by the color of the margin. <bold>d)</bold>, The top 5 images among 10,000 random images from ImageNet (ILSVRC12 dataset, &gt;1.4 M images). The number of random images is matched to the number of images presented during optimization. The top image in all &gt;1.4 M images is shown in <xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2b</xref>.</p>
          </caption>
          <graphic xlink:href="pcbi.1007973.g001"/>
        </fig>
        <p>The image generators and optimization algorithms are detailed below. The code is implemented in Python 3 and runs on Linux, Windows, and MacOS, although the former two platforms are required to use GPU acceleration. The main dependency is Caffe [<xref rid="pcbi.1007973.ref022" ref-type="bibr">22</xref>] (<ext-link ext-link-type="uri" xlink:href="https://caffe.berkeleyvision.org/">https://caffe.berkeleyvision.org/</ext-link>) or PyTorch (<ext-link ext-link-type="uri" xlink:href="https://pytorch.org/">https://pytorch.org/</ext-link>), which are required for neural network computation. Other dependencies are standard Python packages and listed in requirements.txt in the repository, including: numpy, h5py, opencv-python, scipy, and scikit-image.</p>
      </sec>
    </sec>
    <sec id="sec004">
      <title>Image generators</title>
      <p>An image generator is a function that outputs an image given some representation of that image (an <italic>image code</italic>) as input. We tested the family of DeePSiM generators developed in [<xref rid="pcbi.1007973.ref008" ref-type="bibr">8</xref>]; they are generative adversarial networks trained to invert each layer of AlexNet [<xref rid="pcbi.1007973.ref023" ref-type="bibr">23</xref>]. The pre-trained models are available at <ext-link ext-link-type="uri" xlink:href="https://lmb.informatik.uni-freiburg.de/people/dosovits/code.html">https://lmb.informatik.uni-freiburg.de/people/dosovits/code.html</ext-link>. We have converted the models into PyTorch for convenience for future research. Links to the converted models are available in the code repository (see Code availability below). We used the image generator inverting the fc6 layer by default except in <xref ref-type="supplementary-material" rid="pcbi.1007973.s004">S4 Fig</xref>, where we compared different generators. An alternative version of the DeePSiM-fc6 generator was trained on the Places-365 dataset using code from [<xref rid="pcbi.1007973.ref008" ref-type="bibr">8</xref>] and a pre-trained classifier [<xref rid="pcbi.1007973.ref024" ref-type="bibr">24</xref>].</p>
    </sec>
    <sec id="sec005">
      <title>Fitness function</title>
      <p>The key metric XDream optimizes is a scalar value we refer to as <italic>fitness</italic>, which is associated with each image. In the neuroscience context, a fitness function can be the stimulus-evoked spike count for a neuron in visual cortex. In the current study, the fitness function is the activation of the target unit in a ConvNet.</p>
    </sec>
    <sec id="sec006">
      <title>Optimization algorithms</title>
      <p>An optimization algorithm in the context of XDream is a function that iteratively proposes a set of <italic>n</italic> image codes (real-valued vectors) or <italic>codes</italic> for short, <italic>c</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, …, <italic>n</italic>, and then uses their corresponding <italic>fitness</italic> values <italic>y</italic><sub><italic>i</italic></sub>, <italic>i</italic> = 1, …, <italic>n</italic> to propose a new set of codes expected to have higher fitness. We used a genetic algorithm by default, but also considered two other algorithms: finite-difference gradient descent (FDGD) and natural evolution strategies [<xref rid="pcbi.1007973.ref025" ref-type="bibr">25</xref>] (NES). Implementation details for the optimization algorithms are available in <xref ref-type="supplementary-material" rid="pcbi.1007973.s011">S1 Text</xref>.</p>
    </sec>
    <sec id="sec007">
      <title>Computing environment</title>
      <p>Neural network computations were performed on NVIDIA GPUs. Portions of this research were conducted on the O2 High Performance Compute Cluster supported by the Research Computing Group, at Harvard Medical School. See <ext-link ext-link-type="uri" xlink:href="http://rc.hms.harvard.edu">http://rc.hms.harvard.edu</ext-link> for more information.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="sec008">
    <title>Results</title>
    <sec id="sec009">
      <title>Random exploration of stimulus space is inefficient</title>
      <p>A common approach for exploring neuronal selectivity is to use arbitrarily selected images, often from a limited number of categories (for example in [<xref rid="pcbi.1007973.ref007" ref-type="bibr">7</xref>, <xref rid="pcbi.1007973.ref026" ref-type="bibr">26</xref>]). Thus, we considered random exploration as a baseline for comparison. We used the AlexNet architecture as the target model [<xref rid="pcbi.1007973.ref023" ref-type="bibr">23</xref>] (implemented as CaffeNet; <xref ref-type="supplementary-material" rid="pcbi.1007973.s007">S1 Table</xref>) and sampled images from ImageNet [<xref rid="pcbi.1007973.ref027" ref-type="bibr">27</xref>] (ILSVRC12 dataset, 1,431,167 images), a large dataset common in computer vision that also contains the training set of CaffeNet. We randomly sampled <italic>n</italic> images either from all of ImageNet or from 10 categories randomly selected from the 1,000 training categories in ImageNet (<italic>n</italic>/10 images per category). For units in different layers of the network, we evaluated the activation values in response to these images and calculated the <italic>relative activation</italic>, defined as the ratio between the activations in the <italic>n</italic> random images and the maximum activation in all of ImageNet. By definition, the relative activation for the best image in ImageNet is 1, which is also an upper bound on the observed relative activation values when using random sampling. Randomly selected images typically yielded relative activation values well below 1 (<xref ref-type="supplementary-material" rid="pcbi.1007973.s001">S1 Fig</xref>). As expected, the maximum observed relative activation increased with <italic>n</italic> but only did so slowly, with near-logarithmic growth. Moreover, for later layers (e.g., fc8), sampling from only 10 categories yielded significantly worse results than sampling completely randomly, which we hypothesize is because the small number of categories imposes a bottleneck on the diversity of high-level features represented. In neuroscience studies, category selection is clearly not completely random: Investigators may have intuitions and prior knowledge about the types of stimuli that are more likely to be effective. To the extent that those intuitions are correct, they can enhance the search process. However, those intuitions are seldom guided by systematic examination of stimulus space and could well miss important types of stimuli.</p>
    </sec>
    <sec id="sec010">
      <title>XDream can find strong stimuli for neurons</title>
      <p>XDream has three key components: an image generator representing the search space; an objective function given by the activation of the target unit guiding the search; and an optimization algorithm performing the search (<xref ref-type="fig" rid="pcbi.1007973.g001">Fig 1a</xref>). In each generation, the generator creates images from their latent representations (<italic>codes</italic>), the target unit activation is evaluated for each of the generated images, and the optimizer refines the codes based on the activation values. Initialized randomly (examples shown in <xref ref-type="fig" rid="pcbi.1007973.g001">Fig 1a</xref>), the algorithm is iterated for 10,000 total image presentations, a relatively small and accessible number in a typical neuroscience experiment [<xref rid="pcbi.1007973.ref013" ref-type="bibr">13</xref>]. Crucially, the algorithm does not use any prior knowledge about the architecture or weights of the target model.</p>
      <p>An example experiment with unit 1 in the output layer (layer fc8) of CaffeNet is shown in <xref ref-type="fig" rid="pcbi.1007973.g001">Fig 1b and 1c</xref>. In 500 generations of 20 images each, the activation of the target unit increased rapidly and saturated at approximately generation 300. <xref ref-type="fig" rid="pcbi.1007973.g001">Fig 1c</xref> shows example images at a few generations (log-spaced to show a range of activations), illustrating the evolution of the images from the initial noise pattern to the final image. In the following analyses, we concentrate on the best image in the last generation, which we refer to as the <italic>optimized image</italic>. However, it is worth noting that responses to all the 10,000 unique images during the evolution may illuminate features of the neuron’s tuning (see Discussion).</p>
      <p>How strong was the activation achieved by XDream-generated images? We compared the optimized image to images from ImageNet. Unit 1 in layer fc8 was trained to be a “goldfish” detector. Correspondingly, when we randomly sampled 10,000 images from ImageNet, the best images are photos of goldfish (<xref ref-type="fig" rid="pcbi.1007973.g001">Fig 1d</xref>). The highest activation value observed in this random sample was 30.67. The best image from ImageNet for this unit was a picture of a goldfish and elicited an activation of 40.55 (<xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2b</xref>). Consistent with <xref ref-type="supplementary-material" rid="pcbi.1007973.s001">S1 Fig</xref>, the best image found by random sampling produced a much lower activation value than the best example in ImageNet. In comparison, the optimized image generated by XDream elicited an activation of 72.42. In other words, using a limited number of presentations, XDream generated images that elicited higher activation than any natural image from ImageNet. We refer to such images with relative activation &gt; 1 as <italic>super stimuli</italic>.</p>
      <fig id="pcbi.1007973.g002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1007973.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>XDream generalizes across layers, architectures, and training sets.</title>
          <p><bold>a)</bold>, Violin plot showing the distributions of relative activation (activation of optimized stimulus relative to highest activation in &gt;1.4 M ImageNet images) over 100 randomly selected units per layer. For each target model, we investigated early, middle, late, and output layers (see <xref ref-type="supplementary-material" rid="pcbi.1007973.s007">S1 Table</xref> for the specific layers). The violin contours indicate kernel density estimates of the distributions, white circles indicate the medians, thick bars indicate first and third quartiles, and whiskers indicate 1.5× interquartile ranges. For comparison, grey boxes (interquartile ranges) and lines (medians) show the distribution of maximum relative activation for 10,000 random ImageNet images. The horizontal dashed line corresponds to the best ImageNet image. <bold>b)</bold>, Optimized (top row) and best ImageNet (bottom row) images and activations for 10 example units across layers and architectures. For output units, corresponding category labels are shown below the images.</p>
        </caption>
        <graphic xlink:href="pcbi.1007973.g002"/>
      </fig>
    </sec>
    <sec id="sec011">
      <title>XDream generalizes across layers, architectures, and training sets</title>
      <p>The default generative network used in XDream was trained to invert the internal representations at layer fc6 of CaffeNet, which was in turn trained on ImageNet [<xref rid="pcbi.1007973.ref008" ref-type="bibr">8</xref>]. Could this generator allow XDream to generalize to other network layers, architectures, and training sets? If XDream is specific to certain layers and architectures, or specific to ImageNet-trained networks, this may limit its applicability to real neurons.</p>
      <p>We first assessed whether XDream could extrapolate to other layers in CaffeNet by selecting 100 units respectively from the early, middle, late, and output layers of CaffeNet (<xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2a</xref>). XDream was able to find optimized images that are better than the best randomly selected images across all layers (<italic>p</italic> &lt; 10<sup>−16</sup>, false discovery rate (FDR) corrected for 28 tests in this section). The optimized images were also significantly better than the best images in ImageNet (<italic>p</italic> &lt; 10<sup>−9</sup>, FDR corrected).</p>
      <p>Next, we tested 100 units from each of 4 layers from 5 different network architectures: ResNet-v2 152- and 269-layer variants [<xref rid="pcbi.1007973.ref028" ref-type="bibr">28</xref>], Inception-v3 [<xref rid="pcbi.1007973.ref029" ref-type="bibr">29</xref>], Inception-v4, and Inception-ResNet-v2 [<xref rid="pcbi.1007973.ref030" ref-type="bibr">30</xref>]. These models were all trained on ImageNet. XDream was able to generate better images than the best random images for the vast majority of units across all layers and architectures (<xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2a</xref>; <italic>p</italic> &lt; 10<sup>−8</sup> across layers) except the early layer of Inception-v3 (<italic>p</italic> = 0.2) and of Inception-ResNet-v2 (<italic>p</italic> = 0.09). With the same exceptions, XDream generated super stimuli for all other tested layers (<italic>p</italic> = 0.01 for the early layer of Inception-v4, <italic>p</italic> = 2 × 10<sup>−4</sup> for the middle layer of Inception-ResNet-v2, and <italic>p</italic> &lt; 10<sup>−9</sup> for all other layers). Example optimized images for units in different layers and architectures are shown in <xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2b</xref> and <xref ref-type="supplementary-material" rid="pcbi.1007973.s002">S2 Fig</xref>. Furthermore, several generators trained on different layer representations performed equally well across classifier layers (<xref ref-type="supplementary-material" rid="pcbi.1007973.s004">S4 Fig</xref>).</p>
      <p>Finally, we tested the ability of XDream to optimize unit responses when the generator and target networks are trained on different datasets. We tested PlacesCNN [<xref rid="pcbi.1007973.ref031" ref-type="bibr">31</xref>], a network with the same architecture as CaffeNet but trained on a different dataset, PlacesCNN. PlacesCNN also contains photographic images, but they mainly depict scenes rather than objects. Again, XDream was able to find super stimuli across all layers in this network (<xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2a</xref>, last four distributions; <italic>p</italic> &lt; 10<sup>−6</sup> across layers), even when using a generative network trained on different images. Conversely, when using a generator trained on the Places dataset, XDream still performed similarly well in optimizing CaffeNet and PlacesCNN (<xref ref-type="supplementary-material" rid="pcbi.1007973.s004">S4 Fig</xref>).</p>
      <p>These results show that XDream can efficiently create images that trigger high activations in a target unit without making assumptions about the type of images a unit may prefer and without any knowledge of the target model architecture or connectivity, suggesting that XDream may well be applicable to biological neurons. Furthermore, XDream generalizes across layers in a ConvNet, while different layers roughly correspond to areas along the ventral visual stream [<xref rid="pcbi.1007973.ref017" ref-type="bibr">17</xref>, <xref rid="pcbi.1007973.ref032" ref-type="bibr">32</xref>, <xref rid="pcbi.1007973.ref033" ref-type="bibr">33</xref>], suggesting that XDream may also generalize to several ventral stream areas. Consistent with this observation, results from [<xref rid="pcbi.1007973.ref013" ref-type="bibr">13</xref>] indicated that XDream can find optimized stimuli for V1 as well as inferior temporal cortex (IT) neurons.</p>
    </sec>
    <sec id="sec012">
      <title>XDream is robust to different initial conditions</title>
      <p>XDream starts the search from an initial generation of image codes. In <xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2</xref>, we always initialized the algorithm using the same set of 20 random image codes, 6 of which are shown in <xref ref-type="fig" rid="pcbi.1007973.g001">Fig 1a</xref>. Does the choice of initial conditions affect the results?</p>
      <p>To address this question, we first tested how much the particular choice of random initial codes matters. For each target unit, we repeated the experiment using 10 different random initializations and compared the optimized relative activation to that of the original random initialization. Different initial conditions produced slightly better or worse relative activation values centered around a mean difference of 0, and the standard deviation of the fractional change was lower than 10% (<xref ref-type="fig" rid="pcbi.1007973.g003">Fig 3a</xref>).</p>
      <fig id="pcbi.1007973.g003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1007973.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Comparison of different initializations.</title>
          <p><bold>a,b,c)</bold>, Effect of using different random initializations. <bold>a)</bold>, Distributions of fractional change in optimized activation if 10 different random initializations are used. <bold>b)</bold>, Left, relative activation in response to images interpolated (in the code space) between two optimized images from two different random initial conditions. Right, activation normalized to the endpoints (location 0 or 1), highlighting the change in activation away from the endpoints. <bold>c)</bold>, Optimized images from different initializations for 3 example units in the output layer (one unit per row). Activation values are shown above each image. <bold>d)</bold>, Good versus bad initializations. For each target unit, its best, middle, or worst 20 images from ImageNet were used as the initial generation. The images were converted to the image code space using either an optimization method (“opt”) or an inversion method (“ivt”; Methods). Left to right within the opt and ivt groups are results from initialization with the worst, middle, and best 20 images. Random initialization is shown for comparison. The open and solid violins show the distributions, in the first and last generation respectively, of relative activation over 100 units in each layer.</p>
        </caption>
        <graphic xlink:href="pcbi.1007973.g003"/>
      </fig>
      <p>Similar activation values notwithstanding, the optimized images were different on a pixel level (<xref ref-type="fig" rid="pcbi.1007973.g003">Fig 3b</xref>); they may comprise an “invariance manifold” that contains similar, but not identical, images eliciting comparable activation values. What might this invariance manifold look like? To explore this question, in CaffeNet layers conv2, conv4, fc6, and fc8, we linearly interpolated between two separately optimized images (from different initializations) in the image code space, and measured target unit activation in response to the interpolated images (<xref ref-type="fig" rid="pcbi.1007973.g003">Fig 3b</xref>). The interpolated images were much stronger stimuli compared to the majority of ImageNet images. However, particularly in layers fc6 and fc8, the interpolation midpoint activated the units less strongly than either endpoint, suggesting either that the sets of strong stimuli are disjoint, or that the invariance manifolds may have non-convex geometry. Studies have reported visual neurons that prefer seemingly unrelated stimuli. It remains an interesting open question to identify whether there exists a feature representation space in which neuronal tuning functions have “simple” geometry.</p>
      <p>Next, we tested whether there are particularly good or bad ways of choosing the initial stimuli. We selected, separately for each target unit, the 20 ImageNet images that led to the highest, middle, and lowest activation values and used those images to form the initial population (<xref ref-type="fig" rid="pcbi.1007973.g003">Fig 3d</xref>). To convert images into image codes comprising the initial population, we used either the “opt” or the “ivt” algorithm (Methods). Initializing with better or worse natural images did not improve the optimized images in the conv2 layer (<italic>p</italic> = 0.87 and 0.19 for “opt” and “ivt,” respectively, FDR-corrected for 8 tests in this and the next sentence). In higher layers, initializing with the best natural images led to slightly higher relative activation values (<xref ref-type="fig" rid="pcbi.1007973.g003">Fig 3d</xref>; <xref rid="pcbi.1007973.t001" ref-type="table">Table 1</xref>; <italic>p</italic> &lt; 5 × 10<sup>−3</sup> for “opt” and <italic>p</italic> &lt; 10<sup>−10</sup> for “ivt” across layers). We speculate that the improvement in higher layers is because units in deeper layers are progressively more selective, making it more difficult to optimize their responses. Therefore, more optimal initializations are beneficial. However, in an actual neurophysiology experiment, it is unlikely that the investigator would know, <italic>a priori</italic>, such good stimuli as the best of 1.4 M images. Meanwhile, initializing with the middle or worst natural stimuli were similar to initializing with random images codes. Therefore, initializing randomly seems reasonable.</p>
      <table-wrap id="pcbi.1007973.t001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1007973.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Effect of using good vs. bad initialization.</title>
        </caption>
        <alternatives>
          <graphic id="pcbi.1007973.t001g" xlink:href="pcbi.1007973.t001"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="2" style="border-bottom:thick" colspan="1">Encoding alg.</th>
                <th align="left" rowspan="2" style="border-right:thick;border-bottom:thick" colspan="1">Measure</th>
                <th align="left" colspan="4" rowspan="1">Layer</th>
              </tr>
              <tr>
                <th align="right" style="border-bottom:thick" rowspan="1" colspan="1">conv2</th>
                <th align="right" style="border-bottom:thick" rowspan="1" colspan="1">conv4</th>
                <th align="right" style="border-bottom:thick" rowspan="1" colspan="1">fc6</th>
                <th align="right" style="border-bottom:thick" rowspan="1" colspan="1">fc8</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">opt</td>
                <td align="left" style="border-right:thick" rowspan="1" colspan="1">slope</td>
                <td align="char" char="." rowspan="1" colspan="1">0.010</td>
                <td align="right" rowspan="1" colspan="1">0.037</td>
                <td align="right" rowspan="1" colspan="1">0.047</td>
                <td align="right" rowspan="1" colspan="1">0.056</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" style="border-right:thick" rowspan="1" colspan="1">p-value</td>
                <td align="char" char="." rowspan="1" colspan="1">0.87</td>
                <td align="right" rowspan="1" colspan="1">0.004</td>
                <td align="right" rowspan="1" colspan="1">0.004</td>
                <td align="right" rowspan="1" colspan="1">7 × 10<sup>−5</sup></td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">ivt</td>
                <td align="left" style="border-right:thick" rowspan="1" colspan="1">slope</td>
                <td align="char" char="." rowspan="1" colspan="1">0.044</td>
                <td align="right" rowspan="1" colspan="1">0.113</td>
                <td align="right" rowspan="1" colspan="1">0.241</td>
                <td align="right" rowspan="1" colspan="1">0.353</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1"/>
                <td align="left" style="border-right:thick" rowspan="1" colspan="1">p-value</td>
                <td align="char" char="." rowspan="1" colspan="1">0.19</td>
                <td align="right" rowspan="1" colspan="1">7 × 10<sup>−11</sup></td>
                <td align="right" rowspan="1" colspan="1">4 × 10<sup>−22</sup></td>
                <td align="right" rowspan="1" colspan="1">5 × 10<sup>−77</sup></td>
              </tr>
            </tbody>
          </table>
        </alternatives>
        <table-wrap-foot>
          <fn id="t001fn001">
            <p>For each unit, the 20 worst, middle, and best images from ImageNet, as ranked by that unit, were used to initialize the genetic algorithm. The images were converted to image codes using one of two encoding algorithms, “opt” or “ivt” (see Methods). The slope was calculated, by linear regression, for relative activation (median across 100 random units each layer) as a function of the initialization ({0,1,2} for {worst, middle, best}, respectively). Thus, the slope quantifies the improvement in relative activation when a better initialization is used (worst → middle or middle → best).</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>To summarize, initializing the algorithm with different random conditions resulted in only a small variation in the optimized image activation, and the optimized images were similar, although not identical, at the pixel level. Initializing with prior knowledge has little to no effect on the optimized image activation, unless the seed is comparable to the best image in ∼ 1 M images and only in later layers.</p>
    </sec>
    <sec id="sec013">
      <title>Different optimization algorithms can be incorporated into XDream, but the genetic algorithm consistently works well</title>
      <p>An important component of XDream is the optimization algorithm. The results shown thus far were based on using a genetic algorithm as the optimization algorithm, a choice inspired by previous work [<xref rid="pcbi.1007973.ref009" ref-type="bibr">9</xref>–<xref rid="pcbi.1007973.ref011" ref-type="bibr">11</xref>]. Here, we compared the genetic algorithm to two additional algorithms, a naïve finite-difference gradient descent algorithm (FDGD; Methods) and Natural Evolution Strategies (NES; [<xref rid="pcbi.1007973.ref025" ref-type="bibr">25</xref>], Methods). NES has been used in a related problem [<xref rid="pcbi.1007973.ref034" ref-type="bibr">34</xref>]. FDGD and NES were significantly worse than the genetic algorithm in CaffeNet conv2 (<italic>p</italic> &lt; 10<sup>−13</sup>, FDR corrected for 20 tests here and in the next section) and conv4 layers (<italic>p</italic> &lt; 10<sup>−3</sup>). Yet, both FDGD and NES were significantly better than the genetic algorithm in CaffeNet fc6 (<italic>p</italic> &lt; 10<sup>−16</sup>), fc8 (<italic>p</italic> &lt; 10<sup>−16</sup>), and Inception-ResNet-v2 classifier layers (<italic>p</italic> &lt; 10<sup>−12</sup>; <xref ref-type="fig" rid="pcbi.1007973.g004">Fig 4a</xref>).</p>
      <fig id="pcbi.1007973.g004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pcbi.1007973.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Comparison of optimization algorithms and their robustness to noise.</title>
          <p>We compared 3 gradient-free optimization algorithms (Methods): a genetic algorithm, finite-difference gradient descent (FDGD), and Natural Evolution Strategies (NES; [<xref rid="pcbi.1007973.ref025" ref-type="bibr">25</xref>]). Left and right half of each violin, respectively, correspond to noiseless and noisy units. Dashed lines inside the violins indicate quartiles of the distribution. Otherwise, format of the plot is as in <xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2a</xref>. <bold>b)</bold>, The performance of the genetic algorithm gradually improves with decreasing amounts of noise within a neurophysiologically relevant range. Format of the plot is as in <xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2a</xref> except that the violins are horizontal. On the right, 3 alternative scales for the y-axis are shown, for comparison with common ways of assessing noise.</p>
        </caption>
        <graphic xlink:href="pcbi.1007973.g004"/>
      </fig>
    </sec>
    <sec id="sec014">
      <title>XDream is robust to noise in neuronal responses</title>
      <p>An important difference between model units and real neurons is the lack of noise in model unit activations. Upon presenting the same image, a model unit returns a deterministic activation value. In contrast, in biological neurons, the same image can evoke different responses on repeated presentations (even though trial-averaged responses may be highly consistent; see [<xref rid="pcbi.1007973.ref035" ref-type="bibr">35</xref>]). To test whether XDream could still find super stimuli with noisy units, we implemented a simple model of stochasticity in the units by using the true activation value to control the rate of a homogeneous Poisson process, from which the “observed” activation value on a single trial was drawn (Methods). Homogeneous Poisson processes have been used extensively to model stochasticity in cortical neurons [<xref rid="pcbi.1007973.ref036" ref-type="bibr">36</xref>].</p>
      <p>As expected, performance deteriorated when noise was added (<xref ref-type="fig" rid="pcbi.1007973.g004">Fig 4a</xref>, noisy condition). However, XDream using the genetic algorithm was still able to find optimized stimuli better than random exploration for most layers (<italic>p</italic> &lt; 10<sup>−10</sup> for all tested layers except <italic>p</italic> = 0.19 for CaffeNet fc8, FDR-corrected for 5 tests) and was also able to find super stimuli for some layers (<italic>p</italic> &lt; 10<sup>−5</sup> for CaffeNet conv4 and fc6 layers; FDR-corrected for 5 tests).</p>
      <p>Noise in the unit activations affected different optimization algorithms to different extents. The genetic algorithm was at least as good as, and often superior to, both alternative optimization algorithms when considering noisy units. The NES algorithm performed similarly to the genetic algorithm in CaffeNet fc8 layer and Inception-ResNet-v2 classifier layer (<italic>p</italic> = 0.03 and 0.65, respectively), but was worse in the other 3 tested layers (<italic>p</italic> &lt; 10<sup>−14</sup>). The FDGD algorithm was particularly sensitive to noise, performing worse than the genetic algorithm in all layers tested (<italic>p</italic> &lt; 10<sup>−6</sup>) and frequently failing to find good stimuli.</p>
      <p>In the noisy conditions examined thus far, we assumed that in each presentation, model units yielded approximately 20 spikes for a “good” stimulus (defined as the expected best image in 2,500 random ImageNet images). This choice was motivated by what may be realistically expected when recording from biological neurons (e.g., firing rate of 100 spikes per second to a good stimulus over a 200 ms observation window), but this number will be dependent on individual neurons and specific experimental designs. This number matters because, for a homogeneous Poisson process, its standard deviation-to-mean ratio is inversely proportional to the square root of the rate parameter (average expected number of spikes), and thus a higher firing rate means a higher signal-to-noise ratio. To characterize the performance of XDream under different noise conditions, we varied the rate parameter as defined by the expected max spike number and measured XDream performance on the different noise levels (<xref ref-type="fig" rid="pcbi.1007973.g004">Fig 4b</xref>). The empirical level of noise was quantified with commonly used measures such as trial-to-trial self-correlation, standard deviation-to-mean ratio, and signal-to-noise ratio (SNR). As the amount of noise decreased, the performance of XDream gradually approached its noiseless performance. Notably, even with a high level of noise (5 spikes for a good stimulus, self-correlation of 0.08, and SNR of 2), XDream was able to find super stimuli for around half of the target units in all but the deepest layer (fc8) tested.</p>
    </sec>
    <sec id="sec015">
      <title>Availability and future directions</title>
      <p>The code for XDream can be obtained directly from <ext-link ext-link-type="uri" xlink:href="https://github.com/willwx/XDream/">https://github.com/willwx/XDream/</ext-link>.</p>
      <p>In the computer science literature, activation maximization is a well-known approach for visualizing features represented by units in a ConvNet [<xref rid="pcbi.1007973.ref012" ref-type="bibr">12</xref>, <xref rid="pcbi.1007973.ref021" ref-type="bibr">21</xref>, <xref rid="pcbi.1007973.ref037" ref-type="bibr">37</xref>–<xref rid="pcbi.1007973.ref039" ref-type="bibr">39</xref>]. However, the techniques are only applicable to networks that provide optimization gradients. In other words, perfect knowledge is assumed of the target network architecture and weights. Clearly, such requirements are not met in current neuroscience experiments.</p>
      <p>Recently, several other studies have focused on similar goals to the ones in XDream, but with a different approach [<xref rid="pcbi.1007973.ref032" ref-type="bibr">32</xref>, <xref rid="pcbi.1007973.ref033" ref-type="bibr">33</xref>, <xref rid="pcbi.1007973.ref040" ref-type="bibr">40</xref>, <xref rid="pcbi.1007973.ref041" ref-type="bibr">41</xref>]. In that approach, a ConvNet-based model is first fitted to predict neuronal responses to a set of training images. Then, standard white-box activation maximization techniques are applied to the ConvNet model. The relation between this approach and XDream is similar to the relation between the so-called “substitute model” approach and what, in comparison, we may call a “direct” approach, in research on black-box adversarial attacks. A promising future direction is to combine the two approaches to leverage their unique advantages: unlimited queries (after training) and efficient optimization with substitute models; avoiding model extrapolation and transferability problems with direct optimization.</p>
      <p>The results presented here are based on maximizing activation values, whereas the results shown in [<xref rid="pcbi.1007973.ref013" ref-type="bibr">13</xref>] are based on maximizing spike counts. Activation values and firing rates are commonly-used proxies for internal representation in machine learning and neuroscience, respectively. However, other putative neural codes can be studied, such as pooled activation across multiple units, increase sparseness of the representation across units, match a pre-specified pattern of population firing, correlated firing, synchronized firing of nearby units, maximize power in a certain frequency band in local field potentials, etc. XDream is agnostic to the underpinning of the objective function as long as it is image-specific, quantitatively defined, and computable in real time. Thus, the same algorithm can be readily applied to investigate different putative neural coding mechanisms.</p>
      <p>Finally, it is worth remembering that the identification of an optimal stimulus, or even a diverse set of them, still does not automatically lead to a full characterization of the function of a neuron. Finding preferred stimuli, or “feature visualization” in computer science parlance, has guided thinking about the function of individual neurons in both neuroscience and deep learning [<xref rid="pcbi.1007973.ref006" ref-type="bibr">6</xref>, <xref rid="pcbi.1007973.ref021" ref-type="bibr">21</xref>]. However, optimal stimuli reflect but do not disentangle critical issues like tuning features, invariant features, and context dependence; these questions need to be distinguished by subsequent hypothesis-driven investigation [<xref rid="pcbi.1007973.ref042" ref-type="bibr">42</xref>–<xref rid="pcbi.1007973.ref044" ref-type="bibr">44</xref>]. A method to automatically find preferred stimuli of neurons can suggest initial hypotheses about a poorly-understood visual area, or motivate re-thinking about an extensively-studied region. Of note, during the optimization process, XDream does test thousands of related images, covering the target unit’s response levels both widely and densely (<xref ref-type="fig" rid="pcbi.1007973.g001">Fig 1c</xref>). Closer analyses of these images may reveal richer information about the tuning surface of a neuron (e.g., invariances) than what is reflected by the single best image.</p>
      <p>In summary, XDream is able to discover preferred features of visual units without assuming any knowledge about the structure or connectivity of the system under study. Thus, XDream can be a powerful tool for elucidating the tuning properties of neurons in a variety of visual areas in different species, even where there is no prior knowledge about the neuronal preferences. Furthermore, we speculate that the general framework of XDream can be extended to other sensory domains, such as sounds, language, and music, as long as good generative networks can be built.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material" id="sec016">
    <title>Supporting information</title>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s001">
      <label>S1 Fig</label>
      <caption>
        <title>Expected maximum relative activation in response to random natural images.</title>
        <p>We measured the max relative activation expected in two random sampling schemes. “Random” refers to picking a given number of images randomly from the ImageNet dataset (blue). “10 categories” refers to first randomly picking 10 categories out of the 1000 ImageNet categories and then picking randomly from those categories (gray). We considered 4 layers from the CaffeNet architecture. Lines indicate the median relative activation (activation divided by the highest activation for all ImageNet images). Shading indicates the 25th- to 75th-percentiles among 100 random units per layer.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s001.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s002">
      <label>S2 Fig</label>
      <caption>
        <title>Optimized and best ImageNet images for other example neurons across architectures and layers.</title>
        <p>Two neurons were randomly selected per layer per architecture (<xref ref-type="supplementary-material" rid="pcbi.1007973.s007">S1 Table</xref>). Format is the same as in <xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2</xref>.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s002.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s003">
      <label>S3 Fig</label>
      <caption>
        <title>The image generator can approximate arbitrary images, and XDream can find these images using only scalar distance as a loss function.</title>
        <p>This figure reproduces Supplementary Figure 1 in [<xref rid="pcbi.1007973.ref013" ref-type="bibr">13</xref>]. The generative network is challenged to synthesize arbitrary target images (row 1) using one of two encoding methods, “opt” (row 2) and “ivt” (row 3; Methods). In addition, XDream can discover the target image efficiently (within 10,000 test image presentations) by using the genetic algorithm to minimize the mean squared difference between the target image and any test image as a loss function, either in pixel space (row 4) or in CaffeNet pool5 representation space (row 5).</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s003.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s004">
      <label>S4 Fig</label>
      <caption>
        <title>Comparison of image generators.</title>
        <p><bold>a)</bold> We tested each of the family of image generators from [<xref rid="pcbi.1007973.ref008" ref-type="bibr">8</xref>] as the image generator in XDream, together with a generator directly representing images as pixels. Format of the plot is the same as in <xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2a</xref>. <bold>b)</bold>, The same generator architecture (DeePSiM-fc6) was trained on ImageNet and Places365, respectively, and tested on classifiers trained on either dataset. Each half of a violin corresponds to one generator, and dashed lines inside the violins indicate quartiles of the distribution; otherwise, format of the plot is the same as in <xref ref-type="fig" rid="pcbi.1007973.g002">Fig 2a</xref>.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s004.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s005">
      <label>S5 Fig</label>
      <caption>
        <title>Comparison of hyperparameters in the genetic algorithm.</title>
        <p>In each plot, one hyperparameter was varied while the others were held constant at default values indicated by the open circles. Dots indicate the mean of relative activation across 40 target neurons, 10 neurons each in 4 layers specified in <xref ref-type="supplementary-material" rid="pcbi.1007973.s010">S4 Table</xref>. Blue and orange lines indicate noiseless and noisy target units, respectively. Light colored lines indicate the mean across the 10 units within each architecture and layer. Light gray shading indicates the linear portion of a symmetrical log plot, which is used in order to show zero values.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s005.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s006">
      <label>S6 Fig</label>
      <caption>
        <title>Testing XDream on a toy model that mimics the extra-classical effect of surround suppression.</title>
        <p>We took two feature channels (first column, rows 2 &amp; 3) from the conv1 layer of AlexNet and tiled each spatially with positive and negative weights to create a central, circular excitatory region and a concentric suppressive ring, analogous to an excitatory classical receptive field (RF) and a suppressive extraclassical RF (first row). By maximizing responses of the constructed units, XDream created stimuli that are spatially confined and agreed with the varying RF sizes (rows 2 &amp; 3). We also created a unit that preferred a horizontal pattern in the center and a vertical pattern in the surround; XDream was able to uncover this preference pattern as well (row 4).</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s006.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s007">
      <label>S1 Table</label>
      <caption>
        <title>Target networks and layers.</title>
        <p>For each network, 4 layers from what is roughly the early, middle, late stages of processing, together with the output layer before softmax, were selected as targets. PlacesCNN has the same architecture as CaffeNet but is trained on the Places-205 dataset [<xref rid="pcbi.1007973.ref031" ref-type="bibr">31</xref>]. CaffeNet is as implemented in <ext-link ext-link-type="uri" xlink:href="https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet">https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet</ext-link>, PlacesCNN as in [<xref rid="pcbi.1007973.ref031" ref-type="bibr">31</xref>], and the remaining as in <ext-link ext-link-type="uri" xlink:href="https://github.com/GeekLiB/caffe-model">https://github.com/GeekLiB/caffe-model</ext-link>.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s007.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s008">
      <label>S2 Table</label>
      <caption>
        <title>Optimized hyperparameter values for the genetic algorithm.</title>
        <p>Hyperparameters used in the experiments in this paper, obtained as described in Methods separately for each generative network and for noiseless and noisy targets.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s008.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s009">
      <label>S3 Table</label>
      <caption>
        <title>Optimized hyperparameter values for the FDGD and NES algorithms.</title>
        <p>Hyperparameters used in the experiments in this paper, obtained as described in Methods separately for the noiseless and noisy case. The generative network was always deepsim-fc6.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s009.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s010">
      <label>S4 Table</label>
      <caption>
        <title>Inferior temporal cortex-like layers.</title>
        <p>From each layer, 10 units were randomly selected and used in hyperparameter evaluation.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s010.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s011">
      <label>S1 Text</label>
      <caption>
        <title>Methods and additional experiments &amp; discussion.</title>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pcbi.1007973.s011.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pcbi.1007973.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Kuffler</surname><given-names>S</given-names></name>. <article-title>Discharge patterns and functional organization of mammalian retina</article-title>. <source>Journal of Neurophysiology</source>. <year>1953</year>;<volume>16</volume>:<fpage>37</fpage>–<lpage>68</lpage>. <pub-id pub-id-type="doi">10.1152/jn.1953.16.1.37</pub-id><?supplied-pmid 13035466?><pub-id pub-id-type="pmid">13035466</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Hubel</surname><given-names>DH</given-names></name>, <name><surname>Wiesel</surname><given-names>TN</given-names></name>. <article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title>. <source>The Journal of physiology</source>. <year>1962</year>;<volume>160</volume>(<issue>1</issue>):<fpage>106</fpage>–<lpage>154</lpage>. <pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id><?supplied-pmid 14449617?><pub-id pub-id-type="pmid">14449617</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Gallant</surname><given-names>JL</given-names></name>, <name><surname>Braun</surname><given-names>J</given-names></name>, <name><surname>Van Essen</surname><given-names>DC</given-names></name>. <article-title>Selectivity for polar, hyperbolic, and Cartesian gratings in macaque visual cortex</article-title>. <source>Science</source>. <year>1993</year>;<volume>259</volume>(<issue>5091</issue>):<fpage>100</fpage>–<lpage>103</lpage>. <pub-id pub-id-type="doi">10.1126/science.8418487</pub-id><?supplied-pmid 8418487?><pub-id pub-id-type="pmid">8418487</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Pasupathy</surname><given-names>A</given-names></name>, <name><surname>Connor</surname><given-names>CE</given-names></name>. <article-title>Population coding of shape in area V4</article-title>. <source>Nature Neuroscience</source>. <year>2002</year>;<volume>5</volume>(<issue>12</issue>):<fpage>1332</fpage>–<lpage>1338</lpage>. <pub-id pub-id-type="doi">10.1038/972</pub-id><?supplied-pmid 12426571?><pub-id pub-id-type="pmid">12426571</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Logothetis</surname><given-names>NK</given-names></name>, <name><surname>Sheinberg</surname><given-names>DL</given-names></name>. <article-title>Visual object recognition</article-title>. <source>Annual Review of Neuroscience</source>. <year>1996</year>;<volume>19</volume>:<fpage>577</fpage>–<lpage>621</lpage>. <pub-id pub-id-type="doi">10.1146/annurev.ne.19.030196.003045</pub-id><?supplied-pmid 8833455?><pub-id pub-id-type="pmid">8833455</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Desimone</surname><given-names>R</given-names></name>, <name><surname>Albright</surname><given-names>T</given-names></name>, <name><surname>Gross</surname><given-names>C</given-names></name>, <name><surname>Bruce</surname><given-names>C</given-names></name>. <article-title>Stimulus-selective properties of inferior temporal neurons in the macaque</article-title>. <source>Journal of Neuroscience</source>. <year>1984</year>;<volume>4</volume>(<issue>8</issue>):<fpage>2051</fpage>–<lpage>2062</lpage>. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.04-08-02051.1984</pub-id><?supplied-pmid 6470767?><pub-id pub-id-type="pmid">6470767</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Tsao</surname><given-names>DY</given-names></name>, <name><surname>Freiwald</surname><given-names>WA</given-names></name>, <name><surname>Tootell</surname><given-names>RBH</given-names></name>, <name><surname>Livingstone</surname><given-names>MS</given-names></name>. <article-title>A Cortical Region Consisting Entirely of Face-Selective Cells</article-title>. <source>Science</source>. <year>2006</year>;<volume>311</volume>(<issue>5761</issue>):<fpage>670</fpage>–<lpage>674</lpage>. <pub-id pub-id-type="doi">10.1126/science.1119983</pub-id><pub-id pub-id-type="pmid">16456083</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref008">
      <label>8</label>
      <mixed-citation publication-type="other">Dosovitskiy A, Brox T. Generating Images with Perceptual Similarity Metrics based on Deep Networks. In: Advances in Neural Information Processing Systems; 2016. p. 658–666.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Yamane</surname><given-names>Y</given-names></name>, <name><surname>Carlson</surname><given-names>ET</given-names></name>, <name><surname>Bowman</surname><given-names>KC</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Connor</surname><given-names>CE</given-names></name>. <article-title>A neural code for three-dimensional object shape in macaque inferotemporal cortex</article-title>. <source>Nature Neuroscience</source>. <year>2008</year>;<volume>11</volume>(<issue>11</issue>):<fpage>1352</fpage>–<lpage>1360</lpage>. <pub-id pub-id-type="doi">10.1038/nn.2202</pub-id><?supplied-pmid 18836443?><pub-id pub-id-type="pmid">18836443</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Carlson</surname><given-names>ET</given-names></name>, <name><surname>Rasquinha</surname><given-names>RJ</given-names></name>, <name><surname>Zhang</surname><given-names>K</given-names></name>, <name><surname>Connor</surname><given-names>CE</given-names></name>. <article-title>A Sparse Object Coding Scheme in Area V4</article-title>. <source>Current Biology</source>. <year>2011</year>;<volume>21</volume>(<issue>4</issue>):<fpage>288</fpage>–<lpage>293</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2011.01.013</pub-id><?supplied-pmid 21315595?><pub-id pub-id-type="pmid">21315595</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Vaziri</surname><given-names>S</given-names></name>, <name><surname>Carlson</surname><given-names>ET</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <name><surname>Connor</surname><given-names>CE</given-names></name>. <article-title>A Channel for 3D Environmental Shape in Anterior Inferotemporal Cortex</article-title>. <source>Neuron</source>. <year>2014</year>;<volume>84</volume>(<issue>1</issue>):<fpage>55</fpage>–<lpage>62</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.043</pub-id><?supplied-pmid 25242216?><pub-id pub-id-type="pmid">25242216</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref012">
      <label>12</label>
      <mixed-citation publication-type="other">Nguyen A, Dosovitskiy A, Yosinski J, Brox T, Clune J. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks. In: Advances in Neural Information Processing Systems; 2016. p. 3387–3395.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Ponce</surname><given-names>CR</given-names></name>, <name><surname>Xiao</surname><given-names>W</given-names></name>, <name><surname>Schade</surname><given-names>P</given-names></name>, <name><surname>Hartmann</surname><given-names>TS</given-names></name>, <name><surname>Kreiman</surname><given-names>G</given-names></name>, <name><surname>Livingstone</surname><given-names>MS</given-names></name>. <article-title>Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences</article-title>. <source>Cell</source>. <year>2019</year>;<volume>177</volume>:<fpage>999</fpage>–<lpage>1009</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2019.04.005</pub-id><?supplied-pmid 31051108?><pub-id pub-id-type="pmid">31051108</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Pospisil</surname><given-names>DA</given-names></name>, <name><surname>Pasupathy</surname><given-names>A</given-names></name>, <name><surname>Bair</surname><given-names>W</given-names></name>. <article-title>‘Artiphysiology’ reveals V4-like shape tuning in a deep network trained for image classification</article-title>. <source>eLife</source>. <year>2018</year>;<volume>7</volume>:<fpage>e38242</fpage><pub-id pub-id-type="doi">10.7554/eLife.38242</pub-id><?supplied-pmid 30570484?><pub-id pub-id-type="pmid">30570484</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Yamins</surname><given-names>DLK</given-names></name>, <name><surname>Hong</surname><given-names>H</given-names></name>, <name><surname>Cadieu</surname><given-names>CF</given-names></name>, <name><surname>Solomon</surname><given-names>EA</given-names></name>, <name><surname>Seibert</surname><given-names>D</given-names></name>, <name><surname>DiCarlo</surname><given-names>JJ</given-names></name>. <article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title>. <source>Proceedings of the National Academy of Sciences</source>. <year>2014</year>;<volume>111</volume>(<issue>23</issue>):<fpage>8619</fpage>–<lpage>8624</lpage>. <pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref016">
      <label>16</label>
      <mixed-citation publication-type="other">Schrimpf M, Kubilius J, Hong H, Majaj NJ, Rajalingham R, Issa EB, et al. Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? bioRxiv. 2018.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Cadena</surname><given-names>SA</given-names></name>, <name><surname>Denfield</surname><given-names>GH</given-names></name>, <name><surname>Walker</surname><given-names>EY</given-names></name>, <name><surname>Gatys</surname><given-names>LA</given-names></name>, <name><surname>Tolias</surname><given-names>AS</given-names></name>, <name><surname>Bethge</surname><given-names>M</given-names></name>, <etal>et al</etal><article-title>Deep convolutional models improve predictions of macaque V1 responses to natural images</article-title>. <source>PLOS Computational Biology</source>. <year>2019</year>;<volume>15</volume>(<issue>4</issue>):<fpage>1</fpage>–<lpage>27</lpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006897</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Erhan</surname><given-names>D</given-names></name>, <name><surname>Bengio</surname><given-names>Y</given-names></name>, <name><surname>Courville</surname><given-names>A</given-names></name>, <name><surname>Vincent</surname><given-names>P</given-names></name>. <article-title>Visualizing Higher-Layer Features of a Deep Network</article-title>. <source>University of Montreal</source>. <year>2009</year>;<volume>1341</volume>(<issue>3</issue>).</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref019">
      <label>19</label>
      <mixed-citation publication-type="other">Zeiler MD, Fergus R. Visualizing and Understanding Convolutional Networks. In: European conference on computer vision. Springer; 2014. p. 818–833.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref020">
      <label>20</label>
      <mixed-citation publication-type="other">Szegedy C, Zaremba W, Sutskever I, Bruna J, Erhan D, Goodfellow I, et al. Intriguing properties of neural networks. arXiv. 2013.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref021">
      <label>21</label>
      <mixed-citation publication-type="other">Olah C, Mordvintsev A, Schubert L. Feature Visualization. Distill. 2017.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref022">
      <label>22</label>
      <mixed-citation publication-type="other">Jia Y, Shelhamer E, Donahue J, Karayev S, Long J, Girshick R, et al. Caffe: Convolutional Architecture for Fast Feature Embedding. arXiv. 2014.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref023">
      <label>23</label>
      <mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE. ImageNet Classification with Deep Convolutional Neural Networks. In: Pereira F, Burges CJC, Bottou L, Weinberger KQ, editors. Advances in Neural Information Processing Systems 25; 2012. p. 1097–1105.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>B</given-names></name>, <name><surname>Lapedriza</surname><given-names>A</given-names></name>, <name><surname>Khosla</surname><given-names>A</given-names></name>, <name><surname>Oliva</surname><given-names>A</given-names></name>, <name><surname>Torralba</surname><given-names>A</given-names></name>. <article-title>Places: A 10 million Image Database for Scene Recognition</article-title>. <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>. <year>2017</year>;<volume>40</volume>(<issue>6</issue>):<fpage>1452</fpage>–<lpage>1464</lpage>. <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2723009</pub-id><?supplied-pmid 28692961?><pub-id pub-id-type="pmid">28692961</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref025">
      <label>25</label>
      <mixed-citation publication-type="journal"><name><surname>Wierstra</surname><given-names>D</given-names></name>, <name><surname>Schaul</surname><given-names>T</given-names></name>, <name><surname>Glasmachers</surname><given-names>T</given-names></name>, <name><surname>Sun</surname><given-names>Y</given-names></name>, <name><surname>Peters</surname><given-names>J</given-names></name>, <name><surname>Schmidhuber</surname><given-names>J</given-names></name>. <article-title>Natural Evolution Strategies</article-title>. <source>Journal of Machine Learning Research</source>. <year>2014</year>;<volume>15</volume>:<fpage>949</fpage>–<lpage>980</lpage>.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref026">
      <label>26</label>
      <mixed-citation publication-type="journal"><name><surname>Liu</surname><given-names>H</given-names></name>, <name><surname>Agam</surname><given-names>Y</given-names></name>, <name><surname>Madsen</surname><given-names>J</given-names></name>, <name><surname>Kreiman</surname><given-names>G</given-names></name>. <article-title>Timing, timing, timing: Fast decoding of object information from intracranial field potentials in human visual cortex</article-title>. <source>Neuron</source>. <year>2009</year>;<volume>62</volume>:<fpage>281</fpage>–<lpage>290</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.025</pub-id><?supplied-pmid 19409272?><pub-id pub-id-type="pmid">19409272</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Russakovsky</surname><given-names>O</given-names></name>, <name><surname>Deng</surname><given-names>J</given-names></name>, <name><surname>Su</surname><given-names>H</given-names></name>, <name><surname>Krause</surname><given-names>J</given-names></name>, <name><surname>Satheesh</surname><given-names>S</given-names></name>, <name><surname>Ma</surname><given-names>S</given-names></name>, <etal>et al</etal><article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>. <source>International Journal of Computer Vision</source>. <year>2015</year>;<volume>115</volume>(<issue>3</issue>):<fpage>211</fpage>–<lpage>252</lpage>. <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>He</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>X</given-names></name>, <name><surname>Ren</surname><given-names>S</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>. <article-title>Identity Mappings in Deep Residual Networks</article-title>. <source>Lecture Notes in Computer Science</source>. <year>2016</year>; p. <fpage>630</fpage>–<lpage>645</lpage>. <pub-id pub-id-type="doi">10.1007/978-3-319-46493-0_38</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref029">
      <label>29</label>
      <mixed-citation publication-type="other">Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the Inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2016. p. 2818–2826.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref030">
      <label>30</label>
      <mixed-citation publication-type="other">Szegedy C, Ioffe S, Vanhoucke V, Alemi AA. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. In: Thirty-First AAAI Conference on Artificial Intelligence; 2017. p. 4278–4284.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref031">
      <label>31</label>
      <mixed-citation publication-type="other">Zhou B, Lapedriza A, Xiao J, Torralba A, Oliva A. Learning Deep Features for Scene Recognition using Places Database. In: Advances in Neural Information Processing Systems 29; 2014. p. 487–495.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Bashivan</surname><given-names>P</given-names></name>, <name><surname>Kar</surname><given-names>K</given-names></name>, <name><surname>DiCarlo</surname><given-names>JJ</given-names></name>. <article-title>Neural population control via deep image synthesis</article-title>. <source>Science</source>. <year>2019</year>;<volume>364</volume> (<issue>6439</issue>). <pub-id pub-id-type="doi">10.1126/science.aav9436</pub-id>
<?supplied-pmid 31048462?><pub-id pub-id-type="pmid">31048462</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref033">
      <label>33</label>
      <mixed-citation publication-type="other">Abbasi-Asl R, Chen Y, Bloniarz A, Oliver M, Willmore BDB, Gallant JL, et al. The DeepTune framework for modeling and characterizing neurons in visual cortex area V4. bioRxiv. 2018.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref034">
      <label>34</label>
      <mixed-citation publication-type="other">Ilyas A, Engstrom L, Athalye A, Lin J. Black-box Adversarial Attacks with Limited Queries and Information. In: Dy J, Krause A, editors. Proceedings of the 35th International Conference on Machine Learning. vol. 80 of Proceedings of Machine Learning Research; 2018. p. 2137–2146.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref035">
      <label>35</label>
      <mixed-citation publication-type="book"><name><surname>Koch</surname><given-names>C</given-names></name>. <source>Biophysics of Computation</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>; <year>1999</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref036">
      <label>36</label>
      <mixed-citation publication-type="book"><name><surname>Gabbiani</surname><given-names>F</given-names></name>, <name><surname>Cox</surname><given-names>S</given-names></name>. <source>Mathematics for Neuroscientists</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Academic Press</publisher-name>; <year>2010</year>.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref037">
      <label>37</label>
      <mixed-citation publication-type="other">Simonyan K, Vedaldi A, Zisserman A. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. arXiv. 2013.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref038">
      <label>38</label>
      <mixed-citation publication-type="other">Olah C, Satyanarayan A, Johnson I, Carter S, Schubert L, Ye K, et al. The Building Blocks of Interpretability. Distill. 2018.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref039">
      <label>39</label>
      <mixed-citation publication-type="other">Carter S, Armstrong Z, Schubert L, Johnson I, Olah C. Activation Atlas. Distill. 2019.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref040">
      <label>40</label>
      <mixed-citation publication-type="other">Walker EY, Sinz FH, Froudarakis E, Fahey PG, Muhammad T, Ecker AS, et al. Inception in visual cortex: in vivo-silico loops reveal most exciting images. bioRxiv. 2018.</mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Malakhova</surname><given-names>K</given-names></name>. <article-title>Visualization of information encoded by neurons in the higher-level areas of the visual system</article-title>. <source>J Opt Technol</source>. <year>2018</year>;<volume>85</volume>(<issue>8</issue>):<fpage>494</fpage>–<lpage>498</lpage>. <pub-id pub-id-type="doi">10.1364/JOT.85.000494</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref042">
      <label>42</label>
      <mixed-citation publication-type="journal"><name><surname>Kobatake</surname><given-names>E</given-names></name>, <name><surname>Tanaka</surname><given-names>K</given-names></name>. <article-title>Neuronal selectivities to complex object features in the ventral visual pathway of the macaque cerebral cortex</article-title>. <source>Journal of Neurophysiology</source>. <year>1994</year>;<volume>71</volume>:<fpage>856</fpage>–<lpage>867</lpage>. <pub-id pub-id-type="doi">10.1152/jn.1994.71.3.856</pub-id><?supplied-pmid 8201425?><pub-id pub-id-type="pmid">8201425</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Chang</surname><given-names>L</given-names></name>, <name><surname>Tsao</surname><given-names>DY</given-names></name>. <article-title>The Code for Facial Identity in the Primate Brain</article-title>. <source>Cell</source>. <year>2017</year>;<volume>169</volume>:<fpage>1013</fpage>–<lpage>1028.e14</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2017.05.011</pub-id><?supplied-pmid 28575666?><pub-id pub-id-type="pmid">28575666</pub-id></mixed-citation>
    </ref>
    <ref id="pcbi.1007973.ref044">
      <label>44</label>
      <mixed-citation publication-type="other">Geirhos R, Rubisch P, Michaelis C, Bethge M, Wichmann FA, Brendel W. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv. 2018.</mixed-citation>
    </ref>
  </ref-list>
</back>
<sub-article id="pcbi.1007973.r001" article-type="aggregated-review-documents">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1007973.r001</article-id>
    <title-group>
      <article-title>Decision Letter 0</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Einhäuser</surname>
          <given-names>Wolfgang</given-names>
        </name>
        <role>Deputy Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fyshe</surname>
          <given-names>Alona</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Einhäuser, Fyshe</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Einhäuser, Fyshe</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj001" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007973" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>0</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">25 Jan 2020</named-content>
    </p>
    <p>Dear Mr. Xiao,</p>
    <p>Thank you very much for submitting your manuscript "XDream: finding preferred stimuli for visual neurons using generative networks and gradient-free optimization" for consideration at PLOS Computational Biology.</p>
    <p>As with all papers reviewed by the journal, your manuscript was reviewed by members of the editorial board and by several independent reviewers. In light of the constructive reviews (below this email), we would like to invite the resubmission of a significantly-revised version that takes into account the reviewers' comments.</p>
    <p>We cannot make any decision about publication until we have seen the revised manuscript and your response to the reviewers' comments. Your revised manuscript is also likely to be sent to reviewers for further evaluation.</p>
    <p>When you are ready to resubmit, please upload the following:</p>
    <p>[1] A letter containing a detailed list of your responses to the review comments and a description of the changes you have made in the manuscript. Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out.</p>
    <p>[2] Two versions of the revised manuscript: one with either highlights or tracked changes denoting where the text has been changed; the other a clean version (uploaded as the manuscript file).</p>
    <p>Important additional instructions are given below your reviewer comments.</p>
    <p>Please prepare and submit your revised manuscript within 60 days. If you anticipate any delay, please let us know the expected resubmission date by replying to this email. Please note that revised manuscripts received after the 60-day due date may require evaluation and peer review similar to newly submitted manuscripts.</p>
    <p>Thank you again for your submission. We apologize for the length of time these reviews took, but I can assure you we were working to obtain reviews for the entire time your paper was under review.  We hope that our editorial process has been constructive so far, and we welcome your feedback at any time. Please don't hesitate to contact us if you have any questions or comments.</p>
    <p>Sincerely,</p>
    <p>Alona Fyshe, Ph.D.</p>
    <p>Associate Editor</p>
    <p>PLOS Computational Biology</p>
    <p>Wolfgang Einhäuser</p>
    <p>Deputy Editor</p>
    <p>PLOS Computational Biology</p>
    <p>***********************</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Authors:</bold>
    </p>
    <p>
      <bold>Please note here if the review is uploaded as an attachment.</bold>
    </p>
    <p>Reviewer #1: The authors produce an analysis of the XDream method that uses generative networks with a genetic algorithm for obtaining stimuli that will optimally drive neurons. They test this method by using it on pre-trained deep neural networks. They show that XDream can consistently find stimuli that will drive units in pre-trained networks to responses greater than those of the image set from which the networks were trained. This result is approximately independent of depth in the network and is independent of initial conditions. It is relatively robust to the choice of generative network, as long as that generator produces a "high-level" representation. These results are somewhat robust to the choice of optimization but are impacted by adding noise to the representation.</p>
    <p>This looks at an important issue for visual neuroscience, that of finding stimuli that drive neurons in a manner that is not plagued by various experimental and theoretical biases (cf. Olshausen-Field "What is the other 85% of V1 doing?" and Carandini, et al 2005, "Do we know what the early visual system does?"). The XDream tool is an example of an emerging methodology for dealing with this problem and studying its capabilities and drawbacks is therefore important and useful to the field. The paper is clear. I recommend publication after the authors have addressed the feedback below.</p>
    <p>1) The most important issue the authors should address is to provide some acknowledgement or discussion of the notion of optimal or "preferred" stimuli in the first place. By their methodology, they are looking for a single image that will optimally drive neurons, but this is probably not the right way to think about the coding problem, certainly not for neurons and probably not for the units in deep networks as well. The "single best image" approach is going to gloss over issues like contextual dependence. These optimal stimuli may in fact be a very small set that drives responses because they combine some "feature" for which the neuron/unit codes with exactly the right context to enhance the response. Depending on the size of this space and the nature of the context, one might wind up with an image that is not very informative of the actual coding properties of the neuron or unit. The authors use the phrase "true feature preference" in the introduction, but need to acknowledge that their method may not find it either. Furthermore, this problem is compounded in the case of real neurons, in which there are feedback and lateral inputs that produce extra-classical effects. These effects may or may not appear in the models that the authors are testing, since they are all feedforward networks. This set of questions will almost certainly lead to the "invariance manifold" that the authors discussed.</p>
    <p>None of this takes away from the authors' work, but users of the method should be aware of these issues.</p>
    <p>2) Related to point 1, the authors should comment on the possible ethological relevance of super-stimuli. It is not surprising that such stimuli exist. Given that each unit produces a 1D parameterized function of image space, one should be able to find a point in the input space that produces a more extreme result than any finite set of inputs. Do these have a useful meaning in terms of describing that function?</p>
    <p>3) The authors showed that the generative model in XDream was "expressive", but those demonstrations also show what the model does not capture, namely high-frequency content of images (fine details are lost in all the examples in (what should be) Figure 3). Is this a limitation for the methodology? It seems so, since the model shouldn't then be able to capture optimal stimuli for neurons that respond to fine details. I suspect this is not a severe practical limitation, but it is there.</p>
    <p>4) The authors should acknowledge the bias that is built into their methodology by looking at samples from fixed image databases. ImageNet does have a particular structure and, unless I'm mistaken, all the tested generators were trained on this structure (implicitly by inverting AlexNet). This will bias towards finding features similar to those necessary for describing ImageNet in particular. This is not just an issue of whether the generative model can reproduce an image when forced to (e.g. the Figure that should be Figure 3) but whether it will tend to promote certain features over others. ImageNet will provide an implicit bias.</p>
    <p>5) The claim about robustness to noise is overstated. Figure 6 makes the technique look quite susceptible to Poisson noise, depending upon target layer.</p>
    <p>Minor issues:</p>
    <p>1) Figure 1b and 1c: there are 9 minor tick marks and ten example images.</p>
    <p>2) Figure 2 and 3 appear to be swapped (at least in the pdf I received).</p>
    <p>3) Figure 2: label the random ImageNet samples in the Figure (grey boxes). label the dotted line as "maximum ImageNet response" or something similar. This will make the figure easier to read and digest.</p>
    <p>4) line 112: ...we qualitative[ly] assessed...</p>
    <p>5) lines 136-7: delete either "it can generate" or "can be generated"</p>
    <p>6) Figure 4: label the open and solid violin plots on the Figure</p>
    <p>7) Table 1: the caption appears to be a Latinate placeholder.</p>
    <p>8) lines 211-9: this section needs to be rewritten. There needs to be a reference to Table 1, otherwise "slope" is introduced before the reader has seen such a thing. It isn't in a figure or preceding discussion. One is left to infer what variables the linear regression is being performed upon.</p>
    <p>Reviewer #2: I will upload the pdf with my comments in it, where I have found typos and made wording suggestions.</p>
    <p>Review of XDream: finding preferred stimuli for visual neurons using generative networks and gradient-free optimization (Will Xiao and Gabriel Kreiman)</p>
    <p>by Gary Cottrell</p>
    <p>This paper uses an existing method for finding optimal visual stimuli for neurons that has previously been applied in Macaque in a paper in Cell in 2019. The goal here is to elucidate how robust the method is by applying it to several deep network models with varying architectures and at different layers of the networks. Using network models allows for extensive experimentation that is impractical in biological preparations. The model proves to be very robust to various parameter regimes, and finds stimuli that drive the neuron much more than any of the over 1 million images in the ImageNet dataset. One of the most interesting findings here is that, using different image generators or different initial conditions, the model finds multiple images that drive the neurons similarly, and these images resemble one another to the human eye. Hence they suggest that there is an optimal image manifold in the latent space of images. Unfortunately, this point was already made in the prior paper with actual monkey visual neurons.</p>
    <p>Since the authors postulate that there is an invariance manifold, it would be useful to test this idea by looking at what is generated by a linear interpolation of the codes found from different initial conditions, and how well those interpolated images drive the model neuron. While it is unlikely that the manifold is linear, since the images are similar, they are probably nearby in this space.</p>
    <p>In the paragraph on other things one could study with this approach, such as correlated firing, synchronized firing, LFPs, etc., it would be helpful to say what you would optimize in a couple of cases. E.g., you might say, “for example, we could optimize based on increased correlations in firing rates between neurons” or some such.</p>
    <p>The paper could be improved by moving more of the information into the methods section or into supplementary material. There are many details that make the exposition rather tough sledding for the reader. In particular, the section on the effects of different generators has a couple of caveats, e.g., “except for CaffeNet conv2”, and “The pixel-based image generator, compared to generative neural networks, worked more poorly in all target layers other than CaffeNet conv2 except when compared to deepsim-norm2 (p = 1 compared to deepsim-norm2; p &gt; 0.14 compared to other generators in CaffeNet conv2; p &lt; 10^−4 in all other comparisons; FDR-corrected for 32 tests comparing each generator to raw-pixel in each target layer).” This amount of detail makes my eyes glaze over. I think this section doesn’t add a lot to the paper, and could profitably be moved to the supplementary material.</p>
    <p>Similarly, the section on different optimizers doesn’t flow well. The GA works better on some layers and the other two algorithms work better in some other layers. I’m not sure I care, and I’m not sure what the take-home message is. Again, there are similar “this works better in this layer and that works better in other layers” results in the noise experiments. I think the noise experiments are important, as this is a more realistic case. Unfortunately, the results in Figure 6 are not encouraging. I would disagree with the header for this section: “XDream is robust to noise in neuronal responses.” It doesn’t seem that robust. I wonder if there is a fix for this - for example, can you average the rate over some time interval of the Poisson process instead of simply sampling from it? Unfortunately, I don’t know enough about Poisson processes to know if this is a good suggestion or not.</p>
    <p>One concern is that, although the authors state that “we focus on the more biologically relevant scenario where there is no information about the architecture and weights of the target model, and where we only have access to a few, potentially stochastic, activation values from the neurons.” In fact, they don’t have access to only a few activations. The model is used to generate 10,000 images to find the optimal one. This seems biologically unrealistic. I skimmed the previous paper, and there they used many fewer generations for the monkeys - 200. Since this is what is apparently possible in biological preparations, it seems like they should evaluate how much is gained in 200 generations and compare that to the 500 generations they used here. This would provide a better estimate of what is possible today.</p>
    <p>Originality</p>
    <p>The originality is tempered somewhat by the fact that this is investigating an existing method that has already been used in Macaque visual cortex. Obviously, this is different in the sense that using convnets as the preparation allows for much more extensive experimentation with the method than would be possible with a biological one, which is the point of this paper. For example, they can test the neuron’s response to the over one million images in ImageNet, and then compare the results of this brute force search to the effectiveness of XDream. They vary just about everything and still find that the method works well. So it is original in that sense, but I have to think that the big-font headline is the first paper.</p>
    <p>Innovation</p>
    <p>Again, the innovation here is to benchmark their method using in silico models. This kind of thing has been done before with other analyses of deep net features.</p>
    <p>High importance to researchers in the field </p>
    <p>To the extent to which other researchers may start to use this method, the importance here is that neuroscientists can get some assurance that the method is robust, and that they needn’t worry too much about optimizing the meta-parameters.</p>
    <p>“Similar activation values notwithstanding, the optimized images were different on a pixel level (Fig 4b); they may comprise an ”invariance manifold” for each neuron that contains similar but not identical images eliciting comparable activation values (see Discussion).” I think this is an important point, so it would be good to test the hypothesis by looking along a line between some of the image codes to see if there are optimal images between these. This would be of interest to researchers using this method.</p>
    <p>Significant biological and/or methodological insight </p>
    <p>There is not much in the way of biological insight here, but the paper does demonstrate that this methodology is robust.</p>
    <p>Rigorous methodology</p>
    <p>This is quite a rigorous test of the model.</p>
    <p>Substantial evidence for its conclusions</p>
    <p>The evidence that this approach works well over a variety of convnet architectures and layers is extensive. On the other hand, I think the evidence for robustness to noise is weak. This is a point the authors should address in a revision of the paper.</p>
    <p>Minor comments:</p>
    <p>I’m not quite sure what this sentence means: “the standard deviation was lower than 10% of the activation values (Fig 4a).”</p>
    <p>I’m not sure the slopes in Table 1 give a very intuitive idea of how much improvement you get by using good vs. bad initializations. A bar chart might be better. Also, while the text suggests that there isn’t much difference, the slopes seem fairly big as you go deeper in the net using the ivt method. I don’t know if that conclusion of mine is warranted. I.e., I would better understand this if the actual differences in medians were shown instead of the slope. Perhaps another violin plot would work here? It seems slightly counterintuitive to conclude that there is only slight variation in the optimized image activation while the p-values for the differences are on the order of 10^-77.</p>
    <p>**********</p>
    <p>
      <bold>Have all data underlying the figures and results presented in the manuscript been provided?</bold>
    </p>
    <p>Large-scale datasets should be made available via a public repository as described in the <italic>PLOS Computational Biology</italic>
<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/ploscompbiol/s/data-availability">data availability policy</ext-link>, and numerical data that underlies graphs or summary statistics should be provided in spreadsheet form as supporting information.</p>
    <p>Reviewer #1: None</p>
    <p>Reviewer #2: None</p>
    <p>**********</p>
    <p>PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: No</p>
    <p>Reviewer #2: Yes: Garrison W Cottrell</p>
    <p>
      <underline>Figure Files:</underline>
    </p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <underline><ext-link ext-link-type="uri" xlink:href="https://pacev2.apexcovantage.com/" xlink:type="simple">https://pacev2.apexcovantage.com</ext-link></underline>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <underline><email xlink:type="simple">figures@plos.org</email></underline>.</p>
    <p>
      <underline>Data Requirements:</underline>
    </p>
    <p>Please note that, as a condition of publication, PLOS' data policy requires that you make available all data used to draw the conclusions outlined in your manuscript. Data must be deposited in an appropriate repository, included within the body of the manuscript, or uploaded as supporting information. This includes all numerical values that were used to generate graphs, histograms etc.. For an example in PLOS Biology see here: <ext-link ext-link-type="uri" xlink:href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5</ext-link>.</p>
    <p>
      <underline>Reproducibility:</underline>
    </p>
    <p>To enhance the reproducibility of your results, PLOS recommends that you deposit laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. For instructions, please see <underline><ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plospathogens/s/submission-guidelines" xlink:type="simple">http://journals.plos.org/compbiol/s/submission-guidelines#loc-materials-and-methods</ext-link></underline></p>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s012">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">PCOMPBIOL-D-19-01642_reviewer.pdf</named-content></p>
      </caption>
      <media xlink:href="pcbi.1007973.s012.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article id="pcbi.1007973.r002" article-type="author-comment">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1007973.r002</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 0</article-title>
    </title-group>
    <related-article id="rel-obj002" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007973" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">31 Mar 2020</named-content>
    </p>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s013">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">response2.pdf</named-content></p>
      </caption>
      <media xlink:href="pcbi.1007973.s013.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article id="pcbi.1007973.r003" article-type="aggregated-review-documents">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1007973.r003</article-id>
    <title-group>
      <article-title>Decision Letter 1</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Einhäuser</surname>
          <given-names>Wolfgang</given-names>
        </name>
        <role>Deputy Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fyshe</surname>
          <given-names>Alona</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Einhäuser, Fyshe</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Einhäuser, Fyshe</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj003" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007973" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>1</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">12 May 2020</named-content>
    </p>
    <p>Dear Mr. Xiao,</p>
    <p>Thank you very much for submitting your manuscript "XDream: finding preferred stimuli for visual neurons using generative networks and gradient-free optimization" for consideration at PLOS Computational Biology. As with all papers reviewed by the journal, your manuscript was reviewed by members of the editorial board and by several independent reviewers. The reviewers appreciated the attention to an important topic. Based on the reviews, we are likely to accept this manuscript for publication, providing that you modify the manuscript according to the review recommendations.</p>
    <p>It looks like we are very close to consensus on this paper. Reviewer two has a few last comments that need to be addressed.</p>
    <p>Please prepare and submit your revised manuscript within 30 days. If you anticipate any delay, please let us know the expected resubmission date by replying to this email. </p>
    <p>When you are ready to resubmit, please upload the following:</p>
    <p>[1] A letter containing a detailed list of your responses to all review comments, and a description of the changes you have made in the manuscript. Please note while forming your response, if your article is accepted, you may have the opportunity to make the peer review history publicly available. The record will include editor decision letters (with reviews) and your responses to reviewer comments. If eligible, we will contact you to opt in or out</p>
    <p>[2] Two versions of the revised manuscript: one with either highlights or tracked changes denoting where the text has been changed; the other a clean version (uploaded as the manuscript file).</p>
    <p>Important additional instructions are given below your reviewer comments.</p>
    <p>Thank you again for your submission to our journal. We hope that our editorial process has been constructive so far, and we welcome your feedback at any time. Please don't hesitate to contact us if you have any questions or comments.</p>
    <p>Sincerely,</p>
    <p>Alona Fyshe, Ph.D.</p>
    <p>Associate Editor</p>
    <p>PLOS Computational Biology</p>
    <p>Wolfgang Einhäuser</p>
    <p>Deputy Editor</p>
    <p>PLOS Computational Biology</p>
    <p>***********************</p>
    <p>A link appears below if there are any accompanying review attachments. If you believe any reviews to be missing, please contact <email>ploscompbiol@plos.org</email> immediately:</p>
    <p>[LINK]</p>
    <p>It looks like we are very close to consensus on this paper. Reviewer two has a few last comments that need to be addressed</p>
    <p>Reviewer's Responses to Questions</p>
    <p>
      <bold>Comments to the Authors:</bold>
    </p>
    <p>
      <bold>Please note here if the review is uploaded as an attachment.</bold>
    </p>
    <p>Reviewer #1: The authors have addressed my concerns. I am happy to support publication.</p>
    <p>Reviewer #2: Review of Xdream revision 1.</p>
    <p>by Gary Cottrell</p>
    <p>The reviewers have adequately addressed most of my comments, so this version is improved over the previous version, but there are still a few things that need to be addressed, and unfortunately, on the second reading, I have a couple of new questions for you.</p>
    <p>I will also upload my marked-up version of the pdf for wording and typos I found.</p>
    <p>I should say I only skimmed the supplementary material, but I did have one comment on the discussion: In the second paragraph, you refer to “the direct method”, which is not mentioned in the previous paragraph. You need to reintroduce this idea here, as a neuro person will not have any idea what you are talking about without going back to the main article.</p>
    <p>In the paper, you linearly interpolate between the optimized images. In the response, you mention that you don’t do this in pixel space, but in the latent space. You should mention that here.</p>
    <p>BTW, David Sheinberg showed some data at CNS in 2003 that may be relevant here. He told me it didn’t make it into a paper, but he found a cell in macaque that responded to both car images and butterfly images, with the strongest response to a particular car and a particular butterfly - very disjoint stimuli! It would be cool to stick that in here somewhere; I’ll upload the data with my review. I should mention that these images were very well known to the monkey.</p>
    <p>There’s a typo on page 8/14, line 262, where you refer to Figure 3c, but I think you mean Figure 3d. On that same page, you again mention Table 1, without explanation. The explanation does appear in a caption under table (you should also mention the way you calculated the slope - I assume linear regression). Tables don’t have captions, so you will still need to move this into the main text, presumably before you mention the table.</p>
    <p>Line 272 page 8:</p>
    <p>With the “ivt” method, these initializations worked similarly in layers conv2 and conv4, not in layers fc6 (p=0.0014) and fc8 (p=2x10^-15).</p>
    <p>This doesn’t say how they worked in layers fc6 anbd fc8 (again, these details are boring and irrelevant, since, as you say, PIs are not likely to have optimal images to start with. I still recommend leaving them to the supplementary material). But I’m not even sure what you are saying here. What are you referring to? Is it that opt is better than ivt in these layers? When I looked at the figure, I thought you were referring to how the best initialization gave bigger improvements here over medium and worst initializations for these layers. Please clarify this.</p>
    <p>Likewise, the next sentence says that random initializations worked better, but you don’t say better than what, and it sure doesn’t look like they are better than the other results in Figure 3d, at least for medium and bad initializations.</p>
    <p>Again, this paragraph doesn’t flow well. The message you want to get across is: In a realistic situation, where the PI doesn’t have access to optimal starting images, random initialization works about as well as anything else. Start by saying that starting with the optimal image does help in some cases, but given that investigators are not likely to have access to these, using random initialization is sufficient. Details in the supplementary material.</p>
    <p>Figure 3d has some issues. In the caption, you give the order as best, middle, worst, but you’ve reversed the order from the previous manuscript - it’s now worst, middle, best. Also, the y-axis caption is incorrect - it should be “Relative Activation”, not “Target CaffeNet layer.”</p>
    <p>Figure 4B has four different y-axis labels (!). This is not explained in the figure caption (or the text), so you may as well leave the extra three out, as they are confusing and a distraction from the main point.</p>
    <p>Page 9: optimization methods: There are some significant differences between the algorithms. Can you comment on why you think that is?</p>
    <p>Page 9: noise: There is a big difference of the effect of noise on the algorithm in the hidden layers versus the output. Any idea why? This seems odd, given that you’ve optimized your metaparameters on the output neurons, if I understood the methods correctly.</p>
    <p>I trust that the editor can enforce these changes/clarifications; I don’t need to review this again.</p>
    <p>**********</p>
    <p>
      <bold>Have all data underlying the figures and results presented in the manuscript been provided?</bold>
    </p>
    <p>Large-scale datasets should be made available via a public repository as described in the <italic>PLOS Computational Biology</italic>
<ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/ploscompbiol/s/data-availability">data availability policy</ext-link>, and numerical data that underlies graphs or summary statistics should be provided in spreadsheet form as supporting information.</p>
    <p>Reviewer #1: Yes</p>
    <p>Reviewer #2: Yes</p>
    <p>**********</p>
    <p>PLOS authors have the option to publish the peer review history of their article (<ext-link ext-link-type="uri" xlink:href="https://journals.plos.org/ploscompbiol/s/editorial-and-peer-review-process#loc-peer-review-history">what does this mean?</ext-link>). If published, this will include your full peer review and any attached files.</p>
    <p>If you choose “no”, your identity will remain anonymous but your review may still be made public.</p>
    <p><bold>Do you want your identity to be public for this peer review?</bold> For information about this choice, including consent withdrawal, please see our <ext-link ext-link-type="uri" xlink:href="https://www.plos.org/privacy-policy">Privacy Policy</ext-link>.</p>
    <p>Reviewer #1: Yes: Michael Buice</p>
    <p>Reviewer #2: Yes: Garrison W Cottrell</p>
    <p>
      <underline>Figure Files:</underline>
    </p>
    <p>While revising your submission, please upload your figure files to the Preflight Analysis and Conversion Engine (PACE) digital diagnostic tool, <underline><ext-link ext-link-type="uri" xlink:href="https://pacev2.apexcovantage.com/" xlink:type="simple">https://pacev2.apexcovantage.com</ext-link></underline>. PACE helps ensure that figures meet PLOS requirements. To use PACE, you must first register as a user. Then, login and navigate to the UPLOAD tab, where you will find detailed instructions on how to use the tool. If you encounter any issues or have any questions when using PACE, please email us at <underline><email xlink:type="simple">figures@plos.org</email></underline>.</p>
    <p>
      <underline>Data Requirements:</underline>
    </p>
    <p>Please note that, as a condition of publication, PLOS' data policy requires that you make available all data used to draw the conclusions outlined in your manuscript. Data must be deposited in an appropriate repository, included within the body of the manuscript, or uploaded as supporting information. This includes all numerical values that were used to generate graphs, histograms etc.. For an example in PLOS Biology see here: <ext-link ext-link-type="uri" xlink:href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5">http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001908#s5</ext-link>.</p>
    <p>
      <underline>Reproducibility:</underline>
    </p>
    <p>To enhance the reproducibility of your results, PLOS recommends that you deposit laboratory protocols in protocols.io, where a protocol can be assigned its own identifier (DOI) such that it can be cited independently in the future. For instructions see <underline><ext-link ext-link-type="uri" xlink:href="http://journals.plos.org/plospathogens/s/submission-guidelines" xlink:type="simple">http://journals.plos.org/ploscompbiol/s/submission-guidelines#loc-materials-and-methods</ext-link></underline></p>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s014">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">PCOMPBIOL-D-19-01642_R1_reviewer_gwc_comments.pdf</named-content></p>
      </caption>
      <media xlink:href="pcbi.1007973.s014.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s015">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">simon_selectivity.pdf</named-content></p>
      </caption>
      <media xlink:href="pcbi.1007973.s015.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article id="pcbi.1007973.r004" article-type="author-comment">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1007973.r004</article-id>
    <title-group>
      <article-title>Author response to Decision Letter 1</article-title>
    </title-group>
    <related-article id="rel-obj004" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007973" related-article-type="editor-report"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="author-response-date">17 May 2020</named-content>
    </p>
    <supplementary-material content-type="local-data" id="pcbi.1007973.s016">
      <label>Attachment</label>
      <caption>
        <p>Submitted filename: <named-content content-type="submitted-filename">response.pdf</named-content></p>
      </caption>
      <media xlink:href="pcbi.1007973.s016.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </body>
</sub-article>
<sub-article id="pcbi.1007973.r005" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1007973.r005</article-id>
    <title-group>
      <article-title>Decision Letter 2</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Einhäuser</surname>
          <given-names>Wolfgang</given-names>
        </name>
        <role>Deputy Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fyshe</surname>
          <given-names>Alona</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Einhäuser, Fyshe</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Einhäuser, Fyshe</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj005" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007973" related-article-type="reviewed-article"/>
    <custom-meta-group>
      <custom-meta>
        <meta-name>Submission Version</meta-name>
        <meta-value>2</meta-value>
      </custom-meta>
    </custom-meta-group>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">21 May 2020</named-content>
    </p>
    <p>Dear Mr. Xiao,</p>
    <p>We are pleased to inform you that your manuscript 'XDream: finding preferred stimuli for visual neurons using generative networks and gradient-free optimization' has been provisionally accepted for publication in PLOS Computational Biology.</p>
    <p>Before your manuscript can be formally accepted you will need to complete some formatting changes, which you will receive in a follow up email. A member of our team will be in touch with a set of requests.</p>
    <p>Please note that your manuscript will not be scheduled for publication until you have made the required changes, so a swift response is appreciated.</p>
    <p>IMPORTANT: The editorial review process is now complete. PLOS will only permit corrections to spelling, formatting or significant scientific errors from this point onwards. Requests for major changes, or any which affect the scientific understanding of your work, will cause delays to the publication date of your manuscript.</p>
    <p>Should you, your institution's press office or the journal office choose to press release your paper, you will automatically be opted out of early publication. We ask that you notify us now if you or your institution is planning to press release the article. All press must be co-ordinated with PLOS.</p>
    <p>Thank you again for supporting Open Access publishing; we are looking forward to publishing your work in PLOS Computational Biology. </p>
    <p>Best regards,</p>
    <p>Alona Fyshe, Ph.D.</p>
    <p>Associate Editor</p>
    <p>PLOS Computational Biology</p>
    <p>Wolfgang Einhäuser</p>
    <p>Deputy Editor</p>
    <p>PLOS Computational Biology</p>
    <p>***********************************************************</p>
  </body>
</sub-article>
<sub-article id="pcbi.1007973.r006" article-type="editor-report">
  <front-stub>
    <article-id pub-id-type="doi">10.1371/journal.pcbi.1007973.r006</article-id>
    <title-group>
      <article-title>Acceptance letter</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Einhäuser</surname>
          <given-names>Wolfgang</given-names>
        </name>
        <role>Deputy Editor</role>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Fyshe</surname>
          <given-names>Alona</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <permissions>
      <copyright-statement>© 2020 Einhäuser, Fyshe</copyright-statement>
      <copyright-year>2020</copyright-year>
      <copyright-holder>Einhäuser, Fyshe</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <related-article id="rel-obj006" ext-link-type="doi" xlink:href="10.1371/journal.pcbi.1007973" related-article-type="reviewed-article"/>
  </front-stub>
  <body>
    <p>
      <named-content content-type="letter-date">9 Jun 2020</named-content>
    </p>
    <p>PCOMPBIOL-D-19-01642R2 </p>
    <p>XDream: finding preferred stimuli for visual neurons using generative networks and gradient-free optimization</p>
    <p>Dear Dr Xiao,</p>
    <p>I am pleased to inform you that your manuscript has been formally accepted for publication in PLOS Computational Biology. Your manuscript is now with our production department and you will be notified of the publication date in due course.</p>
    <p>The corresponding author will soon be receiving a typeset proof for review, to ensure errors have not been introduced during production. Please review the PDF proof of your manuscript carefully, as this is the last chance to correct any errors. Please note that major changes, or those which affect the scientific understanding of the work, will likely cause delays to the publication date of your manuscript. </p>
    <p>Soon after your final files are uploaded, unless you have opted out, the early version of your manuscript will be published online. The date of the early version will be your article's publication date. The final article will be published to the same URL, and all versions of the paper will be accessible to readers.</p>
    <p>Thank you again for supporting PLOS Computational Biology and open-access publishing. We are looking forward to publishing your work! </p>
    <p>With kind regards,</p>
    <p>Laura Mallard</p>
    <p>PLOS Computational Biology | Carlyle House, Carlyle Road, Cambridge CB4 3DN | United Kingdom <email>ploscompbiol@plos.org</email> | Phone +44 (0) 1223-442824 | <ext-link ext-link-type="uri" xlink:href="http://ploscompbiol.org">ploscompbiol.org</ext-link> | @PLOSCompBiol</p>
  </body>
</sub-article>
