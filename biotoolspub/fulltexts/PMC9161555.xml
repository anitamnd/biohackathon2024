<?DTDIdentifier.IdentifierValue -//NLM//DTD Journal Archiving and Interchange DTD v2.3 20070202//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName archivearticle.dtd?>
<?SourceDTD.Version 2.3?>
<?ConverterInfo.XSLTName nlm2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Front Cell Dev Biol</journal-id>
    <journal-id journal-id-type="iso-abbrev">Front Cell Dev Biol</journal-id>
    <journal-id journal-id-type="publisher-id">Front. Cell Dev. Biol.</journal-id>
    <journal-title-group>
      <journal-title>Frontiers in Cell and Developmental Biology</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2296-634X</issn>
    <publisher>
      <publisher-name>Frontiers Media S.A.</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9161555</article-id>
    <article-id pub-id-type="publisher-id">875044</article-id>
    <article-id pub-id-type="doi">10.3389/fcell.2022.875044</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Cell and Developmental Biology</subject>
        <subj-group>
          <subject>Methods</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>PyZebrascope: An Open-Source Platform for Brain-Wide Neural Activity Imaging in Zebrafish</article-title>
      <alt-title alt-title-type="left-running-head">Barbara et al.</alt-title>
      <alt-title alt-title-type="right-running-head">PyZebrascope: Software for Whole-Brain Imaging</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Barbara</surname>
          <given-names>Rani</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="fn1" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1732387/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Nagathihalli Kantharaju</surname>
          <given-names>Madhu</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff3" ref-type="aff">
          <sup>3</sup>
        </xref>
        <xref rid="fn1" ref-type="author-notes">
          <sup>†</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1782854/overview"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Haruvi</surname>
          <given-names>Ravid</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1783826/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Harrington</surname>
          <given-names>Kyle</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff4" ref-type="aff">
          <sup>4</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">*</xref>
        <uri xlink:href="https://loop.frontiersin.org/people/1058479/overview"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Kawashima</surname>
          <given-names>Takashi</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">
          <sup>1</sup>
        </xref>
        <xref rid="c001" ref-type="corresp">*</xref>
        <uri xlink:href="https://loop.frontiersin.org/people/823132/overview"/>
      </contrib>
    </contrib-group>
    <aff id="aff1"><sup>1</sup><institution>Department of Brain Sciences</institution>, <institution>Weizmann Institute of Science</institution>, <addr-line>Rehovot</addr-line>, <country>Israel</country></aff>
    <aff id="aff2"><sup>2</sup><institution>Max-Delbrück-Centrum for Molecular Medicine in the Helmholtz Association</institution>, <addr-line>Berlin</addr-line>, <country>Germany</country></aff>
    <aff id="aff3"><sup>3</sup><institution>Humboldt University of Berlin</institution>, <addr-line>Berlin</addr-line>, <country>Germany</country></aff>
    <aff id="aff4"><sup>4</sup><institution>Oak Ridge National Laboratory</institution>, <addr-line>Oak Ridge</addr-line>, <addr-line>TN</addr-line>, <country>United States</country></aff>
    <author-notes>
      <fn fn-type="edited-by">
        <p><bold>Edited by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/1237027/overview" ext-link-type="uri">Lilach Avitan</ext-link>, Hebrew University of Jerusalem, Israel</p>
      </fn>
      <fn fn-type="edited-by">
        <p><bold>Reviewed by:</bold><ext-link xlink:href="https://loop.frontiersin.org/people/64344/overview" ext-link-type="uri">Marco Lorenzo Dal Maschio</ext-link>, University of Padua, Italy</p>
        <p><ext-link xlink:href="https://loop.frontiersin.org/people/1328201/overview" ext-link-type="uri">Limor Freifeld</ext-link>, Technion Israel Institute of Technology, Israel</p>
      </fn>
      <corresp id="c001">*Correspondence: Kyle Harrington, <email>kyle@kyleharrington.com</email>; Takashi Kawashima, <email>takashi.kawashima@weizmann.ac.il</email>
</corresp>
      <fn fn-type="equal" id="fn1">
        <label>
          <sup>†</sup>
        </label>
        <p>These authors have contributed equally to this work</p>
      </fn>
      <fn fn-type="other">
        <p>This article was submitted to Molecular and Cellular Pathology, a section of the journal Frontiers in Cell and Developmental Biology</p>
      </fn>
    </author-notes>
    <pub-date pub-type="epub">
      <day>19</day>
      <month>5</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>10</volume>
    <elocation-id>875044</elocation-id>
    <history>
      <date date-type="received">
        <day>13</day>
        <month>2</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>18</day>
        <month>4</month>
        <year>2022</year>
      </date>
      <date date-type="publishedonline">
        <day>18</day>
        <month>4</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>Copyright © 2022 Barbara, Nagathihalli Kantharaju, Haruvi, Harrington and Kawashima.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <copyright-holder>Barbara, Nagathihalli Kantharaju, Haruvi, Harrington and Kawashima</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Understanding how neurons interact across the brain to control animal behaviors is one of the central goals in neuroscience. Recent developments in fluorescent microscopy and genetically-encoded calcium indicators led to the establishment of whole-brain imaging methods in zebrafish, which record neural activity across a brain-wide volume with single-cell resolution. Pioneering studies of whole-brain imaging used custom light-sheet microscopes, and their operation relied on commercially developed and maintained software not available globally. Hence it has been challenging to disseminate and develop the technology in the research community. Here, we present PyZebrascope, an open-source Python platform designed for neural activity imaging in zebrafish using light-sheet microscopy. PyZebrascope has intuitive user interfaces and supports essential features for whole-brain imaging, such as two orthogonal excitation beams and eye damage prevention. Its camera module can handle image data throughput of up to 800 MB/s from camera acquisition to file writing while maintaining stable CPU and memory usage. Its modular architecture allows the inclusion of advanced algorithms for microscope control and image processing. As a proof of concept, we implemented a novel automatic algorithm for maximizing the image resolution in the brain by precisely aligning the excitation beams to the image focal plane. PyZebrascope enables whole-brain neural activity imaging in fish behaving in a virtual reality environment. Thus, PyZebrascope will help disseminate and develop light-sheet microscopy techniques in the neuroscience community and advance our understanding of whole-brain neural dynamics during animal behaviors.</p>
    </abstract>
    <kwd-group>
      <kwd>zebrafish</kwd>
      <kwd>lightsheet microscopy</kwd>
      <kwd>open source</kwd>
      <kwd>software</kwd>
      <kwd>image processing</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source id="cn001">
          <institution-wrap>
            <institution>Azrieli Foundation
</institution>
            <institution-id institution-id-type="doi">10.13039/501100005155</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source id="cn002">
          <institution-wrap>
            <institution>Center for New Scientists, Weizmann Institute of Science
</institution>
            <institution-id institution-id-type="doi">10.13039/501100020296</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source id="cn003">
          <institution-wrap>
            <institution>Oak Ridge National Laboratory
</institution>
            <institution-id institution-id-type="doi">10.13039/100006228</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source id="cn004">
          <institution-wrap>
            <institution>Helmholtz-Gemeinschaft
</institution>
            <institution-id institution-id-type="doi">10.13039/501100001656</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
  </article-meta>
</front>
<body>
  <sec id="s1">
    <title>Introduction</title>
    <p>Animal behaviors occur through the collective actions of neurons distributed across the brain. Understanding such distributed neural dynamics in their entirety has been one of the central goals in neuroscience (<xref rid="B2" ref-type="bibr">Ahrens and Engert, 2015</xref>). Toward this goal, optical recording of neural activity at a brain-wide scale has become possible based on recent developments in genetically-encoded calcium indicators (<xref rid="B8" ref-type="bibr">Chen et al., 2013</xref>; <xref rid="B20" ref-type="bibr">Inoue et al., 2019</xref>) and volumetric fluorescence microscopy (<xref rid="B22" ref-type="bibr">Keller et al., 2008</xref>; <xref rid="B37" ref-type="bibr">Sofroniew et al., 2016</xref>; <xref rid="B43" ref-type="bibr">Voleti et al., 2019</xref>; <xref rid="B14" ref-type="bibr">Demas et al., 2021</xref>). Whole-brain neural activity imaging at cellular resolution was first demonstrated in larval zebrafish (<xref rid="B4" ref-type="bibr">Ahrens et al., 2013</xref>; <xref rid="B31" ref-type="bibr">Panier et al., 2013</xref>; <xref rid="B18" ref-type="bibr">Freeman et al., 2014</xref>) among other vertebrate model organisms using digital scanned laser light-sheet microscopy (DSLM) (<xref rid="B22" ref-type="bibr">Keller et al., 2008</xref>). DSLM excites sample fluorescence in multiple voxels along the light cone of the excitation beam, and rapid scanning of the excitation beam enables fast volumetric scans with high spatial resolution and low light toxicity. These advantages of DSLM are best exploited in the optically transparent brain of larval zebrafish. DSLM enabled studies of whole-brain neural dynamics during visually-evoked swimmming (<xref rid="B40" ref-type="bibr">Vladimirov et al., 2014</xref>; <xref rid="B9" ref-type="bibr">Chen et al., 2018</xref>), motor learning (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>; <xref rid="B24" ref-type="bibr">Markov et al., 2021</xref>), learned helplessness (<xref rid="B28" ref-type="bibr">Mu et al., 2019</xref>), threat escape (<xref rid="B23" ref-type="bibr">Mancienne, 2021</xref>), and body posture change (<xref rid="B26" ref-type="bibr">Migault et al., 2018</xref>). It also revealed spontaneous noise dynamics across the brain (<xref rid="B33" ref-type="bibr">Ponce-Alvarez et al., 2018</xref>) and how those dynamics change during neural perturbations (<xref rid="B41" ref-type="bibr">Vladimirov et al., 2018</xref>; <xref rid="B44" ref-type="bibr">Yang, 2018</xref>) or administration of psychoactive reagents (<xref rid="B7" ref-type="bibr">Burgstaller et al., 2019</xref>). Further, it enabled high-speed voltage imaging on a single axial plane for recording membrane potential and spiking activity from a neural population during swimming in the midbrain (<xref rid="B1" ref-type="bibr">Abdelfattah et al., 2019</xref>) and the spinal cord (<xref rid="B5" ref-type="bibr">Böhm et al., 2022</xref>). These pioneering studies in zebrafish using DSLM expanded our understanding of diverse functionalities of the vertebrate brain.</p>
    <p>Despite the above advantages of DSLM in achieving both high optical resolution and imaging speed compared to other large-scale volumetric imaging methods, such as structured illumination microscopy (<xref rid="B25" ref-type="bibr">Marques et al., 2020</xref>), multiphoton microscopy (<xref rid="B3" ref-type="bibr">Ahrens et al., 2012</xref>; <xref rid="B29" ref-type="bibr">Naumann et al., 2016</xref>; <xref rid="B12" ref-type="bibr">dal Maschio et al., 2017</xref>; <xref rid="B6" ref-type="bibr">Bruzzone, 2021</xref>), and light-field microscopy (<xref rid="B14" ref-type="bibr">Demas et al., 2021</xref>; <xref rid="B11" ref-type="bibr">Cong et al., 2017</xref>), it is still challenging to build a light-sheet microscope customized for zebrafish imaging and operate it for multiple experiments every day. These challenges come from the complexity of the microscope itself and its built-in parameters that the experimenter needs to manipulate. For example, light-sheet microscopes for whole-brain imaging in zebrafish (<xref rid="F1" ref-type="fig">Figure 1A</xref>) typically consist of two excitation optical paths from the lateral and front sides of the fish and one optical detection path above the fish (<xref rid="B40" ref-type="bibr">Vladimirov et al., 2014</xref>; <xref rid="B26" ref-type="bibr">Migault et al., 2018</xref>; <xref rid="B24" ref-type="bibr">Markov et al., 2021</xref>; <xref rid="B23" ref-type="bibr">Mancienne, 2021</xref>) (<xref rid="F1" ref-type="fig">Figure 1B</xref>). The fish is fixed in a water chamber and needs to be precisely maneuvered into focal points of the excitation and detection objectives. Moreover, it is necessary to prevent laser illumination into the eyes to secure fish’s vision for behavioral tasks and prevent eye damage (<xref rid="F1" ref-type="fig">Figure 1B</xref>). This configuration requires the experimenters to set at least ∼20 parameters (camera exposure time per plane, number of planes per volume, the start and end positions for 2d motion for each excitation beam, light intensity and on/off timing for lasers, the start and end positions for detection objective motions, 3-dimensional positions for the fish chamber, and parameters for eye exclusion). The first studies (<xref rid="B4" ref-type="bibr">Ahrens et al., 2013</xref>; <xref rid="B40" ref-type="bibr">Vladimirov et al., 2014</xref>) on whole-brain imaging in zebrafish were made possible by using software custom-developed and maintained by a commercial entity, which charges high-priced service costs and does not provide service globally. This situation prevented the dissemination of the technology in a flexible and customizable manner. Past progress of optical microscopy in neuroscience has been driven by open-source software (<xref rid="B32" ref-type="bibr">Pologruto et al., 2003</xref>; <xref rid="B42" ref-type="bibr">Voigt et al., 2022</xref>) for microscope control written in a programming language widely used in academics, such as ScanImage (<xref rid="B32" ref-type="bibr">Pologruto et al., 2003</xref>) for multiphoton microscopy. Hence, developing an open-source platform for light-sheet microscopy dedicated to neural activity imaging in zebrafish is necessary.</p>
    <fig position="float" id="F1">
      <label>FIGURE 1</label>
      <caption>
        <p>Our microscope setup and modular architecture of PyZebrascope for whole-brain neural activity imaging in zebrafish. <bold>(A)</bold> Schematic drawing of setups for whole-brain neural activity imaging in behaving zebrafish. Excitation objectives on the lateral and front sides of the fish illuminate excitation beams into the brain. The detection objective above the fish moves in the axial direction in sync with the motion of the excitation beams during the volumetric scan. We paralyze the muscle of the fish and record swim-related electrical signals from axonal bundles of spinal motoneurons by using two pipettes attached to the tail. The recorded signals are analyzed online and reflected in the motion of visual scenes projected below the fish. <bold>(B)</bold> Two-beam scanning during whole-brain imaging. The front laser beam scans areas between the eyes, and the side laser beam scans areas behind and above the eye. The side laser beam avoids eye damage by turning off when it overlaps with the eye. <bold>(C)</bold> Schematic of our light-sheet microscope and water chamber for imaging zebrafish. Detailed descriptions are in the main text. TL, tube lens; SL, scan lens; EO, excitation objective; DO, detection objective; EF, emission filter. <bold>(D)</bold> The architecture of PyZebrascope software. Our developed modules (beige), existing Python libraries (green), hardware drivers (purple), hardware (blue) and output files (yellow) are shown. Arrows show information flow between modules with labels such as “Pos.” (linear position information for devices) and “Params.” (device parameters).</p>
      </caption>
      <graphic xlink:href="fcell-10-875044-g001" position="float"/>
    </fig>
    <p>Recently, software interface for light-sheet microscopy in zebrafish, μSPIM (<xref rid="B36" ref-type="bibr">Saska et al., 2021</xref>), was developed based on the open-source microscope control platform μManager (<xref rid="B16" ref-type="bibr">Edelstein et al., 2014</xref>) in Java/C programming language. Although this study only demonstrated partial volumetric imaging of the zebrafish brain based on single excitation beam scanning, it opened the possibility of developing an advanced open-source platform for light-sheet microscopy at a larger scale. Such software will reduce the laboriousness of imaging experiments by providing control of numerous device parameters on a simple and intuitive interface. Its implementation in a standard programming language for researchers will facilitate the addition of innovative functionalities toward their research goals by importing modules from a broad developer ecosystem for hardware control, signal processing, computer vision, and machine learning.</p>
    <p>Toward this goal, we developed PyZebrascope, an open-source Python platform designed for whole-brain imaging in zebrafish using light-sheet microscopy. PyZebrascope supports essential features for whole-brain imaging, such as two orthogonal excitation beams and eye damage prevention. Its user interfaces allow the users to adjust parameters for lasers, filters, beam scanning, camera, sample positions, and eye damage prevention intuitively. Its multi-threaded camera module can handle stable high data throughput from multiple cameras. It is also possible to add advanced algorithms for microscope control and image processing developed in the Python community (<xref rid="B38" ref-type="bibr">Van Der Walt et al., 2014</xref>; <xref rid="B19" ref-type="bibr">Harris et al., 2020</xref>). As a proof of concept, we implemented a novel GPU-based automatic algorithm for maximizing the image resolution in the brain by precisely aligning the excitation beams to the image focal plane, which is usually a time-consuming and indecisive process for the experimenter. Lastly, we demonstrated that PyZebrascope enables whole-brain imaging in zebrafish behaving in a virtual reality environment. Thus, PyZebrascope is a versatile platform for disseminating and advancing technology for large-scale neural activity imaging in zebrafish and will accelerate our understanding of whole-brain neural dynamics during animal behaviors.</p>
  </sec>
  <sec sec-type="methods" id="s2">
    <title>Methods</title>
    <sec>
      <title>Software and Hardware</title>
      <p>PyZebrascope: All the modules of PyZebrascope were written in Python programming language on Spyder IDE (<ext-link xlink:href="https://www.spyder-ide.org/" ext-link-type="uri">https://www.spyder-ide.org/</ext-link>). We used a PC for microscope control that has Windows 10 operation system, two processors (Intel Xeon Gold 6,244), 384 GB of DDR4 memory, a GPU (nVidia GeForce GTX1660), and a 15 TB solid-state drive (Micron 9300 Pro, &gt;3 GB/s writing/reading speed). We followed manufacturers’ manuals for connecting devices, including the camera, data acquisition board and sample stages.</p>
      <p>We benchmarked the resource usage and disk writing speed of PyZebrascope (<xref rid="F3" ref-type="fig">Figure 3</xref>) by using psutil package (<ext-link xlink:href="https://github.com/giampaolo/psutil" ext-link-type="uri">https://github.com/giampaolo/psutil</ext-link>) in Python. We identified the process id of PyZebrascope by using Windows task manager and monitored its CPU and memory usage at an interval of 5 s in a separate Python instance while PyZebrascope records camera images for 30 min. Disk writing speed was also monitored using psutil by tracking the increase of disk usage in the above solid-state drive at the same time interval.</p>
      <p>We calculated the image resolution measure in <xref rid="F4" ref-type="fig">Figure 4B</xref> as half the wavelength of the identified maximum frequency after Fourier and polar transformations based with the below formula<disp-formula id="equ1"><mml:math id="m1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo> </mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo> </mml:mo><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>∗</mml:mo><mml:mi>F</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>whereas [pixel dimension] is 0.405 μm per camera pixel in our setup and <italic>F</italic> is in the identified maximum frequency component (pixel distance after the polar transformation).</p>
      <p>Light-sheet microscope: We designed a custom light-sheet microscope that has a virtual reality setup for behavioral recordings, two optical paths for excitation beams, two cameras for fluorescence detection, and additional space for future implementation of the third optical path for excitation beam and neural perturbations (<xref rid="s11" ref-type="sec">Supplementary Figure S1</xref>). We designed this microscope using Inventor Professionals software (Autodesk), and our CAD model is available upon request. We listed commercially available parts and custom-designed parts for our light-sheet microscope in <xref rid="s11" ref-type="sec">Supplementary Table S1</xref> and their overall costs in <xref rid="s11" ref-type="sec">Supplementary Table S2</xref>.</p>
    </sec>
    <sec id="s2-1">
      <title>Zebrafish Experiments</title>
      <p>We used a 6-day old transgenic zebrafish that pan-neuronally express nuclear-localized, genetically-encoded calcium indicators (<xref rid="B13" ref-type="bibr">Dana et al., 2019</xref>) and co-express RFP in tph2+ neurons (Tg(HuC:H2B-GCaMP7f)<sup>jf96</sup> and Tg(tph2:epNTR-TagRFP)<sup>jf41</sup>, courtesy of Dr. Misha Ahrens) for the imaging experiment. The zebrafish was immobilized and mounted to an imaging chamber as described previously (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>). Briefly, the fish larvae were immobilized by bath application of α-Bungarotoxin (B1601, Fisher Scientific, 1 mg/ml) dissolved in external solution (in mM: 134 NaCl, 2.9 KCl, 2.1 CaCl2, 1.2 MgCl2, 10 HEPES, 10 glucose; pH 7.8; 290 mOsm) for 25–30 s and embedded in agarose on a custom-made pedestal inside a glass-walled chamber with a diffusive screen underneath the fish (<xref rid="F1" ref-type="fig">Figure 1C</xref>). Agarose around the head was removed with a microsurgical knife (#10318-14, Fine Science Tools) to minimize the scattering of the excitation laser. Laser power from the side beam path was, on average, approximately 21 µW. The distance between the fish and the display was about 4 mm.</p>
      <p>We performed a fictive recording of fish’s swim patterns during a motor learning task (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>). Electric signals from motor neuron axons in the tail were recorded using borosilicate pipettes (TW150-3, World Precision Instruments) pulled by a horizontal puller (P-1000, Sutter) and fire-polished by a microforge (MF-900, Narishige). The pipettes were filled with fish-rearing water and connected to the tail using minimal negative pressure. Swim signals were recorded using an amplifier (RHD2132 amplifier connected to RHD-2000 interface board, Intan Technologies). We used custom-written Python software (available upon request) for executing the same algorithms for closed-loop virtual reality experiments as the software used in previous studies (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>; <xref rid="B1" ref-type="bibr">Abdelfattah et al., 2019</xref>). It samples signals from the above amplifier at 6 kilohertz, automatically detects swim events, and moves the visual stimulus projected below the fish in a closed-loop with a delay of 35 milliseconds. The motor learning task consisted of 12 repetitions of a 157-s session, where we presented different types of tasks (7-s training, 15-s training, and 30-s training) in a pseudo-random manner.</p>
      <p>Data analysis: We processed acquired imaging data on a Linux server in High Performance Computing (HPC) division in the Weizmann Institute of Science. This server has two Xeon processors (Xeon Gold 6,248, Intel), 384 GB RAM, 13-TB SSD array, and a GPU computing board (Tesla V100, nVidia). We performed data processing using custom Python scripts that execute the same algorithms as those established in our previous work (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>). All the analyses of imaging data were performed on a remote JupyterLab environment (<ext-link xlink:href="https://jupyterlab.readthedocs.io/" ext-link-type="uri">https://jupyterlab.readthedocs.io/</ext-link>).</p>
      <p>Briefly, we first registered time-series images from the same Z-planes by using phase correlation algorithms on the above GPU. We then examined residual drifts in the lateral and axial directions and discarded data with excessive drifts (&gt;5 μm) in either direction. We then identified individual neurons that express nuclear-localized GCaMP based on the average image by using an algorithm for detecting circular shapes in images. We then extracted fluorescent time series from the central part of identified neurons (49 pixels). We identified 79,176 neurons across the brain in the experiment described in <xref rid="F5" ref-type="fig">Figure 5</xref>. We calculated the baseline fluorescence trace for each extracted fluorescence trace by taking the rolling percentile of the bottom 30% with a window size of 2 min and then divided the original fluorescent time series by this baseline trace to obtain ΔF/F time series for each neuron.</p>
      <p>For the analyses shown in <xref rid="F5" ref-type="fig">Figures 5B</xref>; <xref rid="s11" ref-type="sec">Supplementary Figure S5B</xref>, we focused on neurons whose task-dependent activity modulation was consistent across multiple sessions. For extracting such neurons, we created a matrix of the average ΔF/F in each task period (12 periods) for each session and performed one-way ANOVA across trials for individual neurons. In this way, we identified 36,818 neurons with significant <italic>p</italic>-values (<italic>p</italic> &lt; 0.001) for consistent task-dependent activity modulation across sessions. We then extracted neurons for each brain area by their spatial locations. We classified neurons into three groups in each area by applying a k-means clustering method (n = 3) to the trial average activities of neurons. We then picked a neuron that shows the largest response amplitude from each identified cluster in each brain area and plotted their time series in <xref rid="F5" ref-type="fig">Figure 5B</xref>.</p>
      <p>For the analysis shown in <xref rid="F5" ref-type="fig">Figure 5C</xref>, we used the same set of 36,818 neurons identified in the above statistical test for response reliability. We normalized the ΔF/F trace of each neuron by its 99-percentile value and clipped values more than 1. We then applied non-negative matrix factorization to the activity of neurons (n = 4 components). We extracted neurons whose weight is more than 0.2 for each component and color-plotted their locations in the top projection and side projection images of the brain.</p>
      <p>For the analysis shown in <xref rid="s11" ref-type="sec">Supplementary Figure S5B</xref>, we used a volumetric stack of red fluorescence in the same fish acquired after the whole-brain imaging experiments described in <xref rid="F5" ref-type="fig">Figure 5</xref>. We extracted 55 RFP-positive neurons among the above 36,818 neurons by using a hand-drawn mask over the area of the dorsal raphe nucleus. We identified the RFP-positive neurons by detecting the red fluorescence by a fixed threshold for mean pixel values (120). We calculated an average ΔF/F trace for different tasks (7-s training, 15-s training, and 30-s training) across sessions for each neuron and further averaged it across them.</p>
    </sec>
  </sec>
  <sec sec-type="results" id="s3">
    <title>Results</title>
    <sec id="s3-1">
      <title>Microscope Design and the Architecture of PyZebrascope</title>
      <p>We developed PyZebrascope for our custom-designed light-sheet microscope (Figures 1C; <xref rid="s11" ref-type="sec">Supplementary Figure S1</xref>, <xref rid="s11" ref-type="sec">Supplementary Table S1</xref> for parts lists), for which many components are common to those in the previous studies (<xref rid="B31" ref-type="bibr">Panier et al., 2013</xref>; <xref rid="B40" ref-type="bibr">Vladimirov et al., 2014</xref>; <xref rid="B23" ref-type="bibr">Mancienne, 2021</xref>; <xref rid="B24" ref-type="bibr">Markov et al., 2021</xref>). Our system is equipped with two lasers, providing light sources for two excitation arms that are required to scan fish brains from the lateral and front sides (<xref rid="F1" ref-type="fig">Figure 1B</xref>). We control the lasers’ on/off by digital input into the lasers. The brightnesses of laser outputs are controlled by 1) setting the power level of the laser using serial communication and 2) diminishing the laser output with different levels of neutral density (ND) filters. These two levels of brightness adjustment allow us to maintain laser output levels in a range causes less power fluctuation. Typically, the excitation beam for the front scanning (<xref rid="F1" ref-type="fig">Figure 1B</xref>) requires less output power due to its narrow scanning range. Each brightness-adjusted beam is then scanned by sets of 2-axis galvanometers for volumetric scanning. The beams expand through a pair of a telecentric scan lens (SL) and a tube lens (TL) and then focus onto the sample through an excitation objective (EO). Fluorescence from the fish brain is collected by a detection objective (DO) placed above the brain, and this detection objective moves along the axial direction in sync with the excitation beam by a piezoelectric drive during volumetric scanning. The fluorescent image is focused onto scientific CMOS (sCMOS) camera through a tube lens (TL) and an emission filter (EF, <xref rid="F1" ref-type="fig">Figure 1C</xref>). We can also add another sCMOS camera in the detection path for multicolor imaging, as shown in our CAD model (<xref rid="s11" ref-type="sec">Supplementary Figure S1</xref>, CAD model files available on request).</p>
      <p>PyZebrascope controls the above-mentioned multiple devices through its modular architecture and organized user interface (<xref rid="F1" ref-type="fig">Figures 1D</xref>, <xref rid="F2" ref-type="fig">2</xref>). It does not require compiling and is launchable from any Python development environment that allows PyQT applications. It controls devices through serial communications (laser setting, filter wheels, sample stage), analog output (galvanometer, camera timing), digital output (laser on/off), and camera interface cables (<xref rid="s11" ref-type="sec">Supplementary Figure S1</xref>). The camera unit (camera.py) controls camera settings through a Python binding (pymmcore, <ext-link xlink:href="https://github.com/micro-manager/pymmcore" ext-link-type="uri">https://github.com/micro-manager/pymmcore</ext-link>) for low-level device interface of μManager (<xref rid="B16" ref-type="bibr">Edelstein et al., 2014</xref>). It also acquires images from the camera and saves them in HDF files. The analog/digital output unit (signal.py) generates waveforms and outputs them through a Python binding (ni-daqmx, <ext-link xlink:href="https://nidaqmx-python.readthedocs.io/" ext-link-type="uri">https://nidaqmx-python.readthedocs.io/</ext-link>) for data acquisition interface devices. Its outputs can be modified to drive alternative components of light-sheet microscopes that operate according to voltage inputs, such as electrically tunable lens (<xref rid="B17" ref-type="bibr">Favre-Bulle et al., 2018</xref>) and scanners based on microelectromechanical systems (MEMS) (<xref rid="B26" ref-type="bibr">Migault et al., 2018</xref>). Other modules for controlling devices via serial communication (laser.py, nd_filter.py, piezo.py) are written in a generalizable manner so that external users can adapt them to their preferred devices.</p>
      <fig position="float" id="F2">
        <label>FIGURE 2</label>
        <caption>
          <p>Graphical user interface of PyZebrascope. <bold>(A)</bold> Two main tabbed windows of the user interface of PyZebrascope. The first window provides saving/loading microscope configurations, sample stage control, selection of neutral density filter, and laser control. The second window provides camera exposure control, motion control for the detection objective, scanning control for excitation beams, switch between volumetric and single-plane scans, and FIle saving settings. <bold>(B)</bold> Examples of analog and digital outputs for microscope components during a volumetric scan. Black scale bars next to waveforms represent 1 V. Camera trigger signals have different amplitudes at the start (4.5 V) and the end of a volumetric scan (3.5 V) compared to those in the middle (4.0 V). These varying amplitudes of trigger signals above the camera’s threshold would allow the behavior control software to receive camera trigger signals in parallel and detect the onset of each volumetric scan for synchronization. <bold>(C)</bold> Camera view window. It allows users to view ambient and fluorescent images, scroll through different Z-planes of a volumetric stack, zoom in/out, adjust the image display brightness, set region of interest (ROI) to the camera, and overlay two images in different color channels from two cameras during multicolor imaging. This window can also be stretched to any preferred size. <bold>(D)</bold> Eye exclusion window. The lateral view of a volumetric stack allows the user to set an elliptic exclusion area (red) which will turn off the side laser when it scans over the eye.</p>
        </caption>
        <graphic xlink:href="fcell-10-875044-g002" position="float"/>
      </fig>
      <p>The configuration unit (setting.py) allows the users to save all the microscope configuration parameters in an XML file. Savable parameters include the state of all connected devices, the position of the 3-axis stage, the choice of lasers and their output intensity, type of neutral density filters, the region of interest on the camera’s field of view, and parameters for volumetric scanning. This file is also automatically saved for every experiment with time logs. The experimenter can load this setting file later to connect necessary devices and configure all the device parameters automatically. Although the user still needs to fine-tune sample positions and microscope scanning parameters for each experiment, this automatic loading minimizes the users’ efforts in conducting experiments consistently across sessions.</p>
    </sec>
    <sec id="s3-2">
      <title>Sample Preparation and Stage Control</title>
      <p>The fish for imaging resides Sample Stage Control a water chamber with glass windows (<xref rid="F1" ref-type="fig">Figure 1C</xref>). Within the water chamber, the fish is embedded in low-melting agarose on a small pedestal, and this pedestal allows us to expose the fish head after removing surrounding agarose and at the same time record electrical signals from the tail for fictive swim recording (<xref rid="B3" ref-type="bibr">Ahrens et al., 2012</xref>; <xref rid="B40" ref-type="bibr">Vladimirov et al., 2014</xref>; <xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>). The user mounts the fish into the water chamber outside the microscope at the beginning of each experiment. Then the chamber is placed on a motorized 3-axis stage, which brings the fish to the focus of the detection and excitation objectives (<xref rid="s11" ref-type="sec">Supplementary Figure S1</xref>).</p>
      <p>The PyZebrascope sample stage unit (stage.py) moves the motorized stage holding the water chamber according to the user’s commands from the software interface. It also allows users to automatically move the stage between stereotypical positions, such as a home position for replacing water chambers and an imaging position for performing experiments. During imaging experiments, the water chamber, the detection objective, and the excitation objectives need to be placed in close physical proximity. Therefore, the stage unit moves the stage in a constrained manner to avoid the risk of accidental collisions between these components.</p>
    </sec>
    <sec id="s3-3">
      <title>User Interface and Waveform Outputs of Pyzebrascope</title>
      <p>We designed graphical user interfaces based on QT Designer (<xref rid="F2" ref-type="fig">Figure 2</xref>, <ext-link xlink:href="https://doc.qt.io/qt-5/qtdesigner-manual.html" ext-link-type="uri">https://doc.qt.io/qt-5/qtdesigner-manual.html</ext-link>). All of the above device parameters are organized in two tabs in the main window (<xref rid="F2" ref-type="fig">Figure 2A</xref>). Waveforms of analog and digital outputs (<xref rid="F2" ref-type="fig">Figure 2B</xref>) can be viewed in the user interface window. We implemented a waveform generator module (signal.py) that synchronizes the beam scanning to the full opening of the rolling shutter in the sCMOS camera. It also moves the piezoelectric drives and the beam scanning in the axial direction in a continuous manner (<xref rid="F2" ref-type="fig">Figure 2B</xref>) rather than in a step-wise manner because the piezoelectric drive cannot move the weight of the detection objective with sub-millisecond accuracy (<xref rid="s11" ref-type="sec">Supplementary Figure S3</xref>). Its continuous motion without frequent accelerating/decelerating events provides a better match of axial positions between the detection objective and beam scanning. Also, the waveform for moving piezoelectric drive can be shifted by a user-defined time (10–20 milliseconds) compared to those for scanning galvanometers to compensate for the movement delay between the movement of the piezoelectric drive and its analog input (<xref rid="s11" ref-type="sec">Supplementary Figure S3</xref>). This module uses Numpy (<xref rid="B19" ref-type="bibr">Harris et al., 2020</xref>), a common library for array programming for researchers, to generate waveforms and can be flexibly edited in the code (signal.py) to enable different types of scanning patterns such as bidirectional scanning for faster volumetric imaging (<xref rid="B36" ref-type="bibr">Saska et al., 2021</xref>).</p>
      <p>The camera view window (<xref rid="F2" ref-type="fig">Figure 2C</xref>, camview.py) will pop up separately when the camera is turned on. It allows the users to view the ambient and fluorescent images of the sample and adjust the display brightness and zoom. It also enables the inspection of the quality of the volumetric stack by using a slider for changing the displayed Z planes. PyZebrascope supports two cameras in the detection path for multicolor imaging, and this camera view window can display images from two cameras separately or overlay them in RGB color channels.</p>
      <p>PyZebrascope also has a dedicated module and interface for eye damage prevention (eye_exclusion.py) (<xref rid="F2" ref-type="fig">Figure 2D</xref>). This feature is necessary for the side laser whose scanning pattern overlaps the eye during whole-brain imaging (<xref rid="F1" ref-type="fig">Figure 1B</xref>) and not for the front laser that only needs to scan a narrow width between the eyes. In this interface, the experimenter looks at the side projection of a volumetric stack and draws an elliptic region of interest (ROI) to set where the lateral laser should be turned off (<xref rid="F1" ref-type="fig">Figure 1B</xref>). This module automatically calculates the timing for turning off the laser across multiple Z-planes based on the set ROI. Thus, this interface prevents laser illumination into the eye while performing experiments, enabling behavioral tasks that depends on visual features presented to the fish.</p>
    </sec>
    <sec id="s3-4">
      <title>Acquisition, Buffering and File Saving of Imaging Data</title>
      <p>One of the challenges in neural activity imaging using light-sheet microscopy is its high image data throughput. Modern scientific CMOS cameras for light-sheet microscopy can acquire several megapixels per frame at the speed of over a hundred frames per second, which amounts to a data throughput of several hundred megabytes per second (MB/s). This high load poses significant challenges in ensuring continuous data flow from camera acquisition to file writing while avoiding writing delays and memory overflows.</p>
      <p>PyZebrascope handles such flow of imaging data using three threads and three buffers for each camera (<xref rid="F3" ref-type="fig">Figure 3A</xref>). The reading thread continuously transfers acquired images in the circular buffer of the camera to a first-in-first-out (FIFO) buffer in the system memory for file writing. This system buffer based on the Python queue module is advantageous compared to the camera’s circular buffer in that it allows access from multiple threads, does not require buffer preallocation, and does not allow overwriting of previously queued data. The writing thread continuously takes the image data from this FIFO system buffer and saves them into Hierarchical Data Format 5 (HDF5) files in uncompressed form. We chose this file format because writing in HDF5 format using h5py library (<ext-link xlink:href="https://www.h5py.org/" ext-link-type="uri">https://www.h5py.org/</ext-link>) yielded higher data throughput than writing in other formats, such Tagged Image File Format (TIFF), at the time of our prototyping. Each HDF5 file stores a set of images for each volumetric scan or a set of 100 images for single-plane imaging. In parallel with these file writing processes, the reading thread intermittently copies image data to another FIFO buffer on system memory (<xref rid="F3" ref-type="fig">Figure 3A</xref>). This buffer provides image data to the camera view thread for displaying acquired images on the software interface (<xref rid="F2" ref-type="fig">Figure 2C</xref>) during imaging experiments.</p>
      <fig position="float" id="F3">
        <label>FIGURE 3</label>
        <caption>
          <p>Acquisition, buffering and file saving of imaging data. <bold>(A)</bold> The flow of image data acquisition, buffering and file saving. PyZebrascope uses three threads and three memory buffers to handle imaging data for each camera. Image data first move from the camera to the camera driver’s circular buffer. Once the image arrives at the circular buffer, Reader thread immediately transfers it to the first-in-first-out (FIFO) buffer in the system memory for file writing. Reader thread also intermittently copies the same image data to another FIFO buffer for previewing on the software interface. Writer thread takes the image data from the writing FIFO buffer and saves them into HDF5 files. Each HDF5 files stores a set of images from each volumetric scan or a set of 100 images for single-plane imaging. <bold>(B)</bold> Stable CPU usage, system memory usage, and data writing by PyZebrascope during high data throughput. We tested three conditions that induce higher loads than typical whole-brain imaging experiments demonstrated in <xref rid="F5" ref-type="fig">Figure 5</xref>: full-frame 80 Hz imaging from one camera (cyan), simultaneous 50 Hz imaging from two cameras (green), and 300 Hz imaging from one camera. We measured CPU usage (left), system memory usage (center) and disk writing speed (right) before, during, and after a 30-min recording session for each condition. Colored arrows next to the graph of disk writing speed represent data rates from the camera for different conditions.</p>
        </caption>
        <graphic xlink:href="fcell-10-875044-g003" position="float"/>
      </fig>
      <p>This structure for handling image data, when combined with a high-speed solid-state drive (&gt;3 GB/s writing speed, see Methods for our computer specifications), achieved a throughput of over 800 MB/s during the continuous acquisition of full-frame (2304 by 2304 pixels), 16-bit images at the speed of 80 frames per second (<xref rid="F3" ref-type="fig">Figure 3B</xref>). PyZebrascope’s CPU and memory usage remained stable during a 30-min recording, and its disk writing speed matched the data rate from the camera throughout the recording. We also achieved stable data throughput for multi-camera imaging (1,600 by 2304 pixels, 16-bit, 50 frames per second) and high-speed imaging (668 by 1,024 pixels, 16-bit, 300 frames per second). We expect it will be possible to scale up image dimensions or frame rates further by, for example, implementing a camera acquisition mode with a lower bit depth or by optimizing file saving algorithms.</p>
    </sec>
    <sec id="s3-5">
      <title>Synchronization With Behavioral Recording</title>
      <p>We synchronize behavioral control software to the image acquisition of PyZebrascope by diverging the camera trigger cable to an analog input channel for the behavioral software (<xref rid="s11" ref-type="sec">Supplementary Figure S4A</xref>). The waveform generator (<xref rid="F2" ref-type="fig">Figure 2B</xref>) of PyZebrascope assigns varying voltage amplitudes to camera trigger pulses above the triggering threshold (3.3 V). This varying pulse amplitude allows the behavioral software and post-processing algorithms to detect camera timings for the first axial plane in a volumetric scan (4.5 V), those for the second to the last axial planes (4.0 V), and those that terminate the acquisition of the last axial plane (3.5 V) (<xref rid="s11" ref-type="sec">Supplementary Figure S4B</xref>). For example, behavioral software can switch the behavioral task by counting the number of volumetric stacks, rather than the number of all the pulses, by detecting 4.5 V pulses at the start of the acquisition based on voltage thresholding.</p>
      <p>The waveform generator module (signal.py) allows the addition of digital outputs necessary for synchronizing image acquisition to other types of behavioral software. It is possible to add separate digital outputs for the start of the volumetric scan or individual image acquisitions as described above by adding binary arrays to the digital output channels.</p>
    </sec>
    <sec id="s3-6">
      <title>Automatic Alignment of Excitation Beams to Image Focal Planes</title>
      <p>The Python software ecosystem offers a variety of highly optimized libraries for image processing and machine learning. Such distinct advantages of using Python, as well as the modular architecture of PyZebrascope, enable the implementation of advanced algorithms for microscope control and online image analyses. As a proof of concept to demonstrate this advantage, we developed an autofocusing module (auto_focusing.py) that adjusts the axial position of the excitation beam in the sample to the best focus of the detection objective (<xref rid="F4" ref-type="fig">Figure 4A</xref>). This procedure is usually a time-consuming manual step during the preparation of whole-brain volumetric imaging. The experimenter needs to align the position of the excitation beam from the superficial part to the deep part of the brain. This alignment is necessary for both the front and side excitation beams. In addition, images at the bottom part of the brain are typically blurry due to the diffraction of fluorescence through the sample, which makes the alignment indecisive for the experimenter.</p>
      <fig position="float" id="F4">
        <label>FIGURE 4</label>
        <caption>
          <p>Automatic alignment of excitation beams to the image focal plane. <bold>(A)</bold> Schematic of the alignment between the excitation beams and the detection objective based on acquired fluorescent images in the brain. <bold>(B)</bold> Calculation of focus measures. We apply a Fourier transform to the image, followed by a polar transform to the resulting power spectrum and a 1D projection along the angular axis. Then we calculate the width of the power spectrum above the threshold as a focus measure. Images with the finest resolution yield a higher focus measure. <bold>(C)</bold> Detection of the optimal beam position for the dorsal part of the brain. Normalized focus measures at different beam positions (circles), Gaussian fit (orange line), and the best focus (red dashed line) are plotted. Sample images from different beam positions are shown on the top. Scale bar, 10 μm. <bold>(D)</bold> Same plots as <bold>(C)</bold> for the ventral part of the brain. Scale bar, 10 μm. <bold>(E)</bold> Top, estimation of the nonlinear function between the position of the detection objective (DO) and analog output voltage for the axial scanning of the excitation beams during a volumetric scan. The estimated function (solid lines) may show nonlinearity compared to linear estimation based on the start and end position of the detection objective (dotted lines). <italic>Bottom</italic>, we estimated nonlinear functions between the positions of the detection objective (DO) and analog output voltage for axial beam scanning in the brain of a zebrafish. We determined optimal voltage output at five positions of the detection objective during a volumetric scan of the front beam (cyan) and side beam (magenta) independently and estimated transformation functions by cubic interpolation. The resulting functions (solid lines) showed nonlinearity compared to linear estimations (dashed lines). <italic>Bottom inset</italic>, the difference between the optimal analog output and the linear estimation was as large as 0.043V, which amounts to 7.5 μm of the axial position of the side beam, at the middle plane of a volumetric scan.</p>
        </caption>
        <graphic xlink:href="fcell-10-875044-g004" position="float"/>
      </fig>
      <p>We implemented a novel automatic algorithm to assist such a time-consuming alignment process by using an image resolution measure based on Fourier transformation (<xref rid="B27" ref-type="bibr">Mizutani et al., 2016</xref>) (<xref rid="F4" ref-type="fig">Figure 4B</xref>). Optimal beam alignment yields a higher resolution of fluorescent images, which results in higher powers in the high-frequency domain in images. This algorithm first Fourier-transforms the fluorescent image to obtain its 2D power spectrum. A polar transform is applied to the 2D power spectrum and is projected onto 1D by averaging along the angular dimension. We then use the logarithm of the obtained 1D projected array to quantify image resolution. The threshold for quantifying image resolution is defined as the sum of the mean and three times the standard deviation of the baseline part of the power spectrum. We then count the number of points above the set threshold and use this number as an image resolution measure (<xref rid="F4" ref-type="fig">Figure 4B</xref>). We implemented this algorithm using a library for GPU computing (CuPy, <ext-link xlink:href="https://cupy.dev/" ext-link-type="uri">https://cupy.dev/</ext-link>) to minimize computation time for Fourier transform and polar transform.</p>
      <p>Our auto-focusing module searches for the best focus by acquiring images at different axial beam positions (41 planes at an interval of 1 μm) for a given position of the detection objective (DO). It calculates the above resolution measure for each acquired plane and normalizes resolution measures to between 0 (poor resolution) and 1 (best resolution) across different planes. The peak position is detected by fitting a Gaussian distribution function to the normalized resolution measures. The center of the estimated distribution was designated as the best focal plane. These sampling and computing processes take less than 2 s in total and accurately detect the best focus for the dorsal (<xref rid="F4" ref-type="fig">Figure 4C</xref>) and ventral (<xref rid="F4" ref-type="fig">Figure 4D</xref>) part of the fish brain. We further applied this technique to estimate the nonlinear function between the position of the detection objective (DO) and voltage commands for axial positioning of the excitation beams during a volumetric scan (<xref rid="F4" ref-type="fig">Figure 4E</xref>). Such nonlinearity occurs because the angle of the scanning galvanometer is not necessarily linear to the axial position of the excitation beam. Our auto-focusing module automatically finds the best axial positions of the excitation beams, independently for the side and front beams, for five different DO positions along the <italic>Z</italic>-axis of a volumetric scan. It then estimates the optimal transformation function between the DO movements and scanning voltage for the excitation beams. Transformation functions obtained in a real zebrafish brain showed nonlinearity (<xref rid="F4" ref-type="fig">Figure 4E</xref>), and the optimal beam position at the middle of a volumetric scan differed from the linear estimation by 7.5 μm, a large enough distance to affect image resolution significantly (<xref rid="F4" ref-type="fig">Figures 4C,D</xref>). This estimated nonlinear function is passed to the waveform generator module (signal.py) as <italic>interp1d</italic> function of Scipy package (<xref rid="B39" ref-type="bibr">Virtanen, 2020</xref>) to transform the analog output waveform. Thus, this algorithm allows us to obtain accurate alignment between the objective and the excitation beam beyond manual, linear adjustments based on the start and end position of the volumetric acquisition.</p>
    </sec>
    <sec id="s3-7">
      <title>Whole-Brain Imaging in Behaving Zebrafish</title>
      <p>We tested whether PyZebrascope can perform whole-brain imaging in zebrafish while recording its behavior during a behavioral task (<xref rid="F5" ref-type="fig">Figure 5A</xref>). We used a 6-day old transgenic zebrafish that pan-neuronally express nuclear-localized, genetically-encoded calcium indicators (<xref rid="B13" ref-type="bibr">Dana et al., 2019</xref>) (HuC:H2B-GCaMP7f) and co-express red fluorescent proteins in <italic>tph2+</italic> serotonergic neurons (<xref rid="s11" ref-type="sec">Supplementary Figure S5A</xref>). The tested fish was able to react to the visual stimuli presented from below (<xref rid="F5" ref-type="fig">Figure 5B</xref>) and performed the motor learning task described in our previous work (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>). We were able to acquire continuous volumetric scans (45 planes at 1 Hz) across the entire brain (<xref rid="F5" ref-type="fig">Figure 5A</xref>), from the extremities of the dorsal to the ventral regions (cerebellum and hypothalamus, respectively) and from extremities of the rostral to the caudal regions (forebrain and area postrema, respectively).</p>
      <fig position="float" id="F5">
        <label>FIGURE 5</label>
        <caption>
          <p>PyZebrascope enables stable recording of neural activity at a brain-wide scale in behaving zebrafish. <bold>(A)</bold> Scanning area of whole-brain imaging. The scan area covers the most dorsal part of the brain (cerebellum = CB, habenula = HB), the most ventral part of the brain (hypothalamus = HT), the most rostral part of the brain (forebrain = FB), and the most caudal part of the brain (Area postrema = AP, inferior olive = IO). <bold>(B)</bold> Simultaneous recording of swimming events (black), the visual velocity of the environment (red), and neural activity traces (green) of representative neurons from brain areas designated in <bold>(A)</bold> during a motor learning task. Three neurons are selected from three groups that show distinct activity patterns from each area based on k-means clustering methods. Scale bars on the right represent 100% change in ΔF/F. Scale bar for visual velocity represents 2 mm/s, and traces below dotted lines represent backward motions of the environment triggered by swimming. <bold>(C)</bold> Whole-brain spatial map of neural activity clusters classified based on non-negative matrix factorization (n = 4) of 36,168 neurons. Detailed descriptions are in the main text and method section.</p>
        </caption>
        <graphic xlink:href="fcell-10-875044-g005" position="float"/>
      </fig>
      <p>We analyzed the acquired data by using image registration and segmentation algorithms developed in our previous work (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>). We identified ∼80,000 neurons across the brain, and their neural activity patterns remained stable for the entire duration of the experiment from the above areas of the brain (<xref rid="F5" ref-type="fig">Figure 5B</xref>). These results demonstrate PyZebrascope’s robustness for continuously acquiring large volumetric data over time. We tested the reliability of the data acquired by PyZebrascope by examining whether it is possible to identify behavior-related neural ensembles identified in other studies. We applied non-negative matrix factorization (4 set components) to the neural activity of all neurons across the entire brain and mapped the spatial locations of neurons that have significant weights for the identified components (<xref rid="F5" ref-type="fig">Figure 5C</xref>). We were able to identify a swim-related network in the midbrain and the hindbrain (<xref rid="B30" ref-type="bibr">Orger et al., 2008</xref>; <xref rid="B40" ref-type="bibr">Vladimirov et al., 2014</xref>) (Component 1), a network in the hindbrain that biases the fish’s swimming to the left side and to the right side (<xref rid="B4" ref-type="bibr">Ahrens et al., 2013</xref>; <xref rid="B15" ref-type="bibr">Dunn et al., 2016</xref>) (Component 2, 3), and neurons in the optic tectum, the dorsal raphe and the thalamus that respond to visual feedback during motor learning task (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>) (Component 4).</p>
      <p>Additionally, we analyzed the activity patterns of <italic>tph2+</italic> serotonergic neurons in the dorsal raphe nucleus that mediate motor learning effect in this behavioral task (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>). We identified 55 RFP+ neurons in the DRN whose task-dependent activity modulation was consistent across multiple sessions (see Methods). Their activity patterns show slow integration during a training period when the fish learns weak swim patterns. Their activity then slowly decay during the delay period of no swimming (<xref rid="s11" ref-type="sec">Supplementary Figure S5B</xref>). These patterns are consistent with the finding of our previous work (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>). These results demonstrate that our open-source PyZebrascope allows us to perform whole-brain neural activity imaging in behaving zebrafish in a quality comparable with the pioneering studies that relied on commercially developed software.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="s4">
    <title>Discussion</title>
    <p>Here we described the development of the open-source Python software, PyZebrascope, that controls a light-sheet microscope designed for neural activity imaging experiments in zebrafish. Its intuitive graphical user interfaces and ease of managing complex device parameters allow the users to minimize efforts in setting up consistent whole-brain imaging experiments across experiments. Its multi-thread structure for handling imaging data achieved a high throughput of over 800 MB/s of camera acquisition and file writing while maintaining stable usage of CPU and memory resources. The choice of Python as a programming language allowed us to implement an advanced microscope control algorithm, such as automatic focusing, which further reduces the tedious efforts for ensuring the quality of multi-beam volumetric scans. These features allowed us to perform whole-brain imaging in behaving zebrafish at a significantly larger scale than the first demonstration of open-source software for light-sheet microscopy in zebrafish (<xref rid="B36" ref-type="bibr">Saska et al., 2021</xref>). Its data quality reached at least a comparable level to those demonstrated by commercially developed software (<xref rid="B40" ref-type="bibr">Vladimirov et al., 2014</xref>; <xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>).</p>
    <p>PyZebrascope and its components are written in a generalizable manner, enabling research teams that use different types of devices to adapt the code to their configurations. It is available from Github (<ext-link xlink:href="https://github.com/KawashimaLab/PyZebraScope_public" ext-link-type="uri">https://github.com/KawashimaLab/PyZebraScope_public</ext-link>). It only requires pre-installation of Anaconda Python package, μManager package (<xref rid="B16" ref-type="bibr">Edelstein et al., 2014</xref>) with a matching version of its Python interface (pymmcore), a Python library for controlling data acquisition board (ni-daqmx), a Python library for GPU computing (CuPy), and a few other Python packages, all free of cost. Therefore, we expect that PyZebrascope will help disseminate the whole-brain imaging technique throughout the zebrafish neuroscience community.</p>
    <p>The architecture of PyZebrascope further enables the implementation of advanced microscope control and image processing algorithms. For example, a common issue during whole-brain imaging in zebrafish is the sample’s translational and rotational drift resulting from gravity force, tail motions of unparalyzed fish (<xref rid="B24" ref-type="bibr">Markov et al., 2021</xref>), or the pressure of pipette attachment during fictive recording in paralyzed fish (<xref rid="B3" ref-type="bibr">Ahrens et al., 2012</xref>; <xref rid="B40" ref-type="bibr">Vladimirov et al., 2014</xref>; <xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>). A small amount of drift, especially in the axial direction along which the volumetric scan is under-sampled, can result in the loss of neurons during imaging because the neuronal diameter in the zebrafish brain is usually less than 5 μm. Our previous work detected such drifts during time-consuming post processing (<xref rid="B21" ref-type="bibr">Kawashima et al., 2016</xref>). Given that sample drift usually occurs at a slow rate, it will be possible to monitor it in real-time during the experiment by occasionally sampling the scanned volume. It will also be possible to further compensate for translational drifts by moving the 3-axis stage holding the sample. The effectiveness of such online drift correction is demonstrated in a multi-beam light-sheet microscope (<xref rid="B35" ref-type="bibr">Royer et al., 2015</xref>). Therefore, implementing such algorithms likely to increase the throughput and the success rate of whole-brain imaging experiments in zebrafish.</p>
    <p>It will also be possible to implement real-time image processing features to identify neurons of specific activity patterns for subsequent neural perturbation experiments at a single-cell resolution (<xref rid="B12" ref-type="bibr">dal Maschio et al., 2017</xref>; <xref rid="B41" ref-type="bibr">Vladimirov et al., 2018</xref>). Real-time registration and segmentation of individual neural activity across the brain will require large computing resources and may not be feasible on the microscope control computer. Nonetheless, it will be possible to calculate, for example, trial-averaged neural activation maps based on a simple recurrent formula if the behavioral events or sensory stimulus occur at regular volumetric scan intervals. Once we identify neurons that show specific activity patterns, we can further laser-ablate or photo-stimulate these populations using an open-source Python resource for holographic two-photon stimulation (<xref rid="B12" ref-type="bibr">dal Maschio et al., 2017</xref>; <xref rid="B34" ref-type="bibr">Pozzi and Mapelli, 2021</xref>). Such experiments will allow us to investigate the functional roles of neurons that show specific activity patterns beyond what is obtainable by genetic labeling of neurons.</p>
    <p>Lastly, PyZebrascope will enable further development of voltage imaging techniques in zebrafish. Voltage imaging requires high image resolution, high photon collection efficiency, and high imaging speed at the rate of at least 300 frames per second (<xref rid="B10" ref-type="bibr">Cohen and Venkatachalam, 2014</xref>; <xref rid="B1" ref-type="bibr">Abdelfattah et al., 2019</xref>; <xref rid="B5" ref-type="bibr">Böhm et al., 2022</xref>). Light-sheet microscopy has the potential of realizing these conditions at a brain-wide scale in zebrafish. For example, it will be possible to implement a custom module that performs single-plane high-speed imaging in a sequential manner across different depth while the zebrafish perform a simple, stereotypical sensorimotor task as demonstrated for calcium imaging in zebrafish (<xref rid="B3" ref-type="bibr">Ahrens et al., 2012</xref>). Implementing such a capability will advance our understanding of how whole-brain neural dynamics control animal behaviors at a millisecond timescale.</p>
  </sec>
</body>
<back>
  <ack>
    <p>We thank Dorel Malamud and other members of Kawashima laboratories for experimental help, members of Veterinary Resource at Weizmann Institute of Science for animal care, Meir Alon, Haim Sade, Alex Jahanfard and members of Physics Core Facility at Weizmann Institute of Science for machining and assembling custom microscope parts, and Uri Rosen for the maintenance of our computing server in the High Performance Computing unit. We thank Misha Ahrens, Igor Negrashov, Davis Bennet, Raghav Chhetri, Nikita Vladimirov and Steven Sawtelle (HHMI Janelia) for their advice on our initial design of the microscope components and PyZebrascope, and Niels Cautaerts (Data Minded) for his advice on GPU-based codes. We thank Inbal Shainer for her critical reading of the manuscript.</p>
  </ack>
  <sec sec-type="data-availability" id="s5">
    <title>Data Availability Statement</title>
    <p>The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation.</p>
  </sec>
  <sec id="s6">
    <title>Ethics Statement</title>
    <p>The animal study was reviewed and approved by Institutional Animal Care and Use Committee (IACUC) and the Institutional Biosafety Committee (IBC) of the Weizmann Institute of Science.</p>
  </sec>
  <sec id="s7">
    <title>Author Contributions</title>
    <p>TK and RB conceived the project. RB established the device control and first versions of PyZebrascope. TK designed the custom light-sheet microscope, organized the later versions of PyZebrascope and conducted data analyses. RH provided initial user feedback for refining PyZebrascope and collected whole-brain imaging data in behaving zebrafish. MK, and KH developed algorithms for automatic image focusing and contributed a figure. TK wrote the manuscript with inputs from all authors.</p>
  </sec>
  <sec id="s8">
    <title>Funding</title>
    <p>MNK is supported by iNAMES MDC-Weizmann Research School, “Imaging from NAno to MESo.” This research is supported by Azrieli faculty fellowship (TK), Dan Lebas Ruth Sonnewend (TK), Birnbach Family foundation (TK), Internal grant from the Center for New Scientists at Weizmann Institute of Science (TK), Helmholtz Imaging (KH), and an Oak Ridge National Laboratory, Laboratory Directed Research and Development Strategic Hire grant (KH).</p>
  </sec>
  <sec sec-type="COI-statement" id="s9">
    <title>Conflict of Interest</title>
    <p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
  </sec>
  <sec sec-type="disclaimer" id="s10">
    <title>Publisher’s Note</title>
    <p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p>
  </sec>
  <sec id="s11">
    <title>Supplementary Material</title>
    <p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/fcell.2022.875044/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/fcell.2022.875044/full#supplementary-material</ext-link>
</p>
    <supplementary-material id="SM1" position="float" content-type="local-data">
      <media xlink:href="DataSheet1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdelfattah</surname><given-names>A. S.</given-names></name><name><surname>Kawashima</surname><given-names>T.</given-names></name><name><surname>Singh</surname><given-names>A.</given-names></name><name><surname>Novak</surname><given-names>O.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Shuai</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Bright and Photostable Chemigenetic Indicators for Extended <italic>In Vivo</italic> Voltage Imaging</article-title>. <source>Science</source>
<volume>365</volume>, <fpage>699</fpage>–<lpage>704</lpage>. <pub-id pub-id-type="doi">10.1126/science.aav6416</pub-id>
<pub-id pub-id-type="pmid">31371562</pub-id></mixed-citation>
    </ref>
    <ref id="B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname><given-names>M. B.</given-names></name><name><surname>Engert</surname><given-names>F.</given-names></name></person-group> (<year>2015</year>). <article-title>Large-Scale Imaging in Small Brains</article-title>. <source>Curr. Opin. Neurobiol.</source>
<volume>32</volume>, <fpage>78</fpage>–<lpage>86</lpage>. <pub-id pub-id-type="doi">10.1016/j.conb.2015.01.007</pub-id>
<pub-id pub-id-type="pmid">25636154</pub-id></mixed-citation>
    </ref>
    <ref id="B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname><given-names>M. B.</given-names></name><name><surname>Li</surname><given-names>J. M.</given-names></name><name><surname>Orger</surname><given-names>M. B.</given-names></name><name><surname>Robson</surname><given-names>D. N.</given-names></name><name><surname>Schier</surname><given-names>A. F.</given-names></name><name><surname>Engert</surname><given-names>F.</given-names></name><etal/></person-group> (<year>2012</year>). <article-title>Brain-Wide Neuronal Dynamics During Motor Adaptation in Zebrafish</article-title>. <source>Nature</source>
<volume>485</volume>, <fpage>471</fpage>–<lpage>477</lpage>. <pub-id pub-id-type="doi">10.1038/nature11057</pub-id>
<pub-id pub-id-type="pmid">22622571</pub-id></mixed-citation>
    </ref>
    <ref id="B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrens</surname><given-names>M. B.</given-names></name><name><surname>Orger</surname><given-names>M. B.</given-names></name><name><surname>Robson</surname><given-names>D. N.</given-names></name><name><surname>Li</surname><given-names>J. M.</given-names></name><name><surname>Keller</surname><given-names>P. J.</given-names></name></person-group> (<year>2013</year>). <article-title>Whole-Brain Functional Imaging at Cellular Resolution Using Light-Sheet Microscopy</article-title>. <source>Nat. Methods</source>
<volume>10</volume>, <fpage>413</fpage>–<lpage>420</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.2434</pub-id>
<pub-id pub-id-type="pmid">23524393</pub-id></mixed-citation>
    </ref>
    <ref id="B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Böhm</surname><given-names>U. L.</given-names></name><name><surname>Kimura</surname><given-names>Y.</given-names></name><name><surname>Kawashima</surname><given-names>T.</given-names></name><name><surname>Ahrens</surname><given-names>M. B.</given-names></name><name><surname>Higashijima</surname><given-names>S. I.</given-names></name><name><surname>Engert</surname><given-names>F.</given-names></name></person-group> (<year>2022</year>). <article-title>Voltage Imaging Identifies Spinal Circuits That Modulate Locomotor Adaptation in Zebrafish</article-title>. <source>Neuron</source>
<volume>110</volume>, <fpage>1211</fpage>–<lpage>1222</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2022.01.001</pub-id>
<pub-id pub-id-type="pmid">35104451</pub-id></mixed-citation>
    </ref>
    <ref id="B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruzzone</surname><given-names>M.</given-names></name></person-group> (<year>2021</year>). <article-title>Whole Brain Functional Recordings at Cellular Resolution in Zebrafish Larvae with 3D Scanning Multiphoton Microscopy</article-title>. <source>Sci. Rep.</source>
<volume>111</volume> (<issue>11</issue>), <fpage>1</fpage>–<lpage>11</lpage>. <pub-id pub-id-type="doi">10.1038/s41598-021-90335-y</pub-id>
</mixed-citation>
    </ref>
    <ref id="B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgstaller</surname><given-names>J.</given-names></name><name><surname>Hindinger</surname><given-names>E.</given-names></name><name><surname>Donovan</surname><given-names>J.</given-names></name><name><surname>Dal Maschio</surname><given-names>M.</given-names></name><name><surname>Kist</surname><given-names>A. M.</given-names></name><name><surname>Gesierich</surname><given-names>B.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Light-Sheet Imaging and Graph Analysis of Antidepressant Action in the Larval Zebrafish Brain Network</article-title>. <source>bioRxiv</source>, <fpage>618843</fpage>. <pub-id pub-id-type="doi">10.1101/618843</pub-id>
</mixed-citation>
    </ref>
    <ref id="B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T.-W.</given-names></name><name><surname>Wardill</surname><given-names>T. J.</given-names></name><name><surname>Sun</surname><given-names>Y.</given-names></name><name><surname>Pulver</surname><given-names>S. R.</given-names></name><name><surname>Renninger</surname><given-names>S. L.</given-names></name><name><surname>Baohan</surname><given-names>A.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Ultrasensitive Fluorescent Proteins for Imaging Neuronal Activity</article-title>. <source>Nature</source>
<volume>499</volume>, <fpage>295</fpage>–<lpage>300</lpage>. <pub-id pub-id-type="doi">10.1038/nature12354</pub-id>
<pub-id pub-id-type="pmid">23868258</pub-id></mixed-citation>
    </ref>
    <ref id="B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X.</given-names></name><name><surname>Mu</surname><given-names>Y.</given-names></name><name><surname>Hu</surname><given-names>Y.</given-names></name><name><surname>Kuan</surname><given-names>A. T.</given-names></name><name><surname>Nikitchenko</surname><given-names>M.</given-names></name><name><surname>Randlett</surname><given-names>O.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Brain-wide Organization of Neuronal Activity and Convergent Sensorimotor Transformations in Larval Zebrafish</article-title>. <source>Neuron</source>
<volume>100</volume>, <fpage>876e5</fpage>–<lpage>890</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.042</pub-id>
<pub-id pub-id-type="pmid">30473013</pub-id></mixed-citation>
    </ref>
    <ref id="B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>A. E.</given-names></name><name><surname>Venkatachalam</surname><given-names>V.</given-names></name></person-group> (<year>2014</year>). <article-title>Bringing Bioelectricity to Light</article-title>. <source>Annu. Rev. Biophys.</source>
<volume>43</volume>, <fpage>211</fpage>–<lpage>232</lpage>. <pub-id pub-id-type="doi">10.1146/annurev-biophys-051013-022717</pub-id>
<pub-id pub-id-type="pmid">24773017</pub-id></mixed-citation>
    </ref>
    <ref id="B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cong</surname><given-names>L.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Chai</surname><given-names>Y.</given-names></name><name><surname>Hang</surname><given-names>W.</given-names></name><name><surname>Shang</surname><given-names>C.</given-names></name><name><surname>Yang</surname><given-names>W.</given-names></name><etal/></person-group> (<year>2017</year>). <article-title>Rapid Whole Brain Imaging of Neural Activity in Freely Behaving Larval Zebrafish (<italic>Danio rerio</italic>)</article-title>. <source>Elife</source>
<volume>6</volume>, <fpage>e28158</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.28158</pub-id>
<pub-id pub-id-type="pmid">28930070</pub-id></mixed-citation>
    </ref>
    <ref id="B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>dal Maschio</surname><given-names>M.</given-names></name><name><surname>Donovan</surname><given-names>J. C.</given-names></name><name><surname>Helmbrecht</surname><given-names>T. O.</given-names></name><name><surname>Baier</surname><given-names>H.</given-names></name></person-group> (<year>2017</year>). <article-title>Linking Neurons to Network Function and Behavior by Two-Photon Holographic Optogenetics and Volumetric Imaging</article-title>. <source>Neuron</source>
<volume>94</volume>, <fpage>774e5</fpage>–<lpage>789</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.034</pub-id>
<pub-id pub-id-type="pmid">28521132</pub-id></mixed-citation>
    </ref>
    <ref id="B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dana</surname><given-names>H.</given-names></name><name><surname>Sun</surname><given-names>Y.</given-names></name><name><surname>Mohar</surname><given-names>B.</given-names></name><name><surname>Hulse</surname><given-names>B. K.</given-names></name><name><surname>Kerlin</surname><given-names>A. M.</given-names></name><name><surname>Hasseman</surname><given-names>J. P.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>High-Performance Calcium Sensors for Imaging Activity in Neuronal Populations and Microcompartments</article-title>. <source>Nat. Methods</source>
<volume>16</volume>, <fpage>649</fpage>–<lpage>657</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0435-6</pub-id>
<pub-id pub-id-type="pmid">31209382</pub-id></mixed-citation>
    </ref>
    <ref id="B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demas</surname><given-names>J.</given-names></name><name><surname>Manley</surname><given-names>J.</given-names></name><name><surname>Tejera</surname><given-names>F.</given-names></name><name><surname>Barber</surname><given-names>K.</given-names></name><name><surname>Kim</surname><given-names>H.</given-names></name><name><surname>Traub</surname><given-names>F. M.</given-names></name><etal/></person-group> (<year>2021</year>). <article-title>High-speed, Cortex-wide Volumetric Recording of Neuroactivity at Cellular Resolution Using Light Beads Microscopy</article-title>. <source>Nat. Methods</source>
<volume>18</volume>, <fpage>1103</fpage>–<lpage>1111</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-021-01239-8</pub-id>
<pub-id pub-id-type="pmid">34462592</pub-id></mixed-citation>
    </ref>
    <ref id="B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>T. W.</given-names></name><name><surname>Gebhardt</surname><given-names>C.</given-names></name><name><surname>Naumann</surname><given-names>E. A.</given-names></name><name><surname>Riegler</surname><given-names>C.</given-names></name><name><surname>Ahrens</surname><given-names>M. B.</given-names></name><name><surname>Engert</surname><given-names>F.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>Neural Circuits Underlying Visually Evoked Escapes in Larval Zebrafish</article-title>. <source>Neuron</source>
<volume>89</volume>, <fpage>613</fpage>–<lpage>628</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.021</pub-id>
<pub-id pub-id-type="pmid">26804997</pub-id></mixed-citation>
    </ref>
    <ref id="B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelstein</surname><given-names>A. D.</given-names></name><name><surname>Tsuchida</surname><given-names>M. A.</given-names></name><name><surname>Amodaj</surname><given-names>N.</given-names></name><name><surname>Pinkard</surname><given-names>H.</given-names></name><name><surname>Vale</surname><given-names>R. D.</given-names></name><name><surname>Stuurman</surname><given-names>N.</given-names></name></person-group> (<year>2014</year>). <article-title>Advanced Methods of Microscope Control Using μManager Software</article-title>. <source>J. Biol. Methods</source>
<volume>1</volume>, <fpage>10</fpage>. <pub-id pub-id-type="doi">10.14440/jbm.2014.36</pub-id>
</mixed-citation>
    </ref>
    <ref id="B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Favre-Bulle</surname><given-names>I. A.</given-names></name><name><surname>Vanwalleghem</surname><given-names>G.</given-names></name><name><surname>Taylor</surname><given-names>M. A.</given-names></name><name><surname>Rubinsztein-Dunlop</surname><given-names>H.</given-names></name><name><surname>Scott</surname><given-names>E. K.</given-names></name></person-group> (<year>2018</year>). <article-title>Cellular-Resolution Imaging of Vestibular Processing Across the Larval Zebrafish Brain</article-title>. <source>Curr. Biol.</source>
<volume>28</volume>, <fpage>3711</fpage>–<lpage>3722</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2018.09.060</pub-id>
<pub-id pub-id-type="pmid">30449665</pub-id></mixed-citation>
    </ref>
    <ref id="B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>J.</given-names></name><name><surname>Vladimirov</surname><given-names>N.</given-names></name><name><surname>Kawashima</surname><given-names>T.</given-names></name><name><surname>Mu</surname><given-names>Y.</given-names></name><name><surname>Sofroniew</surname><given-names>N. J.</given-names></name><name><surname>Bennett</surname><given-names>D. V.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Mapping Brain Activity at Scale with Cluster Computing</article-title>. <source>Nat. Methods</source>
<volume>11</volume>, <fpage>941</fpage>–<lpage>950</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3041</pub-id>
<pub-id pub-id-type="pmid">25068736</pub-id></mixed-citation>
    </ref>
    <ref id="B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>C. R.</given-names></name><name><surname>Millman</surname><given-names>K. J.</given-names></name><name><surname>van der Walt</surname><given-names>S. J.</given-names></name><name><surname>Gommers</surname><given-names>R.</given-names></name><name><surname>Virtanen</surname><given-names>P.</given-names></name><name><surname>Cournapeau</surname><given-names>D.</given-names></name><etal/></person-group> (<year>2020</year>). <article-title>Array Programming with NumPy</article-title>. <source>Nature</source>
<volume>585</volume>, <fpage>357</fpage>–<lpage>362</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
<pub-id pub-id-type="pmid">32939066</pub-id></mixed-citation>
    </ref>
    <ref id="B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inoue</surname><given-names>M.</given-names></name><name><surname>Takeuchi</surname><given-names>A.</given-names></name><name><surname>Manita</surname><given-names>S.</given-names></name><name><surname>Horigane</surname><given-names>S.-i.</given-names></name><name><surname>Sakamoto</surname><given-names>M.</given-names></name><name><surname>Kawakami</surname><given-names>R.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Rational Engineering of XCaMPs, a Multicolor GECI Suite for <italic>In Vivo</italic> Imaging of Complex Brain Circuit Dynamics</article-title>. <source>Cell</source>
<volume>177</volume>, <fpage>1346e24</fpage>–<lpage>1360</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2019.04.007</pub-id>
<pub-id pub-id-type="pmid">31080068</pub-id></mixed-citation>
    </ref>
    <ref id="B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawashima</surname><given-names>T.</given-names></name><name><surname>Zwart</surname><given-names>M. F.</given-names></name><name><surname>Yang</surname><given-names>C.-T.</given-names></name><name><surname>Mensh</surname><given-names>B. D.</given-names></name><name><surname>Ahrens</surname><given-names>M. B.</given-names></name></person-group> (<year>2016</year>). <article-title>The Serotonergic System Tracks the Outcomes of Actions to Mediate Short-Term Motor Learning</article-title>. <source>Cell</source>
<volume>167</volume>, <fpage>933</fpage>–<lpage>946</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2016.09.055</pub-id>
<pub-id pub-id-type="pmid">27881303</pub-id></mixed-citation>
    </ref>
    <ref id="B22">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>P. J.</given-names></name><name><surname>Schmidt</surname><given-names>A. D.</given-names></name><name><surname>Wittbrodt</surname><given-names>J.</given-names></name><name><surname>Stelzer</surname><given-names>E. H. K.</given-names></name></person-group> (<year>2008</year>). <article-title>Reconstruction of Zebrafish Early Embryonic Development by Scanned Light Sheet Microscopy</article-title>. <source>Science</source>
<volume>322</volume>, <fpage>1065</fpage>–<lpage>1069</lpage>. <pub-id pub-id-type="doi">10.1126/science.1162493</pub-id>
<pub-id pub-id-type="pmid">18845710</pub-id></mixed-citation>
    </ref>
    <ref id="B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mancienne</surname><given-names>T.</given-names></name></person-group> (<year>2021</year>). <article-title>Contributions of Luminance and Motion to Visual Escape and Habituation in Larval Zebrafish</article-title>. <source>Front. Neural Circuits</source>
<volume>15</volume>, <fpage>115</fpage>. <pub-id pub-id-type="doi">10.3389/fncir.2021.748535</pub-id>
</mixed-citation>
    </ref>
    <ref id="B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markov</surname><given-names>D. A.</given-names></name><name><surname>Petrucco</surname><given-names>L.</given-names></name><name><surname>Kist</surname><given-names>A. M.</given-names></name><name><surname>Portugues</surname><given-names>R.</given-names></name></person-group> (<year>2021</year>). <article-title>A Cerebellar Internal Model Calibrates a Feedback Controller Involved in Sensorimotor Control</article-title>. <source>Nat. Commun.</source>
<volume>12</volume>, <fpage>1</fpage>–<lpage>21</lpage>. <pub-id pub-id-type="doi">10.1038/s41467-021-26988-0</pub-id>
<pub-id pub-id-type="pmid">33397941</pub-id></mixed-citation>
    </ref>
    <ref id="B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marques</surname><given-names>J. C.</given-names></name><name><surname>Li</surname><given-names>M.</given-names></name><name><surname>Schaak</surname><given-names>D.</given-names></name><name><surname>Robson</surname><given-names>D. N.</given-names></name><name><surname>Li</surname><given-names>J. M.</given-names></name></person-group> (<year>2020</year>). <article-title>Internal State Dynamics Shape Brainwide Activity and Foraging Behaviour</article-title>. <source>Nature</source>
<volume>577</volume>, <fpage>239</fpage>–<lpage>243</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-019-1858-z</pub-id>
<pub-id pub-id-type="pmid">31853063</pub-id></mixed-citation>
    </ref>
    <ref id="B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Migault</surname><given-names>G.</given-names></name><name><surname>van der Plas</surname><given-names>T. L.</given-names></name><name><surname>Trentesaux</surname><given-names>H.</given-names></name><name><surname>Panier</surname><given-names>T.</given-names></name><name><surname>Candelier</surname><given-names>R.</given-names></name><name><surname>Proville</surname><given-names>R.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Whole-Brain Calcium Imaging During Physiological Vestibular Stimulation in Larval Zebrafish</article-title>. <source>Curr. Biol.</source>
<volume>28</volume>, <fpage>3723</fpage>–<lpage>3735</lpage>. <pub-id pub-id-type="doi">10.1016/j.cub.2018.10.017</pub-id>
<pub-id pub-id-type="pmid">30449666</pub-id></mixed-citation>
    </ref>
    <ref id="B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mizutani</surname><given-names>R.</given-names></name><name><surname>Saiga</surname><given-names>R.</given-names></name><name><surname>Takekoshi</surname><given-names>S.</given-names></name><name><surname>Inomoto</surname><given-names>C.</given-names></name><name><surname>Nakamura</surname><given-names>N.</given-names></name><name><surname>Itokawa</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2016</year>). <article-title>A Method for Estimating Spatial Resolution of Real Image in the Fourier Domain</article-title>. <source>J. Microsc.</source>
<volume>261</volume>, <fpage>57</fpage>–<lpage>66</lpage>. <pub-id pub-id-type="doi">10.1111/jmi.12315</pub-id>
</mixed-citation>
    </ref>
    <ref id="B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mu</surname><given-names>Y.</given-names></name><name><surname>Bennett</surname><given-names>D. V.</given-names></name><name><surname>Rubinov</surname><given-names>M.</given-names></name><name><surname>Narayan</surname><given-names>S.</given-names></name><name><surname>Yang</surname><given-names>C.-T.</given-names></name><name><surname>Tanimoto</surname><given-names>M.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Glia Accumulate Evidence that Actions Are Futile and Suppress Unsuccessful Behavior</article-title>. <source>Cell</source>
<volume>178</volume>, <fpage>27e19</fpage>–<lpage>43</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2019.05.050</pub-id>
<pub-id pub-id-type="pmid">31230713</pub-id></mixed-citation>
    </ref>
    <ref id="B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naumann</surname><given-names>E. A.</given-names></name><name><surname>Fitzgerald</surname><given-names>J. E.</given-names></name><name><surname>Dunn</surname><given-names>T. W.</given-names></name><name><surname>Rihel</surname><given-names>J.</given-names></name><name><surname>Sompolinsky</surname><given-names>H.</given-names></name><name><surname>Engert</surname><given-names>F.</given-names></name></person-group> (<year>2016</year>). <article-title>From Whole-Brain Data to Functional Circuit Models: The Zebrafish Optomotor Response</article-title>. <source>Cell</source>
<volume>167</volume>, <fpage>947</fpage>–<lpage>960</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2016.10.019</pub-id>
<pub-id pub-id-type="pmid">27814522</pub-id></mixed-citation>
    </ref>
    <ref id="B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orger</surname><given-names>M. B.</given-names></name><name><surname>Kampff</surname><given-names>A. R.</given-names></name><name><surname>Severi</surname><given-names>K. E.</given-names></name><name><surname>Bollmann</surname><given-names>J. H.</given-names></name><name><surname>Engert</surname><given-names>F.</given-names></name></person-group> (<year>2008</year>). <article-title>Control of Visually Guided Behavior by Distinct Populations of Spinal Projection Neurons</article-title>. <source>Nat. Neurosci.</source>
<volume>11</volume>, <fpage>327</fpage>–<lpage>333</lpage>. <pub-id pub-id-type="doi">10.1038/nn2048</pub-id>
<pub-id pub-id-type="pmid">18264094</pub-id></mixed-citation>
    </ref>
    <ref id="B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panier</surname><given-names>T.</given-names></name><name><surname>Romano</surname><given-names>S. A.</given-names></name><name><surname>Olive</surname><given-names>R.</given-names></name><name><surname>Pietri</surname><given-names>T.</given-names></name><name><surname>Sumbre</surname><given-names>G.</given-names></name><name><surname>Candelier</surname><given-names>R.</given-names></name><etal/></person-group> (<year>2013</year>). <article-title>Fast Functional Imaging of Multiple Brain Regions in Intact Zebrafish Larvae Using Selective Plane Illumination Microscopy</article-title>. <source>Front. Neural Circuits</source>
<volume>7</volume>, <fpage>65</fpage>. <pub-id pub-id-type="doi">10.3389/fncir.2013.00065</pub-id>
<pub-id pub-id-type="pmid">23576959</pub-id></mixed-citation>
    </ref>
    <ref id="B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pologruto</surname><given-names>T. A.</given-names></name><name><surname>Sabatini</surname><given-names>B. L.</given-names></name><name><surname>Svoboda</surname><given-names>K.</given-names></name></person-group> (<year>2003</year>). <article-title>ScanImage: Flexible Software for Operating Laser Scanning Microscopes</article-title>. <source>Biomed. Eng. Online</source>
<volume>2</volume>, <fpage>13</fpage>–<lpage>19</lpage>. <pub-id pub-id-type="doi">10.1186/1475-925X-2-13</pub-id>
<pub-id pub-id-type="pmid">12801419</pub-id></mixed-citation>
    </ref>
    <ref id="B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponce-Alvarez</surname><given-names>A.</given-names></name><name><surname>Jouary</surname><given-names>A.</given-names></name><name><surname>Privat</surname><given-names>M.</given-names></name><name><surname>Deco</surname><given-names>G.</given-names></name><name><surname>Sumbre</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>Whole-Brain Neuronal Activity Displays Crackling Noise Dynamics</article-title>. <source>Neuron</source>
<volume>100</volume>, <fpage>1446</fpage>–<lpage>e6</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.045</pub-id>
<pub-id pub-id-type="pmid">30449656</pub-id></mixed-citation>
    </ref>
    <ref id="B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pozzi</surname><given-names>P.</given-names></name><name><surname>Mapelli</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>). <article-title>Real Time Generation of Three Dimensional Patterns for Multiphoton Stimulation</article-title>. <source>Front. Cell. Neurosci.</source>
<volume>15</volume>, <fpage>34</fpage>. <pub-id pub-id-type="doi">10.3389/fncel.2021.609505</pub-id>
</mixed-citation>
    </ref>
    <ref id="B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Royer</surname><given-names>L. A.</given-names></name><name><surname>Weigert</surname><given-names>M.</given-names></name><name><surname>Günther</surname><given-names>U.</given-names></name><name><surname>Maghelli</surname><given-names>N.</given-names></name><name><surname>Jug</surname><given-names>F.</given-names></name><name><surname>Sbalzarini</surname><given-names>I. F.</given-names></name><etal/></person-group> (<year>2015</year>). <article-title>ClearVolume: Open-Source Live 3D Visualization for Light-Sheet Microscopy</article-title>. <source>Nat. Methods</source>
<volume>12</volume>, <fpage>480</fpage>–<lpage>481</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3372</pub-id>
<pub-id pub-id-type="pmid">26020498</pub-id></mixed-citation>
    </ref>
    <ref id="B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saska</surname><given-names>D.</given-names></name><name><surname>Pichler</surname><given-names>P.</given-names></name><name><surname>Qian</surname><given-names>C.</given-names></name><name><surname>Buckley</surname><given-names>C. L.</given-names></name><name><surname>LagnadoToolset</surname><given-names>L.</given-names></name></person-group> (<year>2021</year>). <article-title>μSPIM Toolset: A Software Platform for Selective Plane Illumination Microscopy</article-title>. <source>J. Neurosci. Methods</source>
<volume>347</volume>, <fpage>108952</fpage>. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2020.108952</pub-id>
<pub-id pub-id-type="pmid">33017646</pub-id></mixed-citation>
    </ref>
    <ref id="B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sofroniew</surname><given-names>N. J.</given-names></name><name><surname>Flickinger</surname><given-names>D.</given-names></name><name><surname>King</surname><given-names>J.</given-names></name><name><surname>Svoboda</surname><given-names>K.</given-names></name></person-group> (<year>2016</year>). <article-title>A Large Field of View Two-Photon Mesoscope with Subcellular Resolution for <italic>In Vivo</italic> Imaging</article-title>. <source>Elife</source>
<volume>5</volume>, <fpage>e14472</fpage>. <pub-id pub-id-type="doi">10.7554/eLife.14472</pub-id>
<pub-id pub-id-type="pmid">27300105</pub-id></mixed-citation>
    </ref>
    <ref id="B38">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Der Walt</surname><given-names>S.</given-names></name><name><surname>Schönberger</surname><given-names>J. L.</given-names></name><name><surname>Nunez-Iglesias</surname><given-names>J.</given-names></name><name><surname>Boulogne</surname><given-names>F.</given-names></name><name><surname>Warner</surname><given-names>J. D.</given-names></name><name><surname>Yager</surname><given-names>N.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Scikit-image: Image Processing in python</article-title>. <source>PeerJ</source>
<volume>2</volume>, <fpage>e453</fpage>. <pub-id pub-id-type="doi">10.7717/peerj.453</pub-id>
<pub-id pub-id-type="pmid">25024921</pub-id></mixed-citation>
    </ref>
    <ref id="B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P.</given-names></name></person-group> (<year>2020</year>). <article-title>SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</article-title>. <source>Nat. Methods</source>
<volume>17</volume>, <fpage>261</fpage>–<lpage>272</lpage>. <pub-id pub-id-type="pmid">32015543</pub-id></mixed-citation>
    </ref>
    <ref id="B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vladimirov</surname><given-names>N.</given-names></name><name><surname>Mu</surname><given-names>Y.</given-names></name><name><surname>Kawashima</surname><given-names>T.</given-names></name><name><surname>Bennett</surname><given-names>D. V.</given-names></name><name><surname>Yang</surname><given-names>C.-T.</given-names></name><name><surname>Looger</surname><given-names>L. L.</given-names></name><etal/></person-group> (<year>2014</year>). <article-title>Light-sheet Functional Imaging in Fictively Behaving Zebrafish</article-title>. <source>Nat. Methods</source>
<volume>11</volume>, <fpage>883</fpage>–<lpage>884</lpage>. <pub-id pub-id-type="doi">10.1038/nmeth.3040</pub-id>
<pub-id pub-id-type="pmid">25068735</pub-id></mixed-citation>
    </ref>
    <ref id="B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vladimirov</surname><given-names>N.</given-names></name><name><surname>Wang</surname><given-names>C.</given-names></name><name><surname>Höckendorf</surname><given-names>B.</given-names></name><name><surname>Pujala</surname><given-names>A.</given-names></name><name><surname>Tanimoto</surname><given-names>M.</given-names></name><name><surname>Mu</surname><given-names>Y.</given-names></name><etal/></person-group> (<year>2018</year>). <article-title>Brain-wide Circuit Interrogation at the Cellular Level Guided by Online Analysis of Neuronal Function</article-title>. <source>Nat. Methods</source>
<volume>15</volume>, <fpage>1117</fpage>–<lpage>1125</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-018-0221-x</pub-id>
<pub-id pub-id-type="pmid">30504888</pub-id></mixed-citation>
    </ref>
    <ref id="B42">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voigt</surname><given-names>F. F.</given-names></name><name><surname>Vladimirov</surname><given-names>N.</given-names></name><name><surname>Schulze</surname><given-names>C.</given-names></name><name><surname>Campbell</surname><given-names>R.</given-names></name><name><surname>Helmchen</surname><given-names>F.</given-names></name></person-group> (<year>2022</year>). <article-title>MesoSPIM Control: An Open-Source Acquisition Software for Light-Sheet Microscopy Written in Python and Qt</article-title>. <source>Zenodo</source>. <pub-id pub-id-type="doi">10.5281/ZENODO.6109315</pub-id>
</mixed-citation>
    </ref>
    <ref id="B43">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voleti</surname><given-names>V.</given-names></name><name><surname>Patel</surname><given-names>K. B.</given-names></name><name><surname>Li</surname><given-names>W.</given-names></name><name><surname>Perez Campos</surname><given-names>C.</given-names></name><name><surname>Bharadwaj</surname><given-names>S.</given-names></name><name><surname>Yu</surname><given-names>H.</given-names></name><etal/></person-group> (<year>2019</year>). <article-title>Real-Time Volumetric Microscopy of <italic>In Vivo</italic> Dynamics and Large-Scale Samples with SCAPE 2.0</article-title>. <source>Nat. Methods</source>
<volume>16</volume>, <fpage>1054</fpage>–<lpage>1062</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-019-0579-4</pub-id>
<pub-id pub-id-type="pmid">31562489</pub-id></mixed-citation>
    </ref>
    <ref id="B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>C.</given-names></name></person-group> (<year>2018</year>). <article-title>All-Optical Imaging and Manipulation of Whole-Brain Neuronal Activities in Behaving Larval Zebrafish</article-title>. <source>Biomed. Opt. Express</source>
<volume>9</volume> (<issue>Issue 12</issue>), <fpage>6154</fpage>–<lpage>6169</lpage>. <pub-id pub-id-type="doi">10.1364/BOE.9.006154</pub-id>
<pub-id pub-id-type="pmid">31065420</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
