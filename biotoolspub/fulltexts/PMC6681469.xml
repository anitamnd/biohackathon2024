<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6681469</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btz328</article-id>
    <article-id pub-id-type="publisher-id">btz328</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Ismb/Eccb 2019 Conference Proceedings</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Macromolecular Sequence, Structure, and Function</subject>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Multifaceted protein–protein interaction prediction based on Siamese residual RCNN</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Muhao</given-names>
        </name>
        <xref ref-type="aff" rid="btz328-aff1">1</xref>
        <xref ref-type="corresp" rid="btz328-cor1"/>
        <xref ref-type="author-notes" rid="btz328-FM2"/>
        <!--<email>muhaochen@ucla.edu</email>-->
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ju</surname>
          <given-names>Chelsea J -T</given-names>
        </name>
        <xref ref-type="aff" rid="btz328-aff1">1</xref>
        <xref ref-type="author-notes" rid="btz328-FM2"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhou</surname>
          <given-names>Guangyu</given-names>
        </name>
        <xref ref-type="aff" rid="btz328-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chen</surname>
          <given-names>Xuelu</given-names>
        </name>
        <xref ref-type="aff" rid="btz328-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zhang</surname>
          <given-names>Tianran</given-names>
        </name>
        <xref ref-type="aff" rid="btz328-aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chang</surname>
          <given-names>Kai-Wei</given-names>
        </name>
        <xref ref-type="aff" rid="btz328-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Zaniolo</surname>
          <given-names>Carlo</given-names>
        </name>
        <xref ref-type="aff" rid="btz328-aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wang</surname>
          <given-names>Wei</given-names>
        </name>
        <xref ref-type="aff" rid="btz328-aff1">1</xref>
      </contrib>
    </contrib-group>
    <aff id="btz328-aff1"><label>1</label>Department of Computer Science, University of California, Los Angeles, Los Angeles, CA, USA</aff>
    <aff id="btz328-aff2"><label>2</label>Department of Bioengineering, University of California, Los Angeles, Los Angeles, CA, USA</aff>
    <author-notes>
      <corresp id="btz328-cor1">To whom correspondence should be addressed. <email>muhaochen@ucla.edu</email></corresp>
      <fn id="btz328-FM2">
        <p>The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="ppub">
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2019-07-05">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>05</day>
      <month>7</month>
      <year>2019</year>
    </pub-date>
    <!-- PMC Release delay is 0 months and 0 days and was based on the <pub-date pub-type="epub"/>. -->
    <volume>35</volume>
    <issue>14</issue>
    <fpage>i305</fpage>
    <lpage>i314</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2019. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2019</copyright-year>
      <license license-type="cc-by-nc" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution Non-Commercial License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. For commercial re-use, please contact journals.permissions@oup.com</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btz328.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>Sequence-based protein–protein interaction (PPI) prediction represents a fundamental computational biology problem. To address this problem, extensive research efforts have been made to extract predefined features from the sequences. Based on these features, statistical algorithms are learned to classify the PPIs. However, such explicit features are usually costly to extract, and typically have limited coverage on the PPI information.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>We present an end-to-end framework, <monospace>PIPR</monospace> (<bold>P</bold>rotein–Protein <bold>I</bold>nteraction <bold>P</bold>rediction Based on Siamese Residual <bold>R</bold>CNN), for PPI predictions using only the protein sequences. <monospace>PIPR</monospace> incorporates a deep residual recurrent convolutional neural network in the Siamese architecture, which leverages both robust local features and contextualized information, which are significant for capturing the mutual influence of proteins sequences. <monospace>PIPR</monospace> relieves the data pre-processing efforts that are required by other systems, and generalizes well to different application scenarios. Experimental evaluations show that <monospace>PIPR</monospace> outperforms various state-of-the-art systems on the binary PPI prediction problem. Moreover, it shows a promising performance on more challenging problems of interaction type prediction and binding affinity estimation, where existing approaches fall short.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>The implementation is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/muhaochen/seq_ppi.git">https://github.com/muhaochen/seq_ppi.git</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref ref-type="supplementary-material" rid="sup1">Supplementary data</xref> are available at <italic>Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Institutes of Health</named-content>
          <named-content content-type="funder-identifier">10.13039/100000002</named-content>
        </funding-source>
        <award-id>R01GM115833</award-id>
        <award-id>U54 GM114833</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <named-content content-type="funder-name">National Science Foundation</named-content>
          <named-content content-type="funder-identifier">10.13039/100000001</named-content>
        </funding-source>
        <award-id>DBI-1565137</award-id>
        <award-id>DGE-1829071</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="10"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Detecting protein–protein interactions (PPIs) and characterizing the interaction types are essential toward understanding cellular biological processes in normal and disease states. Knowledge from these studies potentially facilitates therapeutic target identification (<xref rid="btz328-B39" ref-type="bibr">Petta <italic>et al.</italic>, 2016</xref>) and novel drug design (<xref rid="btz328-B47" ref-type="bibr">Skrabanek <italic>et al.</italic>, 2008</xref>). High-throughput experimental technologies have been rapidly developed to discover and validate PPIs on a large scale. These technologies include yeast two-hybrid screens (<xref rid="btz328-B11" ref-type="bibr">Fields and Song, 1989</xref>), tandem affinity purification (<xref rid="btz328-B12" ref-type="bibr">Gavin <italic>et al.</italic>, 2002</xref>) and mass spectrometric protein complex identification (<xref rid="btz328-B17" ref-type="bibr">Ho <italic>et al.</italic>, 2002</xref>). However, experiment-based methods remain expensive, labor-intensive and time-consuming. Most importantly, they often suffer from high levels of false-positive predictions (<xref rid="btz328-B49" ref-type="bibr">Sun <italic>et al.</italic>, 2017</xref>; <xref rid="btz328-B61" ref-type="bibr">You <italic>et al.</italic>, 2015</xref>). Evidently, there is an immense need for reliable computational approaches to identify and characterize PPIs.</p>
    <p>The amino acid sequence represents the primary structure of a protein, which is the simplest type of information either obtained through direct sequencing or translated from DNA sequences. Many research efforts address the PPI problem based on predefined features extracted from protein sequences, such as ontological features of amino acids (<xref rid="btz328-B21" ref-type="bibr">Jansen <italic>et al.</italic>, 2003</xref>), autocovariance (AC) (<xref rid="btz328-B13" ref-type="bibr">Guo <italic>et al.</italic>, 2008</xref>), conjoint triads (CT) (<xref rid="btz328-B45" ref-type="bibr">Shen <italic>et al.</italic>, 2007</xref>) and composition-transition-distribution (CTD) descriptors (<xref rid="btz328-B56" ref-type="bibr">Yang <italic>et al.</italic>, 2010</xref>). These features generally summarize specific aspects of protein sequences such as physicochemical properties, frequencies of local patterns and the positional distribution of amino acids. On top of these features, several statistical learning algorithms (<xref rid="btz328-B13" ref-type="bibr">Guo <italic>et al.</italic>, 2008</xref>; <xref rid="btz328-B19" ref-type="bibr">Huang <italic>et al.</italic>, 2015</xref>; <xref rid="btz328-B60" ref-type="bibr">You <italic>et al.</italic>, 2014</xref>, <xref rid="btz328-B61" ref-type="bibr">2015</xref>) are applied to predict PPIs in the form of binary classification. These approaches provide feasible solutions to the problem. However, the extracted features used in these approaches only have limited coverage on interaction information, as they are dedicated to specific facets of the protein profiles.</p>
    <p>To mitigate the inadequacy of statistical learning methods, deep learning algorithms provide the powerful functionality to process large-scale data and automatically extract useful features for objective tasks (<xref rid="btz328-B25" ref-type="bibr">LeCun <italic>et al.</italic>, 2015</xref>). Recently, deep learning architectures have produced powerful systems to address several bioinformatics problems related to single nucleotide sequences, such as genetic variants detection (<xref rid="btz328-B2" ref-type="bibr">Anderson, 2018</xref>), DNA function classification (<xref rid="btz328-B41" ref-type="bibr">Quang and Xie, 2016</xref>), RNA-binding site prediction (<xref rid="btz328-B63" ref-type="bibr">Zhang <italic>et al.</italic>, 2016</xref>) and chromatin accessibility prediction (<xref rid="btz328-B32" ref-type="bibr">Min <italic>et al.</italic>, 2017</xref>). These works typically use convolutional neural networks (CNN) (<xref rid="btz328-B2" ref-type="bibr">Anderson, 2018</xref>; <xref rid="btz328-B63" ref-type="bibr">Zhang <italic>et al.</italic>, 2016</xref>) for automatically selecting local features, or recurrent neural networks (RNN) (<xref rid="btz328-B41" ref-type="bibr">Quang and Xie, 2016</xref>) that aim at preserving the contextualized and long-term ordering information. In contrast, fewer efforts (discussed in Section 2) have been made to capture the pairwise interactions of proteins with deep learning, which remains a non-trivial problem with the following challenges: (i) Characterization of the proteins requires a model to effectively filter and aggregate their local features, while preserving significant contextualized and sequential information of the amino acids; (ii) extending a deep neural architecture often leads to inefficient learning processes, and suffers from the notorious vanishing gradient problem (<xref rid="btz328-B38" ref-type="bibr">Pascanu <italic>et al.</italic>, 2013</xref>); (iii) an effective mechanism is also needed to apprehend the mutual influence of protein pairs in PPI prediction. Moreover, it is essential for the framework to be scalable to large data, and to be generalized to different prediction tasks.</p>
    <p>In this paper, we introduce <monospace>PIPR</monospace> (<bold>P</bold>rotein–Protein <bold>I</bold>nteraction <bold>P</bold>rediction Based on Siamese Residual <bold>R</bold>CNN), a deep learning framework for PPI prediction using only the sequences of a protein pair. <monospace>PIPR</monospace> employs a Siamese architecture to capture the mutual influence of a protein sequence pair. The learning architecture is based on a residual recurrent convolutional neural network (RCNN), which integrates multiple occurrences of convolution layers and residual gated recurrent units. To represent each amino acid in this architecture, <monospace>PIPR</monospace> applies an efficient property-aware lexicon embedding approach to better capture the contextual and physicochemical relatedness of amino acids. This comprehensive encoding architecture provides a multi-granular feature aggregation process to effectively leverage both sequential and robust local information of the protein sequences. It is important to note that the scope of this work focuses only on the primary sequence as it is the fundamental information to describe a protein.</p>
    <p>Our contributions are 3-fold. First, we construct an end-to-end framework for PPI prediction that relieves the data pre-processing efforts for users. <monospace>PIPR</monospace> requires only the primary protein sequences as the input, and is trained to automatically preserve the critical features from the sequences. Second, we emphasize and demonstrate the needs of considering the contextualized and sequential information when modeling the PPIs. Third, the architecture of <monospace>PIPR</monospace> can be flexibly used to address different PPI tasks. Besides the binary prediction that is widely attempted in previous works, our framework extends its use to two additional challenging problems: multi-class interaction type prediction and binding affinity estimation. We use five datasets to evaluate the performance of our framework on these tasks. <monospace>PIPR</monospace> outperforms various state-of-the-art approaches on the binary prediction task, which confirms the effectiveness in terms of integrating both local features and sequential information. The promising performance of the other two tasks demonstrates the wide usability of our approach. Especially on the binding affinity estimation of mutated proteins, <monospace>PIPR</monospace> is able to respond to the subtle changes of point mutations and provides the best estimation with the smallest errors.</p>
  </sec>
  <sec>
    <title>2 Related works</title>
    <p>Sequence-based approaches provide a critical solution to the binary PPI prediction task. Homology-based methods (<xref rid="btz328-B40" ref-type="bibr">Philipp <italic>et al.</italic>, 2016</xref>) rely on BLAST to map a pair of sequences to known interacting proteins. Alternatively, other works address the task with statistical learning models, including SVM (<xref rid="btz328-B13" ref-type="bibr">Guo <italic>et al.</italic>, 2008</xref>; <xref rid="btz328-B60" ref-type="bibr">You <italic>et al.</italic>, 2014</xref>), kNN (<xref rid="btz328-B56" ref-type="bibr">Yang <italic>et al.</italic>, 2010</xref>), Random Forest (<xref rid="btz328-B55" ref-type="bibr">Wong <italic>et al.</italic>, 2015</xref>), multi-layer perceptron (MLP) (<xref rid="btz328-B10" ref-type="bibr">Du <italic>et al.</italic>, 2017</xref>) and ensemble ELM (EELM) (<xref rid="btz328-B59" ref-type="bibr">You <italic>et al.</italic>, 2013</xref>). These approaches rely on several feature extraction processes for the protein sequences, such as CT (<xref rid="btz328-B49" ref-type="bibr">Sun <italic>et al.</italic>, 2017</xref>; <xref rid="btz328-B59" ref-type="bibr">You <italic>et al.</italic>, 2013</xref>), AC (<xref rid="btz328-B13" ref-type="bibr">Guo <italic>et al.</italic>, 2008</xref>; <xref rid="btz328-B49" ref-type="bibr">Sun <italic>et al.</italic>, 2017</xref>; <xref rid="btz328-B59" ref-type="bibr">You <italic>et al.</italic>, 2013</xref>), CTD (<xref rid="btz328-B10" ref-type="bibr">Du <italic>et al.</italic>, 2017</xref>; <xref rid="btz328-B56" ref-type="bibr">Yang <italic>et al.</italic>, 2010</xref>), multi-scale continuous and discontinuous (MCD) descriptors (<xref rid="btz328-B59" ref-type="bibr">You <italic>et al.</italic>, 2013</xref>) and local phase quantization (LPQ) (<xref rid="btz328-B55" ref-type="bibr">Wong <italic>et al.</italic>, 2015</xref>). These features measure physicochemical properties of the 20 canonical amino acids, and aim at summarizing full sequence information relevant to PPIs. More recent works (<xref rid="btz328-B49" ref-type="bibr">Sun <italic>et al.</italic>, 2017</xref>; <xref rid="btz328-B53" ref-type="bibr">Wang <italic>et al.</italic>, 2017</xref>) propose the use of stacked autoencoders (SAE) to refine these heterogeneous features in low-dimensional spaces, which improve the aforementioned models on the binary prediction task. On the contrary, fewer efforts have been made toward multi-class prediction to infer the interaction types (<xref rid="btz328-B46" ref-type="bibr">Silberberg <italic>et al.</italic>, 2014</xref>; <xref rid="btz328-B66" ref-type="bibr">Zhu <italic>et al.</italic>, 2006</xref>) and the regression task to estimate binding affinity (<xref rid="btz328-B48" ref-type="bibr">Srinivasulu <italic>et al.</italic>, 2015</xref>; <xref rid="btz328-B62" ref-type="bibr">Yugandhar and Gromiha, 2014</xref>). These methods have largely relied on their capability of extracting and selecting better features, while the extracted features are far from fully exploiting the interaction information.</p>
    <p>By nature, the PPI prediction task is comparable to the neural sentence pair modeling tasks in natural language processing (NLP) research, as they both seek to characterize the mutual influence of two sequences based on their latent features. In NLP, neural sentence pair models typically focus on capturing the discourse relations of lexicon sequences, such as textual entailment (<xref rid="btz328-B18" ref-type="bibr">Hu <italic>et al.</italic>, 2014</xref>; <xref rid="btz328-B58" ref-type="bibr">Yin <italic>et al.</italic>, 2016</xref>), paraphrases (<xref rid="btz328-B15" ref-type="bibr">He <italic>et al.</italic>, 2015</xref>; <xref rid="btz328-B57" ref-type="bibr">Yin and Schütze, 2015</xref>) and sub-topic relations (<xref rid="btz328-B5" ref-type="bibr">Chen <italic>et al.</italic>, 2018</xref>). Many recent efforts adopt a Siamese encoding architecture, where encoders based on CNN (<xref rid="btz328-B18" ref-type="bibr">Hu <italic>et al.</italic>, 2014</xref>; <xref rid="btz328-B57" ref-type="bibr">Yin and Schütze, 2015</xref>) and RNN (<xref rid="btz328-B34" ref-type="bibr">Mueller and Thyagarajan, 2016</xref>) are widely used. A binary classifier is then stacked to the sequence pair encoder for the detection of a discourse relation.</p>
    <p>In contrast to sentences, proteins are profiled in sequences with more intractable patterns, as well as in a drastically larger range of lengths. Precisely capturing the PPI requires much more comprehensive learning architectures to distill the latent information from the entire sequences, and to preserve the long-term ordering information. One recent work (<xref rid="btz328-B14" ref-type="bibr">Hashemifar <italic>et al.</italic>, 2018</xref>), DPPI, uses a deep CNN-based architecture which focuses on capturing local features from protein profiles. DPPI represents the first work to deploy deep learning to PPI prediction, and has achieved the state-of-the-art performance on the binary prediction task. However, it requires excessive efforts for data pre-processing such as constructing protein profiles by PSI-BLAST (<xref rid="btz328-B1" ref-type="bibr">Altschul <italic>et al.</italic>, 1997</xref>), and does not incorporate a neural learning architecture that captures the important contextualized and sequential features. DNN-PPI (<xref rid="btz328-B26" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>) represents another relevant work of this line, which deploys a different learning structure with two separated CNN encoders. However, DNN-PPI does not incorporate physicochemical properties into amino acid representations, and does not employ a Siamese learning architecture to fully characterize pairwise relations of sequences.</p>
  </sec>
  <sec>
    <title>3 Materials and methods</title>
    <p>We introduce an end-to-end deep learning framework, <monospace>PIPR</monospace>, for sequence-based PPI prediction tasks. The overall learning architecture is illustrated in <xref ref-type="fig" rid="btz328-F1">Figure 1</xref>. <monospace>PIPR</monospace> employs a Siamese architecture of residual RCNN encoder to better apprehend and utilize the mutual influence of two sequences. To capture the features of the protein sequences from scratch, <monospace>PIPR</monospace> pre-trains the embeddings of canonical amino acids to capture their contextual similarity and physicochemical properties. The latent representation of each protein in a protein pair is obtained by feeding the corresponding amino acid embeddings into the sequence encoder. The embeddings of these two sequences are then combined to form a sequence pair vector. Finally, this sequence pair vector is fed into an MLP with appropriate loss functions, suiting for specific prediction tasks. In this section, we describe the details of each model component. We begin with the denotations and problem specifications.
</p>
    <fig id="btz328-F1" orientation="portrait" position="float">
      <label>Fig. 1.</label>
      <caption>
        <p>The overall learning architecture of our framework</p>
      </caption>
      <graphic xlink:href="btz328f1"/>
    </fig>
    <sec>
      <title>3.1 Preliminary</title>
      <p>We use <italic>A</italic> to denote the vocabulary of 20 canonical amino acids. A protein is profiled as a sequence of amino acids <inline-formula id="IE1"><mml:math id="IM1"><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="true">[</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">]</mml:mo></mml:math></inline-formula> such that each<inline-formula id="IE2"><mml:math id="IM2"><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:math></inline-formula>. For each amino acid <italic>a<sub>i</sub></italic>, we use bold-faced <inline-formula id="IE3"><mml:math id="IM3"><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to denote its embedding representation, which we are going to specify in Section 3.2.3. We use <italic>I</italic> to denote the set of protein pairs, and <inline-formula id="IE4"><mml:math id="IM4"><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>∈</mml:mo><mml:mi>I</mml:mi></mml:math></inline-formula> denotes a pair of proteins of which our framework captures the interaction.</p>
      <p>We address three challenging PPI prediction tasks based only on the primary sequence information: (i) <italic>Binary prediction</italic> seeks to provide a binary classifier to indicate whether the corresponding protein pair interacts, which is the simplest and widely considered problem setting in previous works (<xref rid="btz328-B14" ref-type="bibr">Hashemifar <italic>et al.</italic>, 2018</xref>; <xref rid="btz328-B47" ref-type="bibr">Skrabanek <italic>et al.</italic>, 2008</xref>; <xref rid="btz328-B49" ref-type="bibr">Sun <italic>et al.</italic>, 2017</xref>). (ii) <italic>Interaction type prediction</italic> is a multi-class classification problem, which seeks to identify the interaction type of two proteins. (iii) <italic>Binding affinity estimation</italic> aims at producing a regression model to estimate the strength of the binding interaction.</p>
    </sec>
    <sec>
      <title>3.2 RCNN-based protein sequence encoder</title>
      <p>We employ a deep Siamese architecture of residual RCNN to capture latent semantic features of the protein sequence pairs.</p>
      <sec>
        <label>3.2.1</label>
        <title>Residual RCNN</title>
        <p>The RCNN seeks to leverage both the global sequential information and local features that are significant to the characterization of PPI from the protein sequences. This deep neural encoder stacks multiple instances of two computational modules, i.e. <italic>convolution layers with pooling</italic> and <italic>bidirectional residual gated recurrent units</italic>. The architecture of an RCNN unit is shown on the left of <xref ref-type="fig" rid="btz328-F2">Figure 2</xref>.
</p>
        <fig id="btz328-F2" orientation="portrait" position="float">
          <label>Fig. 2.</label>
          <caption>
            <p>The structure of our residual RCNN encoder is shown on the right, and the RCNN unit is shown on the left. Each RCNN unit contains a convolution-pooling layer followed a bidirectional residual GRU</p>
          </caption>
          <graphic xlink:href="btz328f2"/>
        </fig>
        <p><bold>The convolution layer with pooling.</bold> We use <inline-formula id="IE5"><mml:math id="IM5"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="true">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">]</mml:mo></mml:math></inline-formula> to denote an input vector sequence that corresponds to the embedded amino acids or the outputs of a previous neural layer. A convolution layer applies a weight-sharing kernel <inline-formula id="IE6"><mml:math id="IM6"><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> to generate a <italic>k</italic>-dimension latent vector <inline-formula id="IE7"><mml:math id="IM7"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> from a window <inline-formula id="IE8"><mml:math id="IM8"><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> of the input vector sequence <italic>X</italic>:
<disp-formula id="E1"><mml:math id="M1"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext>Conv</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>h</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula>
for which <italic>h</italic> is the kernel size, and <inline-formula id="IE9"><mml:math id="IM9"><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a bias vector. The convolution layer applies the kernel as a sliding window to produce a sequence of latent vectors <inline-formula id="IE10"><mml:math id="IM10"><mml:mi mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="true">[</mml:mo><mml:msubsup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold">h</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">]</mml:mo></mml:math></inline-formula>, where each latent vector combines the local features from each <italic>h</italic>-gram of the input sequence. The <italic>n</italic>-max-pooling mechanism is applied to every consecutive <italic>n</italic>-length subsequence (i.e. non-overlapped <italic>n</italic>-strides) of the convolution outputs, which takes the maximum value along each dimension <italic>j</italic> by <inline-formula id="IE11"><mml:math id="IM11"><mml:mi mathvariant="normal"> </mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">max</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>:</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo></mml:math></inline-formula>. The purpose of this mechanism is to discretize the convolution results, and preserve the most significant features within each <italic>n</italic>-stride (<xref rid="btz328-B5" ref-type="bibr">Chen <italic>et al.</italic>, 2018</xref>; <xref rid="btz328-B14" ref-type="bibr">Hashemifar <italic>et al.</italic>, 2018</xref>; <xref rid="btz328-B24" ref-type="bibr">Kim, 2014</xref>). By definition, this mechanism divides the size of processed features by <italic>n</italic>. The outputs from the max-pooling are fed into the bidirectional gated recurrent units in our RCNN encoder.</p>
        <p><bold>The residual gated recurrent units.</bold> The gated recurrent unit model (GRU) represents an alternative to the long-short-term memory (LSTM) network (<xref rid="btz328-B6" ref-type="bibr">Cho <italic>et al.</italic>, 2014</xref>), which consecutively characterizes the sequential information without using separated memory cells (<xref rid="btz328-B9" ref-type="bibr">Dhingra <italic>et al.</italic>, 2017</xref>). Each unit consists of two types of gates to track the state of the sequence, i.e. the reset gate <inline-formula id="IE12"><mml:math id="IM12"><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and the update gate<inline-formula id="IE13"><mml:math id="IM13"><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Given the embedding <inline-formula id="IE14"><mml:math id="IM14"><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of an incoming item (either a pre-trained amino acid embedding, or an output of the previous layer), GRU updates the hidden state <inline-formula id="IE15"><mml:math id="IM15"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> of the sequence as a linear combination of the previous state <inline-formula id="IE16"><mml:math id="IM16"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and the candidate state <inline-formula id="IE17"><mml:math id="IM17"><mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mstyle></mml:mrow><mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> of a new item <inline-formula id="IE18"><mml:math id="IM18"><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which is calculated as below.
<disp-formula id="E2"><mml:math id="M2"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>GRU</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>⊙</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:math></disp-formula><disp-formula id="E3"><mml:math id="M3"><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="E4"><mml:math id="M4"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>tanh</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⊙</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="true">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></disp-formula><disp-formula id="E5"><mml:math id="M5"><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">σ</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p>Notation <inline-formula id="IE19"><mml:math id="IM19"><mml:mo>⊙</mml:mo></mml:math></inline-formula> thereof denotes the element-wise multiplication. The update gate <inline-formula id="IE20"><mml:math id="IM20"><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> balances the information of the previous sequence and the new item, where capitalized <inline-formula id="IE21"><mml:math id="IM21"><mml:msub><mml:mrow><mml:mi mathvariant="bold">M</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula id="IE22"><mml:math id="IM22"><mml:msub><mml:mrow><mml:mi mathvariant="bold">N</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> denote different weight matrices, <inline-formula id="IE23"><mml:math id="IM23"><mml:msub><mml:mrow><mml:mi mathvariant="bold">b</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math></inline-formula> denote bias vectors and σ is the sigmoid function. The candidate state <inline-formula id="IE24"><mml:math id="IM24"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> is calculated similarly to those in a traditional recurrent unit, and the reset gate <inline-formula id="IE25"><mml:math id="IM25"><mml:msub><mml:mrow><mml:mi mathvariant="bold">r</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> controls how much information of the past sequence contributes to <inline-formula id="IE26"><mml:math id="IM26"><mml:msubsup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. Note that GRU generally performs comparably to LSTM in sequence encoding tasks, but is less complex and requires much fewer computational resources for training.</p>
        <p>A <italic>bidirectional GRU layer</italic> characterizes the sequential information in two directions. It contains the forward encoding process <inline-formula id="IE27"><mml:math id="IM27"><mml:mrow><mml:mstyle displaystyle="true"><mml:mover><mml:mrow><mml:mtext>GRU</mml:mtext></mml:mrow><mml:mi>→</mml:mi></mml:mover></mml:mstyle></mml:mrow></mml:math></inline-formula> that reads the input vector sequence <inline-formula id="IE28"><mml:math id="IM28"><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="true">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">]</mml:mo></mml:math></inline-formula> from <inline-formula id="IE29"><mml:math id="IM29"><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> to <inline-formula id="IE30"><mml:math id="IM30"><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and a backward encoding process <inline-formula id="IE31"><mml:math id="IM31"><mml:mrow><mml:mstyle displaystyle="true"><mml:mover><mml:mrow><mml:mtext>GRU</mml:mtext></mml:mrow><mml:mi>←</mml:mi></mml:mover></mml:mstyle></mml:mrow></mml:math></inline-formula> that reads in the opposite direction. The encoding results of both processes are concatenated for each input item<inline-formula id="IE32"><mml:math id="IM32"><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, i.e. <inline-formula id="IE33"><mml:math id="IM33"><mml:mi mathvariant="normal"> </mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>BiGRU</mml:mtext><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mover><mml:mrow><mml:mtext>GRU</mml:mtext></mml:mrow><mml:mi>→</mml:mi></mml:mover></mml:mstyle></mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mover><mml:mrow><mml:mtext>GRU</mml:mtext></mml:mrow><mml:mi>←</mml:mi></mml:mover></mml:mstyle></mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo stretchy="true">]</mml:mo></mml:math></inline-formula>.</p>
        <p>The <italic>residual mechanism</italic> passes on an identity mapping of the GRU inputs to its output side through a residual shortcut (<xref rid="btz328-B16" ref-type="bibr">He <italic>et al.</italic>, 2016</xref>). By adding the forwarded input values to the outputs, the corresponding neural layer is only required to capture the difference between the input and output values. This mechanism aims at improving the learning process of non-linear neural layers by increasing the sensitivity of the optimization gradients (<xref rid="btz328-B16" ref-type="bibr">He <italic>et al.</italic>, 2016</xref>; <xref rid="btz328-B23" ref-type="bibr">Kim <italic>et al.</italic>, 2016</xref>), as well as preventing the model from the vanishing gradient problem. It has been widely deployed in deep learning architectures for various tasks of image recognition (<xref rid="btz328-B16" ref-type="bibr">He <italic>et al.</italic>, 2016</xref>), document classification (<xref rid="btz328-B7" ref-type="bibr">Conneau <italic>et al.</italic>, 2017</xref>) and speech recognition (<xref rid="btz328-B64" ref-type="bibr">Zhang <italic>et al.</italic>, 2017</xref>). In our deep RCNN, the bidirectional GRU is incorporated with the residual mechanism, and will pass on the following outputs to its subsequent neural network layer:
<disp-formula id="E6"><mml:math id="M6"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>ResGRU</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mstyle displaystyle="true"><mml:mover><mml:mrow><mml:mi>GRU</mml:mi></mml:mrow><mml:mi>→</mml:mi></mml:mover></mml:mstyle><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mtext>GRU</mml:mtext></mml:mrow><mml:mi>←</mml:mi></mml:mover></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:math></disp-formula></p>
        <p>In our development, we have found that the residual mechanism is able to drastically simplify the training process, and largely decreases the epochs of parameter updates for the model to converge.</p>
      </sec>
      <sec>
        <label>3.2.2</label>
        <title>Protein sequence encoding</title>
        <p><xref ref-type="fig" rid="btz328-F2">Figure 2</xref> shows the entire structure of our RCNN encoder. The RCNN encoder <inline-formula id="IE34"><mml:math id="IM34"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RCNN</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="true">)</mml:mo></mml:math></inline-formula> alternately stacks multiple occurrences of the above two intermediary neural network components. A convolution layer serves as the first encoding layer to extract local features from the input sequence. On top of that, a residual GRU layer takes in the preserved local features, whose outputs are passed to another convolution layer. Repeating of these two components in the network structure conducts an automatic multi-granular feature aggregation process on the protein sequence, while preserving the sequential and contextualized information on each granularity of the selected features. The last residual GRU layer is followed by another convolution layer for a final round of local feature selection to produce the last hidden states <inline-formula id="IE35"><mml:math id="IM35"><mml:mo> </mml:mo><mml:mi>H</mml:mi><mml:mi>'</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="true">[</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mrow><mml:mi>'</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mrow><mml:mi>'</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold">h</mml:mi><mml:msub><mml:mrow><mml:mi>'</mml:mi></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mi>'</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="true">]</mml:mo></mml:math></inline-formula>. Note that the dimensionality of the last hidden states does not need to equal that of the previous hidden states. A high-level sequence embedding of the entire protein sequence is obtained from the global average-pooling (<xref rid="btz328-B28" ref-type="bibr">Lin <italic>et al.</italic>, 2013</xref>) of<inline-formula id="IE36"><mml:math id="IM36"><mml:mo> </mml:mo><mml:mi>H</mml:mi><mml:mi>'</mml:mi></mml:math></inline-formula>, i.e.<inline-formula id="IE37"><mml:math id="IM37"><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RCNN</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:mi>S</mml:mi><mml:mo stretchy="true">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mi>'</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:msubsup><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>H</mml:mi><mml:mi>'</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="bold">h</mml:mi></mml:mrow></mml:mrow><mml:msub><mml:mrow><mml:mi>'</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p>
      </sec>
      <sec>
        <label>3.2.3</label>
        <title>Pre-trained amino acid embeddings</title>
        <p>To support inputting the non-numerical sequence information, we provide a useful embedding method to represent each amino acid <inline-formula id="IE38"><mml:math id="IM38"><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mi>A</mml:mi></mml:math></inline-formula> as a semi-latent vector <bold>a</bold>. Each embedding vector is a concatenation of two sub-embeddings, i.e.<inline-formula id="IE39"><mml:math id="IM39"><mml:mo> </mml:mo><mml:mi mathvariant="bold">a</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="true">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ph</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">]</mml:mo></mml:math></inline-formula>.</p>
        <p>The first part <inline-formula id="IE40"><mml:math id="IM40"><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> measures the co-occurrence similarity of the amino acids, which is obtained by pre-training the Skip-Gram model (<xref rid="btz328-B31" ref-type="bibr">Mikolov <italic>et al.</italic>, 2013</xref>) on protein sequences. The learning objective of Skip-Gram is to minimize the following negative log likelihood loss.
<disp-formula id="E7"><mml:math id="M7"><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">SG</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi>S</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:munder><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>C</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>j</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:math></disp-formula><inline-formula id="IE41"><mml:math id="IM41"><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> thereof is the first-part embedding of the <italic>t</italic>-th amino acid <inline-formula id="IE42"><mml:math id="IM42"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is that of a neighboring amino acid, and <italic>C</italic> is the size of half context. (The context of Skip-Gram means a subsequence of a given protein sequence <italic>S</italic>, such that the subsequence is of <inline-formula id="IE43"><mml:math id="IM43"><mml:mn>2</mml:mn><mml:mi>C</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> length.) The probability <italic>p</italic> is defined as the following softmax:
<disp-formula id="E8"><mml:math id="M8"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow></mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi>'</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula>
where <italic>n</italic> is the negative sampling size, and <inline-formula id="IE44"><mml:math id="IM44"><mml:mi mathvariant="bold">a</mml:mi><mml:msub><mml:mrow><mml:mi>'</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a negative sample that does not co-occur with <inline-formula id="IE45"><mml:math id="IM45"><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the same context.</p>
        <p>The second part <inline-formula id="IE46"><mml:math id="IM46"><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ph</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> represents the similarity of electrostaticity and hydrophobicity among amino acids. The 20 amino acids can be clustered into 7 classes based on their dipoles and volumes of the side chains to reflect this property. Thus, <inline-formula id="IE47"><mml:math id="IM47"><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ph</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is a one-hot encoding based on the classification defined by <xref rid="btz328-B45" ref-type="bibr">Shen <italic>et al.</italic> (2007)</xref>.</p>
      </sec>
    </sec>
    <sec>
      <title>3.3 Learning architecture and learning objectives</title>
      <p>Our framework characterizes the interactions in the following two stages.</p>
      <sec>
        <label>3.3.1</label>
        <title>Siamese architecture</title>
        <p>Given a pair of proteins <inline-formula id="IE48"><mml:math id="IM48"><mml:mo> </mml:mo><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>∈</mml:mo><mml:mi>I</mml:mi></mml:math></inline-formula>, the same RCNN encoder is used to obtain the sequence embeddings <inline-formula id="IE49"><mml:math id="IM49"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RCNN</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:math></inline-formula> and <inline-formula id="IE50"><mml:math id="IM50"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RCNN</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:math></inline-formula> of both proteins. Both sequence embeddings are combined using element-wise multiplication, i.e.<inline-formula id="IE51"><mml:math id="IM51"><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RCNN</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo><mml:mo>⊙</mml:mo><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">RCNN</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true">(</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true">)</mml:mo></mml:math></inline-formula>. This is a commonly used operation to infer the relation of sequence embeddings (<xref rid="btz328-B14" ref-type="bibr">Hashemifar <italic>et al.</italic>, 2018</xref>; <xref rid="btz328-B22" ref-type="bibr">Jiang <italic>et al.</italic>, 2018</xref>; <xref rid="btz328-B43" ref-type="bibr">Rocktäschel <italic>et al.</italic>, 2016</xref>; <xref rid="btz328-B51" ref-type="bibr">Tai <italic>et al.</italic>, 2015</xref>). Note that some works use the concatenation of sequence embeddings (<xref rid="btz328-B49" ref-type="bibr">Sun <italic>et al.</italic>, 2017</xref>; <xref rid="btz328-B57" ref-type="bibr">Yin and Schütze, 2015</xref>) instead of multiplication, which we find to be less effective in modeling the symmetric relations of proteins.</p>
      </sec>
      <sec>
        <label>3.3.2</label>
        <title>Learning objectives</title>
        <p>An MLP with leaky ReLU (<xref rid="btz328-B29" ref-type="bibr">Maas <italic>et al.</italic>, 2013</xref>) is applied to the previous sequence pair representation, whose output <inline-formula id="IE52"><mml:math id="IM52"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is either a vector or a scalar, depending on whether the model solves a classification or a regression task for the protein pair <italic>p</italic>. The entire learning architecture is trained to optimize the following two types of losses according to different PPI prediction problems.
<list list-type="roman-lower"><list-item><p><italic>Cross-entropy loss</italic> is optimized for the two classification problems, i.e. binary prediction and interaction type prediction. In this case, the MLP output <inline-formula id="IE53"><mml:math id="IM53"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a vector, whose dimensionality equals the number of classes <italic>m</italic>. <inline-formula id="IE54"><mml:math id="IM54"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is normalized by a softmax function, where the <italic>i</italic>-th dimension <inline-formula id="IE55"><mml:math id="IM55"><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="normal">exp</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mfrac></mml:math></inline-formula> corresponds to the confidence score for the <italic>i</italic>-th class. The learning objective is to minimize the following cross-entropy loss, where <italic>c<sup>p</sup></italic> is a one-hot indicator for the class label of protein pair <italic>p</italic>.
<disp-formula id="E18"><mml:math id="M9"><mml:msup><mml:mrow><mml:mi mathvariant="italic">L</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:mi mathvariant="italic">I</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi mathvariant="italic">p</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="italic">I</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:munderover><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi mathvariant="italic">i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="italic">m</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="italic">c</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi mathvariant="italic">p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">og</mml:mi><mml:msubsup><mml:mrow><mml:mi mathvariant="italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">p</mml:mi></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:math></disp-formula></p></list-item><list-item><p><italic>Mean squared loss</italic> is optimized for the binding affinity estimation task. In this case, <inline-formula id="IE56"><mml:math id="IM56"><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a scalar output that is normalized by a sigmoid function<inline-formula id="IE57"><mml:math id="IM57"><mml:mi mathvariant="normal"> </mml:mi><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">exp</mml:mi><mml:mo stretchy="true">(</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mfrac></mml:math></inline-formula>, which is trained to approach the normalized ground truth score <inline-formula id="IE58"><mml:math id="IM58"><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>∈</mml:mo><mml:mo stretchy="true">[</mml:mo><mml:mn>0,1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:math></inline-formula> by minimizing the following objective function:
<disp-formula id="E19"><mml:math id="M10"><mml:msup><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>I</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo stretchy="true">∑</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msup><mml:mrow><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p></list-item></list></p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>4 Experiments</title>
    <p>We present the experimental evaluation of the proposed framework on three PPI prediction tasks, i.e. binary prediction, multi-class interaction type prediction and binding affinity estimation. The experiments are conducted on the following datasets.</p>
    <sec>
      <title>4.1 Datasets</title>
      <p><bold>Guo’s datasets.</bold><xref rid="btz328-B13" ref-type="bibr">Guo <italic>et al.</italic> (2008)</xref> generate several datasets from different species for the binary prediction of PPIs. Each dataset contains a balanced number of positive and negative samples. Among these resources, the <italic>Yeast dataset</italic> is a widely used benchmark by most state-of-the-art methods (<xref rid="btz328-B14" ref-type="bibr">Hashemifar <italic>et al.</italic>, 2018</xref>; <xref rid="btz328-B55" ref-type="bibr">Wong <italic>et al.</italic>, 2015</xref>; <xref rid="btz328-B59" ref-type="bibr">You <italic>et al.</italic>, 2013</xref>, <xref rid="btz328-B60" ref-type="bibr">2014</xref>). There are 2497 proteins forming 11 188 cases of PPIs, with half of them representing the positive cases, and the other half the negative cases. The positive cases are selected from the database of interacting proteins DIP_20070219 (<xref rid="btz328-B44" ref-type="bibr">Salwinski <italic>et al.</italic>, 2004</xref>), where proteins with fewer than 50 amino acids or ≥40% sequence identity are excluded. We use the full protein sequences in our model, which are obtained from the UniProt (<xref rid="btz328-B8" ref-type="bibr">Consortium <italic>et al.</italic>, 2018</xref>). The negative cases are generated by randomly pairing the proteins without evidence of interaction, and filtered by their sub-cellular locations. In other words, non-interactive pairs residing in the same location are excluded.</p>
      <p>In addition, we combine the data for <italic>Caenorhabditis elegans</italic>, <italic>Escherichia coli</italic> and <italic>Drosophila melanogaster</italic> as the <italic>multi-species dataset</italic>. We use the cluster analysis of the CD-HIT (<xref rid="btz328-B27" ref-type="bibr">Li and Godzik, 2006</xref>) program to generate non-redundant subsets. Proteins with fewer than 50 amino acids or high sequence identify (40, 25, 10 or 1%) are removed.</p>
      <p><bold>STRING datasets.</bold> The STRING database (<xref rid="btz328-B50" ref-type="bibr">Szklarczyk <italic>et al.</italic>, 2016</xref>) annotates PPIs with their types. There are seven types of interactions: activation, binding, catalysis, expression, inhibition, post-translational modification (ptmod) and reaction. We download all interaction pairs for <italic>Homo sapiens</italic> from database version 10.5 (<xref rid="btz328-B50" ref-type="bibr">Szklarczyk <italic>et al.</italic>, 2016</xref>), along with their full protein sequences. Among the corresponding proteins, we randomly select 3000 proteins and 8000 proteins that share &lt;40% of sequence identity to generate two subsets. In this process, we randomly sample instances of different interaction types to ensure a balanced class distribution. Eventually, the two generated datasets, denoted by SHS27k and SHS148k, contain 26 945 cases and 148 051 cases of interactions respectively. We use these two datasets for the PPI type prediction task.</p>
      <p><bold>SKEMPI dataset.</bold> We obtain the protein binding affinity data from SKEMPI (the structural database of kinetics and energetics of mutant protein interactions) (<xref rid="btz328-B33" ref-type="bibr">Moal and Fernández-Recio, 2012</xref>) for the affinity estimation task. It contains 3047 binding affinity changes upon mutation of protein sub-units within a protein complex. The binding affinity is measured by equilibrium dissociation constant (<italic>K<sub>d</sub></italic>), reflecting the strength of biomolecular interactions. The smaller <italic>K<sub>d</sub></italic> value means the higher binding affinity. Each protein complex contains single or multiple amino acid substitutions. The sequence of the protein complex is retrieved from the protein data bank (PDB) (<xref rid="btz328-B4" ref-type="bibr">Berman <italic>et al.</italic>, 2000</xref>). We manually replace the mutated amino acids. For duplicate entries, we take the average <italic>K<sub>d</sub></italic>. The final dataset results in the binding affinity of 2792 mutant protein complexes, along with 158 wild-types.</p>
    </sec>
    <sec>
      <title>4.2 Binary PPI prediction</title>
      <p>Binary PPI prediction is the primary task targeted by a handful of previous works (<xref rid="btz328-B14" ref-type="bibr">Hashemifar <italic>et al.</italic>, 2018</xref>; <xref rid="btz328-B45" ref-type="bibr">Shen <italic>et al.</italic>, 2007</xref>; <xref rid="btz328-B49" ref-type="bibr">Sun <italic>et al.</italic>, 2017</xref>; <xref rid="btz328-B56" ref-type="bibr">Yang <italic>et al.</italic>, 2010</xref>; <xref rid="btz328-B61" ref-type="bibr">You <italic>et al.</italic>, 2015</xref>). The objective of these works is to identify whether a given pair of proteins interacts or not based on their sequences. We evaluate <monospace>PIPR</monospace> based on Guo’s datasets. The Yeast benchmark dataset thereof is used to compare <monospace>PIPR</monospace> with various baseline approaches, and the multi-species dataset is to demonstrate <monospace>PIPR</monospace>’s capability of predicting interactions for proteins of different species that share very low sequence identity with those in training.</p>
      <p>The baseline approaches include SVM-AC (<xref rid="btz328-B13" ref-type="bibr">Guo <italic>et al.</italic>, 2008</xref>), kNN-CTD (<xref rid="btz328-B56" ref-type="bibr">Yang <italic>et al.</italic>, 2010</xref>), EELM-PCA (<xref rid="btz328-B59" ref-type="bibr">You <italic>et al.</italic>, 2013</xref>), SVM-MCD (<xref rid="btz328-B60" ref-type="bibr">You <italic>et al.</italic>, 2014</xref>), MLP (<xref rid="btz328-B10" ref-type="bibr">Du <italic>et al.</italic>, 2017</xref>), Random Forest LPQ (RF-LPQ) (<xref rid="btz328-B55" ref-type="bibr">Wong <italic>et al.</italic>, 2015</xref>), SAE (<xref rid="btz328-B49" ref-type="bibr">Sun <italic>et al.</italic>, 2017</xref>), DNN-PPI (<xref rid="btz328-B26" ref-type="bibr">Li <italic>et al.</italic>, 2018</xref>) and DPPI (<xref rid="btz328-B14" ref-type="bibr">Hashemifar <italic>et al.</italic>, 2018</xref>). In addition, we report the results of a Siamese Residual GRU (SRGRU) architecture, which is a simplification of <monospace>PIPR</monospace>, where we discard all intermediary convolution layers and keep only the bidirectional residual GRU. The purpose of SRGRU is to show the significance of the contextualized and sequential information of protein profiles in characterizing PPIs. We also report the results of Siamese CNN (SCNN) by removing the residual GRU in <monospace>PIPR</monospace>. This degenerates our framework to a similar architecture to DPPI, but differs in that SCNN directly conducts an end-to-end training on raw sequences instead of requiring the protein profiles constructed by PSI-BLAST.</p>
      <p>We use AMSGrad (<xref rid="btz328-B42" ref-type="bibr">Reddi <italic>et al.</italic>, 2018</xref>) to optimize the cross-entropy loss, for which we set the learning rate α to 0.001, the exponential decay rates β<sub>1</sub> and β<sub>2</sub> to 0.9 and 0.999, and batch size to 256 on both datasets. The number of occurrences for the RCNN units (i.e. one convolution-pooling layer followed by one bidirectional residual GRU layer) is set to five, where we adopt three-max-pooling and the convolution kernel of size three. We set the hidden state size to be 50, and the RCNN output size to be 100. We set this configuration to ensure the RCNN to compress the selected features in a reasonably small vector sequence, before the features are aggregated by the last global average-pooling. We zero-pad short sequences to the longest sequence length in the dataset. This is a widely adopted technique for sequence modeling in NLP (<xref rid="btz328-B5" ref-type="bibr">Chen <italic>et al.</italic>, 2018</xref>; <xref rid="btz328-B15" ref-type="bibr">He <italic>et al.</italic>, 2015</xref>; <xref rid="btz328-B18" ref-type="bibr">Hu <italic>et al.</italic>, 2014</xref>; <xref rid="btz328-B58" ref-type="bibr">Yin <italic>et al.</italic>, 2016</xref>; <xref rid="btz328-B65" ref-type="bibr">Zhou <italic>et al.</italic>, 2017</xref>) as well as in bioinformatics (<xref rid="btz328-B32" ref-type="bibr">Min <italic>et al.</italic>, 2017</xref>; <xref rid="btz328-B35" ref-type="bibr">Müller <italic>et al.</italic>, 2018</xref>; <xref rid="btz328-B36" ref-type="bibr">Pan and Shen, 2018</xref>) for efficient training. Note that the configuration of embedding pre-training is discussed in Section 4.5, and the model configuration study of different hyperparameter values is provided in the <xref ref-type="supplementary-material" rid="sup1">Supplementary Material</xref>. All model variants are trained until converge at each fold of the cross-validation (CV).</p>
      <p><bold>Evaluation protocol.</bold> Following the settings in previous works (<xref rid="btz328-B14" ref-type="bibr">Hashemifar <italic>et al.</italic>, 2018</xref>; <xref rid="btz328-B45" ref-type="bibr">Shen <italic>et al.</italic>, 2007</xref>; <xref rid="btz328-B49" ref-type="bibr">Sun <italic>et al.</italic>, 2017</xref>; <xref rid="btz328-B60" ref-type="bibr">You <italic>et al.</italic>, 2014</xref>, <xref rid="btz328-B61" ref-type="bibr">2015</xref>), we conduct 5-fold CV on the Yeast dataset. Under the <italic>k</italic>-fold CV setting, the data are equally divided into <italic>k</italic> non-overlapping subsets, and each subset has a chance to train and to test the model so as to ensure an unbiased evaluation. We aggregate fix metrics on the test cases of each fold, i.e. the overall <italic>accuracy</italic>, <italic>precision</italic>, <italic>sensitivity</italic>, <italic>specificity</italic>, <italic>F1</italic> and <italic>Matthews correlation coefficient (MCC)</italic> on positive cases. All these metrics are preferred to be higher to indicate better performance. Based on the reported accuracy over 5-folds, we also conduct two-tailed Welch’s <italic>t</italic>-tests (<xref rid="btz328-B54" ref-type="bibr">Welch, 1947</xref>) to evaluate the significance of the improvement on different pairs of approaches. The <italic>P</italic>-values are adjusted by the Benjamini–Hochberg procedure (<xref rid="btz328-B3" ref-type="bibr">Benjamini and Hochberg, 1995</xref>) to control the false discovery rate for multiple hypothesis testing.</p>
      <p><bold>Results.</bold> As shown in <xref rid="btz328-T1" ref-type="table">Table 1</xref>, the CNN-based architecture, DPPI, demonstrates state-of-the-art performance over other baselines that employ statistical learning algorithms or densely connected MLP (We are unable to obtain the source codes of two deep-learning methods, SAE and DNN-PPI. We implement these two models following the descriptions in their papers. Our implementations are verified by achieving comparable performance on the Pan’s dataset (<xref rid="btz328-B37" ref-type="bibr">Pan <italic>et al.</italic>, 2010</xref>) as reported in the papers. However, these two implementations can only achieve 67.17 and 76.61% in overall accuracy respectively on the Yeast dataset.). This shows the superiority of deep-learning-based techniques in encapsulating various types of information of a protein pair, such as amino acid composition and their co-occurrences, and automatically extracting the robust ones for the learning objectives. That said, DPPI requires an extensive effort in data pre-processing, specifically in constructing the protein profile for each sequence. On average, each PSI-BLAST search of a protein against the NCBI non-redundant protein database (184 243 125 sequences) requires around 90 min of computation on our server. Even with eight cores, each search finishes in 15 min. We estimate that processing 2497 sequences of the Yeast dataset from scratch can take about 26 days. It is worth mentioning that <monospace>PIPR</monospace> only requires 8 s to pre-train the amino acid embedding, and 2.5 min to train on the Yeast dataset (<xref rid="btz328-T7" ref-type="table">Table 7</xref>). We implement SCNN to evaluate the performance of a simplified CNN architecture, which produces comparable results as DPPI. These two frameworks show that CNN can already leverage the significant features from primary protein sequences.</p>
      <table-wrap id="btz328-T1" orientation="portrait" position="float">
        <label>Table 1.</label>
        <caption>
          <p>Evaluation of binary PPI prediction on the Yeast dataset based on 5-fold cross-validation. We report the mean and SD for the test sets</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th rowspan="1" colspan="1">Accuracy (%)</th>
              <th rowspan="1" colspan="1">Precision (%)</th>
              <th rowspan="1" colspan="1">Sensitivity (%)</th>
              <th rowspan="1" colspan="1">Specificity (%)</th>
              <th rowspan="1" colspan="1">F1-score (%)</th>
              <th rowspan="1" colspan="1">MCC (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">SVM-AC</td>
              <td rowspan="1" colspan="1">87.35 ± 1.38</td>
              <td rowspan="1" colspan="1">87.82 ± 4.84</td>
              <td rowspan="1" colspan="1">87.30 ± 5.23</td>
              <td rowspan="1" colspan="1">87.41 ± 6.33</td>
              <td rowspan="1" colspan="1">87.34 ± 1.33</td>
              <td rowspan="1" colspan="1">75.09 ± 2.51</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">kNN-CTD</td>
              <td rowspan="1" colspan="1">86.15 ± 1.17</td>
              <td rowspan="1" colspan="1">90.24 ± 1.34</td>
              <td rowspan="1" colspan="1">81.03 ± 1.74</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
              <td rowspan="1" colspan="1">85.39 ± 1.51</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">EELM-PCA</td>
              <td rowspan="1" colspan="1">86.99 ± 0.29</td>
              <td rowspan="1" colspan="1">87.59 ± 0.32</td>
              <td rowspan="1" colspan="1">86.15 ± 0.43</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
              <td rowspan="1" colspan="1">86.86 ± 0.37</td>
              <td rowspan="1" colspan="1">77.36 ± 0.44</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SVM-MCD</td>
              <td rowspan="1" colspan="1">91.36 ± 0.4</td>
              <td rowspan="1" colspan="1">91.94 ± 0.69</td>
              <td rowspan="1" colspan="1">90.67 ± 0.77</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
              <td rowspan="1" colspan="1">91.3 ± 0.73</td>
              <td rowspan="1" colspan="1">84.21 ± 0.66</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MLP</td>
              <td rowspan="1" colspan="1">94.43 ± 0.3</td>
              <td rowspan="1" colspan="1">96.65 ± 0.59</td>
              <td rowspan="1" colspan="1">92.06 ± 0.36</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
              <td rowspan="1" colspan="1">94.3 ± 0.45</td>
              <td rowspan="1" colspan="1">88.97 ± 0.62</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RF-LPQ</td>
              <td rowspan="1" colspan="1">93.92 ± 0.36</td>
              <td rowspan="1" colspan="1">96.45 ± 0.45</td>
              <td rowspan="1" colspan="1">91.10 ± 0.31</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
              <td rowspan="1" colspan="1">93.7 ± 0.37</td>
              <td rowspan="1" colspan="1">88.56 ± 0.63</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SAE</td>
              <td rowspan="1" colspan="1">67.17 ± 0.62</td>
              <td rowspan="1" colspan="1">66.90 ± 1.42</td>
              <td rowspan="1" colspan="1">68.06 ± 2.50</td>
              <td rowspan="1" colspan="1">66.30 ± 2.27</td>
              <td rowspan="1" colspan="1">67.44 ± 1.08</td>
              <td rowspan="1" colspan="1">34.39 ± 1.25</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DNN-PPI</td>
              <td rowspan="1" colspan="1">76.61 ± 0.51</td>
              <td rowspan="1" colspan="1">75.1 ± 0.66</td>
              <td rowspan="1" colspan="1">79.63 ± 1.34</td>
              <td rowspan="1" colspan="1">73.59 ± 1.28</td>
              <td rowspan="1" colspan="1">77.29 ± 0.66</td>
              <td rowspan="1" colspan="1">53.32 ± 1.05</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DPPI</td>
              <td rowspan="1" colspan="1">94.55</td>
              <td rowspan="1" colspan="1">96.68</td>
              <td rowspan="1" colspan="1">92.24</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
              <td rowspan="1" colspan="1">94.41</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SRGRU</td>
              <td rowspan="1" colspan="1">93.77 ± 0.84</td>
              <td rowspan="1" colspan="1">94.60 ± 0.64</td>
              <td rowspan="1" colspan="1">92.85 ± 1.58</td>
              <td rowspan="1" colspan="1">94.69 ± 0.81</td>
              <td rowspan="1" colspan="1">93.71 ± 0.85</td>
              <td rowspan="1" colspan="1">87.56 ± 1.67</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SCNN</td>
              <td rowspan="1" colspan="1">95.03 ± 0.47</td>
              <td rowspan="1" colspan="1">95.51 ± 0.77</td>
              <td rowspan="1" colspan="1">94.51 ± 1.27</td>
              <td rowspan="1" colspan="1">95.55 ± 0.77</td>
              <td rowspan="1" colspan="1">95.00 ± 0.50</td>
              <td rowspan="1" colspan="1">90.08 ± 0.93</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <monospace>PIPR</monospace>
              </td>
              <td rowspan="1" colspan="1"><bold>97.09</bold> ± <bold>0.24</bold></td>
              <td rowspan="1" colspan="1"><bold>97.00</bold> ± <bold>0.65</bold></td>
              <td rowspan="1" colspan="1"><bold>97.17</bold> ± <bold>0.44</bold></td>
              <td rowspan="1" colspan="1">97.00 ± 0.67</td>
              <td rowspan="1" colspan="1"><bold>97.09</bold> ± <bold>0.23</bold></td>
              <td rowspan="1" colspan="1"><bold>94.17</bold> ± <bold>0.48</bold></td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tf1">
            <p>Each boldfaced number indicates the best of the corresponding metric.</p>
          </fn>
          <fn id="tf2">
            <p>NA, not available from the original paper.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>In addition, the SRGRU architecture has offered comparable performance to SCNN. This indicates that preserving the sequential and contextualized features of the protein sequences is as crucial as incorporating the local features. By integrating both significant local features and sequential information, <monospace>PIPR</monospace> outperforms DPPI by 2.54% in accuracy, 4.93% in sensitivity and 2.68% in F1-score. Next, we evaluate whether the improved accuracy of <monospace>PIPR</monospace> is statistically significant. <xref rid="btz328-T2" ref-type="table">Table 2</xref> reports the <italic>P</italic>-values of SRGRU, SCNN and <monospace>PIPR</monospace> compared to other baseline approaches, where the statistically significant comparisons (<italic>P</italic>-values &lt;0.01) are highlighted in red. Since the SD of DPPI is unavailable, we are not able to include DPPI in this analysis. The evaluation shows that <monospace>PIPR</monospace> performs statistically significantly better than all other approaches, including SCNN and SRGRU. On the other hand, SCNN is not statistically significantly better than SRGRU. Thus, the residual RCNN is very promising for modeling binary PPIs.</p>
      <table-wrap id="btz328-T2" orientation="portrait" position="float">
        <label>Table 2.</label>
        <caption>
          <p>Statistical assessment (<italic>t</italic>-test; two-tailed) on the accuracy of binary PPI prediction</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"><italic>P</italic>-value</th>
              <th rowspan="1" colspan="1">SRGRU</th>
              <th rowspan="1" colspan="1">SCNN</th>
              <th rowspan="1" colspan="1">
                <monospace>PIPR</monospace>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">SVM-AC</td>
              <td rowspan="1" colspan="1">9.69E-05</td>
              <td rowspan="1" colspan="1">1.22E-04</td>
              <td rowspan="1" colspan="1">9.69E-05</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">kNN-CTD</td>
              <td rowspan="1" colspan="1">1.03E-05</td>
              <td rowspan="1" colspan="1">2.23E-05</td>
              <td rowspan="1" colspan="1">2.84E-05</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">EELM-PCA</td>
              <td rowspan="1" colspan="1">2.33E-05</td>
              <td rowspan="1" colspan="1">3.94E-08</td>
              <td rowspan="1" colspan="1">2.43E-10</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SVM-MCD</td>
              <td rowspan="1" colspan="1">1.67E-03</td>
              <td rowspan="1" colspan="1">2.60E-06</td>
              <td rowspan="1" colspan="1">1.35E-07</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">MLP</td>
              <td rowspan="1" colspan="1">1.71E-01</td>
              <td rowspan="1" colspan="1">5.29E-02</td>
              <td rowspan="1" colspan="1">1.12E-06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">RF-LPQ</td>
              <td rowspan="1" colspan="1">7.28E-01</td>
              <td rowspan="1" colspan="1">4.10E-03</td>
              <td rowspan="1" colspan="1">1.75E-06</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SAE</td>
              <td rowspan="1" colspan="1">4.27E-10</td>
              <td rowspan="1" colspan="1">1.78E-10</td>
              <td rowspan="1" colspan="1">4.19E-09</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">DNN-PPI</td>
              <td rowspan="1" colspan="1">1.62E-08</td>
              <td rowspan="1" colspan="1">2.27E-10</td>
              <td rowspan="1" colspan="1">2.70E-09</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SRGRU</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
              <td rowspan="1" colspan="1">2.87E-02</td>
              <td rowspan="1" colspan="1">6.60E-04</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SCNN</td>
              <td rowspan="1" colspan="1">2.87E-02</td>
              <td align="center" rowspan="1" colspan="1">NA</td>
              <td rowspan="1" colspan="1">1.80E-04</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic>Note</italic>: The statistically significant differences are highlighted in red.</p>
          </fn>
          <fn id="tf3">
            <p>NA, not available.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>We also report the 5-fold CV performance of <monospace>PIPR</monospace> on variants of the multi-species dataset, where proteins are excluded based on different thresholds of sequence identity. The results in <xref rid="btz328-T3" ref-type="table">Table 3</xref> show that <monospace>PIPR</monospace> performs consistently well under lenient and stringent criteria of sequence identity between training and testing. More importantly, <monospace>PIPR</monospace> is able to train and test on multiple species, and is robust against extremely low sequence identity of &lt;1%.</p>
      <table-wrap id="btz328-T3" orientation="portrait" position="float">
        <label>Table 3.</label>
        <caption>
          <p>Evaluation of binary PPI prediction on variants of multi-species (<italic>C. elegans</italic>, <italic>D. melanogaster</italic> and <italic>E. coli</italic>) dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Seq. identity</th>
              <th rowspan="1" colspan="1"># of proteins</th>
              <th rowspan="1" colspan="1">Pos. pairs</th>
              <th rowspan="1" colspan="1">Neg. pairs</th>
              <th rowspan="1" colspan="1">Accuracy (%)</th>
              <th rowspan="1" colspan="1">F1-score (%)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Any</td>
              <td rowspan="1" colspan="1">11 529</td>
              <td rowspan="1" colspan="1">32 959</td>
              <td rowspan="1" colspan="1">32 959</td>
              <td rowspan="1" colspan="1">98.19</td>
              <td rowspan="1" colspan="1">98.17</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">&lt;0.40</td>
              <td rowspan="1" colspan="1">9739</td>
              <td rowspan="1" colspan="1">25 916</td>
              <td rowspan="1" colspan="1">22 012</td>
              <td rowspan="1" colspan="1">98.29</td>
              <td rowspan="1" colspan="1">98.28</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">&lt;0.25</td>
              <td rowspan="1" colspan="1">7790</td>
              <td rowspan="1" colspan="1">19 458</td>
              <td rowspan="1" colspan="1">15 827</td>
              <td rowspan="1" colspan="1">97.91</td>
              <td rowspan="1" colspan="1">98.08</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">&lt;0.10</td>
              <td rowspan="1" colspan="1">5769</td>
              <td rowspan="1" colspan="1">12 641</td>
              <td rowspan="1" colspan="1">9819</td>
              <td rowspan="1" colspan="1">97.54</td>
              <td rowspan="1" colspan="1">97.79</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">&lt;0.01</td>
              <td rowspan="1" colspan="1">5171</td>
              <td rowspan="1" colspan="1">10 747</td>
              <td rowspan="1" colspan="1">8065</td>
              <td rowspan="1" colspan="1">97.51</td>
              <td rowspan="1" colspan="1">97.80</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>4.3 Interaction type prediction</title>
      <p>The objective of this task is to predict the interaction type of two interacting proteins. We evaluate this task based on SHS27k and SHS148k datasets. To the best of our knowledge, much fewer efforts attempt for the multi-class PPI prediction in contrast to the binary prediction. <xref rid="btz328-B66" ref-type="bibr">Zhu <italic>et al.</italic> (2006)</xref> train a two-stage SVM classifier to distinguish obligate, non-obligate and crystal packing interactions; <xref rid="btz328-B46" ref-type="bibr">Silberberg <italic>et al.</italic> (2014)</xref> use logistic regression to predict several types of enzymatic actions. However, none of their implementations is publicly available. Different from the categories of interaction types used above, we aim at predicting the interaction types annotated by the STRING database.</p>
      <p>We train several statistical learning algorithms on the widely employed AC and CTD features for protein characterization as our baselines. These algorithms include SVM, Random Forest, AdaBoost [SAMME.R algorithm (<xref rid="btz328-B67" ref-type="bibr">Zhu <italic>et al.</italic>, 2009</xref>)], kNN classifier and logistic regression. For deep-learning-based approaches, we deploy the SCNN architecture where an output MLP with categorical cross-entropy loss is incorporated, as well as a similar SRGRU architecture into comparison. Results of two naïve baselines of random guessing and zero rule (i.e. simply predicting the majority class) are also reported for reference.</p>
      <p><bold>Evaluation protocol.</bold> All approaches are evaluated on the two datasets by 10-fold CV, using the same partition scheme for a more unbiased evaluation (<xref rid="btz328-B20" ref-type="bibr">James <italic>et al.</italic>, 2013</xref>; <xref rid="btz328-B30" ref-type="bibr">McLachlan <italic>et al.</italic>, 2005</xref>). We carry forward the model configurations from the last experiment to evaluate the performance of the frameworks under controlled variables. For baseline models, we examine three different ways of combining the feature vectors of the two input proteins, i.e. element-wise multiplication, the Manhattan difference [i.e. the absolute differences of corresponding features (<xref rid="btz328-B34" ref-type="bibr">Mueller and Thyagarajan, 2016</xref>)] and concatenation. The Manhattan difference consistently obtains better performance, considering the small values of the input features and the asymmetry of the captured protein relations.</p>
      <p><bold>Results.</bold> The prediction accuracy and fold changes over the zero rule baseline are reported in <xref rid="btz328-T4" ref-type="table">Table 4</xref>. Note that since the multi-class prediction task is much more challenging than the binary prediction task, it is expected to observe lower accuracy and longer training time (<xref rid="btz328-T7" ref-type="table">Table 7</xref>) than that reported in the previous experiment. Among all the baselines using explicit features, the CTD-based models perform better than the AC-based ones. CTD descriptors seek to cover both continuous and discontinuous interaction information (<xref rid="btz328-B56" ref-type="bibr">Yang <italic>et al.</italic>, 2010</xref>), which potentially better discriminate among PPI types.</p>
      <table-wrap id="btz328-T4" orientation="portrait" position="float">
        <label>Table 4.</label>
        <caption>
          <p>Accuracy (%) and fold changes over zero rule for PPI interaction type prediction on two STRING datasets based on 10-fold cross-validation</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Features</th>
              <th colspan="2" align="left" rowspan="1">N/A<hr/></th>
              <th colspan="5" align="left" rowspan="1">AC<hr/></th>
              <th colspan="5" align="left" rowspan="1">CTD<hr/></th>
              <th colspan="3" align="left" rowspan="1">Embedded raw seqs<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th align="left" rowspan="1" colspan="1">Rand</th>
              <th align="left" rowspan="1" colspan="1">Zero rule</th>
              <th align="left" rowspan="1" colspan="1">SVM</th>
              <th align="left" rowspan="1" colspan="1">RF</th>
              <th align="left" rowspan="1" colspan="1">AdaBoost</th>
              <th align="left" rowspan="1" colspan="1">kNN</th>
              <th align="left" rowspan="1" colspan="1">Logistic</th>
              <th align="left" rowspan="1" colspan="1">SVM</th>
              <th align="left" rowspan="1" colspan="1">RF</th>
              <th align="left" rowspan="1" colspan="1">AdaBoost</th>
              <th align="left" rowspan="1" colspan="1">kNN</th>
              <th align="left" rowspan="1" colspan="1">Logistic</th>
              <th align="left" rowspan="1" colspan="1">SCNN</th>
              <th align="left" rowspan="1" colspan="1">SRGRU</th>
              <th align="left" rowspan="1" colspan="1">
                <monospace>PIPR</monospace>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">SHS27k</td>
              <td rowspan="1" colspan="1">14.28</td>
              <td rowspan="1" colspan="1">16.70</td>
              <td rowspan="1" colspan="1">33.17</td>
              <td rowspan="1" colspan="1">44.82</td>
              <td rowspan="1" colspan="1">28.67</td>
              <td rowspan="1" colspan="1">35.44</td>
              <td rowspan="1" colspan="1">25.47</td>
              <td rowspan="1" colspan="1">35.56</td>
              <td rowspan="1" colspan="1">45.76</td>
              <td rowspan="1" colspan="1">31.81</td>
              <td rowspan="1" colspan="1">35.56</td>
              <td rowspan="1" colspan="1">30.57</td>
              <td rowspan="1" colspan="1">55.54</td>
              <td rowspan="1" colspan="1">51.06</td>
              <td rowspan="1" colspan="1">
                <bold>59.56</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(fold×)</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">1.00×</td>
              <td rowspan="1" colspan="1">1.99×</td>
              <td rowspan="1" colspan="1">2.68×</td>
              <td rowspan="1" colspan="1">1.72×</td>
              <td rowspan="1" colspan="1">2.12×</td>
              <td rowspan="1" colspan="1">1.52×</td>
              <td rowspan="1" colspan="1">2.13×</td>
              <td rowspan="1" colspan="1">2.74×</td>
              <td rowspan="1" colspan="1">1.90×</td>
              <td rowspan="1" colspan="1">2.13×</td>
              <td rowspan="1" colspan="1">1.83×</td>
              <td rowspan="1" colspan="1">3.33×</td>
              <td rowspan="1" colspan="1">3.06×</td>
              <td rowspan="1" colspan="1"><bold>3.57</bold>×</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">SHS148k</td>
              <td rowspan="1" colspan="1">14.28</td>
              <td rowspan="1" colspan="1">16.21</td>
              <td rowspan="1" colspan="1">28.17</td>
              <td rowspan="1" colspan="1">36.01</td>
              <td rowspan="1" colspan="1">27.87</td>
              <td rowspan="1" colspan="1">33.81</td>
              <td rowspan="1" colspan="1">24.96</td>
              <td rowspan="1" colspan="1">31.37</td>
              <td rowspan="1" colspan="1">36.65</td>
              <td rowspan="1" colspan="1">29.67</td>
              <td rowspan="1" colspan="1">33.13</td>
              <td rowspan="1" colspan="1">26.96</td>
              <td rowspan="1" colspan="1">55.29</td>
              <td rowspan="1" colspan="1">54.05</td>
              <td rowspan="1" colspan="1">
                <bold>61.91</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">(fold×)</td>
              <td rowspan="1" colspan="1">—</td>
              <td rowspan="1" colspan="1">1.00×</td>
              <td rowspan="1" colspan="1">1.74×</td>
              <td rowspan="1" colspan="1">2.22×</td>
              <td rowspan="1" colspan="1">1.72×</td>
              <td rowspan="1" colspan="1">2.09×</td>
              <td rowspan="1" colspan="1">1.54×</td>
              <td rowspan="1" colspan="1">1.94×</td>
              <td rowspan="1" colspan="1">2.26×</td>
              <td rowspan="1" colspan="1">1.83×</td>
              <td rowspan="1" colspan="1">2.04×</td>
              <td rowspan="1" colspan="1">1.66×</td>
              <td rowspan="1" colspan="1">3.41×</td>
              <td rowspan="1" colspan="1">3.33×</td>
              <td rowspan="1" colspan="1"><bold>3.82</bold>×</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tf4">
            <p>Each boldfaced number indicates the best of the corresponding metric.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>The best baseline using Random Forest thereof achieves satisfactory results by more than doubling the accuracy of zero rule on the smaller SHS27k dataset. However, on the larger SHS148k dataset, the accuracy of these explicit-feature-based models is notably impaired. We hypothesize that such predefined explicit features are not representative enough to distinguish the PPI types. On the other hand, the deep-learning-based approaches do not need to explicitly utilize these features, and perform consistently well in both settings. The raw sequence information is sufficient for these approaches to drastically outperform the Random Forest by at least 5.30% in accuracy on SHS27k and 17.40% in accuracy on SHS148k. SCNN thereof outperforms SRGRU by 4.48 and 1.24% in accuracy on SHS27k and SHS148k, respectively. This implies that the local interacting features are relatively more deterministic than contextualized and sequential features on this task. The results by the residual RCNN-based framework are very promising, as it outperforms SCNN by 4.02% and 6.62% in accuracy on SHS27k and SHS148k, respectively. It also remarkably outperforms the best explicit-feature-based baselines on the two datasets by 13.80 and 25.26% in accuracy, and more than 3.5 of fold changes over the zero rule on both datasets.</p>
    </sec>
    <sec>
      <title>4.4 Binding affinity estimation</title>
      <p>Lastly, we evaluate <monospace>PIPR</monospace> for binding affinity estimation using the SKEMPI dataset. We employ the mean squared loss variant of <monospace>PIPR</monospace> to address this regression task. Since the lengths of protein sequences in SKEMPI are much shorter than those in the other datasets, we accordingly reduce the occurrences of RCNN units to three, while other configurations remain unchanged. For baselines, we compare against several regression models based on the AC and CTD features, which include Bayesian Ridge regressor (BR), SVM, AdaBoost with decision tree regressors and Random Forest regressor. The corresponding features for two sequences are again combined via the Manhattan difference. We also modify SCNN and SRGRU to their mean squared loss variants, in which we reduce the layers in the same way of RCNN.</p>
      <p><bold>Evaluation protocol.</bold> We aggregate three metrics through 10-fold CV, i.e. <italic>mean squared error</italic> (<italic>MSE</italic>), <italic>mean absolute error</italic> (<italic>MAE</italic>) and <italic>Pearson’s correlation coefficient</italic> (<italic>Corr</italic>). These are three commonly reported metrics for regression tasks, for which lower <italic>MSE</italic> and <italic>MAE</italic> as well as higher <italic>Corr</italic> indicate better performance. In the CV process, we normalize the affinity values of the SKEMPI dataset to <inline-formula id="IE60"><mml:math id="IM59"><mml:mo stretchy="true">[</mml:mo><mml:mn>0,1</mml:mn><mml:mo stretchy="true">]</mml:mo></mml:math></inline-formula> via min–max re-scaling. (This is due to that we use sigmoid function to smooth the output of the regressor. Note that this process does not affect correlation, while MSE, MAE and the original affinity scores can be easily re-scaled back.).</p>
      <p><bold>Results.</bold><xref rid="btz328-T5" ref-type="table">Table 5</xref> reports the results for this experiment. It is noteworthy that, one single change of amino acid can lead to a drastic effect on binding affinity. While such subtle changes are difficult to be reflected by the explicit features, the deep-learning-based methods can competently capture such changes from the raw sequences. Our RCNN-based framework again offers the best performance among the deep-learning-based approaches, and significantly outperforms the best baseline (CTD-based Random Forest) by offering a 0.233 increase in <italic>Corr</italic>, as well as remarkably lower <italic>MSE</italic> and <italic>MAE</italic>. <xref ref-type="fig" rid="btz328-F3">Figure 3</xref> demonstrates an example of the effect of changing an amino acid in a protein complex. Tyrosine at position 61 of Chymotrypsin inhibitor 2 (Chain I) is substituted with Alanine, causing the neighboring region of Subtilisin BPN’ precursor (Chain E) to relax. The binding affinity (<italic>k<sub>d</sub></italic>) changes from 2.24E-12 to 2.70E-10, which is validly captured by <monospace>PIPR</monospace>. While our experiment is conducted on a relatively small dataset, we seek to extend our <monospace>PIPR</monospace> framework to a more generalized solution for binding affinity estimation, once a larger and more heterogeneous corpus is available.</p>
      <table-wrap id="btz328-T5" orientation="portrait" position="float">
        <label>Table 5.</label>
        <caption>
          <p>Results for binding affinity prediction on the SKEMPI dataset</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Features</th>
              <th colspan="4" rowspan="1">AC<hr/></th>
              <th colspan="4" rowspan="1">CTD<hr/></th>
              <th colspan="3" rowspan="1">Embedded raw seqs<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">Methods</th>
              <th rowspan="1" colspan="1">BR</th>
              <th rowspan="1" colspan="1">SVM</th>
              <th rowspan="1" colspan="1">RF</th>
              <th rowspan="1" colspan="1">AdaBoost</th>
              <th rowspan="1" colspan="1">BR</th>
              <th rowspan="1" colspan="1">SVM</th>
              <th rowspan="1" colspan="1">RF</th>
              <th rowspan="1" colspan="1">AdaBoost</th>
              <th rowspan="1" colspan="1">SCNN</th>
              <th rowspan="1" colspan="1">SRGRU</th>
              <th rowspan="1" colspan="1">
                <monospace>PIPR</monospace>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1"><italic>MSE</italic> (<inline-formula id="IE61"><mml:math id="IM60"><mml:mrow><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>)</td>
              <td rowspan="1" colspan="1">1.70</td>
              <td rowspan="1" colspan="1">2.20</td>
              <td rowspan="1" colspan="1">1.77</td>
              <td rowspan="1" colspan="1">1.98</td>
              <td rowspan="1" colspan="1">1.86</td>
              <td rowspan="1" colspan="1">1.84</td>
              <td rowspan="1" colspan="1">1.49</td>
              <td rowspan="1" colspan="1">1.84</td>
              <td rowspan="1" colspan="1">0.87</td>
              <td rowspan="1" colspan="1">0.95</td>
              <td rowspan="1" colspan="1">
                <bold>0.63</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><italic>MAE</italic> (<inline-formula id="IE62"><mml:math id="IM61"><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>)</td>
              <td rowspan="1" colspan="1">9.56</td>
              <td rowspan="1" colspan="1">11.81</td>
              <td rowspan="1" colspan="1">9.81</td>
              <td rowspan="1" colspan="1">11.15</td>
              <td rowspan="1" colspan="1">10.20</td>
              <td rowspan="1" colspan="1">11.04</td>
              <td rowspan="1" colspan="1">9.06</td>
              <td rowspan="1" colspan="1">10.69</td>
              <td rowspan="1" colspan="1">6.49</td>
              <td rowspan="1" colspan="1">7.08</td>
              <td rowspan="1" colspan="1">
                <bold>5.48</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic>Corr</italic>
              </td>
              <td rowspan="1" colspan="1">0.564</td>
              <td rowspan="1" colspan="1">0.353</td>
              <td rowspan="1" colspan="1">0.546</td>
              <td rowspan="1" colspan="1">0.451</td>
              <td rowspan="1" colspan="1">0.501</td>
              <td rowspan="1" colspan="1">0.501</td>
              <td rowspan="1" colspan="1">0.640</td>
              <td rowspan="1" colspan="1">0.508</td>
              <td rowspan="1" colspan="1">0.831</td>
              <td rowspan="1" colspan="1">0.812</td>
              <td rowspan="1" colspan="1">
                <bold>0.873</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic>Note</italic>: Each measurement is an average of the test sets over 10-fold cross-validation.</p>
          </fn>
          <fn id="tf5">
            <p>Each boldfaced number indicates the best of the corresponding metric.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <fig id="btz328-F3" orientation="portrait" position="float">
        <label>Fig. 3.</label>
        <caption>
          <p>Mutation effects on structure and binding affinity. The blue entity is Subtilisin BPN’ precursor (Chain E), and the red entity is Chymotrypsin inhibitor (Chain I). The mutation is highlighted in yellow. The wild-type (1TM1) and mutant (1TO1) complexes are retrieved from PDB</p>
        </caption>
        <graphic xlink:href="btz328f3"/>
      </fig>
    </sec>
    <sec>
      <title>4.5 Amino acid embeddings</title>
      <p>We further investigate the settings of amino acid embeddings in this subsection. Each amino acid is represented by a vector of numerical values that describe its relative physicochemical properties. The first part of the embedding vector<inline-formula id="IE63"><mml:math id="IM62"><mml:mi mathvariant="normal"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which measures the co-occurrence similarity of the amino acids in protein sequences, is empirically set as a five-dimensional vector. <inline-formula id="IE64"><mml:math id="IM63"><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is obtained by pre-training the Skip-Gram model on all 8000 sequences from our largest STRING dataset, SHS148k, using a context window size of seven and a negative sampling size of five. The second part contains a seven-dimensional vector, <inline-formula id="IE65"><mml:math id="IM64"><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ph</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, which describes the categorization of electrostaticity and hydrophobicity for the amino acid. We examine the performance of using each part individually, as well as the performance of combining them as used in our framework. In addition, we include a naïve one-hot vector representation, which does not consider the relatedness of amino acids and treats each of them independently. <xref rid="btz328-T6" ref-type="table">Table 6</xref> shows that, once we remove either of the two parts of the proposed embedding, the performance of the model slightly drops. Meanwhile, the proposed pre-trained embeddings lead to noticeably better performance of the model than adopting the naïve one-hot encodings of the canonical amino acids. This pre-training process completes in 8 s on a commodity workstation as shown in <xref rid="btz328-T7" ref-type="table">Table 7</xref>. This is a one-time effort that can be reused on different tasks and datasets.</p>
      <table-wrap id="btz328-T6" orientation="portrait" position="float">
        <label>Table 6.</label>
        <caption>
          <p>Comparison of amino acid representations based on binary prediction</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">
                <inline-formula id="IE66">
                  <mml:math id="IM65">
                    <mml:mo>[</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold">a</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>c</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi mathvariant="bold">a</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi mathvariant="italic">ph</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>]</mml:mo>
                  </mml:math>
                </inline-formula>
              </th>
              <th rowspan="1" colspan="1"><inline-formula id="IE67"><mml:math id="IM66"><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> only</th>
              <th rowspan="1" colspan="1"><inline-formula id="IE68"><mml:math id="IM67"><mml:msub><mml:mrow><mml:mi mathvariant="bold">a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">ph</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> only</th>
              <th rowspan="1" colspan="1">One-hot</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Dimension</td>
              <td rowspan="1" colspan="1">12</td>
              <td rowspan="1" colspan="1">5</td>
              <td rowspan="1" colspan="1">7</td>
              <td rowspan="1" colspan="1">20</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Accuracy</td>
              <td rowspan="1" colspan="1">
                <bold>97.09</bold>
              </td>
              <td rowspan="1" colspan="1">96.67</td>
              <td rowspan="1" colspan="1">96.03</td>
              <td rowspan="1" colspan="1">96.11</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Precision</td>
              <td rowspan="1" colspan="1">
                <bold>97.00</bold>
              </td>
              <td rowspan="1" colspan="1">96.35</td>
              <td rowspan="1" colspan="1">95.91</td>
              <td rowspan="1" colspan="1">96.34</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">F1-score</td>
              <td rowspan="1" colspan="1">
                <bold>97.09</bold>
              </td>
              <td rowspan="1" colspan="1">96.51</td>
              <td rowspan="1" colspan="1">96.08</td>
              <td rowspan="1" colspan="1">96.10</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <table-wrap id="btz328-T7" orientation="portrait" position="float">
        <label>Table 7.</label>
        <caption>
          <p>Run-time of training embeddings and different prediction tasks</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Task</th>
              <th rowspan="1" colspan="1">Embeddings</th>
              <th rowspan="1" colspan="1">Binary</th>
              <th rowspan="1" colspan="1">Multi-class</th>
              <th rowspan="1" colspan="1">Multi-class</th>
              <th rowspan="1" colspan="1">Regression</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">Dataset</td>
              <td rowspan="1" colspan="1">SHS148k</td>
              <td rowspan="1" colspan="1">Yeast</td>
              <td rowspan="1" colspan="1">SHS27k</td>
              <td rowspan="1" colspan="1">SHS148k</td>
              <td rowspan="1" colspan="1">SKEMPI</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Sample size</td>
              <td rowspan="1" colspan="1">8000</td>
              <td rowspan="1" colspan="1">11 188</td>
              <td rowspan="1" colspan="1">26 945</td>
              <td rowspan="1" colspan="1">148 051</td>
              <td rowspan="1" colspan="1">2 950</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Training time</td>
              <td rowspan="1" colspan="1">8 s</td>
              <td rowspan="1" colspan="1">2.5 min</td>
              <td rowspan="1" colspan="1">15.8 min</td>
              <td rowspan="1" colspan="1">138.3 min</td>
              <td rowspan="1" colspan="1">12.5 min</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec>
      <title>4.6 Run-time analysis</title>
      <p>All of the experiments are conducted on one NVIDIA GeForce GTX 1080 Ti GPU. We report the training time for each experiment, as well as for the amino acid embedding in <xref rid="btz328-T7" ref-type="table">Table 7</xref>. For each experiment, we calculate the average training time over either 5-fold (Yeast dataset) or 10-fold (others) CV. In both binary and multi-class predictions, the training time increases along with the increased number of training cases. The regression estimation generally requires more iterations per training case to converge than classification tasks. Thus, with much fewer cases, the training time on SKEMPI for affinity estimation is more than that on the Yeast dataset for binary prediction.</p>
    </sec>
  </sec>
  <sec>
    <title>5 Conclusion</title>
    <p>In this paper, we propose a novel end-to-end framework for PPI prediction based on the amino acid sequences. Our proposed framework, <monospace>PIPR</monospace>, employs a residual RCNN, which provides an automatic multi-granular feature selection mechanism to capture both local significant features and sequential features from the primary protein sequences. By incorporating the RCNN in a Siamese-based learning architecture, the framework captures effectively the mutual influence of protein pairs, and generalizes well to address different PPI prediction tasks without the need for predefined features. Extensive experimental evaluations on five datasets show promising performance of our framework on three challenging PPI prediction tasks. This also leads to significant amelioration over various baselines. Experiments on datasets of different sizes also demonstrate satisfactory scalability of the framework. For future work, one important direction is to apply the <monospace>PIPR</monospace> framework to other sequence-based inference tasks in bioinformatics, such as modeling RNA and protein interactions. We also seek to incorporate attention mechanisms (<xref rid="btz328-B52" ref-type="bibr">Vaswani <italic>et al.</italic>, 2017</xref>) to help pinpoint interaction sites on protein sequences, and apply <monospace>PIPR</monospace> to predict confidence of interactions in the form of ordinal regression. Since <monospace>PIPR</monospace> has alleviated any costly domain-invariant feature engineering process, how to extend <monospace>PIPR</monospace> with transfer learning based domain adaptation for different species is another meaningful direction.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material content-type="local-data" id="sup1">
      <label>btz328_Supplementary_Data</label>
      <media xlink:href="btz328_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We thank all of the reviewers for their valuable comments and suggestions.</p>
    <sec>
      <title>Funding</title>
      <p>This work was partially supported by the National Institutes of Health [R01GM115833, U54 GM114833]; and the National Science Foundation [DBI-1565137, DGE-1829071].</p>
      <p><italic>Conflict of Interest</italic>: none declared.</p>
    </sec>
  </ack>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btz328-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Altschul</surname><given-names>S.F.</given-names></name></person-group><etal>et al</etal> (<year>1997</year>) 
<article-title>Gapped BLAST and PSI-BLAST: a new generation of protein database search programs</article-title>. <source>Nucleic Acids Res</source>., <volume>25</volume>, <fpage>3389</fpage>–<lpage>3402</lpage>.<pub-id pub-id-type="pmid">9254694</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Anderson</surname><given-names>C.</given-names></name></person-group> (<year>2018</year>) 
<article-title>Google’s AI tool deepvariant promises significantly fewer genome errors</article-title>. <source>Clinical OMICs</source>, <volume>5</volume>, <fpage>33</fpage>–<lpage>33</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Benjamini</surname><given-names>Y.</given-names></name>, <name name-style="western"><surname>Hochberg</surname><given-names>Y.</given-names></name></person-group> (<year>1995</year>) 
<article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title>. <source>J. R. Stat. Soc. Series B (Methodol.)</source>, <volume>57</volume>, <fpage>289</fpage>–<lpage>300</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Berman</surname><given-names>H.M.</given-names></name></person-group><etal>et al</etal> (<year>2000</year>) 
<article-title>The protein data bank</article-title>. <source>Nucleic Acids Res</source>., <volume>28</volume>, <fpage>235</fpage>–<lpage>242</lpage>.<pub-id pub-id-type="pmid">10592235</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Neural article pair modeling for Wikipedia sub-article matching</article-title>. In: <source>ECML-PKDD</source>, pp. <fpage>3</fpage>–<lpage>19</lpage>. Springer, Cham. </mixed-citation>
    </ref>
    <ref id="btz328-B6">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Cho</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) Learning phrase representations using RNN Encoder–Decoder for statistical machine translation. In: <italic>Proceedings of conference on empirical methods in natural language processing</italic>, pp. 1724–1734. ACL, Doha, Qatar.</mixed-citation>
    </ref>
    <ref id="btz328-B7">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Conneau</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Very deep convolutional networks for text classification. In <italic>Proceedings of the European Chapter of the Association for Computational Linguistics</italic>, ACL, pp. 1107–1116.</mixed-citation>
    </ref>
    <ref id="btz328-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Consortium</surname><given-names>U.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>UniProt: the universal protein knowledgebase</article-title>. <source>Nucleic Acids Res</source>., <volume>46</volume>, <fpage>2699.</fpage><pub-id pub-id-type="pmid">29425356</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B9">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Dhingra</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Gated-attention readers for text comprehension. In <italic>Proceedings of ACL</italic>, ACL, Vancouver, Canada, pp. 1832–1846.</mixed-citation>
    </ref>
    <ref id="btz328-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Du</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>DeepPPI: boosting prediction of protein–protein interactions with deep neural networks</article-title>. <source>J. Chem. Inf. Model</source>., <volume>57</volume>, <fpage>1499</fpage>–<lpage>1510</lpage>.<pub-id pub-id-type="pmid">28514151</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fields</surname><given-names>S.</given-names></name>, <name name-style="western"><surname>Song</surname><given-names>O-K.</given-names></name></person-group> (<year>1989</year>) 
<article-title>A novel genetic system to detect protein–protein interactions</article-title>. <source>Nature</source>, <volume>340</volume>, <fpage>245</fpage>–<lpage>246</lpage>. <pub-id pub-id-type="pmid">2547163</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gavin</surname><given-names>A.-C.</given-names></name></person-group><etal>et al</etal> (<year>2002</year>) 
<article-title>Functional organization of the yeast proteome by systematic analysis of protein complexes</article-title>. <source>Nature</source>, <volume>415</volume>, <fpage>141</fpage>–<lpage>147</lpage>.<pub-id pub-id-type="pmid">11805826</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Using support vector machine combined with auto covariance to predict protein–protein interactions from protein sequences</article-title>. <source>Nucleic Acids Res</source>., <volume>36</volume>, <fpage>3025</fpage>–<lpage>3030</lpage>.<pub-id pub-id-type="pmid">18390576</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hashemifar</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Predicting protein–protein interactions through sequence-based deep learning</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i802</fpage>–<lpage>i810</lpage>.<pub-id pub-id-type="pmid">30423091</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Multi-perspective sentence similarity modeling with convolutional neural networks</article-title>. In: <source>Proceedings of the Conference on Empirical Methods in Natural Language Processing</source>, pp. <fpage>1576</fpage>–<lpage>1586</lpage>. ACL, Lisbon, Portugal.</mixed-citation>
    </ref>
    <ref id="btz328-B16">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>K.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <chapter-title>Deep residual learning for image recognition</chapter-title> In: <source>CVPR</source>, pp. <fpage>770</fpage>–<lpage>778</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ho</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2002</year>) 
<article-title>Systematic identification of protein complexes in <italic>Saccharomyces cerevisiae</italic> by mass spectrometry</article-title>. <source>Nature</source>, <volume>415</volume>, <fpage>180</fpage>–<lpage>183</lpage>.<pub-id pub-id-type="pmid">11805837</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B18">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>B.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) <chapter-title>Convolutional neural network architectures for matching natural language sentences</chapter-title> In: Ghahramani,Z. <italic>et al</italic> (eds) <italic>Advances in Neural Information Processing Systems 27</italic>. Curran Associates, Inc., pp. 2042–2050.</mixed-citation>
    </ref>
    <ref id="btz328-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>Y.-A.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Using weighted sparse representation model combined with discrete cosine transformation to predict protein–protein interactions from protein sequence</article-title>. <source>BioMed Res. Int</source>., <volume>2015</volume>, <fpage>902198</fpage>.<pub-id pub-id-type="pmid">26634213</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B20">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>James</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) <source>An Introduction to Statistical Learning</source>. Vol. <volume>112</volume>
<publisher-name>Springer, New York</publisher-name>. </mixed-citation>
    </ref>
    <ref id="btz328-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jansen</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2003</year>) 
<article-title>A Bayesian networks approach for predicting protein–protein interactions from genomic data</article-title>. <source>Science</source>, <volume>302</volume>, <fpage>449</fpage>–<lpage>453</lpage>.<pub-id pub-id-type="pmid">14564010</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B22">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>J.-Y.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) Learning to disentangle interleaved conversational threads with a Siamese hierarchical network and similarity ranking. In: <italic>Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies</italic>, Volume 1 (Long Papers), pp. 1812–1822. ACL, New Orleans, Louisiana.</mixed-citation>
    </ref>
    <ref id="btz328-B23">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) <chapter-title>Accurate image super-resolution using very deep convolutional networks</chapter-title> In: <source>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>, pp. <fpage>1646</fpage>–<lpage>1654</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B24">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>Y.</given-names></name></person-group> (<year>2014</year>) Convolutional neural networks for sentence classification. In: <italic>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, pp. 1746–1751. ACL, Doha, Qatar.</mixed-citation>
    </ref>
    <ref id="btz328-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>LeCun</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Deep learning</article-title>. <source>Nature</source>, <volume>521</volume>, <fpage>436</fpage>–<lpage>444</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Deep neural network based predictions of protein interactions using primary sequences</article-title>. <source>Molecules</source>, <volume>23</volume>, <fpage>1923.</fpage></mixed-citation>
    </ref>
    <ref id="btz328-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Godzik</surname><given-names>A.</given-names></name></person-group> (<year>2006</year>) 
<article-title>CD-HIT: a fast program for clustering and comparing large sets of protein or nucleotide sequences</article-title>. <source>Bioinformatics</source>, <volume>22</volume>, <fpage>1658</fpage>–<lpage>1659</lpage>.<pub-id pub-id-type="pmid">16731699</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B28">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Lin</surname><given-names>M.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) Network in network. In: <italic>International Conference on Learning Representation</italic>, Scottsdale, Arizona.</mixed-citation>
    </ref>
    <ref id="btz328-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maas</surname><given-names>A.L.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Rectifier nonlinearities improve neural network acoustic models</article-title>. In: <source>ICML Workshop on Deep Learning for Audio, Speech and Language Processing,</source> Vol. <volume>30</volume>, p. <fpage>3</fpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B30">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>McLachlan</surname><given-names>G.</given-names></name></person-group><etal>et al</etal> (<year>2005</year>) <source>Analyzing Microarray Gene Expression Data</source>. <volume>Vol. 422</volume>
<publisher-name>John Wiley &amp; Sons</publisher-name>, Hoboken, New Jersey.</mixed-citation>
    </ref>
    <ref id="btz328-B31">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Mikolov</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) <chapter-title>Distributed representations of words and phrases and their compositionality</chapter-title> In: Burges,C.J.C. (eds) <italic>Advances in Neural Information Processing Systems</italic> Curran Associates, Inc., pp. <fpage>3111</fpage>–<lpage>3119</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Min</surname><given-names>X.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Chromatin accessibility prediction via convolutional long short-term memory networks with k-mer embedding</article-title>. <source>Bioinformatics</source>, <volume>33</volume>, <fpage>i92</fpage>–<lpage>i101</lpage>.<pub-id pub-id-type="pmid">28881969</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Moal</surname><given-names>I.H.</given-names></name>, <name name-style="western"><surname>Fernández-Recio</surname><given-names>J.</given-names></name></person-group> (<year>2012</year>) 
<article-title>SKEMPI: a structural kinetic and energetic database of mutant protein interactions and its use in empirical models</article-title>. <source>Bioinformatics</source>, <volume>28</volume>, <fpage>2600</fpage>–<lpage>2607</lpage>.<pub-id pub-id-type="pmid">22859501</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mueller</surname><given-names>J.</given-names></name>, <name name-style="western"><surname>Thyagarajan</surname><given-names>A.</given-names></name></person-group> (<year>2016</year>) <chapter-title>Siamese recurrent architectures for learning sentence similarity</chapter-title> In: <source>Thirtieth AAAI Conference on Artificial Intelligence</source>, Vol. <volume>16</volume>, pp. <fpage>2786</fpage>–<lpage>2792</lpage>. AAAI Press, Menlo Park, CA.</mixed-citation>
    </ref>
    <ref id="btz328-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Müller</surname><given-names>A.T.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) 
<article-title>Recurrent neural network model for constructive peptide design</article-title>. <source>J. Chem. Inf. Model</source>., <volume>58</volume>, <fpage>472</fpage>–<lpage>479</lpage>.<pub-id pub-id-type="pmid">29355319</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B36">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>X.</given-names></name>, <name name-style="western"><surname>Shen</surname><given-names>H.-B.</given-names></name></person-group> (<year>2018</year>) 
<article-title>Predicting RNA–protein binding sites and motifs through combining local and global deep convolutional neural networks</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>3427</fpage>–<lpage>3436</lpage>.<pub-id pub-id-type="pmid">29722865</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pan</surname><given-names>X.-Y.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Large-scale prediction of human protein–protein interactions from amino acid sequence based on latent topic features</article-title>. <source>J. Proteome Res</source>., <volume>9</volume>, <fpage>4992</fpage>–<lpage>5001</lpage>.<pub-id pub-id-type="pmid">20698572</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Pascanu</surname><given-names>R.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) <chapter-title>On the difficulty of training recurrent neural networks</chapter-title> In: <source>Proceedings of the 30th International Conference on Machine Learning</source>, Atlanta, GA, USA, pp. <fpage>1310</fpage>–<lpage>1318</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Petta</surname><given-names>I.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Modulation of protein–protein interactions for the development of novel therapeutics</article-title>. <source>Mol. Ther</source>., <volume>24</volume>, <fpage>707</fpage>–<lpage>718</lpage>.<pub-id pub-id-type="pmid">26675501</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Philipp</surname><given-names>O.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>Path2PPI: an R package to predict protein–protein interaction networks for a set of proteins</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>1427</fpage>–<lpage>1429</lpage>.<pub-id pub-id-type="pmid">26733452</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B41">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Quang</surname><given-names>D.</given-names></name>, <name name-style="western"><surname>Xie</surname><given-names>X.</given-names></name></person-group> (<year>2016</year>) 
<article-title>DanQ: a hybrid convolutional and recurrent deep neural network for quantifying the function of DNA sequences</article-title>. <source>Nucleic Acids Res</source>., <volume>44</volume>, <fpage>e107</fpage>.<pub-id pub-id-type="pmid">27084946</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B42">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Reddi</surname><given-names>S.J.</given-names></name></person-group><etal>et al</etal> (<year>2018</year>) On the convergence of Adam and Beyond. In: <italic>International Conference on Learning Representations</italic>, pp. <fpage>1</fpage>–<lpage>23</lpage>. OpenReview, Amherst, MA.</mixed-citation>
    </ref>
    <ref id="btz328-B43">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Rocktäschel</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) Reasoning about entailment with neural attention. In: <italic>International Conference on Learning Representations (ICLR)</italic>, pp. <fpage>1</fpage>–<lpage>9</lpage>. OpenReview, Amherst, MA.</mixed-citation>
    </ref>
    <ref id="btz328-B44">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Salwinski</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2004</year>) 
<article-title>The database of interacting proteins: 2004 update</article-title>. <source>Nucleic Acids Res</source>., <volume>32</volume>, <fpage>D449</fpage>–<lpage>D451</lpage>.<pub-id pub-id-type="pmid">14681454</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B45">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2007</year>) 
<article-title>Predicting protein–protein interactions based only on sequences information</article-title>. <source>Proc. Natl. Acad. Sci. USA</source>, <volume>104</volume>, <fpage>4337</fpage>–<lpage>4341</lpage>.<pub-id pub-id-type="pmid">17360525</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B46">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Silberberg</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>A method for predicting protein-protein interaction types</article-title>. <source>PLoS One</source>, <volume>9</volume>, <fpage>e90904</fpage>.<pub-id pub-id-type="pmid">24625764</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B47">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Skrabanek</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2008</year>) 
<article-title>Computational prediction of protein–protein interactions</article-title>. <source>Mol. Biotechnol</source>., <volume>38</volume>, <fpage>1</fpage>–<lpage>17</lpage>.<pub-id pub-id-type="pmid">18095187</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B48">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Srinivasulu</surname><given-names>Y.S.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Characterizing informative sequence descriptors and predicting binding affinities of heterodimeric protein complexes</article-title>. <source>BMC Bioinformatics</source>, <volume>16</volume>, <fpage>S14.</fpage><pub-id pub-id-type="pmid">26681483</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B49">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Sequence-based prediction of protein–protein interaction using a deep-learning algorithm</article-title>. <source>BMC Bioinformatics</source>, <volume>18</volume>, <fpage>277</fpage>.<pub-id pub-id-type="pmid">28545462</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B50">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Szklarczyk</surname><given-names>D.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>The string database in 2017: quality-controlled protein–protein association networks, made broadly accessible</article-title>. <source>Nucleic Acids Res</source>., <volume>45</volume>, <fpage>D362</fpage>–<lpage>D368</lpage>.<pub-id pub-id-type="pmid">27924014</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B51">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Tai</surname><given-names>K.S.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) <chapter-title>Improved semantic representations from tree-structured long short-term memory networks</chapter-title> In: <italic>Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</italic> ACL, Beijing, China, pp. <fpage>1556</fpage>–<lpage>1566</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B52">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) Attention is all you need. In: Guyon,I. <italic>et al</italic> (eds) <italic>Advances in Neural Information Processing Systems</italic>. Curran Associates, Inc., pp. 5998–6008.</mixed-citation>
    </ref>
    <ref id="btz328-B53">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y.-B.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) 
<article-title>Predicting protein–protein interactions from protein sequences by a stacked sparse autoencoder deep neural network</article-title>. <source>Mol. Biosyst</source>., <volume>13</volume>, <fpage>1336</fpage>–<lpage>1344</lpage>.<pub-id pub-id-type="pmid">28604872</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B54">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Welch</surname><given-names>B.L.</given-names></name></person-group> (<year>1947</year>) 
<article-title>The generalization of Student’s problem when several different population variances are involved</article-title>. <source>Biometrika</source>, <volume>34</volume>, <fpage>28</fpage>–<lpage>35</lpage>.<pub-id pub-id-type="pmid">20287819</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B55">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Wong</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) <chapter-title>Detection of protein–protein interactions from amino acid sequences using a rotation forest model with a novel PR-LPQ descriptor</chapter-title> In: <source>Advanced Intelligent Computing Theories and Applications</source>. Springer, Cham, pp. <fpage>713</fpage>–<lpage>720</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B56">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>L.</given-names></name></person-group><etal>et al</etal> (<year>2010</year>) 
<article-title>Prediction of protein–protein interactions from protein sequence using local descriptors</article-title>. <source>Protein Pept. Lett</source>., <volume>17</volume>, <fpage>1085</fpage>–<lpage>1090</lpage>.<pub-id pub-id-type="pmid">20509850</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B57">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>W.</given-names></name>, <name name-style="western"><surname>Schütze</surname><given-names>H.</given-names></name></person-group> (<year>2015</year>) <chapter-title>Convolutional neural network for paraphrase identification</chapter-title> In: <source>Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</source>, pp. <fpage>901</fpage>–<lpage>911</lpage>. ACL, Denver, Colorado.</mixed-citation>
    </ref>
    <ref id="btz328-B58">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yin</surname><given-names>W.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>ABCNN: attention-based convolutional neural network for modeling sentence pairs</article-title>. <source>TACL</source>, <volume>4</volume>, <fpage>259</fpage>–<lpage>272</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B59">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>You</surname><given-names>Z.-H.</given-names></name></person-group><etal>et al</etal> (<year>2013</year>) 
<article-title>Prediction of protein–protein interactions from amino acid sequences with ensemble extreme learning machines and principal component analysis</article-title>. <source>BMC Bioinformatics</source>, <volume>14</volume>, <fpage>S10</fpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B60">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>You</surname><given-names>Z.-H.</given-names></name></person-group><etal>et al</etal> (<year>2014</year>) 
<article-title>Prediction of protein–protein interactions from amino acid sequences using a novel multi-scale continuous and discontinuous feature set</article-title>. <source>BMC Bioinformatics</source>, <volume>15</volume>, <fpage>S9.</fpage></mixed-citation>
    </ref>
    <ref id="btz328-B61">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>You</surname><given-names>Z.-H.</given-names></name></person-group><etal>et al</etal> (<year>2015</year>) 
<article-title>Predicting protein–protein interactions from primary protein sequences using a novel multi-scale local feature representation scheme and the Random Forest</article-title>. <source>PLoS One</source>, <volume>10</volume>, <fpage>e0125811</fpage>.<pub-id pub-id-type="pmid">25946106</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B62">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yugandhar</surname><given-names>K.</given-names></name>, <name name-style="western"><surname>Gromiha</surname><given-names>M.M.</given-names></name></person-group> (<year>2014</year>) 
<article-title>Protein–protein binding affinity prediction from amino acid sequence</article-title>. <source>Bioinformatics</source>, <volume>30</volume>, <fpage>3583</fpage>–<lpage>3589</lpage>.<pub-id pub-id-type="pmid">25172924</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B63">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>S.</given-names></name></person-group><etal>et al</etal> (<year>2016</year>) 
<article-title>A deep learning framework for modeling structural features of RNA-binding protein targets</article-title>. <source>Nucleic Acids Res</source>., <volume>44</volume>, <fpage>e32</fpage>.<pub-id pub-id-type="pmid">26467480</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B64">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <chapter-title>Very deep convolutional networks for end-to-end speech recognition</chapter-title> In: <source>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source>, pp. <fpage>4845</fpage>–<lpage>4849</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B65">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>T.</given-names></name></person-group><etal>et al</etal> (<year>2017</year>) <chapter-title>Attention-based natural language person retrieval</chapter-title> In: <source>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</source>, pp. <fpage>27</fpage>–<lpage>34</lpage>.</mixed-citation>
    </ref>
    <ref id="btz328-B66">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>H.</given-names></name></person-group><etal>et al</etal> (<year>2006</year>) 
<article-title>NOXclass: prediction of protein–protein interaction types</article-title>. <source>BMC Bioinformatics</source>, <volume>7</volume>, <fpage>27.</fpage><pub-id pub-id-type="pmid">16423290</pub-id></mixed-citation>
    </ref>
    <ref id="btz328-B67">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>J.</given-names></name></person-group><etal>et al</etal> (<year>2009</year>) 
<article-title>Multi-class AdaBoost</article-title>. <source>Stat. Interface</source>, <volume>2</volume>, <fpage>349</fpage>–<lpage>360</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
