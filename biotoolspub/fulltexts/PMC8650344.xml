<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8650344</article-id>
    <article-id pub-id-type="publisher-id">4493</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-021-04493-x</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>An efficient scRNA-seq dropout imputation method using graph attention network</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Xu</surname>
          <given-names>Chenyang</given-names>
        </name>
        <address>
          <email>cyxu1@buct.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Cai</surname>
          <given-names>Lei</given-names>
        </name>
        <address>
          <email>csuperlei@outlook.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1270-6257</contrib-id>
        <name>
          <surname>Gao</surname>
          <given-names>Jingyang</given-names>
        </name>
        <address>
          <email>gaojy@mail.buct.edu.cn</email>
        </address>
        <xref ref-type="aff" rid="Aff1"/>
      </contrib>
      <aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.48166.3d</institution-id><institution-id institution-id-type="ISNI">0000 0000 9931 8406</institution-id><institution>College of Information Science and Technology, </institution><institution>Beijing University of Chemical Technology, </institution></institution-wrap>Beijing, People’s Republic of China </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>7</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>7</day>
      <month>12</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2021</year>
    </pub-date>
    <volume>22</volume>
    <elocation-id>582</elocation-id>
    <history>
      <date date-type="received">
        <day>4</day>
        <month>5</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>11</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">Single-cell sequencing technology can address the amount of single-cell library data at the same time and display the heterogeneity of different cells. However, analyzing single-cell data is a computationally challenging problem. Because there are low counts in the gene expression region, it has a high chance of recognizing the non-zero entity as zero, which are called dropout events. At present, the mainstream dropout imputation methods cannot effectively recover the true expression of cells from dropout noise such as DCA, MAGIC, scVI, scImpute and SAVER.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In this paper, we propose an autoencoder structure network, named GNNImpute. GNNImpute uses graph attention convolution to aggregate multi-level similar cell information and implements convolution operations on non-Euclidean space on scRNA-seq data. Distinct from current imputation tools, GNNImpute can accurately and effectively impute the dropout and reduce dropout noise. We use mean square error (MSE), mean absolute error (MAE), Pearson correlation coefficient (PCC) and Cosine similarity (CS) to measure the performance of different methods with GNNImpute. We analyze four real datasets, and our results show that the GNNImpute achieves 3.0130 MSE, 0.6781 MAE, 0.9073 PCC and 0.9134 CS. Furthermore, we use Adjusted rand index (ARI) and Normalized mutual information (NMI) to measure the clustering effect. The GNNImpute achieves 0.8199 (ARI) and 0.8368 (NMI), respectively.</p>
      </sec>
      <sec>
        <title>Conclusions</title>
        <p id="Par3">In this investigation, we propose a single-cell dropout imputation method (GNNImpute), which effectively utilizes shared information for imputing the dropout of scRNA-seq data. We test it with different real datasets and evaluate its effectiveness in MSE, MAE, PCC and CS. The results show that graph attention convolution and autoencoder structure have great potential in single-cell dropout imputation.</p>
      </sec>
    </abstract>
    <kwd-group xml:lang="en">
      <title>Keywords</title>
      <kwd>scRNA-seq</kwd>
      <kwd>Dropout imputation</kwd>
      <kwd>Graph attention convolution</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Beijing Natural Science Foundation</institution>
        </funding-source>
        <award-id>5182018</award-id>
        <principal-award-recipient>
          <name>
            <surname>Gao</surname>
            <given-names>Jingyang</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2021</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par13">With the development of single-cell RNA sequencing (scRNA-seq) technology, it provides an easy way to process tens of thousands of single cells in parallel while providing gene expression data with single-cell-level resolution [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. The traditional RNA-seq technology cannot address complex tissues or organs at the cellular level because it measures the average expression of thousands of cells at the same time. Different from the traditional RNA-seq technology, scRNA-seq is widely used to study cell analysis, including cell heterogeneity [<xref ref-type="bibr" rid="CR4">4</xref>], cell subgroups clustering [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>] and cell development trajectories [<xref ref-type="bibr" rid="CR7">7</xref>]. Meanwhile, scRNA-seq technology can enhance the clinical diagnosis of the patient’s disease, and help doctors further customize treatment plans [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR8">8</xref>].</p>
    <p id="Par14">The scRNA-seq technology can produce single-cell-level resolution data. As a result of defects such as low capture rate and low sequencing depth, the sequencing library data contains a lot of noise [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>].</p>
    <p id="Par15">Compared with the next-generation sequencing data, scRNA-seq data usually contains a lot of zero expressions. These zero expressions can arise in two ways: One is that the genes are not expressed in the corresponding cells and the other is that some genes with low expression cannot be detected due to technical limitations. These events are called dropout events [<xref ref-type="bibr" rid="CR11">11</xref>]. There are some reasons for dropout, including nonlinear amplification of mRNA, transcription efficiency when reverse transcription of mRNA to cDNA and low sequencing read depth [<xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR13">13</xref>].</p>
    <p id="Par16">In the downstream analysis of scRNA-seq data, the dimensional reduction and unsupervised clustering are always be used to infer cell development trajectories and identify rare cell clusters [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>]. However, dropout events will seriously affect the calculation of the distance between expression profiles, which leads to downstream results [<xref ref-type="bibr" rid="CR16">16</xref>].</p>
    <p id="Par17">Recently, many methods have been developed to impute dropout in scRNA-seq data. For example, MAGIC [<xref ref-type="bibr" rid="CR17">17</xref>] is based on Markov affinity-based graph, which uses similar cells and genes information to impute missing values. However, this method lacks robustness and cannot adapt to the nonlinear relationship with genes. Furthermore, DCA [<xref ref-type="bibr" rid="CR18">18</xref>] is a neural network-based method that uses deep autoencoding networks for unsupervised learning. It performs zero-inflated negative binomial modeling on scRNA-seq data to solve the problem of noise and gene distribution. In addition, there are some imputation methods based on deep learning or statistical methods such as scVI [<xref ref-type="bibr" rid="CR19">19</xref>], scImpute [<xref ref-type="bibr" rid="CR20">20</xref>], and SAVER [<xref ref-type="bibr" rid="CR21">21</xref>]. These methods can only apply to Euclidean space by using Euclidean spatial data, such as the expression matrix. But they cannot directly deal with Non-Euclidean spatial data like cell graphs [<xref ref-type="bibr" rid="CR22">22</xref>–<xref ref-type="bibr" rid="CR25">25</xref>].</p>
    <p id="Par18">Therefore, we propose a novel structure neural network named GNNImpute, which is an autoencoder structure network that uses graph attention convolution. By building a graph from the scRNA-seq data, GNNImpute uses graph attention convolutional layer to make a targeted selection of similar neighboring nodes. Then, it aggregates these similar neighboring nodes. The nodes in the graph can continuously transmit messages along the edge direction until stability is reached. In this way, GNNImpute enables the expression of the cells in the same tissue area to be embedded in low-dimensional vectors through the autoencoder structure. GNNImpute can not only capture the co-expression patterns between similar cells but also remove sequencing technical noise from imputing dropout, which improves the downstream analysis of scRNA-seq data.</p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>The high-level approach</title>
      <p id="Par19">In scRNA-seq data, each cell has its own expression profile, and the expression profile of each cell is different and unique. But cells from the same tissue or with the same function usually have similar features. Therefore, when a dropout event occurs in any cell, it can be recovered by the gene expression profile of similar cells.</p>
      <p id="Par20">GNNImpute is a deep learning method based on a graph attention neural network. Different from MAGIC, GNNImpute introduces the attention mechanism that can assign weights to different similar cells according to attention coefficients. It can establish nonlinear relationships between genes by learning low-dimensional embedding of expressions through the autoencoder structure network. Compared with DCA, GNNImpute can learn the gene co-expression patterns of similar cells by aggregating information from multi-level neighbors. The co-expression patterns can help recover low-expressed genes. GNNImpute reduces the dropout noise and improves the gene expression profile of cells.</p>
      <p id="Par21">The overall structure of GNNImpute is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a. It is composed of an encoder and a decoder. Figure <xref rid="Fig1" ref-type="fig">1</xref>b shows that the encoder of GNNImpute has two graph attention convolutional layers, which are used to transmit the information of neighbor nodes. And the decoder consists of two linear layers. GNNImpute uses the masked expression matrix as the model input. The output of the model is used to calculate the loss value. And the parameters of the model are optimized by this value.<fig id="Fig1"><label>Fig. 1</label><caption><p>The structure of GNNImpute. <bold>a</bold> Shows the overall framework of the GNNImpute uses the network structure of the encoder and decoder. <bold>b</bold> Shows the encoder composed of two layers of graph attention convolutional layers</p></caption><graphic xlink:href="12859_2021_4493_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Data preprocessing</title>
      <p id="Par22">GNNImpute uses the expression matrix of scRNA-seq data as input. As shown in Table <xref rid="Tab1" ref-type="table">1</xref>, the expression matrix is an <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$nCells*nGenes$$\end{document}</tex-math><mml:math id="M2"><mml:mrow><mml:mi>n</mml:mi><mml:mi>C</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>n</mml:mi><mml:mi>G</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq1.gif"/></alternatives></inline-formula> scale. The rows represent different cells number, and the columns represent different gene sites. Each value in the matrix indicates the expression intensity of a gene in a cell. Due to the sparseness of scRNA-seq data, the expression matrix contains a very large number of zeros. When the expression values in row or column are all zero, it means that those cells or genes are no expression at all. We filtered these no expression values from the matrix, because these values may cause impurity interference and invalid information. Similarly, we filtered cells with overexpression in the matrix, which may be caused by incorrect counting or cell rupture after death.</p>
      <p id="Par23">In the data preprocessing, we use SCANPY [<xref ref-type="bibr" rid="CR26">26</xref>] to filter the original matrix. We address the data in four steps. In the first step, cells with expression values less than 200 and genes with expression values less than 3 are filtered. Second, we filter the cells with overexpression of mitochondrial genes [<xref ref-type="bibr" rid="CR27">27</xref>], as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a. Third, the cells with a high total expression count in the Fig. <xref rid="Fig2" ref-type="fig">2</xref>b should also be filtered. Finally, we normalize the filtered expression matrix, the purpose is to make each row (cell) in the expression matrix have the same value of expressions, and this value is the median of the values of expressions of all cells before normalization.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Expression matrix of PBMC dataset (intercepted)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">MCL1</th><th align="left">ENSA</th><th align="left">GOLPH3L</th><th align="left">HORMAD1</th><th align="left">CTSS</th><th align="left">CTSK</th><th align="left">ARNT</th><th align="left">SETDB1</th><th align="left">CERS2</th><th align="left">RP11-316M1.12</th><th align="left">RP11-316M1.3</th><th align="left">ANXA9</th><th align="left">FAM63A</th><th align="left">PRUNE</th><th align="left">BNIPL</th></tr></thead><tbody><tr><td align="left">AAACATTGCACTAG-1</td><td align="left">0</td><td align="left">2</td><td align="left">0</td><td align="left">0</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr><tr><td align="left">AAACATTGGCTAAC-1</td><td align="left">1</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">2</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left">0</td></tr></tbody></table></table-wrap></p>
      <p id="Par24">
        <fig id="Fig2">
          <label>Fig. 2</label>
          <caption>
            <p><bold>a</bold> shows the mitochondrial gene counts and total expression counts of the PBMC dataset. <bold>b</bold> shows the gene counts and total expression counts of the PBMC dataset. And the red boxs indicate outliers</p>
          </caption>
          <graphic xlink:href="12859_2021_4493_Fig2_HTML" id="MO2"/>
        </fig>
      </p>
    </sec>
    <sec id="Sec5">
      <title>Build connection graph</title>
      <p id="Par25">GNNImpute is a dropout imputation method based on a graph attention network. It is used to obtain gene expression from similar cells to recover the dropout event. In order to aggregate the cells with similar expressions, it is necessary to define a connection graph between the cells. In this graph, we use nodes to represent cells and we use edges to represent the similarity between cells. In this graph, cells can transmit information to adjacent cells. As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, the construction of such a graph is divided into three steps. The first step is to reduce the dimensionality of the expression matrix. Figure <xref rid="Fig3" ref-type="fig">3</xref>a shows the result of scRNA-seq data dimensionality reduction by Principal Component Analysis (PCA). After PCA, we can see that the cells are clustered according to similar expressions (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a). We select the first 50 principal components as the GNNImpute input. The second step is to calculate the Euclidean distance between every two cells in the expression matrix. As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b, we get a heat map of <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$nCells*nCells$$\end{document}</tex-math><mml:math id="M4"><mml:mrow><mml:mi>n</mml:mi><mml:mi>C</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mrow/><mml:mo>∗</mml:mo><mml:mi>n</mml:mi><mml:mi>C</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq2.gif"/></alternatives></inline-formula> scale. Heat map rows and columns represent different cells and the heat map color represent different cell distance. The color is deeper, the distance is closer. In the third step, we select <italic>K</italic> closest cells to construct graph edges. The <italic>K</italic> edges display a similar relation of cells. Through the above steps, we construct a cell-to-cell connection graph (K-nearest neighbor graph). We set the <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$K=5$$\end{document}</tex-math><mml:math id="M6"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq3.gif"/></alternatives></inline-formula> (The K number can be customized).</p>
      <p id="Par26">After constructing the graph, all graph cells have <italic>K</italic> cells with the most similar expression. We call these <italic>K</italic> cells the “first-level” neighbors. These K cells are adjacent to the origin cells. Similarly, there are also <italic>K</italic> cells in the “first-level” neighbors. We named the “second-level” cells. It doesn’t exist edges between origin cells and “second-level” cells, but the transferability of intermediate nodes can still indicate similarity. Figure <xref rid="Fig3" ref-type="fig">3</xref>c shows the origin cell and its neighbors. We use a two-layer graph convolution structure to transfer information within the range of “second-level” neighbors. This structure can not only maximize the aggregation of similar node information but also avoid over smooth node features.<fig id="Fig3"><label>Fig. 3</label><caption><p>There are three steps to construct a connection graph. First, <bold>a</bold> shows the result of visualization after dimensionality reduction by PCA. Second, <bold>b</bold> is the distance matrix represented by a heat map. Third, <bold>c</bold> represents the K-nearest neighbor graph after selecting <italic>K</italic> neighbors</p></caption><graphic xlink:href="12859_2021_4493_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
    <sec id="Sec6">
      <title>Multi-head graph attention convolutional layer</title>
      <p id="Par27">In order to aggregate the information of cells, we need a connection graph and graph attention convolutional layers. The essence of the graph convolutional layer is not to aggregate information around the original nodes, but aggregate nodes connected by edges. The calculation process of graph convolutional layers is as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} H^{(k+1)} = f(H^{(k)},A) = \sigma ({\hat{A}} H^{(k)} W^{(k)}) \end{aligned}$$\end{document}</tex-math><mml:math id="M8" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4493_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula>where <italic>k</italic> is the number of layers of graph convolution. <italic>W</italic> is the trainable weight. <inline-formula id="IEq4"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{A}} = {\tilde{D}}^{-\frac{1}{2}} {\tilde{A}} {\tilde{D}}^{-\frac{1}{2}}$$\end{document}</tex-math><mml:math id="M10"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq4.gif"/></alternatives></inline-formula>, Where <inline-formula id="IEq5"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{A}}$$\end{document}</tex-math><mml:math id="M12"><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq5.gif"/></alternatives></inline-formula> and <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{D}}$$\end{document}</tex-math><mml:math id="M14"><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq6.gif"/></alternatives></inline-formula> are the adjacency matrix and degree matrix of the cell-to-cell connection graph, respectively. The adjacency matrix <inline-formula id="IEq7"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{A}} = A + I$$\end{document}</tex-math><mml:math id="M16"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>+</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq7.gif"/></alternatives></inline-formula>, <italic>I</italic> is the identity matrix, which means add self-connections to the adjacency matrix. The degree matrix <inline-formula id="IEq8"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\tilde{D}} _{ii} = {\textstyle \sum _{j}^{}} {\tilde{A}}_{ij}$$\end{document}</tex-math><mml:math id="M18"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>D</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="italic">ii</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="false" scriptlevel="0"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow/></mml:msubsup></mml:mstyle><mml:msub><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq8.gif"/></alternatives></inline-formula>. <inline-formula id="IEq9"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><mml:math id="M20"><mml:mi>σ</mml:mi></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq9.gif"/></alternatives></inline-formula> is the activation function. ReLU is used as the activation function. <inline-formula id="IEq10"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{(k)}$$\end{document}</tex-math><mml:math id="M22"><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq10.gif"/></alternatives></inline-formula> is the input matrix of the k-th graph convolutional layer. When <inline-formula id="IEq11"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k = 0$$\end{document}</tex-math><mml:math id="M24"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq11.gif"/></alternatives></inline-formula>, <inline-formula id="IEq12"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H^{(k)} = X$$\end{document}</tex-math><mml:math id="M26"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq12.gif"/></alternatives></inline-formula>. In the GNNImpute, it is the expression matrix of scRNA-seq data.</p>
      <p id="Par28">Through the superposition of multiple graph convolutional layers, the information aggregation of multi-order neighbors can be achieved. We use two-layer graph convolution in the encoder, and the output of the encoder is as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} H^{(2)} = f(f(X,A),A)=ReLU({\hat{A}}ReLU({\hat{A}} XW^{(0)})W^{(1)}). \end{aligned}$$\end{document}</tex-math><mml:math id="M28" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>X</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4493_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula>To aggregate the information of neighbors more efficiently, we propose an attention model for neighbor nodes. By adding attention to neighbor nodes in the form of weights, this attention model achieves targeted aggregation of neighbor nodes. Specifically, the more similar the neighbor node is to the target node, the greater the attention coefficient obtained by the neighbor node. In this way, different weights are applied to different neighbors. The calculation of the attention coefficient is as follows:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} e_{ij} = a(\overrightarrow{h_i} ,\overrightarrow{h_j}) = W\overrightarrow{h_i} \cdot W\overrightarrow{h_j} \end{aligned}$$\end{document}</tex-math><mml:math id="M30" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>W</mml:mi><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>·</mml:mo><mml:mi>W</mml:mi><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4493_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq13"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{h_i}$$\end{document}</tex-math><mml:math id="M32"><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq13.gif"/></alternatives></inline-formula> and <inline-formula id="IEq14"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow{h_j}$$\end{document}</tex-math><mml:math id="M34"><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq14.gif"/></alternatives></inline-formula> represent the features of node <italic>i</italic> and node <italic>j</italic>, which is the gene expression profile of the cells. And <inline-formula id="IEq15"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$e_{ij}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq15.gif"/></alternatives></inline-formula> represents the attention coefficient of cell <italic>j</italic> to cell <italic>i</italic>. <italic>a</italic>() is the attention calculation formula. It is used to calculate the similarity of every two nodes. we use the dot product as the attention calculation formula to calculate the similarity. <italic>W</italic> is a shared weight matrix. It transforms the input features into more advanced features, so that each node can obtain sufficient expressive ability. We only calculate the attention coefficient of cell <italic>i</italic> and cell <italic>j</italic>, where <inline-formula id="IEq16"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$j \in N_i$$\end{document}</tex-math><mml:math id="M38"><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq16.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq17"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N_i$$\end{document}</tex-math><mml:math id="M40"><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq17.gif"/></alternatives></inline-formula> is the first-order neighbors of cell <italic>i</italic> in the cell-to-cell connection graph. Further calculation of multiple independent attention coefficients can be extended to the multi-head attention mechanism. In this attention mechanism, the attention coefficients are combined by calculate the average values to stabilize the learning process. The formula is as follows:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} {\overrightarrow{h_i}}' = \sigma \left( \frac{1}{K} \sum _{K=1}^{K} \sum _{K \in N_i}^{} a_{ij}^kW^k\overrightarrow{h_i}\right) . \end{aligned}$$\end{document}</tex-math><mml:math id="M42" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msup><mml:mrow><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mfenced close=")" open="("><mml:mfrac><mml:mn>1</mml:mn><mml:mi>K</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow/></mml:munderover><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4493_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula>In order to compare attention coefficients between different nodes, it is necessary to add the softmax function to standardize it:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} a_{ij} = softmax_j(e_{ij}) = \frac{exp(e_{ij})}{{\textstyle \sum _{k \in N_i}^{}}exp(e_{ij})}. \end{aligned}$$\end{document}</tex-math><mml:math id="M44" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow/></mml:msubsup></mml:mstyle><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4493_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula>Combining the above (<xref rid="Equ1" ref-type="">1</xref>), (<xref rid="Equ2" ref-type="">2</xref>), (<xref rid="Equ3" ref-type="">3</xref>), (<xref rid="Equ4" ref-type="">4</xref>), (<xref rid="Equ5" ref-type="">5</xref>) formula, the final attention weight displays as follows:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} a_{(i,j)} = \frac{exp(LeakyReLU(a(\overrightarrow{h_i},\overrightarrow{h_j})))}{ {\textstyle \sum _{k \in N_i}^{}exp(LeakyReLU(a(\overrightarrow{h_i},\overrightarrow{h_j})))}}. \end{aligned}$$\end{document}</tex-math><mml:math id="M46" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow/></mml:msubsup><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>k</mml:mi><mml:mi>y</mml:mi><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>h</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4493_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
    </sec>
    <sec id="Sec7">
      <title>Architecture and training</title>
      <p id="Par29">GNNImpute builds the model with an autoencoder (encoder and decoder). The input layer and output layer of the model have the same number of nodes. In the hidden layer, the nodes are much lower than encoder and decoder nodes. Different from the traditional autoencoder structure, we make an improvement on the GNNImpute encode layer. We use graph attention networks in the GNNImpute encode layer instead of linear networks. As shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>a, there is the autoencoder structure used by GNNImpute. In the encoder, the input size of the first layer is the number of gene features of the cell, and the first layer output is 512. The second input size is equal to the first layer output (512). GNNImpute decoder is composed of three parts: linear layer, batch normalization layer and ReLU.</p>
      <p id="Par30">Further, GNNImpute adds dropout layers to combat the over-fitting problem of the model. GNNImpute introduces a multi-head attention mechanism to achieve targeted selection when aggregating neighbor nodes. This multi-head attention mechanism can stabilize the learning process and provide robustness for the model. It is noted that GNNImpute uses a semi-supervised learning method to recover from dropout events. The advantage of semi-supervised learning is that some labeled cells can provide soft labels for many other unlabeled cells, and it can help the model recover from dropout events more accurately.</p>
      <p id="Par31">The model can learn the potential features by minimizing the error between the reconstructed expression matrix and the original expression matrix. Meanwhile, the hidden layer can capture the distribution of the matrix and ignore invalid changes in the low-dimensional environment.</p>
      <p id="Par32">Because the dropout event is random, there are few dropout benchmarks. Therefore, we used a fair measurement method [<xref ref-type="bibr" rid="CR28">28</xref>, <xref ref-type="bibr" rid="CR29">29</xref>]. This is a method of constructing a dropout benchmark by randomly masking the expression matrix. Using this fair measurement method can make various methods calculate the corresponding metrics. First, we process the expression matrix of the real scRNA-seq data to obtain the filtered matrix as the ground truth. Then, we randomly masked non-zeros based on a predetermined dropout rate. After two of the above steps, we can obtain the masked expression matrix and unmasked expression matrix. We can use the matrix data to train the GNNImpute model and validate the imputation effectiveness.</p>
      <p id="Par33">In the model training phase, we divide the PBMC dataset according to the ratio of 6:2:2. There are 1706 cells in the training set, 569 cells in the validation set, and 568 cells in the test set. Training set used to train the model, the validation set would be used to test the trained model, and the test set would evaluate the final model. The total parameters of GNNImpute will be adjusted according to the size of the dataset. When using the PBMC dataset, the total parameters of the model are 26.75 M. The loss function of the model is set to the mean square error loss function. The optimizer is Adam, and the learning rate is 0.0001. The maximum number of iterations is set to 3000. If the loss value of the validation set does not decrease in 200 consecutive iterations, it will interrupt training early. The training processes of the model are shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>b–d.<fig id="Fig4"><label>Fig. 4</label><caption><p><bold>a</bold> is the model structure of GNNImpute. <bold>b</bold> is the loss curve of GNNImpute training and validation. <bold>c</bold> is the PCC curve of GNNImpute training and validation. And <bold>d</bold> is the CS curve of GNNImpute training and validation</p></caption><graphic xlink:href="12859_2021_4493_Fig4_HTML" id="MO10"/></fig></p>
    </sec>
    <sec id="Sec8">
      <title>Evaluation metrics</title>
      <p id="Par34">In the experiment, we use four metrics to measure the imputation ability of GNNImpute with the other four methods. The four metrics are mean square error (MSE), mean absolute error (MSE), Pearson correlation coefficient (PCC) and Cosine similarity (CS), respectively. MSE and MAE are used to show whether the imputed gene expression values are the same as the labels. PCC and CS are used to measure whether the express trend of the imputed matrix is consistent with the raw matrix. In downstream data analysis, we employ the Adjusted rand index (ARI) and Normalized mutual information (NMI) to measure the clustering results.</p>
      <p id="Par35">MSE:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} MSE = \frac{1}{N}\sum _{i=1}^{N}(x_i-y_i)^2 \end{aligned}$$\end{document}</tex-math><mml:math id="M48" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4493_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq18"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M50"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq18.gif"/></alternatives></inline-formula> represents the imputed gene expression value, and <inline-formula id="IEq19"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><mml:math id="M52"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq19.gif"/></alternatives></inline-formula> represents the real gene expression value.</p>
      <p id="Par36">MAE:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="M53">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} MAE = \frac{1}{N}\sum _{i=1}^{N}\left| x_i-y_i \right| \end{aligned}$$\end{document}</tex-math><mml:math id="M54" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>M</mml:mi><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mfenced close="|" open="|"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4493_Article_Equ8.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq20"><alternatives><tex-math id="M55">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M56"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq20.gif"/></alternatives></inline-formula> represents the imputed gene expression value, and <inline-formula id="IEq21"><alternatives><tex-math id="M57">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><mml:math id="M58"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq21.gif"/></alternatives></inline-formula> represents the real gene expression value.</p>
      <p id="Par37">PCC:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="M59">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} r = \frac{{\textstyle \sum _{i=1}^{N}}(x_i - {\overline{x}})(y_i - {\overline{y}})}{\sqrt{ {\textstyle \sum _{i=1}^{N}(x-{\overline{x}})^2}{\textstyle \sum _{i=1}^{N}(y-{\overline{y}})^2} } } \end{aligned}$$\end{document}</tex-math><mml:math id="M60" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mover><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mover><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:mover><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mover><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:msqrt></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4493_Article_Equ9.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq22"><alternatives><tex-math id="M61">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_i$$\end{document}</tex-math><mml:math id="M62"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq22.gif"/></alternatives></inline-formula> represents the imputed gene expression value, <inline-formula id="IEq23"><alternatives><tex-math id="M63">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\overline{x}}$$\end{document}</tex-math><mml:math id="M64"><mml:mover><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq23.gif"/></alternatives></inline-formula> represents the average gene expression after imputation, <inline-formula id="IEq24"><alternatives><tex-math id="M65">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><mml:math id="M66"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq24.gif"/></alternatives></inline-formula> represents the real gene expression value, and <inline-formula id="IEq25"><alternatives><tex-math id="M67">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\overline{y}}$$\end{document}</tex-math><mml:math id="M68"><mml:mover><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq25.gif"/></alternatives></inline-formula> represents the real average gene expression.</p>
      <p id="Par38">CS:<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="M69">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} cos(\theta ) = \frac{A\cdot B}{\left\| A \right\| \left\| B \right\| } = \frac{ {\textstyle \sum _{i=1}^{n}} A_i \times B_i}{\sqrt{ {\textstyle \sum _{i=1}^{n}(A_i)^2}} \times \sqrt{ {\textstyle \sum _{i=1}^{n}}(B_i)^2 }} \end{aligned}$$\end{document}</tex-math><mml:math id="M70" display="block"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mo>·</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mfenced close="∥" open="∥"><mml:mi>A</mml:mi></mml:mfenced><mml:mfenced close="∥" open="∥"><mml:mi>B</mml:mi></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mstyle><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:msqrt><mml:mo>×</mml:mo><mml:msqrt><mml:mrow><mml:mstyle displaystyle="false" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math><graphic xlink:href="12859_2021_4493_Article_Equ10.gif" position="anchor"/></alternatives></disp-formula>where <italic>A</italic> and <italic>B</italic> represent the gene expression profile of the cell after imputation and the real gene expression profile of the cell respectively. And they are represented in the form of vectors. <inline-formula id="IEq26"><alternatives><tex-math id="M71">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$A_i$$\end{document}</tex-math><mml:math id="M72"><mml:msub><mml:mi>A</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq26.gif"/></alternatives></inline-formula> represents the expression value of the ith gene of the cell after imputation, and <inline-formula id="IEq27"><alternatives><tex-math id="M73">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$B_i$$\end{document}</tex-math><mml:math id="M74"><mml:msub><mml:mi>B</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="12859_2021_4493_Article_IEq27.gif"/></alternatives></inline-formula> represents the real expression value of the ith gene of the cell.</p>
    </sec>
    <sec id="Sec9">
      <title>Datasets</title>
      <p id="Par39">We use four different real datasets in experiments. The real datasets list as following: <list list-type="order"><list-item><p id="Par40">Human Frozen Peripheral Blood Mononuclear Cells (PBMCs), which from 10X GENOMICS. It contains 2900 cells and 32738 genes.</p></list-item><list-item><p id="Par41">Mouse Brain cells published by Campbell (GSE93374). It uses Drop-seq technology to perform single-cell analysis on brain cells of adult mice, which contains 21,086 cells and 26,774 genes.</p></list-item><list-item><p id="Par42">Mouse Brain cells published by Chen (GSE87544). It is the diversity analysis of mouse hypothalamic cells, which contains 14,437 cells and 23,284 genes.</p></list-item><list-item><p id="Par43">Mouse embryo cell analysis published by Klein (GSE65525). It contains 2717 cells and 24,021 genes.</p></list-item></list></p>
    </sec>
  </sec>
  <sec id="Sec10">
    <title>Result</title>
    <p id="Par44">In order to validate the dropout imputation performance of different methods, we compare GNNImpute results with other five methods, including DCA, MAGIC, scVI, scImpute and SAVER. DCA uses a zero-inflated negative binomial distribution model to denoise the autoencoder network. This denoising network can solve the problem of count distribution, over-dispersion and sparsity. MAGIC is an imputation method based on Markov affinity-based graph, which imputes dropout values by sharing information among similar cells. scVI can capture the basic low-dimensional structure in the scRNA-seq data by introducing a robust latent variable model, which can eliminate the noise in the data. scImpute is a statistical method that can automatically identify possible dropout events and recover them. It can also exclude outliers without introducing new bias. SAVER uses regularized regression prediction and empirical Bayesian methods to recover the gene expression profile in noisy and sparse data. We conduct experiments on four single-cell sequencing datasets of humans and mice. To illustrate the imputation performance of methods, we use dropout recovery index, clustering, robustness to evaluate the results.</p>
    <sec id="Sec11">
      <title>Imputation evaluation</title>
      <p id="Par45">By randomly masking the expression matrix on the four real datasets, we can obtain positive and negative training data. We compare the imputation performance of GNNImpute with the other five imputation methods using four real datasets. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows the performance of GNNImpute with the other five methods.</p>
      <p id="Par46">Overall, In the Fig. <xref rid="Fig5" ref-type="fig">5</xref>a, b, we can see the average MSE and MAE of GNNImpute can achieve 3.0130 and 0.6781. The results are better than DCA (3.0130 vs. 5.1888, and 0.6781 vs. 0.9036). The reason is that GNNImpute uses semi-supervised learning, which can learn from the labeled data to recover the dropout event. Since scVI is also a neural network method based on autoencoders, its performance is second only to DCA. For the scImpute, the MSE and MAE in the four datasets are the worst, because scImpute has an overall bias in the imputation. As shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>c, d, there are PCC and CS of GNNImpute and the other five methods in the four datasets. In PCC and CS, GNNImpute reaches the best result of 0.9073 and 0.9134 among all six methods. The performance is 8.69% and 8.71% better than the second place DCA (0.9073 vs. 0.8347, 0.9134 vs. 0.8402). It is because GNNImpute uses graph attention convolutional layer to aggregate information of similar cells. The performance of MAGIC, scImpute and SAVER in the four datasets are not stable. The average PCC and CS of MAGIC and SAVER on small datasets (PBMC, Klein) are 0.8226 and 0.5715 respectively. However, there are only 0.3146 and 0.2188 on the larger dataset (Chen, Campbell), which indicates that they cannot perform effective imputation on the large dataset. Another reason why scImpute may have an overall bias in imputation is that it has the worst performance in MSE and MAE but the PCC and CS are better than MAGIC and SAVER.<fig id="Fig5"><label>Fig. 5</label><caption><p><bold>a</bold> shows the MSE between the gene expression value after imputation and the real gene expression value. <bold>b</bold> shows the MAE between the gene expression value after imputation and the real gene expression value. <bold>c</bold> represents the PCC between the gene expression value after imputation and the real gene expression value. <bold>d</bold> represents the CS between the gene expression value after imputation and the real gene expression value</p></caption><graphic xlink:href="12859_2021_4493_Fig5_HTML" id="MO15"/></fig></p>
    </sec>
    <sec id="Sec12">
      <title>Heat map and clustering evaluation</title>
      <p id="Par47">The purpose of imputation is to improve the downstream analysis of scRNA-seq data. Therefore, we use clustering results to evaluate the downstream analysis. We use two metrics (ARI, NMI) to measure the performance of cell clustering after imputation.</p>
      <p id="Par48">In the clustering analysis, we used the data published by Klein. They analyzed mouse embryonic stem cells, revealing in detail the population structure and the heterogeneous onset of differentiation after leukemia inhibitory factor (LIF) withdrawal. The cluster labels are determined by the intervals of LIF withdrawal (0, 2, 4, 7 days). The t-distributed stochastic neighbor embedding (t-SNE) algorithm is used to reduce the dimension of the expression matrix. And it can realize the visual analysis of clustering. In Fig. <xref rid="Fig6" ref-type="fig">6</xref>a–c, we show the figures of the raw matrix, noised matrix and the denoised matrix after GNNImpute imputation. The dimensions of these matrices are all reduced by t-SNE for visualization. The visual analysis of the noised expression matrix in Fig. <xref rid="Fig6" ref-type="fig">6</xref>b shows that four cell clusters have different degrees of mixing, and there is no obvious dividing line. But the expression matrix imputed by GNNImpute can separate different clusters, as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>c. After imputing the matrices with different methods, we use k-means algorithm to measure the performance of matrix clustering. Then, we use ARI and NMI to measure the clustering results obtained by the k-means algorithm. GNNImpute reaches 0.8368 (ARI) and 0.8199 (NMI), which are at least 1.82% and 1.21% better than other methods (shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>d, e).</p>
      <p id="Par49">By calculating the gene heat map of all cells in the imputed expression matrix, the results can also be visualized to determine which methods can improve the downstream analysis of scRNA-seq data. Because the PBMC dataset does not have real cluster labels, we use Leiden algorithm to calculate pseudo labels. Then we find highly differentiated marker genes in each cluster by t-test based on the pseudo-labels. Finally, we select 50 marker genes most relevant to cluster classification in the PBMC dataset to measure the performance of GNNImpute and other five methods. Figure <xref rid="Fig6" ref-type="fig">6</xref>f, g shows the heat maps of the raw matrix and noised matrix. GNNImpute can recover the dropout events that occurred in different clusters, especially rare cell clusters (No. 7, No. 8 and No. 9 clusters, shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>h). The expression matrix after imputation by MAGIC shows that the large cell clusters are almost the same. The recovery of the dropout event is too smooth (such as LTB, RPS5 and CD74, shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>j). As a result, it lost the unique heterogeneity of scRNA-seq data. For scVI, it can only impute limited dropout values. The reason may be that low-expressed genes are mistaken for noise and ignored, such as RPL31 and RPS6 shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>k. scImpute can impute the dropout genes. But it changes the expression intensity of most genes, which further illustrates that scImpute imputes dropout values with a certain overall bias. The genes marked in Fig. <xref rid="Fig6" ref-type="fig">6</xref>l show that the expression intensity of these genes has been changed. SAVER does not perform obvious imputation. It may be that it cannot handle data with a high dropout rate.<fig id="Fig6"><label>Fig. 6</label><caption><p><bold>a</bold>–<bold>c</bold> show the visualizations of the raw matrix, noised matrix and denoised matrix after GNNImpute. <bold>d</bold>, <bold>e</bold> show the ARI and NMI of different methods. <bold>f</bold>, <bold>g</bold> show the heat maps of the raw expression matrix and noised matrix. <bold>h</bold>– <bold>m</bold> show the heat maps of the expression matrix imputed by GNNImpute and the other five methods</p></caption><graphic xlink:href="12859_2021_4493_Fig6_HTML" id="MO16"/></fig></p>
    </sec>
    <sec id="Sec13">
      <title>Robustness analysis under different dropout rates</title>
      <p id="Par50">Next, we evaluate the ability of the imputation method for scRNA-seq data under different dropout rates. The dropout rates are 10%, 20%, 30%, 40%, 50%, and 60%, using PBMC dataset with random mask expression matrix.</p>
      <p id="Par51">Figure <xref rid="Fig7" ref-type="fig">7</xref>a–d shows the performance of the six scRNA-seq dropout imputation methods under different dropout rates. From Fig. <xref rid="Fig7" ref-type="fig">7</xref> we can see that GNNImpute is not sensitive to the dropout rate. It can recover the most dropout events at a high dropout rate (60%). The MSE and MAE are 3.4783 and 0.8141. The PCC and CS are 0.9353 and 0.9438, respectively. After GNNImpute is DCA and MAGIC. Under different dropout rates, the MSE and MAE of DCA are hardly decrease. But the PCC and CS decrease by 1.7% and 1.4%. The MSE and MAE of MAGIC increased by 27.3% and 5.3%. And the PCC and CS decreased by 8.1% and 5.7%. The performance of scImpute and SAVER is in the middle. With the increase in the dropout rate, MSE and MAE show a clear increasing trend, while PCC and CS show a slowly decreasing trend. scVI is the most sensitive method for the dropout rate. MSE and MAE increased significantly (6.9536 to 17.0714 and 1.3264 to 1.8961). And both PCC and CS are decreased (0.8767 to 0.6332 and 0.8979 to 0.7017).<fig id="Fig7"><label>Fig. 7</label><caption><p><bold>a</bold> shows the MSE of six methods at different dropout rates. <bold>b</bold> shows the MAE of six methods at different dropout rates. <bold>c</bold> shows the PCC of six methods at different dropout rates. <bold>d</bold> shows the CS of six methods at different dropout rates</p></caption><graphic xlink:href="12859_2021_4493_Fig7_HTML" id="MO17"/></fig></p>
    </sec>
    <sec id="Sec14">
      <title>Analysis of different training sets for semi-supervised learning</title>
      <p id="Par52">GNNImpute uses a semi-supervised learning method to train the model and learn the dropout knowledge. The advantage of semi-supervised is that it can use only a small amount of labeled data and a large amount of unlabeled data for training, which greatly reduces the requirements for manually labeling data. In this experiment, we use 80 ~10% data with labels for model training. Even if only 10% labeled data is used, the model still shows great imputation performance, as shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. The MSE and MAE are 3.4685 and 0.8147, and the PCC and CS are 0.9351 and 0.9436, respectively.<fig id="Fig8"><label>Fig. 8</label><caption><p><bold>a</bold> shows the MSE and MAE of GNNImpute in different scales of training set. <bold>b</bold> shows the PCC and CS of GNNImpute in different scales of training set</p></caption><graphic xlink:href="12859_2021_4493_Fig8_HTML" id="MO18"/></fig></p>
    </sec>
    <sec id="Sec15">
      <title>Imputation performance in simulated data</title>
      <p id="Par53">To further evaluate the performance of GNNImpute, we evaluate the performance in simulated data. Following the previous work, we used the Splatter package [<xref ref-type="bibr" rid="CR30">30</xref>] to generate two simulation datasets. The first dataset has 2 groups, and the second dataset has 6 groups. Both simulation datasets contain expression matrices of 4000 cells and 20,000 genes. Table <xref rid="Tab2" ref-type="table">2</xref> shows the MSE, MAE, PCC and CS of GNNImpute and the other five methods in simulated dataset (2 groups). GNNImpute is better than other methods on MAE, PCC and CS. Only on MSE, our method is slightly inferior to DCA (21.2961 vs. 19.9282), but compared with the other four remaining methods, our method still has a significant improvement. When we evaluate in the simulated dataset (6 groups), we can get similar results, as shown in Table <xref rid="Tab3" ref-type="table">3</xref>. The reason for this phenomenon may be that the simulated data generated by the Splatter package is quite different from the real data. After calculation, we can confirm that the sparsity of the simulated data is much lower than the real data (simulated data: 0.47, PBMC: 0.94, Campbell: 0.89, Chen: 0.92, Klein: 0.66).<table-wrap id="Tab2"><label>Table 2</label><caption><p>Imputation performance in simulated data (2 groups)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">MSE</th><th align="left">MAE</th><th align="left">PCC</th><th align="left">CS</th></tr></thead><tbody><tr><td align="left">GNNImpute</td><td align="left">21.2961</td><td align="left"><bold>1.8432</bold></td><td align="left"><bold>0.9259</bold></td><td align="left"><bold>0.9344</bold></td></tr><tr><td align="left">DCA</td><td align="left"><bold>19.9282</bold></td><td align="left">1.8981</td><td align="left">0.9092</td><td align="left">0.9135</td></tr><tr><td align="left">MAGIC</td><td align="left">42.1510</td><td align="left">4.1618</td><td align="left">0.9022</td><td align="left">0.9068</td></tr><tr><td align="left">scVI</td><td align="left">149.9295</td><td align="left">2.7659</td><td align="left">0.6282</td><td align="left">0.6465</td></tr><tr><td align="left">scImpute</td><td align="left">99.0206</td><td align="left">3.1611</td><td align="left">0.7140</td><td align="left">0.7088</td></tr><tr><td align="left">SAVER</td><td align="left">102.1126</td><td align="left">3.3974</td><td align="left">0.7962</td><td align="left">0.8152</td></tr></tbody></table><table-wrap-foot><p>The bold values indicate the best or better scores that can be obtained through different methods under different indicators</p></table-wrap-foot></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption><p>Imputation performance in simulated data (6 groups)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">MSE</th><th align="left">MAE</th><th align="left">PCC</th><th align="left">CS</th></tr></thead><tbody><tr><td align="left">GNNImpute</td><td align="left">25.7855</td><td align="left"><bold>1.9330</bold></td><td align="left"><bold>0.9202</bold></td><td align="left"><bold>0.9295</bold></td></tr><tr><td align="left">DCA</td><td align="left"><bold>21.5326</bold></td><td align="left"><bold>1.9313</bold></td><td align="left">0.9072</td><td align="left">0.9113</td></tr><tr><td align="left">MAGIC</td><td align="left">45.0551</td><td align="left">4.2573</td><td align="left">0.8999</td><td align="left">0.9050</td></tr><tr><td align="left">scVI</td><td align="left">147.4127</td><td align="left">2.758</td><td align="left">0.6272</td><td align="left">0.6460</td></tr><tr><td align="left">scImpute</td><td align="left">99.4479</td><td align="left">3.1622</td><td align="left">0.7134</td><td align="left">0.7085</td></tr><tr><td align="left">SAVER</td><td align="left">101.7722</td><td align="left">3.3927</td><td align="left">0.7951</td><td align="left">0.8143</td></tr></tbody></table><table-wrap-foot><p>The bold values indicate the best or better scores that can be obtained through different methods under different indicators</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec16">
      <title>Analysis of attention mechanism of GNNImpute</title>
      <p id="Par54">In order to verify the effectiveness of the attention mechanism, we specially added experiments to evaluate our method and GCN architecture model (without attention). As shown in Tables <xref rid="Tab4" ref-type="table">4</xref> and <xref rid="Tab5" ref-type="table">5</xref>, we evaluated the performance of five models on the PBMC dataset and Klein dataset. They are GNNImpute (GCN architecture without attention), GNNImpute (with 1 attention head), GNNImpute (with 3 attention heads), GNNImpute (with 5 attention heads) and GNNImpute (with 8 attention heads). The experiment method uses five independent repeated experiments to take the average value. The results of PBMC dataset in Table <xref rid="Tab4" ref-type="table">4</xref> show that the performance of the model using the attention mechanism is all better than the GCN model (without attention). In the evaluation of MSE and MAE, the model using the attention mechanism is at least better than GCN model (without attention) by 12.9% (3.3047 vs. 2.8800) and 3.6% (0.8022 vs. 0.7736). And they are also better than GCN (without attention) on PCC and CS (0.9436 vs. 0.9478, 0.9510 vs. 0.9547). Table <xref rid="Tab5" ref-type="table">5</xref> shows the improvement of the clustering effect of the attention mechanism on the Klein dataset. In the evaluation of ARI, the attention mechanism provided about 2% (0.7998 vs. 0.8155). For the evaluation of NMI, it is also better than GCN (without attention) by 1.6% (0.8204 vs. 0.8049). We also observed that the performance of GNNImpute (with 1 attention head) is not stable, so we recommend using the multi-head attention mechanism to stabilize the performance of the model.<table-wrap id="Tab4"><label>Table 4</label><caption><p>Imputation performance of GNNImpute and GCN architecture model (PBMC dataset)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">MSE</th><th align="left">MAE</th><th align="left">PCC</th><th align="left">CS</th></tr></thead><tbody><tr><td align="left">GCN (without attention)</td><td align="left">3.3047</td><td align="left">0.8022</td><td align="left">0.9436</td><td align="left">0.9510</td></tr><tr><td align="left">GNNImpute (1 attention head)</td><td align="left">2.8800</td><td align="left">0.7736</td><td align="left">0.9478</td><td align="left">0.9547</td></tr><tr><td align="left">GNNImpute (3 attention heads)</td><td align="left"><bold>2.7767</bold></td><td align="left"><bold>0.7684</bold></td><td align="left"><bold>0.9494</bold></td><td align="left"><bold>0.9561</bold></td></tr><tr><td align="left">GNNImpute (5 attention heads)</td><td align="left">2.8748</td><td align="left">0.7730</td><td align="left">0.9480</td><td align="left">0.9550</td></tr><tr><td align="left">GNNImpute (8 attention heads)</td><td align="left">2.8494</td><td align="left">0.7699</td><td align="left">0.9482</td><td align="left">0.9551</td></tr></tbody></table><table-wrap-foot><p>The bold values indicate the best or better scores that can be obtained through different methods under different indicators</p></table-wrap-foot></table-wrap><table-wrap id="Tab5"><label>Table 5</label><caption><p>Imputation performance of GNNImpute and GCN architecture model (Klein dataset)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">MSE</th><th align="left">MAE</th><th align="left">PCC</th><th align="left">CS</th><th align="left">ARI</th><th align="left">NMI</th></tr></thead><tbody><tr><td align="left">GCN (without attention)</td><td align="left">6.7994</td><td align="left">0.8841</td><td align="left">0.8805</td><td align="left">0.8882</td><td align="left">0.7998</td><td align="left">0.8049</td></tr><tr><td align="left">GNNImpute (1 attention head)</td><td align="left">5.7850</td><td align="left">0.9059</td><td align="left">0.8779</td><td align="left">0.8867</td><td align="left">0.6945</td><td align="left">0.7232</td></tr><tr><td align="left">GNNImpute (3 attention heads)</td><td align="left">6.1119</td><td align="left">0.8725</td><td align="left">0.8867</td><td align="left">0.8942</td><td align="left">0.8183</td><td align="left">0.8204</td></tr><tr><td align="left">GNNImpute (5 attention heads)</td><td align="left"><bold>4.9420</bold></td><td align="left">0.8629</td><td align="left"><bold>0.8950</bold></td><td align="left"><bold>0.9024</bold></td><td align="left">0.8155</td><td align="left">0.8311</td></tr><tr><td align="left">GNNImpute (8 attention heads)</td><td align="left">6.4669</td><td align="left"><bold>0.8592</bold></td><td align="left">0.8921</td><td align="left">0.8988</td><td align="left"><bold>0.8202</bold></td><td align="left"><bold>0.8366</bold></td></tr></tbody></table><table-wrap-foot><p>The bold values indicate the best or better scores that can be obtained through different methods under different indicators</p></table-wrap-foot></table-wrap></p>
    </sec>
  </sec>
  <sec id="Sec17">
    <title>Discussion</title>
    <p id="Par55">On the high level, imputing the dropout of scRNA-seq data is a process of denoising the expression matrix data of the raw scRNA-seq library. Many existing approaches show that deep learning methods, especially autoencoders, can effectively denoise data.</p>
    <p id="Par56">Our GNNImpute method extends this high-level approach to the case of Non-Euclidean spatial data like cell graphs. By reconstructing the expression matrix of scRNA-seq data, GNNImpute can establish a learning mechanism between the input and output of the model. It allows our model to capture non-linear relationships between genes and make better utilization of data.</p>
    <p id="Par57">Usually, the dropout imputation performance is affected by the sparseness of scRNA-seq data. For noise data, it is usually more difficult to impute dropout values. However, GNNImpute shows excellent performance compared with other methods. GNNImpute compensates for the lack of low expression intensity of some genes by aggregating the features information of similar cells. Meanwhile, it can recover the dropout events in the scRNA-seq data and remain the specificity between cells to avoid excessive smoothing of expression.</p>
    <p id="Par58">Compared with other dropout imputation methods, GNNImpute has great adaptability for addressing the different sizes of datasets (especially large datasets). The difficulty in processing large datasets is that there are many cell types, and each cell contains a lot of gene information. Since GNNImpute uses a neural network model, it can capture important features from all information, and then reduce the dimensionality to ignore unimportant features.</p>
    <p id="Par59">Moreover, GNNImpute is a semi-supervised learning model, which does not require manually labeled data. It can be trained with few labeled data. This model only uses 10% of the dataset for training can still achieve great results.</p>
  </sec>
  <sec id="Sec18">
    <title>Conclusions</title>
    <p id="Par60">In this paper, a novel imputation method based on graph attention convolution is proposed, which is a semi-supervised learning method using an autoencoding structure network. GNNImpute focuses on determining the similarity between cells and constructs a connection graph to capture the features of similar cells. This method also introduces an attention mechanism of weighted neighbor nodes to select the cell node with the most useful features information. In the experiments of four datasets, the performance of GNNImpute is better than other existing methods for four metrics of MSE, MAE, PCC and CS. When we explore the limits of GNNImpute, we find that it cannot provide the interpretability of cell clusters. Future investigations will focus on how to make GNNImpute more explainable.</p>
  </sec>
</body>
<back>
  <app-group>
    <app id="App1">
      <sec id="Sec19">
        <title>Appendix</title>
      </sec>
    </app>
    <app id="App2">
      <sec id="Sec20">
        <title>Implementation</title>
        <p id="Par64">GNNImpute is implemented in Python 3 using deep learning framework PyTorch Geometric [<xref ref-type="bibr" rid="CR30">30</xref>]. Training on CPU or GPU is supported using PyTorch and PyTorch Geometric.</p>
      </sec>
    </app>
    <app id="App3">
      <sec id="Sec21">
        <title>Simulated scRNA-seq data</title>
        <p id="Par65">We used the Splatter [<xref ref-type="bibr" rid="CR31">31</xref>] package to generate simulation datasets. The parameters used in the generated two sets of simulation data are as follows. For two group simulation: nGroup = 2, dropout.mid = 5, dropout.shape = − 1, dropout.type = “experiment”, de.facScale = 0.25, nGenes = 20000, batchCells = 4000. For six group simulation: nGroup = 6, dropout.mid = 5, dropout.shape = − 1, dropout.type = “experiment”, de.facScale = 0.25, nGenes = 20000, batchCells = 4000.</p>
      </sec>
    </app>
  </app-group>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>scRNA-seq</term>
        <def>
          <p id="Par4">Single-cell RNA sequencing</p>
        </def>
      </def-item>
      <def-item>
        <term>RNA-seq</term>
        <def>
          <p id="Par5">RNA sequencing</p>
        </def>
      </def-item>
      <def-item>
        <term>PCA</term>
        <def>
          <p id="Par6">Principal component analysis</p>
        </def>
      </def-item>
      <def-item>
        <term>RuLU</term>
        <def>
          <p id="Par7">Rectified linear unit</p>
        </def>
      </def-item>
      <def-item>
        <term>MSE</term>
        <def>
          <p id="Par8">Mean square error</p>
        </def>
      </def-item>
      <def-item>
        <term>MAE</term>
        <def>
          <p id="Par9">Mean absolute error</p>
        </def>
      </def-item>
      <def-item>
        <term>ARI</term>
        <def>
          <p id="Par10">Adjusted rand index</p>
        </def>
      </def-item>
      <def-item>
        <term>NMI</term>
        <def>
          <p id="Par11">Normalized mutual information</p>
        </def>
      </def-item>
      <def-item>
        <term>t-SNE</term>
        <def>
          <p id="Par12">t-distributed stochastic neighbor embedding</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>Not applicable.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>Conceived and designed the experiments: CX, JG, Performed the experiments: CX, LC, Analyzed the data: CX, LC. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>Project supported by Beijing Natural Science Foundation (5182018).</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The datasets used in this study are publicly available. Single-cell library data and raw count expression matrices of PBMCs are downloaded from 10X GENOMICS (<ext-link ext-link-type="uri" xlink:href="https://www.10xgenomics.com/resources/datasets/frozen-pbm-cs-donor-a-1-standard-1-1-0">https://www.10xgenomics.com/resources/datasets/frozen-pbm-cs-donor-a-1-standard-1-1-0</ext-link>). The mouse brain cell data released by Campbell is available at Gene Expression Omnibus (GEO) under accession code GSE93374. The single-cell data and expression matrix data of mouse brain cells published by Chen are available in GEO under accession code GSE87544. The mouse embryo single-cell data published by Klein was downloaded from GEO, and the accession code is GSE65525. The source code in this paper is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/Lav-i/GNNImpute">https://github.com/Lav-i/GNNImpute</ext-link>.</p>
  </notes>
  <notes>
    <title>Declarations</title>
    <notes id="FPar2">
      <title>Ethics approval and consent to participate</title>
      <p id="Par61">Not applicable.</p>
    </notes>
    <notes id="FPar3">
      <title>Consent for publication</title>
      <p id="Par62">Not applicable.</p>
    </notes>
    <notes id="FPar4" notes-type="COI-statement">
      <title>Competing interests</title>
      <p id="Par63">The authors declare that they have no competing interests.</p>
    </notes>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zeisel</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Muñoz-Manchado</surname>
            <given-names>AB</given-names>
          </name>
          <name>
            <surname>Codeluppi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Lönnerberg</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>La Manno</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Juréus</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Marques</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Munguba</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>He</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Betsholtz</surname>
            <given-names>C</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cell types in the mouse cortex and hippocampus revealed by single-cell rna-seq</article-title>
        <source>Science</source>
        <year>2015</year>
        <volume>347</volume>
        <issue>6226</issue>
        <fpage>1138</fpage>
        <lpage>1142</lpage>
        <?supplied-pmid 25700174?>
        <pub-id pub-id-type="pmid">25700174</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <mixed-citation publication-type="other">Villani A-C, Satija R, Reynolds G, Sarkizova S, Shekhar K, Fletcher J, Griesbeck M, Butler A, Zheng S, Lazo S, et al. Single-cell RNA-Seq reveals new types of human blood dendritic cells, monocytes, and progenitors. Science. 2017;356(6335).</mixed-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Ning</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Shi</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Single-cell RNA-Seq technologies and related computational data analysis</article-title>
        <source>Front Genet</source>
        <year>2019</year>
        <volume>10</volume>
        <fpage>317</fpage>
        <?supplied-pmid 31024627?>
        <pub-id pub-id-type="pmid">31024627</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Cao</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Packer</surname>
            <given-names>JS</given-names>
          </name>
          <name>
            <surname>Ramani</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Cusanovich</surname>
            <given-names>DA</given-names>
          </name>
          <name>
            <surname>Huynh</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Daza</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Qiu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Furlan</surname>
            <given-names>SN</given-names>
          </name>
          <name>
            <surname>Steemers</surname>
            <given-names>FJ</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Comprehensive single-cell transcriptional profiling of a multicellular organism</article-title>
        <source>Science</source>
        <year>2017</year>
        <volume>357</volume>
        <issue>6352</issue>
        <fpage>661</fpage>
        <lpage>667</lpage>
        <?supplied-pmid 5894354?>
        <pub-id pub-id-type="pmid">28818938</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Stephenson</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Donlin</surname>
            <given-names>LT</given-names>
          </name>
          <name>
            <surname>Butler</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Rozo</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Bracken</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Rashidfarrokhi</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Goodman</surname>
            <given-names>SM</given-names>
          </name>
          <name>
            <surname>Ivashkiv</surname>
            <given-names>LB</given-names>
          </name>
          <name>
            <surname>Bykerk</surname>
            <given-names>VP</given-names>
          </name>
          <name>
            <surname>Orange</surname>
            <given-names>DE</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Single-cell rna-seq of rheumatoid arthritis synovial tissue using low-cost microfluidic instrumentation</article-title>
        <source>Nat Commun</source>
        <year>2018</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="pmid">29317637</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <mixed-citation publication-type="other">Keren-Shaul H, Spinrad A, Weiner A, Matcovitch-Natan O, Dvir-Szternfeld R, Ulland TK, David E, Baruch K, Lara-Astaiso D, Toth B, et al. A unique microglia type associated with restricting development of Alzheimer’s disease. Cell. 2017;169(7):1276–90.</mixed-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Moignard</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Woodhouse</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Haghverdi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Lilly</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Tanaka</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wilkinson</surname>
            <given-names>AC</given-names>
          </name>
          <name>
            <surname>Buettner</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Macaulay</surname>
            <given-names>IC</given-names>
          </name>
          <name>
            <surname>Jawaid</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Diamanti</surname>
            <given-names>E</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Decoding the regulatory network of early blood development from single-cell gene expression measurements</article-title>
        <source>Nat Biotechnol</source>
        <year>2015</year>
        <volume>33</volume>
        <issue>3</issue>
        <fpage>269</fpage>
        <lpage>276</lpage>
        <?supplied-pmid 25664528?>
        <pub-id pub-id-type="pmid">25664528</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Potter</surname>
            <given-names>SS</given-names>
          </name>
        </person-group>
        <article-title>Single-cell RNA sequencing for the study of development, physiology and disease</article-title>
        <source>Nat Rev Nephrol</source>
        <year>2018</year>
        <volume>14</volume>
        <issue>8</issue>
        <fpage>479</fpage>
        <lpage>492</lpage>
        <?supplied-pmid 29789704?>
        <pub-id pub-id-type="pmid">29789704</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Van Buren</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Y</given-names>
          </name>
        </person-group>
        <article-title>Dropout imputation and batch effect correction for single-cell RNA sequencing data</article-title>
        <source>J Bio-X Res</source>
        <year>2019</year>
        <volume>2</volume>
        <issue>4</issue>
        <fpage>169</fpage>
        <lpage>177</lpage>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Luecken</surname>
            <given-names>MD</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>Current best practices in single-cell rna-seq analysis: a tutorial</article-title>
        <source>Mol Syst Biol</source>
        <year>2019</year>
        <volume>15</volume>
        <issue>6</issue>
        <fpage>8746</fpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kharchenko</surname>
            <given-names>PV</given-names>
          </name>
          <name>
            <surname>Silberstein</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Scadden</surname>
            <given-names>DT</given-names>
          </name>
        </person-group>
        <article-title>Bayesian approach to single-cell differential expression analysis</article-title>
        <source>Nat Methods</source>
        <year>2014</year>
        <volume>11</volume>
        <issue>7</issue>
        <fpage>740</fpage>
        <lpage>742</lpage>
        <?supplied-pmid 24836921?>
        <pub-id pub-id-type="pmid">24836921</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lun</surname>
            <given-names>AT</given-names>
          </name>
          <name>
            <surname>Bach</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Marioni</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>Pooling across cells to normalize single-cell rna sequencing data with many zero counts</article-title>
        <source>Genome Biol</source>
        <year>2016</year>
        <volume>17</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="pmid">26753840</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Vallejos</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Risso</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Scialdone</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dudoit</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Marioni</surname>
            <given-names>JC</given-names>
          </name>
        </person-group>
        <article-title>Normalizing single-cell rna sequencing data: challenges and opportunities</article-title>
        <source>Nat Methods</source>
        <year>2017</year>
        <volume>14</volume>
        <issue>6</issue>
        <fpage>565</fpage>
        <?supplied-pmid 28504683?>
        <pub-id pub-id-type="pmid">28504683</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sun</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Accuracy, robustness and scalability of dimensionality reduction methods for single-cell rna-seq analysis</article-title>
        <source>Genome Biol</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>21</lpage>
        <pub-id pub-id-type="pmid">30606230</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Long, J., Xia, Y.: Cluster analysis of high-dimensional SCRNA sequencing data (2019). arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1912.08400">arXiv:1912.08400</ext-link>.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hicks</surname>
            <given-names>SC</given-names>
          </name>
          <name>
            <surname>Townes</surname>
            <given-names>FW</given-names>
          </name>
          <name>
            <surname>Teng</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Irizarry</surname>
            <given-names>RA</given-names>
          </name>
        </person-group>
        <article-title>Missing data and technical variability in single-cell rna-sequencing experiments</article-title>
        <source>Biostatistics</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>4</issue>
        <fpage>562</fpage>
        <lpage>578</lpage>
        <?supplied-pmid 29121214?>
        <pub-id pub-id-type="pmid">29121214</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van Dijk</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sharma</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Nainys</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Yim</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kathail</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Carr</surname>
            <given-names>AJ</given-names>
          </name>
          <name>
            <surname>Burdziak</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Moon</surname>
            <given-names>KR</given-names>
          </name>
          <name>
            <surname>Chaffer</surname>
            <given-names>CL</given-names>
          </name>
          <name>
            <surname>Pattabiraman</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Recovering gene interactions from single-cell data using data diffusion</article-title>
        <source>Cell</source>
        <year>2018</year>
        <volume>174</volume>
        <issue>3</issue>
        <fpage>716</fpage>
        <lpage>729</lpage>
        <?supplied-pmid 29961576?>
        <pub-id pub-id-type="pmid">29961576</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Eraslan</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Simon</surname>
            <given-names>LM</given-names>
          </name>
          <name>
            <surname>Mircea</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Mueller</surname>
            <given-names>NS</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>Single-cell rna-seq denoising using a deep count autoencoder</article-title>
        <source>Nat Commun</source>
        <year>2019</year>
        <volume>10</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="pmid">30602773</pub-id>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ding</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Condon</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>SP</given-names>
          </name>
        </person-group>
        <article-title>Interpretable dimensionality reduction of single cell transcriptome data with deep generative models</article-title>
        <source>Nat Commun</source>
        <year>2018</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="pmid">29317637</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>WV</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>JJ</given-names>
          </name>
        </person-group>
        <article-title>An accurate and robust imputation method scimpute for single-cell rna-seq data</article-title>
        <source>Nat Commun</source>
        <year>2018</year>
        <volume>9</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="pmid">29317637</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Torre</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Dueck</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Shaffer</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Bonasio</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Murray</surname>
            <given-names>JI</given-names>
          </name>
          <name>
            <surname>Raj</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>NR</given-names>
          </name>
        </person-group>
        <article-title>Saver: gene expression recovery for single-cell rna sequencing</article-title>
        <source>Nat Methods</source>
        <year>2018</year>
        <volume>15</volume>
        <issue>7</issue>
        <fpage>539</fpage>
        <lpage>542</lpage>
        <?supplied-pmid 29941873?>
        <pub-id pub-id-type="pmid">29941873</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks (2016). arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1609.02907">arXiv:1609.02907</ext-link></mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <mixed-citation publication-type="other">Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y.: Graph attention networks (2017). arXiv preprint <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1710.10903">arXiv:1710.10903</ext-link></mixed-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <mixed-citation publication-type="other">Ravindra N, Sehanobish A, Pappalardo JL, Hafler DA, van Dijk D. Disease state prediction from single-cell data using graph attention networks. In: Proceedings of the ACM conference on health, inference, and learning, p. 121–30 (2020).</mixed-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Shao X, Yang H, Zhuang X, Liao J, Yang Y, Yang P, Cheng J, Lu X, Chen H, Fan X. Reference-free cell-type annotation for single-cell transcriptomics using deep learning with a weighted graph neural network. bioRxiv (2020)</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wolf</surname>
            <given-names>FA</given-names>
          </name>
          <name>
            <surname>Angerer</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Theis</surname>
            <given-names>FJ</given-names>
          </name>
        </person-group>
        <article-title>Scanpy: large-scale single-cell gene expression data analysis</article-title>
        <source>Genome Biol</source>
        <year>2018</year>
        <volume>19</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="pmid">29301551</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Lun AT, McCarthy DJ, Marioni JC. A step-by-step workflow for low-level analysis of single-cell rna-seq data with bioconductor. F1000Research. 2016;5.</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Leote AC, Wu X, Beyer A. Network-based imputation of dropouts in single-cell rna sequencing data. bioRxiv: 611517 (2019).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Arisdakessian</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Poirion</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Yunits</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Garmire</surname>
            <given-names>LX</given-names>
          </name>
        </person-group>
        <article-title>Deepimpute: an accurate, fast, and scalable deep neural network method to impute single-cell rna-seq data</article-title>
        <source>Genome Biol</source>
        <year>2019</year>
        <volume>20</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="pmid">30606230</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <mixed-citation publication-type="other">Fey M, Lenssen JE. Fast graph representation learning with PyTorch geometric. In: ICLR workshop on representation learning on graphs and manifolds (2019)</mixed-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zappia</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Phipson</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Oshlack</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>Splatter: simulation of single-cell rna sequencing data</article-title>
        <source>Genome Biol</source>
        <year>2017</year>
        <volume>18</volume>
        <issue>1</issue>
        <fpage>1</fpage>
        <lpage>15</lpage>
        <pub-id pub-id-type="pmid">28077169</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
