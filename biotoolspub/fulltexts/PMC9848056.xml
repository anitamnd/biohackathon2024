<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9848056</article-id>
    <article-id pub-id-type="pmid">36511587</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac803</article-id>
    <article-id pub-id-type="publisher-id">btac803</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Original Paper</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Genetics and Population Analysis</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>HyperHMM: efficient inference of evolutionary and progressive dynamics on hypercubic transition graphs</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Moen</surname>
          <given-names>Marcus T</given-names>
        </name>
        <aff><institution>Department of Mathematics, University of Bergen</institution>, Bergen, Vestland, <country country="NO">Norway</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8559-3519</contrib-id>
        <name>
          <surname>Johnston</surname>
          <given-names>Iain G</given-names>
        </name>
        <aff><institution>Department of Mathematics, University of Bergen</institution>, Bergen, Vestland, <country country="NO">Norway</country></aff>
        <aff><institution>Computational Biology Unit, University of Bergen</institution>, Bergen, Vestland, <country country="NO">Norway</country></aff>
        <aff><institution>CAMRIA Centre for Antimicrobial Resistance</institution>, Vestland, <country country="NO">Norway</country></aff>
        <xref rid="btac803-cor1" ref-type="corresp"/>
        <!--iain.johnston@uib.no-->
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Schwartz</surname>
          <given-names>Russell</given-names>
        </name>
        <role>Associate Editor</role>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac803-cor1">To whom correspondence should be addressed. <email>iain.johnston@uib.no</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-12-13">
      <day>13</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>13</day>
      <month>12</month>
      <year>2022</year>
    </pub-date>
    <volume>39</volume>
    <issue>1</issue>
    <elocation-id>btac803</elocation-id>
    <history>
      <date date-type="received">
        <day>11</day>
        <month>8</month>
        <year>2022</year>
      </date>
      <date date-type="rev-recd">
        <day>11</day>
        <month>11</month>
        <year>2022</year>
      </date>
      <date date-type="editorial-decision">
        <day>06</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>12</day>
        <month>12</month>
        <year>2022</year>
      </date>
      <date date-type="corrected-typeset">
        <day>29</day>
        <month>12</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac803.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Motivation</title>
        <p>The evolution of bacterial drug resistance and other features in biology, the progression of cancer and other diseases and a wide range of broader questions can often be viewed as the sequential stochastic acquisition of binary traits (e.g. genetic changes, symptoms or characters). Using potentially noisy or incomplete data to learn the sequences by which such traits are acquired is a problem of general interest. The problem is complicated for large numbers of traits, which may, individually or synergistically, influence the probability of further acquisitions both positively and negatively. Hypercubic inference approaches, based on hidden Markov models on a hypercubic transition network, address these complications, but previous Bayesian instances can consume substantial time for converged results, limiting their practical use.</p>
      </sec>
      <sec id="s2">
        <title>Results</title>
        <p>Here, we introduce HyperHMM, an adapted Baum–Welch (expectation–maximization) algorithm for hypercubic inference with resampling to quantify uncertainty, and show that it allows orders-of-magnitude faster inference while making few practical sacrifices compared to previous hypercubic inference approaches. We show that HyperHMM allows any combination of traits to exert arbitrary positive or negative influence on the acquisition of other traits, relaxing a common limitation of only independent trait influences. We apply this approach to synthetic and biological datasets and discuss its more general application in learning evolutionary and progressive pathways.</p>
      </sec>
      <sec id="s3">
        <title>Availability and implementation</title>
        <p>Code for inference and visualization, and data for example cases, is freely available at <ext-link xlink:href="https://github.com/StochasticBiology/hypercube-hmm" ext-link-type="uri">https://github.com/StochasticBiology/hypercube-hmm</ext-link>.</p>
      </sec>
      <sec id="s5">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Trond Mohn Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100016190</institution-id>
          </institution-wrap>
        </funding-source>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>TMS2021TMT09</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>Many questions in biology, medicine and beyond concern the dynamics by which a set of traits or ‘characters’ is acquired over time. These traits could be, e.g. evolving drug resistance features in pathogens, other physiological characters in evolutionary biology, mutations in cancer progression, symptoms in progressive disease, task completions in a workflow and more. Efficient ways of learning about these dynamics from available data—which may be single-time snapshots, without longitudinal tracking of individuals—can be challenging to implement.</p>
    <p>Specific fields of study have given rise to different approaches to this question. The field of cancer evolution (<xref rid="btac803-B34" ref-type="bibr">Schwartz and Schäffer, 2017</xref>) has developed methods focussing on the construction of mutation graphs describing the ordering and dependence of mutational changes. Several of these approaches use a Bayesian networks picture (<xref rid="btac803-B3" ref-type="bibr">Beerenwinkel <italic toggle="yes">et al.</italic>, 2015</xref>; <xref rid="btac803-B20" ref-type="bibr">Loohuis <italic toggle="yes">et al.</italic>, 2014</xref>; <xref rid="btac803-B21" ref-type="bibr">Montazeri <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac803-B29" ref-type="bibr">Ramazzotti <italic toggle="yes">et al.</italic>, 2015</xref>, <xref rid="btac803-B30" ref-type="bibr">2019</xref>; <xref rid="btac803-B32" ref-type="bibr">Ross and Markowetz, 2016</xref>; <xref rid="btac803-B35" ref-type="bibr">Szabo and Boucher, 2002</xref>), which may describe dependencies between mutations as deterministic and one-way (i.e. detecting when <italic toggle="yes">X</italic> is required for <italic toggle="yes">Y</italic>, but not when <italic toggle="yes">Z</italic> has the effect of lowering the probability of <italic toggle="yes">Y</italic>). These restrictions are relaxed in approaches allowing more general influences between features. Hypercubic transition path sampling [HyperTraPS (<xref rid="btac803-B16" ref-type="bibr">Johnston and Williams, 2016</xref>)] and mutual hazard networks [MHN (<xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic>, 2020</xref>)] both consider general (positive or negative, two-way) pairwise interactions between features (in the same form, though the inference approaches differ); the HyperTraPS picture has also been generalized further to include different structures of influence (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>), and the pairwise-interaction picture has been recently developed and accelerated (<xref rid="btac803-B12" ref-type="bibr">Gotovos <italic toggle="yes">et al.</italic>, 2021</xref>). Other approaches for cancer progression have been developed that use alternative methods based, for example, on the analysis of permutations (<xref rid="btac803-B27" ref-type="bibr">Peterson and Kovyrshina, 2017</xref>; <xref rid="btac803-B40" ref-type="bibr">Zhang and Wang, 2018</xref>), and Markov modelling (<xref rid="btac803-B14" ref-type="bibr">Hjelm <italic toggle="yes">et al.</italic>, 2006</xref>); meta-studies have compared the performance of several of these approaches (<xref rid="btac803-B9" ref-type="bibr">Diaz-Colunga and Diaz-Uriarte, 2021</xref>; <xref rid="btac803-B10" ref-type="bibr">Diaz-Uriarte and Vasallo, 2019</xref>).</p>
    <p>In the (not disconnected) field of evolutionary biology, several approaches have been developed for describing and predicting the appearance of traits (typically called ‘characters’ in an evolutionary setting) on phylogenies. The well-known ‘Mk’ model (<xref rid="btac803-B19" ref-type="bibr">Lewis, 2001</xref>; <xref rid="btac803-B23" ref-type="bibr">Pagel, 1994</xref>) and its extensions, for example, use a Markov model picture to consider how a discrete-valued character changes on a phylogeny. In an evolutionary setting, the combined problem of inferring character evolution on a phylogeny and the phylogeny structure itself is often considered (<xref rid="btac803-B31" ref-type="bibr">Ronquist, 2004</xref>; <xref rid="btac803-B38" ref-type="bibr">Yang, 2014</xref>). Bayesian approaches for phylogenetic reconstruction (<xref rid="btac803-B5" ref-type="bibr">Bollback, 2006</xref>; <xref rid="btac803-B24" ref-type="bibr">Pagel and Meade, 2006</xref>) are often combined with Markov models for character dynamics (<xref rid="btac803-B31" ref-type="bibr">Ronquist, 2004</xref>), describing the different states of a character or characters and allowing stochastic transitions between those states with some rates that are model parameters to be estimated. Recent developments including generalizing the influences between evolving characters to include dependence and conditionality (<xref rid="btac803-B4" ref-type="bibr">Bianchini and Sánchez-Baracaldo, 2021</xref>), employing flexible hidden Markov models (HMMs) to describe character dynamics (<xref rid="btac803-B6" ref-type="bibr">Boyko and Beaulieu, 2021</xref>), and simulation-free approaches allowing computationally tractable treatment of problems involving ensembles of possible trees (<xref rid="btac803-B25" ref-type="bibr">Pasqualin <italic toggle="yes">et al.</italic>, 2017</xref>). Notably, the acquisition of resistance in HIV has been explored with a ‘mutagenic tree’ approach—akin to the mutation graphs above—combining an HMM and bootstrap resampling (<xref rid="btac803-B2" ref-type="bibr">Beerenwinkel and Drton, 2007</xref>). The links between the cancer and evolutionary fields have been explicitly explored by several studies picturing cancer progression as an evolutionary process (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac803-B39" ref-type="bibr">Youn and Simon, 2012</xref>).</p>
    <p>In parallel, several studies have considered a particular class of applied problems, which we will call ‘hypercubic inference’, applied in cancer progression (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic>, 2020</xref>), evolutionary biology (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac803-B15" ref-type="bibr">Johnston and Røyrvik, 2020</xref>; <xref rid="btac803-B16" ref-type="bibr">Johnston and Williams, 2016</xref>) and progression of other diseases including severe malaria (<xref rid="btac803-B17" ref-type="bibr">Johnston <italic toggle="yes">et al.</italic>, 2019</xref>). In terms of the systems involved, this picture involves evolution progressing via the sequential, irreversible, stochastic acquisition of discrete traits [also referred to as monotonic accumulation (<xref rid="btac803-B30" ref-type="bibr">Ramazzotti <italic toggle="yes">et al.</italic>, 2019</xref>)]. Rather than focussing on individual traits/characters as the elements of the system, these approaches consider every possible state of a system involving <italic toggle="yes">L</italic> traits—thus, explicitly considering every combination of trait values, and thus accounting for completely general influences of any subset of current trait values and the stochastic acquisition of another. There are therefore no assumptions of deterministic or one-way relationships (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic>, 2020</xref>). The transition graph linking possible states is then a directed hypercubic graph, and edge weights (model parameters to be estimated) can be used to control the probabilities of different dynamic pathways (<xref rid="btac803-F1" ref-type="fig">Fig. 1</xref>). We are concerned with the inverse problem of learning the structure of, and variability in, pathways on this hypercube from observed samples of the evolving system. The set of observations used to parameterize the hypercubic model may, in different scientific contexts, be cross-sectional, longitudinal or phylogenetically coupled (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>). The ability to account for samples linked by temporal or phylogenetic relationships, rather than only independent cross-sectional samples, is another strength of this class of approach.</p>
    <fig position="float" id="btac803-F1">
      <label>Fig. 1.</label>
      <caption>
        <p>Overview of hypercubic inference. (<bold>A</bold>) Observed data in the form of presence/absence ‘barcodes’ for each observation, which may be incomplete or noisy, and may be independent (cross-sectional), longitudinal or phylogenetically coupled; here, TB resistance data from <xref rid="btac803-B7" ref-type="bibr">Casali <italic toggle="yes">et al.</italic> (2014)</xref>. (<bold>B</bold>) The hypercubic transition network model for dynamics, where a system proceeds via a series of transitions from one vertex to another. Each vertex is a different ‘barcode’ state, edges give transition probabilities between states. Hypercubic inference learns these transition probabilities from data, finding the parameterization most compatible with a set of emitted observations. (<bold>C</bold>) The learned parameterization can be interpreted in several ways—as a probability map of which feature is likely acquired at which stage, explicit pathways through the hypercube space, relationships between feature orderings and more. (<bold>D</bold>) Scientific insight follows from interpreting these results</p>
      </caption>
      <graphic xlink:href="btac803f1" position="float"/>
    </fig>
    <p>HyperTraPS (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac803-B16" ref-type="bibr">Johnston and Williams, 2016</xref>) and its precursor phenotypic landscape inference (<xref rid="btac803-B37" ref-type="bibr">Williams <italic toggle="yes">et al.</italic>, 2013</xref>) are Bayesian approaches, using likelihood estimates to build posterior distributions over the parameterizations of the hypercube. The representation of parameters is flexible, with a function used to construct each edge weight from some potentially lower-dimensional representation, including the proportional hazards picture also used in <xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic> (2020)</xref>. Regularization approaches have been used to seek the parameter representation most compatible with given data, which itself informs on the generative relationships between features (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>). HyperTraPS has been used to infer the evolution of efficient photosynthesis in plants (<xref rid="btac803-B37" ref-type="bibr">Williams <italic toggle="yes">et al.</italic>, 2013</xref>), gene loss in mitochondria (<xref rid="btac803-B16" ref-type="bibr">Johnston and Williams, 2016</xref>), as well as the progression of severe malaria (<xref rid="btac803-B17" ref-type="bibr">Johnston <italic toggle="yes">et al.</italic>, 2019</xref>), the emergence of tool use in animals (<xref rid="btac803-B15" ref-type="bibr">Johnston and Røyrvik, 2020</xref>) and the participation of students in digital learning (<xref rid="btac803-B26" ref-type="bibr">Peach <italic toggle="yes">et al.</italic>, 2021</xref>).</p>
    <p>In addition to relaxing the common deterministic, acyclic, one-way dependencies as cited in <xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic> (2020)</xref>, these hypercubic inference approaches have several features, which potentially allow a more general analysis than alternative methods. First, the complete state space of all possible presence/absence combinations is considered, rather than a restricted set of pathways as in the mutagenic network picture (<xref rid="btac803-B2" ref-type="bibr">Beerenwinkel and Drton, 2007</xref>). Second, no restrictions are placed on how traits influence each other. The adoption of the parameterization protocol called <italic toggle="yes">L</italic><sup>2</sup> [in <xref rid="btac803-B16" ref-type="bibr">Johnston and Williams (2016)</xref> and <xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic> (2020)</xref>] or mutual hazards [in <xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic> (2020)</xref> and <xref rid="btac803-B12" ref-type="bibr">Gotovos <italic toggle="yes">et al.</italic> (2021)</xref>] means that each trait can independently influence the acquisition of another; the less restricted parameterization allowed by hypercubic inference supports (positive and negative) synergistic influence of pairs, triplets and any combinations of traits. If there is sufficient data, any state-specific transition probability can be inferred (and if there is insufficient data, model selection and uncertainty quantification approaches can be used to place bounds on such probabilities).</p>
    <p>While general, these Bayesian approaches, which to date rely both on Markov chain Monte Carlo (MCMC) and a sampling approach to estimate likelihoods, are computationally expensive and approximate. The inclusion of prior information is natural and arguably important for low sample sizes, to avoid overfitting a small sample. However, for larger samples, we may expect lower influence of priors on the posterior. We may also wish to avoid the Bayesian paradigm outright. We can thus naturally ask if a computationally cheaper approach can provide an output akin to a maximum-likelihood estimate for hypercube parameters—as MHN (<xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic>, 2020</xref>) and their extension (<xref rid="btac803-B12" ref-type="bibr">Gotovos <italic toggle="yes">et al.</italic>, 2021</xref>) have done for the pairwise-interaction picture. Here, we will develop and apply HyperHMM, an alternative approach for inference of dynamic pathways on directed hypercubes without restrictions on state space or trait interactions.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <p><bold>Hypercubic Baum–Welch algorithm.</bold> The derivation and intuition behind the hypercubic Baum–Welch algorithm is given more fully in the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>. Here, we simply state the essential aspects. We are concerned with estimating the probabilities <inline-formula id="IE40"><mml:math id="IM40" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, for a stochastic process <inline-formula id="IE41"><mml:math id="IM41" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> on the hypercubic graph, where each state <italic toggle="yes">s</italic> is a node and edge <inline-formula id="IE42"><mml:math id="IM42" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> exists iff <italic toggle="yes">j</italic> differs from <italic toggle="yes">i</italic> by exactly one feature acquisition (<xref rid="btac803-F1" ref-type="fig">Fig. 1</xref>). The algorithm broadly considers the set of possible paths on the hypercube that could lead to observations being ‘emitted’ that are compatible with our observations (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S1–S3</xref> for examples), and seeks to find transition weights that maximize the probability of these paths.</p>
    <p>The data, we will use are a set of potentially sequential observations <italic toggle="yes">O</italic>, where <inline-formula id="IE43"><mml:math id="IM43" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, is the <italic toggle="yes">r</italic>th sequence of observation, each labelled by an observation ordering <inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. Any of these observations may be absent.</p>
    <p>The key idea is to find probabilities, <inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, that maximize the likelihood of seeing all of our observations. This is done by first calculating the probabilities going forward and backward in time and storing the values for each time step. The forward probability is the probability of seeing everything up to a given time <italic toggle="yes">t</italic> given our current estimate of the transition matrix, <italic toggle="yes">A</italic>, and the backward probability is the probability of seeing everything from a given time <italic toggle="yes">t</italic> until the end. The forward probability of observation <italic toggle="yes">i</italic> at time <italic toggle="yes">t</italic> is defined recursively as:
<disp-formula id="E6"><label>(1)</label><mml:math id="M6" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>being the sum of probabilities of a transition to state <italic toggle="yes">i</italic> over previous states <italic toggle="yes">j</italic>, weighted by the probability of being at <italic toggle="yes">j</italic> at time <italic toggle="yes">t</italic>−1. The recursion is bounded by <inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, since our first observation will always be <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, and <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for all other states <italic toggle="yes">i</italic>. The backward probabilities are similarly defined as:
<disp-formula id="E7"><label>(2)</label><mml:math id="M7" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>summing weighted probabilities of transitions from state <italic toggle="yes">i</italic> at time <italic toggle="yes">t</italic> to state <italic toggle="yes">j</italic> at time <italic toggle="yes">t </italic>+<italic toggle="yes"> </italic>1, where the recursion now follows backwards in time bounded by <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for all <italic toggle="yes">i</italic>, as an observation at <italic toggle="yes">t </italic>=<italic toggle="yes"> T</italic> begins a backwards-time pathway.</p>
    <p>Combining these probabilities will give us a ‘skeleton’ with all possible pathways given the observation (<xref rid="sup1" ref-type="supplementary-material">Supplementary Figs S1–S3</xref>). Using this, we can calculate the probability of being in any two states at times <italic toggle="yes">t</italic> and <italic toggle="yes">t </italic>+<italic toggle="yes"> </italic>1 and update the transition matrix based on this information. These probabilities are called the <italic toggle="yes">ξ</italic>-probabilities and are defined as:
<disp-formula id="E8"><label>(3)</label><mml:math id="M8" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ξ</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mi>O</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mo>|</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the probability of seeing the given observation sequence given our current estimate of the transition matrix <italic toggle="yes">A</italic>. We then update the transition probabilities for each round according to <xref rid="E9" ref-type="disp-formula">Equation (4)</xref>, which is just a normalization of the <italic toggle="yes">ξ</italic>-probabilities (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref> for derivation):
<disp-formula id="E9"><label>(4)</label><mml:math id="M9" display="block" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mo>ξ</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mrow><mml:mo>ξ</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p>
    <p>To assess convergence of the algorithm, we calculate the maximum change in any transition probability. For all of the cases in Section 2, we have used a convergence criterion of 0.001, so that if no transition probability changes by more than an absolute value of 0.001 between one iteration and the next, we terminate the algorithm.</p>
    <p><bold>Bootstrap resampling.</bold> We used 100 bootstrap resamples of observed transitions for estimating uncertainty in the HBW algorithm, and report the standard deviation of summary statistics over the set of resamples.</p>
    <p><bold>HyperTraPS.</bold> HyperTraPS was implemented following <xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic> (2020)</xref>, using <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> samples to estimate likelihoods and a reduced parameterization mapping <italic toggle="yes">L</italic><sup>2</sup> values to the edges of the hypercubic transition network. Priors involved a uniform probability of any remaining feature being acquired at any time step. The results from HyperTraPS were obtained after preliminary investigation to find the optimal perturbation kernel for MCMC convergence. For the simple synthetic cases this was <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:mo>σ</mml:mo><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:math></inline-formula>; for the ovarian and TB cases it was <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:mo>σ</mml:mo><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula>.</p>
    <p><bold>Convergence.</bold> For the HyperHMM approach, we recorded how long it took for the inferred parameter set to converge to a 0.001 level, with 100 bootstraps to quantify uncertainty. For HyperTraPS, we recorded how long it took for the inferred ordering posteriors to converge to a 0.001 level. We did not focus on the posteriors on individual transition parameters, because many of these are unconstrained by a given dataset and (in the absence of a sparsity prior) take a long time to recapture the prior, for negligible contribution to the inferred dynamics. In a sense this criterion allows HyperTraPS a looser definition of convergence; however, even given this laxity, the speedup from the HyperHMM approach is striking (see text).</p>
    <p><bold>Summary of inferred dynamics with ordering probabilities.</bold> To summarize HyperHMM outputs, 10<sup>5</sup> random walkers are simulated on the maximum-likelihood hypercube, and the ordering of trait acquisition for each walker is recorded. Where the bootstrap is used, this process is repeated for each resampled dataset, and the standard deviation of each ordering probability over the resampled set is recorded. To summarize HyperTraPS outputs, 10<sup>3</sup> random walkers are simulated on each of 10<sup>5</sup> sampled hypercubes from the posterior, and the corresponding sampled trait-ordering posteriors are recorded.</p>
    <p><bold>Ancestral reconstruction for TB dataset.</bold> Here, we followed the minimum evolution approach in <xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic> (2020)</xref>, where the ancestor of two descendants was inferred to possess a trait iff both descendants also possess it. This approach assumes that the acquisition of traits is rare, so convergent acquisition is correspondingly rare; in cases where this assumption is not safe, our approach can readily be applied across an ensemble of possible phylogenies and the resulting inferred dynamics summarized accordingly.</p>
    <p><bold>Implementation.</bold> The implementation of the code is in C++; R scripts are provided for preparing data, externally running the C++ code and retrieving and plotting results. Currently, the implementation of the HyperHMM algorithm works readily for a system of at least 20 traits on a normal laptop. The implementation is made using the Compressed Row Storage format for representing sparse matrices. Code for inference and visualization is freely available at <ext-link xlink:href="https://github.com/StochasticBiology/hypercube-hmm" ext-link-type="uri">https://github.com/StochasticBiology/hypercube-hmm</ext-link>.</p>
  </sec>
  <sec>
    <title>3 Results</title>
    <sec>
      <title>3.1 A hypercubic Baum–Welch algorithm for efficient inference of paths on a transition hypercube</title>
      <p>Under a hypercubic transition graph model, every possible state of the system is represented as a binary string of length <italic toggle="yes">L</italic>, where 0 and 1 at the <italic toggle="yes">i</italic>th position correspond, respectively, to absence or presence of the <italic toggle="yes">i</italic>th trait (<xref rid="btac803-F1" ref-type="fig">Fig. 1</xref>). Traits are acquired stochastically and irreversibly, meaning once a trait has been acquired it cannot be lost (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>). A hypercubic transition graph is then constructed, with each node corresponding to a state, and a weighted edge from node <italic toggle="yes">a</italic> to node <italic toggle="yes">b</italic> if <italic toggle="yes">b</italic> differs from <italic toggle="yes">a</italic> by the acquisition of exactly one trait. We usually picture a given instance of the system (e.g. a patient with a progressive disease) moving over the hypercube from the binary string of all 0s towards (but not necessarily reaching) the binary string of all 1s, probabilistically following outgoing edges from a given state according to their relative weights. The goal of hypercubic inference is to learn the set(s) of edge weights that are most compatible with the observed dynamics of a given system. To this end, we consider a HMM likelihood based on emissions from processes on this transition graph (<xref rid="btac803-B28" ref-type="bibr">Rabiner and Juang, 1986</xref>). In the simplest case, an emission corresponds simply to the state at the current node, but an HMM approach also allows us to account for noisy and incomplete emissions.</p>
      <p>HyperTraPS is an algorithm estimating the likelihood of a given observed transition from some state <italic toggle="yes">a</italic> to some state <italic toggle="yes">b</italic> (not necessarily only one trait apart). Given the large number of paths that can generally exist between two nodes, HyperTraPS uses biased random walkers to estimate this likelihood, which is then embedded in a Bayesian framework using MCMC for parameter estimation. The fact that this likelihood is approximate raises issues of MCMC convergence, which require corresponding algorithmic complexity to address (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac803-B22" ref-type="bibr">Murray and Graham, 2016</xref>) and the Bayesian nature of the parameter search require substantial computer time.</p>
      <p>Here, we propose HyperHMM, an alternative using (i) an adaptation of the well-known Baum–Welch algorithm (<xref rid="btac803-B1" ref-type="bibr">Baum <italic toggle="yes">et al.</italic>, 1970</xref>; <xref rid="btac803-B28" ref-type="bibr">Rabiner and Juang, 1986</xref>) for the hypercubic transition graph to estimate parameters without requiring an approximate likelihood, and (ii) a frequentist approach using resampling rather than the fully Bayesian approach for uncertainty quantification. Both (i) and (ii) allow substantial computational gains over the usual implementation of HyperTraPS, reducing runtimes from hours to seconds (see below). A similar computational paradigm has been used previously [e.g. in <xref rid="btac803-B2" ref-type="bibr">Beerenwinkel and Drton (2007)</xref>]—here, following HyperTraPS, it is instead applied to learn the hypercubic transition network without restrictions on parameter structure or state space. The Baum–Welch algorithm is an expectation–maximization algorithm that learns maximum-likelihood transition probabilities in an HMM from a given set of observations, and provides the maximum-likelihood counterpart to the Bayesian approach previously used with HyperTraPS (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac803-B16" ref-type="bibr">Johnston and Williams, 2016</xref>). The core hypercubic Baum–Welch algorithm is given in <xref rid="btac803-BOX1" ref-type="boxed-text">Algorithm 1</xref>, illustrated in <xref rid="btac803-F1" ref-type="fig">Figure 1</xref> and derived in Section 4 and <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>.</p>
      <p>
        <boxed-text id="btac803-BOX1" position="float">
          <label>Algorithm 1</label>
          <caption>
            <p>The Baum–Welch algorithm for inference on hypercubic transition graphs. The algorithm proceeds by iteratively estimating forward and backward probabilities of the different transitions observed in the dataset, given a current estimate of the hypercubic transition matrix, then updating this estimate to increase the probabilities of these observations, until a convergence criterion is met. The specific form of the <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> functions, providing the probability estimates, is given in Section 4.</p>
          </caption>
          <p>Input: <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula> the number of traits, and <inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:mi>O</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:math></inline-formula> all the <italic toggle="yes">R</italic> independent observations. Each <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is a sequence of specific states, or markers denoting an unknown state, that arises from observed data (e.g. <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mn>001</mml:mn><mml:mo>,</mml:mo><mml:mo>?</mml:mo><mml:mo>,</mml:mo><mml:mo>?</mml:mo></mml:mrow></mml:math></inline-formula> corresponds to an observation of <inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mn>000</mml:mn><mml:mo>→</mml:mo><mml:mn>001</mml:mn></mml:mrow></mml:math></inline-formula> for an <italic toggle="yes">L </italic>=<italic toggle="yes"> </italic>3 system). Output: estimated transition matrix <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> describing the probability of a transition between two states.</p>
          <p>1: Select a first estimation of transition matrix <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>. A natural choice is uniform probabilities over all outgoing edges from a given node.</p>
          <p>2: Let <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> be the number of states, and <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p>
          <p>3: <bold>while</bold> Not convergence <bold>do</bold></p>
          <p>4:  <bold>for</bold> <italic toggle="yes">r</italic> = 1,…,<italic toggle="yes">r</italic> = <italic toggle="yes">R</italic> <bold>do</bold></p>
          <p>5:    <bold>for</bold> <italic toggle="yes">t</italic> = 0,…,<italic toggle="yes">t</italic> = <italic toggle="yes">T</italic> <bold>do</bold></p>
          <p>6:     <inline-formula id="E111"><mml:math id="M111" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p>
          <p>7:     <inline-formula id="E112"><mml:math id="M112" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></p>
          <p>8:    <bold>end for</bold></p>
          <p>9:    <bold>for</bold> <italic toggle="yes">t</italic> = 0,…,<italic toggle="yes">t</italic> = <italic toggle="yes">T</italic>−1 <bold>do</bold></p>
          <p>10:     <inline-formula id="E113"><mml:math id="M113" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mo>ξ</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo>α</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo>β</mml:mo></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>O</mml:mi><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula></p>
          <p>11:   <bold>end for</bold></p>
          <p>12:   <bold>end for</bold></p>
          <p>13:    <inline-formula id="E114"><mml:math id="M114" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>r</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo>ξ</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>r</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>k</mml:mi></mml:munder><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mrow><mml:mo>ξ</mml:mo></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:mo>∀</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula></p>
          <p>14:    <inline-formula id="E115"><mml:math id="M115" display="inline" overflow="scroll"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>A</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>a</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></p>
          <p>15: <bold>end while</bold></p>
        </boxed-text>
      </p>
    </sec>
    <sec>
      <title>3.2 Synthetic test cases requiring different interaction structures between traits</title>
      <p>We will start by looking at two synthetic cases involving simple preconstructed datasets, previously used to test HyperTraPS (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>). In the first case, we construct a dataset reflecting a single evolutionary pathway, where the acquisition of features proceeds from the first to the last indexed. We begin with this simple system with <italic toggle="yes">L </italic>=<italic toggle="yes"> </italic>5 traits, with corresponding ‘data’ consisting of the observations <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mn>10000</mml:mn><mml:mo>,</mml:mo><mml:mn>11000</mml:mn><mml:mo>,</mml:mo><mml:mn>11100</mml:mn><mml:mo>,</mml:mo><mml:mn>11110</mml:mn></mml:math></inline-formula>. To demonstrate the algorithm’s ability to capture distinct pathway structures, and negative interactions between traits, we also consider a case with two competing pathways. Here, the first trait to be acquired is equally likely to be the first or last indexed. If the first, evolution proceeds as previously, but if the last, it proceeds in the ‘opposite direction’, with traits being acquired last to first in indexing order. The first step on each pathway thus represses the other pathway. </p>
      <p><xref rid="btac803-F2" ref-type="fig">Figure 2A and B</xref> shows the results of HyperHMM inference for these two example cases. <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S4</xref> expands these results to compare inference results for different sample size <italic toggle="yes">N</italic>, trait count <italic toggle="yes">L</italic>, and to compare HyperHMM and Bayesian HyperTraPS approaches. The single pathway structure is intuitively captured, with increasing certainty as the dataset size increases. The uncertainty derived from resampling with the HBW algorithm agrees well with the posteriors from HyperTraPS (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S4</xref>). HyperTraPS, as a Bayesian approach, is informed by its priors, which in this case are simply uniform acquisition probability over all options. For low <italic toggle="yes">N</italic> the influence of these priors on the posteriors is greater, and correspondingly the uncertainty quantified from HBW resampling is higher.</p>
      <fig position="float" id="btac803-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Inferred dynamics for synthetic test datasets. Synthetic examples from the text: (<bold>A</bold>) single pathway; (<bold>B</bold>) double competing pathways; and (<bold>C</bold>) synergistic logical interactions between traits. (i) Summary output of HyperHMM algorithm reflecting averaged trait orderings. Bubbles show the probability of acquiring trait <italic toggle="yes">y</italic> at time <italic toggle="yes">x</italic>; black circles in the HyperHMM plots shows the standard deviation after 100 bootstraps. (ii) Visualization of inferred paths on the hypercubic transition network. Individual edge labels describe which feature is changed at each transition; edge weights correspond to the probability of a given transition. (iii) Probabilistic feature graphs for orderings of features changes. An edge from <italic toggle="yes">a</italic> to <italic toggle="yes">b</italic> corresponds to acquisition of <italic toggle="yes">b</italic> directly following acquisition of <italic toggle="yes">a</italic> in inferred dynamics—gives the initial state with no features. Edge weights correspond to the frequency with which given transitions are observed in simulated dynamics</p>
        </caption>
        <graphic xlink:href="btac803f2" position="float"/>
      </fig>
      <p><xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S5</xref> further compares the output of other inference approaches, including Capri (<xref rid="btac803-B29" ref-type="bibr">Ramazzotti <italic toggle="yes">et al.</italic>, 2015</xref>) [from TRONCO (<xref rid="btac803-B8" ref-type="bibr">De Sano <italic toggle="yes">et al.</italic>, 2016</xref>)], Oncotree (<xref rid="btac803-B36" ref-type="bibr">Szabo and Pappas, 2022</xref>) (<ext-link xlink:href="https://CRAN.R-project.org/package=Oncotree" ext-link-type="uri">https://CRAN.R-project.org/package=Oncotree</ext-link>) and MHN (<xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic>, 2020</xref>), for the double-pathway case. As Capri and Oncotree cannot account for mutually repressing pathways, they do not capture the full structure of this system, instead identifying a single path or putting uniform probability over many. MHN and HyperTraPS allow mutual repression, and hence capture this structure (to a very similar extent, as the target of MHN inference is the same as that of HyperTraPS).</p>
      <p>A strength of HyperHMM is its ability to capture arbitrary influences of sets of multiple traits on the acquisition of other traits (other approaches allow individual, but not synergistic, influences). To demonstrate this ability, we consider another synthetic case consisting of <italic toggle="yes">L </italic>=<italic toggle="yes"> </italic>6 traits. Initially, the final three traits have zero acquisition probability, and the first three traits have equal and independent acquisition probabilities. What happens next depends on whether Trait 1 is acquired before both Traits 2 and 3 are acquired. If Traits 1 and 2 (but not 3) or Traits 1 and 3 (but not 2) are acquired, evolution then proceeds to acquire Traits 4, 5 and 6, before acquiring the remaining 3 or 2. If both 2 and 3 are acquired before 1, evolution proceeds down the different pathway 6, 5 and 4, before acquiring the remaining 1. Succinctly, 1 AND (2 XOR 3)—states 110000 and 101000—leads to the 4–5–6 path; and 1 AND (2 AND 3)—state 111000—leads to 6–5–4. These multiple, bidirectional logic interactions cannot be precisely captured in a reduced-order system like HyperTraPS’ <italic toggle="yes">L</italic><sup>2</sup> or mutual hazards approaches (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic>, 2020</xref>). <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S6</xref> shows a comparison of inference outputs for these and other approaches; while the pairwise approaches approximate the shape of the system, they include many spurious transitions and omit the full logical dependence between features (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. S6C and D</xref>). As above, as the target of inference for MHN and HyperTraPS is the same <italic toggle="yes">L</italic><sup>2</sup> parameter structure, the models produce very similar approximations. By contrast, HyperHMM captures the higher-order behaviour through the corresponding explicit paths on the hypercubic space (<xref rid="btac803-F2" ref-type="fig">Fig. 2C</xref>)—although neither summary of average ordering behaviour reveals this logical dependence (<xref rid="btac803-F2" ref-type="fig">Fig. 2Ci and iii</xref>), it is revealed by considering the explicit hypercubic paths (<xref rid="btac803-F2" ref-type="fig">Fig. 2Cii</xref>).</p>
    </sec>
    <sec>
      <title>3.3 Ovarian cancer data</title>
      <p>Following the verification of the HyperHMM approach on synthetic datasets, we next turned to a medical dataset that, while dated, has been used to test several algorithms for evolutionary inference (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac803-B20" ref-type="bibr">Loohuis <italic toggle="yes">et al.</italic>, 2014</xref>; <xref rid="btac803-B35" ref-type="bibr">Szabo and Boucher, 2002</xref>). This dataset consists of snapshots of patterns of chromosomal aberrations in 87 ovarian cancer patients (<xref rid="btac803-B18" ref-type="bibr">Knutsen <italic toggle="yes">et al.</italic>, 2005</xref>). These are labelled by chromosome (<inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>23</mml:mn></mml:mrow></mml:math></inline-formula> and <italic toggle="yes">X</italic>), chromosomal arm (<italic toggle="yes">p</italic> or <italic toggle="yes">q</italic>) and variant type (addition + or loss −). HyperTraPS and others have used these data to test and benchmark inference approaches (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>). The questions include: do some aberrations occur systematically before others, does the presence of one aberration influence the acquisition probability of another and how distinct or separate are the different pathways by which cancer can evolve in this dataset?</p>
      <p><xref rid="btac803-F3" ref-type="fig">Figure 3</xref> shows the inferred orderings from the HyperHMM approach and from HyperTraPS. The same features are clear in both cases, with sampled hypercubic trajectories and probabilistic feature graph (<xref rid="btac803-F3" ref-type="fig">Fig. 3C and D</xref>) displaying notable similarities with the HyperTraPS output found in <xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic> (2020)</xref>—including strong weighting to the <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mrow><mml:mn>8</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mo>→</mml:mo><mml:mn>3</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo><mml:mo>→</mml:mo><mml:mn>1</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:math></inline-formula> sequence, then a more diverse range of potential dynamics thereafter. The supported trajectories here underline the ability of HyperHMM to characterize competing pathways (and negative influence of one trait on the acquisition probability of another). As discussed in <xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic> (2020)</xref>, this leads to increased flexibility in pathway identification compared to other approaches: e.g. the HyperHMM output supports acquisition of <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mn>4</mml:mn><mml:mi>q</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:math></inline-formula> prior to <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn><mml:mi>q</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:math></inline-formula>, observed in 12% of samples in the data but not identified by inference approaches based on Bayesian networks (<xref rid="btac803-B21" ref-type="bibr">Montazeri <italic toggle="yes">et al.</italic>, 2016</xref>; <xref rid="btac803-B29" ref-type="bibr">Ramazzotti <italic toggle="yes">et al.</italic>, 2015</xref>). Further comparisons with other approaches are shown in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S7</xref>. HyperHMM favours the same set of initial steps as Oncotree, but thereafter shares clear similarities in inferred hypercubic transitions with HyperTraPS (and MHN, which here assigns a more uniform spread of probabilities over pathways). Generally, hypercubic inference approaches allow a relaxation of the graph of trait relationships away from the tree structure enforced by many approaches to allow bidirectional interactions; and they also capture more detailed information about transitions between individual states, allowing general representations of trait dependencies.</p>
      <fig position="float" id="btac803-F3">
        <label>Fig. 3.</label>
        <caption>
          <p>Inferred dynamics for ovarian cancer progression. Summary results using (<bold>A</bold>) HyperTraPS using an <italic toggle="yes">L</italic><sup>2</sup> parameterization and (<bold>B</bold>) HyperHMM on the ovarian cancer dataset. (i) Bubbles show the probability of getting trait <italic toggle="yes">y</italic> at time <italic toggle="yes">x</italic>; black circles in the HyperHMM plots show the standard deviation after 100 bootstraps. (ii) Visualization of inferred paths on the hypercubic transition network. Individual edge labels describe which feature is changed at each transition; edge weights correspond to the probability of a given transition. (iii) Probabilistic feature graphs for orderings of features changes. An edge from <italic toggle="yes">a</italic> to <italic toggle="yes">b</italic> corresponds to acquisition of <italic toggle="yes">b</italic> directly following acquisition of <italic toggle="yes">a</italic> in inferred dynamics; 0 gives the initial state with no features. Edge weights correspond to the frequency with which given transitions are observed in simulated dynamics. Feature labels: 1 <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:mn>8</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:math></inline-formula>, 2 <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:mn>3</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:math></inline-formula>, 3 <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn><mml:mi>q</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:math></inline-formula>, 4 <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:mn>4</mml:mn><mml:mi>q</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:math></inline-formula>, 5 <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mn>8</mml:mn><mml:mi>p</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:math></inline-formula>, 6 <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:math></inline-formula> and 7 <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:mi>X</mml:mi><mml:mi>p</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:math></inline-formula></p>
        </caption>
        <graphic xlink:href="btac803f3" position="float"/>
      </fig>
      <p>Several examples exist in this inferred example of where higher-order interactions than simple pairwise influences (as in MHN or <italic toggle="yes">L</italic><sup>2</sup> HyperTraPS) are present. A non-exhaustive set of examples for which such higher-order interactions are important can easily be found by recording instances of <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo><mml:mo>∅</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>|</mml:mo><mml:mi>B</mml:mi><mml:mtext>and</mml:mtext><mml:mi> </mml:mi><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. In other words, <italic toggle="yes">B</italic> and <italic toggle="yes">C</italic> both independently decrease the probability of <italic toggle="yes">A</italic>, but their combination re-increases its probability. Several examples of this behaviour exist in the ovarian cancer dataset. One example is the influence of <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn><mml:mi>q</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:math></inline-formula> on the probability of acquiring <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mrow><mml:mn>8</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:math></inline-formula>. The base probability of the <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mrow><mml:mn>8</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:math></inline-formula> feature is 0.473; this is substantially reduced to <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mrow><mml:mn>1.04</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and 0.0485 by the individual prior acquisition of either <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mrow><mml:mn>5</mml:mn><mml:mi>q</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:math></inline-formula> or <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:math></inline-formula>, respectively (the base probabilities of which are 0.196 and 0.234 respectively). However, the prior acquisition of both these traits re-elevates the probability of <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mrow><mml:mn>8</mml:mn><mml:mi>q</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:math></inline-formula> to 0.132. Of course, this condition is sufficient but not necessary for higher-order influence; many other possibilities exist, but we use this as a simple example.</p>
    </sec>
    <sec>
      <title>3.4 Multidrug resistance in tuberculosis</title>
      <p>As a final test of the approach using biomedically pertinent data, we next turned to an evolutionary question—how multidrug resistance evolves in tuberculosis (TB). This global problem involves the TB bacterium acquiring resistance to the antimicrobial drugs used to treat it. We again use a dataset that has previously been used to test HyperTraPS (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>): specifically, the study of <xref rid="btac803-B7" ref-type="bibr">Casali <italic toggle="yes">et al.</italic> (2014)</xref>. Here, drug resistance profiles of different but related strains of TB were experimentally characterized, producing barcodes of resistance/susceptibility that are connected by an estimated phylogeny.</p>
      <p>Because of this phylogenetic relationship between samples (<xref rid="btac803-F1" ref-type="fig">Fig. 1A</xref>), cross-sectional approaches that assume sample independence cannot be applied, and we use the longitudinal form of HyperHMM, where pairs of observations (ancestor–descendant) are considered as the fundamental sampling elements. Following <xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic> (2020)</xref>, we estimate the barcodes of ancestral states using maximum parsimony and hence extract the set of transitions that have occurred throughout the whole lineage. This transition set is the input for our inference algorithms.</p>
      <p>Once more, the structure and variability of multidrug resistance pathways is readily revealed by the HyperHMM algorithm. Again, the inferred ordering probabilities from HyperHMM agree well with the fully Bayesian approach, with some instances where HyperHMM assigns marginally increased confidence to some orderings (as above), due to the influence of the uniform priors used in HyperTraPS. Following findings in <xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic> (2020)</xref>, a strong weighting to the initial steps INH →RIF →PZA is apparent, after which the potential pathways diversify around a central ‘core’ pathway. Interestingly, this suggests that acquisition of multidrug resistance in each lineage in these samples follows one of a related set of evolutionary pathways, with some variability around later steps but the same qualitative patterning (there are not multiple, distinct, competing pathways in <xref rid="btac803-F4" ref-type="fig">Fig. 4C</xref> to the extent seen in <xref rid="btac803-F2" ref-type="fig">Fig. 2B</xref>). This raises the possibility of predicting the next evolutionary step for a strain with a given drug resistance profile, with potential applications to optimal treatment design [akin to <xref rid="btac803-B17" ref-type="bibr">Johnston <italic toggle="yes">et al.</italic> (2019)</xref> and <xref rid="btac803-B9" ref-type="bibr">Diaz-Colunga and Diaz-Uriarte (2021)</xref>].</p>
      <fig position="float" id="btac803-F4">
        <label>Fig. 4.</label>
        <caption>
          <p>Inferred dynamics for multidrug resistance evolution in TB. Summary results using (<bold>A</bold>) HyperTraPS using an <italic toggle="yes">L</italic><sup>2</sup> parameterization and (<bold>B</bold>) HyperHMM on the TB drug resistance dataset. Each trait code is a particular drug (three-letter codes below). (i) Bubbles show the probability of getting trait <italic toggle="yes">y</italic> at time <italic toggle="yes">x</italic>; black circles in the HyperHMM plots show the standard deviation after 100 bootstraps. (ii) Visualization of inferred paths on the hypercubic transition network. Individual edge labels describe which feature is changed at each transition; edge weights correspond to the probability of a given transition. (iii) Probabilistic feature graphs for orderings of features changes. An edge from <italic toggle="yes">a</italic> to <italic toggle="yes">b</italic> corresponds to acquisition of <italic toggle="yes">b</italic> directly following acquisition of <italic toggle="yes">a</italic> in inferred dynamics; 0 gives the initial state with no features. Edge weights correspond to the frequency with which given transitions are observed in simulated dynamics. Feature labels: 1 INH, 2 RIF, 3 STR, 4 EMB, 5 PZA, 6 PRO, 7 OFL, 8 MOX, 9 CAP and 10 AMI</p>
        </caption>
        <graphic xlink:href="btac803f4" position="float"/>
      </fig>
      <p>The TB dataset also contains several examples where higher-order interactions are inferred. One striking example is <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="italic">AMI</mml:mi><mml:mo>|</mml:mo><mml:mo>∅</mml:mo><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.0778</mml:mn></mml:math></inline-formula>; <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">AMI</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="italic">INH</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.0382</mml:mn></mml:mrow></mml:math></inline-formula>; <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">AMI</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="italic">PZA</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>90</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>; <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">AMI</mml:mi><mml:mo>|</mml:mo><mml:mi mathvariant="italic">INH</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">PZA</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.0687</mml:mn></mml:mrow></mml:math></inline-formula> (where <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">INH</mml:mi><mml:mo>|</mml:mo><mml:mo>∅</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.762</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">PZA</mml:mi><mml:mo>|</mml:mo><mml:mo>∅</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.0136</mml:mn></mml:mrow></mml:math></inline-formula>). In other words, the probability of acquiring resistance to AMI is decreased by the acquisition of either INH or PZA independently, but their combination re-elevates the probability of acquiring AMI almost to its original level.</p>
    </sec>
    <sec>
      <title>3.5 Comparison to Bayesian approach: benchmarking performance, prior information and model selection</title>
      <p>Throughout the above investigations, we observed that the hypercubic Baum–Welch algorithm yielded results much more quickly than the HyperTraPS approach—intuitively, as a maximum-likelihood approach will typically involve less computation than Bayesian inference attempting to summarize the full parameter space. To quantify the difference in speeds, we considered simple synthetic test cases of the single- and double-pathway systems introduced above with <italic toggle="yes">L </italic>=<italic toggle="yes"> </italic>5, 7 and 9, along with the real-world datasets (see Section 4). HyperHMM typically converges 2–3 orders-of-magnitude more quickly than the Bayesian implementation of HyperTraPS, without requiring preliminary or parallel tuning of the MCMC parameters (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table S1</xref>). Quantitative details on the convergence time overall and per iteration with number of features <italic toggle="yes">L</italic> are given in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figure S8</xref>. In <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S5–S7</xref>, we show comparisons with other approaches from TRONCO (<xref rid="btac803-B8" ref-type="bibr">De Sano <italic toggle="yes">et al.</italic>, 2016</xref>), Oncotree (<xref rid="btac803-B36" ref-type="bibr">Szabo and Pappas, 2022</xref>) and MHN (<xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic>, 2020</xref>). Implementation issues made it hard to precisely benchmark the runtimes of each of these approaches on the same machine, but we generally observed that Capri and Oncotree ran very quickly, with MHN taking longer (on the order of seconds with our trial datasets), but using less time than HyperHMM—as expected, as the associated parameter space is smaller.</p>
      <p>One sacrifice in going from the fully Bayesian HyperTraPS is a reduced ability to include prior information in the inference process (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>). However, this can be partly addressed within HyperHMM by changing the structure of the hypercubic transition network that underlies our model—which also allows us to perform model selection and regularization within HyperHMM. In the <xref rid="sup1" ref-type="supplementary-material">Supplementary Material</xref>, we outline how edge removal can capture prior information and different model structures within HyperHMM, and how simple information-based comparison statistics like the Akaike Information Criterion can be used to select an appropriate model structure for a given dataset.</p>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>We have shown that a hypercubic Baum–Welch algorithm is an efficient alternative for inferring dynamic pathways on hypercubic transition graphs with arbitrary, potentially high-order interactions between features. Samples linked by a longitudinal or phylogenetic relationship (as in the TB case) can readily be used in the inference process, generalizing beyond independent cross-sectional samples. Arbitrary (not just pairwise) influences of sets of features on acquisition probabilities can be captured. The considerable speedup afforded over the original HyperTraPS implementation increases the set of problems that can be addressed; at the same time, no case-specific choices about perturbation kernels and likelihood estimation parameters need to be made. This simplification and speedup does not substantially challenge the approach’s ability to capture and quantify uncertainty, making it a competitive choice for inferring the structure of, and variability in, dynamic pathways—and for predicting future behaviour given current states (<xref rid="btac803-B9" ref-type="bibr">Diaz-Colunga and Diaz-Uriarte, 2021</xref>). In particular, HyperHMM directly gives predictions about the next step(s) that will occur from any specific state, providing potentially actionable information—e.g. to which drug a given bacterial strain will likely next evolve resistance, helping to choose a safer treatment alternative.</p>
    <p>One advantage of the likelihood estimation process here is that noisy and incomplete observations can very easily be included in the source data, via appropriate choices for emission probabilities in the algorithm. Accounting for noisy observations is achieved by assigning each state a non-zero probability of emitting a signal that differs from its signature. The choice of this probability can be informed by the particular system, and could involve, e.g. a constant error probability <italic toggle="yes">ϵ</italic> per bit, so that the probability of emitting a signal that differs by <italic toggle="yes">l</italic> bits from the current state is <italic toggle="yes">ϵ<sup>l</sup></italic> (appropriately normalized). Incomplete observations can be modelled by assigning each state compatible with a signal an equal probability of emitting that signal (e.g. 100 and 110 may emit <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>?</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> with equal probability). This emission probability does not itself influence the probability of those states arising in the dynamics, which is determined by the inferred parameterization.</p>
    <p>The hypercubic Baum–Welch algorithm considers every transition edge on the hypercube—allowing arbitrary interactions between traits, as in 2C. In the examples above, because of the dramatic increase in computational efficiency, we have not coarse-grained the associated parameter space in any way. Examples up to around <italic toggle="yes">L </italic>=<italic toggle="yes"> </italic>24 traits can readily be analysed on a modern computer, with that value requiring about 4 GB of memory during processing; higher trait numbers will require more memory. However, faced with larger problems, the same strategies for dimensionality reduction that were employed with HyperTraPS (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>)—and have been employed in similar hypercubic models (<xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic>, 2020</xref>) could readily be employed here. This would involve a potentially simple function in Algorithm 1 mapping a reduced parameter set <inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:mo>θ</mml:mo><mml:mo>′</mml:mo></mml:mrow></mml:math></inline-formula>, e.g. involving a proportional hazard mapping as in <xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic> (2020)</xref> and <xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic> (2020)</xref> to transition edge weights, and its inverse being used in the parameter update step each iteration.</p>
    <p>It should be noted that technically the uncertainty obtained here from bootstrap resampling can be viewed as a measure of the uncertainty of the outcome of the algorithm rather than directly as a measure of the uncertainty of the parameter estimation, and that this difference can be considerable in some cases, particularly for non-identifiable parameters (<xref rid="btac803-B11" ref-type="bibr">Fröhlich <italic toggle="yes">et al.</italic>, 2014</xref>). In most cases, there will be edge weights in the hypercubic network that cannot be precisely estimated given a dataset (e.g. those that fall far from any likely paths generating the observations), but the contribution of these parameters to the summary behaviour of an inferred model is by nature very small. Nonetheless, uncertainty analysis without resampling, e.g. via automatic or numerical calculation of likelihood derivatives and Fisher information, would be an appropriate future extension for this approach.</p>
    <p>Comparisons with alternative approaches for inferring dynamics in a similar way to the hypercubic picture have been shown in <xref rid="btac803-B16" ref-type="bibr">Johnston and Williams (2016)</xref> and <xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic> (2020)</xref>, and in <xref rid="sup1" ref-type="supplementary-material">Supplementary Figures S5–S7</xref> here. Classes of approach for this problem broadly include simple logistic regression, approaches based on Bayesian networks, approaches for modelling and learning stochastic processes on phylogenies and approaches harnessing topology and/or dimensionality reduction before performing inference (<xref rid="btac803-B13" ref-type="bibr">Greenbury <italic toggle="yes">et al.</italic>, 2020</xref>). Previous comparisons have demonstrated that HyperTraPS typically has advantages of scaling and generality over several other approaches. Of particular note are the early method of <xref rid="btac803-B14" ref-type="bibr">Hjelm <italic toggle="yes">et al.</italic> (2006)</xref>, where a Markov chain picture is used to construct networks describing longitudinal observations with pairwise trait interactions; <xref rid="btac803-B2" ref-type="bibr">Beerenwinkel and Drton (2007)</xref>, who use a HMM with bootstrapping to infer mutagenic tree representations of resistance dynamics in HIV (but do not consider the dynamics through an unrestricted space of presence/absence sets); and <xref rid="btac803-B33" ref-type="bibr">Schill <italic toggle="yes">et al.</italic> (2020)</xref>, who apply a different set of inference approaches to a very similar hypercubic model setup, while using the mutual hazards parameterization (corresponding to individual, but not pairwise or above, influences of trait on each other’s acquisition)—extended and accelerated in <xref rid="btac803-B12" ref-type="bibr">Gotovos <italic toggle="yes">et al.</italic> (2021)</xref>. We believe that hypercubic inference approaches (HyperHMM and HyperTraPS) complement these approaches by relaxing assumptions on both the state space considered and the influences between traits, generalizing both aspects of the model (while being restricted to learning orderings, rather than continuous timings). Our aim here is to demonstrate that the HyperHMM matches HyperTraPS output—and hence retains these and other advantages—while allowing a considerable speedup that will render more large-scale problems computationally tractable.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac803_Supplementary_Data</label>
      <media xlink:href="btac803_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>Acknowledgements</title>
    <p>We are grateful to Kostas Giannakis for helpful discussions and for Jessica Renz for valuable comments on the manuscript.</p>
  </ack>
  <sec>
    <title>Author contributions</title>
    <p>M.T.M. and I.G.J. conceived the project, wrote the code, analysed the results, and wrote and reviewed the manuscript.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>This work was supported by the Trond Mohn Foundation [project HyperEvol under grant agreement No. TMS2021TMT09], through the Centre for Antimicrobial Resistance in Western Norway (CAMRIA) [TMS2020TMT11]. This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme [grant agreement No. 805046 (EvoConBiO)].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac803-B1">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Baum</surname><given-names>L.E.</given-names></string-name></person-group><etal>et al</etal> (<year>1970</year>) <article-title>A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains</article-title>. <source>Ann. Math. Stat</source>., <volume>41</volume>, <fpage>164</fpage>–<lpage>171</lpage>.</mixed-citation>
    </ref>
    <ref id="btac803-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beerenwinkel</surname><given-names>N.</given-names></string-name>, <string-name><surname>Drton</surname><given-names>M.</given-names></string-name></person-group> (<year>2007</year>) <article-title>A mutagenetic tree hidden Markov model for longitudinal clonal HIV sequence data</article-title>. <source>Biostatistics</source>, <volume>8</volume>, <fpage>53</fpage>–<lpage>71</lpage>.<pub-id pub-id-type="pmid">16569743</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Beerenwinkel</surname><given-names>N.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>Cancer evolution: mathematical models and computational inference</article-title>. <source>Syst. Biol</source>., <volume>64</volume>, <fpage>e1</fpage>–<lpage>e25</lpage>.<pub-id pub-id-type="pmid">25293804</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bianchini</surname><given-names>G.</given-names></string-name>, <string-name><surname>Sánchez-Baracaldo</surname><given-names>P.</given-names></string-name></person-group> (<year>2021</year>) <article-title>sMap: evolution of independent, dependent and conditioned discrete characters in a Bayesian framework</article-title>. <source>Methods Ecol. Evol</source>., <volume>12</volume>, <fpage>479</fpage>–<lpage>486</lpage>.</mixed-citation>
    </ref>
    <ref id="btac803-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bollback</surname><given-names>J.P.</given-names></string-name></person-group> (<year>2006</year>) <article-title>SIMMAP: stochastic character mapping of discrete traits on phylogenies</article-title>. <source>BMC Bioinformatics</source>, <volume>7</volume>, <fpage>1</fpage>–<lpage>7</lpage>.<pub-id pub-id-type="pmid">16393334</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Boyko</surname><given-names>J.D.</given-names></string-name>, <string-name><surname>Beaulieu</surname><given-names>J.M.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Generalized hidden Markov models for phylogenetic comparative datasets</article-title>. <source>Methods Ecol. Evol</source>., <volume>12</volume>, <fpage>468</fpage>–<lpage>478</lpage>.</mixed-citation>
    </ref>
    <ref id="btac803-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Casali</surname><given-names>N.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) <article-title>Evolution and transmission of drug-resistant tuberculosis in a Russian population</article-title>. <source>Nat. Genet</source>., <volume>46</volume>, <fpage>279</fpage>–<lpage>286</lpage>.<pub-id pub-id-type="pmid">24464101</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>De Sano</surname><given-names>L.</given-names></string-name></person-group><etal>et al</etal> (<year>2016</year>) <article-title>TRONCO: an R package for the inference of cancer progression models from heterogeneous genomic data</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>1911</fpage>–<lpage>1913</lpage>.<pub-id pub-id-type="pmid">26861821</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diaz-Colunga</surname><given-names>J.</given-names></string-name>, <string-name><surname>Diaz-Uriarte</surname><given-names>R.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Conditional prediction of consecutive tumor evolution using cancer progression models: what genotype comes next?</article-title><source>PLoS Comput. Biol</source>., <volume>17</volume>, <fpage>e1009055</fpage>.<pub-id pub-id-type="pmid">34932572</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Diaz-Uriarte</surname><given-names>R.</given-names></string-name>, <string-name><surname>Vasallo</surname><given-names>C.</given-names></string-name></person-group> (<year>2019</year>) <article-title>Every which way? On predicting tumor evolution using cancer progression models</article-title>. <source>PLoS Comput. Biol</source>., <volume>15</volume>, <fpage>e1007246</fpage>.<pub-id pub-id-type="pmid">31374072</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B11">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Fröhlich</surname><given-names>F.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) Uncertainty analysis for non-identifiable dynamical systems: profile likelihoods, bootstrapping and more. In: <italic toggle="yes">International Conference on Computational Methods in Systems Biology</italic>, Springer, Manchester, UK, pp. <fpage>61</fpage>–<lpage>72</lpage>.</mixed-citation>
    </ref>
    <ref id="btac803-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gotovos</surname><given-names>A.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Scaling up continuous-time Markov chains helps resolve underspecification</article-title>. <source>Adv. Neural Inf. Process. Syst</source>., <volume>34</volume>, <fpage>14580</fpage>–<lpage>14592</lpage>.</mixed-citation>
    </ref>
    <ref id="btac803-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Greenbury</surname><given-names>S.F.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>HyperTraPS: inferring probabilistic patterns of trait acquisition in evolutionary and disease progression pathways</article-title>. <source>Cell Syst</source>., <volume>10</volume>, <fpage>39</fpage>–<lpage>51.e10</lpage>.<pub-id pub-id-type="pmid">31786211</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hjelm</surname><given-names>M.</given-names></string-name></person-group><etal>et al</etal> (<year>2006</year>) <article-title>New probabilistic network models and algorithms for oncogenesis</article-title>. <source>J. Comput. Biol</source>., <volume>13</volume>, <fpage>853</fpage>–<lpage>865</lpage>.<pub-id pub-id-type="pmid">16761915</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnston</surname><given-names>I.G.</given-names></string-name>, <string-name><surname>Røyrvik</surname><given-names>E.C.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Data-driven inference reveals distinct and conserved dynamic pathways of tool use emergence across animal taxa</article-title>. <source>iScience</source>, <volume>23</volume>, <fpage>101245</fpage>.<pub-id pub-id-type="pmid">32629611</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B16">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnston</surname><given-names>I.G.</given-names></string-name>, <string-name><surname>Williams</surname><given-names>B.P.</given-names></string-name></person-group> (<year>2016</year>) <article-title>Evolutionary inference across eukaryotes identifies specific pressures favoring mitochondrial gene retention</article-title>. <source>Cell Syst</source>., <volume>2</volume>, <fpage>101</fpage>–<lpage>111</lpage>.<pub-id pub-id-type="pmid">27135164</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Johnston</surname><given-names>I.G.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Precision identification of high-risk phenotypes and progression pathways in severe malaria without requiring longitudinal data</article-title>. <source>NPJ Digit. Med</source>., <volume>2</volume>, <fpage>1</fpage>–<lpage>9</lpage>.<pub-id pub-id-type="pmid">31304351</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Knutsen</surname><given-names>T.</given-names></string-name></person-group><etal>et al</etal> (<year>2005</year>) <article-title>The interactive online SKY/M-FISH &amp; CGH database and the Entrez cancer chromosomes search database: linkage of chromosomal aberrations with the genome sequence</article-title>. <source>Genes Chromosomes Cancer</source>, <volume>44</volume>, <fpage>52</fpage>–<lpage>64</lpage>.<pub-id pub-id-type="pmid">15934046</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lewis</surname><given-names>P.O.</given-names></string-name></person-group> (<year>2001</year>) <article-title>A likelihood approach to estimating phylogeny from discrete morphological character data</article-title>. <source>Syst. Biol</source>., <volume>50</volume>, <fpage>913</fpage>–<lpage>925</lpage>.<pub-id pub-id-type="pmid">12116640</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Loohuis</surname><given-names>L.O.</given-names></string-name></person-group><etal>et al</etal> (<year>2014</year>) <article-title>Inferring tree causal models of cancer progression with probability raising</article-title>. <source>PLoS One</source>, <volume>9</volume>, <fpage>e108358</fpage>.<pub-id pub-id-type="pmid">25299648</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Montazeri</surname><given-names>H.</given-names></string-name></person-group><etal>et al</etal>; <collab>Swiss HIV Cohort Study</collab>. (<year>2016</year>) <article-title>Large-scale inference of conjunctive Bayesian networks</article-title>. <source>Bioinformatics</source>, <volume>32</volume>, <fpage>i727</fpage>–<lpage>i735</lpage>.<pub-id pub-id-type="pmid">27587695</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B22">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Murray</surname><given-names>I.</given-names></string-name>, <string-name><surname>Graham</surname><given-names>M.</given-names></string-name></person-group> (<year>2016</year>) <part-title>Pseudo-marginal slice sampling</part-title>. In: <source>Artificial Intelligence and Statistics</source>, PMLR, Cadiz, Spain, pp. <fpage>911</fpage>–<lpage>919</lpage>.</mixed-citation>
    </ref>
    <ref id="btac803-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pagel</surname><given-names>M.</given-names></string-name></person-group> (<year>1994</year>) <article-title>Detecting correlated evolution on phylogenies: a general method for the comparative analysis of discrete characters</article-title>. <source>Proc. R. Soc. Lond. B Biol. Sci</source>., <volume>255</volume>, <fpage>37</fpage>–<lpage>45</lpage>.</mixed-citation>
    </ref>
    <ref id="btac803-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pagel</surname><given-names>M.</given-names></string-name>, <string-name><surname>Meade</surname><given-names>A.</given-names></string-name></person-group> (<year>2006</year>) <article-title>Bayesian analysis of correlated evolution of discrete characters by reversible-jump Markov chain Monte Carlo</article-title>. <source>Am. Nat</source>., <volume>167</volume>, <fpage>808</fpage>–<lpage>825</lpage>.<pub-id pub-id-type="pmid">16685633</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pasqualin</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2017</year>) <article-title>SFREEMAP-a simulation-free tool for stochastic mapping</article-title>. <source>BMC Bioinformatics</source>, <volume>18</volume>, <fpage>1</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">28049414</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peach</surname><given-names>R.L.</given-names></string-name></person-group><etal>et al</etal> (<year>2021</year>) <article-title>Understanding learner behaviour in online courses with Bayesian modelling and time series characterisation</article-title>. <source>Sci. Rep</source>., <volume>11</volume>, <fpage>1</fpage>–<lpage>15</lpage>.<pub-id pub-id-type="pmid">33414495</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B27">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Peterson</surname><given-names>L.E.</given-names></string-name>, <string-name><surname>Kovyrshina</surname><given-names>T.</given-names></string-name></person-group> (<year>2017</year>) <article-title>Progression inference for somatic mutations in cancer</article-title>. <source>Heliyon</source>, <volume>3</volume>, <fpage>e00277</fpage>.<pub-id pub-id-type="pmid">28492066</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Rabiner</surname><given-names>L.</given-names></string-name>, <string-name><surname>Juang</surname><given-names>B.</given-names></string-name></person-group> (<year>1986</year>) <article-title>An introduction to hidden Markov models</article-title>. <source>IEEE ASSP Mag</source>., <volume>3</volume>, <fpage>4</fpage>–<lpage>16</lpage>.</mixed-citation>
    </ref>
    <ref id="btac803-B29">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramazzotti</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2015</year>) <article-title>CAPRI: efficient inference of cancer progression models from cross-sectional data</article-title>. <source>Bioinformatics</source>, <volume>31</volume>, <fpage>3016</fpage>–<lpage>3026</lpage>.<pub-id pub-id-type="pmid">25971740</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ramazzotti</surname><given-names>D.</given-names></string-name></person-group><etal>et al</etal> (<year>2019</year>) <article-title>Efficient computational strategies to learn the structure of probabilistic graphical models of cumulative phenomena</article-title>. <source>J. Comput. Sci</source>., <volume>30</volume>, <fpage>1</fpage>–<lpage>10</lpage>.</mixed-citation>
    </ref>
    <ref id="btac803-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ronquist</surname><given-names>F.</given-names></string-name></person-group> (<year>2004</year>) <article-title>Bayesian inference of character evolution</article-title>. <source>Trends Ecol. Evol</source>., <volume>19</volume>, <fpage>475</fpage>–<lpage>481</lpage>.<pub-id pub-id-type="pmid">16701310</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ross</surname><given-names>E.M.</given-names></string-name>, <string-name><surname>Markowetz</surname><given-names>F.</given-names></string-name></person-group> (<year>2016</year>) <article-title>OncoNEM: inferring tumor evolution from single-cell sequencing data</article-title>. <source>Genome Biol</source>., <volume>17</volume>, <fpage>1</fpage>–<lpage>14</lpage>.<pub-id pub-id-type="pmid">26753840</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B33">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schill</surname><given-names>R.</given-names></string-name></person-group><etal>et al</etal> (<year>2020</year>) <article-title>Modelling cancer progression using mutual hazard networks</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>241</fpage>–<lpage>249</lpage>.<pub-id pub-id-type="pmid">31250881</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Schwartz</surname><given-names>R.</given-names></string-name>, <string-name><surname>Schäffer</surname><given-names>A.A.</given-names></string-name></person-group> (<year>2017</year>) <article-title>The evolution of tumour phylogenetics: principles and practice</article-title>. <source>Nat. Rev. Genet</source>., <volume>18</volume>, <fpage>213</fpage>–<lpage>229</lpage>.<pub-id pub-id-type="pmid">28190876</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szabo</surname><given-names>A.</given-names></string-name>, <string-name><surname>Boucher</surname><given-names>K.</given-names></string-name></person-group> (<year>2002</year>) <article-title>Estimating an oncogenetic tree when false negatives and positives are present</article-title>. <source>Math. Biosci</source>., <volume>176</volume>, <fpage>219</fpage>–<lpage>236</lpage>.<pub-id pub-id-type="pmid">11916510</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B36">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Szabo</surname><given-names>A.</given-names></string-name>, <string-name><surname>Pappas</surname><given-names>L.</given-names></string-name></person-group> (<year>2022</year>) <italic toggle="yes">Oncotree: Estimating Oncogenetic Trees</italic>. R package version 0.3.4.</mixed-citation>
    </ref>
    <ref id="btac803-B37">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Williams</surname><given-names>B.P.</given-names></string-name></person-group><etal>et al</etal> (<year>2013</year>) <article-title>Phenotypic landscape inference reveals multiple evolutionary paths to C4 photosynthesis</article-title>. <source>Elife</source>, <volume>2</volume>, <fpage>e00961</fpage>.<pub-id pub-id-type="pmid">24082995</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B38">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>Z.</given-names></string-name></person-group> (<year>2014</year>) <source>Molecular Evolution: A Statistical Approach</source>. <publisher-name>Oxford University Press, Oxford, UK</publisher-name>.</mixed-citation>
    </ref>
    <ref id="btac803-B39">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Youn</surname><given-names>A.</given-names></string-name>, <string-name><surname>Simon</surname><given-names>R.</given-names></string-name></person-group> (<year>2012</year>) <article-title>Estimating the order of mutations during tumorigenesis from tumor genome sequencing data</article-title>. <source>Bioinformatics</source>, <volume>28</volume>, <fpage>1555</fpage>–<lpage>1561</lpage>.<pub-id pub-id-type="pmid">22492649</pub-id></mixed-citation>
    </ref>
    <ref id="btac803-B40">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>W.</given-names></string-name>, <string-name><surname>Wang</surname><given-names>S.-L.</given-names></string-name></person-group> (<year>2018</year>) <article-title>Inference of cancer progression with probabilistic graphical model from cross-sectional mutation data</article-title>. <source>IEEE Access</source>, <volume>6</volume>, <fpage>22889</fpage>–<lpage>22898</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
