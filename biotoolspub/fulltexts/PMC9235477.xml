<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">Bioinformatics</journal-id>
    <journal-id journal-id-type="publisher-id">bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1367-4803</issn>
    <issn pub-type="epub">1367-4811</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9235477</article-id>
    <article-id pub-id-type="pmid">35758793</article-id>
    <article-id pub-id-type="doi">10.1093/bioinformatics/btac258</article-id>
    <article-id pub-id-type="publisher-id">btac258</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>ISCB/Ismb 2022</subject>
        <subj-group subj-group-type="category-toc-heading">
          <subject>Macromolecular Sequence, Structure, and Function</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI01060</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Topsy-Turvy: integrating a global view into sequence-based PPI prediction</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-4084-7340</contrib-id>
        <name>
          <surname>Singh</surname>
          <given-names>Rohit</given-names>
        </name>
        <aff><institution>Computer Science and Artificial Intelligence Lab., Massachusetts Institute of Technology</institution>, Cambridge, MA 02139, <country country="US">USA</country></aff>
        <xref rid="btac258-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-6093-6260</contrib-id>
        <name>
          <surname>Devkota</surname>
          <given-names>Kapil</given-names>
        </name>
        <aff><institution>Department of Computer Science, Tufts University</institution>, Medford, MA 02155, <country country="US">USA</country></aff>
        <xref rid="btac258-FM1" ref-type="author-notes"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0170-3029</contrib-id>
        <name>
          <surname>Sledzieski</surname>
          <given-names>Samuel</given-names>
        </name>
        <aff><institution>Computer Science and Artificial Intelligence Lab., Massachusetts Institute of Technology</institution>, Cambridge, MA 02139, <country country="US">USA</country></aff>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-2724-7228</contrib-id>
        <name>
          <surname>Berger</surname>
          <given-names>Bonnie</given-names>
        </name>
        <aff><institution>Department of Mathematics, Massachusetts Institute of Technology</institution>, Cambridge, MA 02139, <country country="US">USA</country></aff>
        <aff><institution>MIT Schwarzman College of Computing Cambridge</institution>, MA 02139, <country country="US">USA</country></aff>
        <xref rid="btac258-cor1" ref-type="corresp"/>
        <!--bab@mit.edu-->
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6698-6413</contrib-id>
        <name>
          <surname>Cowen</surname>
          <given-names>Lenore</given-names>
        </name>
        <aff><institution>Department of Computer Science, Tufts University</institution>, Medford, MA 02155, <country country="US">USA</country></aff>
        <xref rid="btac258-cor1" ref-type="corresp"/>
        <!--cowen@cs.tufts.edu-->
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="btac258-cor1">To whom correspondence should be addressed. E-mail: <email>bab@mit.edu</email> or <email>cowen@cs.tufts.edu</email></corresp>
      <fn id="btac258-FM1">
        <p>The authors wish it to be known that, in their opinion, Rohit Singh and Kapil Devkota should be regarded as Joint First Authors.</p>
      </fn>
    </author-notes>
    <pub-date pub-type="collection">
      <month>7</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="epub" iso-8601-date="2022-06-27">
      <day>27</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>27</day>
      <month>6</month>
      <year>2022</year>
    </pub-date>
    <volume>38</volume>
    <issue>Suppl 1</issue>
    <issue-title>ISCB ISMB 2022 Proceedings</issue-title>
    <fpage>i264</fpage>
    <lpage>i272</lpage>
    <permissions>
      <copyright-statement>© The Author(s) 2022. Published by Oxford University Press.</copyright-statement>
      <copyright-year>2022</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xlink:href="btac258.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="s1">
        <title>Summary</title>
        <p>Computational methods to predict protein–protein interaction (PPI) typically segregate into sequence-based ‘bottom-up’ methods that infer properties from the characteristics of the individual protein sequences, or global ‘top-down’ methods that infer properties from the pattern of already known PPIs in the species of interest. However, a way to incorporate top-down insights into sequence-based bottom-up PPI prediction methods has been elusive. We thus introduce Topsy-Turvy, a method that newly synthesizes both views in a sequence-based, multi-scale, deep-learning model for PPI prediction. While Topsy-Turvy makes predictions using only sequence data, during the training phase it takes a transfer-learning approach by incorporating patterns from both global and molecular-level views of protein interaction. In a cross-species context, we show it achieves state-of-the-art performance, offering the ability to perform genome-scale, interpretable PPI prediction for non-model organisms with no existing experimental PPI data. In species with available experimental PPI data, we further present a Topsy-Turvy hybrid (TT-Hybrid) model which integrates Topsy-Turvy with a purely network-based model for link prediction that provides information about species-specific network rewiring. TT-Hybrid makes accurate predictions for both well- and sparsely-characterized proteins, outperforming both its constituent components as well as other state-of-the-art PPI prediction methods. Furthermore, running Topsy-Turvy and TT-Hybrid screens is feasible for whole genomes, and thus these methods scale to settings where other methods (e.g. AlphaFold-Multimer) might be infeasible. The generalizability, accuracy and genome-level scalability of Topsy-Turvy and TT-Hybrid unlocks a more comprehensive map of protein interaction and organization in both model and non-model organisms.</p>
      </sec>
      <sec id="s2">
        <title>Availability and implementation</title>
        <p><ext-link xlink:href="https://topsyturvy.csail.mit.edu" ext-link-type="uri">https://topsyturvy.csail.mit.edu</ext-link>.</p>
      </sec>
      <sec id="s4">
        <title>Supplementary information</title>
        <p><xref rid="sup1" ref-type="supplementary-material">Supplementary data</xref> are available at <italic toggle="yes">Bioinformatics</italic> online.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Institutes of Health</institution>
            <institution-id institution-id-type="DOI">10.13039/100000002</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>R35GM141861</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Science Foundation</institution>
            <institution-id institution-id-type="DOI">10.13039/100000001</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>CCF-1934553</award-id>
      </award-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>National Science Foundation Graduate Research Fellowship</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1745302</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="9"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec>
    <title>1 Introduction</title>
    <p>We focus on the problem of predicting PPIs from sequence data without the computational expense of multiple sequence alignments, thus enabling genome-scale predictions. Classically, the physical protein–protein interaction (PPI) prediction problem has been studied in two settings: one, where we only have access to each protein’s amino acid sequence and must determine from the sequence data alone if the two proteins bind (e.g. <xref rid="btac258-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac258-B18" ref-type="bibr">Hashemifar <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac258-B30" ref-type="bibr">Sledzieski <italic toggle="yes">et al.</italic>, 2021</xref>; <xref rid="btac258-B34" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic>, 2012</xref>). The other infers new interactions from the global topological properties of known PPI connections using either a simple rule such as ‘proteins with many common interaction partners are likely to also interact’, or more sophisticated diffusion-based network embeddings (e.g. <xref rid="btac258-B8" ref-type="bibr">Coşkun and Koyutürk, 2021</xref>; <xref rid="btac258-B9" ref-type="bibr">Cowen <italic toggle="yes">et al.</italic>, 2017</xref>; <xref rid="btac258-B11" ref-type="bibr">Devkota <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac258-B17" ref-type="bibr">Hamilton <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac258-B19" ref-type="bibr">Huang <italic toggle="yes">et al.</italic>, 2020</xref>; <xref rid="btac258-B21" ref-type="bibr">Kovács <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac258-B33" ref-type="bibr">Yuen and Jansson, 2020</xref>).</p>
    <p>Our previous work introduced D-SCRIPT (<xref rid="btac258-B30" ref-type="bibr">Sledzieski <italic toggle="yes">et al.</italic>, 2021</xref>), a structure-aware deep-learning model for predicting protein interactions. D-SCRIPT takes a bottom-up view, learning about protein interactions pair-by-pair through the lens of (inferred) protein structure and, by leveraging a natural language based protein sequence representation, was shown to achieve state-of-the-art cross-species generalizability. While we originally trained D-SCRIPT on pairwise human PPI data, we pursue here the intuition that the wealth of network-level global information available could potentially improve predictive performance if integrated during the training phase. Unfortunately, we found scant guidance in the literature for how to make use of both types of information simultaneously: existing PPI prediction methods (such as those listed above) either take exclusively a top-down or bottom-up approach, ignoring the other approach entirely.</p>
    <p>Here, we propose a new approach, <bold>Topsy-Turvy</bold>, that integrates graph-theoretic (top-down) and sequence-based (bottom-up) approaches to PPI prediction in the training phase of our sequence-based predictor. Topsy-Turvy introduces a multi-objective training framework that takes a pair of protein sequences as input, with the supervision provided by <italic toggle="yes">both</italic> experimentally determined PPIs (in the same manner as D-SCRIPT), as well as with global topological measures of protein pair compatibility. Importantly, it only requires protein sequences as inputs when making predictions—network information is used only during training. Since the trained Topsy-Turvy model makes predictions using just sequence data, it is particularly valuable in non-model organisms where almost no PPI data is available (<xref rid="btac258-B30" ref-type="bibr">Sledzieski <italic toggle="yes">et al.</italic>, 2021</xref>). We also investigate whether AlphaFold-Multimer (<xref rid="btac258-B12" ref-type="bibr">Evans <italic toggle="yes">et al.</italic>, 2021</xref>), a very recent method for protein-complex structure prediction, can instead be adapted to solve our PPI prediction task; however, we found it to be 100 000 times slower than Topsy-Turvy. Due to its computational efficiency, Topsy-Turvy is applicable in genome-wide prediction settings where AlphaFold-Multimer would be infeasible.</p>
    <p>While Topsy-Turvy requires no pre-existing experimental data in the species of interest, for cases where some such data <italic toggle="yes">is</italic> available (e.g. in worm or fly) we devise a hybrid model, <bold>TT-Hybrid</bold>, that is able to take advantage of species-specific network data. TT-Hybrid embodies a principled approach to combining the Topsy-Turvy sequence scores with GLIDE (<xref rid="btac258-B11" ref-type="bibr">Devkota <italic toggle="yes">et al.</italic>, 2020</xref>) scores to make PPI predictions; we chose GLIDE after benchmarking it against the widely used node2vec (<xref rid="btac258-B16" ref-type="bibr">Grover and Leskovec, 2016</xref>) (Section 3.1). We show that TT-Hybrid performs better than its competitors, or just Topsy-Turvy or GLIDE alone.</p>
    <p>This work has several key conceptual advances—(i) whereas the D-SCRIPT algorithm showed that informative features generated by a protein language model enable transfer learning of the structural basis of interaction, we show that we can likewise transfer global patterns of PPI organization by integrating a topological compatibility score into the loss function. (ii) We approach the synthesis of bottom-up and top-down approaches as a multi-objective training problem that balances between structural and topological considerations when predicting PPIs. Except for the recent work of <xref rid="btac258-B32" ref-type="bibr">Yang <italic toggle="yes">et al.</italic> (2020)</xref>, such integrative approaches in prior work have been rare. (iii) We provide a framework for accurately predicting PPIs in a variety of settings—both cross-species, where no training data is available in the target species, as well as in species that have limited experimentally determined PPIs.</p>
    <p>In a cross-species setting, Topsy-Turvy achieves state-of-the-art results, substantially improving upon the cross-species generalizability of PIPR (<xref rid="btac258-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2019</xref>), a deep learning method by <xref rid="btac258-B27" ref-type="bibr">Richoux <italic toggle="yes">et al.</italic> (2019)</xref> and D-SCRIPT. We investigate Topsy-Turvy’s improved performance, finding that it performs better not only on interactions involving hub nodes in the target species but even more so on low-degree nodes; this suggests that the measured outperformance is not simply due to ascertainment bias (<xref rid="btac258-B5" ref-type="bibr">Carter <italic toggle="yes">et al.</italic>, 2013</xref>) (Sections 3.3 and 3.4). We also investigated Topsy-Turvy’s usefulness in settings where sufficient PPI data exists so that a putative interaction between two proteins <italic toggle="yes">could</italic> also be predicted using global methods. We show that TT-Hybrid’s principled synthesis of the scores from the network-based GLIDE method (<xref rid="btac258-B11" ref-type="bibr">Devkota <italic toggle="yes">et al.</italic>, 2020</xref>) and Topsy-Turvy yields state-of-the-art performance in this setting as well.</p>
  </sec>
  <sec>
    <title>2 Materials and methods</title>
    <sec>
      <title>2.1 Overview of Topsy-Turvy</title>
      <p>Topsy-Turvy provides a general paradigm to integrate a bottom-up sequence-based and top-down global method: for these two components in Topsy-Turvy we choose D-SCRIPT for the sequence-based prediction, and GLIDE for the network-base prediction. We next briefly review D-SCRIPT and GLIDE. In Topsy-Turvy, we adapt the D-SCRIPT model to synthesize the two by adding to it a network-dependent loss term inferred from the GLIDE model (<xref rid="btac258-F1" ref-type="fig">Fig. 1</xref>).</p>
      <fig position="float" id="btac258-F1">
        <label>Fig. 1.</label>
        <caption>
          <p>Topsy-Turvy synthesizes sequence-to-structure-based prediction using D-SCRIPT with network-based prediction using GLIDE. (<bold>A</bold>) D-SCRIPT uses a protein language model to generate representative embeddings of protein sequences, which are combined with a convolutional neural network to predict protein interaction. It is supervised using binary interaction labels from the training network and regularized by a measure of contact map sparsity. (<bold>B</bold>) GLIDE scores all possible edges using a weighted combination of global and local network scores which are learned from the edges already in the training network. (<bold>C</bold>) Topsy-Turvy is supervised with both the binary interaction labels of the true (training) network and with the GLIDE predicted scores, thus integrating bottom-up and top-down approaches for PPI prediction into the learned Topsy-Turvy model</p>
        </caption>
        <graphic xlink:href="btac258f1" position="float"/>
      </fig>
    </sec>
    <sec>
      <title>2.2 Background: sequence-based prediction with D-SCRIPT</title>
      <p>To make bottom-up, structure-aware predictions of PPIs, we use D-SCRIPT, a state of the art method for sequence-based PPI prediction across species. Briefly, D-SCRIPT operates in two stages. First, we generate a feature-rich representation of each protein using a protein language model (PLM) (<xref rid="btac258-B1" ref-type="bibr">Bepler and Berger, 2019</xref>, <xref rid="btac258-B2" ref-type="bibr">2021</xref>); next, these features are combined using a convolutional neural network to predict interaction. The Bepler &amp; Berger PLM was chosen to extract structurally relevant features. Leveraging it, the D-SCRIPT architecture mimics the structural mechanism of protein interaction and includes an intermediate representation that encodes the intra-protein contact map. During inference, these predicted contact maps were shown to substantially recapitulate ground-truth binding mechanisms despite no structure-based supervision or inputs. To achieve this, the training procedure for D-SCRIPT minimizes a hybrid loss that contains terms measuring both the binary cross-entropy of predictions <inline-formula id="IE1"><mml:math id="IM1" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">BCE</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and the overall magnitude of the contact map <inline-formula id="IE2"><mml:math id="IM2" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">MAG</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> which enables sparse and realistic contact map prediction. The relative weight of these loss terms are balanced by a hyperparameter <italic toggle="yes">λ</italic>. We emphasize that D-SCRIPT requires only the amino acid sequence of a protein pair to make predictions.</p>
    </sec>
    <sec>
      <title>2.3 Background: network-based prediction with GLIDE</title>
      <p>To make top-down, network-based predictions of PPIs in a species, we use GLIDE (<xref rid="btac258-B11" ref-type="bibr">Devkota <italic toggle="yes">et al.</italic>, 2020</xref>), a state-of-the-art method that combines local (neighborhood-based) and global (spectral) graph-theoretic techniques for quantifying the likelihood of an interaction between every protein-pair in the network. As part of our initial explorations, we also evaluated node2vec (<xref rid="btac258-B16" ref-type="bibr">Grover and Leskovec, 2016</xref>), another spectral approach for link prediction. However, we found GLIDE to outperform node2vec substantially on the PPI link prediction task (Section 3.1) and hence chose it as the link prediction technique in this article. GLIDE combines a simple local score that captures shared-neighbor relationships in the dense core with a diffusion-based embedding that encapsulates the network structure in the periphery. While local metrics accurately capture the likelihood of links between proteins in the same local neighborhood, their performance drops significantly as the distance between proteins increases. The opposite is true for global metrics.</p>
      <p>GLIDE incorporates both local and global metrics into a single score in such a way that each metric is leveraged in the region of the network where it is most accurate. We use Common Weighted Normalized (CWN) as our local metric, and the inverse of the Diffusion State Distance (<inline-formula id="IE3"><mml:math id="IM3" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mtext>UDSED</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>) as our global metric while computing the GLIDE score. For a more detailed description of CWN and <inline-formula id="IE4"><mml:math id="IM4" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mtext>UDSED</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> metrics, see the <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix S1.1</xref>.</p>
      <p>Following <xref rid="btac258-B11" ref-type="bibr">Devkota <italic toggle="yes">et al.</italic> (2020)</xref>, we compute the aggregate GLIDE score between each pair of nodes as:
<disp-formula id="E1"><label>(1)</label><mml:math id="M1" display="block" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">GLIDE</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>exp</mml:mtext><mml:mo> </mml:mo><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo>α</mml:mo><mml:mo>·</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>β</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow><mml:mi>C</mml:mi><mml:mi>W</mml:mi><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo> </mml:mo><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where (<italic toggle="yes">p</italic>, <italic toggle="yes">q</italic>) is a candidate protein pair and <inline-formula id="IE5"><mml:math id="IM5" display="inline" overflow="scroll"><mml:mrow><mml:mi>u</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msup><mml:mrow><mml:mtext>UDSED</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We chose the default values of <italic toggle="yes">α</italic> and <italic toggle="yes">β</italic> as suggested by <xref rid="btac258-B11" ref-type="bibr">Devkota <italic toggle="yes">et al.</italic> (2020)</xref> (<inline-formula id="IE6"><mml:math id="IM6" display="inline" overflow="scroll"><mml:mrow><mml:mo>α</mml:mo><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn><mml:mo>,</mml:mo><mml:mo>β</mml:mo><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>). These choices make the local embedding dominant, whenever available, with the global embedding being used to break ties and order nodes with the same local score. For the CWN local score, node-pairs with no common neighbors will have <inline-formula id="IE7"><mml:math id="IM7" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">CWN</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and only the global <italic toggle="yes">u</italic> term will be used.</p>
    </sec>
    <sec>
      <title>2.4 Network-dependent loss term</title>
      <p>Topsy-Turvy retains the protein language model feature generation and convolutional neural net architecture of D-SCRIPT, with changes made to the training approach and loss function. To synthesize this model with link-based prediction, we introduce the additional task of predicting GLIDE scores between proteins, formulating it as an extra loss term in the objective. The entire model is then trained end-to-end.</p>
      <p>In the original D-SCRIPT model, the loss function was a weighted sum, <inline-formula id="IE8"><mml:math id="IM8" display="inline" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>λ</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">BCE</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>λ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">MAG</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, that combined the binary cross-entropy (BCE; <xref rid="btac258-B30" ref-type="bibr">Sledzieski <italic toggle="yes">et al.</italic>, 2021</xref>) loss with a regularization penalty related to the contact map’s magnitude. To incorporate a network term, we add a sub-objective to the classification component:
<disp-formula id="E2"><label>(2)</label><mml:math id="M2" display="block" overflow="scroll"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>λ</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">BCE</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GLIDE</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>λ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">MAG</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula>where <italic toggle="yes">L<sup>GLIDE</sup></italic> represents the loss when predicting GLIDE estimates and <inline-formula id="IE9"><mml:math id="IM9" display="inline" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> is a hyperparameter indicating its relative importance (at <italic toggle="yes">g<sub>p</sub></italic> = 0, the function reduces to the original D-SCRIPT loss). To compute <italic toggle="yes">L<sup>GLIDE</sup></italic>, we first generate GLIDE scores for every negative training example by computing the component <inline-formula id="IE10"><mml:math id="IM10" display="inline" overflow="scroll"><mml:mrow><mml:mtext>CWN</mml:mtext></mml:mrow></mml:math></inline-formula> and <inline-formula id="IE11"><mml:math id="IM11" display="inline" overflow="scroll"><mml:mrow><mml:msup><mml:mrow><mml:mtext>UDSED</mml:mtext></mml:mrow><mml:mo>γ</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> scores on the PPI network defined by the positive examples in the training set. For a protein pair (<italic toggle="yes">p</italic>, <italic toggle="yes">q</italic>), the loss <italic toggle="yes">L<sup>GLIDE</sup></italic> is defined as
<disp-formula id="E3"><label>(3)</label><mml:math id="M3" display="block" overflow="scroll"><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">GLIDE</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>;</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>BCE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo> </mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:mo> </mml:mo><mml:msub><mml:mn mathvariant="double-struck">1</mml:mn><mml:mrow><mml:mi mathvariant="italic">GLIDE</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula id="IE12"><mml:math id="IM12" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> is a hyperparameter, <italic toggle="yes">y</italic>(<italic toggle="yes">p</italic>, <italic toggle="yes">q</italic>) is Topsy-Turvy’s predicted score for the protein pair (<italic toggle="yes">p</italic>, <italic toggle="yes">q</italic>). <inline-formula id="IE13"><mml:math id="IM13" display="inline" overflow="scroll"><mml:mn mathvariant="double-struck">1</mml:mn></mml:math></inline-formula> is the indicator function corresponding to the predicate <inline-formula id="IE14"><mml:math id="IM14" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="italic">GLIDE</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. This formulation corresponds to binarizing GLIDE scores at the score threshold <italic toggle="yes">g<sub>t</sub></italic> and then applying the standard BCE loss. For convenience, we define <italic toggle="yes">g<sub>t</sub></italic> in terms of a percentile cutoff on the distribution of <italic toggle="yes">GLIDE</italic>(<italic toggle="yes">p</italic>, <italic toggle="yes">q</italic>) scores (i.e. <inline-formula id="IE15"><mml:math id="IM15" display="inline" overflow="scroll"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>), rather than directly as a numeric threshold.</p>
      <p>In formulating <italic toggle="yes">L<sup>GLIDE</sup></italic>, we chose to binarize GLIDE scores and compute a BCE loss, rather than keeping continuous-valued GLIDE scores and using a different functional form for the loss. Doing so allowed us to mimic the form of the existing BCE-based loss, letting us calibrate the relative weights of <italic toggle="yes">L<sup>BCE</sup></italic> and <italic toggle="yes">L<sup>GLIDE</sup></italic> simply by <italic toggle="yes">g<sub>p</sub></italic>. Using GLIDE’s continuous scores would have made this calibration difficult, since the un-normalized GLIDE scores are unevely distributed (for the human PPI training network: minimum = 0, median = 0.31; 75th-percentile = 0.40; maximum = 2.71) and do not follow a convenient closed form.</p>
      <p>The addition of the GLIDE loss term to the model training accounts for the observation that the original D-SCRIPT loss measures only pairwise interaction, and is unaware of global network structure. Since the GLIDE score of a protein pair takes into account local and global network properties, the GLIDE component of the loss should incorporate network-wide information into the predictions. Specifically, since D-SCRIPT prioritizes precision and is more likely to miss true interacting pairs than GLIDE, the absence of strong structural evidence of interaction could be supplemented by strong network evidence.</p>
    </sec>
    <sec>
      <title>2.5 TT-Hybrid</title>
      <p>During inference, Topsy-Turvy requires only protein sequences as input. When making predictions in a species where some PPI data is also available, predictions from pre-trained Topsy-Turvy (trained on data from another species) can be combined with GLIDE predictions informed by the target species’ PPI network. We note that these GLIDE scores are distinct from those corresponding to the training species; the latter were used only during training. To take advantage of the PPI network in the target species when available, we designed TT-Hybrid that can be applied on query protein-pairs where both GLIDE and Topsy-Turvy scores are available. We note that this requires both proteins of the queried pair to be present in the target species’ PPI network; otherwise, only Topsy-Turvy can be used. TT-Hybrid computes a weighted sum of Topsy-Turvy and GLIDE predictions for a query protein-pair, with the score for a protein pair (<italic toggle="yes">p</italic>, <italic toggle="yes">q</italic>) being:
<disp-formula id="E4"><label>(4)</label><mml:math id="M4" display="block" overflow="scroll"><mml:mrow><mml:mtext>TT-Hybrid</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>·</mml:mo><mml:mi mathvariant="italic">GLIDE</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>w</mml:mi><mml:mo>·</mml:mo><mml:mtext>Topsy-Turvy</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p>
      <p>For simplicity, we have set the weight of GLIDE scores to 1, since only the relative weighting of the two scores matters. In this article, we trained Topsy-Turvy on human PPI data and have evaluated it on other species. During the training phase, we held out some human PPI data for validation. We calibrated <italic toggle="yes">w</italic> on this held-out human data using logistic regression.</p>
      <p>We started by selecting protein pairs corresponding to the edges of the held-out human PPI subnetwork (see Section 3.2 for dataset details). These pairs were labeled positive; negatively labeled pairs corresponded to random pairs of proteins from the subnetwork. The ratio of negative to positive examples was set to 10:1 to account for the inherent class imbalance in PPI data (see Section 3.2 for discussion). To avoid bias arising from data leakage, we also required that none of the examples occur in the original training data for Topsy-Turvy. We computed GLIDE and Topsy-Turvy scores for each protein pair, these methods having previously been trained on the rest of human PPI data. We then fitted a logistic regression model that sought to predict the label of a protein pair using its GLIDE and Topsy-Turvy score. The TT-Hybrid calibration weight <italic toggle="yes">w</italic> is chosen as the ratio of logistic regression coefficients, <inline-formula id="IE16"><mml:math id="IM16" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mtext>Topsy-Turvy</mml:mtext></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mtext>GLIDE</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Our computation yielded <italic toggle="yes">w </italic>=<italic toggle="yes"> </italic>0.3268, and we recommend the use of this value when applying TT-Hybrid in other species, as is done in the results presented here. If enough PPI data is available in the target species that a portion of it can be set aside, the held-out portion can be used to calibrate <italic toggle="yes">w</italic> specifically for the target species. To avoid the risk of data leakage, however, the same set of PPIs should not be used to both calibrate <italic toggle="yes">w</italic> and compute the GLIDE score inputs to TT-Hybrid.</p>
    </sec>
    <sec>
      <title>2.6 Hyperparameter selection and model training</title>
      <p>The hyperparameters <italic toggle="yes">g<sub>p</sub></italic> (the relative weight of GLIDE versus binary cross-entropy loss) and <italic toggle="yes">g<sub>t</sub></italic> (the binarization threshold for GLIDE scores) play a crucial role in Topsy-Turvy and we sought to estimate them from cross-validation runs on the human PPI dataset. We note that all Topsy-Turvy and TT-Hybrid results presented in this article are from models trained on human data but evaluated on out-of-sample, non-human data. To perform the hyperparameter search, we did cross-validation runs on the <italic toggle="yes">entire</italic> human PPI network, since GLIDE scores computed on smaller subnetworks might not be representative of the full network’s characteristics. Due to the computational expense of such runs, however, we modified the standard grid-search approach. Initial, small scale explorations suggested <italic toggle="yes">g<sub>t</sub></italic> = 90 to be a promising choice. We first performed a grid search on <italic toggle="yes">g<sub>p</sub></italic>, fixing <italic toggle="yes">g<sub>t</sub></italic> to 90. This yielded <inline-formula id="IE17"><mml:math id="IM17" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> as the suggested choice (<xref rid="btac258-T1" ref-type="table">Table 1a</xref>) and we then performed a grid search for <italic toggle="yes">g<sub>t</sub></italic>, with <italic toggle="yes">g<sub>p</sub></italic> fixed to this choice. The second search indicated <inline-formula id="IE18"><mml:math id="IM18" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>92.5</mml:mn></mml:mrow></mml:math></inline-formula> to be the best choice (<xref rid="btac258-T1" ref-type="table">Table 1b</xref>), and we accordingly chose <inline-formula id="IE19"><mml:math id="IM19" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mo> </mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>92.5</mml:mn></mml:mrow></mml:math></inline-formula> as the hyperparameter settings for Topsy-Turvy training.</p>
      <table-wrap position="float" id="btac258-T1">
        <label>Table 1.</label>
        <caption>
          <p>Hyperparameter search: cross-validation AUPR (area under precision–recall curve) scores on full human PPI network for (a) grid search for <italic toggle="yes">g<sub>p</sub></italic>, with <italic toggle="yes">g<sub>t</sub></italic> fixed to 90 (estimated from small-scale explorations), (b) grid search for <italic toggle="yes">g<sub>t</sub></italic>, with <italic toggle="yes">g<sub>p</sub></italic> fixed to 0.2 [i.e. the optimal value from (a)]</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <tbody>
            <tr>
              <td colspan="2" rowspan="1">
                <bold>(a)</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">g<sub>p</sub> (with g<italic toggle="yes"><sub>t</sub></italic> = 90)</td>
              <td align="center" rowspan="1" colspan="1">AUPR</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">0.1</td>
              <td rowspan="1" colspan="1">0.739</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>0.2</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.802</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">0.4</td>
              <td rowspan="1" colspan="1">0.759</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">0.8</td>
              <td rowspan="1" colspan="1">0.760</td>
            </tr>
            <tr>
              <td colspan="2" rowspan="1">
                <bold>(b)</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><italic toggle="yes">g<sub>t</sub></italic> (with g<italic toggle="yes"><sub>p</sub></italic> = 0.2)</td>
              <td align="center" rowspan="1" colspan="1">AUPR</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">90</td>
              <td rowspan="1" colspan="1">0.697</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <bold>92.5</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.824</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">95</td>
              <td rowspan="1" colspan="1">0.691</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">97.5</td>
              <td rowspan="1" colspan="1">0.690</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn1">
            <p><italic toggle="yes">Note</italic>: The metrics reported in the tables are the validation AUPR scores maximized over three epochs of training.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <sec>
        <title>2.6.1 Additional implementation details</title>
        <p>We implemented Topsy-Turvy in PyTorch 1.2.0 and trained with a NVIDIA Tesla V100 with 32 GB of memory. Embeddings from the pre-trained Bepler and Berger model were produced by concatenating the final values of the output and all hidden layers. Apart from these pre-trained embeddings, Topsy-Turvy was trained end-to-end and did not use pre-trained D-SCRIPT model weights. However, we used the same hyperparameters as in <xref rid="btac258-B30" ref-type="bibr">Sledzieski <italic toggle="yes">et al.</italic> (2021)</xref> for the relevant components of our model’s architecture: a projection dimension of <italic toggle="yes">d </italic>=<italic toggle="yes"> </italic>100, a hidden dimension of <italic toggle="yes">h </italic>=<italic toggle="yes"> </italic>50, a convolutional filter with width <inline-formula id="IE20"><mml:math id="IM20" display="inline" overflow="scroll"><mml:mrow><mml:mn>2</mml:mn><mml:mi>w</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula>, and a local max-pooling width of <italic toggle="yes">l </italic>=<italic toggle="yes"> </italic>9. Furthermore, we used <inline-formula id="IE21"><mml:math id="IM21" display="inline" overflow="scroll"><mml:mrow><mml:mo>λ</mml:mo><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula> for calculating the training loss, choosing it based on early, small-scale explorations. Weights were initialized using PyTorch defaults. Model training parameters were set within ranges commonly used in deep learning literature: we used a batch size of 25, the Adam optimizer with a learning rate of 0.001, and trained all models for 10 epochs.</p>
      </sec>
    </sec>
  </sec>
  <sec>
    <title>3 Results</title>
    <p>We start by presenting a comparative assessment of GLIDE and node2vec for PPI link prediction; the results of this analysis motivated our choice of GLIDE as the network-theoretic component of the Topsy-Turvy model. We next evaluate the cross-species generalizability of Topsy-Turvy, showing how incorporating network data during training results in superior performance in other species, using only sequence data for prediction. We note that in the typical cross-species setting, purely network-based methods like GLIDE are not applicable since they can only make predictions for pairs where both proteins exist in the training PPI network and hence cannot be applied to out-of-sample proteins. We therefore evaluated Topsy-Turvy against methods that require only sequence-based inputs (like D-SCRIPT), assessing if co-supervising Topsy-Turvy with topological information allows it to learn aspects of protein interaction that carry across species. As we show, it does, and in subsequent analyses we investigate various aspects of the comparison more deeply, also addressing the issue of ascertainment bias in the evaluation network. Lastly, we study how to best apply Topsy-Turvy in instances where PPI data <italic toggle="yes">is</italic> available and GLIDE would be applicable directly. We find that while GLIDE is broadly informative about the species-specific network rewiring, better performance can be achieved by TT-Hybrid, a combination of Topsy-Turvy and GLIDE.</p>
    <sec>
      <title>3.1 Comparison of GLIDE and node2vec</title>
      <p>In our initial explorations, we sought to identify the most appropriate top-down PPI link prediction technique. Toward this, we compared GLIDE to node2vec (<xref rid="btac258-B16" ref-type="bibr">Grover and Leskovec, 2016</xref>). The node2vec algorithm, also a spectral approach, uses a biased random walk procedure to construct low-dimensional node embeddings. Following the original study, we trained a logistic regression classifier on the Hadamard product of the node embeddings to predict the existence of a link given two candidate proteins. We compared the two methods on the <italic toggle="yes">Drosophila</italic> BioGRID network consisting of 3093 nodes and 25 427 edges. A certain fraction <inline-formula id="IE22"><mml:math id="IM22" display="inline" overflow="scroll"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> of the edges were removed from the network (while protecting a random spanning tree to ensure connectivity), and the remaining subnetwork was used to train the node2vec and the GLIDE models. The removed edges were then used as positive test examples for evaluation. For negative examples, we randomly sampled 254 270 node-pairs (or 10 times the positive edge count) that were not present in the original network. The negative examples, like the positive edges, were also separated into train and test sets using the same parameter <italic toggle="yes">p</italic>. The dimension of the node2vec embedding was set to 300, i.e. approximately 10% of the node count [following <xref rid="btac258-B7" ref-type="bibr">Cho <italic toggle="yes">et al.</italic> (2016)</xref>; this is also higher than the minimum value of 100, as prescribed by Grover et al.]. We evaluated both node2vec and GLIDE for different values of <italic toggle="yes">p</italic> (which correspond to varying levels of network sparsity), finding that GLIDE outperformed node2vec consistently (<xref rid="btac258-T2" ref-type="table">Table 2</xref>).</p>
      <table-wrap position="float" id="btac258-T2">
        <label>Table 2.</label>
        <caption>
          <p>GLIDE and node2vec comparison: AUPR scores for PPI prediction on the <italic toggle="yes">Drosophila</italic> BioGRID network</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">
                <italic toggle="yes">p</italic>
              </th>
              <th rowspan="1" colspan="1">GLIDE</th>
              <th rowspan="1" colspan="1">node2vec</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">0.8</td>
              <td rowspan="1" colspan="1">
                <bold>0.737</bold>
              </td>
              <td rowspan="1" colspan="1">0.681</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">0.6</td>
              <td rowspan="1" colspan="1">
                <bold>0.818</bold>
              </td>
              <td rowspan="1" colspan="1">0.721</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">0.4</td>
              <td rowspan="1" colspan="1">
                <bold>0.839</bold>
              </td>
              <td rowspan="1" colspan="1">0.664</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">0.2</td>
              <td rowspan="1" colspan="1">
                <bold>0.805</bold>
              </td>
              <td rowspan="1" colspan="1">0.574</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn2">
            <p><italic toggle="yes">Note</italic>: Higher values of <italic toggle="yes">p</italic> correspond to a higher proportion of edges preserved in the training network. Bold entries represent best performance.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.2 Integrating network-level information improves predictive performance</title>
      <p><italic toggle="yes">Datasets</italic>: We trained Topsy-Turvy on human PPI data and evaluated it on <italic toggle="yes">Mus</italic> <italic toggle="yes">musculus</italic>, <italic toggle="yes">Drosophila</italic> <italic toggle="yes">melanogaster</italic>, <italic toggle="yes">Caenorhabditis</italic> <italic toggle="yes">elegans</italic>, <italic toggle="yes">Saccharomyces</italic> <italic toggle="yes">cerevisiae</italic> and <italic toggle="yes">Escherichia</italic> <italic toggle="yes">coli</italic>. The dataset selection and pre-processing follows <xref rid="btac258-B30" ref-type="bibr">Sledzieski <italic toggle="yes">et al.</italic> (2021)</xref>: we sourced positive examples from the STRING database (v11) (<xref rid="btac258-B31" ref-type="bibr">Szklarczyk <italic toggle="yes">et al.</italic>, 2021</xref>), selecting only physical binding interactions associated with a positive experimental-evidence score. Our human PPI set consists of 47 932 positive and 479 320 negative protein interactions, of which we set apart 80% (38 345) for training and 20% (9587) for validation (see <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix S1.2</xref> for details). For each of 5 model organisms (<xref rid="btac258-T3" ref-type="table">Table 3</xref>) we selected 5000 positive interactions and 50 000 negative interactions using this procedure, with the exception of <italic toggle="yes">E.coli</italic> (2000/20 000) where the available set of positive examples in STRING was limited. Each model was trained three times, with different random seeds, and we evaluated the average performance across these runs. We emphasize that Topsy-Turvy is trained end-to-end and does not use a pretrained D-SCRIPT sub-component. For benchmarking, a separate D-SCRIPT model was trained and evaluated identically.</p>
      <table-wrap position="float" id="btac258-T3">
        <label>Table 3.</label>
        <caption>
          <p>Topsy-Turvy improves upon D-SCRIPT (<xref rid="btac258-B30" ref-type="bibr">Sledzieski <italic toggle="yes">et al.</italic>, 2021</xref>), PIPR (<xref rid="btac258-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2019</xref>) and DeepPPI (<xref rid="btac258-B27" ref-type="bibr">Richoux <italic toggle="yes">et al.</italic>, 2019</xref>) for cross-species PPI prediction</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="2" colspan="1">Species</th>
              <th rowspan="2" colspan="1">Model</th>
              <th rowspan="2" colspan="1">AUPR</th>
              <th rowspan="2" colspan="1">AUROC</th>
              <th colspan="2" align="center" rowspan="1">FPR<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1">0.1 Recall</th>
              <th rowspan="1" colspan="1">0.5 Recall</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">M.musculus</italic>
              </td>
              <td rowspan="1" colspan="1">PIPR</td>
              <td rowspan="1" colspan="1">0.526</td>
              <td rowspan="1" colspan="1">0.839</td>
              <td rowspan="1" colspan="1">0.002</td>
              <td rowspan="1" colspan="1">0.057</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">DeepPPI</td>
              <td rowspan="1" colspan="1">0.518</td>
              <td rowspan="1" colspan="1">0.816</td>
              <td rowspan="1" colspan="1">
                <bold>0.0002</bold>
              </td>
              <td rowspan="1" colspan="1">0.059</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">D-SCRIPT</td>
              <td rowspan="1" colspan="1">0.663 ± 0.05</td>
              <td rowspan="1" colspan="1">0.901 ± 0.02</td>
              <td rowspan="1" colspan="1">0.002</td>
              <td rowspan="1" colspan="1">0.014</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Topsy-Turvy</td>
              <td rowspan="1" colspan="1"><bold>0.735</bold>  <inline-formula id="IE23"><mml:math id="IM23" display="inline" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>  <bold>0.03</bold></td>
              <td rowspan="1" colspan="1"><bold>0.934</bold>  <inline-formula id="IE24"><mml:math id="IM24" display="inline" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>  <bold>0.01</bold></td>
              <td rowspan="1" colspan="1">0.001</td>
              <td rowspan="1" colspan="1">
                <bold>0.009</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">D.melanogaster</italic>
              </td>
              <td rowspan="1" colspan="1">PIPR</td>
              <td rowspan="1" colspan="1">0.278</td>
              <td rowspan="1" colspan="1">0.728</td>
              <td rowspan="1" colspan="1">0.007</td>
              <td rowspan="1" colspan="1">0.197</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">DeepPPI</td>
              <td rowspan="1" colspan="1">0.231</td>
              <td rowspan="1" colspan="1">0.659</td>
              <td rowspan="1" colspan="1">0.012</td>
              <td rowspan="1" colspan="1">0.274</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">D-SCRIPT</td>
              <td rowspan="1" colspan="1">0.605 ± 0.06</td>
              <td rowspan="1" colspan="1">0.890 ± 0.02</td>
              <td rowspan="1" colspan="1">0.003</td>
              <td rowspan="1" colspan="1">0.022</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Topsy-Turvy</td>
              <td rowspan="1" colspan="1"><bold>0.713</bold>  <inline-formula id="IE25"><mml:math id="IM25" display="inline" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>  <bold>0.05</bold></td>
              <td rowspan="1" colspan="1"><bold>0.921</bold>  <inline-formula id="IE26"><mml:math id="IM26" display="inline" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>  <bold>0.02</bold></td>
              <td rowspan="1" colspan="1">
                <bold>0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.011</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">C.elegans</italic>
              </td>
              <td rowspan="1" colspan="1">PIPR</td>
              <td rowspan="1" colspan="1">0.346</td>
              <td rowspan="1" colspan="1">0.757</td>
              <td rowspan="1" colspan="1">0.002</td>
              <td rowspan="1" colspan="1">0.148</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">DeepPPI</td>
              <td rowspan="1" colspan="1">0.252</td>
              <td rowspan="1" colspan="1">0.671</td>
              <td rowspan="1" colspan="1">0.007</td>
              <td rowspan="1" colspan="1">0.252</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">D-SCRIPT</td>
              <td rowspan="1" colspan="1">0.550 ± 0.08</td>
              <td rowspan="1" colspan="1">0.853 ± 0.04</td>
              <td rowspan="1" colspan="1">0.003</td>
              <td rowspan="1" colspan="1">0.032</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Topsy-Turvy</td>
              <td rowspan="1" colspan="1"><bold>0.700</bold>  <inline-formula id="IE27"><mml:math id="IM27" display="inline" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>  <bold>0.04</bold></td>
              <td rowspan="1" colspan="1"><bold>0.906</bold>  <inline-formula id="IE28"><mml:math id="IM28" display="inline" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>  <bold>0.03</bold></td>
              <td rowspan="1" colspan="1">
                <bold>0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.011</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">S.cerevisiae</italic>
              </td>
              <td rowspan="1" colspan="1">PIPR</td>
              <td rowspan="1" colspan="1">0.230</td>
              <td rowspan="1" colspan="1">0.718</td>
              <td rowspan="1" colspan="1">0.017</td>
              <td rowspan="1" colspan="1">0.213</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">DeepPPI</td>
              <td rowspan="1" colspan="1">0.201</td>
              <td rowspan="1" colspan="1">0.652</td>
              <td rowspan="1" colspan="1">0.018</td>
              <td rowspan="1" colspan="1">0.288</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">D-SCRIPT</td>
              <td rowspan="1" colspan="1">0.399 ± 0.09</td>
              <td rowspan="1" colspan="1">0.790 ± 0.06</td>
              <td rowspan="1" colspan="1">0.005</td>
              <td rowspan="1" colspan="1">0.089</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Topsy-Turvy</td>
              <td rowspan="1" colspan="1"><bold>0.534</bold>  <inline-formula id="IE29"><mml:math id="IM29" display="inline" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>  <bold>0.01</bold></td>
              <td rowspan="1" colspan="1"><bold>0.850</bold>  <inline-formula id="IE30"><mml:math id="IM30" display="inline" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>  <bold>0.02</bold></td>
              <td rowspan="1" colspan="1">
                <bold>0.002</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.038</bold>
              </td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">
                <italic toggle="yes">E.coli</italic>
              </td>
              <td rowspan="1" colspan="1">PIPR</td>
              <td rowspan="1" colspan="1">0.271</td>
              <td rowspan="1" colspan="1">0.675</td>
              <td rowspan="1" colspan="1">0.005</td>
              <td rowspan="1" colspan="1">0.246</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">DeepPPI</td>
              <td rowspan="1" colspan="1">0.271</td>
              <td rowspan="1" colspan="1">0.688</td>
              <td rowspan="1" colspan="1">0.004</td>
              <td rowspan="1" colspan="1">0.243</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">D-SCRIPT</td>
              <td rowspan="1" colspan="1">0.513 ± 0.09</td>
              <td rowspan="1" colspan="1">0.770 ± 0.03</td>
              <td rowspan="1" colspan="1">0.002</td>
              <td rowspan="1" colspan="1">0.040</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"/>
              <td rowspan="1" colspan="1">Topsy-Turvy</td>
              <td rowspan="1" colspan="1"><bold>0.556</bold>  <inline-formula id="IE31"><mml:math id="IM31" display="inline" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>  <bold>0.09</bold></td>
              <td rowspan="1" colspan="1"><bold>0.805</bold>  <inline-formula id="IE32"><mml:math id="IM32" display="inline" overflow="scroll"><mml:mo>±</mml:mo></mml:math></inline-formula>  <bold>0.07</bold></td>
              <td rowspan="1" colspan="1">
                <bold>0.001</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.038</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn3">
            <p><italic toggle="yes">Note</italic>: All species were evaluated using models trained on a large corpus of human PPIs. For D-SCRIPT and Topsy-Turvy, we report the average and standard deviation of results from three random initializations. For PIPR and DeepPPI, we report here the results from the study in <xref rid="btac258-B30" ref-type="bibr">Sledzieski <italic toggle="yes">et al.</italic> (2021)</xref> where the same evaluation scheme and data was used. For all datasets, there is a 1:10 ratio of positive to negative pairs, which means a random baseline would have an AUPR of 0.091 and an AUROC of 0.5. Bold entries represent best performance.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>In <xref rid="btac258-T3" ref-type="table">Table 3</xref>, we report the area under precision recall curve (AUPR) and area under receiver operating curve (AUROC) for each model in each species. As our dataset and evaluation approach is the same as in <xref rid="btac258-B30" ref-type="bibr">Sledzieski <italic toggle="yes">et al.</italic> (2021)</xref>, we also include results reported there for two other state-of-the-art sequence-based PPI prediction methods, PIPR (<xref rid="btac258-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2019</xref>) and DeepPPI (<xref rid="btac258-B27" ref-type="bibr">Richoux <italic toggle="yes">et al.</italic>, 2019</xref>). We note that for unbalanced data, AUPR is generally considered the more representative metric. We also report the false positive rate (FPR) at 10% and 50% recall, which measures the likelihood that a protein pair predicted to interact is incorrectly classified—an important metric in the case where high-likelihood pairs are then tested experimentally. We find that Topsy-Turvy achieves the highest AUPR and AUROC of all the methods we evaluated in each of five species, and has the lowest FPR at both recall levels. We also observe that Topsy-Turvy retains the structural interpretability of D-SCRIPT: for each queried protein pair, the model also outputs a predicted inter-protein contact map for the putative binding between the two proteins.</p>
      <p><italic toggle="yes">Runtime and memory usage</italic>: Topsy-Turvy took approximately 79 h to train for 10 epochs on 421 792 training pairs, and fits within a single 32GB GPU. Running time and GPU memory usage, like in D-SCRIPT, scales quadratically, <inline-formula id="IE33"><mml:math id="IM33" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, with protein lengths <italic toggle="yes">n</italic>, <italic toggle="yes">m</italic>, since Topsy-Turvy models the full <italic toggle="yes">n </italic>×<italic toggle="yes"> m</italic> contact map as an intermediate step. The prediction of new candidate pairs with a trained model is very fast, requiring on average 0.02 s/pair. Since Topsy-Turvy generalizes well across species, it needs to be trained only once on a large corpus of data and can be used to make predictions in a variety of settings. The additional run time for TT-Hybrid is minimal (approx. 15 minutes, most of it for GLIDE) since it just computes a weighted sum of predictions from Topsy-Turvy and GLIDE. The actual computation of TT-Hybrid scores, provided that the Topsy-Turvy and GLIDE results are already available, is a linear time operation (less than 1 minutes for the candidate set with 10 million pairs) since it is simply a weighted sum of the two.</p>
      <sec>
        <title>3.2.1 Ablation study: using network-level information for negative edge selection</title>
        <p>Notably, Topsy-Turvy achieves greater cross-species generalization even though network information is used only during training. We hypothesize this may be partially due to GLIDE-based interaction scores mitigating the impact of incorrect labels in training data. To create negative training examples, we followed the common practice of randomly selecting protein pairs not experimentally reported as interacting (<xref rid="btac258-B6" ref-type="bibr">Chen <italic toggle="yes">et al.</italic>, 2019</xref>; <xref rid="btac258-B18" ref-type="bibr">Hashemifar <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac258-B30" ref-type="bibr">Sledzieski <italic toggle="yes">et al.</italic>, 2021</xref>). However, it might be that such a pair actually <italic toggle="yes">does</italic> interact but has not yet been experimentally assayed. In such cases, the GLIDE score for the pair is likely to be high, thus improving the supervision and training of Topsy-Turvy. To further investigate our hypothesis, we evaluated an alternative approach to incorporating network topology in the model, by modifying the set of negative examples in the training set to reflect network information. Prior work in PPI prediction has argued that better selection of negative samples in the training set could improve the model, with <xref rid="btac258-B35" ref-type="bibr">Zhang <italic toggle="yes">et al.</italic> (2018)</xref> exploring a random-walk distance on the PPI graph to distinguish between and low- and high-confidence negative examples. We explored the strategy of selecting only protein pairs with low GLIDE scores as negative examples, but found the performance to be poorer than the baseline. Drilling down, we found that this was due to a reduction in diversity of negative examples available for training, since using graph-theoretic measures to select negative examples restricts us to nodes occurring in the training PPI network (<xref rid="sup1" ref-type="supplementary-material">Supplementary Fig. SA.1 and Appendix S1.3</xref>). In contrast, our incorporation of GLIDE scores in the objective allows us to handle a broader set of negative examples.</p>
      </sec>
    </sec>
    <sec>
      <title>3.3 Cross-species improvement is not limited to hub nodes</title>
      <p>Noting that Topsy-Turvy makes use of global PPI organization in the training phase but makes predictions solely using sequence data, we sought to characterize the kind of topological knowledge being learned by the trained model. Specifically, we investigated if the performance improvement of Topsy-Turvy over D-SCRIPT was limited to certain categories of proteins/nodes.</p>
      <p>Since network-based methods work by learning network connectivity patterns, and some network structure is conserved across species, such methods tend to work well for proteins that already have many known interactions. Thus, it could be possible that the outperformance of Topsy-Turvy comes exclusively or primarily from, say, hub nodes whose interactions may be better conserved across species. To investigate this, we evaluated human-PPI trained Topsy-Turvy and D-SCRIPT on physical interactions in <italic toggle="yes">D.melanogaster</italic>, sourcing the latter from BioGRID (we found BioGRID’s fly PPI annotations clearer than STRING’s). Limiting ourselves to fly proteins that occur in the PPI network, we partitioned the fly evaluation set into four sub-groups by degree: each putative edge (<italic toggle="yes">p</italic>, <italic toggle="yes">q</italic>) was grouped as per <inline-formula id="IE34"><mml:math id="IM34" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi mathvariant="script">M</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="italic">max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <italic toggle="yes">d</italic>(<italic toggle="yes">p</italic>) and <italic toggle="yes">d</italic>(<italic toggle="yes">q</italic>) are the degrees of <italic toggle="yes">p</italic> and <italic toggle="yes">q</italic> in the fly PPI network, respectively. Thus, the sub-group corresponding to <inline-formula id="IE35"><mml:math id="IM35" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi><mml:mo>≥</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:math></inline-formula> consists of putative interactions where at least one of the proteins is a hub-like protein.</p>
      <p>Even though baseline D-SCRIPT is not explicitly informed about network structure, it too demonstrated better performance as <inline-formula id="IE36"><mml:math id="IM36" display="inline" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi></mml:math></inline-formula> increased. This may be due to the information encoded in the frequency with which each protein appears in the positive examples D-SCRIPT is trained on. Because of that, along with stronger conservation of PPIs involving hub nodes (<xref rid="btac258-B3" ref-type="bibr">Brown and Jurisica, 2007</xref>; <xref rid="btac258-B13" ref-type="bibr">Fox <italic toggle="yes">et al.</italic>, 2009</xref>), some network aspects can be implicitly learned by a purely sequence-based approach like D-SCRIPT. This also illustrates one of the core points of this article—the connection between bottom-up and top-down views of protein interaction.</p>
      <p>We also observed that Topsy-Turvy improved upon D-SCRIPT in each sub-group, indicating that the outperformance is not only coming from high-degree nodes. While Topsy-Turvy also achieves its highest performance on the <inline-formula id="IE37"><mml:math id="IM37" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi><mml:mo>≥</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:math></inline-formula> sub-group, its improvement over D-SCRIPT is not limited to the highest-degree hub nodes. In fact, the relative AUPR improvement of Topsy-Turvy over D-SCRIPT is 2.22-fold when <inline-formula id="IE38"><mml:math id="IM38" display="inline" overflow="scroll"><mml:mi mathvariant="script">M</mml:mi></mml:math></inline-formula> is in the 2–20 range, compared to a 1.31-fold improvement for hub nodes (<inline-formula id="IE39"><mml:math id="IM39" display="inline" overflow="scroll"><mml:mrow><mml:mi mathvariant="script">M</mml:mi><mml:mo>≥</mml:mo></mml:mrow></mml:math></inline-formula> 21) (<xref rid="btac258-T4" ref-type="table">Table 4</xref>). Topsy-Turvy thus not only improves predictive performance for high-degree nodes, but the GLIDE loss term additionally informs the model about global structure, leading to improvement for more sparsely connected nodes.</p>
      <table-wrap position="float" id="btac258-T4">
        <label>Table 4.</label>
        <caption>
          <p>Cross-species performance of D-SCRIPT and Topsy-Turvy, subdivided by node degree in target species</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
            <col valign="top" align="center" span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Model</th>
              <th rowspan="1" colspan="1">Overall AUPR</th>
              <th colspan="4" rowspan="1">AUPR by maximum degree<hr/></th>
            </tr>
            <tr>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1"/>
              <th rowspan="1" colspan="1">
                <inline-formula id="IE40">
                  <mml:math id="IM40" display="inline" overflow="scroll">
                    <mml:mrow>
                      <mml:mn>2</mml:mn>
                      <mml:mo>−</mml:mo>
                      <mml:mn>5</mml:mn>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </th>
              <th rowspan="1" colspan="1">
                <inline-formula id="IE41">
                  <mml:math id="IM41" display="inline" overflow="scroll">
                    <mml:mrow>
                      <mml:mn>6</mml:mn>
                      <mml:mo>−</mml:mo>
                      <mml:mn>10</mml:mn>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </th>
              <th rowspan="1" colspan="1">
                <inline-formula id="IE42">
                  <mml:math id="IM42" display="inline" overflow="scroll">
                    <mml:mrow>
                      <mml:mn>11</mml:mn>
                      <mml:mo>−</mml:mo>
                      <mml:mn>20</mml:mn>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </th>
              <th rowspan="1" colspan="1">
                <inline-formula id="IE43">
                  <mml:math id="IM43" display="inline" overflow="scroll">
                    <mml:mrow>
                      <mml:mo>≥</mml:mo>
                      <mml:mn>21</mml:mn>
                    </mml:mrow>
                  </mml:math>
                </inline-formula>
              </th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1">D-SCRIPT</td>
              <td rowspan="1" colspan="1">0.356</td>
              <td rowspan="1" colspan="1">0.030</td>
              <td rowspan="1" colspan="1">0.067</td>
              <td rowspan="1" colspan="1">0.118</td>
              <td rowspan="1" colspan="1">0.475</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1">Topsy-Turvy</td>
              <td rowspan="1" colspan="1">
                <bold>0.538</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.073</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.168</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.237</bold>
              </td>
              <td rowspan="1" colspan="1">
                <bold>0.622</bold>
              </td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn4">
            <p><italic toggle="yes">Note</italic>: Both methods were trained on human PPI data and tested on fly (BioGRID). The analysis is limited to protein pairs where both proteins occur in the fly PPI graph. In addition to overall AUPR, we also group each protein pair by the maximum of the degrees of its nodes in the fly PPI network. Both methods improve as maximum degree increases, and Topsy-Turvy consistently outperforms D-SCRIPT across all subsets—especially so for putative interactions between low-degree nodes. Bold entries represent best performance.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
    <sec>
      <title>3.4 Topsy-Turvy’s improved performance is unlikely to be driven by ascertainment bias</title>
      <p>In the setting where bottom-up sequence methods are compared to top-down network-based methods (or synthesis approaches like Topsy-Turvy), issues of ascertainment bias (<xref rid="btac258-B5" ref-type="bibr">Carter <italic toggle="yes">et al.</italic>, 2013</xref>) in the available ground truth network data become particularly acute. The issue is a simple one: existing PPI network data in all organisms [with the possible exception of recently described HuRI (<xref rid="btac258-B25" ref-type="bibr">Luck <italic toggle="yes">et al.</italic>, 2020</xref>)] is biased toward pairs of proteins a biologist decided to experimentally test for interaction, and biologists are more likely to include proteins already known to be of interest, or nodes that are already adjacent to other previously studied nodes in the network. The result is that nearly all ground-truth existing networks will over-estimate the performance of methods that incorporate network information, and under-estimate the performance of methods that utilize only sequence information, since missing edges are more likely to be falsely scored as negatives for the sequence-based methods. When comparing network methods against network methods, or sequence methods against sequence methods, the respective alternative is likely to be similarly biased, making it less of a concern. However, when comparing methods across both types of information, addressing the bias becomes more important.</p>
      <p>Our results in Section 3.3 begin to address the issue of ascertainment bias. Although the BioGRID <italic toggle="yes">D.melanogaster</italic> network is not fully unbiased, if the improvement of Topsy-Turvy over D-SCRIPT were coming only from this bias, we would expect to see disproportionate improvement in the dense core of the network, where interactions are most likely to be experimentally tested. Instead, we see improvement across the network, which suggests that Topsy-Turvy’s cross-species performance gains come from successfully learning global network organization properties rather than suffering from ascertainment bias. We discuss the issue of this bias and how it might be addressed by future methods further in Discussion.</p>
    </sec>
    <sec>
      <title>3.5 Comparison with AlphaFold-Multimer</title>
      <p>We next investigated if recent advances in protein structure determination (<xref rid="btac258-B20" ref-type="bibr">Jumper <italic toggle="yes">et al.</italic>, 2021</xref>) that have enabled extremely high-quality protein complex structure prediction (in particular, AlphaFold-Multimer), could be leveraged for PPI prediction. While these methods were not designed to directly address <italic toggle="yes">if</italic> two proteins interact—they only predict the putative complex structure <italic toggle="yes">assuming</italic> an interaction—we investigated if AlphaFold-Multimer could nonetheless be adapted for our PPI prediction setting. From AlphaFold-Multimer results, we obtained their reported ipTM (interface predicted template modeling) score, a value between 0 and 1, that was shown in the original study to be correlated with the quality of the docked complex (DockQ score). For each candidate protein pair, we compute its mean ipTM score over the five AlphaFold-Multimer models. In our evaluations, we used this score as a predictor of protein interaction and assessed AlphaFold-Multimer on PPIs from the STRING <italic toggle="yes">D.</italic> <italic toggle="yes">melanogaster</italic> testing set used in Section 3.2.</p>
      <p>We find that AlphaFold-Multimer is several orders of magnitude slower than Topsy-Turvy, requiring an average of 6 h per pair (AlphaFold-reported time, min = 2.87 h, mean = 5.89 h, max = 12.97 h) compared to 0.02 seconds per pair for Topsy-Turvy (hardware described in Section 2.6.1). Of the total AlphaFold-Multimer runtime, an average of 3.22 h were spent on feature generation (min = 1.62 h, max = 8.34 h) and 2.66 h were GPU time spent on model computation (min = 1.16 h, max = 4.64 h). We note that feature generation time cannot necessarily be amortized over input pairs, since an important part of adapting AlphaFold to protein complexes is the proper alignment of paired multiple sequence alignments (MSAs) for each candidate protein pair. Thus, AlphaFold-Multimer is infeasible for genome-scale <italic toggle="yes">de novo</italic> PPI prediction for organisms with limited experimental data.</p>
      <p>We compared AlphaFold-Multimer PPI predictions with those of Topsy-Turvy in a small-scale study, constrained by the computational requirements of AlphaFold-Multimer. We selected 18 candidate pairs that span the range of Topsy-Turvy scores as well as ground-truth labels: six protein-pairs each with high (<inline-formula id="IE44"><mml:math id="IM44" display="inline" overflow="scroll"><mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>), medium (<inline-formula id="IE45"><mml:math id="IM45" display="inline" overflow="scroll"><mml:mrow><mml:mn>0.25</mml:mn><mml:mo>≤</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>) or low (<inline-formula id="IE46"><mml:math id="IM46" display="inline" overflow="scroll"><mml:mrow><mml:mo>≤</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula>) Topsy-Turvy prediction scores, with three truly interacting and three non-interacting pairs in each subset. We note that distribution of Topsy-Turvy scores on these pairs is not representative of their full-sample distribution; for example, we expressly included examples where Topsy-Turvy was very confident but wrong, even though such instances comprise a small part of the broader distribution (89.8% of Topsy-Turvy scores are &lt; 0.05). We found general agreement between AlphaFold-Multimer and Topsy-Turvy’s predictions (Pearson’s <inline-formula id="IE47"><mml:math id="IM47" display="inline" overflow="scroll"><mml:mrow><mml:mo>ρ</mml:mo><mml:mo>=</mml:mo><mml:mn>0.310</mml:mn></mml:mrow></mml:math></inline-formula>), though there were examples where each method correctly predicted an interaction that the other missed. Full results are available in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix S1.7</xref>. Compared to Topsy-Turvy, AlphaFold-Multimer’s scores seem calibrated for fewer false positives and more false negatives. In particular, AlphaFold-Multimer only scored two pairs with probability <inline-formula id="IE48"><mml:math id="IM48" display="inline" overflow="scroll"><mml:mrow><mml:mo>≥</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula> both of which were true positives and also had high Topsy-Turvy scores; all other pairs were scored under 0.45. On three Topsy-Turvy false positives where it was highly confident but incorrect, AlphaFold-Multimer ipTM scores were low (mean = 0.3676). Conversely, AlphaFold-Multimer had substantial false negatives, missing three true interactions pairs that Topsy-Turvy correctly identified with medium or high probability. For pairs that Topsy-Turvy scored low, AlphaFold-Multimer agreed with it, with low ipTM scores (mean = 0.365).</p>
      <p>These results suggest that Topsy-Turvy and AlphaFold-Multimer can each fill a valuable niche for predicting PPIs. Due to its low FPR, AlphaFold can be used to verify shortlisted interactions and accurately determine their complex structure. However, due to its run time constraints, it is infeasible to use for genome-scale predictions, a domain for which Topsy-Turvy would be more suitable. Additionally, the ipTM score is more a measure of complex stability than a predicted probability of interaction. Future work could seek to adapt the AlphaFold-Multimer architecture to explicitly address the PPI <italic toggle="yes">prediction</italic> task. For example, the calibration of interaction scores could be improved using insights gained from complete cross-docking approaches (<xref rid="btac258-B24" ref-type="bibr">Lopes <italic toggle="yes">et al.</italic>, 2013</xref>). Recently, <xref rid="btac258-B10" ref-type="bibr">Dequeker <italic toggle="yes">et al.</italic> (2022)</xref> have described physics-based energy, interface matching and protein sociability as useful metrics for identifying the likely partners from an all-versus-all docking study.</p>
    </sec>
    <sec>
      <title>3.6 Integrative methods are applicable even in species with some available PPI data</title>
      <p>We have shown that human-trained Topsy-Turvy improves on human-trained D-SCRIPT when predicting PPIs in an organism using only sequence information (Sections 3.2–3.4). In non-model organisms, there might not be any experimentally tested physical interaction data—this is the situation for which D-SCRIPT was designed, and for which we have thus far tested Topsy-Turvy. However, we are also interested in applying Topsy-Turvy to predict PPIs in the case where some sparse network does exist in the species of interest. Specifically, we ask the following question: if some network edges exist in the target species of interest, should one use a purely network-based method, or a synthesis method like Topsy-Turvy when predicting new PPIs? Sequence-based synthesis methods are necessary to attach previously unseen proteins to an existing network, but either method could be used to predict new interactions between proteins already in the network. Here, we show that a hybrid of Topsy-Turvy and GLIDE (TT-Hybrid, Section 2.5) improves upon either method alone in the case where some sparse network is available.</p>
      <p>We consider situations where both proteins in the pair of interest occur in the PPI network, so that a network-only prediction can be made. Here, we evaluate GLIDE, Topsy-Turvy and TT-Hybrid on the <italic toggle="yes">D.melanogaster</italic> BioGRID network, which has been partitioned to measure the performance on networks of varying sparsity characterized by a parameter <inline-formula id="IE49"><mml:math id="IM49" display="inline" overflow="scroll"><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0.8</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>0.4</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. More specifically, <italic toggle="yes">p</italic> describes the fraction of total edges in <italic toggle="yes">G</italic> used to construct a subset network <inline-formula id="IE50"><mml:math id="IM50" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Full details on the construction of <italic toggle="yes">G<sub>p</sub></italic> are in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix S1.4</xref>. Characteristics of the sparse network datasets are described in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix S1.5</xref>. The sparsified network Gp is then used to compute GLIDE scores.</p>
      <p>To construct the test set at different <italic toggle="yes">P</italic>-values, we (a) selected the set of positive edges <inline-formula id="IE51"><mml:math id="IM51" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> as all edges in <italic toggle="yes">G</italic> left out during the construction of <italic toggle="yes">G<sub>p</sub></italic>, i.e. <inline-formula id="IE52"><mml:math id="IM52" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mi mathvariant="normal">∖</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and (b) randomly sampled negative examples from the set <inline-formula id="IE53"><mml:math id="IM53" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>×</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi mathvariant="normal">∖</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:math></inline-formula> to obtain <inline-formula id="IE54"><mml:math id="IM54" display="inline" overflow="scroll"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. The test set <inline-formula id="IE55"><mml:math id="IM55" display="inline" overflow="scroll"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>∪</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> was used to evaluate the performance of D-SCRIPT and Topsy-Turvy (trained on human), and GLIDE (trained on <italic toggle="yes">G<sub>p</sub></italic>) (AUPRs in <xref rid="sup1" ref-type="supplementary-material">Supplementary Appendix S1.6</xref>). We also broke down the analysis into subsets of the evaluation set, based on shortest-path distance <italic toggle="yes">d</italic> in <italic toggle="yes">G<sub>p</sub></italic> connecting the two proteins. Our intuition here was to check the relative performance of these methods on closely- versus distantly connected proteins. Detailed descriptions of the training network <italic toggle="yes">G<sub>p</sub></italic> and the test datasets <italic toggle="yes">S<sub>p</sub></italic> are provided in <xref rid="sup1" ref-type="supplementary-material">Supplementary Tables SA.1 and SA.2</xref>.</p>
      <p>Upon initial investigation, we found that while GLIDE outperformed Topsy-Turvy overall, their relative performance on a protein pair depended on the shortest-path distance between the proteins (<xref rid="sup1" ref-type="supplementary-material">Supplementary Table SA.3</xref>). Since GLIDE performance is primarily driven by hubs, to more clearly investigate relative performance we then performed the same set of evaluations after removing any edges incident upon hubs [i.e. (<italic toggle="yes">u</italic>, <italic toggle="yes">v</italic>) where <inline-formula id="IE56"><mml:math id="IM56" display="inline" overflow="scroll"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">degree</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mn>21</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>∨</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="italic">degree</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mn>21</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>]. We then observed that Topsy-Turvy was stronger on nearly every subset of data (<xref rid="btac258-F2" ref-type="fig">Fig. 2</xref>). However, GLIDE still performed better than Topsy-Turvy overall.</p>
      <fig position="float" id="btac258-F2">
        <label>Fig. 2.</label>
        <caption>
          <p>Comparing Topsy-Turvy and GLIDE in situations when both can be used. GLIDE was trained on a subset of the fly PPI network (e.g. training on 80% of PPIs when <italic toggle="yes">p </italic>=<italic toggle="yes"> </italic>0.8); Topsy-Turvy was trained on human PPI data and had no access to fly data for training. Both methods were evaluated on held-out positives as well as a randomly sampled set of negative examples, where pairs containing proteins with degree <inline-formula id="IE57"><mml:math id="IM57" display="inline" overflow="scroll"><mml:mo>≥</mml:mo></mml:math></inline-formula>21 on the subset networks were removed from the held-out examples during testing; the analysis is limited to proteins in the fly PPI network. In addition to reporting overall AUPR, we also group each protein-pair in the evaluation set by their shortest-path distance in the training network</p>
        </caption>
        <graphic xlink:href="btac258f2" position="float"/>
      </fig>
      <p>These results indicate that while GLIDE is able to separate PPIs by their network distance (which strongly correlates with whether or not there will be a reported interaction), once separated by network distance, Topsy-Turvy is able to finely organize similarly distant proteins using the information gleaned from sequence and structure. Thus, we introduced TT-Hybrid, which uses GLIDE and Topsy-Turvy to partition PPIs both coarsely and finely. We show in <xref rid="btac258-T5" ref-type="table">Table 5</xref> that TT-Hybrid improves upon either component method alone, achieving the highest overall AUPR on the fly network at all levels of sparsity (with hub nodes included).</p>
      <table-wrap position="float" id="btac258-T5">
        <label>Table 5.</label>
        <caption>
          <p>TT-Hybrid improves upon both of its constituent components on in-species prediction</p>
        </caption>
        <table frame="hsides" rules="groups">
          <colgroup span="1">
            <col valign="top" align="left" span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
            <col valign="top" align="char" char="." span="1"/>
          </colgroup>
          <thead>
            <tr>
              <th rowspan="1" colspan="1">Sparsity</th>
              <th rowspan="1" colspan="1">GLIDE</th>
              <th rowspan="1" colspan="1">Topsy-Turvy</th>
              <th rowspan="1" colspan="1">TT-Hybrid</th>
              <th rowspan="1" colspan="1">Random</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td rowspan="1" colspan="1"><italic toggle="yes">p </italic>=<italic toggle="yes"> </italic>0.8</td>
              <td rowspan="1" colspan="1">0.380</td>
              <td rowspan="1" colspan="1">0.038</td>
              <td rowspan="1" colspan="1">
                <bold>0.387</bold>
              </td>
              <td rowspan="1" colspan="1">0.004</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><italic toggle="yes">p </italic>=<italic toggle="yes"> </italic>0.6</td>
              <td rowspan="1" colspan="1">0.437</td>
              <td rowspan="1" colspan="1">0.079</td>
              <td rowspan="1" colspan="1">
                <bold>0.451</bold>
              </td>
              <td rowspan="1" colspan="1">0.009</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><italic toggle="yes">p </italic>=<italic toggle="yes"> </italic>0.4</td>
              <td rowspan="1" colspan="1">0.412</td>
              <td rowspan="1" colspan="1">0.105</td>
              <td rowspan="1" colspan="1">
                <bold>0.423</bold>
              </td>
              <td rowspan="1" colspan="1">0.014</td>
            </tr>
            <tr>
              <td rowspan="1" colspan="1"><italic toggle="yes">p </italic>=<italic toggle="yes"> </italic>0.2</td>
              <td rowspan="1" colspan="1">0.318</td>
              <td rowspan="1" colspan="1">0.133</td>
              <td rowspan="1" colspan="1">
                <bold>0.354</bold>
              </td>
              <td rowspan="1" colspan="1">0.019</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="tblfn5">
            <p><italic toggle="yes">Note</italic>: We generated partitions of the fly network of varying sparsity, using the sparsified networks as training for GLIDE. Sparsity <italic toggle="yes">p</italic> corresponds to the proportion of edges retained in the training network (<italic toggle="yes">p </italic>=<italic toggle="yes"> </italic>0.8 is the least sparse). Topsy-Turvy was trained on human PPIs. TT-Hybrid combines the predictions from both GLIDE and Topsy-Turvy. Here, we report the AUPR of each method on the held out edges removed from each network subset. We also show the AUPR of the random control; due to varying class imbalances, AUPR scores increase slightly with increasing sparsity. Bold entries represent best performance.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
    </sec>
  </sec>
  <sec>
    <title>4 Discussion</title>
    <p>We have presented Topsy-Turvy, a new method that integrates top-down global view of PPI organization into a bottom-up sequence-based PPI prediction model. The neural network design of Topsy-Turvy builds upon the architecture of D-SCRIPT and, like the latter, includes a bottleneck layer designed to model the inter-protein contact map, thus offering interpretability and insight into the mechanism of interaction. We show that Topsy-Turvy is highly accurate in a cross-species context, and applicable to species with few or no known protein interactions. For cases where PPI data is available in the target species, we present TT-Hybrid, that can leverage this additional information for more accurate predictions.</p>
    <p>Topsy-Turvy thus improves upon the state-of-the-art in PPI prediction broadly—both in species without available PPI data and in those with PPI data. For the former, it is able to transfer knowledge of network structure from other species, leading to more accurate <italic toggle="yes">de novo</italic> predictions. For the latter, it improves prediction coverage as well as accuracy. For instance, even in well-studied species like human, mouse, and fly, there remain many proteins with no characterized PPIs [24.9%, 44.9% and 19.8% of proteins in the three species, respectively (<xref rid="btac258-B26" ref-type="bibr">Pray, 2008</xref>; <xref rid="btac258-B28" ref-type="bibr">Serres <italic toggle="yes">et al.</italic>, 2001</xref>)]. Topsy-Turvy can be used to attach these hitherto uncharacterized proteins to existing PPI networks. Since GLIDE and other network methods are limited to predicting links between proteins that both already exist in the network, they cannot be used for putative interactions involving such proteins. When both proteins do exist in the PPI network, the hybrid approach TT-Hybrid that combines GLIDE with Topsy-Turvy performs better than either approach alone, with the former achieving a coarsely accurate network-theoretic organization and latter fine-tuning it locally. Here, we hypothesize that GLIDE confers species-specific network information unable to be transferred by Topsy-Turvy due to network rewiring.</p>
    <p>The TT-Hybrid results also give some hint as to what Topsy-Turvy might be learning from including a network loss term in the <italic toggle="yes">training</italic> stage. As shown in <xref rid="btac258-F2" ref-type="fig">Figure 2</xref>, the GLIDE network score helps segregate proteins into buckets that give a macro range of potential probabilities that an edge exists, while the bottom-up sequence approach does best at ranking the specific pairs within each bucket. This is not the first time we have seen network-based information assist in making sequence-level information more accurate; the Isorank network alignment algorithm (<xref rid="btac258-B29" ref-type="bibr">Singh <italic toggle="yes">et al.</italic>, 2007</xref>) also receives a gain in performance in discovering orthologs by a global top-down network similarity score that augments the bottom-up pairwise sequence score.</p>
    <p>In this regard, Topsy-Turvy presents an approach to an often-faced challenge in systems biology: how to resolve the dichotomy between a bottom-up and top-down view of the same biological phenomenon? Considered at the molecular level, protein interaction is a purely physicochemical process. However, these proteins primarily function through their interactions. With proteins performing most of the functions in the cell, evolution constrains the space of possible protein folds, resulting in emergent properties at the network level. The approach embodied by Topsy-Turvy and TT-Hybrid could be more generally applied to situations where network-theoretic and molecular views need to be integrated. To make a social interaction analogy, D-SCRIPT and other sequence-based bottom-up methods are learning features that make two people likely to be compatible as friends, but not global organization of the friend network that would indicate if those two people share enough mutual friends to be likely to have had the opportunity to meet at the same event.</p>
    <p>While we took steps to rule out the effect of ascertainment bias, this remains an important question in both the training and evaluation of link prediction methods. In this work, we sourced PPIs from the STRING database where data from a variety of assays has been conglomerated. An unbiased, all-versus-all screen as exemplified by the Human Reference Interactome (HuRI) database (<xref rid="btac258-B25" ref-type="bibr">Luck <italic toggle="yes">et al.</italic>, 2020</xref>) offers the promise of addressing ascertainment bias in the specific case of yeast two-hybrid (Y2H) screens. However, to test Topsy-Turvy in our transfer-learning context, we would also need similar unbiased Y2H screens in a different species.</p>
    <p>By approaching integration of orthogonal information sources as a multi-objective learning problem, Topsy-Turvy lays the groundwork for incorporation of additional data modalities. For instance, while the GLIDE score incorporates both global and local scores, it would be possible to directly supervise Topsy-Turvy with global and local loss terms, each with a respective hyper-parameter to finely control their effects. Loss terms that quantify protein functional similarity (<xref rid="btac258-B15" ref-type="bibr">Ghersi and Singh, 2014</xref>) or interface similarity (<xref rid="btac258-B4" ref-type="bibr">Budowski-Tal <italic toggle="yes">et al.</italic>, 2018</xref>; <xref rid="btac258-B14" ref-type="bibr">Gainza <italic toggle="yes">et al.</italic>, 2020</xref>) could be added to the framework to further inform predictions. Topsy-Turvy demonstrates that a general, scalable framework that allows us to transfer both low-level (sequence-to-structure) and high-level (network topology) insights across species can enable researchers to fill in the missing links in our knowledge of biological function.</p>
  </sec>
  <sec>
    <title>Funding</title>
    <p>R.S., S.S. and B.B. were supported by the National Institutes of Health grant [R35GM141861]. K.D. and L.C. were supported by National Science Foundation (NSF) grant [CCF-1934553]. S.S. was supported by the National Science Foundation Graduate Research Fellowship [1745302].</p>
    <p><italic toggle="yes">Conflict of Interest</italic>: none declared.</p>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>btac258_Supplementary_Data</label>
      <media xlink:href="btac258_supplementary_data.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="btac258-B1">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Bepler</surname><given-names>T.</given-names></string-name>, <string-name><surname>Berger</surname><given-names>B.</given-names></string-name></person-group> (<year>2019</year>) Learning protein sequence embeddings using information from structure. <italic toggle="yes">Int. Conf. Learn. Represent. (ICLR).</italic> arXiv: 1902.08661.</mixed-citation>
    </ref>
    <ref id="btac258-B2">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bepler</surname><given-names>T.</given-names></string-name>, <string-name><surname>Berger</surname><given-names>B.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Learning the protein language: evolution, structure, and function</article-title>. <source>Cell Syst</source>., <volume>12</volume>, <fpage>654</fpage>–<lpage>669</lpage>.<pub-id pub-id-type="pmid">34139171</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B3">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Brown</surname><given-names>K.R.</given-names></string-name>, <string-name><surname>Jurisica</surname><given-names>I.</given-names></string-name></person-group> (<year>2007</year>) <article-title>Unequal evolutionary conservation of human protein interactions in interologous networks</article-title>. <source>Genome Biol</source>., <volume>8</volume>, <fpage>R95</fpage>.<pub-id pub-id-type="pmid">17535438</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B4">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Budowski-Tal</surname><given-names>I.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>A novel geometry-based approach to infer protein interface similarity</article-title>. <source>Sci. Rep</source>., <volume>8</volume>, <fpage>1</fpage>–<lpage>10</lpage>.<pub-id pub-id-type="pmid">29311619</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B5">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Carter</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) <article-title>Genotype to phenotype via network analysis</article-title>. <source>Curr. Opin. Genet. Dev</source>., <volume>23</volume>, <fpage>611</fpage>–<lpage>621</lpage>.<pub-id pub-id-type="pmid">24238873</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B6">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname><given-names>M.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Multifaceted protein–protein interaction prediction based on siamese residual RCNN</article-title>. <source>Bioinformatics</source>, <volume>35</volume>, <fpage>i305</fpage>–<lpage>i314</lpage>.<pub-id pub-id-type="pmid">31510705</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B7">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cho</surname><given-names>H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2016</year>) <article-title>Compact integration of multi-network topology for functional analysis of genes</article-title>. <source>Cell Syst</source>., <volume>3</volume>, <fpage>540</fpage>–<lpage>548.e5</lpage>.<pub-id pub-id-type="pmid">27889536</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B8">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Coşkun</surname><given-names>M.</given-names></string-name>, <string-name><surname>Koyutürk</surname><given-names>M.</given-names></string-name></person-group> (<year>2021</year>) <article-title>Node similarity based graph convolution for link prediction in biological networks</article-title>. <source>Bioinformatics</source>, <volume>37</volume>, <fpage>4501</fpage>–<lpage>4508</lpage>.<pub-id pub-id-type="pmid">34152393</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B9">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Cowen</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2017</year>) <article-title>Network propagation: a universal amplifier of genetic associations</article-title>. <source>Nat. Rev. Genet</source>., <volume>18</volume>, <fpage>551</fpage>–<lpage>562</lpage>.<pub-id pub-id-type="pmid">28607512</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B10">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dequeker</surname><given-names>C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2022</year>) <article-title>From complete cross-docking to partners identification and binding sites predictions</article-title>. <source>PLoS Comput. Biol</source>., <volume>18</volume>, <fpage>e1009825</fpage>.<pub-id pub-id-type="pmid">35089918</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B11">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Devkota</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>GLIDE: combining local methods and diffusion state embeddings to predict missing interactions in biological networks</article-title>. <source>Bioinformatics</source>, <volume>36</volume>, <fpage>i464</fpage>–<lpage>473</lpage>.<pub-id pub-id-type="pmid">32657369</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B12">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Evans</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Protein complex prediction with AlphaFold-Multimer</article-title>. <source>bioRxiv</source>. <pub-id pub-id-type="doi">10.1101/2021.10.04.463034</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac258-B13">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fox</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2009</year>) <article-title>High throughput interaction data reveals degree conservation of hub proteins</article-title>. <source>Pac. Symp. Biocomput</source>., <bold>14</bold>, <fpage>391</fpage>–<lpage>402</lpage>.<pub-id pub-id-type="pmid">19209717</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B14">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gainza</surname><given-names>P.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</article-title>. <source>Nat. Methods</source>, <volume>17</volume>, <fpage>184</fpage>–<lpage>192</lpage>.<pub-id pub-id-type="pmid">31819266</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B15">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Ghersi</surname><given-names>D.</given-names></string-name>, <string-name><surname>Singh</surname><given-names>M.</given-names></string-name></person-group> (<year>2014</year>) <article-title>Interaction-based discovery of functionally important genes in cancers</article-title>. <source>Nucleic Acids Res</source>., <volume>42</volume>, <fpage>e18</fpage>.<pub-id pub-id-type="pmid">24362839</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B16">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Grover</surname><given-names>A.</given-names></string-name>, <string-name><surname>Leskovec</surname><given-names>J.</given-names></string-name></person-group> (<year>2016</year>) Node2vec: scalable feature learning for networks. In: <italic toggle="yes">KDD ’16</italic>. Association for Computing Machinery, New York, NY, USA, pp. <fpage>855</fpage>–<lpage>864</lpage>.</mixed-citation>
    </ref>
    <ref id="btac258-B17">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hamilton</surname><given-names>W.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Embedding logical queries on knowledge graphs</article-title>. In: <source>NIPS'18: Proceedings of the 32nd International Conference on Neural Information Processing Systems, December 2018</source>, pp. <fpage>2030</fpage>–<lpage>2041</lpage>.</mixed-citation>
    </ref>
    <ref id="btac258-B18">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hashemifar</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Predicting protein–protein interactions through sequence-based deep learning</article-title>. <source>Bioinformatics</source>, <volume>34</volume>, <fpage>i802</fpage>–<lpage>i810</lpage>.<pub-id pub-id-type="pmid">30423091</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B19">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Huang</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>SkipGNN: predicting molecular interactions with skip-graph networks</article-title>. <source>Sci. Rep</source>., <volume>10</volume>, <fpage>1</fpage>–<lpage>16</lpage>.<pub-id pub-id-type="pmid">31913322</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B20">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Jumper</surname><given-names>J.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Highly accurate protein structure prediction with AlphaFold</article-title>. <source>Nature</source>, <volume>596</volume>, <fpage>583</fpage>–<lpage>589</lpage>.<pub-id pub-id-type="pmid">34265844</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B21">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kovács</surname><given-names>I.A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) <article-title>Network-based prediction of protein interactions</article-title>. <source>Nat. Commun</source>., <volume>10</volume>, <fpage>1</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">30602773</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B23">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kumar</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>Transfer of knowledge from model organisms to evolutionarily distant non-model organisms: the coral pocillopora damicornis membrane signaling receptome</article-title>. <source>bioRxiv</source>.</mixed-citation>
    </ref>
    <ref id="btac258-B24">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lopes</surname><given-names>A.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2013</year>) <article-title>Protein–protein interactions in a crowded environment: an analysis via cross-docking simulations and evolutionary information</article-title>. <source>PLoS Comput. Biol</source>., <volume>9</volume>, <fpage>e1003369</fpage>.<pub-id pub-id-type="pmid">24339765</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B25">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Luck</surname><given-names>K.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>A reference map of the human binary protein interactome</article-title>. <source>Nature</source>, <volume>580</volume>, <fpage>402</fpage>–<lpage>408</lpage>.<pub-id pub-id-type="pmid">32296183</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B26">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pray</surname><given-names>L.</given-names></string-name></person-group> (<year>2008</year>) <article-title>Eukaryotic genome complexity</article-title>. <source>Nat. Educ</source>., <volume>1</volume>, <fpage>96</fpage>.</mixed-citation>
    </ref>
    <ref id="btac258-B27">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Richoux</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2019</year>) Comparing two deep learning sequence-based models for protein–protein interaction prediction. <italic toggle="yes">arXiv</italic>, preprint arXiv:1901.06268. <pub-id pub-id-type="doi">10.48550/arXiv.1901.06268</pub-id>.</mixed-citation>
    </ref>
    <ref id="btac258-B28">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Serres</surname><given-names>M.H.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2001</year>) <article-title>A functional update of the <italic toggle="yes">Escherichia coli</italic> k-12 genome</article-title>. <source>Genome Biol</source>., <volume>2</volume>, <fpage>research0035.1</fpage>.<pub-id pub-id-type="pmid">11574054</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B29">
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Singh</surname><given-names>R.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2007</year>) <part-title>Pairwise global alignment of protein interaction networks by matching neighborhood topology</part-title>. In: <source>RECOMB, Oakland, California</source>, <publisher-name>Springer</publisher-name>, pp. <fpage>16</fpage>–<lpage>31</lpage>.</mixed-citation>
    </ref>
    <ref id="btac258-B30">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Sledzieski</surname><given-names>S.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>D-SCRIPT translates genome to phenome with sequence-based, structure-aware, genome-scale predictions of protein–protein interactions</article-title>. <source>Cell Syst</source>., <volume>12</volume>, <fpage>969</fpage>–<lpage>982</lpage>. Focus on RECOMB.<pub-id pub-id-type="pmid">34536380</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B31">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Szklarczyk</surname><given-names>D.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2021</year>) <article-title>The STRING database in 2021: customizable protein–protein networks, and functional characterization of user-uploaded gene/measurement sets</article-title>. <source>Nucleic Acids Res</source>., <volume>49</volume>, <fpage>D605</fpage>–<lpage>D612</lpage>.<pub-id pub-id-type="pmid">33237311</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B32">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yang</surname><given-names>F.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2020</year>) <article-title>Graph-based prediction of protein–protein interactions with attributed signed graph embedding</article-title>. <source>BMC Bioinformatics</source>, <volume>21</volume>, <fpage>323</fpage>.<pub-id pub-id-type="pmid">32693790</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B33">
      <mixed-citation publication-type="other"><person-group person-group-type="author"><string-name><surname>Yuen</surname><given-names>H.Y.</given-names></string-name>, <string-name><surname>Jansson</surname><given-names>J.</given-names></string-name></person-group> (<year>2020</year>) <article-title>Better link prediction for protein–protein interaction networks</article-title>. In: <italic toggle="yes">2020 IEEE 20th International Conference on Bioinformatics and Bioengineering (BIBE), Cincinnati, OH</italic>. IEEE, pp. <fpage>53</fpage>–<lpage>60</lpage>.</mixed-citation>
    </ref>
    <ref id="btac258-B34">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>Q.C.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2012</year>) <article-title>Structure-based prediction of protein–protein interactions on a genome-wide scale</article-title>. <source>Nature</source>, <volume>490</volume>, <fpage>556</fpage>–<lpage>560</lpage>.<pub-id pub-id-type="pmid">23023127</pub-id></mixed-citation>
    </ref>
    <ref id="btac258-B35">
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname><given-names>L.</given-names></string-name></person-group>  <etal>et al</etal> (<year>2018</year>) <article-title>Predicting protein–protein interactions using high-quality non-interacting pairs</article-title>. <source>BMC Bioinformatics</source>, <volume>19</volume>.</mixed-citation>
    </ref>
  </ref-list>
</back>
