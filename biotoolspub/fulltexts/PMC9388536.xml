<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sci Rep</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id>
    <journal-title-group>
      <journal-title>Scientific Reports</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2045-2322</issn>
    <publisher>
      <publisher-name>Nature Publishing Group UK</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9388536</article-id>
    <article-id pub-id-type="publisher-id">17753</article-id>
    <article-id pub-id-type="doi">10.1038/s41598-022-17753-4</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Automated diagnosing primary open-angle glaucoma from fundus image by simulating human’s grading with deep learning</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Lin</surname>
          <given-names>Mingquan</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hou</surname>
          <given-names>Bojian</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Lei</given-names>
        </name>
        <xref ref-type="aff" rid="Aff2">2</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Gordon</surname>
          <given-names>Mae</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kass</surname>
          <given-names>Michael</given-names>
        </name>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Wang</surname>
          <given-names>Fei</given-names>
        </name>
        <address>
          <email>few2001@med.cornell.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Van Tassel</surname>
          <given-names>Sarah H.</given-names>
        </name>
        <address>
          <email>sjh2006@med.cornell.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Peng</surname>
          <given-names>Yifan</given-names>
        </name>
        <address>
          <email>yip4002@med.cornell.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.5386.8</institution-id><institution-id institution-id-type="ISNI">000000041936877X</institution-id><institution>Department of Population Health Sciences, </institution><institution>Weill Cornell Medicine, </institution></institution-wrap>New York, NY USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.4367.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2355 7002</institution-id><institution>Institute for Public Health, </institution><institution>Washington University School of Medicine, </institution></institution-wrap>St. Louis, MO USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.4367.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2355 7002</institution-id><institution>Department of Ophthalmology and Visual Sciences, </institution><institution>Washington University School of Medicine, </institution></institution-wrap>St. Louis, MO USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.5386.8</institution-id><institution-id institution-id-type="ISNI">000000041936877X</institution-id><institution>Department of Ophthalmology, </institution><institution>Weill Cornell Medicine, </institution></institution-wrap>New York, NY USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>18</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>18</day>
      <month>8</month>
      <year>2022</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2022</year>
    </pub-date>
    <volume>12</volume>
    <elocation-id>14080</elocation-id>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>2</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>30</day>
        <month>7</month>
        <year>2022</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2022</copyright-statement>
      <license>
        <ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <p id="Par1">Primary open-angle glaucoma (POAG) is a leading cause of irreversible blindness worldwide. Although deep learning methods have been proposed to diagnose POAG, it remains challenging to develop a robust and explainable algorithm to automatically facilitate the downstream diagnostic tasks. In this study, we present an automated classification algorithm, GlaucomaNet, to identify POAG using variable fundus photographs from different populations and settings. GlaucomaNet consists of two convolutional neural networks to simulate the human grading process: learning the discriminative features and fusing the features for grading. We evaluated GlaucomaNet on two datasets: Ocular Hypertension Treatment Study (OHTS) participants and the Large-scale Attention-based Glaucoma (LAG) dataset. GlaucomaNet achieved the highest AUC of 0.904 and 0.997 for POAG diagnosis on OHTS and LAG datasets. An ensemble of network architectures further improved diagnostic accuracy. By simulating the human grading process, GlaucomaNet demonstrated high accuracy with increased transparency in POAG diagnosis (comprehensiveness scores of 97% and 36%). These methods also address two well-known challenges in the field: the need for increased image data diversity and relying heavily on perimetry for POAG diagnosis. These results highlight the potential of deep learning to assist and enhance clinical POAG diagnosis. GlaucomaNet is publicly available on <ext-link ext-link-type="uri" xlink:href="https://github.com/bionlplab/GlaucomaNet">https://github.com/bionlplab/GlaucomaNet</ext-link>. </p>
    </abstract>
    <kwd-group kwd-group-type="npg-subject">
      <title>Subject terms</title>
      <kwd>Eye diseases</kwd>
      <kwd>Diagnosis</kwd>
    </kwd-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id>
            <institution>National Eye Institute</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000096</institution-id>
            <institution>National Center on Minority Health and Health Disparities</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>National Institutes of Health</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100007414</institution-id>
            <institution>Horncrest Foundation</institution>
          </institution-wrap>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>NIH Vision Core Grant</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Merck Research Laboratories</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Pfizer, Inc., White House Station, New Jersey</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution>Prevent Blindness, Inc., New York, NY</institution>
        </funding-source>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000092</institution-id>
            <institution>U.S. National Library of Medicine</institution>
          </institution-wrap>
        </funding-source>
        <award-id>4R00LM013001</award-id>
        <principal-award-recipient>
          <name>
            <surname>Peng</surname>
            <given-names>Yifan</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2022</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Introduction</title>
    <p id="Par2">Primary open-angle glaucoma (POAG) is one of the leading causes of blindness in the US and worldwide<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. It has been projected to affect approximately 111.8 million people by 2040. Among these patients, 5.3 million may be bilaterally blind<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. In the United States, POAG is the most common form of glaucoma and is the leading cause of blindness among African-Americans<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> and Hispanics<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. POAG is asymptomatic until it reaches an advanced stage when visual field (VF) loss occurs. Therefore, accurately identifying individuals with glaucoma is critical to clinical decision-making, which can help inform the need for medical and surgical treatments and patient monitoring<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>.</p>
    <p id="Par3">Screening is an efficient and effective way to detect POAG to prevent blindness<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. However, screening needs expensive facilities and experienced ophthalmologists, which causes low prevalence<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Meanwhile, fundus photography is convenient and inexpensive for recording optic nerve head structure. But detecing POAG on fundus photographs still requires human labor and severely depends on experienced ophthalmologists. Therefore, it is important to develop an automatic model to detect POAG with high accuracy and low cost from fundus photographs.</p>
    <p id="Par4">Developments in artificial intelligence have provided potential opportunities for automatic POAG diagnosis using fundus photographs. Singh et al. segmented optic disc and optic cup images to obtain the vertical cup-to-disc ratio (VCDR) and extracted the handcrafted features from the VCDR to perform the classification<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. Acharya et al. extracted texture features and higher-order spectral features to detect POAG based on a Support Vector Machine (SVM) and Naïve Bayesian (NB) classifier<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Dua et al. also used SVM and NB to classify the wavelet-based energy features<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. An adaptive threshold-based image processing method was adopted by Issac et al. for POAG classification<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. However, these methods are influenced by the segmentation accuracy of the optic disc and optic cup or have relatively low classification accuracy because they only consider handcrafted features. Recently, deep learning methods have demonstrated promising results in biology and medicine<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR27">27</xref></sup>. For POAG detection, several works directly used Convolutional Neural Networks (CNN) to detect glaucoma<sup><xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR33">33</xref></sup>. Fu et al. provided a novel dis-aware ensemble network for glaucoma screening from fundus images<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. Li et al. put forward a framework that integrates holistic and local deep features for glaucoma classification<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.</p>
    <p id="Par5">While these studies have brought significant improvements, several limitations hinder their translation into clinical practice. First, most models were developed on small datasets from a single institution<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR35">35</xref></sup>. It makes the methods less generalizable to different populations and settings. Second is the requirement of refined image annotation such as attention maps<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, image cropping<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, and optic nerve head segmentation<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Today, it remains challenging to build highly-accurate systems using image-level labels. Finally, all these works lack interpretability which reduces their utility for medical applications. Thus, it is critical to develop an interpretable AI system for ophthalmologists besides providing a fast and accurate POAG diagnosis. Specifically, interest has recently grown in designing clinical systems that can reveal why models make specific predictions<sup><xref ref-type="bibr" rid="CR36">36</xref>–<xref ref-type="bibr" rid="CR38">38</xref></sup>. To the best of our knowledge, few works in POAG diagnosis have been studied in this direction.</p>
    <p id="Par6">This work addresses these issues by proposing a joint fusion network (GlaucomaNet) to accurately and robustly diagnose POAG using variable fundus photographs. Different from previous studies, GlaucomaNet consists of two convolutional neural networks to simulate the human grading process: one performing preliminary grading and the other detailed grading. It is trained in an end-to-end strategy without manual cropping so that the whole training process is fast and less labor-intensive and can be quickly deployed to provide a first-line assessment. In addition, an ensemble of network architectures further improved diagnostic accuracy. We validated the models on two independent datasets (Table <xref rid="Tab1" ref-type="table">1</xref>). One is the Ocular Hypertension Treatment Study (OHTS) cohort, one of the largest longitudinal clinical trials in POAG (1636 participants and 37,399 images) from 22 centers in the United States<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The other is the Large-scale Attention-based Glaucoma (LAG) database, a publicly available database collected at the Chinese Glaucoma Study Alliance and Beijing Tongren Hospital<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. GlaucomaNet achieved the highest AUC for POAG diagnosis on these two datasets. To further evaluate the rationale of our model, we consider the prediction from the model for the POAG diagnosis once the supporting rationales (optic disc) are stripped. To this end, we constructed a new dataset by masking the optic disc portion of the fundus photographs. When stripping the bounding box of optic discs and random bounding boxes were masked, we showed that the rationales (optic disc) are influential in the POAG diagnosis in our network.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Characteristics of the OHTS and LAG datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">Dataset</th><th align="left">OHTS</th><th align="left">LAG</th></tr></thead><tbody><tr><td align="left">#Participants</td><td char="." align="char">1636</td><td char="." align="char">4855</td></tr><tr><td align="left">#Images</td><td char="." align="char">37,339</td><td char="." align="char">4915</td></tr><tr><td align="left">  #POAG</td><td char="." align="char">2321</td><td char="." align="char">1711</td></tr><tr><td align="left">  #Normal</td><td char="." align="char">35,018</td><td char="." align="char">3144</td></tr></tbody></table></table-wrap></p>
  </sec>
  <sec id="Sec2">
    <title>Methods</title>
    <sec id="Sec3">
      <title>Data acquisition</title>
      <p id="Par7">In this study, we include two independent datasets. These two databases are large, cross-sectional, longitudinal, and population-based studies. All participants provided informed consent at original study entry. This study does not need institutional review board approval because it does not constitute human subjects research. All experiments were performed in accordance with relevant guidelines and regulations.</p>
      <sec id="Sec4">
        <title>Ocular hypertension treatment study</title>
        <p id="Par8">The first dataset was obtained from the Ocular Hypertension Treatment Study (OHTS)<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. It is one of the largest longitudinal clinical trials in POAG from 22 centers in the United States. The study protocol was approved by an independent Institutional Review Board at each clinical center.</p>
        <p id="Par9">The participants in this dataset were selected according to both eligibility and exclusion criteria<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. Briefly, the eligibility criteria include intraocular pressure (IOP) (between 24 and 32 mmHg in one eye and between 21 and 32 mmHg in the fellow eye) and age (between 40 and 80 years old). The visual field tests were interpreted by the Visual Field Reading Center, and stereoscopic photographs of the optics discs were interpreted by the Optic Disc Reading Center. Exclusion criteria included previous intraocular surgery, visual acuity worse than 20/40 in either eye, and diseases that may cause optic disc deterioration and visual field loss (such as diabetic retinopathy).</p>
        <p id="Par10">The reading center workflow has been described in detail in Kass et al.<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. In brief, two certified readers independently assessed optic disc deterioration. If there was disagreement between two readers, a senior reader adjudicated in a masked fashion. The POAG diagnosis in a quality control sample of 86 eyes (50 normal eyes and 36 with progression) showed test–retest agreement at κ = 0.70 (95% confidence interval [CI], 0.55–0.85).</p>
        <p id="Par11">Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the creation of the OHTS dataset in our study. Because of the inherent redundancy in a pair of stereoscopic photographs, we used only one of the photographs for each eye. There are 7964 images (11.9%) that contain the stereo pair in a single image file. We split them into two images. In the end, we obtained 39,339 fundus photographs from 3272 eyes of 1636 subjects. On the image level, 6.2% (2321) of fundus photographs were diagnosed with POAG (Table <xref rid="Tab1" ref-type="table">1</xref>).<fig id="Fig1"><label>Figure 1</label><caption><p>Creation of the OHTS dataset.</p></caption><graphic xlink:href="41598_2022_17753_Fig1_HTML" id="MO1"/></fig></p>
      </sec>
      <sec id="Sec5">
        <title>Large-scale Attention-based Glaucoma database</title>
        <p id="Par12">The second dataset was obtained from the Large-scale Attention-based Glaucoma (LAG) database, a publicly available database collected at the Chinese Glaucoma Study Alliance and Beijing Tongren Hospital<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> (<ext-link ext-link-type="uri" xlink:href="https://github.com/smilell/AG-CNN">https://github.com/smilell/AG-CNN</ext-link>). LAG contains 4855 fundus images, of which 35% (1711) have POAG. This dataset has a high proportion of glaucoma cases (41.2%), exceeding what would be expected when screening for glaucoma, which was designed intentionally to create a balanced sample. All research adhered to the tenets of the Declaration of Helsinki. Qualified glaucoma specialists diagnosed each fundus image by considering both morphologic and functional analyses such as intraocular pressure, visual field loss, and manual optic disc assessment. Finally, all fundus images were confirmed with positive or negative glaucoma, seen as the gold standard.</p>
      </sec>
    </sec>
    <sec id="Sec6">
      <title>Model development</title>
      <sec id="Sec7">
        <title>Overall architecture</title>
        <p id="Par13">GlaucomaNet comprises two convolutional blocks followed by seven layers (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). First, a single fundus image <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{i}$$\end{document}</tex-math><mml:math id="M2"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq1.gif"/></alternatives></inline-formula> is passed through two convolutional neural networks, DenseNet-201<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> and ResNet-152<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. For DenseNet-201, we used the output of the 200 layers. For ResNet-152, we used the output of the 151st layer. We denote the outputs as <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{d}$$\end{document}</tex-math><mml:math id="M4"><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq2.gif"/></alternatives></inline-formula> and <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{r}$$\end{document}</tex-math><mml:math id="M6"><mml:msub><mml:mi>F</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq3.gif"/></alternatives></inline-formula>, respectively. We then concatenated <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{d}$$\end{document}</tex-math><mml:math id="M8"><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq4.gif"/></alternatives></inline-formula> and <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{r}$$\end{document}</tex-math><mml:math id="M10"><mml:msub><mml:mi>F</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq5.gif"/></alternatives></inline-formula> and used <inline-formula id="IEq6"><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1\times 1$$\end{document}</tex-math><mml:math id="M12"><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq6.gif"/></alternatives></inline-formula> convolution (conv1d) followed by a batch normalization layer (BN)<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> and rectified linear units (ReLU)<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> as transition layers. In the end, a global average pooling (AvgPooling)<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> with a fully connected layer is performed, and a sigmoid classifier is attached.<fig id="Fig2"><label>Figure 2</label><caption><p>The architecture of the proposed GlaucomaNet.</p></caption><graphic xlink:href="41598_2022_17753_Fig2_HTML" id="MO2"/></fig><disp-formula id="Equa"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{d}=DenseNet 201({x}_{i})$$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mn>201</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2022_17753_Article_Equa.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equb"><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{r}=ResNet152({x}_{i})$$\end{document}</tex-math><mml:math id="M16" display="block"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>N</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mn>152</mml:mn><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2022_17753_Article_Equb.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equc"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{1}=conv1d\left(concat\left({F}_{d}, {F}_{r}\right)\right)$$\end{document}</tex-math><mml:math id="M18" display="block"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mn>1</mml:mn><mml:mi>d</mml:mi><mml:mfenced close=")" open="("><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>F</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2022_17753_Article_Equc.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equd"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{2}=ReLU\left(BN\left({F}_{1}\right)\right)$$\end{document}</tex-math><mml:math id="M20" display="block"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>L</mml:mi><mml:mi>U</mml:mi><mml:mfenced close=")" open="("><mml:mi>B</mml:mi><mml:mi>N</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mfenced></mml:mfenced></mml:mrow></mml:math><graphic xlink:href="41598_2022_17753_Article_Equd.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Eque"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F}_{3}=AvgPooling({F}_{2})$$\end{document}</tex-math><mml:math id="M22" display="block"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>v</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2022_17753_Article_Eque.gif" position="anchor"/></alternatives></disp-formula><disp-formula id="Equf"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$o={sigmoid(W}_{3}{F}_{3}+{b}_{3})$$\end{document}</tex-math><mml:math id="M24" display="block"><mml:mrow><mml:mi>o</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi></mml:mrow><mml:mn>3</mml:mn></mml:msub><mml:msub><mml:mi>F</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2022_17753_Article_Equf.gif" position="anchor"/></alternatives></disp-formula></p>
      </sec>
      <sec id="Sec8">
        <title>Loss function</title>
        <p id="Par14">On both datasets, there is a severe class imbalance between non-POAG and POAG images. For example, only 6.22% of images in the OHTS dataset contain POAG. To overcome this data imbalance issue, we applied weighted cross-entropy<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>, a commonly used loss function in classification. The adopted weighted cross-entropy was as follows:<disp-formula id="Equg"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathcal{L}=-\frac{1}{N}\sum_{n=1}^{N}[{w}_{1}{y}_{n}\mathit{log}\left({\widehat{y}}_{n}\left({x}_{n},{\theta }_{s}\right)\right)+{w}_{2}(1-{y}_{n})\mathit{log}\left(1-{\widehat{y}}_{n}\left({x}_{n},{\theta }_{s}\right)\right)]$$\end{document}</tex-math><mml:math id="M26" display="block"><mml:mrow><mml:mi mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mi mathvariant="italic">log</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="italic">log</mml:mi><mml:mfenced close=")" open="("><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfenced></mml:mfenced><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math><graphic xlink:href="41598_2022_17753_Article_Equg.gif" position="anchor"/></alternatives></disp-formula>where <inline-formula id="IEq7"><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N$$\end{document}</tex-math><mml:math id="M28"><mml:mi>N</mml:mi></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq7.gif"/></alternatives></inline-formula> is the number of training examples, <inline-formula id="IEq8"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{1}$$\end{document}</tex-math><mml:math id="M30"><mml:msub><mml:mi>w</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq8.gif"/></alternatives></inline-formula> and <inline-formula id="IEq9"><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${w}_{2}$$\end{document}</tex-math><mml:math id="M32"><mml:msub><mml:mi>w</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq9.gif"/></alternatives></inline-formula> are controlling hyperparameters, <inline-formula id="IEq10"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{n}$$\end{document}</tex-math><mml:math id="M34"><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq10.gif"/></alternatives></inline-formula> is the ground truth while <inline-formula id="IEq11"><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\widehat{y}}_{n}$$\end{document}</tex-math><mml:math id="M36"><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="true">^</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq11.gif"/></alternatives></inline-formula> is the likelihood predicted by the classifier, and <inline-formula id="IEq12"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\theta }_{s}$$\end{document}</tex-math><mml:math id="M38"><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq12.gif"/></alternatives></inline-formula> represents the parameters of the neural network.</p>
      </sec>
      <sec id="Sec9">
        <title>Image augmentation</title>
        <p id="Par15">A stochastic image augmentation was randomly applied to transform a fundus photograph into an augmented view. In this work, we sequentially appled three simple augmentation operations: (1) random rotation between 0° and 10°, (2) random translation: an image was translated randomly along the x- and y-axes by distances ranging from 0 to 10% of width or height of the image, and (3) random flipping. These data augmentation operations are crucial in increasing the diversity of the dataset and thus yielding effective and robust representations<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>.</p>
      </sec>
      <sec id="Sec10">
        <title>Training strategy</title>
        <p id="Par16">We first trained two different single models, DenseNet-201<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> and ResNet-152<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Then, we selected the convolutional part of the DenseNet-201and ResNet-152 as the convolutional part of the proposed model. Afterward, we trained the entire network in an end-to-end manner. During the training, we updated the entire network parameters. Therefore, the loss was propagated back to the individual neural networks, creating better feature representations for each training iteration. Since we learned the feature representations from intermediate layers, our training strategy is closed to a joint fusion strategy<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>.</p>
        <p id="Par17">To use the pre-trained model on ImageNet, all images were resized to 224 × 224. All models were implemented by Keras with a backend of Tensorflow. The proposed network was optimized using the Adam optimizer method<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. The learning rate was <inline-formula id="IEq13"><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$5\times {10}^{-5}$$\end{document}</tex-math><mml:math id="M40"><mml:mrow><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq13.gif"/></alternatives></inline-formula>. The experiments were performed on Intel Core i9-9960 X 16 cores processor and NVIDIA Quadro RTX 6000 GPU.</p>
      </sec>
      <sec id="Sec11">
        <title>Ensemble methods</title>
        <p id="Par18">In machine learning, ensemble methods combine multiple models to produce improved results, surpassing single models. This study also evaluated the performance of three ensemble methods: macro averaging, random forest<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, and linear regression. The macro averaging model averages the predicted class probabilities of the seven models together. The random forest model computes a number of decision trees. The final classification corresponds to the majority vote among the individual trees. Here, we used the predicted class probabilities of the seven models and trained the random forest using 500 trees on the development set. For linear regression, we also used the predicted class probabilities of the seven models as input.</p>
      </sec>
      <sec id="Sec12">
        <title>Model’s clinical rationales</title>
        <p id="Par19">State-of-the-art models for POAG diagnosis, including ours, are now predominantly deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models that reveal the ‘reasoning’ behind model outputs<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. One common way to explain the deep learning model is through a saliency map where important regions on the images are highlighted to correspond to the disease of interest<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>. For POAG diagnosis, segmented optic disc and optic cup are typically used. However, such spatial annotation requires substantial effort and is not generally available in large quantities. In addition, such approaches do not establish whether the model relied on these regions to make a prediction.</p>
        <p id="Par20">In this work, we proposed a different way to assess the plausibility of rationales by measuring rationale faithfulness—rationales ought to have meaningfully influenced its prediction<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>. Following DeYoung et al.<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>, we conducted a contrasting experiment to explore the rationales of the optic disc. Figure <xref rid="Fig3" ref-type="fig">3</xref> provides two demos for the experiment.<fig id="Fig3"><label>Figure 3</label><caption><p>Examples of fundus photographs and their corresponding masked images: (<bold>a</bold>) POAG due to VF only (the left one is based on the optic disc mask and the right one is based on a random mask) and (<bold>b</bold>) POAG due to glaucomatous disc criteria (the left one is based on the optic disc mask and the right one is based on a random mask).</p></caption><graphic xlink:href="41598_2022_17753_Fig3_HTML" id="MO3"/></fig></p>
        <p id="Par21">First, we manually masked the optic disc <inline-formula id="IEq14"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${r}_{i}$$\end{document}</tex-math><mml:math id="M42"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq14.gif"/></alternatives></inline-formula> from the fundus photograph with a bounding box. Let <inline-formula id="IEq15"><alternatives><tex-math id="M43">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m\left({x}_{i}\right)$$\end{document}</tex-math><mml:math id="M44"><mml:mrow><mml:mi>m</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq15.gif"/></alternatives></inline-formula> be the original prediction provided by a model <inline-formula id="IEq16"><alternatives><tex-math id="M45">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m$$\end{document}</tex-math><mml:math id="M46"><mml:mi>m</mml:mi></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq16.gif"/></alternatives></inline-formula>. We then consider the predicted class from the model once the supporting bounding box is stripped. Intuitively, the model should generate a less “correct” class. We then measure this as comprehensiveness<disp-formula id="Equh"><alternatives><tex-math id="M47">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$comprehensiveness=\frac{1}{n}{\sum }_{n}(m\left({x}_{i}\right)-m\left({x}_{i}/{r}_{i}\right)).$$\end{document}</tex-math><mml:math id="M48" display="block"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:msub><mml:mo>∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>-</mml:mo><mml:mi>m</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">/</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic xlink:href="41598_2022_17753_Article_Equh.gif" position="anchor"/></alternatives></disp-formula></p>
        <p id="Par22">In particular, if we selected glaucomatous images where their labels were predicted by the model correctly (<inline-formula id="IEq17"><alternatives><tex-math id="M49">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m\left({x}_{i}\right)\equiv 1$$\end{document}</tex-math><mml:math id="M50"><mml:mrow><mml:mi>m</mml:mi><mml:mfenced close=")" open="("><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>≡</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href="41598_2022_17753_Article_IEq17.gif"/></alternatives></inline-formula>), a score of 1.0 indicates that the optic discs (rationale) are indeed influential in the prediction, while 0.0 indicates that the rationales are not the reasons for the prediction.</p>
        <p id="Par23">For comparison, we also randomly masked the image without completely overlapping the optic disc’s bounding box. The overlapping ratio (i.e., the intersection of the union (IOU)) is no larger than a threshold. Specifically, we randomly selected the IOU’s threshold from 1/2, 2/5, 1/3, and 1/4. The ratio of the area of the original bounding box can also be variant (1, 1/4, 1/9, and 1/16). Finally, we generated ten images with bounding boxes of different thresholds and areas.</p>
      </sec>
    </sec>
    <sec id="Sec13">
      <title>Evaluation metrics</title>
      <p id="Par24">Our experiments report accuracy, precision, sensitivity (recall), specificity, and F1-score.<disp-formula id="Equi"><alternatives><tex-math id="M51">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Accuracy=\frac{TP+TN}{TP+TN+FP+FN}, Precision=\frac{TP}{TP+FP}, Sensitivity= \frac{TP}{TP+FN}, Specificity= \frac{TN}{TN+FP}, F1-score=2\frac{precision\bullet recall}{precision+recall}$$\end{document}</tex-math><mml:math id="M52" display="block"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="italic">TN</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>∙</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><graphic xlink:href="41598_2022_17753_Article_Equi.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par25">TP, TN, FP, and FP denote true positive, true negative, false positive, and false negative. In addition, we report the AUC (Area Under the Curve) ROC (Receiver Operating Characteristics) curve. A ROC curve plots true positive rate (TPR, also called sensitivity) vs. false positive rate (FPR) at different classification thresholds. These metrics were assessed by the Mann–Whitney U test. The Mann–Whitney U test is associated with a p-value, which is the probability of obtaining a mean difference between two methods as extreme as observed in the test, assuming the null hypothesis is correct (i.e., there is no difference between the results generated by two models).</p>
      <p id="Par26">In this study, we used the fivefold cross-validation to obtain a distribution of the experimental metrics and reported standard deviations. For the OHTS dataset, we split the entire dataset randomly into five groups at the patient level. This ensured that no participant was in more than one group to avoid cross-contamination between the training and test datasets. In each fold of the cross-validation, we took one group (20% of total subjects) as the hold-out test set and the remaining 4 groups as the training set. For the LAG dataset, we conducted fivefold cross-validation using the same strategy.</p>
    </sec>
  </sec>
  <sec id="Sec14">
    <title>Results</title>
    <sec id="Sec15">
      <title>POAG diagnosis on the OHTS dataset</title>
      <p id="Par27">The performance was compared with DenseNet-201 and ResNet-152 (Fig. <xref rid="Fig4" ref-type="fig">4</xref>) and four state-of-the-art neural networks, VGG-16<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, NASNetMobile<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, Xception<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>, MobileV2<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>, and ResNet-50<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> (Supplementary Table <xref rid="MOESM2" ref-type="media">S1</xref>). Our model achieved the best results, with an accuracy of 0.930, an F1-score of 0.490, and an AUC of 0.904. Compared to the best baseline (DenseNet-201), GlaucomaNet has higher accuracy (4.80%), F1-score (7.50%), and AUC (1.70%). The p-value indicates that accuracy, precision, specificity, F1-score, and AUC obtained by GlaucomaNet are statically higher than state-of-the-art neural networks. From Supplementary Table <xref rid="MOESM2" ref-type="media">S1</xref>, we also found that DenseNet-201 and ResNet-152 achieved better results than other baselines (1.7–14.9%). It is sufficient to compare our model to these two models hereafter.<fig id="Fig4"><label>Figure 4</label><caption><p>Comparison of different metrics (standard deviation) for different model architectures in the OHTS dataset. p-values are calculated between GlaucomaNet and other models. *p-value ≤ 0.05, **p-value ≤ 0.01.</p></caption><graphic xlink:href="41598_2022_17753_Fig4_HTML" id="MO4"/></fig></p>
      <p id="Par28">By combining the different network architectures in a model ensemble, we found that the random forest method improved accuracy to 0.939 and AUC to 0.910 (Fig. <xref rid="Fig5" ref-type="fig">5</xref> and Supplementary Table <xref rid="MOESM2" ref-type="media">S2</xref>). Meanwhile, the F1-score decreased, leading to reduced sensitivity. However, these changes were not statistically significant.<fig id="Fig5"><label>Figure 5</label><caption><p>Comparison of different metrics (standard deviation) for proposed and ensemble methods in the OHTS dataset.</p></caption><graphic xlink:href="41598_2022_17753_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par29">To evaluate the model’s rationales, we randomly selected 100 “correctly predicted” glaucomatous fundus photographs from the OHTS dataset. After stripping the bounding box of optic discs, 97% of images (comprehensiveness score of 97%) were detected as “normal” by GlaucomaNet (Supplementary Table <xref rid="MOESM2" ref-type="media">S3</xref>). On the contrary, 36% of images (comprehensiveness score of 36%) were detected as “normal” when the random bounding boxes were masked. The results demonstrate that our model is dramatically less confident in its POAG prediction once the optic discs are masked. We applied DenseNet-201 to the same experimental settings and obtained comprehensiveness scores of 82% and 20%, respectively. The comparison between GlaucomaNet and DenseNet-201 suggests that optic discs are more needed in our model to predict POAG from fundus photographs—that is, our model is more rational than DenseNet.</p>
      <p id="Par30">Supplementary Figure <xref rid="MOESM1" ref-type="media">S1</xref> presents more specific results by histograms. The scores for the IOU’s thresholds 1/2, 2/5, 1/3, and 1/4 are 40.3%, 38.4%, 33.0%, and 30.7%, and 25.4%, 23.6%, 18.0%, and 12.9% for GlaucomaNet and DenseNet-201, respectively; the scores for the area ratios 1, 1/4, 1/9, and 1/16 are 65.7%, 32.8%, 26.4%, and 21.1%, and 47.0%, 13.4%, 11.7%, and 8.8% for GlaucomaNet and DenseNet-201, respectively.</p>
    </sec>
    <sec id="Sec16">
      <title>POAG diagnosis on the LAG dataset</title>
      <p id="Par31">Figure <xref rid="Fig6" ref-type="fig">6</xref> compares the results of GlaucomaNet with DenseNet-201 and ResNet-152 on the LAG dataset. Our model obtained the best results, with an accuracy of 0.969, an F1-score of 0.955, and an AUC of 0.997 (Supplementary Table <xref rid="MOESM2" ref-type="media">S4</xref>). GlaucomaNet also outperforms the state-of-the-art methods for all evaluation metrics as shown in Supplementary Table <xref rid="MOESM2" ref-type="media">S4</xref>. In addition, we include the results reported in Li et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, although they used a much larger LAG dataset to train the model. GlaucomaNet achieves superior results even though we only used 41.3% of the LAG dataset for training.<fig id="Fig6"><label>Figure 6</label><caption><p>Comparison of different metrics for different model architectures in the LAG dataset. The model reported in Li et al. was trained on 10,928 images, with 4528 having POAG. There is no reported F1-score for Li et al. P-values are calculated between GlaucomaNet and other models. *p-value ≤ 0.05, **p-value ≤ 0.01.</p></caption><graphic xlink:href="41598_2022_17753_Fig6_HTML" id="MO6"/></fig></p>
    </sec>
    <sec id="Sec17">
      <title>T-distributed stochastic neighbor embedding (t-SNE) method</title>
      <p id="Par32">In this study, the internal features learned by GlaucomaNet were studied using t-distributed Stochastic Neighbor Embedding, which is well suited for visualizing high-dimensional datasets<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. We first obtained the 1024-dimensional features from the output of averaging pooling layer based on the proposed model. Then, we applied the t-SNE technique to reduce the vector into 2 dimensions for visualization. Figure <xref rid="Fig7" ref-type="fig">7</xref> demonstrates that glaucomatous images can be separated from non-glaucomatous images.<fig id="Fig7"><label>Figure 7</label><caption><p>The t-SNE visualization of the proposed model on the OHTS dataset. Each point represents a fundus image. Red and gray dots represent the glaucomatous and non-glaucomatous images.</p></caption><graphic xlink:href="41598_2022_17753_Fig7_HTML" id="MO7"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec18">
    <title>Discussion</title>
    <p id="Par33">The primary aim of this study is to develop a fully automatic deep learning network for POAG diagnosis using fundus photographs. On two datasets, our proposed model was superior to the state-of-the-art model. Unlike single neural networks such as DenseNet, our method fused the features from two state-of-the-art networks. This simulates the fusion of two readers who first learn the discriminative features separately and then fuse the features for grading.</p>
    <p id="Par34">Our experiment also shows that ensemble methods produce more accurate solutions than a single model. On the OHTS dataset, the random forest achieves the highest AUC. There might be two reasons behind this. First, each tree in the random forests is built from a sample drawn with a replacement from the training set. Second, random forests select a subset of features, rather than all features to train the model. As can be seen from Supplementary Table <xref rid="MOESM2" ref-type="media">S2</xref>, its variance decreases, resulting in an overall better model.</p>
    <p id="Par35">Of note, although most evaluation metrics of our proposed model were superior, the sensitivity was lower than other models on the OHTS dataset. One potential reason is that the positive (glaucomatous) examples for model training in the OHTS dataset were as low as 11.9% of the total images. We postulate that further training our model with a larger number of POAG images may improve its performance. We also noticed two studies conducted on OHTS. One was performed by Thakur et al.<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, which achieved an AUC of 0.945. There are two main differences between our approach and theirs. First, we used the whole OHTS dataset while Thakur et al.<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> discarded 24% of the fundus photographs with poor image quality. We believe our experimental settings allow us to more closely access the model’s real-world clinical performance. Second, we did not manually crop the images in the data preprocessing stage, which minimizes the labor required to deploy our model to healthcare centers with a quick turn-around time and again more closely resembles a real-world workflow. To make a fair comparison, we applied the same method in Thakur et al.<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> to our dataset (MobileV2 in Supplementary Table <xref rid="MOESM2" ref-type="media">S1</xref>). The AUC was 0.799, lower than the GlaucomaNet under the same setting. The other study was performed by Fan et al.<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>, which achieved an AUC of 0.88 based on optic disc or VF change attributable to POAG by the Endpoint Committee (the same criteria as us). We applied their model (ResNet-50) to our data split under the same setting and achieved the AUC of 0.863, which is similar to the result obtained by Fan et al. Because we used different data split and preprocessing methods, the results are not strictly comparable.</p>
    <p id="Par36">We also observed that the performance of GlaucomaNet varies dramatically across two datasets. There are potentially three reasons. First, digital fundus photographs have a higher resolution than digitized film fundus photographs. The LAG dataset contains digital fundus photographs, while OHTS contains digitized film fundus photographs. However, GlaucomaNet can still get an AUC of 0.904 on the OHTS dataset. Second, the images in the LAG datasets were carefully selected by the ophthalmologists, and all glaucomatous images are due to structural glaucomatous optic nerve abnormalities. In contrast, some of POAG fundus photographs in OHTS are due to visual field changes in the absence of glaucomatous disc changes over time. Therefore, the task on LAG is less challenging than OHTS.</p>
    <sec id="Sec19">
      <title>Limitation and future work</title>
      <p id="Par37">One limitation of our proposed model arises from the imbalance in the dataset used for its training, particularly the extremely low proportion of images with POAG. As described previously, this likely contributed to the relatively lower sensitivity. However, as evidenced by the results on more balanced LAG, this limitation may be addressed by further training using image datasets with a higher proportion of POAG cases.</p>
      <p id="Par38">Another limitation lies in detecting POAG eyes due to visual field abnormality without any obvious structural sign of Glaucomatous Optic Neuropathy (GON). These images had a higher tendency to be missed by our model. It would be interesting for future studies to differentiate POAG due to VF abnormality and GON.</p>
      <p id="Par39">In conclusion, this study proposed a new end-to-end deep learning network for automatic POAG detection from fundus photographs. Two datasets were used to evaluate the proposed model. The results demonstrated that the proposed network has a good performance on POAG diagnosis. We also analyzed the performance of several ensemble methods; lessons from these approaches may have applicability to developing deep learning models for other retinal diseases, such as diabetic retinopathy, age-related macular degeneration, and even for image-based deep learning systems outside of ophthalmology. Although deep learning models are often considered “black-box” entities, we aimed to improve the transparency of our algorithm by constructing a new dataset by masking the optic disc on the fundus photographs. These “contrasting” examples help us understand if the rationales (optic disc) influence the POAG diagnosis. These efforts to demystify deep learning models may help improve levels of acceptability to patients and adoption by ophthalmologists.</p>
      <p id="Par40">The deep learning model and data partition are publicly available (<ext-link ext-link-type="uri" xlink:href="https://github.com/bionlplab/‌GlaucomaNet">https://github.com/bionlplab/‌GlaucomaNet</ext-link>). By making them available, we aim to maximize this study's transparency and reproducibility and provide a benchmark for further refinement and development of the algorithm. In addition, this deep learning model, trained on one of the largest POAG fundus photograph datasets, may allow for future deep learning studies on POAG diagnosis where only smaller datasets are available.</p>
      <p id="Par41">In the future, we aim to improve the model by incorporating multi-modal data. It would also be interesting to compare the model's accuracy with different groups of ophthalmologists (e.g., graders at Reading Centers, glaucoma specialists, general ophthalmologists, and trainee ophthalmologists). These results could then be tested and validated by further studies from different countries and populations. In that case, it is possible that the integration of deep learning models into clinical practice might become increasingly acceptable to patients and ophthalmologists and may ultimately enhance clinical decision-making.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Information</title>
    <sec id="Sec20">
      <p>
        <supplementary-material content-type="local-data" id="MOESM1">
          <media xlink:href="41598_2022_17753_MOESM1_ESM.docx">
            <caption>
              <p>Supplementary Figure S1.</p>
            </caption>
          </media>
        </supplementary-material>
        <supplementary-material content-type="local-data" id="MOESM2">
          <media xlink:href="41598_2022_17753_MOESM2_ESM.docx">
            <caption>
              <p>Supplementary Tables.</p>
            </caption>
          </media>
        </supplementary-material>
      </p>
    </sec>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher's note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
    <fn>
      <p>These authors jointly supervised this work: Fei Wang, Sarah H. Van Tassel and Yifan Peng.</p>
    </fn>
  </fn-group>
  <sec>
    <title>Supplementary Information</title>
    <p>The online version contains supplementary material available at 10.1038/s41598-022-17753-4.</p>
  </sec>
  <ack>
    <title>Acknowledgements</title>
    <p>This project was supported by the National Library of Medicine under award number 4R00LM013001. This work was also supported by awards from the National Eye Institute, the National Center on Minority Health and Health Disparities, National Institutes of Health (grants EY09341, EY09307), Horncrest Foundation, awards to the Department of Ophthalmology and Visual Sciences at Washington University, the NIH Vision Core Grant P30 EY 02687, Merck Research Laboratories, Pfizer, Inc., White House Station, New Jersey, and unrestricted grants from Research to Prevent Blindness, Inc., New York, NY.</p>
  </ack>
  <notes notes-type="author-contribution">
    <title>Author contributions</title>
    <p>M.L. implemented the methods, conducted the experiments and wrote the paper. Y.P. advised on all aspects of the work involved in this project and assisted in the paper writing. S.H.V.T. provided knowledge about POAG, suggestions for experiments, and edited the paper. F.W. advised the overall direction of the project and edited the paper. B.H. advised the methods and edited the paper. L.L., M.G., and M.K. provided ideas about the POAG detection based on the dataset and edited the paper. All authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data availability</title>
    <p>The Ocular Hypertension Treatment Study (OHTS) dataset is available upon request due to patient protection (<ext-link ext-link-type="uri" xlink:href="https://ohts.wustl.edu/">https://ohts.wustl.edu/</ext-link>), please contact the author in<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> (Michael Kas: kass@wustl.edu). The large-scale attention-based glaucoma (LAG) database is available in <ext-link ext-link-type="uri" xlink:href="https://github.com/smilell/AG-CNN">https://github.com/smilell/AG-CNN</ext-link> (please contact liliu1995@buaa.edu.cn or liliu419@foxmail.com to get the password to download the dataset).</p>
  </notes>
  <notes id="FPar1" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par42">The authors declare no competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bourne</surname>
            <given-names>RR</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Causes of vision loss worldwide, 1990–2010: a systematic analysis</article-title>
        <source>Lancet Glob. Health</source>
        <year>2013</year>
        <volume>1</volume>
        <fpage>e339</fpage>
        <lpage>e349</lpage>
        <pub-id pub-id-type="doi">10.1016/S2214-109X(13)70113-X</pub-id>
        <?supplied-pmid 25104599?>
        <pub-id pub-id-type="pmid">25104599</pub-id>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Quigley</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Broman</surname>
            <given-names>AT</given-names>
          </name>
        </person-group>
        <article-title>The number of people with glaucoma worldwide in 2010 and 2020</article-title>
        <source>Br. J. Ophthalmol.</source>
        <year>2006</year>
        <volume>90</volume>
        <fpage>262</fpage>
        <lpage>267</lpage>
        <pub-id pub-id-type="doi">10.1136/bjo.2005.081224</pub-id>
        <?supplied-pmid 16488940?>
        <pub-id pub-id-type="pmid">16488940</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Sommer</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Racial differences in the cause-specific prevalence of blindness in east Baltimore</article-title>
        <source>N. Engl. J. Med.</source>
        <year>1991</year>
        <volume>325</volume>
        <fpage>1412</fpage>
        <lpage>1417</lpage>
        <pub-id pub-id-type="doi">10.1056/NEJM199111143252004</pub-id>
        <?supplied-pmid 1922252?>
        <pub-id pub-id-type="pmid">1922252</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Torres</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Varma</surname>
            <given-names>R</given-names>
          </name>
          <collab>Group, L. A. L. E. S.</collab>
        </person-group>
        <article-title>Variation in intraocular pressure and the risk of developing open-angle glaucoma: The Los Angeles Latino eye study</article-title>
        <source>Am. J. Ophthalmol.</source>
        <year>2018</year>
        <volume>188</volume>
        <fpage>51</fpage>
        <lpage>59</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ajo.2018.01.013</pub-id>
        <?supplied-pmid 29360458?>
        <pub-id pub-id-type="pmid">29360458</pub-id>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Doshi</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Ying-Lai</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Azen</surname>
            <given-names>SP</given-names>
          </name>
          <name>
            <surname>Varma</surname>
            <given-names>R</given-names>
          </name>
          <collab>Group, L. A. L. E. S.</collab>
        </person-group>
        <article-title>Sociodemographic, family history, and lifestyle risk factors for open-angle glaucoma and ocular hypertension: The Los Angeles Latino Eye Study</article-title>
        <source>Ophthalmology</source>
        <year>2008</year>
        <volume>115</volume>
        <fpage>639</fpage>
        <lpage>647.e632</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ophtha.2007.05.032</pub-id>
        <?supplied-pmid 17900693?>
        <pub-id pub-id-type="pmid">17900693</pub-id>
      </element-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Quigley</surname>
            <given-names>HA</given-names>
          </name>
          <name>
            <surname>Katz</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Derick</surname>
            <given-names>RJ</given-names>
          </name>
          <name>
            <surname>Gilbert</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Sommer</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <article-title>An evaluation of optic disc and nerve fiber layer examinations in monitoring progression of early glaucoma damage</article-title>
        <source>Ophthalmology</source>
        <year>1992</year>
        <volume>99</volume>
        <fpage>19</fpage>
        <lpage>28</lpage>
        <pub-id pub-id-type="doi">10.1016/S0161-6420(92)32018-4</pub-id>
        <?supplied-pmid 1741133?>
        <pub-id pub-id-type="pmid">1741133</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kolomeyer</surname>
            <given-names>NN</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Lessons learned from two large community-based glaucoma screening studies</article-title>
        <source>J. Glaucoma</source>
        <year>2021</year>
        <volume>21</volume>
        <fpage>2123</fpage>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Singh</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Dutta</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>ParthaSarathi</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Uher</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Burget</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Image processing based automatic diagnosis of glaucoma using wavelet features of segmented optic disc from fundus image</article-title>
        <source>Comput. Methods Programs Biomed.</source>
        <year>2016</year>
        <volume>124</volume>
        <fpage>108</fpage>
        <lpage>120</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2015.10.010</pub-id>
        <?supplied-pmid 26574297?>
        <pub-id pub-id-type="pmid">26574297</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Acharya</surname>
            <given-names>UR</given-names>
          </name>
          <name>
            <surname>Dua</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Du</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Chua</surname>
            <given-names>CK</given-names>
          </name>
        </person-group>
        <article-title>Automated diagnosis of glaucoma using texture and higher order spectra features</article-title>
        <source>IEEE Trans. Inf. Technol. Biomed.</source>
        <year>2011</year>
        <volume>15</volume>
        <fpage>449</fpage>
        <lpage>455</lpage>
        <pub-id pub-id-type="doi">10.1109/TITB.2011.2119322</pub-id>
        <?supplied-pmid 21349793?>
        <pub-id pub-id-type="pmid">21349793</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Dua</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Acharya</surname>
            <given-names>UR</given-names>
          </name>
          <name>
            <surname>Chowriappa</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Sree</surname>
            <given-names>SV</given-names>
          </name>
        </person-group>
        <article-title>Wavelet-based energy features for glaucomatous image classification</article-title>
        <source>IEEE Trans. Inf Technol. Biomed.</source>
        <year>2011</year>
        <volume>16</volume>
        <fpage>80</fpage>
        <lpage>87</lpage>
        <pub-id pub-id-type="doi">10.1109/TITB.2011.2176540</pub-id>
        <?supplied-pmid 22113813?>
        <pub-id pub-id-type="pmid">22113813</pub-id>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Issac</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Sarathi</surname>
            <given-names>MP</given-names>
          </name>
          <name>
            <surname>Dutta</surname>
            <given-names>MK</given-names>
          </name>
        </person-group>
        <article-title>An adaptive threshold based image processing technique for improved glaucoma detection and classification</article-title>
        <source>Comput. Methods Programs Biomed.</source>
        <year>2015</year>
        <volume>122</volume>
        <fpage>229</fpage>
        <lpage>244</lpage>
        <pub-id pub-id-type="doi">10.1016/j.cmpb.2015.08.002</pub-id>
        <?supplied-pmid 26321351?>
        <pub-id pub-id-type="pmid">26321351</pub-id>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Fully automated segmentation of brain tumor from multiparametric MRI using 3D context deep supervised U-Net</article-title>
        <source>Med. Phys.</source>
        <year>2021</year>
        <volume>48</volume>
        <fpage>4365</fpage>
        <pub-id pub-id-type="doi">10.1002/mp.15032</pub-id>
        <?supplied-pmid 34101845?>
        <pub-id pub-id-type="pmid">34101845</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Lin</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Chiu</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Zhao</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Deep-recursive residual network for image semantic segmentation</article-title>
        <source>Neural Comput. Appl.</source>
        <year>2020</year>
        <volume>32</volume>
        <fpage>12935</fpage>
        <lpage>12947</lpage>
        <pub-id pub-id-type="doi">10.1007/s00521-020-04738-5</pub-id>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tang</surname>
            <given-names>Y-X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated abnormality classification of chest radiographs using deep convolutional neural networks</article-title>
        <source>NPJ Digit. Med.</source>
        <year>2020</year>
        <volume>3</volume>
        <fpage>1</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1038/s41746-020-0273-z</pub-id>
        <pub-id pub-id-type="pmid">31934645</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <name>
            <surname>Jiang</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Cheng</surname>
            <given-names>K-T</given-names>
          </name>
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Multi-task siamese network for retinal artery/vein separation via deep convolution along vessel</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2020</year>
        <volume>39</volume>
        <fpage>2904</fpage>
        <lpage>2919</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2020.2980117</pub-id>
        <?supplied-pmid 32167888?>
        <pub-id pub-id-type="pmid">32167888</pub-id>
      </element-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Automated detection of clinically significant prostate cancer in mp-MRI images based on an end-to-end deep neural network</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <fpage>1127</fpage>
        <lpage>1139</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2017.2789181</pub-id>
        <?supplied-pmid 29727276?>
        <pub-id pub-id-type="pmid">29727276</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Cascaded triplanar autoencoder m-net for fully automatic segmentation of left ventricle myocardial scar from three-dimensional late gadolinium-enhanced mr images</article-title>
        <source>IEEE Journal of Biomedical and Health Informatics</source>
        <year>2022</year>
        <volume>26</volume>
        <issue>6</issue>
        <fpage>2582</fpage>
        <lpage>2593</lpage>
        <pub-id pub-id-type="doi">10.1109/JBHI.2022.3146013</pub-id>
        <?supplied-pmid 35077377?>
        <pub-id pub-id-type="pmid">35077377</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <mixed-citation publication-type="other">Wanyan, T. <italic>et al.</italic> Supervised pretraining through contrastive categorical positive samplings to improve COVID-19 mortality prediction. <italic>Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</italic>. (2022).</mixed-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gulshan</surname>
            <given-names>V</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</article-title>
        <source>JAMA</source>
        <year>2016</year>
        <volume>316</volume>
        <fpage>2402</fpage>
        <lpage>2410</lpage>
        <pub-id pub-id-type="doi">10.1001/jama.2016.17216</pub-id>
        <?supplied-pmid 27898976?>
        <pub-id pub-id-type="pmid">27898976</pub-id>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting risk of late age-related macular degeneration using deep learning</article-title>
        <source>NPJ Digit. Med.</source>
        <year>2020</year>
        <volume>3</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1038/s41746-020-00317-z</pub-id>
        <pub-id pub-id-type="pmid">31934645</pub-id>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keenan</surname>
            <given-names>TD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep learning automated detection of reticular pseudodrusen from fundus autofluorescence images or color fundus photographs in AREDS2</article-title>
        <source>Ophthalmology</source>
        <year>2020</year>
        <volume>127</volume>
        <fpage>1674</fpage>
        <lpage>1687</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ophtha.2020.05.036</pub-id>
        <?supplied-pmid 32447042?>
        <pub-id pub-id-type="pmid">32447042</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Peng</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>DeepSeeNet: A deep learning model for automated classification of patient-based age-related macular degeneration severity from color fundus photographs</article-title>
        <source>Ophthalmology</source>
        <year>2019</year>
        <volume>126</volume>
        <fpage>565</fpage>
        <lpage>575</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ophtha.2018.11.015</pub-id>
        <?supplied-pmid 30471319?>
        <pub-id pub-id-type="pmid">30471319</pub-id>
      </element-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Keenan</surname>
            <given-names>TD</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep learning approach for automated detection of geographic atrophy from color fundus photographs</article-title>
        <source>Ophthalmology</source>
        <year>2019</year>
        <volume>126</volume>
        <fpage>1533</fpage>
        <lpage>1540</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ophtha.2019.06.005</pub-id>
        <?supplied-pmid 31358385?>
        <pub-id pub-id-type="pmid">31358385</pub-id>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Grassmann</surname>
            <given-names>F</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A deep learning algorithm for prediction of age-related eye disease study severity scale for age-related macular degeneration from color fundus photography</article-title>
        <source>Ophthalmology</source>
        <year>2018</year>
        <volume>125</volume>
        <fpage>1410</fpage>
        <lpage>1420</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ophtha.2018.02.037</pub-id>
        <?supplied-pmid 29653860?>
        <pub-id pub-id-type="pmid">29653860</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Poplin</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning</article-title>
        <source>Nat. Biomed. Eng.</source>
        <year>2018</year>
        <volume>2</volume>
        <fpage>158</fpage>
        <lpage>164</lpage>
        <pub-id pub-id-type="doi">10.1038/s41551-018-0195-0</pub-id>
        <?supplied-pmid 31015713?>
        <pub-id pub-id-type="pmid">31015713</pub-id>
      </element-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lin</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Artificial intelligence in tumor subregion analysis based on medical imaging: A review</article-title>
        <source>J. Appl. Clin. Med. Phys.</source>
        <year>2017</year>
        <volume>22</volume>
        <issue>7</issue>
        <fpage>10</fpage>
        <lpage>26</lpage>
        <pub-id pub-id-type="doi">10.1002/acm2.13321</pub-id>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <mixed-citation publication-type="other">Ghahramani, G. <italic>et al.</italic> Multi-task deep learning-based survival analysis on the prognosis of late AMD using the longitudinal data in AREDS. <italic>AMIA Annual Symposium Proceedings.</italic> Vol. <bold>2021</bold>. (American Medical Informatics Association, 2021).</mixed-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Chen, X. <italic>et al.</italic> Glaucoma detection based on deep convolutional neural network. <italic>37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC).</italic> 715–718 (2015).</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Raghavendra</surname>
            <given-names>U</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Deep convolution neural network for accurate diagnosis of glaucoma using digital fundus images</article-title>
        <source>Inf. Sci.</source>
        <year>2018</year>
        <volume>441</volume>
        <fpage>41</fpage>
        <lpage>49</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ins.2018.01.051</pub-id>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>L</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A large-scale database and a CNN model for attention-based glaucoma detection</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2019</year>
        <volume>39</volume>
        <fpage>413</fpage>
        <lpage>424</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2019.2927226</pub-id>
        <?supplied-pmid 31283476?>
        <pub-id pub-id-type="pmid">31283476</pub-id>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>Z</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Efficacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs</article-title>
        <source>Ophthalmology</source>
        <year>2018</year>
        <volume>125</volume>
        <fpage>1199</fpage>
        <lpage>1206</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ophtha.2018.01.023</pub-id>
        <?supplied-pmid 29506863?>
        <pub-id pub-id-type="pmid">29506863</pub-id>
      </element-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Thakur</surname>
            <given-names>A</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Predicting glaucoma before onset using deep learning</article-title>
        <source>Ophthalmol. Glaucoma</source>
        <year>2020</year>
        <volume>3</volume>
        <fpage>262</fpage>
        <lpage>268</lpage>
        <pub-id pub-id-type="doi">10.1016/j.ogla.2020.04.012</pub-id>
        <?supplied-pmid 33012331?>
        <pub-id pub-id-type="pmid">33012331</pub-id>
      </element-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Christopher</surname>
            <given-names>M</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Performance of deep learning architectures and transfer learning for detecting glaucomatous optic neuropathy in fundus photographs</article-title>
        <source>Sci. Rep.</source>
        <year>2018</year>
        <volume>8</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
        <pub-id pub-id-type="doi">10.1038/s41598-018-35044-9</pub-id>
        <pub-id pub-id-type="pmid">29311619</pub-id>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fu</surname>
            <given-names>H</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Disc-aware ensemble network for glaucoma screening from fundus image</article-title>
        <source>IEEE Trans. Med. Imaging</source>
        <year>2018</year>
        <volume>37</volume>
        <fpage>2493</fpage>
        <lpage>2501</lpage>
        <pub-id pub-id-type="doi">10.1109/TMI.2018.2837012</pub-id>
        <?supplied-pmid 29994764?>
        <pub-id pub-id-type="pmid">29994764</pub-id>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <mixed-citation publication-type="other">Li, A. <italic>et al.</italic> Integrating holistic and local deep features for glaucoma classification. <italic>38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC).</italic> 1328–1331 (2016).</mixed-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Yang</surname>
            <given-names>X</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Co-trained convolutional neural networks for automated detection of prostate cancer in multi-parametric MRI</article-title>
        <source>Med. Image Anal.</source>
        <year>2017</year>
        <volume>42</volume>
        <fpage>212</fpage>
        <lpage>227</lpage>
        <pub-id pub-id-type="doi">10.1016/j.media.2017.08.006</pub-id>
        <?supplied-pmid 28850876?>
        <pub-id pub-id-type="pmid">28850876</pub-id>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zoabi</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Deri-Rozov</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Shomron</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>Machine learning-based prediction of COVID-19 diagnosis based on symptoms</article-title>
        <source>npj Digit. Med.</source>
        <year>2021</year>
        <volume>4</volume>
        <fpage>1</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1038/s41746-020-00372-6</pub-id>
        <pub-id pub-id-type="pmid">33398041</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Monaghan</surname>
            <given-names>Y</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Machine Learning for Prediction of Patients on Hemodialysis with an Undetected SARS-CoV-2 Infection</article-title>
        <source>Kidney360</source>
        <year>2021</year>
        <volume>13</volume>
        <fpage>456</fpage>
        <lpage>468</lpage>
        <pub-id pub-id-type="doi">10.34067/KID.0003802020</pub-id>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kass</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Parrish</surname>
            <given-names>RK</given-names>
            <suffix>2nd</suffix>
          </name>
          <name>
            <surname>Wilson</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Gordon</surname>
            <given-names>MO</given-names>
          </name>
        </person-group>
        <article-title>The ocular hypertension treatment study: A randomized trial determines that topical ocular hypotensive medication delays or prevents the onset of primary open-angle glaucoma</article-title>
        <source>Arch. Ophthalmol</source>
        <year>2002</year>
        <volume>120</volume>
        <fpage>701</fpage>
        <lpage>713</lpage>
        <pub-id pub-id-type="doi">10.1001/archopht.120.6.701</pub-id>
        <?supplied-pmid 12049574?>
        <pub-id pub-id-type="pmid">12049574</pub-id>
      </element-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <mixed-citation publication-type="other">Li, L. <italic>et al.</italic> Attention Based Glaucoma Detection: A Large-Scale Database and CNN Model. <italic>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.</italic> 10571–10580 (2019).</mixed-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <mixed-citation publication-type="other">Huang, G. <italic>et al.</italic> Densely Connected Convolutional Networks. <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</italic> 4700–4708 (2017).</mixed-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">He, K. <italic>et al.</italic> Deep Residual Learning for Image Recognition. <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</italic> 770–778 (2016).</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <mixed-citation publication-type="other">Ioffe, S. <italic>et al.</italic> Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. <italic>International Conference on Machine Learning.</italic> 448–456 (2015).</mixed-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <mixed-citation publication-type="other">Glorot, X. <italic>et al.</italic> Deep Sparse Rectifier Neural Networks. <italic>Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics.</italic> 315–323 (2011).</mixed-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>LeCun</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bottou</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Bengio</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Haffner</surname>
            <given-names>P</given-names>
          </name>
        </person-group>
        <article-title>Gradient-based learning applied to document recognition</article-title>
        <source>Proc. IEEE</source>
        <year>1998</year>
        <volume>86</volume>
        <fpage>2278</fpage>
        <lpage>2324</lpage>
        <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ho</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wookey</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>The real-world-weight cross-entropy loss function: Modeling the costs of mislabeling</article-title>
        <source>IEEE Access</source>
        <year>2019</year>
        <volume>8</volume>
        <fpage>4806</fpage>
        <lpage>4813</lpage>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2962617</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Huang</surname>
            <given-names>S-C</given-names>
          </name>
          <name>
            <surname>Pareek</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Seyyedi</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Banerjee</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Lungren</surname>
            <given-names>MP</given-names>
          </name>
        </person-group>
        <article-title>Fusion of medical imaging and electronic health records using deep learning: A systematic review and implementation guidelines</article-title>
        <source>NPJ Digit. Med.</source>
        <year>2020</year>
        <volume>3</volume>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1038/s41746-019-0211-0</pub-id>
        <pub-id pub-id-type="pmid">31934645</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: A method for stochastic optimization. <italic>arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">arXiv:1412.6980</ext-link> (2014).</mixed-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ho</surname>
            <given-names>TK</given-names>
          </name>
        </person-group>
        <article-title>The random subspace method for constructing decision forests</article-title>
        <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        <year>1998</year>
        <volume>20</volume>
        <fpage>832</fpage>
        <lpage>844</lpage>
        <pub-id pub-id-type="doi">10.1109/34.709601</pub-id>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hou</surname>
            <given-names>B-J</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>Z-H</given-names>
          </name>
        </person-group>
        <article-title>Learning with interpretable structure from gated RNN</article-title>
        <source>IEEE Trans. Neural Netw. Learn. Syst.</source>
        <year>2020</year>
        <volume>31</volume>
        <fpage>2267</fpage>
        <lpage>2279</lpage>
        <?supplied-pmid 32071002?>
        <pub-id pub-id-type="pmid">32071002</pub-id>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <mixed-citation publication-type="other">Zaidan, O. <italic>et al.</italic> Using “annotator rationales” to improve machine learning for text categorization. <italic>Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference.</italic> 260–267 (2007).</mixed-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <mixed-citation publication-type="other">Yu, M., Chang, S., Zhang, Y. &amp; Jaakkola, T. S. Rethinking cooperative rationalization: Introspective extraction and complement control. <italic>arXiv preprint </italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1910.13294">arXiv:1910.13294</ext-link> (2019).</mixed-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <mixed-citation publication-type="other">DeYoung, J. <italic>et al.</italic> ERASER: A benchmark to evaluate rationalized NLP models. <italic>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1911.03429">arXiv:1911.03429</ext-link>. 4443–4458 (2019).</mixed-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <mixed-citation publication-type="other">Simonyan, K. &amp; Zisserman, A. Very deep convolutional networks for large-scale image recognition. <italic>arXiv preprint </italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.1556">arXiv:1409.1556</ext-link> (2014).</mixed-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <mixed-citation publication-type="other">Zoph, B. <italic>et al.</italic> Learning Transferable Architectures for Scalable Image Recognition. <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</italic> 8697–8710 (2018).</mixed-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <mixed-citation publication-type="other">Chollet, F. <italic>et al.</italic> Xception: Deep Learning with Depthwise Separable Convolutions. <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</italic> 1251–1258 (2017).</mixed-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <mixed-citation publication-type="other">Sandler, M. <italic>et al.</italic> MobileNetV2: Inverted Residuals and Linear Bottlenecks. <italic>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</italic> 4510–4520 (2018).</mixed-citation>
    </ref>
    <ref id="CR58">
      <label>58.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Van der Maaten</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Hinton</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Visualizing data using t-SNE</article-title>
        <source>J. Mach. Learn. Res.</source>
        <year>2008</year>
        <volume>9</volume>
        <fpage>11</fpage>
      </element-citation>
    </ref>
    <ref id="CR59">
      <label>59.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Fan</surname>
            <given-names>R</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Detecting glaucoma in the ocular hypertension study using deep learning</article-title>
        <source>JAMA Ophthalmol.</source>
        <year>2022</year>
        <volume>140</volume>
        <fpage>383</fpage>
        <lpage>391</lpage>
        <pub-id pub-id-type="doi">10.1001/jamaophthalmol.2022.0244</pub-id>
        <?supplied-pmid 35297959?>
        <pub-id pub-id-type="pmid">35297959</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
