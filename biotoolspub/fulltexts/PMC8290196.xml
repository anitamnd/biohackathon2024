<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Gigascience</journal-id>
    <journal-id journal-id-type="iso-abbrev">Gigascience</journal-id>
    <journal-id journal-id-type="publisher-id">gigascience</journal-id>
    <journal-title-group>
      <journal-title>GigaScience</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2047-217X</issn>
    <publisher>
      <publisher-name>Oxford University Press</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8290196</article-id>
    <article-id pub-id-type="pmid">34282452</article-id>
    <article-id pub-id-type="doi">10.1093/gigascience/giab052</article-id>
    <article-id pub-id-type="publisher-id">giab052</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research</subject>
      </subj-group>
      <subj-group subj-group-type="category-taxonomy-collection">
        <subject>AcademicSubjects/SCI00960</subject>
        <subject>AcademicSubjects/SCI02254</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ChronoRoot: High-throughput phenotyping by deep segmentation networks reveals novel temporal parameters of plant root system architecture</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-6684-5300</contrib-id>
        <name>
          <surname>Gaggion</surname>
          <given-names>Nicolás</given-names>
        </name>
        <aff><institution>Research Institute for Signals, Systems and Computational Intelligence (sinc(i)), CONICET, FICH, Universidad Nacional del Litoral</institution>, Ciudad Universitaria UNL, Santa Fe, <country country="AR">Argentina</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8478-8808</contrib-id>
        <name>
          <surname>Ariel</surname>
          <given-names>Federico</given-names>
        </name>
        <aff><institution>Instituto de Agrobiotecnología del Litoral (IAL)</institution>, CONICET, FBCB, Universidad Nacional del Litoral, Colectora Ruta Nacional 168 km 0, Santa Fe, <country country="AR">Argentina</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Daric</surname>
          <given-names>Vladimir</given-names>
        </name>
        <aff><institution>Institute of Plant Sciences Paris-Saclay (IPS2)</institution>, CNRS, INRA, University Paris-Saclay and University of Paris Bâtiment 630, 91192 Gif sur Yvette, <country country="FR">France</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Lambert</surname>
          <given-names>Éric</given-names>
        </name>
        <aff><institution>Institute of Plant Sciences Paris-Saclay (IPS2)</institution>, CNRS, INRA, University Paris-Saclay and University of Paris Bâtiment 630, 91192 Gif sur Yvette, <country country="FR">France</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Legendre</surname>
          <given-names>Simon</given-names>
        </name>
        <aff><institution>Institute of Plant Sciences Paris-Saclay (IPS2)</institution>, CNRS, INRA, University Paris-Saclay and University of Paris Bâtiment 630, 91192 Gif sur Yvette, <country country="FR">France</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-6661-9357</contrib-id>
        <name>
          <surname>Roulé</surname>
          <given-names>Thomas</given-names>
        </name>
        <aff><institution>Institute of Plant Sciences Paris-Saclay (IPS2)</institution>, CNRS, INRA, University Paris-Saclay and University of Paris Bâtiment 630, 91192 Gif sur Yvette, <country country="FR">France</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-9929-287X</contrib-id>
        <name>
          <surname>Camoirano</surname>
          <given-names>Alejandra</given-names>
        </name>
        <aff><institution>Instituto de Agrobiotecnología del Litoral (IAL)</institution>, CONICET, FBCB, Universidad Nacional del Litoral, Colectora Ruta Nacional 168 km 0, Santa Fe, <country country="AR">Argentina</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-2182-4351</contrib-id>
        <name>
          <surname>Milone</surname>
          <given-names>Diego H</given-names>
        </name>
        <aff><institution>Research Institute for Signals, Systems and Computational Intelligence (sinc(i)), CONICET, FICH, Universidad Nacional del Litoral</institution>, Ciudad Universitaria UNL, Santa Fe, <country country="AR">Argentina</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-5698-9482</contrib-id>
        <name>
          <surname>Crespi</surname>
          <given-names>Martin</given-names>
        </name>
        <aff><institution>Institute of Plant Sciences Paris-Saclay (IPS2)</institution>, CNRS, INRA, University Paris-Saclay and University of Paris Bâtiment 630, 91192 Gif sur Yvette, <country country="FR">France</country></aff>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9788-5201</contrib-id>
        <name>
          <surname>Blein</surname>
          <given-names>Thomas</given-names>
        </name>
        <!--thomas.blein@ips2.universite-paris-saclay.fr-->
        <aff><institution>Institute of Plant Sciences Paris-Saclay (IPS2)</institution>, CNRS, INRA, University Paris-Saclay and University of Paris Bâtiment 630, 91192 Gif sur Yvette, <country country="FR">France</country></aff>
        <xref rid="cor1" ref-type="corresp"/>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-8500-788X</contrib-id>
        <name>
          <surname>Ferrante</surname>
          <given-names>Enzo</given-names>
        </name>
        <!--eferrante@sinc.unl.edu.ar-->
        <aff><institution>Research Institute for Signals, Systems and Computational Intelligence (sinc(i)), CONICET, FICH, Universidad Nacional del Litoral</institution>, Ciudad Universitaria UNL, Santa Fe, <country country="AR">Argentina</country></aff>
        <xref rid="cor2" ref-type="corresp"/>
      </contrib>
    </contrib-group>
    <author-notes>
      <corresp id="cor1">Correspondence address. Thomas Blein, Institute of Plant Sciences Paris-Saclay (IPS2), CNRS, INRA, University Paris-Saclay and University of Paris Bâtiment 630, 91192 Gif sur Yvette, France. E-mail: <email>thomas.blein@ips2.universite-paris-saclay.fr</email></corresp>
      <corresp id="cor2">Correspondence address. Enzo Ferrante, Research Institute for Signals, Systems and Computational Intelligence (sinc(i)), CONICET, FICH, Universidad Nacional del Litoral, Ciudad Universitaria UNL, Santa Fe, Argentina. E-mail: <email>eferrante@sinc.unl.edu.ar</email></corresp>
    </author-notes>
    <pub-date pub-type="epub" iso-8601-date="2021-07-20">
      <day>20</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>20</day>
      <month>7</month>
      <year>2021</year>
    </pub-date>
    <volume>10</volume>
    <issue>7</issue>
    <elocation-id>giab052</elocation-id>
    <history>
      <date date-type="received">
        <day>21</day>
        <month>12</month>
        <year>2020</year>
      </date>
      <date date-type="rev-recd">
        <day>07</day>
        <month>6</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>25</day>
        <month>6</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2021. Published by Oxford University Press GigaScience.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p>
      </license>
    </permissions>
    <self-uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052.pdf"/>
    <abstract>
      <title>Abstract</title>
      <sec id="abs1">
        <title>Background</title>
        <p>Deep learning methods have outperformed previous techniques in most computer vision tasks, including image-based plant phenotyping. However, massive data collection of root traits and the development of associated artificial intelligence approaches have been hampered by the inaccessibility of the rhizosphere. Here we present ChronoRoot, a system that combines 3D-printed open-hardware with deep segmentation networks for high temporal resolution phenotyping of plant roots in agarized medium.</p>
      </sec>
      <sec id="abs2">
        <title>Results</title>
        <p>We developed a novel deep learning–based root extraction method that leverages the latest advances in convolutional neural networks for image segmentation and incorporates temporal consistency into the root system architecture reconstruction process. Automatic extraction of phenotypic parameters from sequences of images allowed a comprehensive characterization of the root system growth dynamics. Furthermore, novel time-associated parameters emerged from the analysis of spectral features derived from temporal signals.</p>
      </sec>
      <sec id="abs3">
        <title>Conclusions</title>
        <p>Our work shows that the combination of machine intelligence methods and a 3D-printed device expands the possibilities of root high-throughput phenotyping for genetics and natural variation studies, as well as the screening of clock-related mutants, revealing novel root traits.</p>
      </sec>
    </abstract>
    <kwd-group kwd-group-type="keywords">
      <kwd>convolutional neural networks</kwd>
      <kwd>image segmentation</kwd>
      <kwd>root system architecture</kwd>
      <kwd>temporal phenotyping</kwd>
      <kwd>3D-printed hardware</kwd>
    </kwd-group>
    <funding-group>
      <award-group award-type="grant">
        <funding-source>
          <institution-wrap>
            <institution>Agencia Nacional de Promoción Científica y Tecnológica</institution>
            <institution-id institution-id-type="DOI">10.13039/501100003074</institution-id>
          </institution-wrap>
        </funding-source>
        <award-id>PICT2018-3907</award-id>
        <award-id>50220140100084LI</award-id>
        <award-id>50620190100145LI</award-id>
        <award-id>PICT2019-04137</award-id>
      </award-group>
    </funding-group>
    <counts>
      <page-count count="15"/>
    </counts>
  </article-meta>
</front>
<body>
  <sec sec-type="Background" id="sec1">
    <title>Background</title>
    <p>Plants are sessile organisms unable to seek out optimal environmental conditions for development and survival. Strikingly, a remarkable developmental plasticity allows plants to complete their life cycle under changing growth conditions [<xref rid="bib1" ref-type="bibr">1</xref>]. Understanding plant root plastic growth is crucial to assess how different populations may respond to the same soil properties or environmental conditions and to link this developmental adaptation to their genetic background [<xref rid="bib2" ref-type="bibr">2</xref>]. Under controlled conditions, root development is generally observed on the basis of images of plants growing vertically on the surface of a semisolid agarized medium. Root system architecture (RSA) is then characterized by parameterization of a grown plant, which relies on the combination of a subset of variables such as main root (MR) length or density and length of the lateral roots (LRs) [<xref rid="bib3" ref-type="bibr">3</xref>]. Several semi-automatic tools have been developed to assist root phenotyping at specific time points [<xref rid="bib4" ref-type="bibr">4</xref>]. However, temporal phenotyping is generally hindered by technological limitations, ignoring potentially useful phenotypical parameters that may be linked to the temporal dynamics of root growth. Here we present ChronoRoot, a low-cost system based on off-the-shelf electronics, 3D-printed hardware components, and deep learning models, allowing high-throughput temporal phenotyping of <italic toggle="yes">Arabidopsis thaliana</italic> RSA. Figure <xref rid="fig1" ref-type="fig">1</xref> illustrates the different components of ChronoRoot. Temporal sequences of pictures, automatically snapped, are processed for root segmentation through a convolutional neural network (CNN) model. We leverage the latest advances in CNNs for image segmentation and propose an architecture for RSA delineation that incorporates deep supervision, producing fast and accurate segmentations. The root extraction workflow is completed by a temporal consistency refinement step and a final graph generation process, which generates a labelled root graph for every image. An exploratory approach assessing root growth under alternative photoperiods served to demonstrate that temporal phenotyping performed by ChronoRoot allows deciphering the evolution of the traditional RSA parameters throughout time. Moreover, novel parameters emerged, including architectural features, oscillating growth speed, and other characteristics derived from spectral analysis of the growth signals in the Fourier domain. The combination between a low-cost automatic device for image acquisition and machine intelligence methods for image segmentation gave rise to a powerful tool for root phenomics potentially applicable to natural variation studies, the characterization of root-related subtle disorders, and screening for clock-associated mutants.</p>
    <fig position="float" id="fig1">
      <label>Figure 1</label>
      <caption>
        <p>Main components of ChronoRoot. (1) Open hardware specification (see <xref rid="sup13" ref-type="supplementary-material">Supplementary Figures S3-S7</xref> for a detailed description). (2) 3D-printed ChronoRoot system mounted in a plant growth chamber. (3) Temporal sequence of images acquired by the system are provided as input to the CNN-based segmentation module. The diagram corresponding to the proposed CNN architecture is included in the center of the figure. (4) The deep learning model produces dense segmentation maps for all the plants, which are enhanced taking into account the temporal consistency of the results. (5) Independent plants can be selected to be processed individually. (6,7) The roots are skeletonized and a graph is constructed by traversing the skeleton. Pixels in the skeleton are identified as belonging to the main root (green) or lateral root (blue). The graph nodes are labelled as being the main root seed and tip (green), lateral root tip (red), or bifurcation (yellow).</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052fig1" position="float"/>
    </fig>
  </sec>
  <sec sec-type="data" id="sec2">
    <title>Data Description</title>
    <sec id="sec2-1">
      <title>Plant material and growth conditions</title>
      <p><italic toggle="yes">Arabidopsis thaliana</italic> ecotype Col-0 seeds were surface sterilized and stratified at 4°C for 2 d before being grown under long day conditions (16 h light, 140 μE m<sup>−2</sup> s<sup>−1</sup>/8 h dark) or continuous light (24 h light, 140 μE m<sup>−2</sup> s<sup>−1</sup>) at 22°C, on half-strength Murashige and Skoog media (1/2 MS) (Duchefa, Haarlem, Netherlands) with 0.8% plant agar (Duchefa, Haarlem, Netherlands). Four seeds were used per plate. All the experiments where performed under laboratory conditions according to the local institutional guidelines.</p>
    </sec>
    <sec id="sec2-2">
      <title>Datasets</title>
      <p>We generated 2 different datasets in this work: the first one was used to train and evaluate the segmentation performance of the CNN models, while the second one served as an exploratory use case, to assess root growth under alternative photoperiods and provide an example of the novel temporal phenotypical parameters that can be extracted with ChronoRoot. Note that all these images were obtained with the proposed 3D-printed hardware, and both are available to encourage reproducible research. Importantly, when splitting the training, validation, and test partitions, we were careful not to include images corresponding to the same video on different partitions, to avoid overoptimistic biased evaluations.</p>
      <list list-type="bullet">
        <list-item>
          <p>Dataset used to train and validate the deep learning models for root segmentation: The dataset used for training consisted of 331 images from 55 videos (on average 6 images from the same plate at different states of growth); 11 of those videos were annotated by an expert biologist. The dataset used for testing consisted of 55 images from 11 different videos, all annotated by the same expert. The tool used for the manual annotation was ITK-SNAP [<xref rid="bib5" ref-type="bibr">5</xref>]. In total, 240 plants distributed over the 66 videos were used for training/testing the methods.</p>
        </list-item>
        <list-item>
          <p>Use case dataset for plant phenotyping under alternative photoperiods: We used 12 videos for each photoperiod, with pictures taken every 15 minutes. We took the first 17 days (1,632 frames), and after processing the videos we proceeded to discard the results from the first 3 days prior to seed germination. We selected 25 plants from each photoperiod to perform the temporal growth analyses.</p>
        </list-item>
      </list>
    </sec>
  </sec>
  <sec sec-type="analyses" id="sec3">
    <title>Analyses</title>
    <p>We designed an automatic method to perform RSA delineation in temporal image sequences of plant roots. Our framework takes a sequence of images as input and outputs a labelled graph for each frame, representing the current root growing state. Graphs are powerful data structures particularly useful to represent curvilinear shapes like plant roots (details on the graph generation process are provided in the Methods section). The main module of the RSA delineation method is a deep CNN that produces a dense segmentation mask, where every pixel is classified as belonging to the root or the background. We proposed different CNN architectures for this task (described in the Methods section) and compared their performance with state-of-the-art models using manual annotations produced by expert biologists. We measured 3 different metrics: (i) Dice coefficient quantifies the overlapping between the prediction and the ground-truth, (ii) Haussdorf distance indicates the maximum distance between them, and (iii) the recall (or sensitivity) refers to the fraction of root pixels retrieved over the total amount of root pixels. Quantitative results are included in Table <xref rid="tbl1" ref-type="table">1</xref>. On the basis of these results, we chose 2 models, depending on whether we aim at having a faster or more accurate method:</p>
    <list list-type="bullet">
      <list-item>
        <p>Fast method: The fastest models are the proposed UNet [<xref rid="bib6" ref-type="bibr">6</xref>] variants, requiring up to half a second to process a high-resolution image using a standard GPU. These models have lower parameter complexity compared to state-of-the-art architectures like SegNet [<xref rid="bib7" ref-type="bibr">7</xref>] and DeepLab [<xref rid="bib8" ref-type="bibr">8</xref>], which explains the lower running time. Among the fast UNet models, we observed that the proposed Deeply Supervised ResUNet (DSResUNet) shows a significantly lower value for Hausdorff distance, while keeping equivalently good Dice and Recall. The proposed DSResUNet architecture (depicted in Fig. <xref rid="fig1" ref-type="fig">1</xref>) combines residual blocks [<xref rid="bib9" ref-type="bibr">9</xref>] with deep supervision [<xref rid="bib10" ref-type="bibr">10</xref>], improving the results of a standard UNet with a minimum increase in model complexity.</p>
      </list-item>
      <list-item>
        <p>Accurate method: We proposed to combine all the implemented architectures into a single ensemble method, increasing model diversity by creating an ensemble of multiple models and architectures [<xref rid="bib11" ref-type="bibr">11</xref>]. This ensemble of deep models increased the running time by a factor of 9 but achieved the best performance across all metrics, outperforming state-of-the-art models like SegNet and DeepLab.</p>
      </list-item>
    </list>
    <table-wrap position="float" id="tbl1">
      <label>Table 1.</label>
      <caption>
        <p>Quantitative evaluation for the different CNN architectures compared in this work</p>
      </caption>
      <table frame="hsides" rules="groups">
        <thead>
          <tr>
            <th rowspan="1" colspan="1"/>
            <th colspan="2" align="center" rowspan="1">Dice, mean ± SD</th>
            <th colspan="2" align="center" rowspan="1">Recall, mean ± SD</th>
            <th colspan="2" align="center" rowspan="1">Hausdorff distance, mean ± SD (mm)</th>
            <th rowspan="1" colspan="1"/>
            <th rowspan="2" colspan="1">No. Parameters</th>
          </tr>
          <tr>
            <th align="left" rowspan="1" colspan="1">Model</th>
            <th align="center" rowspan="1" colspan="1">Threshhold</th>
            <th align="center" rowspan="1" colspan="1">CRF</th>
            <th align="center" rowspan="1" colspan="1">Threshhold</th>
            <th align="center" rowspan="1" colspan="1">CRF</th>
            <th align="center" rowspan="1" colspan="1">Threshhold</th>
            <th align="center" rowspan="1" colspan="1">CRF</th>
            <th align="center" rowspan="1" colspan="1">Time (S)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td rowspan="1" colspan="1">UNet</td>
            <td rowspan="1" colspan="1">0.769 ± 0.048</td>
            <td rowspan="1" colspan="1">0.774 ± 0.044</td>
            <td rowspan="1" colspan="1">0.871 ± 0.044</td>
            <td rowspan="1" colspan="1">0.830 ± 0.056</td>
            <td rowspan="1" colspan="1">10.25 ± 7.45</td>
            <td rowspan="1" colspan="1">9.39 ± 7.94</td>
            <td rowspan="1" colspan="1">0.29</td>
            <td rowspan="1" colspan="1">488.212</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">ResUNet</td>
            <td rowspan="1" colspan="1">0.768 ± 0.050</td>
            <td rowspan="1" colspan="1">0.770 ± 0.047</td>
            <td rowspan="1" colspan="1">0.862 ± 0.046</td>
            <td rowspan="1" colspan="1">0.823 ± 0.057</td>
            <td rowspan="1" colspan="1">8.83 ± 6.71</td>
            <td rowspan="1" colspan="1">7.53 ± 5.91</td>
            <td rowspan="1" colspan="1">0.33</td>
            <td rowspan="1" colspan="1">505.046</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Deeply Supervised ResUNet (our)</td>
            <td rowspan="1" colspan="1">0.769 ± 0.048</td>
            <td rowspan="1" colspan="1">0.772 ± 0.045</td>
            <td rowspan="1" colspan="1">0.861 ± 0.044</td>
            <td rowspan="1" colspan="1">0.815 ± 0.057</td>
            <td rowspan="1" colspan="1">8.14 ± 7.34</td>
            <td rowspan="1" colspan="1">6.95 ± 5.42</td>
            <td rowspan="1" colspan="1">0.49</td>
            <td rowspan="1" colspan="1">532.336</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">SegNet</td>
            <td rowspan="1" colspan="1">0.768 ± 0.043</td>
            <td rowspan="1" colspan="1">0.773 ± 0.040</td>
            <td rowspan="1" colspan="1">0.862 ± 0.044</td>
            <td rowspan="1" colspan="1">0.824 ± 0.053</td>
            <td rowspan="1" colspan="1">7.42 ± 6.40</td>
            <td rowspan="1" colspan="1">6.81 ± 5.65</td>
            <td rowspan="1" colspan="1">1.49</td>
            <td rowspan="1" colspan="1">29.460.450</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">DeepLab</td>
            <td rowspan="1" colspan="1">0.666 ± 0.055</td>
            <td rowspan="1" colspan="1">0.609 ± 0.079</td>
            <td rowspan="1" colspan="1">0.763 ± 0.077</td>
            <td rowspan="1" colspan="1">0.600 ± 0.113</td>
            <td rowspan="1" colspan="1">7.58 ± 7.79</td>
            <td rowspan="1" colspan="1">7.56 ± 7.52</td>
            <td rowspan="1" colspan="1">1.86</td>
            <td rowspan="1" colspan="1">58.009.410</td>
          </tr>
          <tr>
            <td rowspan="1" colspan="1">Ensamble (our)</td>
            <td rowspan="1" colspan="1">0.772 ± 0.048</td>
            <td rowspan="1" colspan="1">0.774 ± 0.044</td>
            <td rowspan="1" colspan="1">0.864 ± 0.044</td>
            <td rowspan="1" colspan="1">0.804 ± 0.061</td>
            <td rowspan="1" colspan="1">6.68 ± 5.08</td>
            <td rowspan="1" colspan="1">6.45 ± 4.98</td>
            <td rowspan="1" colspan="1">4.5</td>
            <td rowspan="1" colspan="1"/>
          </tr>
        </tbody>
      </table>
      <table-wrap-foot>
        <fn id="req-162612250602453290">
          <p>We measured the Dice coefficient, recall, and Hausdorff distance for dense root segmentation task. We compared state-of-the-art models (including UNet, ResUNet, SegNet, and DeepLab) and compared with the proposed Deeply Supervised ResUNet and the ensemble of multiple models and architectures. On the one side, we found that our Ensemble of Multiple Models and Architectures produced equal or more accurate results than the rest of the models in terms of Dice and Haussdorf, at the expense of increasing the processing time. On the other side, the proposed Deeply Supervised ResUNet is fast (less than half a second) and shows a significantly lower value for Hausdorff distance than the other fast models, while keeping equivalently good Dice and Recall.</p>
        </fn>
      </table-wrap-foot>
    </table-wrap>
    <p>ChronoRoot implements both fast and accurate variants, giving the users the opportunity to decide according to their requirements. In this study, we used the fast method based on the proposed Deeply Supervised ResUNet model, which offered a good trade-off between running time and accuracy. We apply several post-processing steps after segmentation, which are independent of the CNN model. We first apply a conditional random field (CRF) [<xref rid="bib12" ref-type="bibr">12</xref>,<xref rid="bib13" ref-type="bibr">13</xref>] model to improve the homogeneity of the labels assigned to neighbouring pixels. Then, we enhance the temporal consistency of the segmentations by considering its weighted average. These steps serve to remove spurious segmentations by analysing a temporal sequence of images, which ultimately translates into generating more stable phenotypic measurements. A graph structure is then constructed where every node is assigned a class label indicating whether it is associated with the plant seed, MR, RL, bifurcation, or root tip. Temporal consistency on the graph structures is finally improved by tracking the labelled nodes and solving conflicting cases. A more detailed description of these steps can be found in the Methods section. After the graph generation process, we proceed to extract phenotypic features for RSA characterization.</p>
    <sec id="sec3-1">
      <title>Temporal dimension of traditional and novel RSA parameters</title>
      <p>We analysed temporal sequences of plant roots growing under different conditions. To assess the potential of ChronoRoot, we decided to compare the RSA of <italic toggle="yes">A. thaliana</italic> ecotype Col-0 grown under 2 distinctive photoperiods, i.e., long day (LD; 16 h of light, 8 h of dark) or continuous light (CL; 24 h of light). Light availability and photosynthesis in the shoot determine the amount of sugar transported to the roots, thus modulating underground plant growth. Moreover, ample evidence suggests that root developmental plasticity depends on the light environment, involving a more sophisticated impact on endogenous signaling pathways [<xref rid="bib14" ref-type="bibr">14</xref>].</p>
      <p>Traditional parameterization of RSA expanded to temporal dynamics revealed the progression of root growth under CL and LD conditions. A representation of root automatic segmentation is shown in Fig. <xref rid="fig1" ref-type="fig">1</xref> (see Methods and Fig. <xref rid="fig6" ref-type="fig">2</xref> for more details). Our experiments show that MR length, the sum of LR length, and the resulting total root system (TR) begin to differ between conditions at ∼200–250 h (8–10 days) after germination (Fig. <xref rid="fig2" ref-type="fig">3A</xref>–<xref rid="fig2" ref-type="fig">C</xref>), together with LR number (Fig. <xref rid="fig2" ref-type="fig">3D</xref>). Notably, root growth was not only faster under CL but also resulted in a different RSA, exhibiting a higher density of LRs and a lower component of the MR over the total root system (Fig. <xref rid="fig2" ref-type="fig">3E</xref> and <xref rid="fig2" ref-type="fig">F</xref>). Notably, between 250 h (10 days) and the end of the experiment (336 h, 14 days), the contrast between both photoperiods increased gradually in every measured parameter, hinting at a temporal reorganization of root development under different light conditions.</p>
      <fig position="float" id="fig6">
        <label>Figure 2</label>
        <caption>
          <p>Qualitative segmentation results obtained with the benchmarked methods. We observe how the ensemble and the ensemble with temporal consistency improve the quality of the results, especially in areas with root occlusion.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052fig6" position="float"/>
      </fig>
      <fig position="float" id="fig2">
        <label>Figure 3</label>
        <caption>
          <p>Traditional RSA parameters expanded to the temporal dimension. (A) Main root (MR) length. (B) The sum of all lateral root (LR) lengths. (C) Total root (TR) length, representing the sum of LR total length and MR length. (D) Number of LRs. (E) LR density, expressed as LR number/MR length. (F) MR component of the RSA, expressed as MR length/total root (TR) length, which is the sum of MR and LRs. Data are shown for plants grown under continuous light and long day. The lines indicate the mean, and the shadows represent the standard deviation (SD) throughout the experiment.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052fig2" position="float"/>
      </fig>
      <p>Based on the information derived from temporal phenotyping, we explored in more detail the reconfiguration of RSA under alternative photoperiods. We identified the time point at which the sum of LR lengths equals the MR length as a novel parameter of RSA dynamics (Fig. <xref rid="fig3" ref-type="fig">4A</xref>). However, no significant difference was observed in the distribution of individual time-length points of plants grown under CL or under LD (Fig. <xref rid="fig3" ref-type="fig">4B</xref>). The analysis of the relationship between the MR and LRs along time, determined by the difference between both measurements (MR − LRs) aligned to the time point at which MR and LRs are of the same length (time 0), is shown in Fig. <xref rid="fig3" ref-type="fig">4C</xref>. It reflects that the difference between MR and LRs tends to have a significantly larger absolute value for plants grown under CL than under LD. Moreover, we extracted different indicators to analyse the dynamics of these curves. Figure <xref rid="fig3" ref-type="fig">4D</xref> shows the approximate derivative (computed by means of finite differences) at the time point at which MR and LRs are of the same lengths (time 0). These differences are not statistically significant according to a Mann Withney <italic toggle="yes">U</italic>-test. However, when extending the analysis to the full ±24 h range by fitting a linear function to every curve from Fig. <xref rid="fig3" ref-type="fig">4C</xref> and plotting the corresponding slopes (Fig.  E), we found strong differences in the distribution (statistically significant according to a Mann-Whitney <italic toggle="yes">U</italic>-test) for both photoperiods. This novel time-related parameter reflects the dynamics of root growth by determining how long it takes for the system to be composed of more LRs than the MR.</p>
      <fig position="float" id="fig3">
        <label>Figure 4</label>
        <caption>
          <p>Novel RSA parameters analysed along time. (A) Example of 1 individual main root (MR) and the sum of lateral root (LR) length along time, revealing the time point of the intersection between the 2 curves. (B) Distribution of the intersection points for all individuals from both conditions (continuous light [CL] and long day [LD]). At the top and on the right, the distributions of both populations are represented. For this experiment, no significant difference was observed with respect to the intersection time point. (C) The difference between the MR length and total LRs length at each time point was calculated and aligned around the point of MR − LRs = 0 mm for each individual considering ±24 h. (D) Distribution of the derivative value for the individual curves from panel C at time of equal length of MR and total LRs. The difference between the means is not statistically significant according to a non-parametric Mann-Whitney <italic toggle="yes">U</italic>-test (<italic toggle="yes">P</italic> &gt; 0.05). (E) For every individual, the tendency of the MR − LR curves shown in panel C was determined by fitting a linear function to every curve, considering ±24 h. The boxplot shows the distribution of the slope of the fitted curves, revealing a clear difference between LC and CL. Difference between the means is statistically significant according to a non-parametric Mann-Whitney <italic toggle="yes">U</italic>-test (<italic toggle="yes">P</italic> &lt; 0.05). In the boxplots every dot is an individual and green triangles represent the mean. The *indicate the difference between means is statistically significant acording to Mann-Whitney <italic toggle="yes">U</italic>-test (<italic toggle="yes">P</italic> &lt; 0.05).</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052fig3" position="float"/>
      </fig>
      <p>To assess the effect of RSA reconfiguration on the area explored by roots under distinct photoperiods, we calculated the dynamic convex hull for each subset of plants. Interestingly, the observation of the convex hull resulting from the overlap of all individuals grown in the corresponding conditions reveals an extended high density of LRs along the MR axis at the end of the experiment under CL (14 days after germination, Fig. <xref rid="fig4" ref-type="fig">5A</xref>–<xref rid="fig4" ref-type="fig">C</xref>). Notably, the area of the average convex hulls between 8 and 14 days does not differ between CL and LD conditions (Fig. <xref rid="fig4" ref-type="fig">5D</xref>). Nonetheless, the quantification of the sum of LRs length over the convex hull area indicates that the density of LRs is higher under CL between 10 and 14 days (Fig. <xref rid="fig4" ref-type="fig">5E</xref>). Collectively, our analyses indicate that global LR length increases under CL as a result of more numerous LRs growing simultaneously, although the area explored by the RSA does not differ between the 2 photoperiods. Thus, the global density of the resulting RSA is higher under CL.</p>
      <fig position="float" id="fig4">
        <label>Figure 5</label>
        <caption>
          <p>Area and density of the RSA analysed along time. (A) Overlapped segmentations of the whole root system at 14 days after germination. Blue background indicates no roots. The brightness of the signal increases as more roots occupy the same position. (B) Same as A, represented as a heat map of the convex hull extracted for each individual. (C) Overlapped convex hull contours for each condition. (D) Average convex hull area for different time points under CL or LD, represented as violin plots. The mean is indicated as a green point. (E) LR density calculated as the sum of LR length over the area included in the respective convex hull. The distribution of each population for the corresponding time points is shown as violin plots. The green points indicate the mean. *Statistically significant. We used Shapiro-Wilk test to assess Gaussianity, Levene test to confirm equal variances and <italic toggle="yes">t</italic>-test to confirm that differences between the means of both populations are statistically significant (<italic toggle="yes">P</italic> &lt; 0.01).</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052fig4" position="float"/>
      </fig>
    </sec>
    <sec id="sec3-2">
      <title>Novel speed-based parameters derived from temporal phenotyping</title>
      <p>The information derived from the temporal dimension of traditional and novel RSA parameters indicated that the difference in root growth rate became broader throughout time under CL vs LD. It has been shown that the <italic toggle="yes">A. thaliana</italic> MR exhibits an oscillating growth, which likely depends on the lunisolar tide [<xref rid="bib14" ref-type="bibr">14</xref>,<xref rid="bib15" ref-type="bibr">15</xref>] and light-associated carbon partitioning [<xref rid="bib16" ref-type="bibr">16</xref>]. Therefore, based on the segmentations obtained with our deep learning models, we calculated the growth speed throughout the experiment in both conditions, showcasing how novel speed-based parameters can be derived via ChronoRoot. MR speed increased steadily until ∼150 h under LD and 200 h under CL after germination, and the average maximum speed reached in CL was higher than in LD (Fig. <xref rid="fig5" ref-type="fig">6A</xref>). Strikingly, the difference in the growth speed of the global root system (TR) between the 2 conditions became increasingly larger since the moment when the speed of the MR was stabilized (Fig. <xref rid="fig5" ref-type="fig">6B</xref>), hinting at a different acceleration rate between conditions. The observed root growth dynamics further supports the increasing relevance of LRs as a main component of RSA throughout time.</p>
      <fig position="float" id="fig5">
        <label>Figure 6</label>
        <caption>
          <p>Novel time-derived parameters of RSA. (A) Main root (MR) and (B) total root (TR) growth speed along time. (C) Fast Fourier transform of the growth speed signal of MR. The largest energy differences are indicated in the graph. (D) The post-processed (high-pass followed by normalization) MR growth speed showing a 7-day window centered on Day 3. The sine curves corresponding to the frequency 1/12 (in black) and 1/24 (in red) found in C are indicated at the top. Note the correlation between the LD growth speed oscillation and the 2 components 1/12 and 1/24. (E) The energy at 1/24 frequency calculated in a 7-day-window centered at consecutive time points for MR. (F) The energy at 1/12 frequency calculated in a 7-day window centered at consecutive time points for MR.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052fig5" position="float"/>
      </fig>
      <p>Notably, the analyses of growth speed uncovered an oscillating behavior in both conditions (Fig. <xref rid="fig5" ref-type="fig">6A</xref> and <xref rid="fig5" ref-type="fig">B</xref>). To better understand the different growth patterns exhibited by LD and CL conditions, we performed a Fourier decomposition of the growth speed signals. Fourier transform decomposes functions depending on time into functions depending on frequency. In other words, the Fourier transform of a given function describes how much of any given frequency is present in the original signal. When comparing growth speed signals, analysing their Fourier spectrum helps us to see how much this signal correlates with particular oscillation frequencies. For example, if high Fourier coefficients are associated with the frequency 1/24 h, it means that the plant growth speed follows a daily oscillation (corresponding to what is known as circadian rhythm). Differences in the Fourier coefficients at a given frequency between growth conditions would indicate an alteration in the oscillatory pattern of plant growth. A fast Fourier transformation (FFT) of the signal of MR growth speed in CL vs LD revealed a major energy difference in the components corresponding to the frequencies of 1/24 h and 1/12 h, respectively (Fig. <xref rid="fig5" ref-type="fig">6C</xref>). Remarkably, these 2 components distinguish circadian and ultradian rhythms displayed by plants grown in LD, with a pronounced local minimum of the growth speed at 1/24 h and a minor local minimum at 1/12 h (Fig. <xref rid="fig5" ref-type="fig">6D</xref>). Strikingly, the most pronounced differences revealed by FFT (Fig. <xref rid="fig5" ref-type="fig">6C</xref>) served to uncover a root growth clock-related disorder experienced under CL, coinciding with a blurred daily oscillation of growth speed, in comparison with the corresponding sine curves (Fig. <xref rid="fig5" ref-type="fig">6D</xref>; a detailed comparison of MR, LR, and TR growth speed analyses is shown <xref rid="sup13" ref-type="supplementary-material">Supplementary Fig. 1</xref>). Although an oscillating behavior can be observed under CL towards the end of the experiment (Fig. <xref rid="fig5" ref-type="fig">6A</xref> and <xref rid="fig5" ref-type="fig">B</xref>), the energy at 1/24 and 1/12 frequencies was higher under an LD photoperiod throughout the complete time lapse analysed (Fig. <xref rid="fig5" ref-type="fig">6E</xref> and <xref rid="fig5" ref-type="fig">F</xref>). Notably, the difference between conditions of the TR oscillating speed of growth is mainly due to the MR contribution (<xref rid="sup13" ref-type="supplementary-material">Supplementary Fig. S2</xref>). Altogether, our study of wild-type <italic toggle="yes">A. thaliana</italic> Col-0 plants growing under alternative photoperiods using ChronoRoot served to reveal novel temporal parameters of root development, notably including clock-related features depending on the light environment.</p>
    </sec>
    <sec id="sec3-3">
      <title>3D-printed device for temporal image acquisition</title>
      <p>The ChronoRoot device is an affordable and modular imaging system based on 3D-printed and laser-cut pieces and off-the-shelf electronics (Fig. <xref rid="fig1" ref-type="fig">1</xref>.1 and <xref rid="fig1" ref-type="fig">1</xref>.2). Each module consists of a Raspberry Pi (v3)-embedded computer controlling 4 fixed-zoom and fixed-focus cameras (RaspiCam v2), and an array of infrared (IR) LED back-light. In between each camera and the corresponding IR array, there is a vertical 12 × 12 cm plate for seedling growth, allowing automatic image acquisition repeatedly along the experiment without any modification or movement of the imaging set-up. The 4-plate module is small (62 × 36 × 20 cm) and can be placed in any growth chamber. The different parts of the imaging set-up (back-light, plate support, and camera) can be positioned along a horizontal double-rail to control the field of view of the camera and accurate lighting. In addition, the camera can be moved vertically. ChronoRoot allows image acquisition at a high temporal resolution (a set of pictures every minute). The use of an IR back-light (850 nm) and optional long-pass IR filters (&gt;830 nm) allow images of the same quality to be acquired independently from the light conditions required for the experiment, during day and night.</p>
      <p>Each module is connected to the network either by Wi-Fi or Ethernet cable. A web interface allows the control of the device offering live feed of the cameras for field of view and focus set-up. The user can program the activation of cameras and IR back-light, starting and ending dates, the time basis for picture acquisition, and finally follow the progression of the experiment. The pictures are saved directly on an external drive plugged on the Raspberry Pi. Once the experimental set-up is ready, each module is completely independent from the external environment and the access to the network (for more details see Methods, <xref rid="sup13" ref-type="supplementary-material">Supplementary Figs S3–S7</xref>, and the Supplementary 3D printing and laser cutting files).</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec4">
    <title>Discussion</title>
    <p>The plant phenotype can be defined as the integration of structural, physiological, and performance-related traits of a genotype in a given environment. Plant phenotyping is therefore the act of determining the quantitative or qualitative values of these traits [<xref rid="bib17" ref-type="bibr">17</xref>]. The advent of novel imaging technologies and image processing have revolutionized plant phenotyping, expanding the frontiers of phenotypic trait measurement. Plant roots have a major role in plant anchorage and resource acquisition while offering environmental benefits such as carbon sequestration and soil erosion mitigation [<xref rid="bib18" ref-type="bibr">18</xref>]. The growing knowledge linking genetics with functional properties of plant roots is of crucial interest to plant breeding, notably for the design of novel strategies for sustainable agriculture and environmental stewardship in the face of the impending climate change. Whereas high-throughput genotyping, sequencing-based genotyping, and genomic breeding are behind current agricultural practices in the era of omics technologies, the collection of phenotypic data for a thorough characterization of the RSA is increasingly becoming a limiting factor [<xref rid="bib19" ref-type="bibr">19</xref>]. Although significant advancements in the application of imaging sensors for high-throughput data collection have allowed comprehensive plant phenotyping [<xref rid="bib20" ref-type="bibr">20</xref>], the characterization of root traits has been hampered by the inaccessibility of the rhizosphere.</p>
    <p>Large and sophisticated phenotyping platforms are deployed worldwide and allow the simultaneous phenotyping of several hundreds of plants (see the International Plant Phenotyping Network [<xref rid="bib21" ref-type="bibr">21</xref>]). However, their big dimensions and high automatization reserve their implementation to specialized locations and mainly for large phenotyping experiments. Root imaging systems of intermediate complexity, like the one proposed in [<xref rid="bib22" ref-type="bibr">22</xref>], serve to address the temporal phenotyping of plant roots using vertical plates for plant growth in agarized medium. However, such a system still requires a single-axis mobile robot, which implies more expensive electronics and plant chamber space devoted to the equipment set-up. In contrast, low-cost ChronoRoot modules can be located easily in already existing facilities without major modifications or permanent movement. The number of modules to be built and used will only depend on the available space and the experimental design (e.g., a few modules for the characterization of given genotypes vs multiple units for genome-wide association studies [GWAS] approaches using tens to hundreds of plant accessions). It also allows the hardware set-up to be scaled according to available funding and experiment requirements progressively. In parallel, the advancement of the do-it-yourself movement has promoted the development of a growing number of low-cost phenotyping devices combining 3D-printed, laser-cut, captor, and microcontroller coming from open-source and open-hardware communities such as Arduino [<xref rid="bib23" ref-type="bibr">23</xref>] or Raspberry Pi [<xref rid="bib24" ref-type="bibr">24</xref>]. Successful inexpensive devices have allowed plant leaves to be monitored [<xref rid="bib25" ref-type="bibr">25</xref>], including Phenotiki, an affordable open software and hardware platform for image-based phenotyping of plant aerial organs [<xref rid="bib26" ref-type="bibr">26</xref>]. More recently, the Phenotiki sensor interface was used to characterize cotton RSA on soil-containing big Rhizoboxes, allowing the determination of basic architectural parameters [<xref rid="bib27" ref-type="bibr">27</xref>] (e.g., total root area, convex hull area, total root length). In contrast, ChronoRoot allows for a more fine-grained high-throughput temporal phenotyping, e.g., making it possible to distinguish between MR and LRs. Finally, modular rhizotrons of more sophisticated designs (including more expensive cameras and light flashes, precluding observations during the night) also served for RSA characterization of crops [<xref rid="bib28" ref-type="bibr">28</xref>].</p>
    <p>In the past few years, images of the root system from different plant species have been acquired manually using a flat-bed scanner or a camera positioned in front of vertical petri dishes. Root phenotyping is generally performed upon single-time-point images or using several images acquired during growth in time-lapse sequences. More recently, semi-automatic devices and softwares have also helped to increase the efficiency of image acquisition and associated analyses [<xref rid="bib29" ref-type="bibr">29–34</xref>]. The great need of throughput in screening experiments to uncover the genetic basis of root development justifies the use of simplified artificial culture conditions and standardized environments to make the RSA accessible to image acquisition [<xref rid="bib17" ref-type="bibr">17</xref>]. Software tools use input images of root systems grown under a variety of conditions, including hydroponic and aeroponic systems, agarized medium, paper pouches, or soil [<xref rid="bib32" ref-type="bibr">32</xref>,<xref rid="bib33" ref-type="bibr">33</xref>]. Here we propose to use vertical square Petri dishes for plant growth on the surface of transparent agarized medium, for automatic acquisition of photographs allowing a high-resolution temporal phenotyping of the RSA.</p>
    <p>According to Quantitative Plant [<xref rid="bib35" ref-type="bibr">35</xref>,<xref rid="bib36" ref-type="bibr">36</xref>], &gt;40 image processing software tools are available for root system analysis [<xref rid="bib33" ref-type="bibr">33</xref>,<xref rid="bib37" ref-type="bibr">37</xref>]. RSA parameters are extracted from various types of 2D images captured from agar plates or washed roots extracted from soil. Moreover, 3D RSA reconstruction is possible using X-ray computed tomography [<xref rid="bib38" ref-type="bibr">38</xref>] or magnetic resonance imaging [<xref rid="bib39" ref-type="bibr">39</xref>]. Nearly all the reported tools need human input to be operated and retrieve precise numbers. RootTrace [<xref rid="bib40" ref-type="bibr">40</xref>,<xref rid="bib41" ref-type="bibr">41</xref>], for example, which also focuses on high-throughput analyses of root growth, uses traditional image processing and tracking techniques, resulting in a program that can only extract MR length and count the number of emerged LRs. On the contrary, our model relies on deep networks producing a detailed segmentation of the RSA, which is then classified into MR and LR, allowing for fine-grained measurements such as the total length of the LRs, which is not provided by RootTrace. Other tools such as GiaRoots [<xref rid="bib34" ref-type="bibr">34</xref>] and EZ Rhizo [<xref rid="bib31" ref-type="bibr">31</xref>] use simple threshold strategies for root segmentation. In contrast to ChronoRoot, these tools fall short at handling segmentation problems emerging from drops due to water condensation, they require manual human calibration, and they do not take advantage of the redundancy provided by the temporal resolution of the high-throughput videos to filter out spurious segmentations. Another alternative tool is WinRHIZO [<xref rid="bib42" ref-type="bibr">42</xref>], a commercial and non-open-source tool designed to work with images captured with high-resolution desktop optical scanners. Such a requirement makes it virtually impossible to capture high-throughput temporal sequences of growing plants. On the contrary, ChronoRoot is open-source and designed to work with low-cost cameras. Another option is BRAT [<xref rid="bib43" ref-type="bibr">43</xref>], designed for high-throughput phenotyping of root growth and development. The main disadvantage of BRAT is that it can only handle early root growth and does not provide measurements for LRs.</p>
    <p>The previously discussed methods are mostly based on conventional image processing approaches and extract a limited number of RSA features. Advances in machine learning applied to image analysis allowed these limitations to be partially overcome. For example, deep learning techniques have been used to improve the consistency of classic approaches, enhancing the quality of root segmentations [<xref rid="bib44" ref-type="bibr">44</xref>,<xref rid="bib45" ref-type="bibr">45</xref>]. Closest to our work is the recent RootNav 2 [<xref rid="bib46" ref-type="bibr">46</xref>], which is also based on deep learning models and provides fine-grained metrics distinguishing between MR and LRs. However, RootNav 2 does not exploit the redundancy provided by the temporal resolution and follows a different architectural design, which makes ground truth annotations more difficult to obtain and prevents us from training the model with our dataset. Compared to ChronoRoot, RootNav 2 uses a more complex neural network architecture with 2 output paths: the first one is used to predict root segmentation masks (differentiating between MR and LRs) while the second one produces heat maps associated with root tips. This design choice requires the ground truth annotations to be composed of 3 parts: (i) MR pixel-level annotations, (ii) LR pixel-level annotations, and (iii) root tip annotations. Conversely, ChronoRoot just requires binary segmentation maps (background vs foreground root) for training because the MR and LR labelling is performed after segmentation following a depth-first search approach on the skeletonized binary segmentation. Thus, our dataset is just composed of images with foreground/background pixel-level annotations, which is not enough for training the RootNav 2 model.</p>
  </sec>
  <sec sec-type="potential" id="sec5">
    <title>Potential Implications</title>
    <p>ChronoRoot expands the possibilities for high-throughput root phenotyping, which is of major importance for natural variation and GWAS, as well as mutant characterization and screening. Notably, it has been shown that clock-related mutants exhibit a differential oscillating MR growth under alternative conditions [<xref rid="bib15" ref-type="bibr">15</xref>,<xref rid="bib16" ref-type="bibr">16</xref>,<xref rid="bib47" ref-type="bibr">47</xref>]. ChronoRoot offers an ideal platform for the identification of genotypes associated with altered clock traits, based on the analysis of spectral features extracted from temporal signals.</p>
    <p>Note that the specification of all ChronoRoot hardware components (e.g., camera) is released in this article. Thus, for anybody installing the system and using the same imaging set-up, our software for <italic toggle="yes">A. thaliana</italic> analysis should work without further retraining. In case the system fails owing to different lighting conditions or hardware components, a minimal fine-tuning of the model by using a few annotated images from the new set-up may be required.</p>
  </sec>
  <sec sec-type="methods" id="sec6">
    <title>Methods</title>
    <sec id="sec6-1">
      <title>Hardware description</title>
      <p>An automated imaging set-up was designed and built in the shape of an independent module of 62 × 36 × 20 cm (Fig. <xref rid="fig1" ref-type="fig">1</xref>). It is aimed at imaging ≤4 vertical plates either in color or in near-infrared (NIR) lightning. Each module consists of a single board computer (Raspberry Pi) controlling 4 cameras through a multiplexer module and an array of NIR LED illumination through a relay. The main support of each module is a 620 × 36 × 5 mm acrylic sheet cut using a laser cutter to allow to screw the different parts or let pass strips connecting the camera to the camera multiplexer. Several 3D pieces were designed and printed to place the different components of the module. Each module is separated in 4 subparts, each of them along a double aluminum axis. This axis allows adjustment of the distance of the different parts of the imaging set-up: NIR illumination, plate support, camera. The underpart of the module was used to fix the LED AC/DC adaptor, the relay, and the computer. The supports under the platform raise and stabilize the module. <xref rid="sup13" ref-type="supplementary-material">Supplementary File 1</xref> includes a full description of the components and the steps for the assembly of the device. The 3D-printing and laser-cut plans are available online [<xref rid="bib61_1626128066218" ref-type="bibr">48</xref>] under the CERN Open Hardware Licence Version 2.</p>
    </sec>
    <sec id="sec6-2">
      <title>Computational methods</title>
      <p>We evaluated different state-of-the-art architectures for image segmentation, and proposed new variants that achieved a good compromise between processing time, model complexity, and accuracy, as discussed in the Results section. This segmentation module is followed by several post-processing stages including a CRF post-processing to enhance label homogenity, a temporal consistency refinement step, skeletonization, graph construction, and node tracking. ChronoRoot outputs a labelled graph per image indicating which nodes correspond to the seed, MR, LRs, bifurcations, and the root tips. For each time step, the complete RSA is saved following the RSML format [<xref rid="bib48" ref-type="bibr">49</xref>].</p>
    </sec>
    <sec id="sec6-3">
      <title>Deep learning models for root segmentation</title>
      <p>CNNs are representation learning methods with multiple abstraction levels, which compose simple but nonlinear modules transforming representations at one level into a representation at a higher, slightly more abstract level [<xref rid="bib49" ref-type="bibr">50</xref>]. These models are specially suited for computer vision tasks, in particular for image segmentation [<xref rid="bib50" ref-type="bibr">51</xref>]. We explored 6 different convolutional neural network architectures to perform plant root segmentation. Four of them are state-of-the art existing architectures, while the other 2 were proposed in this work. In what follows, we first present a brief description of the state-of-the-art architectures (namely, the UNet [<xref rid="bib6" ref-type="bibr">6</xref>], ResUNet [<xref rid="bib51" ref-type="bibr">52</xref>], SegNet [<xref rid="bib7" ref-type="bibr">7</xref>], and DeepLab [<xref rid="bib8" ref-type="bibr">8</xref>]) and then discuss the 2 models proposed in this work.</p>
      <sec id="sec6-3-1">
        <title>UNet</title>
        <p>The first model is a modified lightweight version of the standard UNet [<xref rid="bib6" ref-type="bibr">6</xref>], which uses a fully convolutional encoder-decoder architecture and produces a dense segmentation map at the pixel level. Based on the original UNet model, we implemented a lightweight version reducing by 4 the number of feature maps per convolutional layer. Skip connections were implemented via summations of the signals in the up-sampling part of the network, instead of the concatenation used in the original version. We also replaced the max-pooling layers with avg-pooling and used an exponential linear unit (ELU) as non-linearity instead of a rectified linear unit (RELU). See <xref rid="sup13" ref-type="supplementary-material">Supplementary Table 1</xref> for a detailed description of the implemented architecture.</p>
      </sec>
      <sec id="sec6-3-2">
        <title>ResUNet</title>
        <p>For the second model (ResUNet), we replaced the convolutional layers in the aforementioned UNet architecture by residual blocks [<xref rid="bib9" ref-type="bibr">9</xref>]. Residual blocks help to prevent the degradation problem that occurs in very deep neural networks by learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. Recent works suggest that residual blocks are effective at segmenting tubular structures like plant roots or roads in a map [<xref rid="bib51" ref-type="bibr">52</xref>]. See <xref rid="sup13" ref-type="supplementary-material">Supplementary Table 2</xref> for a detailed description of the implemented architecture.</p>
      </sec>
      <sec id="sec6-3-3">
        <title>SegNet</title>
        <p>The SegNet architecture [<xref rid="bib7" ref-type="bibr">7</xref>] is a fully convolutional encoder-decoder neural network, widely adopted by the computer vision community to perform dense image segmentation. The architecture of the encoder is identical to the first 13 layers of VGG-16 [<xref rid="bib52" ref-type="bibr">53</xref>], and the role of the decoder network is to map the low-resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. Differently from the UNet where skip connections are used to propagate the complete feature maps from the encoder to the decoder, the upsampling in the decoder part of the SegNet model uses the memorized max-pooling indices from the corresponding encoder level. Our implementation was based on a publicly available model [<xref rid="bib53" ref-type="bibr">54</xref>].</p>
      </sec>
      <sec id="sec6-3-4">
        <title>DeepLab v3</title>
        <p>The DeepLab V3 model [<xref rid="bib8" ref-type="bibr">8</xref>] follows a different approach to generate dense segmentation maps. Differently from the previous models, which use skip connections (UNet) or memorized max-pooling indices (SegNet), this model uses atrous convolutions with upsampled filters to extract dense feature maps and capture long-range context.</p>
      </sec>
    </sec>
    <sec id="h2content1626128516356">
      <title>Proposed models</title>
      <p>On top of these state-of-the-art architectures, we propose 2 different CNN models. In the first model, named DSResUNet, we aimed at improving the segmentation accuracy while keeping at the same time a fast lightweight model. In the second case, we focused on increasing the robustness and boosting the accuracy of the segmentation method, at the expense of a more complex model that follows the principle of ensemble learning.</p>
      <sec id="sec6-3-5">
        <title>DSResUNet</title>
        <p>Taking the ResUNet as a baseline model, we propose here a new architecture that combines residual connections and deep supervision [<xref rid="bib10" ref-type="bibr">10</xref>] to improve the accuracy of the results. Deep supervision integrates additional loss terms, which are computed using feature maps from the intermediate CNN layers, instead of the last one only. We concatenated the ResUNet output with the original input image and processed these feature maps with 2 additional convolutional layers. This resulted in a cascade of 2 networks that are trained jointly, where the first one produces an initial segmentation map that is then refined by the second part of the network. We computed 2 loss terms, one after the output of the standard ResUNet and another one after the additional convolutions. The sum of both terms constitutes the loss function used to train the DSResUNet model. See Fig. <xref rid="fig1" ref-type="fig">1</xref> for a graphical illustration of the architecture and <xref rid="sup13" ref-type="supplementary-material">Supplementary Table 2</xref> for a detailed description.</p>
      </sec>
      <sec id="sec6-3-6">
        <title>Ensemble</title>
        <p>Our final segmentation method is an ensemble model. The idea of ensembling is that we can create higher performing models by combining multiple predictors using an aggregation function. One of the most common strategies to implement ensemble models is bagging [<xref rid="bib54" ref-type="bibr">55</xref>], where the same classifier is trained multiple times using different samples of the training set, and the final output is obtained as the average of the independent predictions. In this work, we followed a different principle that had been successfully applied in the context of medical image segmentation, where instead of combining several instances of the same model trained with different training samples, we combined different models and architectures trained with the same datasets [<xref rid="bib11" ref-type="bibr">11</xref>]. The idea is to average out the bias infused by individual model configurations, to approximate more reliably the true posterior distribution. In the context of image segmentation, given a dataset <italic toggle="yes">L</italic> = (<italic toggle="yes">x, y</italic>)<sub><italic toggle="yes">i</italic></sub>, where <italic toggle="yes">x</italic> is an intensity image and <italic toggle="yes">y</italic> the corresponding ground-truth segmentation, we aim at learning the underlying conditional distribution <italic toggle="yes">P</italic>(<italic toggle="yes">y</italic>|<italic toggle="yes">x</italic>), which maps input images <italic toggle="yes">x</italic> into segmentation maps <italic toggle="yes">y</italic>. This is commonly approximated by a model <italic toggle="yes">P</italic>(<italic toggle="yes">y</italic>|<italic toggle="yes">x</italic>; θ<sub><italic toggle="yes">m</italic></sub>), which has trainable parameters, determined in our case by the neural network architecture. These parameters were learnt so that they minimize a particular loss function (see next section for more details in the loss functions used in our work) using the dataset. Given different architectures, we obtained independent estimates and combined them following [<xref rid="bib11" ref-type="bibr">11</xref>], approximating the posterior <italic toggle="yes">P</italic>(<italic toggle="yes">y</italic>|<italic toggle="yes">x</italic>) as:
<disp-formula id="update1626208041010"><label>(1)</label><tex-math id="M1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
}{}$$\begin{equation*}
P(y|x) \approx \frac{1}{M} \sum \nolimits^{M}_{m} P(y|x,\theta _{m}).
\end{equation*}$$\end{document}</tex-math></disp-formula></p>
        <p>We implemented this ensemble of multiple models and architectures by averaging the predictions of the 5 previous models (UNet, ResUNet, DSResUNet, SegNet, and DeepLab v3), obtaining a more robust and accurate segmentation method that significantly outperforms the independent instances.</p>
      </sec>
      <sec id="sec6-3-7">
        <title>Training details</title>
        <p>All the CNN models were trained using binary cross-entropy as the loss function, Adam optimizer with default parameters, learning rate of 0.0001, and weight decay = 1e−8 for UNet-like models, 1e−9 for DeepLab, and 1e−10 for SegNet. The hyperparameters were chosen by grid search using the validation data. All models were implemented in TensorFlow 1, and the source code is publicly available. The training was done on a standard workstation with Intel® Core™ i7-8700 CPU, 64 GB RAM, and a NVIDIA Titan X GPU.</p>
        <p>Because we are dealing with a relatively small dataset, data augmentation was crucial to achieve good segmentation performance. We implemented online data augmentation through a variety of patch-based augmentation procedures including addition of Gaussian noise, random Gamma corrections to simulate different lighting conditions, artificial blur, and horizontal flipping. These transformations were applied to both the images and their corresponding ground-truth segmentation masks. The proposed architectures are all fully convolutional, enabling a patch-based training procedure. Because this is a highly unbalanced problem (we have fewer pixels corresponding to root class than background) we implemented the following patch sampling strategy: we sampled patches from random positions centered in root pixels with the same probability as patches centered in background pixels. After performing a grid search of hyperparameters, the size of the training patches was set to 256 × 256 and we used batches of 8 patches. At test time, we worked with the full-resolution images, which can be fed to the network and processed by the fully convolutional architectures.</p>
      </sec>
      <sec id="sec6-3-8">
        <title>CRF Post-processing</title>
        <p>The CNN segmentations are post-processed using a standard fully connected CRF [<xref rid="bib55" ref-type="bibr">56</xref>]. The CRF operates under the hypothesis that pixels that are contiguous and have similar intensity values should be assigned the same label. We used an efficient publicly available implementation [<xref rid="bib56" ref-type="bibr">57</xref>] of a dense CRF [<xref rid="bib13" ref-type="bibr">13</xref>] with Potts compatibility function and hand-tuned parameters θ<sub>α</sub> = 5 and θ<sub>β</sub> = 3.</p>
      </sec>
    </sec>
    <sec id="sec6-4">
      <title>Graph generation and temporal consistency improvement</title>
      <p>The CNN output can be interpreted as a soft segmentation. Because we processed temporal sequences of growing plant roots, we applied a post-processing step to improve temporal consistency using a variation of the weighted trailing average. The current segmentation and an accumulation of the previous ones are averaged to avoid losing parts of the root due to droplets or other type of occlusion. Given the current segmentation <italic toggle="yes">s</italic><sup><italic toggle="yes">t</italic></sup> at time <italic toggle="yes">t</italic>, and the accumulated mask up to the previous time step <italic toggle="yes">a</italic><sup><italic toggle="yes">t</italic> − 1</sup>, we compute the current map <italic toggle="yes">a</italic><sup><italic toggle="yes">t</italic></sup> = <italic toggle="yes">s</italic><sup><italic toggle="yes">t</italic></sup> + α<italic toggle="yes">a</italic><sup><italic toggle="yes">t</italic> − 1</sup>. The weight α is chosen depending on the size of the time step (we used α = 0.9 in our experiments). The aim is to use the root segmentation masks obtained in previous time steps to correct for potentially missing root segments. In our experiments, we processed images every 15 minutes to ensure that the plant has not grown much between 2 time steps. The average helped to alleviate certain problems caused by root occlusion or water droplets because the probability maps associated to previous frames act as memory mechanisms, resulting in more stable segmentations (see Fig. <xref rid="fig6" ref-type="fig">2</xref> for a visual example).</p>
      <p>At this point, as the user selects a Region of Interest (ROI) for each plant, the algorithm starts working one by one. We proceed to threshold the accumulated probability map for the selected plant, perform closing and opening morphological operations [<xref rid="bib57" ref-type="bibr">58</xref>] to eliminate spurious pixels, and then we select the biggest connected component as the root segmentation. Finally, we proceed to skeletonize [<xref rid="bib58" ref-type="bibr">59</xref>] the segmentation and construct a graph that represents the root system architecture.</p>
      <p>We run a depth first search (DFS) algorithm [<xref rid="bib59" ref-type="bibr">60</xref>] in order to label the bifurcation and end nodes of the unlabelled root graph given by the skeletonized binary segmentation. We use the DFS algorithm, starting from a seed that can be automatically chosen as the top pixel in the plant ROI or manually specified. For assigning labels to the MR, we work on the basis of the assumption that in early growing stages, there will only be an MR with seed (top pixel) and tip (bottom pixel). We then use nearest neighbours for matching the node graphs in the succeeding iterations. As more nodes appear deviating from the MR, they will be added as bifurcation (&gt;1 neighbour) or LR tip (1 neighbour, different from the MR tip). In case that 1 LR collides with the MR or another LR, the tip will still be a tip because of the matching process. Following this procedure, labels are assigned for the seed, MR tip, bifurcation, and LR tip nodes. Node graph matching based on a nearest neighbour criterion was performed between the labelled nodes of successive graphs in the temporal sequence to track the evolution of the root. These graph structures allowed us to extract phenotyping features such as MR length, total LRs length, or number of LRs at every temporal step. By processing the complete temporal sequence for a given root, we can obtain temporal features such as growing speed or information about the root behavior on day-night cycles, enabling the emergence of novel temporal plant phenotypes, such as those shown in the Results section. Figure <xref rid="fig7" ref-type="fig">7</xref> includes several examples of RSAs extracted from images with different levels of complexity. Note that we visualize the graphs using a simplified version where only nodes corresponding to seed, bifurcation, and tips are shown and connected. However, it is important to highlight that MR and LR length are computed considering the real length along the labelled skeleton, which are stored as an edge attribute in the simplified graph for optimization reasons.</p>
      <fig position="float" id="fig7">
        <label>Figure 7</label>
        <caption>
          <p>Examples of images, labelled skeleton, and simplified graphs corresponding to RSAs exhibiting different levels of complexity. Note that we visualize the graphs using a simplified version where only nodes corresponding to seed, bifurcation, and tips are shown and connected. However, because the full skeleton is labelled, the MR and LR length are computed considering the real length along the skeleton.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052fig7" position="float"/>
      </fig>
    </sec>
  </sec>
  <sec id="sec7">
    <title>Availability of Source Code and Requirements</title>
    <p>The source code corresponding to ChronoRoot, namely, the deep learning model and the graph generation procedures:</p>
    <list list-type="bullet">
      <list-item>
        <p>Project name: ChronoRoot: High-throughput phenotyping by deep learning reveals novel temporal parameters of plant root system architecture</p>
      </list-item>
      <list-item>
        <p>Project home page: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ngaggion/ChronoRoot" ext-link-type="uri">https://github.com/ngaggion/ChronoRoot</ext-link></p>
      </list-item>
      <list-item>
        <p>
          <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://scicrunch.org/resources/Any/record/nlx_144509-1/SCR_021259/resolver" ext-link-type="uri">RRID:SCR_021259</ext-link>
        </p>
      </list-item>
      <list-item>
        <p>Operating systems: Platform independent</p>
      </list-item>
      <list-item>
        <p>Programming language: Python</p>
      </list-item>
      <list-item>
        <p>Other requirements: Python &gt;3.3, Anaconda, TensorFlow 1.15, PyDenseCRF</p>
      </list-item>
      <list-item>
        <p>License: GNU GPL</p>
      </list-item>
    </list>
    <p>The source code corresponding to ChronoRoot imaging controller, namely, the web interface to check and set up the image acquisition parameters:</p>
    <list list-type="bullet">
      <list-item>
        <p>Project name: ChronoRoot: Module Controller</p>
      </list-item>
      <list-item>
        <p>Project home page: <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ThomasBlein/ChronoRootControl" ext-link-type="uri">https://github.com/ThomasBlein/ChronoRootControl</ext-link></p>
      </list-item>
      <list-item>
        <p>Operating systems: GNU/Linux</p>
      </list-item>
      <list-item>
        <p>Programming language: Python</p>
      </list-item>
      <list-item>
        <p>Other requirements: NGINX, uWSGI, Python ≥3.5, Flask ≥1.1.0, APScheduler, RPi.GPIO, picamera, WTForms, smbus2</p>
      </list-item>
      <list-item>
        <p>License: OSI-approved CeCILL-2.1 license</p>
      </list-item>
    </list>
  </sec>
  <sec sec-type="supplementary-material">
    <title>Supplementary Material</title>
    <supplementary-material id="sup1" position="float" content-type="local-data">
      <label>giab052_GIGA-D-20-00372_Original_Submission</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_giga-d-20-00372_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup2" position="float" content-type="local-data">
      <label>giab052_GIGA-D-20-00372_Revision_1</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_giga-d-20-00372_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup3" position="float" content-type="local-data">
      <label>giab052_GIGA-D-20-00372_Revision_2</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_giga-d-20-00372_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup4" position="float" content-type="local-data">
      <label>giab052_GIGA-D-20-00372_Revision_3</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_giga-d-20-00372_revision_3.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup5" position="float" content-type="local-data">
      <label>giab052_Response_to_Reviewer_Comments_Original_Submission</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_response_to_reviewer_comments_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup6" position="float" content-type="local-data">
      <label>giab052_Response_to_Reviewer_Comments_Revision_1</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_response_to_reviewer_comments_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup7" position="float" content-type="local-data">
      <label>giab052_Response_to_Reviewer_Comments_Revision_2</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_response_to_reviewer_comments_revision_2.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup8" position="float" content-type="local-data">
      <label>giab052_Reviewer_1_Report_Original_Submission</label>
      <caption>
        <p>Michael Pound -- 1/26/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_reviewer_1_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup9" position="float" content-type="local-data">
      <label>giab052_Reviewer_1_Report_Revision_1</label>
      <caption>
        <p>Michael Pound -- 6/17/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_reviewer_1_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup10" position="float" content-type="local-data">
      <label>giab052_Reviewer_2_Report_Original_Submission</label>
      <caption>
        <p>Chris Armit -- 1/28/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_reviewer_2_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup11" position="float" content-type="local-data">
      <label>giab052_Reviewer_2_Report_Revision_1</label>
      <caption>
        <p>Chris Armit -- 6/15/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_reviewer_2_report_revision_1.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup12" position="float" content-type="local-data">
      <label>giab052_Reviewer_3_Report_Original_Submission</label>
      <caption>
        <p>Valerio Giuffrida -- 3/10/2021 Reviewed</p>
      </caption>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_reviewer_3_report_original_submission.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material id="sup13" position="float" content-type="local-data">
      <label>giab052_Supplemental_Files</label>
      <media xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="giab052_supplemental_files.zip">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="ack1">
    <title>ACKNOWLEDGEMENTS</title>
    <p>We would like to thank Fablab Digiscope | LRI | UPSACLAY, and in particular Romain Di Vozzo, for fruitful discussions, his advice on the design, and for the access to their digital fabrication equipment. We thank Jean-Paul Bares and Maël Jeuffrard from IPS2 for support and assembling of the ChronoRoot modules. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp used for this research.</p>
  </ack>
  <sec sec-type="data-availability" id="sec8">
    <title>Data Availability</title>
    <p>All data gathered and reported in this study are available as supplementary material. The 2 datasets of images and annotations described in the Datasets section, as well as the 3D printing and laser cutting files, are publicly available at <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ThomasBlein/ChronoRootModuleHardware" ext-link-type="uri">https://github.com/ThomasBlein/ChronoRootModuleHardware</ext-link> under the CERN Open Hardware License Version 2—Strongly Reciprocal licence. <xref rid="sup13" ref-type="supplementary-material">Supplementary figures and tables</xref> referenced in this work, as well as a detailed description of the hardware system, are available in the supplementary file. Snapshots of our code and other data further supporting this work are openly available in the GigaScience Repository, GigaDB [<xref rid="bib60" ref-type="bibr">61</xref>].</p>
  </sec>
  <sec sec-type="supplementary-material" id="h1content1626101177091">
    <title>Additional Files</title>
    <p>Supplementary Table S1: Detailed description of the UNet architecture implemented in this work.</p>
    <p>Supplementary Table S2: Detailed description of the Residual UNet implemented in this work and the proposed Deeply Supervised Residual UNet.</p>
    <p>Supplementary Figure S1: Novel time-derived parameters of RSA.</p>
    <p>Supplementary Figure S2: Novel time-derived parameters of RSA.</p>
    <p>Supplementary Figure S3: Low-cost device for automatic image acquisition of plant plates.</p>
    <p>Supplementary Figure S4: LED near-infrared panel front view and back view.</p>
    <p>Supplementary Figure S5: Plate support.</p>
    <p>Supplementary Figure S6: The camera set-up.</p>
    <p>Supplementary Figure S7: Electronic connection of a module.</p>
    <p>Supplementary Video Abstract</p>
  </sec>
  <sec id="sec9">
    <title>Abbreviations</title>
    <p>CL: continuous light; CNN: convolutional neural network; CRF: conditional random field; DFS: depth first search; DSResUNet: Deeply Supervised ResUNet; ELU: exponential linear unit; FCN: fully convolutional network; GPU: graphical processing unit; GWAS: genome-wide association studies; GT: ground truth; IR: infrared; LD: long day; LR: lateral root; MR: main root; NIR: near-infrared; RAM: random access memory; RELU: rectified linear unit; ROI: region of interest; RSA: root system architecture; RSML: Root System Markup Language; SD: standard deviation; TR: total root.</p>
  </sec>
  <sec id="h1content1626101885396">
    <title>Competing Interests</title>
    <p>The authors declare that they have no competing interests.</p>
  </sec>
  <sec id="h1content1626132317322">
    <title>Funding</title>
    <p>This work was supported by grants from French State (Saclay Plant Sciences, reference No. ANR-17-EUR-0007, EUR SPS-GSR) managed by the French National Research Agency under an Investments for the Future program (reference No. ANR-11-IDEX-0003-02) to V.D., T.R., M.C., and T.B.; CNRS through the MITI interdisciplinary programs to T.B.; AXA Research Fund, ANPCyT (PICT2018-3907) and UNL (CAI+D 50220140100084LI and 50620190100145LI) to E.F.; ANPCyT (PICT2019-04137) to F.A.; ANPCyT (PICT 2018-3384) to DM.; and CNRS (Laboratoire International Associé NOCOSYM) to M.C. and F.A.</p>
  </sec>
  <sec id="h1content1626132327632">
    <title>Authors' Contributions</title>
    <p>T.B., M.C., E.F., and F.A. conceived the project. N.G. and E.F. designed the deep learning models. N.G. implemented the deep learning models, ran the numerical experiments, and generated the figures. T.B., V.D., E.L., and S.L. designed and built the hardware system. T.B. and V.D. implemented the web control interface. T.R. prepared the plates with <italic toggle="yes">A. thaliana</italic> seeds and launched the experiments for image acquisition. A.C. and N.G. annotated the images used to train the deep learning models. N.G., E.F., F.A., D.H.M., and T.B. analysed and interpreted the results. E.F., F.A., T.R., D.H.M., M.C., T.B., and N.G. wrote the manuscript.</p>
  </sec>
  <sec id="sec10">
    <title>Authors’ Information</title>
    <p>F.A., D.H.M., and E.F. are researchers of CONICET; N.G. and A.C. are fellows of the same institution. T.B. and M.C. are researchers and V.D. is an engineer of CNRS. E.L. and S.L. are technicians and T.R. is a fellow of University Paris-Saclay.</p>
  </sec>
  <ref-list id="ref1">
    <title>References</title>
    <ref id="bib1">
      <label>1.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Palmer</surname>  <given-names>CM</given-names></string-name>, <string-name><surname>Bush</surname>  <given-names>SM</given-names></string-name>, <string-name><surname>Maloof</surname>  <given-names>JN</given-names></string-name></person-group>. <article-title>Phenotypic and developmental plasticity in plants</article-title>. In: <source>eLS</source>. <publisher-loc>Chichester, UK</publisher-loc>: <publisher-name>Wiley</publisher-name>; <year>2012</year>, doi:<pub-id pub-id-type="doi">10.1002/9780470015902.a0002092.pub2</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib2">
      <label>2.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Tracy</surname>  <given-names>SR</given-names></string-name>, <string-name><surname>Nagel</surname>  <given-names>KA</given-names></string-name>, <string-name><surname>Postma</surname>  <given-names>JA</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Crop improvement from phenotyping roots: Highlights reveal expanding opportunities</article-title>. <source>Trends Plant Sci</source>. <year>2020</year>;<volume>25</volume>(<issue>1</issue>):<fpage>105</fpage>–<lpage>18</lpage>.<pub-id pub-id-type="pmid">31806535</pub-id></mixed-citation>
    </ref>
    <ref id="bib3">
      <label>3.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ingram</surname>  <given-names>PA</given-names></string-name>, <string-name><surname>Malamy</surname>  <given-names>JE</given-names></string-name></person-group>. In: <source>Root System Architecture, vol. 55 of Advances in Botanical Research</source>. <publisher-name>Elsevier</publisher-name>; <year>2010</year>:<fpage>75</fpage>–<lpage>117</lpage>.</mixed-citation>
    </ref>
    <ref id="bib4">
      <label>4.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Narisetti</surname>  <given-names>N</given-names></string-name>, <string-name><surname>Henke</surname>  <given-names>M</given-names></string-name>, <string-name><surname>Seiler</surname>  <given-names>C</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Semi-automated Root Image Analysis (saRIA)</article-title>. <source>Sci Rep</source>. <year>2019</year>; <volume>9</volume>(<issue>1</issue>):<fpage>19674</fpage>.<pub-id pub-id-type="pmid">31873104</pub-id></mixed-citation>
    </ref>
    <ref id="bib5">
      <label>5.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yushkevich</surname>  <given-names>PA</given-names></string-name>, <string-name><surname>Piven</surname>  <given-names>J</given-names></string-name>, <string-name><surname>Hazlett</surname>  <given-names>HC</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability</article-title>. <source>Neuroimage</source>. <year>2006</year>;<volume>31</volume>(<issue>3</issue>):<fpage>1116</fpage>–<lpage>28</lpage>.<pub-id pub-id-type="pmid">16545965</pub-id></mixed-citation>
    </ref>
    <ref id="bib6">
      <label>6.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Ronneberger</surname>  <given-names>O</given-names></string-name>, <string-name><surname>Fischer</surname>  <given-names>P</given-names></string-name>, <string-name><surname>Brox</surname>  <given-names>T</given-names></string-name></person-group>. <article-title>U-net: Convolutional networks for biomedical image segmentation</article-title>. In: <source>International Conference on Medical Image Computing and Computer-Assisted Intervention</source>. <publisher-name>Springer</publisher-name>; <year>2015</year>:<fpage>234</fpage>–<lpage>41</lpage>.</mixed-citation>
    </ref>
    <ref id="bib7">
      <label>7.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Badrinarayanan</surname>  <given-names>V</given-names></string-name>, <string-name><surname>Kendall</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Cipolla</surname>  <given-names>R</given-names></string-name></person-group>. <article-title>SegNet: A deep convolutional encoder-decoder architecture for image segmentation</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2017</year>; <volume>39</volume>(<issue>12</issue>):<fpage>2481</fpage>–<lpage>95</lpage>.<pub-id pub-id-type="pmid">28060704</pub-id></mixed-citation>
    </ref>
    <ref id="bib8">
      <label>8.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>  <given-names>LC</given-names></string-name>, <string-name><surname>Papandreou</surname>  <given-names>G</given-names></string-name>, <string-name><surname>Kokkinos</surname>  <given-names>I</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source>. <year>2017</year>;<volume>40</volume>(<issue>4</issue>):<fpage>834</fpage>–<lpage>48</lpage>.<pub-id pub-id-type="pmid">28463186</pub-id></mixed-citation>
    </ref>
    <ref id="bib9">
      <label>9.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>He</surname>  <given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname>  <given-names>X</given-names></string-name>, <string-name><surname>Ren</surname>  <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Deep residual learning for image recognition</article-title>. In: <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>; <year>2016</year>:<fpage>770</fpage>–<lpage>8</lpage>.</mixed-citation>
    </ref>
    <ref id="bib10">
      <label>10.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lee</surname>  <given-names>CY</given-names></string-name>, <string-name><surname>Xie</surname>  <given-names>S</given-names></string-name>, <string-name><surname>Gallagher</surname>  <given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Deeply-supervised nets</article-title>. In: <source>Artificial Intelligence and Statistics</source>. <year>2015</year>:<fpage>562</fpage>–<lpage>70</lpage>.</mixed-citation>
    </ref>
    <ref id="bib11">
      <label>11.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Kamnitsas</surname>  <given-names>K</given-names></string-name>, <string-name><surname>Bai</surname>  <given-names>W</given-names></string-name>, <string-name><surname>Ferrante</surname>  <given-names>E</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Ensembles of multiple models and architectures for robust brain tumour segmentation</article-title>. In: <source>International MICCAI Brainlesion Workshop</source>. <publisher-name>Springer</publisher-name>; <year>2017</year>:<fpage>450</fpage>–<lpage>62</lpage>.</mixed-citation>
    </ref>
    <ref id="bib12">
      <label>12.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Orlando</surname>  <given-names>JI</given-names></string-name>, <string-name><surname>Manterola</surname>  <given-names>HL</given-names></string-name>, <string-name><surname>Ferrante</surname>  <given-names>E</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title><italic toggle="yes">Arabidopsis</italic> roots segmentation based on morphological operations and CRFs</article-title>. <year>2017</year>; <comment>arXiv:1704.07793</comment>.</mixed-citation>
    </ref>
    <ref id="bib13">
      <label>13.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Krähenbühl</surname>  <given-names>P</given-names></string-name>, <string-name><surname>Koltun</surname>  <given-names>V</given-names></string-name></person-group>. <article-title>Efficient inference in fully connected CRFs with Gaussian edge potentials</article-title>. In: <source>Advances in Neural Information Processing Systems</source>. <year>2011</year>:<fpage>109</fpage>–<lpage>17</lpage>.</mixed-citation>
    </ref>
    <ref id="bib14">
      <label>14.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Gelderen</surname>  <given-names>K</given-names></string-name>, <string-name><surname>Kang</surname>  <given-names>C</given-names></string-name>, <string-name><surname>Pierik</surname>  <given-names>R</given-names></string-name></person-group>. <article-title>Light signaling, root development, and plasticity</article-title>. <source>Plant Physiol</source>. <year>2018</year>;<volume>176</volume>(<issue>2</issue>):<fpage>1049</fpage>–<lpage>60</lpage>.<pub-id pub-id-type="pmid">28939624</pub-id></mixed-citation>
    </ref>
    <ref id="bib15">
      <label>15.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Fisahn</surname>  <given-names>J</given-names></string-name>, <string-name><surname>Yazdanbakhsh</surname>  <given-names>N</given-names></string-name>, <string-name><surname>Klingele</surname>  <given-names>E</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title><italic toggle="yes">Arabidopsis thaliana</italic> root growth kinetics and lunisolar tidal acceleration</article-title>. <source>New Phytol</source>. <year>2012</year>;<volume>195</volume>(<issue>2</issue>):<fpage>346</fpage>–<lpage>55</lpage>.<pub-id pub-id-type="pmid">22583121</pub-id></mixed-citation>
    </ref>
    <ref id="bib16">
      <label>16.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yazdanbakhsh</surname>  <given-names>N</given-names></string-name>, <string-name><surname>Sulpice</surname>  <given-names>R</given-names></string-name>, <string-name><surname>Graf</surname>  <given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Circadian control of root elongation and C partitioning in <italic toggle="yes">Arabidopsis thaliana</italic></article-title>. <source>Plant Cell Environ</source>. <year>2011</year>;<volume>34</volume>(<issue>6</issue>):<fpage>877</fpage>–<lpage>94</lpage>.<pub-id pub-id-type="pmid">21332506</pub-id></mixed-citation>
    </ref>
    <ref id="bib17">
      <label>17.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Dhondt</surname>  <given-names>S</given-names></string-name>, <string-name><surname>Wuyts</surname>  <given-names>N</given-names></string-name>, <string-name><surname>Inzé</surname>  <given-names>D</given-names></string-name></person-group>. <article-title>Cell to whole-plant phenotyping: the best is yet to come</article-title>. <source>Trends Plant Sci</source>. <year>2013</year>;<volume>18</volume>(<issue>8</issue>):<fpage>428</fpage>–<lpage>39</lpage>.<pub-id pub-id-type="pmid">23706697</pub-id></mixed-citation>
    </ref>
    <ref id="bib18">
      <label>18.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lobet</surname>  <given-names>G</given-names></string-name>, <string-name><surname>Paez-Garcia</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Schneider</surname>  <given-names>H</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Demystifying roots: A need for clarification and extended concepts in root phenotyping</article-title>. <source>Plant Sci</source>. <year>2019</year>;<volume>282</volume>:<fpage>11</fpage>–<lpage>3</lpage>.<pub-id pub-id-type="pmid">31003606</pub-id></mixed-citation>
    </ref>
    <ref id="bib19">
      <label>19.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Kuijken</surname>  <given-names>RCP</given-names></string-name>, <string-name><surname>van Eeuwijk</surname>  <given-names>FA</given-names></string-name>, <string-name><surname>Marcelis</surname>  <given-names>LFM</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Root phenotyping: From component trait in the lab to breeding</article-title>. <source>J Exp Bot</source>. <year>2015</year>;<volume>66</volume>(<issue>18</issue>):<fpage>5389</fpage>–<lpage>401</lpage>.<pub-id pub-id-type="pmid">26071534</pub-id></mixed-citation>
    </ref>
    <ref id="bib20">
      <label>20.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Coppens</surname>  <given-names>F</given-names></string-name>, <string-name><surname>Wuyts</surname>  <given-names>N</given-names></string-name>, <string-name><surname>Inzé</surname>  <given-names>D</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Unlocking the potential of plant phenotyping data through integration and data-driven approaches</article-title>. <source>Curr Opin Syst Biol</source>. <year>2017</year>;<volume>4</volume>:<fpage>58</fpage>–<lpage>63</lpage>.<pub-id pub-id-type="pmid">32923745</pub-id></mixed-citation>
    </ref>
    <ref id="bib21">
      <label>21.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>International Plant Phenotyping Network</collab>.</person-group>  <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.plant-phenotyping.org/" ext-link-type="uri">https://www.plant-phenotyping.org/.</ext-link> Accessed 18 December 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib22">
      <label>22.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wells</surname>  <given-names>DM</given-names></string-name>, <string-name><surname>French</surname>  <given-names>AP</given-names></string-name>, <string-name><surname>Naeem</surname>  <given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Recovering the dynamics of root growth and development using novel image acquisition and analysis methods</article-title>. <source>Philos Trans R Soc Lond B Biol Sci</source>. <year>2012</year>;<volume>367</volume>(<issue>1595</issue>):<fpage>1517</fpage>–<lpage>24</lpage>.<pub-id pub-id-type="pmid">22527394</pub-id></mixed-citation>
    </ref>
    <ref id="bib23">
      <label>23.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Arduino</collab>.</person-group>  <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.arduino.cc" ext-link-type="uri">https://www.arduino.cc.</ext-link> Accessed 18 December 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib24">
      <label>24.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Raspberry Pi</collab>.</person-group>  <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.raspberrypi.org" ext-link-type="uri">https://www.raspberrypi.org</ext-link>. Accessed 18 December 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib25">
      <label>25.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Valle</surname>  <given-names>B</given-names></string-name>, <string-name><surname>Simonneau</surname>  <given-names>T</given-names></string-name>, <string-name><surname>Boulord</surname>  <given-names>R</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>PYM: A new, affordable, image-based method using a Raspberry Pi to phenotype plant leaf area in a wide diversity of environments</article-title>. <source>Plant Methods</source>. <year>2017</year>;<volume>13</volume>:<fpage>98</fpage>.<pub-id pub-id-type="pmid">29151844</pub-id></mixed-citation>
    </ref>
    <ref id="bib26">
      <label>26.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Minervini</surname>  <given-names>M</given-names></string-name>, <string-name><surname>Giuffrida</surname>  <given-names>MV</given-names></string-name>, <string-name><surname>Perata</surname>  <given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Phenotiki: An open software and hardware platform for affordable and easy image-based phenotyping of rosette-shaped plants</article-title>. <source>Plant J</source>. <year>2017</year>;<volume>90</volume>(<issue>1</issue>):<fpage>204</fpage>–<lpage>16</lpage>.<pub-id pub-id-type="pmid">28066963</pub-id></mixed-citation>
    </ref>
    <ref id="bib27">
      <label>27.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Bontpart</surname>  <given-names>T</given-names></string-name>, <string-name><surname>Concha</surname>  <given-names>C</given-names></string-name>, <string-name><surname>Giuffrida</surname>  <given-names>MV</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Affordable and robust phenotyping framework to analyse root system architecture of soil-grown plants</article-title>. <source>Plant J</source>. <year>2020</year>;<volume>103</volume>(<issue>6</issue>):<fpage>2330</fpage>–<lpage>43</lpage>.<pub-id pub-id-type="pmid">32530068</pub-id></mixed-citation>
    </ref>
    <ref id="bib28">
      <label>28.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Wu</surname>  <given-names>J</given-names></string-name>, <string-name><surname>Wu</surname>  <given-names>Q</given-names></string-name>, <string-name><surname>Pagès</surname>  <given-names>L</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>RhizoChamber-Monitor: A robotic platform and software enabling characterization of root growth</article-title>. <source>Plant Methods</source>. <year>2018</year>;<volume>14</volume>:<fpage>44</fpage>.<pub-id pub-id-type="pmid">29930694</pub-id></mixed-citation>
    </ref>
    <ref id="bib29">
      <label>29.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Iyer-Pascuzzi</surname>  <given-names>AS</given-names></string-name>, <string-name><surname>Symonova</surname>  <given-names>O</given-names></string-name>, <string-name><surname>Mileyko</surname>  <given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Imaging and analysis platform for automatic phenotyping and trait ranking of plant root systems</article-title>. <source>Plant Physiol</source>. <year>2010</year>;<volume>152</volume>(<issue>3</issue>):<fpage>1148</fpage>–<lpage>57</lpage>.<pub-id pub-id-type="pmid">20107024</pub-id></mixed-citation>
    </ref>
    <ref id="bib30">
      <label>30.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Hund</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Trachsel</surname>  <given-names>S</given-names></string-name>, <string-name><surname>Stamp</surname>  <given-names>P</given-names></string-name></person-group>. <article-title>Growth of axile and lateral roots of maize: I development of a phenotying platform</article-title>. <source>Plant Soil</source>. <year>2009</year>;<volume>325</volume>(<issue>1-2</issue>):<fpage>335</fpage>–<lpage>49</lpage>.</mixed-citation>
    </ref>
    <ref id="bib31">
      <label>31.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Armengaud</surname>  <given-names>P</given-names></string-name>, <string-name><surname>Zambaux</surname>  <given-names>K</given-names></string-name>, <string-name><surname>Hills</surname>  <given-names>A</given-names></string-name></person-group>. <article-title>EZ-Rhizo: Integrated software for the fast and accurate measurement of root system architecture</article-title>. <source>Plant J</source>. <year>2009</year>;<volume>57</volume>(<issue>5</issue>):<fpage>945</fpage>–<lpage>56</lpage>.<pub-id pub-id-type="pmid">19000163</pub-id></mixed-citation>
    </ref>
    <ref id="bib32">
      <label>32.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Clark</surname>  <given-names>RT</given-names></string-name>, <string-name><surname>Famoso</surname>  <given-names>AN</given-names></string-name>, <string-name><surname>Zhao</surname>  <given-names>K</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>High-throughput two-dimensional root system phenotyping platform facilitates genetic analysis of root growth and development</article-title>. <source>Plant Cell Environ</source>. <year>2013</year>;<volume>36</volume>(<issue>2</issue>):<fpage>454</fpage>–<lpage>66</lpage>.<pub-id pub-id-type="pmid">22860896</pub-id></mixed-citation>
    </ref>
    <ref id="bib33">
      <label>33.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lobet</surname>  <given-names>G</given-names></string-name>, <string-name><surname>Draye</surname>  <given-names>X</given-names></string-name>, <string-name><surname>Périlleux</surname>  <given-names>C</given-names></string-name></person-group>. <article-title>An online database for plant image analysis software tools</article-title>. <source>Plant Methods</source>. <year>2013</year>;<volume>9</volume>(<issue>1</issue>):<fpage>38</fpage>.<pub-id pub-id-type="pmid">24107223</pub-id></mixed-citation>
    </ref>
    <ref id="bib34">
      <label>34.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Galkovskyi</surname>  <given-names>T</given-names></string-name>, <string-name><surname>Mileyko</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Bucksch</surname>  <given-names>A</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>GiA Roots: Software for the high throughput analysis of plant root system architecture</article-title>. <source>BMC Plant Biol</source>. <year>2012</year>;<volume>12</volume>:<fpage>116</fpage>.<pub-id pub-id-type="pmid">22834569</pub-id></mixed-citation>
    </ref>
    <ref id="bib35">
      <label>35.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Lobet</surname>  <given-names>G</given-names></string-name></person-group>. <article-title>Image analysis in plant sciences: Publish then perish</article-title>. <source>Trends Plant Sci</source>. <year>2017</year>;<volume>22</volume>(<issue>7</issue>):<fpage>559</fpage>–<lpage>66</lpage>.<pub-id pub-id-type="pmid">28571940</pub-id></mixed-citation>
    </ref>
    <ref id="bib36">
      <label>36.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>Quantitative Plant</collab>.</person-group>  <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://www.quantitative-plant.org" ext-link-type="uri">https://www.quantitative-plant.org</ext-link>, Accessed: 2020-12-18</comment>.</mixed-citation>
    </ref>
    <ref id="bib37">
      <label>37.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Pound</surname>  <given-names>MP</given-names></string-name>, <string-name><surname>Atkinson</surname>  <given-names>JA</given-names></string-name>, <string-name><surname>Townsend</surname>  <given-names>AJ</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Deep machine learning provides state-of-the-art performance in image-based plant phenotyping</article-title>. <source>Gigascience</source>. <year>2017</year>;<volume>6</volume>(<issue>10</issue>):doi:<pub-id pub-id-type="doi">10.1093/gigascience/gix083</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib38">
      <label>38.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Teramoto</surname>  <given-names>S</given-names></string-name>, <string-name><surname>Takayasu</surname>  <given-names>S</given-names></string-name>, <string-name><surname>Kitomi</surname>  <given-names>Y</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>High-throughput three-dimensional visualization of root system architecture of rice using X-ray computed tomography</article-title>. <source>Plant Methods</source>. <year>2020</year>;<volume>16</volume>:<fpage>66</fpage>.<pub-id pub-id-type="pmid">32426023</pub-id></mixed-citation>
    </ref>
    <ref id="bib39">
      <label>39.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>van Dusschoten</surname>  <given-names>D</given-names></string-name>, <string-name><surname>Metzner</surname>  <given-names>R</given-names></string-name>, <string-name><surname>Kochs</surname>  <given-names>J</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Quantitative 3D analysis of plant roots growing in soil using magnetic resonance imaging</article-title>. <source>Plant Physiol</source>. <year>2016</year>;<volume>170</volume>(<issue>3</issue>):<fpage>1176</fpage>–<lpage>88</lpage>.<pub-id pub-id-type="pmid">26729797</pub-id></mixed-citation>
    </ref>
    <ref id="bib40">
      <label>40.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>French</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Ubeda-Tomás</surname>  <given-names>S</given-names></string-name>, <string-name><surname>Holman</surname>  <given-names>TJ</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>High-throughput quantification of root growth using a novel image-analysis tool</article-title>. <source>Plant Physiol</source>. <year>2009</year>;<volume>150</volume>(<issue>4</issue>):<fpage>1784</fpage>–<lpage>95</lpage>.<pub-id pub-id-type="pmid">19515787</pub-id></mixed-citation>
    </ref>
    <ref id="bib41">
      <label>41.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Naeem</surname>  <given-names>A</given-names></string-name>, <string-name><surname>French</surname>  <given-names>AP</given-names></string-name>, <string-name><surname>Wells</surname>  <given-names>DM</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>High-throughput feature counting and measurement of roots</article-title>. <source>Bioinformatics</source>. <year>2011</year>;<volume>27</volume>(<issue>9</issue>):<fpage>1337</fpage>–<lpage>8</lpage>.<pub-id pub-id-type="pmid">21398671</pub-id></mixed-citation>
    </ref>
    <ref id="bib42">
      <label>42.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>WinRHIZO Website</collab>.</person-group>  <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://regentinstruments.com/assets/winrhizo_mostrecent.html" ext-link-type="uri">https://regentinstruments.com/assets/winrhizo_mostrecent.html</ext-link>. Accessed: 13th July, 2021</comment>.</mixed-citation>
    </ref>
    <ref id="bib43">
      <label>43.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Slovak</surname>  <given-names>R</given-names></string-name>, <string-name><surname>Göschl</surname>  <given-names>C</given-names></string-name>, <string-name><surname>Su</surname>  <given-names>X</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>A scalable open-source pipeline for large-scale root phenotyping of<italic toggle="yes">Arabidopsis</italic></article-title>. <source>Plant Cell</source>. <year>2014</year>;<volume>26</volume>(<issue>6</issue>):<fpage>2390</fpage>–<lpage>403</lpage>.<pub-id pub-id-type="pmid">24920330</pub-id></mixed-citation>
    </ref>
    <ref id="bib44">
      <label>44.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Chen</surname>  <given-names>H</given-names></string-name>, <string-name><surname>Valerio Giuffrida</surname>  <given-names>M</given-names></string-name>, <string-name><surname>Doerner</surname>  <given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Adversarial large-scale root gap inpainting</article-title>. In: <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</source>. <year>2019</year>,<pub-id pub-id-type="doi">10.1109/CVPRW.2019.00318</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib45">
      <label>45.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Chen</surname>  <given-names>H</given-names></string-name>, <string-name><surname>Giuffrida</surname>  <given-names>MV</given-names></string-name>, <string-name><surname>Doerner</surname>  <given-names>P</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Blind inpainting of large-scale masks of thin structures with adversarial and reinforcement learning</article-title>. <year>2019</year>; <comment>arXiv:1912.02470</comment>.</mixed-citation>
    </ref>
    <ref id="bib46">
      <label>46.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Yasrab</surname>  <given-names>R</given-names></string-name>, <string-name><surname>Atkinson</surname>  <given-names>JA</given-names></string-name>, <string-name><surname>Wells</surname>  <given-names>DM</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>RootNav 2.0: Deep learning for automatic navigation of complex plant root architectures</article-title>. <source>Gigascience</source>. <year>2019</year>;<volume>8</volume>(<issue>11</issue>),doi:<pub-id pub-id-type="doi">10.1093/gigascience/giz123</pub-id>.</mixed-citation>
    </ref>
    <ref id="bib47">
      <label>47.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Iijima</surname>  <given-names>M</given-names></string-name>, <string-name><surname>Matsushita</surname>  <given-names>N</given-names></string-name></person-group>. <article-title>A circadian and an ultradian rhythm are both evident in root growth of rice</article-title>. <source>J Plant Physiol</source>. <year>2011</year>;<volume>168</volume>(<issue>17</issue>):<fpage>2072</fpage>–<lpage>80</lpage>.<pub-id pub-id-type="pmid">21802171</pub-id></mixed-citation>
    </ref>
    <ref id="bib61_1626128066218">
      <label>48.</label>
      <mixed-citation publication-type="journal"><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/ThomasBlein/ChronoRootModuleHardware" ext-link-type="uri">https://github.com/ThomasBlein/ChronoRootModuleHardware</ext-link>. <comment>Accessed: 13th July, 2021</comment>.</mixed-citation>
    </ref>
    <ref id="bib48">
      <label>49.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>RootSystemML home page</collab>.</person-group>  <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://rootsystemml.github.io/" ext-link-type="uri">http://rootsystemml.github.io/.</ext-link> Accessed 18 December, 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib49">
      <label>50.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>LeCun</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Bengio</surname>  <given-names>Y</given-names></string-name>, <string-name><surname>Hinton</surname>  <given-names>G</given-names></string-name></person-group>. <article-title>Deep learning</article-title>. <source>Nature</source>. <year>2015</year>;<volume>521</volume>(<issue>7553</issue>):<fpage>436</fpage>–<lpage>44</lpage>.<pub-id pub-id-type="pmid">26017442</pub-id></mixed-citation>
    </ref>
    <ref id="bib50">
      <label>51.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Garcia-Garcia</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Orts-Escolano</surname>  <given-names>S</given-names></string-name>, <string-name><surname>Oprea</surname>  <given-names>S</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>A survey on deep learning techniques for image and video semantic segmentation</article-title>. <source>Appl Soft Comput</source>. <year>2018</year>;<volume>70</volume>:<fpage>41</fpage>–<lpage>65</lpage>.</mixed-citation>
    </ref>
    <ref id="bib51">
      <label>52.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>  <given-names>Z</given-names></string-name>, <string-name><surname>Liu</surname>  <given-names>Q</given-names></string-name>, <string-name><surname>Wang</surname>  <given-names>Y</given-names></string-name></person-group>. <article-title>Road extraction by deep residual u-net</article-title>. <source>IEEE Geosci Remote Sens Lett</source>. <year>2018</year>;<volume>15</volume>(<issue>5</issue>):<fpage>749</fpage>–<lpage>53</lpage>.</mixed-citation>
    </ref>
    <ref id="bib52">
      <label>53.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Simonyan</surname>  <given-names>K</given-names></string-name>, <string-name><surname>Zisserman</surname>  <given-names>A</given-names></string-name></person-group>. <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. In: <source>International Conference on Learning Representations</source>; <year>2015</year>. <ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1409.1556v6" ext-link-type="uri">https://arxiv.org/abs/1409.1556v6</ext-link>.</mixed-citation>
    </ref>
    <ref id="bib53">
      <label>54.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>SegNet implementation in tensorflow</collab>.</person-group>  <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/aizawan/segnet" ext-link-type="uri">https://github.com/aizawan/segnet.</ext-link> Accessed 18 December 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib54">
      <label>55.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Breiman</surname>  <given-names>L</given-names></string-name></person-group>. <article-title>Bagging predictors</article-title>. <source>Mach Learn</source>. <year>1996</year>;<volume>24</volume>(<issue>2</issue>):<fpage>123</fpage>–<lpage>40</lpage>.</mixed-citation>
    </ref>
    <ref id="bib55">
      <label>56.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Lafferty</surname>  <given-names>J</given-names></string-name>, <string-name><surname>McCallum</surname>  <given-names>A</given-names></string-name>, <string-name><surname>Pereira</surname>  <given-names>FC</given-names></string-name></person-group>. <article-title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data</article-title>. In: <person-group person-group-type="editor"><string-name><surname>Kaufmann</surname>  <given-names>Morgan</given-names></string-name></person-group>  <source>ICML: Proceedings of the Eighteenth International Conference on Machine Learning</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <year>2001</year>:<fpage>282</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="bib56">
      <label>57.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><collab>PyDenseCRF Implementation</collab>.</person-group>  <comment><ext-link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/lucasb-eyer/pydensecrf" ext-link-type="uri">https://github.com/lucasb-eyer/pydensecrf.</ext-link> Accessed 18 December 2020</comment>.</mixed-citation>
    </ref>
    <ref id="bib57">
      <label>58.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Gonzalez</surname>  <given-names>RC</given-names></string-name>, <string-name><surname>Woods</surname>  <given-names>RE</given-names></string-name></person-group>. <source>Digital Image Processing</source>. <publisher-name>Pearson Higher Ed</publisher-name>; <year>2011</year>.</mixed-citation>
    </ref>
    <ref id="bib58">
      <label>59.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Zhang</surname>  <given-names>T</given-names></string-name>, <string-name><surname>Suen</surname>  <given-names>CY</given-names></string-name></person-group>. <article-title>A fast parallel algorithm for thinning digital patterns</article-title>. <source>Commun ACM</source>. <year>1984</year>;<volume>27</volume>(<issue>3</issue>):<fpage>236</fpage>–<lpage>9</lpage>.</mixed-citation>
    </ref>
    <ref id="bib59">
      <label>60.</label>
      <mixed-citation publication-type="book"><person-group person-group-type="author"><string-name><surname>Cormen</surname>  <given-names>TH</given-names></string-name>, <string-name><surname>Leiserson</surname>  <given-names>CE</given-names></string-name>, <string-name><surname>Rivest</surname>  <given-names>RL</given-names></string-name>, <etal>et al.</etal></person-group>  <source>Introduction to Algorithms</source>. <publisher-name>MIT Press</publisher-name>; <year>2001</year>.</mixed-citation>
    </ref>
    <ref id="bib60">
      <label>61.</label>
      <mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name><surname>Gaggion</surname>  <given-names>N</given-names></string-name>, <string-name><surname>Ariel</surname>  <given-names>F</given-names></string-name>, <string-name><surname>Daric</surname>  <given-names>V</given-names></string-name>, <etal>et al.</etal></person-group>  <article-title>Supporting data for “ChronoRoot: High-throughput phenotyping by deep segmentation networks reveals novel temporal parameters of plant root system architecture."</article-title>. <source>GigaScience Database</source>. <year>2021</year>. <pub-id pub-id-type="doi">10.5524/100911</pub-id>.</mixed-citation>
    </ref>
  </ref-list>
</back>
