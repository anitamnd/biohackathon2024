<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?properties open_access?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS One</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-title-group>
      <journal-title>PLOS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9970073</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-22-24958</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0282268</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Bioassays and Physiological Analysis</subject>
          <subj-group>
            <subject>Electrophysiological Techniques</subject>
            <subj-group>
              <subject>Brain Electrophysiology</subject>
              <subj-group>
                <subject>Electroencephalography</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Physiology</subject>
          <subj-group>
            <subject>Electrophysiology</subject>
            <subj-group>
              <subject>Neurophysiology</subject>
              <subj-group>
                <subject>Brain Electrophysiology</subject>
                <subj-group>
                  <subject>Electroencephalography</subject>
                </subj-group>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neurophysiology</subject>
            <subj-group>
              <subject>Brain Electrophysiology</subject>
              <subj-group>
                <subject>Electroencephalography</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Brain Mapping</subject>
            <subj-group>
              <subject>Electroencephalography</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Clinical Medicine</subject>
          <subj-group>
            <subject>Clinical Neurophysiology</subject>
            <subj-group>
              <subject>Electroencephalography</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
          <subj-group>
            <subject>Neuroimaging</subject>
            <subj-group>
              <subject>Electroencephalography</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neuroimaging</subject>
            <subj-group>
              <subject>Electroencephalography</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Mathematical and Statistical Techniques</subject>
          <subj-group>
            <subject>Statistical Methods</subject>
            <subj-group>
              <subject>Forecasting</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Physical Sciences</subject>
        <subj-group>
          <subject>Mathematics</subject>
          <subj-group>
            <subject>Statistics</subject>
            <subj-group>
              <subject>Statistical Methods</subject>
              <subj-group>
                <subject>Forecasting</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Signal Processing</subject>
          <subj-group>
            <subject>Signal Filtering</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Signal Processing</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Neurology</subject>
          <subj-group>
            <subject>Epilepsy</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Artificial Intelligence</subject>
          <subj-group>
            <subject>Machine Learning</subject>
            <subj-group>
              <subject>Deep Learning</subject>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Software Tools</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Software Tools</subject>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>EPViz: A flexible and lightweight visualizer to facilitate predictive modeling for multi-channel EEG</article-title>
      <alt-title alt-title-type="running-head">EPViz: A flexible and lightweight visualizer to facilitate predictive modeling for multi-channel EEG</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Currey</surname>
          <given-names>Danielle</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/software/">Software</role>
        <role content-type="http://credit.niso.org/contributor-roles/validation/">Validation</role>
        <role content-type="http://credit.niso.org/contributor-roles/visualization/">Visualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <xref rid="aff001" ref-type="aff">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" equal-contrib="yes">
        <name>
          <surname>Craley</surname>
          <given-names>Jeff</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role>
        <role content-type="http://credit.niso.org/contributor-roles/investigation/">Investigation</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Hsu</surname>
          <given-names>David</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff003" ref-type="aff">
          <sup>3</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ahmed</surname>
          <given-names>Raheel</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/data-curation/">Data curation</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-review-editing/">Writing – review &amp; editing</role>
        <xref rid="aff004" ref-type="aff">
          <sup>4</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2653-5591</contrib-id>
        <name>
          <surname>Venkataraman</surname>
          <given-names>Archana</given-names>
        </name>
        <role content-type="http://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role>
        <role content-type="http://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role>
        <role content-type="http://credit.niso.org/contributor-roles/methodology/">Methodology</role>
        <role content-type="http://credit.niso.org/contributor-roles/project-administration/">Project administration</role>
        <role content-type="http://credit.niso.org/contributor-roles/resources/">Resources</role>
        <role content-type="http://credit.niso.org/contributor-roles/supervision/">Supervision</role>
        <role content-type="http://credit.niso.org/contributor-roles/writing-original-draft/">Writing – original draft</role>
        <xref rid="aff002" ref-type="aff">
          <sup>2</sup>
        </xref>
        <xref rid="aff005" ref-type="aff">
          <sup>5</sup>
        </xref>
        <xref rid="cor001" ref-type="corresp">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Department of Computer Science, Johns Hopkins University, Baltimore, MD, United States of America</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Department of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, United States of America</addr-line>
    </aff>
    <aff id="aff003">
      <label>3</label>
      <addr-line>Department of Neurology, University of Wisconsin Madison, Madison, WI, United States of America</addr-line>
    </aff>
    <aff id="aff004">
      <label>4</label>
      <addr-line>Department of Neurosurgery, University of Wisconsin Madison, Madison, WI, United States of America</addr-line>
    </aff>
    <aff id="aff005">
      <label>5</label>
      <addr-line>Department of Electrical and Computer Engineering, Boston University, Boston, MA, United States of America</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>M</surname>
          <given-names>Murugappan</given-names>
        </name>
        <role>Editor</role>
        <xref rid="edit1" ref-type="aff"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Kuwait College of Science and Technology, KUWAIT</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>archanav@bu.edu</email></corresp>
    </author-notes>
    <pub-date pub-type="collection">
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>27</day>
      <month>2</month>
      <year>2023</year>
    </pub-date>
    <volume>18</volume>
    <issue>2</issue>
    <elocation-id>e0282268</elocation-id>
    <history>
      <date date-type="received">
        <day>7</day>
        <month>9</month>
        <year>2022</year>
      </date>
      <date date-type="accepted">
        <day>11</day>
        <month>2</month>
        <year>2023</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2023 Currey et al</copyright-statement>
      <copyright-year>2023</copyright-year>
      <copyright-holder>Currey et al</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0282268.pdf"/>
    <abstract>
      <p>Scalp Electroencephalography (EEG) is one of the most popular noninvasive modalities for studying real-time neural phenomena. While traditional EEG studies have focused on identifying group-level statistical effects, the rise of machine learning has prompted a shift in computational neuroscience towards spatio-temporal predictive analyses. We introduce a novel open-source viewer, the EEG Prediction Visualizer (EPViz), to aid researchers in developing, validating, and reporting their predictive modeling outputs. EPViz is a lightweight and standalone software package developed in Python. Beyond viewing and manipulating the EEG data, EPViz allows researchers to load a PyTorch deep learning model, apply it to EEG features, and overlay the output channel-wise or subject-level temporal predictions on top of the original time series. These results can be saved as high-resolution images for use in manuscripts and presentations. EPViz also provides valuable tools for clinician-scientists, including spectrum visualization, computation of basic data statistics, and annotation editing. Finally, we have included a built-in EDF anonymization module to facilitate sharing of clinical data. Taken together, EPViz fills a much needed gap in EEG visualization. Our user-friendly interface and rich collection of features may also help to promote collaboration between engineers and clinicians.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1822575</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2653-5591</contrib-id>
          <name>
            <surname>Venkataraman</surname>
            <given-names>Archana</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100000001</institution-id>
            <institution>National Science Foundation</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1845430</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2653-5591</contrib-id>
          <name>
            <surname>Venkataraman</surname>
            <given-names>Archana</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution>National Institutes of Health</institution>
        </funding-source>
        <award-id>1R21CA263804</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2653-5591</contrib-id>
          <name>
            <surname>Venkataraman</surname>
            <given-names>Archana</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This work is supported by: 1) The National Science Foundation CRCNS award 1822575 (AV) - <ext-link xlink:href="https://www.nsf.gov/" ext-link-type="uri">https://www.nsf.gov/</ext-link> 2) The National Science Foundation CAREER award 1845430 (AV) - <ext-link xlink:href="https://www.nsf.gov/" ext-link-type="uri">https://www.nsf.gov/</ext-link> 3) The National Institutes of Health award 1R21CA263804 (AV) - <ext-link xlink:href="https://www.nih.gov/" ext-link-type="uri">https://www.nih.gov/</ext-link>. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="7"/>
      <table-count count="2"/>
      <page-count count="18"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>EPViz Software: We have included three ways for users to download and install EPViz. First, users can clone our GitHub repository, which contains the most up-to-date version of the code. The repository includes information for developers about how to use EPViz along with test EDF files from the public Children’s Hospital of Boston (CHB) and Temple University Hospital (TUH) datasets [<xref rid="pone.0282268.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0282268.ref059" ref-type="bibr">59</xref>, <xref rid="pone.0282268.ref060" ref-type="bibr">60</xref>] that can be used to explore the visualizer functionality. The GitHub repository is linked on our lab webpage: {<ext-link xlink:href="https://engineering.jhu.edu/nsa/" ext-link-type="uri">https://engineering.jhu.edu/nsa/</ext-link>}. Second, EPViz is available on PyPI at {<ext-link xlink:href="https://pypi.org/project/EPViz/" ext-link-type="uri">https://pypi.org/project/EPViz/</ext-link>}. This page provides instructions on how to install EPViz in Linux, MacOS and Windows, links to our online documentation, and a summary of features and command-line options. There is also a description of the unit tests created for EPViz and instructions for running pylint on any code modifications to ensure compatibility. Third, EPViz can be downloaded as a standalone package for MacOS and Windows. This option is geared towards users with limited programming experience, who simply want to access the functionalities of EPViz. These packages are available for download at {<ext-link xlink:href="https://engineering.jhu.edu/nsa/" ext-link-type="uri">https://engineering.jhu.edu/nsa/</ext-link>}. As EPViz was built using the PyQt library, it is licensed under General Public License (GPL) 3.0. Example Use Case: The deep learning models described in our example use case can be downloaded at the following link: {<ext-link xlink:href="https://github.com/jcraley/jhu-sz-detect" ext-link-type="uri">https://github.com/jcraley/jhu-sz-detect</ext-link>}. The UW EEG data was curated as part of an institutional IRB and cannot be made publicly available at this time.</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>EPViz Software: We have included three ways for users to download and install EPViz. First, users can clone our GitHub repository, which contains the most up-to-date version of the code. The repository includes information for developers about how to use EPViz along with test EDF files from the public Children’s Hospital of Boston (CHB) and Temple University Hospital (TUH) datasets [<xref rid="pone.0282268.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0282268.ref059" ref-type="bibr">59</xref>, <xref rid="pone.0282268.ref060" ref-type="bibr">60</xref>] that can be used to explore the visualizer functionality. The GitHub repository is linked on our lab webpage: {<ext-link xlink:href="https://engineering.jhu.edu/nsa/" ext-link-type="uri">https://engineering.jhu.edu/nsa/</ext-link>}. Second, EPViz is available on PyPI at {<ext-link xlink:href="https://pypi.org/project/EPViz/" ext-link-type="uri">https://pypi.org/project/EPViz/</ext-link>}. This page provides instructions on how to install EPViz in Linux, MacOS and Windows, links to our online documentation, and a summary of features and command-line options. There is also a description of the unit tests created for EPViz and instructions for running pylint on any code modifications to ensure compatibility. Third, EPViz can be downloaded as a standalone package for MacOS and Windows. This option is geared towards users with limited programming experience, who simply want to access the functionalities of EPViz. These packages are available for download at {<ext-link xlink:href="https://engineering.jhu.edu/nsa/" ext-link-type="uri">https://engineering.jhu.edu/nsa/</ext-link>}. As EPViz was built using the PyQt library, it is licensed under General Public License (GPL) 3.0. Example Use Case: The deep learning models described in our example use case can be downloaded at the following link: {<ext-link xlink:href="https://github.com/jcraley/jhu-sz-detect" ext-link-type="uri">https://github.com/jcraley/jhu-sz-detect</ext-link>}. The UW EEG data was curated as part of an institutional IRB and cannot be made publicly available at this time.</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Scalp electroencephalography (EEG) has long been used as a window into the complex inner-workings of the human brain. Formally, EEG measures the effects of postsynaptic currents in the brain and provides real-time information about neural activity [<xref rid="pone.0282268.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0282268.ref002" ref-type="bibr">2</xref>]. Its cost-effectiveness and relative ease of acquisition has made EEG ubiquitous in both research and clinical practice. To a large extent, traditional EEG analysis has focused on group-level effects. Broadly, these studies extract quantitative features from the EEG data and use statistical testing either to identify significant differences between groups or to compute the explained variance with respect to some external measure. Common features include the amplitude and timing of evoked response potentials (ERPs) [<xref rid="pone.0282268.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0282268.ref004" ref-type="bibr">4</xref>], spectral power across the standard EEG frequency bands [<xref rid="pone.0282268.ref005" ref-type="bibr">5</xref>–<xref rid="pone.0282268.ref007" ref-type="bibr">7</xref>], quantitative metrics of the brain network organization [<xref rid="pone.0282268.ref008" ref-type="bibr">8</xref>–<xref rid="pone.0282268.ref010" ref-type="bibr">10</xref>], and spatial arrangement of ICA components [<xref rid="pone.0282268.ref004" ref-type="bibr">4</xref>, <xref rid="pone.0282268.ref011" ref-type="bibr">11</xref>]. One commonality across these methods is that they draw “static” conclusions at the level of an EEG channel or a brain network. Hence, visualization of these findings is straightforward.</p>
    <p>The rise of machine learning has spurred new directions in computational electrophysiology focused on time-varying and patient-specific predictive analyses. This paradigm shift has been accelerated by deep learning and platforms, such as PyTorch and TensorFlow, which make such techniques readily available to the research community. Two common application domains are epilepsy monitoring and brain computer interface (BCI) systems. Much of the work in epilepsy focuses on the problem of seizure detection. This setting is often cast as a binary classification problem, where the goal is to classify whether short windows (1-10 sec) of multi-channel EEG correspond to baseline or seizure activity [<xref rid="pone.0282268.ref012" ref-type="bibr">12</xref>–<xref rid="pone.0282268.ref014" ref-type="bibr">14</xref>]. The methods range from traditional machine learning algorithms applied to hand-crafted features, such as wavelet coefficients [<xref rid="pone.0282268.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0282268.ref015" ref-type="bibr">15</xref>–<xref rid="pone.0282268.ref021" ref-type="bibr">21</xref>], spectral power [<xref rid="pone.0282268.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0282268.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0282268.ref022" ref-type="bibr">22</xref>–<xref rid="pone.0282268.ref026" ref-type="bibr">26</xref>], and non-linear measures [<xref rid="pone.0282268.ref005" ref-type="bibr">5</xref>, <xref rid="pone.0282268.ref017" ref-type="bibr">17</xref>, <xref rid="pone.0282268.ref020" ref-type="bibr">20</xref>, <xref rid="pone.0282268.ref027" ref-type="bibr">27</xref>–<xref rid="pone.0282268.ref031" ref-type="bibr">31</xref>], to end-to-end deep neural networks based on convolutional and recurrent architectures [<xref rid="pone.0282268.ref032" ref-type="bibr">32</xref>–<xref rid="pone.0282268.ref044" ref-type="bibr">44</xref>]. Recent work in epilepsy has pivoted towards localizing the seizure onset from EEG, which adds a spatial component to the temporal predictions [<xref rid="pone.0282268.ref023" ref-type="bibr">23</xref>, <xref rid="pone.0282268.ref045" ref-type="bibr">45</xref>, <xref rid="pone.0282268.ref046" ref-type="bibr">46</xref>]. On the other hand, BCI systems try to decode user intent based on the EEG signals in order to control the environment [<xref rid="pone.0282268.ref047" ref-type="bibr">47</xref>]. One approach detects sensorimotor rhythms generated by motor imagery [<xref rid="pone.0282268.ref048" ref-type="bibr">48</xref>, <xref rid="pone.0282268.ref049" ref-type="bibr">49</xref>], typically by evaluating the EEG frequency content in the C3 and C4 electrodes [<xref rid="pone.0282268.ref050" ref-type="bibr">50</xref>]. Similarly, steady state visually evoked potentials measure stable responses to flickering visual stimuli [<xref rid="pone.0282268.ref051" ref-type="bibr">51</xref>]. These potentials are observed in the occipital lobe and can be detected using methods such as filterbank analysis and canonical correlation analysis [<xref rid="pone.0282268.ref052" ref-type="bibr">52</xref>].</p>
    <p>Software packages for EEG can be divided into two categories. The first category focuses on specific analytical techniques, with the visualization options for each package highly targeted towards the method under consideration. Examples include EEGLab [<xref rid="pone.0282268.ref053" ref-type="bibr">53</xref>, <xref rid="pone.0282268.ref054" ref-type="bibr">54</xref>], which is geared towards ERP analysis, EEGNet [<xref rid="pone.0282268.ref055" ref-type="bibr">55</xref>], which emphasizes brain connectivity and network analyses, and BrainStorm [<xref rid="pone.0282268.ref056" ref-type="bibr">56</xref>], which tries to link multimodal information in a common reference space. While these software packages represent seminal contributions to the field, none of them are geared towards viewing the results of time-varying and spatially-varying predictive analyses. The second category of software includes EEG viewers that display and manipulate the raw time series data. The most popular viewer is EDFBrowser [<xref rid="pone.0282268.ref057" ref-type="bibr">57</xref>], which provides a wide range of preprocessing, display, and annotation functionalities. While EDFBrowser is and will remain a valuable resource to the community, it has some notable limitations. For example, the large number of tools makes the interface clunky and difficult to navigate. In addition, EDF Browser does not have native support for visualizing model predictions, a need that is growing in popularity with machine learning analyses.</p>
    <p>In this paper, we introduce the EEG Prediction Visualizer (EPViz), a lightweight and flexible EEG viewer that complements existing software resources in the field. EPViz is targeted towards machine learning applications and is built around four core functionalities: (1) displaying and manipulating the multi-channel EEG time series, (2) running PyTorch deep learning models on the data, (3) overlaying channel-wise and time-varying predictions on top of the EEG time series, and (4) saving high-quality images of the results. In addition, EPViz includes basic preprocessing operations, spectral feature extraction, and annotation editing. Finally, EPViz has a built-in anonymizer to facilitate sharing of clinical EEG data between clinicians and engineers. EPViz is freely available for download at <ext-link xlink:href="https://engineering.jhu.edu/nsa/links/" ext-link-type="uri">https://engineering.jhu.edu/nsa/links/</ext-link>.</p>
  </sec>
  <sec sec-type="materials|methods" id="sec002">
    <title>Materials and methods</title>
    <p>EPViz is a streamlined viewer designed for predictive modeling applications. EPViz is built using the PyQt package (5.15.4) in Python. PyQt allows for easy integration with a range of Python deep learning and machine learning libraries.</p>
    <p>The multi-channel EEG data is plotted using the PyQtGraph package, which provides fast updating and real-time user interaction capabilities. The PyEDFlib package is used for loading EDF files, and the Matplotlib package is used for saving high-quality images. Finally, the MNE package [<xref rid="pone.0282268.ref058" ref-type="bibr">58</xref>] is used to generate a 2-D topographic map of channel-wise model predictions on the scalp for enhanced visualization capabilities. This representation is also known as a topoplot.</p>
    <sec id="sec003">
      <title>Overview of the GUI</title>
      <p><xref rid="pone.0282268.g001" ref-type="fig">Fig 1</xref> illustrates the EPViz graphical user interface. The “Select File” button allows the user to load an EDF file containing multi-channel EEG data. The popup window asks the user to select which channels to plot. We have included the standard 10-10, 10-20 and bipolar 10-20 montages as preset selections. The user also has the option to load a custom EEG montage via a separate text file.</p>
      <fig position="float" id="pone.0282268.g001">
        <object-id pub-id-type="doi">10.1371/journal.pone.0282268.g001</object-id>
        <label>Fig 1</label>
        <caption>
          <title>Main GUI window.</title>
          <p>The signal organization window can be used to change the order of signals (top left). This window is opened from the signal selection window. Also shown are the filtering and image saving windows. The main window includes a leftside-panel with various options and the main signal plot to the right.</p>
        </caption>
        <graphic xlink:href="pone.0282268.g001" position="float"/>
      </fig>
      <p>The EEG signals appear in the main display pane. Signals from the default montages are color-coded according to hemisphere (red for left, blue for right, and green for the midline). This is in contrast to EDFBrowser, which defaults to plotting all signals in black. Users can change the ordering and number of plotted signals using the “Change Signals” button. Annotations in the EDF files are plotted as “Notes” at the bottom of the display pane. These are particularly relevant for clinical EEG data. Users can vary the time scale of the plot (1, 5, 10, 20, 25, 30, or 45 seconds) using the “Change Window Size” button. Likewise, they can change the intensity scale via the “Change Amplitude” button. Finally, the “Open Zoom” button allows the user to zoom in on a selected region of the plotting window.</p>
      <p>EPViz includes basic filtering operations. The high- and low-pass parameters, implemented using the SciPy library, can be set in the “Change Filter” pop-up. To allow for real-time updating, only the region shown on the screen is filtered. These filtering operations mimic those used in epilepsy and BCI applications. More complex preprocessing, such as ICA, should be done offline prior to loading the file into EPViz.</p>
    </sec>
    <sec id="sec004">
      <title>Obtaining and displaying temporal predictions</title>
      <p>EPViz supports two types of predictions. The first is a continuous value between [0, 1], corresponding a soft binary assignment. By default, EPViz assumes that “0” is the baseline condition and “1” is the condition of interest. The second is a categorical assignment into one of <italic toggle="yes">K</italic> classes plus a default class of “0” again denoting a baseline condition outside of the main assignments.</p>
      <p>The user can load predictions in one of two ways. The first method is via an auxiliary file. The file should either contain a single row, corresponding to a subject-level prediction for each time point, or contain the same number of rows as plotted EEG channels, corresponding to a channel-wise prediction for each time point. The second method is by loading a pre-trained PyTorch model and running it directly on loaded data. Here, the PyTorch model should generate an output that is an integer modulo the number of samples in the signal. This format accounts for models that generate window-wise predictions across short (e.g., 1-10 sec) snippets of the full EEG recordings.</p>
      <sec id="sec005">
        <title>Figure generation and export</title>
        <p><xref rid="pone.0282268.g002" ref-type="fig">Fig 2</xref> illustrates the model predictions in the main display pane. As seen, the predictions are overlaid in a light cyan across the appropriate channels. In the case of binary classification, the detection threshold can be swept using the built-in slider bar. This strategy allows users to identify salient features of the underlying EEG that may coincide with the predictions. Not only does this mimic clinical review of scalp EEG data, but it may facilitate interpretability of the corresponding algorithms.</p>
        <fig position="float" id="pone.0282268.g002">
          <object-id pub-id-type="doi">10.1371/journal.pone.0282268.g002</object-id>
          <label>Fig 2</label>
          <caption>
            <title>Visualizing predictions.</title>
            <p>Model predictions are shown in light cyan across the channels. Since multi-channel predictions are plotted, topoplots have been generated.</p>
          </caption>
          <graphic xlink:href="pone.0282268.g002" position="float"/>
        </fig>
        <p>Going one step further, EPViz can display multi-channel predictions on a topological scalp plot (topoplot). The topoplots are generated using the MNE <monospace>plot_topomap</monospace> function. The user can select the time point to display by moving a black vertical line on the main display pane. Once positioned, the topoplot is automatically updated as the user scrolls through the EEG data.</p>
        <p>Finally, the “Save to .png” option allows the user to export a high-quality image of the main display pane. Here, the user first selects the desired options (filtering signals, overlaying predictions, adding annotations) and proceeds to an image editor. The editor allows them to change the plot title, the EEG signal thickness, and the text font size. Topoplots can be exported to an image using the “Save topoplot” button. Users can toggle subplot titles which default to the times for each topoplot. The Matplotlib package is used for exporting the * .png files.</p>
      </sec>
    </sec>
    <sec id="sec006">
      <title>Other functionalities</title>
      <sec id="sec007">
        <title>Data statistics and spectrum</title>
        <p>EPViz computes and displays basic statistics of the EEG data. These include signal mean, variance, and line length. Line length is computed as the sum of distances between consecutive time points of the signal; it is a particularly useful metric in EEG analysis [<xref rid="pone.0282268.ref029" ref-type="bibr">29</xref>]. Beyond these time-domain features, EPViz computes the power within the standard EEG frequency bands: delta (1-4 Hz), theta (4-8 Hz), alpha (8-14 Hz), beta (14-30 Hz), and gamma (30-45 Hz). As shown in <xref rid="pone.0282268.g003" ref-type="fig">Fig 3</xref>, the user can control the channel and time interval over which the statistics are computed by moving the red rectangle.</p>
        <fig position="float" id="pone.0282268.g003">
          <object-id pub-id-type="doi">10.1371/journal.pone.0282268.g003</object-id>
          <label>Fig 3</label>
          <caption>
            <title>Annotation editor and statistics.</title>
            <p>The dock on the right side of the plot contains the annotation editor (top) and signal statistics (bottom). The red rectangle on the main plot can be used to select a region over which statistics will be computed.</p>
          </caption>
          <graphic xlink:href="pone.0282268.g003" position="float"/>
        </fig>
        <p>EPViz also plots the spectrogram of a selected EEG channel. The spectrogram is extracted based on the Fast Fourier Transform magnitude. This time-frequency representation is popular in many EEG applications [<xref rid="pone.0282268.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0282268.ref007" ref-type="bibr">7</xref>, <xref rid="pone.0282268.ref022" ref-type="bibr">22</xref>–<xref rid="pone.0282268.ref026" ref-type="bibr">26</xref>]. Users can toggle the spectrogram via the “Power Spectrum” button. EPViz computes and displays basic statistics of the EEG data. These include signal mean, variance, and line length. As its name suggests, line length is computed as the sum of distances between consecutive time points of the signal; it is a particularly useful metric in EEG analysis. Beyond these time-domain features, EPViz computes the power within the standard EEG frequency bands: delta (1-4 Hz), theta (4-8 Hz), alpha (8-14 Hz), beta (14-30 Hz), and gamma (30-45 Hz). As shown in <xref rid="pone.0282268.g003" ref-type="fig">Fig 3</xref>, the user can control the channel and time interval over which the statistics are computed by moving the red rectangle.</p>
      </sec>
      <sec id="sec008">
        <title>Annotation editor</title>
        <p>The EDF file format provides a means to store text annotations linked to specific time points of the data. Annotations are often added by clinicians to flag salient events, for example during epilepsy monitoring. The are also useful in research studies to indicate the timing of different stimuli or experimental conditions. Similar to EDFBrowser, our EPViz includes tools for extracting and modifying textual annotations. Specifically, the annotations are displayed both as a list on the right of the window and in the “Notes” row of the main display pane. As seen in <xref rid="pone.0282268.g003" ref-type="fig">Fig 3</xref>, we have also included an annotation editor that lets the user both modify the text of existing annotations and add new annotations. Changes made using the annotation editor will not persist outside of EPViz unless they are saved into a new EDF file.</p>
      </sec>
      <sec id="sec009">
        <title>Clinical anonymization</title>
        <p>To facilitate the sharing of clinical data, EPViz includes a built-in anonymizer to strip identifiable information from the EDF file prior to it being saved. <xref rid="pone.0282268.g004" ref-type="fig">Fig 4</xref> shows the annonymization window. Here, users can opt for the default setting, which removes names and dates from the EDF header, or they can selectively edit each field themselves. The Python code underlying the anonymizer has been verified by the University of Wisconsin (UW) Madison Institutional Review Board and is currently being used for data sharing between UW Madison and Johns Hopkins University.</p>
        <fig position="float" id="pone.0282268.g004">
          <object-id pub-id-type="doi">10.1371/journal.pone.0282268.g004</object-id>
          <label>Fig 4</label>
          <caption>
            <title>Anonymization window.</title>
            <p>This option can be used to strip identifiable information from the EDF file prior to saving.</p>
          </caption>
          <graphic xlink:href="pone.0282268.g004" position="float"/>
        </fig>
        <p>For convenience, the EEG data and the overlaid model predictions can be saved into a single EDF file. This file can easily be re-loaded into the EPViz for further analysis and visualization. Logistically, the model predictions are stored as a new data channel and should not interfere with other EEG software packages.</p>
      </sec>
      <sec id="sec010">
        <title>Command line options</title>
        <p>We have provided command-line support for figure generation of the main display pane (EEG signals) and for data anonymization. These command-line option can be integrated into batch processing pipelines and are particularly useful for comparing different predictive modeling outputs. In summary, EPViz is built so that people with and without technical expertise are able to easily interact with its tools.</p>
      </sec>
    </sec>
    <sec id="sec011">
      <title>Development and release</title>
      <sec id="sec012">
        <title>Software testing procedures</title>
        <p>To ensure a smooth user experience, we have added unit testing for EPViz. Our tests cover the main functionalities of the visualizer (e.g., loading, plotting, manipulating, and saving data/images) with extensive coverage of the corresponding source files. Exceptions included purely UI functionalities, which require constant user interaction. <xref rid="pone.0282268.t001" ref-type="table">Table 1</xref> shows the code coverage of our unit tests. We have also created a GitHub Action to run the unit tests on each pull request to encourage high-quality code integration.</p>
        <table-wrap position="float" id="pone.0282268.t001">
          <object-id pub-id-type="doi">10.1371/journal.pone.0282268.t001</object-id>
          <label>Table 1</label>
          <caption>
            <title>Unit testing code coverage for EPViz source files.</title>
          </caption>
          <alternatives>
            <graphic xlink:href="pone.0282268.t001" id="pone.0282268.t001g" position="float"/>
            <table frame="box" rules="all" border="0">
              <colgroup span="1">
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
                <col align="left" valign="middle" span="1"/>
              </colgroup>
              <thead>
                <tr>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Functionality</th>
                  <th align="left" style="border-bottom-width:thick;border-right-width:thick" rowspan="1" colspan="1">File</th>
                  <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">Coverage (%)</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td align="center" rowspan="2" colspan="1">Main plot</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">plot.py</td>
                  <td align="center" rowspan="1" colspan="1">82</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">plot_utils.py</td>
                  <td align="center" rowspan="1" colspan="1">93</td>
                </tr>
                <tr>
                  <td align="center" rowspan="3" colspan="1">EDF saving</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">anonymizer.py</td>
                  <td align="center" rowspan="1" colspan="1">91</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">saveEdf_info.py s</td>
                  <td align="center" rowspan="1" colspan="1">100</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">saveEdf_options.py s</td>
                  <td align="center" rowspan="1" colspan="1">100</td>
                </tr>
                <tr>
                  <td align="center" rowspan="1" colspan="1">Filtering</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">filter_options.py</td>
                  <td align="center" rowspan="1" colspan="1">100</td>
                </tr>
                <tr>
                  <td align="center" rowspan="2" colspan="1">Predictions</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">prediction_info.py</td>
                  <td align="center" rowspan="1" colspan="1">82</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">prediction_options.py</td>
                  <td align="center" rowspan="1" colspan="1">74</td>
                </tr>
                <tr>
                  <td align="center" rowspan="1" colspan="1">Preprocessing</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">edf_loader.py</td>
                  <td align="center" rowspan="1" colspan="1">100</td>
                </tr>
                <tr>
                  <td align="center" rowspan="2" colspan="1">Signal loading</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">channel_info.py</td>
                  <td align="center" rowspan="1" colspan="1">93</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">channel_options.py</td>
                  <td align="center" rowspan="1" colspan="1">78</td>
                </tr>
                <tr>
                  <td align="center" rowspan="2" colspan="1">Signal statistics</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">signalStats_info</td>
                  <td align="center" rowspan="1" colspan="1">100</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">signalStats_options</td>
                  <td align="center" rowspan="1" colspan="1">92</td>
                </tr>
                <tr>
                  <td align="center" rowspan="1" colspan="1">Spectrograms</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">spec_options.py</td>
                  <td align="center" rowspan="1" colspan="1">99</td>
                </tr>
                <tr>
                  <td align="center" rowspan="2" colspan="1">Image saving</td>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">saveImg_options.py</td>
                  <td align="center" rowspan="1" colspan="1">81</td>
                </tr>
                <tr>
                  <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">saveTopoplot_options.py</td>
                  <td align="center" rowspan="1" colspan="1">93</td>
                </tr>
              </tbody>
            </table>
          </alternatives>
        </table-wrap>
        <p>Finally, we have used Pylint (<ext-link xlink:href="https://pylint.pycqa.org/en/latest/" ext-link-type="uri">https://pylint.pycqa.org/en/latest/</ext-link>) during the development process to ensure that our EPViz source code conforms to the PEP 8 style guidelines for Python.</p>
      </sec>
      <sec id="sec013">
        <title>Software dissemination</title>
        <p>We have included three ways for users to download and install EPViz. First, users can clone our GitHub repository, which contains the most up-to-date version of the code. The repository includes information for developers about how to use EPViz along with test EDF files from the public Children’s Hospital of Boston (CHB) and Temple University Hospital (TUH) datasets [<xref rid="pone.0282268.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0282268.ref059" ref-type="bibr">59</xref>, <xref rid="pone.0282268.ref060" ref-type="bibr">60</xref>] that can be used to explore the visualizer functionality. The GitHub repository is linked on our lab webpage: <ext-link xlink:href="https://engineering.jhu.edu/nsa/" ext-link-type="uri">https://engineering.jhu.edu/nsa/</ext-link>.</p>
        <p>Second, EPViz is available on PyPI at <ext-link xlink:href="https://pypi.org/project/EPViz" ext-link-type="uri">https://pypi.org/project/EPViz</ext-link>. This page provides instructions on how to install EPViz in Linux, MacOS and Windows, links to our online documentation, and a summary of features and command-line options. There is also a description of the unit tests created for EPViz and instructions for running pylint on any code modifications to ensure compatibility.</p>
        <p>Third, EPViz can be downloaded as a standalone package for MacOS and Windows. This option is geared towards users with limited programming experience, who simply want to access the functionalities of EPViz. These packages are available for download at <ext-link xlink:href="https://engineering.jhu.edu/nsa/" ext-link-type="uri">https://engineering.jhu.edu/nsa/</ext-link>.</p>
        <p>EPViz is licensed under General Public License (GPL) 3.0.</p>
      </sec>
      <sec id="sec014">
        <title>Online documentation</title>
        <p>We have created an extensive online documentation page for EPViz, which can be accessed via our lab website at: <ext-link xlink:href="https://engineering.jhu.edu/nsa/epviz/" ext-link-type="uri">https://engineering.jhu.edu/nsa/epviz/</ext-link>. The documentation includes a short video of EPViz, followed by detailed information about each of the main features and tips to help users interact with the visualizer. The user can also download the test EDF files in our GitHub repository and follow along with a step-by-step demo at the end of the page. This demo reviews a common use case for EPViz including loading a file, selecting a montage, loading predictions, navigating through the signal time series, saving a figure, and anonymizing the file. These steps are also covered in the linked video.</p>
      </sec>
    </sec>
  </sec>
  <sec id="sec015">
    <title>Real-world use case: Seizure detection</title>
    <sec id="sec016">
      <title>Data</title>
      <p>We demonstrate the real-world utility of EPViz via a seizure detection experiment. Our scalp EEG dataset was acquired by University of Wisconsin-Madison pediatric epilepsy monitoring unit. It includes 192 EEG recordings in the standard 10-20 montage [<xref rid="pone.0282268.ref061" ref-type="bibr">61</xref>] from 16 patients for a total of 100 seizures. There are an average of 6.25 seizures per patient (min = 1 seizure, max = 33 seizures). The total recording time is 33.1 hours with an average of 124 minutes of recorded EEG per patient, as seen in <xref rid="pone.0282268.g005" ref-type="fig">Fig 5</xref>. The EEG data was sampled at 256 Hz and resampled to 200 Hz for the purpose of analysis. The EEG was annotated for seizure onset and offset by a clinician using video-EEG.</p>
      <fig position="float" id="pone.0282268.g005">
        <object-id pub-id-type="doi">10.1371/journal.pone.0282268.g005</object-id>
        <label>Fig 5</label>
        <caption>
          <title>UWM dataset recording times.</title>
          <p>Total time in minutes of EEG included for each patient in the UWM dataset.</p>
        </caption>
        <graphic xlink:href="pone.0282268.g005" position="float"/>
      </fig>
    </sec>
    <sec id="sec017">
      <title>Experimental setup</title>
      <sec id="sec018">
        <title>Predictive models</title>
        <p>We compare the performance of eight different methods for seizure detection drawn from recently published work. These methods encompass a range of feature extraction and machine learning techniques. The inputs to each method are one-second windows of multichannel EEG data, provided as a sequence. The outputs are window-level predictions of seizure versus baseline activity. Here, we provide a concise summary of each method and refer the reader to the citations below for additional details.</p>
        <list list-type="bullet">
          <list-item>
            <p><bold>CNN-BLSTM</bold>: Introduced in [<xref rid="pone.0282268.ref032" ref-type="bibr">32</xref>], this deep learning architecture couples a convolutional neural network (CNN) feature encoder with a recurrent bidirectional long short-term memory (BLSTM) classifier.</p>
          </list-item>
          <list-item>
            <p><bold>CNN-MLP</bold>: Proposed as a baseline for the CNN-BLSTM [<xref rid="pone.0282268.ref032" ref-type="bibr">32</xref>], this method uses the same convolutional encoder, but replaces the BLSTM with a multi-layer perceptron that operates independently on each one-second window of the EEG.</p>
          </list-item>
          <list-item>
            <p><bold>Wei-CNN</bold>: Developed by [<xref rid="pone.0282268.ref041" ref-type="bibr">41</xref>], this is a fully-convolutional deep learning method that uses a single CNN to obtain window-wise seizure versus baseline predictions.</p>
          </list-item>
          <list-item>
            <p><bold>CNN-2D</bold>: Also used a baseline in [<xref rid="pone.0282268.ref032" ref-type="bibr">32</xref>], this method concatenates the FFT of the channel-wise EEG signals into a 2D matrix and operates on it like an image. This method is inspired by the works of [<xref rid="pone.0282268.ref034" ref-type="bibr">34</xref>, <xref rid="pone.0282268.ref037" ref-type="bibr">37</xref>], which use a similar strategy.</p>
          </list-item>
          <list-item>
            <p><bold>MLP</bold>-<inline-formula id="pone.0282268.e001"><alternatives><graphic xlink:href="pone.0282268.e001.jpg" id="pone.0282268.e001g" position="anchor"/><mml:math id="M1" display="inline" overflow="scroll"><mml:mrow><mml:mtext mathvariant="sans-serif">XXX</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>: These three methods rely on hand-crafted features extracted channel-wise from the one-second windows as described in [<xref rid="pone.0282268.ref045" ref-type="bibr">45</xref>]. The “time” features consist of sample entropy, signal energy, line length, and largest Lyapunov exponent. The “filterbank” features consist of spectral power in different EEG frequency bands. The classification is performed by a multi-layer perceptron.</p>
          </list-item>
          <list-item>
            <p><bold>Kaleem-SVM</bold>: As introduced in [<xref rid="pone.0282268.ref018" ref-type="bibr">18</xref>], this method operates on the combined time and filterbank features described above but uses a support vector machine (SVM) classifier instead of a deep neural network.</p>
          </list-item>
        </list>
        <p>Training and calibration is performed according to [<xref rid="pone.0282268.ref032" ref-type="bibr">32</xref>]. For the CNN-BLSTM, the CNN encoder is pre-trained on individual windows for 10 epochs prior to joint training of the full architecture. The outputs for each method are averaged 20 consecutive windows to reduce noise in the final predictions. Likewise, the seizure detection thresholds for each method are independently set to allow only two minutes of false positive seizure classifications per hour <italic toggle="yes">on the training data</italic>.</p>
        <p>Finally, we note that our objective in this study is to demonstrate how EPViz can be used to visualize the results of a real-world predictive analysis, rather than to advocate for any particular seizure detection method. Thus, we have selected models that are simple to implement and train, while still being current in the field.</p>
      </sec>
      <sec id="sec019">
        <title>Performance metrics</title>
        <p>We evaluate performance at the level of one-second EEG windows and at the level of whole seizures. In the former case, we treat the window-level seizure versus baseline predictions as independent outputs and compute the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) and the Area Under the Precision-Recall Curve (AUC-PR). These metrics capture the behavior when varying the detection threshold. We also compute sensitivity and specificity using the detection thresholds obtained during callibration (see previous section). In the latter (whole seizure) case, we first determine the intervals of contiguous seizure classification based on the callibrated detection thresholds. Any interval that intersect a clinician-annotated seizure is considered a true positive detection; the remaining are considered false positive detections. From here, we quantify the duration of false positive detections, the sensitivity (true positives divided by total number of seizures), and the onset latency.</p>
      </sec>
      <sec id="sec020">
        <title>Experimental results</title>
        <p><xref rid="pone.0282268.g006" ref-type="fig">Fig 6</xref> reports the seizure detection results. The box plots are constructed by first averaging the performance metrics across all seizures for a given patient and then plotting the distribution of these averages across patients. Note that the CNN-BLSTM and CNN-MLP have the best median performance at the window level, which suggests that the convolutional encoder is key to learning discriminative representations from the data. At the seizure level, while the deep learning methods achieve similar sensitivity and onset latency, the CNN-BLSTM shines with respect to low false positives. This observation indicates that (1) the calibration thresholds set during training are more likely to generalize to testing data for the CNN-BLSTM, and (2) the temporal modeling of this architecture may help to suppress less certain seizure predictions.</p>
        <fig position="float" id="pone.0282268.g006">
          <object-id pub-id-type="doi">10.1371/journal.pone.0282268.g006</object-id>
          <label>Fig 6</label>
          <caption>
            <title>Seizure detection performance on the UWM dataset.</title>
            <p>Box plots capture the average performance for each metric across the 16 patients. The orange bar denotes the median performance, boxes correspond to the inter-quartile range, and the whiskers denote the 10th and 90th percentiles. <bold>Top</bold>: Window level results are calculated on one-second windows of EEG. <bold>Bottom</bold>: Seizure level results are calculated over the duration of the seizure period.</p>
          </caption>
          <graphic xlink:href="pone.0282268.g006" position="float"/>
        </fig>
        <p><xref rid="pone.0282268.g007" ref-type="fig">Fig 7</xref> demonstrates the utility of EPViz in comparing different methods. Here, the unsmoothed seizure detections for each model are superimposed in blue over the original EEG signal, here displayed using the longitudinal bipolar montage. Annotated seizure onset is displayed using the dashed vertical line at 739 seconds. The CNN-BLSTM makes a continuous seizure detection with low latency. With the exception of the MLP-Time Domain, CNN and MLP models make non-continuous seizure classifications, alternating between detected seizure and periods of baseline. The Kaleem-SVM misses the seizure entirely. Beyond this example, EPViz can be used to compare model behaviors during baseline activity and seizure offset. Researchers can also use these images to identify confounding features in the underlying EEG signals, then can be re-incorporated into their models for improved performance.</p>
        <fig position="float" id="pone.0282268.g007">
          <object-id pub-id-type="doi">10.1371/journal.pone.0282268.g007</object-id>
          <label>Fig 7</label>
          <caption>
            <title>EPViz visualizations of seizure detections from each model.</title>
            <p>Detected seizures are shown superimposed in blue on the EEG. Annotated seizure onset at 739 seconds is shown by the vertical dashed line. The CNN-BLSTM shows a contiguous seizure detection with low latency while alternative models show higher latency, non-continuous detection, or misses.</p>
          </caption>
          <graphic xlink:href="pone.0282268.g007" position="float"/>
        </fig>
        <p>Finally, we note that the seizure detection performance in the UWM dataset is lower than the performance of the models reported in [<xref rid="pone.0282268.ref032" ref-type="bibr">32</xref>] using the CHB dataset [<xref rid="pone.0282268.ref025" ref-type="bibr">25</xref>, <xref rid="pone.0282268.ref026" ref-type="bibr">26</xref>]. This trend may be partially attributed to the smaller sample size of the UWM dataset. In addition, the CHB dataset contains primarily generalized seizures, whereas the UWM dataset contains a more heterogeneous focal epilepsy cohort.</p>
      </sec>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec021">
    <title>Discussion</title>
    <sec id="sec022">
      <title>Comparison between EPViz and existing tools</title>
      <p>EPViz provides a streamlined interface that allows users to easily interact with the EEG data and visualize model predictions. Accordingly, EPViz fills an unoccupied niche amongst other EEG visualization and analyses tools. Perhaps the most widely used too is EDF Browser [<xref rid="pone.0282268.ref057" ref-type="bibr">57</xref>], which provides similar EDF loading and manipulation capabilities. EDF Browser also has a large suite of analytical tools, from filtering operations to spectral analysis [<xref rid="pone.0282268.ref057" ref-type="bibr">57</xref>]. While EDF Browser will remain a powerhouse in the EEG community, we believe that EPViz provides key functionality not incorporated in EDF Browser. First, EPViz is a streamlined and user-friendly application, which makes it easy for non-technical users to adopt. Second, it is geared towards spatio-temporal predictive analyses; the predictive overlay and topoplots are currently not supported in EDF Browser. Finally, it generates high-quality images of the data and results, which can be used in scientific publications and presentations.</p>
      <p>There are also a variety of EEG software toolboxes developed for Matlab. For example, the popular EEGLab focuses on independent component analysis (ICA) and other time-frequency techniques [<xref rid="pone.0282268.ref053" ref-type="bibr">53</xref>, <xref rid="pone.0282268.ref054" ref-type="bibr">54</xref>]. It also provides a GUI for visualizing events detected by these methods. In contrast, EEGNET provides tools for functional connectivity analysis [<xref rid="pone.0282268.ref055" ref-type="bibr">55</xref>] and includes a pipeline for the relevant preprocessing to construct an EEG connectome. Finally, BrainStorm registers multiple data formats including MEG, EEG, and MRI producing visualizations and analysis [<xref rid="pone.0282268.ref056" ref-type="bibr">56</xref>]. While these packages provide some overlapping functionality to EPViz, they do not lend themselves towards predictive analysis, like our PyTorch integration. They also rely on Matlab, which is an expensive and not-universally available platform. In contrast, EPViz is based on open-source Python packages and is freely available to the community. <xref rid="pone.0282268.t002" ref-type="table">Table 2</xref> summarizes the key features offered by EPViz, as compared to existing software packages. As seen, EPViz fills a much needed gap in EEG visualization.</p>
      <table-wrap position="float" id="pone.0282268.t002">
        <object-id pub-id-type="doi">10.1371/journal.pone.0282268.t002</object-id>
        <label>Table 2</label>
        <caption>
          <title>Summary of features provided by EPViz, as compared to existing EEG software.</title>
        </caption>
        <alternatives>
          <graphic xlink:href="pone.0282268.t002" id="pone.0282268.t002g" position="float"/>
          <table frame="box" rules="all" border="0">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" style="border-bottom-width:thick;border-right-width:thick" rowspan="1" colspan="1"/>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">EPViz</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">EDF Browser</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">EEGLab</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">EEGNET</th>
                <th align="center" style="border-bottom-width:thick" rowspan="1" colspan="1">BrainStorm</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">View EEG Signals</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
              </tr>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Compute Summary Statistics</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
              </tr>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Annotate Data</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1">✓</td>
              </tr>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">PyTorch Integration</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Overlay Predictions</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">Save High-Res Images</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1"/>
              </tr>
              <tr>
                <td align="left" style="border-right-width:thick" rowspan="1" colspan="1">License-Free</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1">✓</td>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1"/>
                <td align="center" rowspan="1" colspan="1">✓</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>Finally, EPViz leverages the MNE library [<xref rid="pone.0282268.ref058" ref-type="bibr">58</xref>] to produce topoplots in a user-friendly manner. While the native MNE library provides many visualization, preprocessing, and analysis tools, using them requires advanced scripting knowledge.</p>
    </sec>
    <sec id="sec023">
      <title>Application domains</title>
      <p>EPViz can be used in a variety of clinical and research applications, where the goal is to detect an event from the EEG data. One natural domain is epilepsy. In fact, the experimental testbed in this paper uses EPViz to compare the seizure detection performance across different machine learning methods. Other phenomenon of interest are auras and non-epileptic events, both of which can be detected using a similar training and evaluation strategy. Going one step further, EPViz supports channel-wise predictions, which makes it a natural tool for <italic toggle="yes">seizure localization</italic> studies [<xref rid="pone.0282268.ref045" ref-type="bibr">45</xref>, <xref rid="pone.0282268.ref046" ref-type="bibr">46</xref>], where the goal is to identify a specific area of onset (e.g., lobe and/or hemisphere) and track the seizure activity as it propagates from that location.</p>
      <p>Another application domain is BCI. Once again, some studies try to predict subject-level events, such as viewing a particular stimuli, while others zone in on specific EEG channels to tease apart the activity.</p>
      <p>Finally, even though we have focused this paper on predictive analytics, the features of EPViz can be used to emphasize other aspects of the data. Recall that EPViz can overlay “predictions” contained in an auxiliary file. Hence, the user can create “predictions” that correspond to different experimental conditions. Another option is to create “predictions” that select certain EEG channels and time intervals based on an ERP analysis. Thus, EPViz is a flexible tool that users can adapt for their own needs.</p>
    </sec>
    <sec id="sec024">
      <title>Limitations and future work</title>
      <p>EPViz is currently designed to load and manipulate a single EEG recording. This setup makes it amenable to the <italic toggle="yes">testing</italic> phase of machine learning approaches, i.e., evaluating the performance of a model on new data. However, EPViz cannot be used for <italic toggle="yes">model training</italic>, which would require processing multiple EEG recordings at a time. Future work will integrate command-line options into EPViz for the user to <italic toggle="yes">train</italic> a PyTorch deep learning model given a data directory and subject ID list. This trained model can be loaded back into EPViz and applied to new EEG data using the existing functionality. Along the same lines, we will add an option for users to load multiple EEG recordings and toggle between them in the main display pane.</p>
      <p>From a visualization standpoint, EPViz is optimized for the 10-20 electrode placement system [<xref rid="pone.0282268.ref061" ref-type="bibr">61</xref>]. While the user can load data from other montages, the signals will not be colored according to hemisphere, and EPViz will not generate a topoplot since the electrode placements are not provided in the EDF file. Future work will tackle this issue by allowing the user to add the electrode positions, hemisphere information, and desired ordering to the auxiliary text file mentioned above.</p>
      <p>Along the same lines, EPViz has difficulty displaying more than 50 EEG signals at a time. Not only are the signals difficult to see, but the plot updates much more slowly after user interaction (e.g., signal filtering, scrolling through time, zoom functionality). To address this issue, we will integrate a memory management system that allows EPViz to efficiently cache and update data as needed. We will also add a pop-out window for the main display pane to accommodate the additional EEG signals.</p>
      <p>Finally, EPViz is configured to apply the loaded models to the entire EEG recording to obtain predictions. While suitable for research purposes, clinicians desire real-time analysis capabilities to assist in their review of continuous recordings. We will explore such add-ons to EPViz in the future. These will likely rely on memory management systems and closer integration with existing clinical software packages.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec025">
    <title>Conclusion</title>
    <p>We have introduced EPViz, a lightweight and user-friendly visualizer for EEG data. EPViz is designed for predictive modeling applications, which are becoming increasingly popular in EEG research. Specifically, EPViz allows the user to generate and overlay predictions on top of the EEG signals, thus providing a mechanism to interpret the model output with respect to the data. EPViz can also generate high-quality images of the predictive modeling outputs to aid in scientific reporting [<xref rid="pone.0282268.ref062" ref-type="bibr">62</xref>]. EPViz is completely open-source and uses Python, which is the fastest-growing programming language for machine learning. Finally, EPViz has been designed for both engineers and clinician-scientists. In particular, we have included spectrogram visualization, which is often used in clinical review of EEG, and a built-in anonymizer to remove identifiable information from the EDF files. EPViz has helped our own team to build an interdisciplinary and inter-institutional collaboration in epilepsy. We hope that it will promote such collaborations for other researchers.</p>
  </sec>
</body>
<back>
  <ref-list>
    <title>References</title>
    <ref id="pone.0282268.ref001">
      <label>1</label>
      <mixed-citation publication-type="book"><name><surname>Marcuse</surname><given-names>LV</given-names></name>, <name><surname>Fields</surname><given-names>MC</given-names></name>, <name><surname>Yoo</surname><given-names>JJ</given-names></name>. <source>Rowan’s Primer of EEG E-Book</source>. <publisher-name>Elsevier Health Sciences</publisher-name>; <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref002">
      <label>2</label>
      <mixed-citation publication-type="book"><name><surname>Krauss</surname><given-names>GL</given-names></name>, <name><surname>Fisher</surname><given-names>RS</given-names></name>. <source>The Johns Hopkins atlas of digital EEG: an interactive training guide</source>. <publisher-name>Johns Hopkins University Press</publisher-name>; <year>2006</year>.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Mitzdorf</surname><given-names>U</given-names></name>. <article-title>Current source-density method and application in cat cerebral cortex: investigation of evoked potentials and EEG phenomena</article-title>. <source>Physiological reviews</source>. <year>1985</year>;<volume>65</volume>(<issue>1</issue>):<fpage>37</fpage>–<lpage>100</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1152/physrev.1985.65.1.37</pub-id><?supplied-pmid 3880898?><pub-id pub-id-type="pmid">3880898</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref004">
      <label>4</label>
      <mixed-citation publication-type="book"><name><surname>Luck</surname><given-names>SJ</given-names></name>, <name><surname>Kappenman</surname><given-names>ES</given-names></name>. <source>The Oxford handbook of event-related potential components</source>. <publisher-name>Oxford university press</publisher-name>; <year>2011</year>.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Adeli</surname><given-names>H</given-names></name>, <name><surname>Ghosh-Dastidar</surname><given-names>S</given-names></name>, <name><surname>Dadmehr</surname><given-names>N</given-names></name>. <article-title>A wavelet-chaos methodology for analysis of EEGs and EEG subbands to detect seizure and epilepsy</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2007</year>;<volume>54</volume>(<issue>2</issue>):<fpage>205</fpage>–<lpage>211</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2006.886855</pub-id><?supplied-pmid 17278577?><pub-id pub-id-type="pmid">17278577</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Bandarabadi</surname><given-names>M</given-names></name>, <name><surname>Teixeira</surname><given-names>CA</given-names></name>, <name><surname>Rasekhi</surname><given-names>J</given-names></name>, <name><surname>Dourado</surname><given-names>A</given-names></name>. <article-title>Epileptic seizure prediction using relative spectral power features</article-title>. <source>Clinical Neurophysiology</source>. <year>2015</year>;<volume>126</volume>(<issue>2</issue>):<fpage>237</fpage>–<lpage>248</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.clinph.2014.05.022</pub-id><?supplied-pmid 24969376?><pub-id pub-id-type="pmid">24969376</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Logesparan</surname><given-names>L</given-names></name>, <name><surname>Casson</surname><given-names>AJ</given-names></name>, <name><surname>Rodriguez-Villegas</surname><given-names>E</given-names></name>. <article-title>Optimal features for online seizure detection</article-title>. <source>Medical &amp; Biological Engineering &amp; Computing</source>. <year>2012</year>;<volume>50</volume>(<issue>7</issue>):<fpage>659</fpage>–<lpage>669</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s11517-012-0904-x</pub-id><?supplied-pmid 22476713?><pub-id pub-id-type="pmid">22476713</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Luckett</surname><given-names>P</given-names></name>, <name><surname>Pavelescu</surname><given-names>E</given-names></name>, <name><surname>McDonald</surname><given-names>T</given-names></name>, <name><surname>Hively</surname><given-names>L</given-names></name>, <name><surname>Ochoa</surname><given-names>J</given-names></name>. <article-title>Predicting state transitions in brain dynamics through spectral difference of phase-space graphs</article-title>. <source>Journal of computational neuroscience</source>. <year>2019</year>;<volume>46</volume>(<issue>1</issue>):<fpage>91</fpage>–<lpage>106</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1007/s10827-018-0700-1</pub-id><?supplied-pmid 30315514?><pub-id pub-id-type="pmid">30315514</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Li</surname><given-names>A</given-names></name>, <name><surname>Chennuri</surname><given-names>B</given-names></name>, <name><surname>Subramanian</surname><given-names>S</given-names></name>, <name><surname>Yaffe</surname><given-names>R</given-names></name>, <name><surname>Gliske</surname><given-names>S</given-names></name>, <name><surname>Stacey</surname><given-names>W</given-names></name>, <etal>et al</etal>. <article-title>Using network analysis to localize the epileptogenic zone from invasive EEG recordings in intractable focal epilepsy</article-title>. <source>Network Neuroscience</source>. <year>2018</year>;<volume>2</volume>(<issue>02</issue>):<fpage>218</fpage>–<lpage>240</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1162/netn_a_00043</pub-id><?supplied-pmid 30215034?><pub-id pub-id-type="pmid">30215034</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Micheloyannis</surname><given-names>S</given-names></name>, <name><surname>Pachou</surname><given-names>E</given-names></name>, <name><surname>Stam</surname><given-names>CJ</given-names></name>, <name><surname>Vourkas</surname><given-names>M</given-names></name>, <name><surname>Erimaki</surname><given-names>S</given-names></name>, <name><surname>Tsirka</surname><given-names>V</given-names></name>. <article-title>Using graph theoretical analysis of multi channel EEG to evaluate the neural efficiency hypothesis</article-title>. <source>Neuroscience letters</source>. <year>2006</year>;<volume>402</volume>(<issue>3</issue>):<fpage>273</fpage>–<lpage>277</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neulet.2006.04.006</pub-id><?supplied-pmid 16678344?><pub-id pub-id-type="pmid">16678344</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Onton</surname><given-names>J</given-names></name>, <name><surname>Westerfield</surname><given-names>M</given-names></name>, <name><surname>Townsend</surname><given-names>J</given-names></name>, <name><surname>Makeig</surname><given-names>S</given-names></name>. <article-title>Imaging human EEG dynamics using independent component analysis</article-title>. <source>Neuroscience &amp; biobehavioral reviews</source>. <year>2006</year>;<volume>30</volume>(<issue>6</issue>):<fpage>808</fpage>–<lpage>822</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neubiorev.2006.06.007</pub-id><?supplied-pmid 16904745?><pub-id pub-id-type="pmid">16904745</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Alotaiby</surname><given-names>TN</given-names></name>, <name><surname>Alshebeili</surname><given-names>SA</given-names></name>, <name><surname>Alshawi</surname><given-names>T</given-names></name>, <name><surname>Ahmad</surname><given-names>I</given-names></name>, <name><surname>El-Samie</surname><given-names>FEA</given-names></name>. <article-title>EEG seizure detection and prediction algorithms: a survey</article-title>. <source>EURASIP Journal on Advances in Signal Processing</source>. <year>2014</year>;<volume>2014</volume>(<issue>1</issue>):<fpage>183</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1186/1687-6180-2014-183</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Orosco</surname><given-names>L</given-names></name>. <article-title>a survey of performance and techniques for automatic epilepsy detection</article-title>. <source>Journal of Medical and Biological Engineering</source>. <year>2013</year>;<volume>33</volume>(<issue>6</issue>):<fpage>526</fpage>–<lpage>537</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.5405/jmbe.1463</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref014">
      <label>14</label>
      <mixed-citation publication-type="book"><name><surname>Osorio</surname><given-names>I</given-names></name>, <name><surname>Zaveri</surname><given-names>HP</given-names></name>, <name><surname>Frei</surname><given-names>MG</given-names></name>, <name><surname>Arthurs</surname><given-names>S</given-names></name>. <source>Epilepsy: the intersection of neurosciences, biology, mathematics, engineering, and physics</source>. <publisher-name>CRC press</publisher-name>; <year>2016</year>.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Alickovic</surname><given-names>E</given-names></name>, <name><surname>Kevric</surname><given-names>J</given-names></name>, <name><surname>Subasi</surname><given-names>A</given-names></name>. <article-title>Performance evaluation of empirical mode decomposition, discrete wavelet transform, and wavelet packed decomposition for automated epileptic seizure detection and prediction</article-title>. <source>Biomedical signal processing and control</source>. <year>2018</year>;<volume>39</volume>:<fpage>94</fpage>–<lpage>102</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bspc.2017.07.022</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Faust</surname><given-names>O</given-names></name>, <name><surname>Acharya</surname><given-names>UR</given-names></name>, <name><surname>Adeli</surname><given-names>H</given-names></name>, <name><surname>Adeli</surname><given-names>A</given-names></name>. <article-title>Wavelet-based EEG processing for computer-aided seizure detection and epilepsy diagnosis</article-title>. <source>Seizure</source>. <year>2015</year>;<volume>26</volume>:<fpage>56</fpage>–<lpage>64</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.seizure.2015.01.012</pub-id><?supplied-pmid 25799903?><pub-id pub-id-type="pmid">25799903</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Ghosh-Dastidar</surname><given-names>S</given-names></name>, <name><surname>Adeli</surname><given-names>H</given-names></name>, <name><surname>Dadmehr</surname><given-names>N</given-names></name>. <article-title>Mixed-band wavelet-chaos-neural network methodology for epilepsy and epileptic seizure detection</article-title>. <source>IEEE transactions on biomedical engineering</source>. <year>2007</year>;<volume>54</volume>(<issue>9</issue>):<fpage>1545</fpage>–<lpage>1551</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2007.891945</pub-id><?supplied-pmid 17867346?><pub-id pub-id-type="pmid">17867346</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Kaleem</surname><given-names>M</given-names></name>, <name><surname>Guergachi</surname><given-names>A</given-names></name>, <name><surname>Krishnan</surname><given-names>S</given-names></name>. <article-title>Patient-specific seizure detection in long-term EEG using wavelet decomposition</article-title>. <source>Biomedical Signal Processing and Control</source>. <year>2018</year>;<volume>46</volume>:<fpage>157</fpage>–<lpage>165</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bspc.2018.07.006</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Khan</surname><given-names>Y</given-names></name>, <name><surname>Gotman</surname><given-names>J</given-names></name>. <article-title>Wavelet based automatic seizure detection in intracerebral electroencephalogram</article-title>. <source>Clinical Neurophysiology</source>. <year>2003</year>;<volume>114</volume>(<issue>5</issue>):<fpage>898</fpage>–<lpage>908</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/S1388-2457(03)00035-X</pub-id><?supplied-pmid 12738437?><pub-id pub-id-type="pmid">12738437</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Ocak</surname><given-names>H</given-names></name>. <article-title>Automatic detection of epileptic seizures in EEG using discrete wavelet transform and approximate entropy</article-title>. <source>Expert Systems with Applications</source>. <year>2009</year>;<volume>36</volume>(<issue>2</issue>):<fpage>2027</fpage>–<lpage>2036</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.eswa.2007.12.065</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Zandi</surname><given-names>AS</given-names></name>, <etal>et al</etal>. <article-title>Automated real-time epileptic seizure detection in scalp EEG recordings using an algorithm based on wavelet packet transform</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2010</year>;<volume>57</volume>(<issue>7</issue>):<fpage>1639</fpage>–<lpage>1651</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2010.2046417</pub-id><?supplied-pmid 20659825?><pub-id pub-id-type="pmid">20659825</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref022">
      <label>22</label>
      <mixed-citation publication-type="journal"><name><surname>Craley</surname><given-names>J</given-names></name>, <name><surname>Johnson</surname><given-names>E</given-names></name>, <name><surname>Venkataraman</surname><given-names>A</given-names></name>. <article-title>A Spatio-Temporal Model of Seizure Propagation in Focal Epilepsy</article-title>. <source>IEEE Transactions on Medical Imaging</source>. <year>2019</year>; p. <fpage>1</fpage>–<lpage>1</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TMI.2019.2950252</pub-id><?supplied-pmid 31675325?><pub-id pub-id-type="pmid">31675325</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref023">
      <label>23</label>
      <mixed-citation publication-type="other">Craley J, Johnson E, Venkataraman A. A novel method for epileptic seizure detection using coupled hidden markov models. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer; 2018. p. 482–489.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref024">
      <label>24</label>
      <mixed-citation publication-type="journal"><name><surname>Saab</surname><given-names>M</given-names></name>, <name><surname>Gotman</surname><given-names>J</given-names></name>. <article-title>A system to detect the onset of epileptic seizures in scalp EEG</article-title>. <source>Clinical Neurophysiology</source>. <year>2005</year>;<volume>116</volume>(<issue>2</issue>):<fpage>427</fpage>–<lpage>442</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.clinph.2004.08.004</pub-id><?supplied-pmid 15661120?><pub-id pub-id-type="pmid">15661120</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref025">
      <label>25</label>
      <mixed-citation publication-type="other">Shoeb AH. Application of machine learning to epileptic seizure onset detection and treatment. Massachusetts Institute of Technology; 2009.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref026">
      <label>26</label>
      <mixed-citation publication-type="other">Shoeb AH, Guttag JV. Application of machine learning to epileptic seizure detection. In: International Conference on Machine Learning; 2010. p. 975–982.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref027">
      <label>27</label>
      <mixed-citation publication-type="journal"><name><surname>Acharya</surname><given-names>UR</given-names></name>, <etal>et al</etal>. <article-title>Automated diagnosis of epileptic EEG using entropies</article-title>. <source>Biomedical Signal Processing and Control</source>. <year>2012</year>;<volume>7</volume>(<issue>4</issue>):<fpage>401</fpage>–<lpage>408</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bspc.2011.07.007</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref028">
      <label>28</label>
      <mixed-citation publication-type="journal"><name><surname>Andrzejak</surname><given-names>RG</given-names></name>, <name><surname>Lehnertz</surname><given-names>K</given-names></name>, <name><surname>Mormann</surname><given-names>F</given-names></name>, <name><surname>Rieke</surname><given-names>C</given-names></name>, <name><surname>David</surname><given-names>P</given-names></name>, <name><surname>Elger</surname><given-names>CE</given-names></name>. <article-title>Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state</article-title>. <source>Physical Review E</source>. <year>2001</year>;<volume>64</volume>(<issue>6</issue>):<fpage>061907</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1103/PhysRevE.64.061907</pub-id><?supplied-pmid 11736210?><pub-id pub-id-type="pmid">11736210</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref029">
      <label>29</label>
      <mixed-citation publication-type="other">Esteller R, Echauz J, Tcheng T, Litt B, Pless B. Line length: an efficient feature for seizure onset detection. In: Engineering in Medicine and Biology Society, 2001. Proceedings of the 23rd Annual International Conference of the IEEE. vol. 2. IEEE; 2001. p. 1707–1710.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref030">
      <label>30</label>
      <mixed-citation publication-type="journal"><name><surname>Mormann</surname><given-names>F</given-names></name>, <name><surname>Lehnertz</surname><given-names>K</given-names></name>, <name><surname>David</surname><given-names>P</given-names></name>, <name><surname>Elger</surname><given-names>CE</given-names></name>. <article-title>Mean Phase Coherence as a Measure for Phase Synchronization and its Application to the EEG of Epilepsy Patients</article-title>. <source>Physica D: Nonlinear Phenomena</source>. <year>2000</year>;<volume>144</volume>(<issue>3</issue>):<fpage>358</fpage>–<lpage>369</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/S0167-2789(00)00087-7</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref031">
      <label>31</label>
      <mixed-citation publication-type="journal"><name><surname>Mormann</surname><given-names>F</given-names></name>, <name><surname>Kreuz</surname><given-names>T</given-names></name>, <name><surname>Andrzejak</surname><given-names>RG</given-names></name>, <name><surname>David</surname><given-names>P</given-names></name>, <name><surname>Lehnertz</surname><given-names>K</given-names></name>, <name><surname>Elger</surname><given-names>CE</given-names></name>. <article-title>Epileptic Seizures are Preceded by a Decrease in Synchronization</article-title>. <source>Epilepsy Research</source>. <year>2003</year>;<volume>53</volume>(<issue>3</issue>):<fpage>173</fpage>–<lpage>185</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/S0920-1211(03)00002-0</pub-id><?supplied-pmid 12694925?><pub-id pub-id-type="pmid">12694925</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref032">
      <label>32</label>
      <mixed-citation publication-type="journal"><name><surname>Craley</surname><given-names>J</given-names></name>, <name><surname>Johnson</surname><given-names>E</given-names></name>, <name><surname>Jouny</surname><given-names>C</given-names></name>, <name><surname>Venkataraman</surname><given-names>A</given-names></name>. <article-title>Automated inter-patient seizure detection using multichannel Convolutional and Recurrent Neural Networks</article-title>. <source>Biomedical Signal Processing and Control</source>. <year>2021</year>;<volume>64</volume>:<fpage>102360</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bspc.2020.102360</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref033">
      <label>33</label>
      <mixed-citation publication-type="other">Affes A, Mdhaffar A, Triki C, Jmaiel M, Freisleben B. A Convolutional Gated Recurrent Neural Network for Epileptic Seizure Prediction. In: International Conference on Smart Homes and Health Telematics. Springer; 2019. p. 85–96.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref034">
      <label>34</label>
      <mixed-citation publication-type="journal"><name><surname>Gao</surname><given-names>Y</given-names></name>, <name><surname>Gao</surname><given-names>B</given-names></name>, <name><surname>Chen</surname><given-names>Q</given-names></name>, <name><surname>Liu</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>Y</given-names></name>. <article-title>Deep Convolutional Neural Network-Based Epileptic Electroencephalogram (EEG) Signal Classification</article-title>. <source>Frontiers in Neurology</source>. <year>2020</year>;<volume>11</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fneur.2020.00375</pub-id><?supplied-pmid 32528398?><pub-id pub-id-type="pmid">32528398</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref035">
      <label>35</label>
      <mixed-citation publication-type="journal"><name><surname>Güler</surname><given-names>NF</given-names></name>, <etal>et al</etal>. <article-title>Recurrent neural networks employing Lyapunov exponents for EEG signals classification</article-title>. <source>Expert systems with applications</source>. <year>2005</year>;<volume>29</volume>(<issue>3</issue>):<fpage>506</fpage>–<lpage>514</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.eswa.2005.04.011</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref036">
      <label>36</label>
      <mixed-citation publication-type="journal"><name><surname>Hu</surname><given-names>X</given-names></name>, <name><surname>Yuan</surname><given-names>S</given-names></name>, <name><surname>Xu</surname><given-names>F</given-names></name>, <name><surname>Leng</surname><given-names>Y</given-names></name>, <name><surname>Yuan</surname><given-names>K</given-names></name>, <name><surname>Yuan</surname><given-names>Q</given-names></name>. <article-title>Scalp EEG classification using deep Bi-LSTM network for seizure detection</article-title>. <source>Computers in Biology and Medicine</source>. <year>2020</year>;<volume>124</volume>:<fpage>103919</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.compbiomed.2020.103919</pub-id><?supplied-pmid 32771673?><pub-id pub-id-type="pmid">32771673</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref037">
      <label>37</label>
      <mixed-citation publication-type="journal"><name><surname>Khan</surname><given-names>H</given-names></name>, <name><surname>Marcuse</surname><given-names>L</given-names></name>, <name><surname>Fields</surname><given-names>M</given-names></name>, <name><surname>Swann</surname><given-names>K</given-names></name>, <name><surname>Yener</surname><given-names>B</given-names></name>. <article-title>Focal onset seizure prediction using convolutional networks</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2017</year>;<volume>65</volume>(<issue>9</issue>):<fpage>2109</fpage>–<lpage>2118</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2017.2785401</pub-id><?supplied-pmid 29989952?><pub-id pub-id-type="pmid">29989952</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref038">
      <label>38</label>
      <mixed-citation publication-type="journal"><name><surname>Oâ€™Shea</surname><given-names>A</given-names></name>, <name><surname>Lightbody</surname><given-names>G</given-names></name>, <name><surname>Boylan</surname><given-names>G</given-names></name>, <name><surname>Temko</surname><given-names>A</given-names></name>. <article-title>Neonatal seizure detection from raw multi-channel EEG using a fully convolutional architecture</article-title>. <source>Neural Networks</source>. <year>2020</year>;<volume>123</volume>:<fpage>12</fpage>–<lpage>25</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neunet.2019.11.023</pub-id><pub-id pub-id-type="pmid">31821947</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref039">
      <label>39</label>
      <mixed-citation publication-type="other">Park C, Choi G, Kim J, Kim S, Kim TJ, Min K, et al. Epileptic seizure detection for multi-channel EEG with deep convolutional neural network. In: 2018 International Conference on Electronics, Information, and Communication (ICEIC). IEEE; 2018. p. 1–5.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref040">
      <label>40</label>
      <mixed-citation publication-type="other">Vidyaratne L, Glandon A, Alam M, Iftekharuddin KM. Deep recurrent neural network for seizure detection. In: 2016 International Joint Conference on Neural Networks (IJCNN). IEEE; 2016. p. 1202–1207.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref041">
      <label>41</label>
      <mixed-citation publication-type="journal"><name><surname>Wei</surname><given-names>Z</given-names></name>, <name><surname>Zou</surname><given-names>J</given-names></name>, <name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Xu</surname><given-names>J</given-names></name>. <article-title>Automatic epileptic EEG detection using convolutional neural network with improvements in time-domain</article-title>. <source>Biomedical Signal Processing and Control</source>. <year>2019</year>;<volume>53</volume>:<fpage>101551</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.bspc.2019.04.028</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref042">
      <label>42</label>
      <mixed-citation publication-type="journal"><name><surname>Yuan</surname><given-names>Y</given-names></name>, <name><surname>Xun</surname><given-names>G</given-names></name>, <name><surname>Jia</surname><given-names>K</given-names></name>, <name><surname>Zhang</surname><given-names>A</given-names></name>. <article-title>A Multi-View Deep Learning Framework for EEG Seizure Detection</article-title>. <source>IEEE journal of biomedical and health informatics</source>. <year>2018</year>;<volume>23</volume>(<issue>1</issue>):<fpage>83</fpage>–<lpage>94</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/JBHI.2018.2871678</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref043">
      <label>43</label>
      <mixed-citation publication-type="journal"><name><surname>Zhou</surname><given-names>M</given-names></name>, <name><surname>Tian</surname><given-names>C</given-names></name>, <name><surname>Cao</surname><given-names>R</given-names></name>, <name><surname>Wang</surname><given-names>B</given-names></name>, <name><surname>Niu</surname><given-names>Y</given-names></name>, <name><surname>Hu</surname><given-names>T</given-names></name>, <etal>et al</etal>. <article-title>Epileptic seizure detection based on EEG signals and CNN</article-title>. <source>Frontiers in neuroinformatics</source>. <year>2018</year>;<volume>12</volume>:<fpage>95</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fninf.2018.00095</pub-id><?supplied-pmid 30618700?><pub-id pub-id-type="pmid">30618700</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref044">
      <label>44</label>
      <mixed-citation publication-type="other">Zou L, Liu X, Jiang A, Zhousp X. Epileptic Seizure Detection Using Deep Convolutional Network. In: 2018 IEEE 23rd International Conference on Digital Signal Processing (DSP). IEEE; 2018. p. 1–4.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref045">
      <label>45</label>
      <mixed-citation publication-type="other">Craley J, Johnson E, Venkataraman A. Integrating convolutional neural networks and probabilistic graphical modeling for epileptic seizure detection in multichannel EEG. In: International Conference on Information Processing in Medical Imaging. Springer; 2019. p. 291–303.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref046">
      <label>46</label>
      <mixed-citation publication-type="other">Craley J, Johnson E, Jouny C, Venkataraman A. Automated Noninvasive Seizure Detection and Localization Using Switching Markov Models and Convolutional Neural Networks. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer; 2019. p. 253–261.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref047">
      <label>47</label>
      <mixed-citation publication-type="journal"><name><surname>Abiri</surname><given-names>R</given-names></name>, <name><surname>Borhani</surname><given-names>S</given-names></name>, <name><surname>Sellers</surname><given-names>EW</given-names></name>, <name><surname>Jiang</surname><given-names>Y</given-names></name>, <name><surname>Zhao</surname><given-names>X</given-names></name>. <article-title>A comprehensive review of EEG-based brain–computer interface paradigms</article-title>. <source>Journal of neural engineering</source>. <year>2019</year>;<volume>16</volume>(<issue>1</issue>):<fpage>011001</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1088/1741-2552/aaf12e</pub-id><?supplied-pmid 30523919?><pub-id pub-id-type="pmid">30523919</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref048">
      <label>48</label>
      <mixed-citation publication-type="journal"><name><surname>He</surname><given-names>B</given-names></name>, <name><surname>Baxter</surname><given-names>B</given-names></name>, <name><surname>Edelman</surname><given-names>BJ</given-names></name>, <name><surname>Cline</surname><given-names>CC</given-names></name>, <name><surname>Wenjing</surname><given-names>WY</given-names></name>. <article-title>Noninvasive brain-computer interfaces based on sensorimotor rhythms</article-title>. <source>Proceedings of the IEEE</source>. <year>2015</year>;<volume>103</volume>(<issue>6</issue>):<fpage>907</fpage>–<lpage>925</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/jproc.2015.2407272</pub-id><?supplied-pmid 34334804?><pub-id pub-id-type="pmid">34334804</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref049">
      <label>49</label>
      <mixed-citation publication-type="journal"><name><surname>Yuan</surname><given-names>H</given-names></name>, <name><surname>He</surname><given-names>B</given-names></name>. <article-title>Brain–computer interfaces using sensorimotor rhythms: current state and future perspectives</article-title>. <source>IEEE Transactions on Biomedical Engineering</source>. <year>2014</year>;<volume>61</volume>(<issue>5</issue>):<fpage>1425</fpage>–<lpage>1435</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1109/TBME.2014.2312397</pub-id><?supplied-pmid 24759276?><pub-id pub-id-type="pmid">24759276</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref050">
      <label>50</label>
      <mixed-citation publication-type="journal"><name><surname>LaFleur</surname><given-names>K</given-names></name>, <name><surname>Cassady</surname><given-names>K</given-names></name>, <name><surname>Doud</surname><given-names>A</given-names></name>, <name><surname>Shades</surname><given-names>K</given-names></name>, <name><surname>Rogin</surname><given-names>E</given-names></name>, <name><surname>He</surname><given-names>B</given-names></name>. <article-title>Quadcopter control in three-dimensional space using a noninvasive motor imagery-based brain–computer interface</article-title>. <source>Journal of neural engineering</source>. <year>2013</year>;<volume>10</volume>(<issue>4</issue>):<fpage>046003</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1088/1741-2560/10/4/046003</pub-id><?supplied-pmid 23735712?><pub-id pub-id-type="pmid">23735712</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref051">
      <label>51</label>
      <mixed-citation publication-type="journal"><name><surname>Vialatte</surname><given-names>FB</given-names></name>, <name><surname>Maurice</surname><given-names>M</given-names></name>, <name><surname>Dauwels</surname><given-names>J</given-names></name>, <name><surname>Cichocki</surname><given-names>A</given-names></name>. <article-title>Steady-state visually evoked potentials: focus on essential paradigms and future perspectives</article-title>. <source>Progress in neurobiology</source>. <year>2010</year>;<volume>90</volume>(<issue>4</issue>):<fpage>418</fpage>–<lpage>438</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.pneurobio.2009.11.005</pub-id><?supplied-pmid 19963032?><pub-id pub-id-type="pmid">19963032</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref052">
      <label>52</label>
      <mixed-citation publication-type="journal"><name><surname>Chen</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>Y</given-names></name>, <name><surname>Nakanishi</surname><given-names>M</given-names></name>, <name><surname>Gao</surname><given-names>X</given-names></name>, <name><surname>Jung</surname><given-names>TP</given-names></name>, <name><surname>Gao</surname><given-names>S</given-names></name>. <article-title>High-speed spelling with a noninvasive brain–computer interface</article-title>. <source>Proceedings of the national academy of sciences</source>. <year>2015</year>;<volume>112</volume>(<issue>44</issue>):<fpage>E6058</fpage>–<lpage>E6067</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1073/pnas.1508080112</pub-id><?supplied-pmid 26483479?><pub-id pub-id-type="pmid">26483479</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref053">
      <label>53</label>
      <mixed-citation publication-type="journal"><name><surname>Delorme</surname><given-names>A</given-names></name>, <name><surname>Makeig</surname><given-names>S</given-names></name>. <article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title>. <source>Journal of neuroscience methods</source>. <year>2004</year>;<volume>134</volume>(<issue>1</issue>):<fpage>9</fpage>–<lpage>21</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><?supplied-pmid 15102499?><pub-id pub-id-type="pmid">15102499</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref054">
      <label>54</label>
      <mixed-citation publication-type="other">Delorme A, Mullen T, Kothe C, Akalin Acar Z, Bigdely-Shamlo N, Vankov A, et al. EEGLAB, SIFT, NFT, BCILAB, and ERICA: new tools for advanced EEG processing. Computational intelligence and neuroscience. 2011;2011.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref055">
      <label>55</label>
      <mixed-citation publication-type="journal"><name><surname>Hassan</surname><given-names>M</given-names></name>, <name><surname>Shamas</surname><given-names>M</given-names></name>, <name><surname>Khalil</surname><given-names>M</given-names></name>, <name><surname>El Falou</surname><given-names>W</given-names></name>, <name><surname>Wendling</surname><given-names>F</given-names></name>. <article-title>EEGNET: An open source tool for analyzing and visualizing M/EEG connectome</article-title>. <source>PloS one</source>. <year>2015</year>;<volume>10</volume>(<issue>9</issue>):<fpage>e0138297</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1371/journal.pone.0138297</pub-id><?supplied-pmid 26379232?><pub-id pub-id-type="pmid">26379232</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref056">
      <label>56</label>
      <mixed-citation publication-type="journal"><name><surname>Tadel</surname><given-names>F</given-names></name>, <name><surname>Baillet</surname><given-names>S</given-names></name>, <name><surname>Mosher</surname><given-names>JC</given-names></name>, <name><surname>Pantazis</surname><given-names>D</given-names></name>, <name><surname>Leahy</surname><given-names>RM</given-names></name>. <article-title>Brainstorm: a user-friendly application for MEG/EEG analysis</article-title>. <source>Computational intelligence and neuroscience</source>. <year>2011</year>;<volume>2011</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1155/2011/879716</pub-id><?supplied-pmid 21584256?><pub-id pub-id-type="pmid">21584256</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref057">
      <label>57</label>
      <mixed-citation publication-type="other">EDFbrowswer;. <ext-link xlink:href="https://www.teuniz.net/edfbrowser/" ext-link-type="uri">https://www.teuniz.net/edfbrowser/</ext-link>.</mixed-citation>
    </ref>
    <ref id="pone.0282268.ref058">
      <label>58</label>
      <mixed-citation publication-type="journal"><name><surname>Gramfort</surname><given-names>A</given-names></name>, <name><surname>Luessi</surname><given-names>M</given-names></name>, <name><surname>Larson</surname><given-names>E</given-names></name>, <name><surname>Engemann</surname><given-names>DA</given-names></name>, <name><surname>Strohmeier</surname><given-names>D</given-names></name>, <name><surname>Brodbeck</surname><given-names>C</given-names></name>, <etal>et al</etal>. <article-title>MEG and EEG data analysis with MNE-Python</article-title>. <source>Frontiers in Neuroscience</source>. <year>2013</year>;<volume>7</volume>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id><?supplied-pmid 24431986?><pub-id pub-id-type="pmid">24431986</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref059">
      <label>59</label>
      <mixed-citation publication-type="journal"><name><surname>Goldberger</surname><given-names>AL</given-names></name>, <name><surname>Amaral</surname><given-names>LA</given-names></name>, <name><surname>Glass</surname><given-names>L</given-names></name>, <name><surname>Hausdorff</surname><given-names>JM</given-names></name>, <name><surname>Ivanov</surname><given-names>PC</given-names></name>, <name><surname>Mark</surname><given-names>RG</given-names></name>, <etal>et al</etal>. <article-title>PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals</article-title>. <source>Circulation</source>. <year>2000</year>;<volume>101</volume>(<issue>23</issue>):<fpage>215</fpage>–<lpage>220</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1161/01.CIR.101.23.e215</pub-id><?supplied-pmid 10851218?><pub-id pub-id-type="pmid">10851218</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref060">
      <label>60</label>
      <mixed-citation publication-type="journal"><name><surname>Obeid</surname><given-names>I</given-names></name>, <name><surname>Picone</surname><given-names>P</given-names></name>. <article-title>The Temple University Hospital EEG Data corpus</article-title>. <source>Frontiers in Neuroscience</source>. <year>2016</year>;<volume>10</volume>:<fpage>196</fpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.3389/fnins.2016.00196</pub-id><?supplied-pmid 27242402?><pub-id pub-id-type="pmid">27242402</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref061">
      <label>61</label>
      <mixed-citation publication-type="journal"><name><surname>Jurcak</surname><given-names>V</given-names></name>, <etal>et al</etal>. <article-title>10/20, 10/10, and 10/5 systems revisited: their validity as relative head-surface-based positioning systems</article-title>. <source>Neuroimage</source>. <year>2007</year>;<volume>34</volume>(<issue>4</issue>):<fpage>1600</fpage>–<lpage>1611</lpage>. <comment>doi: </comment><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.09.024</pub-id><?supplied-pmid 17207640?><pub-id pub-id-type="pmid">17207640</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0282268.ref062">
      <label>62</label>
      <mixed-citation publication-type="other">Currey D, Hsu D, Ahmed R, Venkataraman A, Craley J. Cross-site Epileptic Seizure Detection Using Convolutional Neural Networks. In: CISS: Conference on Information Sciences and Systems; 2021. p. 1–6.</mixed-citation>
    </ref>
  </ref-list>
</back>
