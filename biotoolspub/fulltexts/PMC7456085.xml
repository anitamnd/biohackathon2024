<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//Springer-Verlag//DTD A++ V2.4//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName A++V2.4.dtd?>
<?SourceDTD.Version 2.4?>
<?ConverterInfo.XSLTName springer2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">BMC Bioinformatics</journal-id>
    <journal-id journal-id-type="iso-abbrev">BMC Bioinformatics</journal-id>
    <journal-title-group>
      <journal-title>BMC Bioinformatics</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1471-2105</issn>
    <publisher>
      <publisher-name>BioMed Central</publisher-name>
      <publisher-loc>London</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">7456085</article-id>
    <article-id pub-id-type="publisher-id">3718</article-id>
    <article-id pub-id-type="doi">10.1186/s12859-020-03718-9</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Methodology Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>binomialRF: interpretable combinatoric efficiency of random forests to identify biomarker interactions</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Rachid Zaim</surname>
          <given-names>Samir</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Kenost</surname>
          <given-names>Colleen</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Berghout</surname>
          <given-names>Joanne</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Chiu</surname>
          <given-names>Wesley</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Wilson</surname>
          <given-names>Liam</given-names>
        </name>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <name>
          <surname>Zhang</surname>
          <given-names>Hao Helen</given-names>
        </name>
        <address>
          <email>hzhang@math.arizona.edu</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff4">4</xref>
      </contrib>
      <contrib contrib-type="author" corresp="yes">
        <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9854-1005</contrib-id>
        <name>
          <surname>Lussier</surname>
          <given-names>Yves A.</given-names>
        </name>
        <address>
          <email>Lussier.Y@gmail.com</email>
        </address>
        <xref ref-type="aff" rid="Aff1">1</xref>
        <xref ref-type="aff" rid="Aff2">2</xref>
        <xref ref-type="aff" rid="Aff3">3</xref>
        <xref ref-type="aff" rid="Aff5">5</xref>
        <xref ref-type="aff" rid="Aff6">6</xref>
        <xref ref-type="aff" rid="Aff7">7</xref>
      </contrib>
      <aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.134563.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2168 186X</institution-id><institution>Center for Biomedical Informatics and Biostatistics, </institution><institution>University of Arizona Health Sciences, </institution></institution-wrap>1230 N. Cherry Ave, Tucson, AZ 85721 USA </aff>
      <aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.134563.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2168 186X</institution-id><institution>The Graduate Interdisciplinary Program in Statistics, </institution><institution>The University of Arizona, </institution></institution-wrap>617 N. Santa Rita Ave., Tucson, AZ 85721 USA </aff>
      <aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.134563.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2168 186X</institution-id><institution>College of Medicine, Tucson, </institution></institution-wrap>1501 N. Campbell Ave, Tucson, AZ 85721 USA </aff>
      <aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.134563.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2168 186X</institution-id><institution>Department of Mathematics, College of Sciences, </institution><institution>The University of Arizona, </institution></institution-wrap>617 N. Santa Rita Ave., Tucson, AZ 85721 USA </aff>
      <aff id="Aff5"><label>5</label>The Center for Applied Genetic and Genomic Medicine, 1295 N. Martin, Tucson, AZ 85721 USA </aff>
      <aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.134563.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2168 186X</institution-id><institution>The University of Arizona Cancer Center, </institution></institution-wrap>3838 N. Campbell Ave, Tucson, AZ 85721 USA </aff>
      <aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.134563.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2168 186X</institution-id><institution>The University of Arizona BIO5 Institute, </institution></institution-wrap>1657 E. Helen Street, Tucson, AZ 85721 USA </aff>
    </contrib-group>
    <pub-date pub-type="epub">
      <day>28</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>28</day>
      <month>8</month>
      <year>2020</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2020</year>
    </pub-date>
    <volume>21</volume>
    <elocation-id>374</elocation-id>
    <history>
      <date date-type="received">
        <day>26</day>
        <month>3</month>
        <year>2020</year>
      </date>
      <date date-type="accepted">
        <day>19</day>
        <month>8</month>
        <year>2020</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© The Author(s) 2020</copyright-statement>
      <license license-type="OpenAccess">
        <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
      </license>
    </permissions>
    <abstract id="Abs1">
      <sec>
        <title>Background</title>
        <p id="Par1">In this era of data science-driven bioinformatics, machine learning research has focused on feature selection as users want more interpretation and post-hoc analyses for biomarker detection. However, when there are more features (i.e., transcripts) than samples (i.e., mice or human samples) in a study, it poses major statistical challenges in biomarker detection tasks as traditional statistical techniques are underpowered in high dimension. Second and third order interactions of these features pose a substantial combinatoric dimensional challenge. In computational biology, random forest (<bold>RF</bold>) classifiers are widely used due to their flexibility, powerful performance, their ability to rank features, and their robustness to the “P &gt; &gt; <italic>N</italic>” high-dimensional limitation that many matrix regression algorithms face. We propose binomialRF, a feature selection technique in RFs that provides an alternative interpretation for features using a correlated binomial distribution and scales efficiently to analyze multiway interactions.</p>
      </sec>
      <sec>
        <title>Results</title>
        <p id="Par2">In both simulations and validation studies using datasets from the TCGA and UCI repositories, binomialRF showed computational gains (up to 5 to 300 times faster) while maintaining competitive variable precision and recall in identifying biomarkers’ main effects and interactions. In two clinical studies, the binomialRF algorithm prioritizes previously-published relevant pathological molecular mechanisms (features) with high classification precision and recall using features alone, as well as with their statistical interactions alone.</p>
      </sec>
      <sec>
        <title>Conclusion</title>
        <p id="Par3">binomialRF extends upon previous methods for identifying interpretable features in RFs and brings them together under a correlated binomial distribution to create an efficient hypothesis testing algorithm that identifies biomarkers’ main effects and interactions. Preliminary results in simulations demonstrate computational gains while retaining competitive model selection and classification accuracies. Future work will extend this framework to incorporate ontologies that provide pathway-level feature selection from gene expression input data.</p>
      </sec>
    </abstract>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000060</institution-id>
            <institution>National Institute of Allergy and Infectious Diseases</institution>
          </institution-wrap>
        </funding-source>
        <award-id>U01AI122275</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lussier</surname>
            <given-names>Yves A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000054</institution-id>
            <institution>National Cancer Institute</institution>
          </institution-wrap>
        </funding-source>
        <award-id>P30CA023074</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lussier</surname>
            <given-names>Yves A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <funding-group>
      <award-group>
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id>
            <institution>National Institutes of Health</institution>
          </institution-wrap>
        </funding-source>
        <award-id>1UG3OD023171</award-id>
        <principal-award-recipient>
          <name>
            <surname>Lussier</surname>
            <given-names>Yves A.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
    </funding-group>
    <custom-meta-group>
      <custom-meta>
        <meta-name>issue-copyright-statement</meta-name>
        <meta-value>© The Author(s) 2020</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="Sec1">
    <title>Background</title>
    <p id="Par12">Recent advances in machine learning and data science tools have led to a revamped effort for improving clinical decision-making anchored in genomic data analysis and biomarker detection. However, despite these novel advances, random forests (<bold>RFs</bold>) [<xref ref-type="bibr" rid="CR1">1</xref>] remain a widely popular machine learning algorithm choice in genomics given their ability to i) accurately predict phenotypes using genomic data and ii) identify relevant genes and gene products used for predicting the phenotype. Literature over the past 20 years has demonstrated [<xref ref-type="bibr" rid="CR2">2</xref>–<xref ref-type="bibr" rid="CR9">9</xref>] their broad success in being able to robustly handle the “<italic>P</italic> &gt; &gt; N” high-dimensional statistical limitation (i.e., when there are more predictors or features “<bold><italic>P</italic></bold>” (i.e., genes) than there are human subjects “<italic>N</italic>”) while maintaining competitive predictive and gene selection abilities. However, the translational utility of random forests has not been fully understood as they are often viewed as “black box” algorithms by physicians and geneticists. Therefore, a substantial effort over the past decade has focused around “feature selection” in random forests (<bold>RF</bold>) [<xref ref-type="bibr" rid="CR5">5</xref>, <xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR10">10</xref>–<xref ref-type="bibr" rid="CR14">14</xref>] to better provide explanatory power of these models and to identify important genes and gene products in classification models. Table <xref rid="Tab1" ref-type="table">1</xref> describes methods of existing feature selection commonly used in random forests as either permutation-type measures of importance, heuristic rankings without formal decision boundaries (i.e., no <italic>p</italic>-values) or a combination of both.
<table-wrap id="Tab1"><label>Table 1</label><caption><p>Random forest feature selection methods and their permutation requirements</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Permute</th><th>Method</th><th><bold><italic>P</italic></bold>-value</th><th>Brief description</th></tr></thead><tbody><tr><td rowspan="7">No</td><td><bold>binomialRF</bold> [<xref ref-type="bibr" rid="CR15">15</xref>]</td><td><bold>Yes</bold></td><td><bold>Optimal splitting features’</bold><bold><italic>p</italic></bold><bold>-values obtained via one-sided</bold><bold><italic>correlated</italic></bold><bold>binomial tests</bold></td></tr><tr><td>EFS [<xref ref-type="bibr" rid="CR16">16</xref>]</td><td>No</td><td>Calculates a global score for each feature using 8 different metrics to measure importance and selects features whose score exceeds the median global score</td></tr><tr><td>AUC-RF [<xref ref-type="bibr" rid="CR17">17</xref>]</td><td>No</td><td>Iteratively trains a random forest algorithm and removes predictors in a stepwise fashion to maximize an AUC increase</td></tr><tr><td>RFE, dRFE [<xref ref-type="bibr" rid="CR18">18</xref>]</td><td>No</td><td>Iteratively trains a random forest (RF) model and drops uninformative features based on a user-defined criterion</td></tr><tr><td>RF-ACE [<xref ref-type="bibr" rid="CR19">19</xref>]</td><td>No</td><td>Creates phony variables called “Artificial Contrasts with Ensembles”, and compares how often these sham variables are used over the real ones</td></tr><tr><td>R2VIM [<xref ref-type="bibr" rid="CR12">12</xref>]</td><td>No</td><td>Calculates variable importance (VI) and divides by minimum VI to create relative VI, and choose important features based on a pre-selected cutoff</td></tr><tr><td>VarSelRF, geneSrF [<xref ref-type="bibr" rid="CR5">5</xref>]</td><td>No</td><td>Iteratively removes worst .20 (or x-percentage) of all features; retrains RF; selects smallest feature set within one set of best models</td></tr><tr><td rowspan="5">Yes</td><td>Vita [<xref ref-type="bibr" rid="CR20">20</xref>]</td><td>Yes</td><td>P-values are calculated based on empirical null distribution of non-positive importance scores that accelerate null distribution estimates</td></tr><tr><td>Perm [<xref ref-type="bibr" rid="CR20">20</xref>]</td><td>Yes</td><td>Permutes outcomes (Y) and determines importance based on which features retained a larger importance in <italic>Y</italic><sub><italic>original</italic></sub> vs. <italic>Y</italic><sub><italic>permuted</italic></sub></td></tr><tr><td>PIMP [<xref ref-type="bibr" rid="CR14">14</xref>]</td><td>Yes</td><td>Permutes outcome and determines features’ priority based on increases in mutual information or Gini errors. A feature’s <italic>p</italic>-values is produced by an importance measure fitted to a distribution</td></tr><tr><td>VSURF [<xref ref-type="bibr" rid="CR17">17</xref>]</td><td>No</td><td>Two-step FS algorithm: 1) uses predictor permutations to identify features robust to noise, and 2) refines model by conducting step-forward inclusion of features until error convergence</td></tr><tr><td>Boruta [<xref ref-type="bibr" rid="CR13">13</xref>]</td><td>No</td><td>Creates phony predictors by permuting the values of the shadow vars. Runs RF to identify features’ Z-scores. Eliminates features whose Z-score are less than a threshold. Repeats until convergence</td></tr></tbody></table><table-wrap-foot><p>Absence of permutations generally decreases substantially computing time. <italic>P</italic>-values provide explicit ranking of features, which enables objective feature thresholding</p></table-wrap-foot></table-wrap></p>
    <p id="Par13">While the bioinformatics community have been widely using the above-mentioned approaches to feature selection approaches in multi-analyte biomarker discovery [<xref ref-type="bibr" rid="CR5">5</xref>], two problems have been hampering their impact in biomedicine. First, random-forests implementations are generally computationally expansive and memory intensive, particularly for identifying molecular interactions. In addition, conventional fully-specified RF classifiers remain opaque to human interpretation, yet there is an increasing consensus among clinicians and machine learning experts that ethical and safe translation of machine learned algorithms for high stake clinical decisions should be interpretable and explainable [<xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR24">24</xref>].</p>
    <p id="Par14">We hypothesized that a binomial probabilistic framework for feature selection could both improve the computational efficiency of RF classifiers and unveil their otherwise hidden variables for increasing their review and usability by domain experts. We propose the <bold><italic>binomialRF</italic></bold> feature selection algorithm, a wrapper feature selection algorithm that identifies significant genes and gene sets in a memory-efficient, scalable fashion, with explicit features for biologists and clinicians. Building upon the “inclusion frequency” [<xref ref-type="bibr" rid="CR25">25</xref>] feature ranking, binomialRF formalizes this concept into a binomial probabilistic framework to measure feature importance and extends to identify K-way nonlinear interactions among gene sets. The results and evaluation of the simulation, numerical and clinical studies are presented in Section 2. The Discussion and conclusion and presented in Sections 3 and 4, respectively, and the proposed method is formulated in Section 5.</p>
  </sec>
  <sec id="Sec2">
    <title>Results</title>
    <p id="Par15">The simulation and numerical studies used to evaluate the techniques are listed and reviewed in this section. The results and analyses are organized by memory and computational efficiency (Section 2.1), followed by feature selection accuracy and false discovery rates (Section 2.2–2.3) in the simulations and proceeds to detail the numerical studies using the Madelon benchmark (Section 2.4) and the clinical validations from the TCGA repository (Section 2.5) examining breast and kidney cancers.</p>
    <sec id="Sec3">
      <title>Memory efficiency and runtime analysis</title>
      <p id="Par16">To measure memory gains and computational efficiency, two different analyses were conducted in these simulation studies. The first was a theoretical analyses of memory requirements for interaction detection in simulated genomes with 100, 1000, and 10,000 genes. These are clearly smaller than the human genome but serve to illustrate the drastic combinatoric efficiency gained in small dimensional settings. In Table <xref rid="Tab2" ref-type="table">2</xref>, the analyses show the memory efficiency attained by binomialRF to detect 2-way and 3-way interactions. As shown, it can require as much as 170,000 times less memory to calculate 3-way interactions with binomialRF as compared to a classical RF in a moderately large dataset with 1000 variables, potentially impacting memory requirements of grid computers. Note that in linear models, efficient solution paths for <inline-formula id="IEq1"><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \otimes {X}_{i=1}^K $$\end{document}</tex-math><mml:math id="M2" display="inline"><mml:mo>⊗</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq1.gif"/></alternatives></inline-formula> only exist for <italic>K</italic> ∈ {1, 2} (LASSO [<xref ref-type="bibr" rid="CR26">26</xref>] for <italic>K</italic> =1 and RAMP [<xref ref-type="bibr" rid="CR27">27</xref>] for <italic>K</italic> =2). For <italic>K</italic> &gt; 2, to our knowledge, no algorithm guarantees computational efficiency. In RF-based feature selection techniques, the majority of the techniques requires one to explicitly multiply interactions in order to detect them.
<table-wrap id="Tab2"><label>Table 2</label><caption><p>BinomialRF improves the memory requirements</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Features dimension</th><th rowspan="2">Interaction order</th><th colspan="2">Memory requirements for interactions</th><th rowspan="2">Memory efficiency</th></tr><tr><th><italic>binomialF</italic></th><th><italic>Other methods of</italic> Table <xref rid="Tab1" ref-type="table">1</xref></th></tr></thead><tbody><tr><td rowspan="2">10</td><td>2</td><td rowspan="2">N ×  10</td><td>N ×  55</td><td><bold>~  5</bold></td></tr><tr><td>3</td><td>N ×  175</td><td><bold>~  17</bold></td></tr><tr><td rowspan="2">100</td><td>2</td><td rowspan="2">N × 100</td><td>N ×  5050</td><td><bold>~  50</bold></td></tr><tr><td>3</td><td>N ×  166,750</td><td><bold>~  1700</bold></td></tr><tr><td rowspan="2">1000</td><td>2</td><td rowspan="2">N × 1000</td><td>N ×  500,500</td><td><bold>~  500</bold></td></tr><tr><td>3</td><td>N × 166,667,500</td><td><bold>~  170,000</bold></td></tr></tbody></table><table-wrap-foot><p>The improvement is on the orders of magnitude in 2-way and 3-way interactions when compared to other methods of Table <xref rid="Tab1" ref-type="table">1</xref>. One advantage of the binomialRF algorithm is that it can screen for sets of gene interactions in a memory efficient manner by only requiring a constant-sized matrix whereas the current state of the art requires the predictor matrix to increase in size in a combinatoric fashion to screen for interactions. Memory efficiency is defined by <inline-formula id="IEq2"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \raisebox{1ex}{$\mathrm{Dim}\ \left(\otimes {X}_{i=1}^K\right)$}\!\left/ \!\raisebox{-1ex}{$ Dim(X)$}\right. $$\end{document}</tex-math><mml:math id="M4" display="inline"><mml:mfrac bevelled="true"><mml:mrow><mml:mi>Dim</mml:mi><mml:mspace width="0.25em"/><mml:mfenced close=")" open="("><mml:mrow><mml:mo>⊗</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="italic">Dim</mml:mi><mml:mfenced close=")" open="("><mml:mi>X</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq2.gif"/></alternatives></inline-formula>, and interaction memory requirements are defined by the number of columns required to map all k-way interactions</p></table-wrap-foot></table-wrap></p>
      <p id="Par17">To compare each algorithm’s runtime, we strictly measure the time for the algorithm to produce its feature ranking and omit other portions using the base <italic>system.time</italic> R function. This runtime is measured in seconds. The boxplot in Fig. <xref rid="Fig1" ref-type="fig">1</xref> displays the range of runtimes (measured in seconds) and graphs them in incremental powers of 10 (i.e., 10<sup>1</sup>, 10<sup>2</sup>, 10<sup>3</sup>, …) to illustrate the difference in magnitudes. As shown in the rightmost panel (10,000 genes) of Fig. <xref rid="Fig1" ref-type="fig">1</xref>, the binomialRF algorithm takes, on average, 16.6 s to run, while Boruta averages 779 s, resulting in a 47-fold increase for conducting the same analysis. The techniques omitted from Fig. <xref rid="Fig1" ref-type="fig">1</xref> all resulted in runtimes larger than Boruta (i.e., at least 20X slower than binomialRF), and several of them were unable to process datasets with 10,000 to 20,000 features.
<fig id="Fig1"><label>Fig. 1</label><caption><p>BinomialRF showing substantially improved computational time. The simulation runtimes are measured in seconds and are plotted in powers of ten to show the difference in magnitudes of computation time. The simulation scenarios are detailed in Section 2.1, where the length of the coefficient vector, β varies from 10 to 100 and 1000 features. All simulations were conducted on a 2017 MacBook Pro with 3.1 GHz Intel Core i5 and 16 GB of RAM. All simulations resulted in the binomialRF being the fastest</p></caption><graphic xlink:href="12859_2020_3718_Fig1_HTML" id="MO1"/></fig></p>
    </sec>
    <sec id="Sec4">
      <title>Feature selection accuracy in simulations</title>
      <p id="Par18">To measure scalability in the predictor space, 500 random forest objects are grown with 500 trees, using simulated genomes sizes 100, 1000, and 10,000 (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Table <xref rid="Tab3" ref-type="table">3</xref>a illustrates and summarizes the results for the main effects analysis across 32 simulation studies including up to 2000 features. Boruta, EFS, VSURF, and binomialRF all attain high precision, while PERM and AUCRF attain the largest recall, and EFS the lowest test error. To mimic a human genome (≈ 20–25,000 genes), a limited simulation scenario generated a synthetic genome with 10,000 genes. However, several techniques other than binomialRF faced rate-limiting computational and memory challenges, preventing us from conducting a full evaluation. Table <xref rid="Tab3" ref-type="table">3</xref>b summarizes the simulation results for <italic>p</italic> = 10,000 where a total of 100 genes were seeded. In this scenario, Boruta and binomialRF again obtained the highest precision values on average, PERM attained the highest recall. However, PERM labeled nearly half the genome as significant, resulting in a precision value near 0. AUCRF and binomialRF produced the most accurate classifiers, though most techniques operated within a similar accuracy range.
<table-wrap id="Tab3"><label>Table 3</label><caption><p>Simulation results of biomarkers</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Precision</th><th>Recall</th><th>Test error</th><th>Model size</th></tr></thead><tbody><tr><td char="(" align="char" colspan="5">3A. Results: 100–2000 features</td></tr><tr><td> AUCRF</td><td>0.54 (0.25)</td><td>0.74 (0.26)</td><td>0.27 (0.1)</td><td>8.74 (0.13)</td></tr><tr><td> binomialRF</td><td><bold>0.91 (0.13)</bold></td><td>0.37 (0.36)</td><td>0.33 (0.13)</td><td>81.72 (0.08)</td></tr><tr><td> Boruta</td><td>0.89 (0.15)</td><td>0.41 (0.37)</td><td>0.32 (0.13)</td><td>63.38 (0.1)</td></tr><tr><td> EFS</td><td>0.83 (0.16)</td><td>0.69 (0.27)</td><td><bold>0.25 (0.1)</bold></td><td>8.66 (0.13)</td></tr><tr><td> Perm</td><td>0.33 (0.33)</td><td><bold>0.82 (0.18)</bold></td><td>0.30 (0.09)</td><td>59.42 (0.1)</td></tr><tr><td> PIMP<sup>a</sup></td><td>0.18 (0.36)</td><td>0.00 (0.01)</td><td>0.35 (0.1)</td><td><bold>1.47 (0.11)</bold></td></tr><tr><td> RFE</td><td>0.49 (0.35)</td><td>0.61 (0.23)</td><td>0.3 (0.08)</td><td>250.29 (0.09)</td></tr><tr><td> VarSelRF</td><td>0.67 (0.24)</td><td>0.65 (0.29)</td><td>0.27 (0.1)</td><td>12.31 (0.12)</td></tr><tr><td> Vita</td><td>0.46 (0.28)</td><td>0.66 (0.29)</td><td>0.28 (0.1)</td><td>35.44 (0.1)</td></tr><tr><td> VSURF</td><td>0.86 (0.15)</td><td>0.44 (0.36)</td><td>0.31 (0.12)</td><td>40.95 (0.1)</td></tr><tr><td colspan="5">3B. Results: 10,000 features</td></tr><tr><td> AUCRF</td><td>0.17 (0.05)</td><td>0.33 (0.05)</td><td><bold>0.41 (0.05)</bold></td><td>215.68 (0.01)</td></tr><tr><td> binomialRF</td><td>0.51 (0.12)</td><td>0.14 (0.12)</td><td><bold>0.41 (0.03)</bold></td><td>28.6 (0.03)</td></tr><tr><td> Boruta</td><td><bold>0.72 (0.18)</bold></td><td>0.03 (0.18)</td><td>0.47 (0.01)</td><td><bold>4.68 (0.02)</bold></td></tr><tr><td> Perm</td><td>0.02 (0)</td><td><bold>0.82 (0)</bold></td><td>0.46 (0.03)</td><td>4958.26 (0.03)</td></tr><tr><td> RFE</td><td>0.03 (0)</td><td>0.66 (0)</td><td>0.44 (0.04)</td><td>1950.11 (0.02)</td></tr><tr><td> Vita</td><td>0.03 (0)</td><td>0.52 (0)</td><td>0.45 (0.05)</td><td>1954.32 (0.02)</td></tr></tbody></table><table-wrap-foot><p>The binomialRF and the algorithms in Table <xref rid="Tab1" ref-type="table">1</xref> were tested across a range of simulation scenarios (Table <xref rid="Tab6" ref-type="table">6</xref>). Mean (standard deviation) results are shown and ranked according to decreasing F1-score. In 3A, the results for all techniques are shown up to 2000 features. In 3B, the results are shown for a limited simulation scenario with 10,000 features and 100 seeded genes. Only a subset of methods are presented in 3B as the remaining were either unable to process 10,000 features (i.e., induced memory errors) or introduced rate-limiting computational challenges (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Across both tables, Boruta and binomialRF attain the highest precisions, while PERM the highest recall. More studies are required in high dimensional scenarios to better understand each technique’s behavior. Top accuracies are bolded</p><p><sup>a</sup>Across many runs – the PIMP algorithm resulted in no gene predictions, despite running them using their default parameters, resulting in these low precision and recall values. We varied the parameters with no additional success – so we report these results with an asterisk to note they warrant further investigation</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec5">
      <title>Pure noise selection rate</title>
      <p id="Par19">To complement the variable precision and recall analyses (and thus FDR), and to better understand how often the binomialRF’s detects random noise in the absence of signal, we ran additional simulations in which none of features were informative (i.e., genes seeded <bold><italic>β</italic></bold> =0). Therefore, with an outcome fully independent from the predictors, any selection is based on noise, thus measuring the algorithm’s pure noise selection rate. We ran these analyses using 100, 500, 1000, and 2000 features, and the binomialRF produced – on average – a type I error ranging between 0.5–2%. Future simulations will explore artificial datasets with main effects in absence of interactions to quantify these type I errors.</p>
    </sec>
    <sec id="Sec6">
      <title>UCI ML benchmark data repository</title>
      <p id="Par20">The results for the Madelon dataset show the performance attained by all techniques in a benchmark dataset used to evaluate machine learning algorithms. The results in Table <xref rid="Tab4" ref-type="table">4</xref> indicate that all techniques attain a similar precision and recall, however, with varying model sizes and run times. PIMP, Boruta, and VSURF all result with the smallest models, while PERM results in the largest model. With regards to runtime, similar to the simulations (see Fig. <xref rid="Fig1" ref-type="fig">1</xref>), the binomialRF algorithm runs about 4 times as fast the 2nd fastest algorithm, and about 200 times as fast as the slowest.
<table-wrap id="Tab4"><label>Table 4</label><caption><p>UCI ML madelon dataset validation</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Model size</th><th>Run time</th><th>Precision</th><th>Recall</th></tr></thead><tbody><tr><td>VarSelRF</td><td>23 (13)</td><td>129 (21)</td><td><bold>0.56 (0.01)</bold></td><td><bold>0.56 (0.02)</bold></td></tr><tr><td>VSURF</td><td>3.5 (1.4)</td><td>321 (267)</td><td><bold>0.56 (0.02)</bold></td><td><bold>0.56 (0.03)</bold></td></tr><tr><td>binomialRF</td><td>17.1 (3.9)</td><td><bold>5.6 (2.2)</bold></td><td>0.55 (0.02)</td><td>0.55 (0.01)</td></tr><tr><td>Vita</td><td>13 (5.68)</td><td>1007 (1220)</td><td>0.55 (0.02)</td><td>0.55 (0.02)</td></tr><tr><td>Boruta</td><td>2 (2)</td><td>139 (45)</td><td>0.54 (0.03)</td><td><bold>0.56 (0.04)</bold></td></tr><tr><td>Perm</td><td>240 (13)</td><td>269. (329)</td><td>0.56 (0.08)</td><td>0.54 (0.01)</td></tr><tr><td>AUCRF</td><td>31 (30)</td><td>33 (7.5)</td><td>0.55 (0.04)</td><td>0.54 (0.02)</td></tr><tr><td>RFE</td><td>81 (4.2)</td><td>20 (1.4)</td><td>0.54 (0.06)</td><td>0.54 (0.01)</td></tr><tr><td>EFS</td><td>20 (8.3)</td><td>2617 (2126)</td><td>0.53 (0.02)</td><td>0.54 (0.02)</td></tr><tr><td>PIMP</td><td>1.7 (1.3)</td><td>482 (128)</td><td>0.50 (0.04)</td><td>0.50 (0.01)</td></tr></tbody></table><table-wrap-foot><p>The algorithms in Table <xref rid="Tab1" ref-type="table">1</xref> were tested and compared using the Madelon benchmark dataset from UCI (described in Methods). Mean (standard deviation) results are shown and ranked according to decreasing harmonic mean of precision and recall of variables. Top accuracies are bolded</p></table-wrap-foot></table-wrap></p>
    </sec>
    <sec id="Sec7">
      <title>TCGA clinical validations in breast and kidney cancers</title>
      <p id="Par21">Table <xref rid="Tab5" ref-type="table">5</xref> shows the results for the breast and kidney cancer TCGA validation studies. The same algorithms from Fig. <xref rid="Fig1" ref-type="fig">1</xref> were included as they were the best suited to analyze high-dimensional datasets. Of note, AUCRF generated memory errors when analyzing the TCGA data and was thus not able to produce results. As demonstrated by prior studies [<xref ref-type="bibr" rid="CR28">28</xref>], some TCGA datasets are relatively easy classification tasks, as the matched samples are separable, allowing reasonable algorithms to accurately split the samples across the class labels. Therefore, one aspect of value-added in bioinformatics feature selection algorithms is to develop an accurate classifier with a minimal set of genes. In Table <xref rid="Tab5" ref-type="table">5</xref>, Boruta and binomialRF both develop strong classifiers with a small set of genes, however binomialRF provides a more interpretable test statistic, runs about 20X faster, and – as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref> – extends to detect interactions at no additional cost.
<table-wrap id="Tab5"><label>Table 5</label><caption><p>TCGA dataset validation</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Time</th><th>Test error</th><th>Model size</th></tr></thead><tbody><tr><td colspan="4">5A. Breast cancer</td></tr><tr><td> binomialRF</td><td>83 (11)</td><td>0 (0)</td><td>27 (4)</td></tr><tr><td> RFE</td><td>100 (13)</td><td>0 (0)</td><td>692 (23)</td></tr><tr><td> Perm</td><td>112 (16)</td><td>0 (0)</td><td>1092 (39)</td></tr><tr><td> Vita</td><td>493 (88)</td><td>0 (0)</td><td>19,933 (10)</td></tr><tr><td> Boruta</td><td>1667 (617)</td><td>0 (0)</td><td>92 (3)</td></tr><tr><td colspan="4">5B. Kidney cancer</td></tr><tr><td> binomialRF</td><td>51 (10)</td><td>0 (0)</td><td>48 (3)</td></tr><tr><td> RFE</td><td>67 (10)</td><td>0 (0)</td><td>592 (55)</td></tr><tr><td> Perm</td><td>73 (12)</td><td>0 (0)</td><td>867 (55)</td></tr><tr><td> Vita</td><td>315 (72)</td><td>0 (0)</td><td>19,760 (41)</td></tr><tr><td> Boruta</td><td>987 (363)</td><td>0 (0)</td><td>24 (2)</td></tr></tbody></table><table-wrap-foot><p>The algorithms in Table <xref rid="Tab1" ref-type="table">1</xref> were tested and compared using the TCGA breast cancer and kidney datasets, reporting the mean (and standard deviation in parentheses). Half of the methods were not included as they encountered computation or memory limitations in running the TCGA datasets</p></table-wrap-foot></table-wrap><fig id="Fig2"><label>Fig. 2</label><caption><p>Biomarker accuracies of the TCGA validation study<bold>.</bold> The TCGA validation study was conducted using breast and kidney cancer datasets, accessed via the R package <italic>TCGA2STAT</italic>. The matched-sample datasets were utilized to determine whether binomialRF could produce an accurate classifier via main effects and interactions. Left, the two binomialRF classifiers (51 identified gene main effects; 39 identified gene-gene interactions) and obtained a classifier as accurate as the original black-box RF model with all ~ 20,000 genes. Right, the two binomialRF classifiers (16 identified gene main effects; 11 identified gene-gene interactions) obtained a classifier as accurate as the original black-box RF model with all ~ 20,000 genes</p></caption><graphic xlink:href="12859_2020_3718_Fig2_HTML" id="MO2"/></fig></p>
      <p id="Par22">Figure <xref rid="Fig2" ref-type="fig">2</xref> illustrates how the binomialRF classifiers, with only 51 genes in breast cancer and 16 in kidney cancer, respectively, obtained comparable performances to that of the highly-accurate black-box classifier with &gt; 19,000 genes results (i.e., precision and recall &gt; 0.98). Furthermore, after identifying key statistical interactions (39 in breast, 11 in kidney), we validated their signal by building a classifier exclusively from them with comparable accuracy.</p>
      <p id="Par23">To validate the identified interactions across both TCGA studies, we constructed networks of their pairwise statistical interactions and assessed whether the log-ratio of the gene expression were distributed differently across tumor and normal samples. Figure <xref rid="Fig3" ref-type="fig">3</xref> provides the statistical interaction networks, as well as exemplar cases of gene-gene interactions in each study. For breast cancer, we present an interaction between SPRY2 and C0L10A1 and for kidney one between TFAP2A and SGPP1. In each study, the two individual genes in isolation are expressed differently across normal-tumor samples indicative of their discrimination power. Further, the log-ratios of both genes show an additional level of statistical signal that is captured from the interaction, suggesting the possibility of biological interaction.
<fig id="Fig3"><label>Fig. 3</label><caption><p>Statistical interactions prioritized by binomialRF in TCGA cancers recapitulate known cancer driver genes. The statistical interaction gene networks (Top) indicate the pairwise biomarker interactions identified by the binomialRF algorithm for the breast (Left) and kidney (Right) cancer datasets. Key features are involved in multiple interactors (super-interactors; e.g., SPRY2; COL10A1). Features names (gene products) found in the literature as associated to cancer pathophysiology are shown in black; those also documented as driving cancer genes in COSMIC are shown in green (Methods); the remainder are grey. Two exemplar statistical interactions (one per dataset) are circled and the log expression of their gene products and of their ratios are shown in the bottom panels. The distribution separation across tumor (green) and normal (orange) cases indicates a potential interaction between these two genes across the cohorts</p></caption><graphic xlink:href="12859_2020_3718_Fig3_HTML" id="MO3"/></fig></p>
    </sec>
  </sec>
  <sec id="Sec8">
    <title>Discussion</title>
    <sec id="Sec9">
      <title>Numerical studies, RF-based feature selection techniques, efficiency gains, and interactions</title>
      <p id="Par24">The averaged results across all simulation designs are presented in Table <xref rid="Tab3" ref-type="table">3</xref><bold>,</bold> with the best values of each category bolded, separated into simulations with up to 2000 features (Table <xref rid="Tab3" ref-type="table">3</xref>A) and a set of analyses with 10,000 features (Table <xref rid="Tab3" ref-type="table">3</xref>B) to account for rate-limiting computational and memory challenges introduced by a number of techniques. In low-dimensional numerical studies, techniques such as AUCRF and EFS result in the smallest prediction error, showcasing their strength in the prediction task. The permutation resampling strategy attains the highest recall, which provides users a tool to identify gene products that are potentially relevant for a disease. Boruta, VSURF, and binomialRF algorithms attain the highest precisions (positive predictive value) with reasonable recall. The results in Table <xref rid="Tab3" ref-type="table">3</xref>B illustrate the need to further develop techniques to better operate in high-dimensional scenarios. Attaining a high recall while labeling half the genome as significant is not ideal; on the other hand, attaining a high precision in labeling only a handful of genes might miss some of the biology at play. The techniques in Table <xref rid="Tab1" ref-type="table">1</xref> do not have a complete grasp of the signal in high-dimensional settings suggesting to a.) continue developing and refining them, and b.) to enrich the analyses at the pathway-level as previous studies have shown that this may facilitate signal detection [<xref ref-type="bibr" rid="CR29">29</xref>] and introduce a biologically-meaningful dimension-reduction step.</p>
      <p id="Par25">Boruta and binomialRF have very similar performances despite sharing no structural similarities (Boruta builds its selection based on creating phony variables to threshold important ones, while binomialRF models splits via correlated Bernoulli trials). This is likely since both impose a rigid cutoff for selection, resulting in small but highly precise feature sets. However, due to these structural differences, binomialRF runs orders of magnitude faster (see Fig. <xref rid="Fig1" ref-type="fig">1</xref> and Table <xref rid="Tab4" ref-type="table">4</xref>) and can explicitly identify statistical interactions, resulting in computational and statistical advantages<bold>.</bold> The PIMP algorithm with the default parameters resulted in many runs with no feature predictions, demonstrating poor performances. In various additional runs, we modified their function parameters with similar results. binomialRF distinguishes itself with the most optimal memory utilization and runtimes. However, it is worth noting that since the algorithm concentrates its search space in the root of the tree, this strategy of feature selection likely results in attaining higher precision as the algorithm tries to find the features with the largest impact in the decision tree. This trade-off translates to our algorithm missing features with smaller impact that appear further down the tree, resulting in a lower recall, as seen in the simulation studies.</p>
      <p id="Par26">Strobl and Zeileis [<xref ref-type="bibr" rid="CR30">30</xref>] demonstrate that i) the <italic>Gini importance</italic> (measure of entropy) is biased towards predictors with many categories, and ii) that growing more trees inflates anticonservative power estimates. To address (i), we recommend the user evaluates sets of genes according to their baseline expression levels [<xref ref-type="bibr" rid="CR31">31</xref>]. For the latter (ii), the binomialRF uses <italic>ntree</italic> parameter (number of trees; Table <xref rid="Tab6" ref-type="table">6</xref>) to calculate a conservative cumulative distribution function (cdf) rather than calculating an anticonservative <bold><italic>F</italic></bold><sub><bold><italic>j</italic></bold></sub> (Eq. <xref rid="Equ1" ref-type="">1</xref>), which mitigates the possibility of overtraining. Our simulations were ran using 500 and 1000 trees with no visible differences across results. We ran five additional simulations (seeding 5/100 genes) using 100, 200, 500, 1000, and 2000 trees to determine the effect of growing more trees. The median results indicate that as the number of trees increases, the metrics tend to converge (data not shown), indicating a stability in the number of trees. For the sampled features parameter, the percentage of features tested in our analyses ranged from 20 to 60%. In addition, for the number of features at each split, we recommend tuning this hyper-parameter via cross-validation. The cross-validated binomialRF function (implemented in our R package) runs a grid-search of equally spaced proportions between 0 and 1 based on the number of folds, and then returns the optimal proportion of features selected for each split.
<table-wrap id="Tab6"><label>Table 6</label><caption><p>Parameters settings for the simulation study</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Parameter</th><th>Values</th></tr></thead><tbody><tr><td>Genome size (<bold><italic>P)</italic></bold></td><td>100, 500, 1000, 2000, 10,000</td></tr><tr><td>Genes seeded (<bold><italic>β</italic></bold>)</td><td>5, 25, 50, 100</td></tr><tr><td>Number of trees (<bold><italic>V)</italic></bold></td><td>500, 1000</td></tr></tbody></table></table-wrap></p>
      <p id="Par27">There are other complementary efforts to improve the efficiency of random forests. Studies [<xref ref-type="bibr" rid="CR32">32</xref>–<xref ref-type="bibr" rid="CR35">35</xref>] focus on subspace sampling methods, reducing the search, and ensuring diversity among the features or cases sampled to make the node-splitting process more efficient, rather than biomarker discoveries. Other sets of techniques such as [<xref ref-type="bibr" rid="CR36">36</xref>] gain efficiency by modifying the learning process. These methods are independent of feature selection and could be combined with any method from Table <xref rid="Tab1" ref-type="table">1</xref> to further improve RF efficiencies.</p>
      <p id="Par28">binomialRF proposes an automated combinatoric memory reduction in the original predictor matrix (Table <xref rid="Tab2" ref-type="table">2</xref>), while other methods from Table <xref rid="Tab1" ref-type="table">1</xref> generally require rate-limiting and memory consuming user-defined explicit interactions by multiplying the <inline-formula id="IEq3"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left(\genfrac{}{}{0pt}{}{P}{k}\right) $$\end{document}</tex-math><mml:math id="M6" display="inline"><mml:mfenced close=")" open="("><mml:mfrac linethickness="0pt"><mml:mi>P</mml:mi><mml:mi>k</mml:mi></mml:mfrac></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq3.gif"/></alternatives></inline-formula> interactions. One limitation of assessing memory computation is the inability to conduct a purely theoretical analysis of memory requirements. Further, it is difficult to assess true memory load across different algorithms as some algorithms are serialized while others offer distributed computing across cores. For example, some memory profiling functions in R simply do not function properly in parallel, making such calculations unfeasible. We will continue looking into this in future studies.</p>
      <p id="Par29">Using trees to identify interactions dates back to [<xref ref-type="bibr" rid="CR37">37</xref>] and partial dependence plots to examine candidate feature interactions. Some algorithms identify sets of conditional or sequential splits, while other strategies (i.e., [<xref ref-type="bibr" rid="CR37">37</xref>]) measure their effect in prediction error. More recently, works such as [<xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR38">38</xref>] look at the frequency of sequence of splits or “decision paths” as a way to determine whether two features interact in the tree-splitting process. For example, iterative random forests (iRF) [<xref ref-type="bibr" rid="CR38">38</xref>] identify decision paths along random forests and captures their prevalence, therefore benefitting from a combinatoric feature space reduction in the interaction search. Similarly, BART conducts interaction screening by looking at inclusion frequencies of pairs of predictors [<xref ref-type="bibr" rid="CR31">31</xref>]. Both of these techniques (one in a frequentist and the other in a Bayesian setting) use inclusion frequencies to determine interaction importance and then provide additional tools to provide cutoffs. We extend on these by modeling decision paths (i.e., pairs of splits) as exchangeable but correlated Bernoulli random variables from which we can conduct hypothesis tests. We construct our algorithm on the same principle of using sequence of splits (i.e., decision paths) to identify interactions and extend them by introducing our modeling framework. binomialRF automatically models these sequential split frequencies into a hypothesis testing framework using a generalization of the binomial distribution that adjusts for tree-to-tree data co-dependency. This contribution provides an alternative <italic>p</italic>-value-based strategy to explicitly rank feature interactions in any order with the binomialRF, using a simple modification of a user-determined parameter, <italic>k</italic>. In future studies, we will focus our experiments and numerical analyses to compare techniques that are explicitly designed to identify interactions (i.e., binomialRF and iRF). Future work will also aim to refine and polish interaction detection within the binomialRF framework and extend the preliminary results and techniques.</p>
      <p id="Par30">In future studies, we will extend these analyses beyond random forest classifiers and compare binomialRF against variable selection techniques across other algorithms. For main effects, a future study should consider comparing binomialRF to the L-norm family of penalties in logistic regression (i.e., LASSO and elastic net), as well as importance metrics in tree boosting models and neural networks, and variables selected in SVM algorithms. To assess the efficacy of interactions and biological networks, one possibility is to implement network-based and graph-based family of penalties in logistic regression. These simulation comparisons across other machine and statistical learning algorithms must be carefully designed to not simulate data that would introduce biases nor favor one set of methods over another, which is beyond the scope of the current study. For example, in our simulation studies, the data were generated following a logistic distribution that would biasedly favor a logistic regression over binomialRF. Therefore, a more comprehensive simulation with various generative models is required to adequately compare binomialRF (and tree-based methods) to feature selection in generalized linear models, neural networks, and support vector machines.</p>
      <p id="Par31">Finally, datasets from the UCI and TCGA repositories were used to externally validate the simulations. While the UCI datasets are not novel, they provide reliable benchmarks for the machine learning community to measure against as well as confirmatory power to the results of the simulations. In addition, validations with TCGA labels served as accuracy measurements (Table <xref rid="Tab5" ref-type="table">5</xref>) in a high-dimensional setting (datasets had approximately 20 thousand features). As shown in Table <xref rid="Tab5" ref-type="table">5</xref>, several of the algorithms listed in Table <xref rid="Tab1" ref-type="table">1</xref> were unable to provide adequate analyses either due to computational or memory limitations, limiting their usability in certain high-dimensional bioinformatics tasks.</p>
    </sec>
    <sec id="Sec10">
      <title>Moving towards interpretable, white-box algorithms</title>
      <p id="Par32">In recent years, there have been substantial efforts to develop more human-interpretable machine learning tools in response to the ethical and safety concerns of using ‘blackbox’ algorithms in medicine [<xref ref-type="bibr" rid="CR21">21</xref>] or in high stake decisions [<xref ref-type="bibr" rid="CR22">22</xref>]. A perspective on <italic>Nature Machine Intelligence</italic> [<xref ref-type="bibr" rid="CR22">22</xref>], the Explainable Machine Learning Challenge in 2018 [<xref ref-type="bibr" rid="CR39">39</xref>], and other initiatives serve as reminders of the ethical advantages of using interpretable white-box models over blackbox ones. Novel software packages and methods (i.e., [<xref ref-type="bibr" rid="CR40">40</xref>, <xref ref-type="bibr" rid="CR41">41</xref>]) bring elements of ensemble learning and RFs into the linear model space to combine the high accuracy of ensemble learners with interpretability of generalized linear models. Other initiatives such as the <italic>iml</italic> R package [<xref ref-type="bibr" rid="CR41">41</xref>] provide post-hoc interpretability tools for blackbox algorithms or provide model-agnostic strategies “to <italic>trust</italic> and act on predictions” [<xref ref-type="bibr" rid="CR42">42</xref>]. These white-box efforts are converging towards producing more explanatory power that improves ethical and safe decision making. Feature selection methods also improve the transparency of machine learning methods. Further, there is a need to develop algorithms that can better illustrate how they identify and rank features. Among feature selection techniques, binomialRF provides more explicit features and their interactions than conventional RF as well as a prioritization statistic. This differs from the majority of other feature selection methods that have been developed for RF, as they do not provide a prioritization among features (Table <xref rid="Tab1" ref-type="table">1</xref>; <italic>p</italic>-value = no). For those that provide <italic>p</italic>-values, they require memory intensive and time-consuming permutation tests.</p>
      <p id="Par33">The feature selection algorithms in Table <xref rid="Tab1" ref-type="table">1</xref> are designed to take a high-dimensional set of features (i.e., genes in a genome) and recommend or prioritize a small but important subset of them. They do this either via soft or hard decisions (i.e., p-value ranks vs. sets of discovered genes), but do not provide directionality of effect (i.e., harmful v. protective effect), limiting actionability. The binomialRF provides an effect size along with a p-value, providing a small improvement in this direction to make these algorithms more ‘white-box’ and interpretable, but it is still not a fully a white box algorithm. In contrast, novel algorithms, such as TreeExplainer [<xref ref-type="bibr" rid="CR43">43</xref>], provide great visualization and model-interpretation tools that provide directionality for feature effects by measuring each feature’s contributions to the prediction. However, TreeExplainer differs from the algorithms in Table <xref rid="Tab1" ref-type="table">1</xref> as it does not provide an automated or decision-boundary-based mechanism to prioritize features. This does not allow for a fair comparison between these methods, resulting in its exclusion from the analysis. Thus, future work should incorporate the interpretive power of new algorithms (such as TreeExplainer) into feature selection, in order to provide a set of prioritized genes as well as the direction of their effect on the outcome.</p>
      <p id="Par34">As recent work by our lab and others have shown, there is a subspace of genomic classifiers and biomarker detection anchored in pathways and ontologies [<xref ref-type="bibr" rid="CR44">44</xref>–<xref ref-type="bibr" rid="CR46">46</xref>] that has yielded promising results in biomarker detection using a priori defined gene sets (i.e., GO [<xref ref-type="bibr" rid="CR47">47</xref>]). Hsueh et al. have explored the subdomain of ontology-anchored gene expression classifiers in random forests [<xref ref-type="bibr" rid="CR48">48</xref>]. They also discuss alternate statistical techniques available for geneset analyses and paved the way towards RF-based geneset analysis. In future work, we will direct our efforts along this path and extend binomialRF to incorporate gene set-anchored feature selection algorithms that explore pathway interactions.</p>
    </sec>
  </sec>
  <sec id="Sec11">
    <title>Conclusion</title>
    <p id="Par35">We propose a new feature selection method for exploring feature interactions in random forests, binomialRF, which substantially improves the computational and memory usage efficiency of random forest classifier algorithms and explicitly reveals RF Classifier features for human interpretation. The simulation studies and theoretical analyses compared to previous methods have shown that binomialRF attains a substantially improved runtime (between 30 and 300 fold speed reduction) and a combinatoric reduction in memory requirement for interaction detection (a 500-fold and 170,000-fold memory reduction, for 2-way and 3-way interactions in genomes with 1000 genes). Out of the ten techniques, binomialRF is also among the top four most accurate (precision, recall) across large scale simulations and benchmark datasets. In addition, in clinical datasets, the prioritized interaction classifiers attain high performance with less than 1% of the features and produce pathophysiologically relevant features (evaluated via curation and external reference standards). We have released an open source package in R on GitHub and have submitted it to the CRAN (R archive) for consideration.</p>
    <p id="Par36">Machine learning algorithms are increasingly required to explain their predictions and features in human-interpretable form for high stake decision making. Therefore, there is a need for methods that provide explicit white-box-style classifiers with the high accuracy rates otherwise observed in conventional blackbox-style algorithms (e.g., random forests). Among feature selection methods designed for random forests, binomialRF proves to be more efficient and as accurate for exploring high order interactions between biomolecular features as compared to ten published methods. This increased efficiency for exploring complexity may contribute to improving therapeutic decision making, which may address existing machine learning gaps in precision medicine.</p>
  </sec>
  <sec id="Sec12">
    <title>Methods</title>
    <p id="Par37">We propose a new method for feature selection in random forests, binomialRF (Fig. <xref rid="Fig4" ref-type="fig">4</xref>), which extends and generalizes the “inclusion frequency” strategy to rank features [<xref ref-type="bibr" rid="CR25">25</xref>] by modeling variable splits at the root of each tree, <bold><italic>T</italic></bold><sub><bold><italic>z</italic></bold></sub><bold><italic>,</italic></bold> as a random variable in a stochastic binomial process. This is used to develop a hypothesis-based procedure to model and determine significant features. In the literature, there are a number of existing powerful feature selection algorithms in RF algorithms (Table <xref rid="Tab1" ref-type="table">1</xref>). However, this work proposes an alternative feature selection method using a binomial framework and demonstrates its operating characteristics in comparison to existing technology. Table <xref rid="Tab1" ref-type="table">1</xref> illustrates the advantages of the proposed binomialRF as it is both <italic>p</italic>-value-based and permutation-free, features not identified in our review of literature.
<fig id="Fig4"><label>Fig. 4</label><caption><p>The binomialRF feature selection algorithm. The binomialRF algorithm is a feature selection technique in random forests (<bold>RF</bold>) that treats each tree as a stochastic binomial process and determines whether a feature is selected more often than by random chance as the optimal splitting variable, using a top-bottom sampling without replacement scheme. The main effects algorithm identifies whether the optimal splitting variables at the root of each tree are selected at random or whether certain features are selected with significantly higher frequencies. The interaction-screening extension is detailed in Section 3. <italic>Legend</italic>: <italic>T</italic><sub>z</sub> = <italic>z</italic><sup><italic>th</italic></sup> tree in random forest; <italic>X</italic><sub><italic>j</italic></sub> = feature j; <italic>F</italic><sub><italic>j</italic></sub> = the observed frequency of selecting <italic>X</italic><sub><italic>j</italic></sub>; Pr = probability; <italic>P</italic> = number of (#) of features; <italic>V</italic> = # of trees in a RF; m = user parameter to limit <italic>P</italic>; g = index of the product</p></caption><graphic xlink:href="12859_2020_3718_Fig4_HTML" id="MO4"/></fig></p>
    <sec id="Sec13">
      <title>binomialRF notation and information gain from tree splits</title>
      <p id="Par38">Given a dataset, we denote the input information by, which is comprised of <bold><italic>N</italic></bold> subjects (usually &lt; 1000) and <bold><italic>P</italic></bold>
<bold>features (</bold>genes in the genome; usually <italic>P</italic>≈ 25,000 expressed genes). Genomics data typically represent the “high-dimensional” scenario, where the number of features is much larger than the sample size <bold><italic>N</italic></bold> (e.g., “ <bold><italic>P &gt;  &gt; N</italic></bold> ”). In the context of binary classification, we denote the outcome variable by <bold><italic>Y</italic></bold>, which differentiates the case and control groups (i.e., “healthy” vs. “tumor” tissue samples). Random Forests <bold>(RF</bold>) are ensemble learning methods that train a collection of randomized decision trees and construct the decision rule based on combining <bold><italic>V</italic></bold> individual trees. We denote a random forest as <bold><italic>RF =</italic></bold> {<bold><italic>T</italic></bold><sub><bold>1</bold></sub>, …, <bold><italic>T</italic></bold><sub><bold><italic>V</italic></bold></sub>}<bold>.</bold> Each individual decision tree, <bold><italic>T</italic></bold><sub><bold><italic>z</italic></bold></sub> (z = 1, …, <bold><italic>V</italic></bold>)<italic>,</italic> is trained by using a random subset of the data and features. This randomization encourages a diverse set of trees and allows each individual tree to make predictions across a variety of features and cases. Each tree only sees <bold><italic>m &lt; P</italic></bold> features in the root when it determines the first optimal feature for splitting the data into two subgroups. The parameter, <bold><italic>m</italic></bold>, is a user-determined input in the random forest algorithm with default values set usually to either <inline-formula id="IEq4"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \boldsymbol{m}=\sqrt{\boldsymbol{P}} $$\end{document}</tex-math><mml:math id="M8" display="inline"><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo mathvariant="bold-italic">=</mml:mo><mml:msqrt><mml:mi mathvariant="bold-italic">P</mml:mi></mml:msqrt></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq4.gif"/></alternatives></inline-formula> or <inline-formula id="IEq5"><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \boldsymbol{m}=\raisebox{1ex}{$\boldsymbol{P}$}\!\left/ \!\raisebox{-1ex}{$\mathbf{3}\ $}\right. $$\end{document}</tex-math><mml:math id="M10" display="inline"><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo mathvariant="bold-italic">=</mml:mo><mml:mfrac bevelled="true"><mml:mi mathvariant="bold-italic">P</mml:mi><mml:mrow><mml:mn mathvariant="bold">3</mml:mn><mml:mspace width="0.25em"/></mml:mrow></mml:mfrac></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq5.gif"/></alternatives></inline-formula>. <bold><italic>F</italic></bold><sub><bold><italic>j,z</italic></bold></sub> denotes the random variable measuring whether feature <italic>X</italic><sub><italic>j</italic></sub> is selected as the splitting variable for tree <italic>T</italic><sub><italic>z</italic></sub> ’s root (Eq. <xref rid="Equ1" ref-type="">1</xref>):
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {F}_{j,z}=\left\{\begin{array}{cc}1\kern0.5em ,&amp; if\kern0.5em root\kern0.5em \left({T}_z\right)={X}_j\\ {}0\kern0.5em ,&amp; otherwise\end{array}\right. $$\end{document}</tex-math><mml:math id="M12" display="block"><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mtable columnalign="center"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext mathvariant="italic">if</mml:mtext><mml:mspace width="0.5em"/><mml:mtext mathvariant="italic">root</mml:mtext><mml:mspace width="0.5em"/><mml:mfenced close=")" open="("><mml:msub><mml:mi>T</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mspace width="0.5em"/><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext mathvariant="italic">otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3718_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par39">This results in <italic>F</italic><sub><italic>j</italic>, <italic>z</italic></sub> following a Bernoulli random variable, <italic>F</italic><sub><italic>j</italic>, <italic>z</italic></sub> ∼ <italic>Bern</italic>(<italic>p</italic><sub><italic>root</italic></sub>). In <bold>binomialRF</bold>, to test whether the feature <italic>X</italic><sub><italic>j</italic></sub> is significant in predicting the outcome <bold>Y</bold>, we build a test statistic <inline-formula id="IEq6"><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\boldsymbol{F}}_{\boldsymbol{j}}={\sum}_{\boldsymbol{z}=\mathbf{1}}^{\boldsymbol{V}}{\boldsymbol{F}}_{\boldsymbol{j},\boldsymbol{z}} $$\end{document}</tex-math><mml:math id="M14" display="inline"><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:msub><mml:mo mathvariant="bold-italic">=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo mathvariant="bold-italic">=</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mi mathvariant="bold-italic">V</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="bold-italic">F</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi><mml:mo mathvariant="bold-italic">,</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:msub></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq6.gif"/></alternatives></inline-formula> to the the null hypothesis of no feature being significant. One would expect that the probability of selecting a feature <italic>X</italic><sub><italic>j</italic></sub> is equal to that of every other feature <italic>X</italic><sub><italic>i</italic></sub>. Therefore, under the null hypothesis, <italic>p</italic><sub><italic>root</italic></sub> is constant across all features and trees. Since trees are not independent as they are sampling the same data, <italic>F</italic><sub><italic>j</italic></sub> follow a <bold><italic>correlated</italic></bold>
<bold>binomial distribution</bold> that accounts for the tree-to-tree sampling co-dependencies (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). The following sections will describe combining the probabilistic framework (2.3), the tree-to-tree sampling co-dependency adjustment (2.4), and the test for significance (2.5).</p>
    </sec>
    <sec id="Sec14">
      <title>Optimal splitting variable and decision trees</title>
      <p id="Par40">Consider a decision tree, <italic>T</italic><sub><italic>z</italic></sub>, in a random forest (Fig. <xref rid="Fig5" ref-type="fig">5</xref>). At the top-most “root” node, <italic>m</italic> features are randomly subsampled from the set of <italic>P</italic> features, and the optimal splitting variable, <italic>X</italic><sub><italic>opt</italic></sub>, is selected as the best feature for separating two classes. Formally, this is stated in Eq. <xref rid="Equ2" ref-type="">2</xref>.
<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\mathrm{X}}_{opt}=\mathrm{argma}{\mathrm{x}}_{X_j}\left(\mathrm{Information}\ \mathrm{Gain}\right) $$\end{document}</tex-math><mml:math id="M16" display="block"><mml:msub><mml:mi mathvariant="normal">X</mml:mi><mml:mi mathvariant="italic">opt</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>argma</mml:mtext><mml:msub><mml:mi mathvariant="normal">x</mml:mi><mml:msub><mml:mi>X</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:msub><mml:mfenced close=")" open="("><mml:mtext>Information Gain</mml:mtext></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3718_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula><fig id="Fig5"><label>Fig. 5</label><caption><p>Decision tree and node variables. In the binary split decision tree, <italic>X</italic><sub>1</sub> is the optimal splitting feature at the root of the tree, and <inline-formula id="IEq7"><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\left\{{\boldsymbol{X}}_{\boldsymbol{j}}\right\}}_{\boldsymbol{j}=\mathbf{1}}^{\mathbf{3}}=\left\{{\boldsymbol{X}}_{\mathbf{1}},{\boldsymbol{X}}_{\mathbf{2}},{\boldsymbol{X}}_{\mathbf{3}}\right\} $$\end{document}</tex-math><mml:math id="M18" display="inline"><mml:msubsup><mml:mfenced close="}" open="{"><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi></mml:msub></mml:mfenced><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi><mml:mo mathvariant="bold-italic">=</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mn mathvariant="bold">3</mml:mn></mml:msubsup><mml:mo mathvariant="bold-italic">=</mml:mo><mml:mfenced close="}" open="{" separators=",,"><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mn mathvariant="bold">1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mn mathvariant="bold">2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mn mathvariant="bold">3</mml:mn></mml:msub></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq7.gif"/></alternatives></inline-formula> is the optimal splitting sequence that indicates a potential <italic>X</italic><sub>1</sub> ⊗ <italic>X</italic><sub>2</sub> ⊗ <italic>X</italic><sub>3</sub> 3-way interaction, where the symbol “ ⊗ ” denotes interactions</p></caption><graphic xlink:href="12859_2020_3718_Fig5_HTML" id="MO5"/></fig></p>
      <p id="Par41">Focusing on the root, under a null hypothesis, each feature has the same probability of being selected as the optimal root splitting feature, denoted by <italic>p</italic><sub><italic>root</italic></sub> = Pr(<italic>X</italic><sub><italic>opt</italic></sub> = <italic>X</italic><sub><italic>j</italic></sub>) ∀ <italic>j</italic> ∈ {1, …, <italic>P</italic>}. The random variable <italic>F</italic><sub><italic>j</italic>, <italic>z</italic></sub> (shown in Eq. <xref rid="Equ1" ref-type="">1</xref>) is an indicator variable that tracks if <italic>X</italic><sub><italic>j</italic></sub> is selected as the optimal variable for the root at tree <italic>T</italic><sub><italic>z</italic></sub> . <italic>F</italic><sub><italic>j</italic>, <italic>z</italic></sub> is a Bernoulli random variable, <italic>F</italic><sub><italic>j</italic>, <italic>z</italic></sub> ∼ <italic>Bern</italic>(<italic>p</italic><sub><italic>root</italic></sub>). If all trees are independent, summing across trees yields <inline-formula id="IEq8"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {F}_j={\sum}_{z=1}^V{F}_{j,z} $$\end{document}</tex-math><mml:math id="M20" display="inline"><mml:msub><mml:mi>F</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>V</mml:mi></mml:msubsup><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq8.gif"/></alternatives></inline-formula> (a binomial random variable). However, trees are not entirely independent since the sampling process creates a co-dependency or correlation across trees.</p>
    </sec>
    <sec id="Sec15">
      <title>Adjusting for tree-to-tree co-dependencies</title>
      <p id="Par42">Each tree in a RF samples <italic>n</italic> ⊂ <italic>N</italic> observations either by subsampling or bootstrapping, which creates a tree-to-tree sampling co-dependency, denoted as <bold><italic>ρ</italic></bold>. In subsampling, the co-dependency between trees is exactly <inline-formula id="IEq9"><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \rho \le \raisebox{1ex}{$n$}\!\left/ \!\raisebox{-1ex}{$m$}\right. $$\end{document}</tex-math><mml:math id="M22" display="inline"><mml:mspace width="0.25em"/><mml:mi>ρ</mml:mi><mml:mo>≤</mml:mo><mml:mfrac bevelled="true"><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mfrac></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq9.gif"/></alternatives></inline-formula>, whereas in bootstrapping, the co-dependency is bounded above, i.e., <inline-formula id="IEq10"><alternatives><tex-math id="M23">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \rho \le \raisebox{1ex}{$n$}\!\left/ \!\raisebox{-1ex}{$m$}\right. $$\end{document}</tex-math><mml:math id="M24" display="inline"><mml:mspace width="0.25em"/><mml:mi>ρ</mml:mi><mml:mo>≤</mml:mo><mml:mfrac bevelled="true"><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mfrac><mml:mspace width="0.25em"/></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq10.gif"/></alternatives></inline-formula>. Therefore, in all cases, <inline-formula id="IEq11"><alternatives><tex-math id="M25">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \rho \le \raisebox{1ex}{$n$}\!\left/ \!\raisebox{-1ex}{$m$}\right. $$\end{document}</tex-math><mml:math id="M26" display="inline"><mml:mi>ρ</mml:mi><mml:mo>≤</mml:mo><mml:mfrac bevelled="true"><mml:mi>n</mml:mi><mml:mi>m</mml:mi></mml:mfrac></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq11.gif"/></alternatives></inline-formula> provides a conservative upper bound on the co-dependency between trees. This upper bound adjusts for this tree-to-tree sampling co-dependency. Since the number of sampled cases is determined by the user as a RF parameter, the tree-to-tree co-dependency is known and does not require any estimations. Kuk and Witt both developed a generalization of the family of distributions for exchangeable binary data [<xref ref-type="bibr" rid="CR49">49</xref>, <xref ref-type="bibr" rid="CR50">50</xref>] by adding an extra parameter to model for correlation or association between binary trials when the correlation/association parameter is known. We model this co-dependency among trees by introducing either Kuk’s or Witt’s generalized correlation adjustment in the <italic>correlbinom</italic> R package [<xref ref-type="bibr" rid="CR49">49</xref>], which is incorporated into the binomialRF model.</p>
    </sec>
    <sec id="Sec16">
      <title>Calculating significance of main RF features</title>
      <p id="Par43">At each <italic>T</italic><sub><italic>z</italic></sub>, <italic>m</italic> &lt; <italic>P</italic> features are subsampled resulting in a probability, <italic>p</italic><sub><italic>root</italic></sub>, of <italic>X</italic><sub><italic>j</italic></sub> being selected by a tree, <italic>T</italic><sub><italic>z</italic></sub>, as shown in Eq. <xref rid="Equ3" ref-type="">3</xref>:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M27">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {p}_{root}=1-\left({\prod}_{g=1}^m\frac{P-g}{P-\left(g-1\right)}\left(\frac{1}{m}\right)\right) $$\end{document}</tex-math><mml:math id="M28" display="block"><mml:msub><mml:mi>p</mml:mi><mml:mtext mathvariant="italic">root</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3718_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par44">Using Eq. <xref rid="Equ3" ref-type="">3</xref>, we can calculate whether <italic>X</italic><sub><italic>j</italic></sub> provides a statistically significant information gain to discriminate among classes if <italic>F</italic><sub><italic>j</italic></sub> exceeds the critical value <italic>Q</italic><sub><italic>α</italic>, <italic>V</italic>, <italic>p</italic></sub>, (where <italic>Q</italic><sub><italic>α</italic>, <italic>V</italic>, <italic>p</italic></sub> is the 1 − <italic>α</italic>
<sup>th</sup> quantile of a correlated binomial distribution with <italic>V</italic> trials, <italic>p</italic> is the probability of success, and <italic>ρ</italic> correlation). For multiple hypothesis tests, we adjust our procedure for multiplicity using Benjamini- Yekutieli (BY) [<xref ref-type="bibr" rid="CR51">51</xref>] false discovery rate.</p>
    </sec>
    <sec id="Sec17">
      <title>Calculating significance of RF feature interactions</title>
      <p id="Par45">In classical linear models when detecting 2-way interactions, interactions are included in a multiplicative fashion and treated as separate features with their own linear coefficients. Here, we denote <bold><italic>X</italic></bold><sub><bold><italic>i</italic></bold></sub> <bold><italic>⊗ X</italic></bold><sub><bold><italic>j</italic></bold></sub> as an interaction between features <italic>X</italic><sub><italic>i</italic></sub> and <italic>X</italic><sub><italic>j</italic></sub>. One condition imposed in mathematical interaction selection is strong heredity which states that if the interaction <italic>X</italic><sub><italic>i</italic></sub> ⊗ <italic>X</italic><sub><italic>j</italic></sub> is included in the model, then their main effects <italic>X</italic><sub><italic>i</italic></sub> and <italic>X</italic><sub><italic>j</italic></sub> must be included. Similarly, under weak heredity, at least one of the two main effects must be included in the model if their interaction term is included. In the context of linear models, several existing methods have been proposed to select interactions and studied in terms of their feasibility and utility [<xref ref-type="bibr" rid="CR52">52</xref>, <xref ref-type="bibr" rid="CR53">53</xref>]. Tree-based methods uniquely bypass these conditions as strong heredity hierarchy is automatically induced resulting from the binary split tree’s structure. As Friedman explains, trees naturally identify interactions based on their sequential, conditional splitting process [<xref ref-type="bibr" rid="CR38">38</xref>]. This “greedy” search strategy reduces the space from all possible, <inline-formula id="IEq12"><alternatives><tex-math id="M29">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left(\genfrac{}{}{0pt}{}{P}{2}\right) $$\end{document}</tex-math><mml:math id="M30" display="inline"><mml:mfenced close=")" open="("><mml:mfrac linethickness="0pt"><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq12.gif"/></alternatives></inline-formula> interactions, to only those selected by trees, greatly reducing computational cost and inefficiencies in identifying interactions. We generalize the binomialRF to model interactions by considering pairs or sets of sequential splits as random variables and modeling them with the appropriate test statistic and hypothesis test.</p>
      <p id="Par46">To modify the binomialRF algorithm to search for 2-way interactions, we add another product term to Eq. <xref rid="Equ3" ref-type="">3</xref> denoting the second feature in the interaction set to calculate <italic>p</italic><sub>2 − <italic>way</italic></sub> (Eq. <xref rid="Equ4" ref-type="">4</xref>).
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M31">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {p}_{2- way}=\frac{1}{2}\left[\ \left(\ 1-\left({\prod}_{g=1}^m\frac{P-g}{P-\left(g-1\right)}\left(\frac{1}{m}\right)\right)\ \right)\left(\ 1-\left({\prod}_{g=1}^m\frac{\left(P-1\right)-g}{\left(P-1\right)-\left(g-1\right)}\left(\frac{1}{m}\right)\right)\ \right)\right] $$\end{document}</tex-math><mml:math id="M32" display="block"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="italic">way</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mfenced close="]" open="["><mml:mrow><mml:mspace width="0.25em"/><mml:mfenced close=")" open="("><mml:mrow><mml:mspace width="0.25em"/><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace width="0.25em"/></mml:mrow></mml:mfenced><mml:mfenced close=")" open="("><mml:mrow><mml:mspace width="0.25em"/><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace width="0.25em"/></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3718_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par47">Since we are interested in selecting interactions across variables, if <italic>X</italic><sub><italic>j</italic></sub> is selected at the root node, then it is no longer available for subsequent selection. Thus, we replace <italic>P</italic> with (<italic>P</italic> − 1). Further, since the interaction can happen two different ways (via the left or right child node), we include a normalizing constant of ½ to account for both ways in which the interaction could occur. Figure <xref rid="Fig6" ref-type="fig">6</xref>a illustrates the binomialRF extension to identify 2-way interactions by looking at feature pairs at the root node.
<fig id="Fig6"><label>Fig. 6</label><caption><p>Calculating RF features’ interactions. <bold>a</bold> 2-way Interactions. To extend the binomialRF algorithm for 2-way interaction selection, we define the test statistic which reflects the frequency, <italic>F</italic><sub><italic>ij</italic></sub> of the pair <italic>X</italic><sub><italic>i</italic></sub> ⊗ <italic>X</italic><sub><italic>j</italic></sub> occurring in the random forest. In particular, the probability of an interaction term occurring by random chance is recalculated and normalized by a factor of a half. <bold>b</bold>
<bold><italic>K</italic></bold>-way interactions, <bold><italic>K</italic></bold> = 4. Here, we illustrate the tree traversal process to identify all 4-way interactions, <inline-formula id="IEq13"><alternatives><tex-math id="M33">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \otimes {\boldsymbol{X}}_{i=1}^4 $$\end{document}</tex-math><mml:math id="M34" display="inline"><mml:mo>⊗</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq13.gif"/></alternatives></inline-formula>, with each color denoting a possible interaction path. The legend on the right shows how each interaction path results in a set of 4-way feature interactions. In general, for any user-desired <italic>K</italic>, the k.binomialRF algorithm traverses the tree via dynamic tree programming to identify all possible paths from the <italic>K</italic>-terminal nodes to the root, where <bold><italic>K</italic></bold>-terminal nodes are all nodes <italic>K</italic>-steps away from the root node</p></caption><graphic xlink:href="12859_2020_3718_Fig6_HTML" id="MO6"/></fig></p>
      <p id="Par48">To generalize Eq. <xref rid="Equ4" ref-type="">4</xref> into multi-way interactions and calculate <italic>p</italic><sub><italic>K</italic> − <italic>way</italic></sub>, we first note that for any multi-way interaction of size K in a binary split tree results in at most 2<sup><italic>K</italic> − 1</sup> terminal nodes. Therefore, there are 2<sup><italic>K</italic> − 1</sup> possible ways of obtaining the <italic>K</italic>-way interaction (Fig. <xref rid="Fig6" ref-type="fig">6</xref>b). Thus, the normalizing constant in Eq. <xref rid="Equ4" ref-type="">4</xref> is replaced with 2<sup><italic>K</italic> − 1</sup> in Eq. <xref rid="Equ5" ref-type="">5</xref> as a conservative bound on the probability<bold>.</bold> The product of two terms in Eq. <xref rid="Equ4" ref-type="">4</xref> is now expanded to the product of <italic>K</italic> terms (each term representing the probability of selecting one individual feature in the interaction set), and (<italic>P</italic> − 1) is replaced with (<italic>P</italic> − <italic>k</italic>) to account for sampling without replacement, which yields Eq. <xref rid="Equ5" ref-type="">5</xref>.
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M35">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {p}_{K- way}=\frac{1}{2^{K-1}}{\prod}_{k=1}^K\left(\ 1-\left({\prod}_{g=1}^m\frac{\left(P-k\right)-g}{\left(P-k\right)-\left(g-1\right)}\left(\frac{1}{m}\right)\right)\ \right) $$\end{document}</tex-math><mml:math id="M36" display="block"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="italic">way</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:mi>K</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mfrac><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:mspace width="0.25em"/><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mo>∏</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mi>g</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac></mml:mfenced></mml:mrow></mml:mfenced><mml:mspace width="0.25em"/></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="12859_2020_3718_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par49">Next, we update the hypothesis test and modify it to identify 2-way interactions for all possible <inline-formula id="IEq14"><alternatives><tex-math id="M37">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \otimes {X}_{i=1}^K $$\end{document}</tex-math><mml:math id="M38" display="inline"><mml:mo mathvariant="bold-italic">⊗</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:msubsup></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq14.gif"/></alternatives></inline-formula> sets.</p>
    </sec>
    <sec id="Sec18">
      <title>Evaluation via simulations</title>
      <p id="Par50">To understand the strengths and limitations of the binomialRF feature selection algorithm and to compare its performance with state-of-the-art methods, we conduct a variety of simulations and trials against the Madelon benchmark dataset from the University of California – Irvine (UCI), and clinical datasets from The Cancer Genome Atlas (TCGA).</p>
      <p id="Par51">To evaluate each technique’s feature selection accuracy, we measure model size (# of genes discovered), test error, variable precision and recall, and pure noise selection rate. For variable precision and recall, we measure how precise the gene discoveries were and what proportion of the seeded genes in the simulation they captured. Since precision is 1-False Discovery Rate (FDR), variable FDR is implicitly illustrated in Table <xref rid="Tab3" ref-type="table">3</xref> via the variable precision column, and states how much noise is detected on average relative to the signal detected by the model. The pure noise statistic complements the FDR analysis by analyzing how much pure noise the algorithm detects in absence of a true signal. The five metrics listed above were measured using the equations below:
<disp-formula id="Equ6"><label>6-10</label><alternatives><tex-math id="M39">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\displaystyle \begin{array}{c}\mathrm{Model}\kern0.5em \mathrm{Size}=\left|\mathrm{Genes}\kern0.5em \mathrm{discovered}\right|,\mathrm{Precision}=\frac{TP}{TP+ FP\hbox{'}}\mathrm{Recall}=\frac{TP}{TP+ FN\hbox{'}}\\ {}\mathrm{Test}\kern0.5em \mathrm{Error}={\sum}_i\left({\overset{\frown }{y}}_i={y}_i\right),\kern0.5em \mathrm{Pure}\kern0.5em \mathrm{Noise}\kern0.5em \mathrm{Selection}\kern0.5em \mathrm{Rate}=\frac{\#\kern0.5em Uninformative\kern0.5em features}{\#\kern0.5em Total\kern0.5em features}\end{array}} $$\end{document}</tex-math><mml:math id="M40" display="block"><mml:mtable columnalign="center" displaystyle="true"><mml:mtr><mml:mtd><mml:mtext>Model</mml:mtext><mml:mspace width="0.5em"/><mml:mtext>Size</mml:mtext><mml:mo>=</mml:mo><mml:mfenced close="|" open="|"><mml:mrow><mml:mtext>Genes</mml:mtext><mml:mspace width="0.5em"/><mml:mtext>discovered</mml:mtext></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mtext>Precision</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="italic">TP</mml:mi><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FP</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:mfrac><mml:mtext>Recall</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mi mathvariant="italic">TP</mml:mi><mml:mrow><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>Test</mml:mtext><mml:mspace width="0.5em"/><mml:mtext>Error</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>⌢</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>,</mml:mo><mml:mspace width="0.5em"/><mml:mtext>Pure</mml:mtext><mml:mspace width="0.5em"/><mml:mtext>Noise</mml:mtext><mml:mspace width="0.5em"/><mml:mtext>Selection</mml:mtext><mml:mspace width="0.5em"/><mml:mtext>Rate</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>#</mml:mo><mml:mspace width="0.5em"/><mml:mtext mathvariant="italic">Uninformative</mml:mtext><mml:mspace width="0.5em"/><mml:mtext mathvariant="italic">features</mml:mtext></mml:mrow><mml:mrow><mml:mo>#</mml:mo><mml:mspace width="0.5em"/><mml:mtext mathvariant="italic">Total</mml:mtext><mml:mspace width="0.5em"/><mml:mtext mathvariant="italic">features</mml:mtext></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="12859_2020_3718_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula></p>
      <p id="Par52">These simulation scenarios generate logistically-distributed data to mimic binary classification settings in gene expression data using parameters described in Table <xref rid="Tab6" ref-type="table">6</xref>: genome size = the dimension of the <bold>X</bold> matrix, a coefficient vector <bold><italic>β</italic></bold> that denotes the number of genes seeded linked to the outcome, and the number of trees <bold><italic>V</italic></bold> grown in the random forest. The parameters used to grow the random forests were V = 500 and 1000 trees, while the number of features selected at each split was set to the default value of 33% (see discussion for additional sensitivity analysis experiments on this parameter). The first two parameters are used to generate the design matrix <bold><italic>X</italic></bold><sub><bold><italic>N × P</italic></bold></sub>, generate the binary class vector <bold><italic>Y</italic></bold> using a logistic regression model.</p>
      <p id="Par53">To determine the performance of binomialRF in detecting important interactions, we conduct a simulation study with 30 total features in which we seeded 4 main effects and all 6 possible pairwise interactions. Since the interactions have to be explicitly multiplied in the design matrix, all techniques except binomialRF had a design matrix with all <inline-formula id="IEq15"><alternatives><tex-math id="M41">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ 30+\left(\genfrac{}{}{0pt}{}{30}{2}\right) $$\end{document}</tex-math><mml:math id="M42" display="inline"><mml:mn>30</mml:mn><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mfrac linethickness="0pt"><mml:mn>30</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mfenced></mml:math><inline-graphic xlink:href="12859_2020_3718_Article_IEq15.gif"/></alternatives></inline-formula> = 465 features, and the task was to detect all 6 interactions. Since binomialRF can detect interactions from the original design matrix, we used the original matrix with 30 variables first to identify the main effects and then a second time to identify interactions from main effects.</p>
      <p id="Par54">To evaluate computational runtime and efficiency, we measure the theoretical and empirical results of running the feature selection algorithms (Table <xref rid="Tab1" ref-type="table">1</xref>). To measure empirical runtime, 3 simulation studies were run using simulated genomes with 10, 100, and 1000 genes, and we measured their runtime (in seconds) 500 times across each scenario. Figure <xref rid="Fig1" ref-type="fig">1</xref> presents the boxplot of runtimes, measured in seconds and graphed in incremental powers of 10 (i.e., 10<sup>1</sup>, 10<sup>2</sup>, 10<sup>3</sup>, …), to illustrate the difference in magnitudes. To evaluate the theoretical computational efficiency of binomialRF, we compare the theoretical memory requirements of each method described in Table <xref rid="Tab1" ref-type="table">1</xref> to identify interactions. Since binomialRF can detect interactions using the original design matrix, while other techniques require explicitly mapping the gene-gene interactions, Table <xref rid="Tab2" ref-type="table">2</xref> compares the memory gain attained across genomes with 10, 100, and 1000 genes when trying to identify 2-way and 3-way interactions.</p>
    </sec>
    <sec id="Sec19">
      <title>Evaluation in UCI benchmark and TCGA clinical sets</title>
      <p id="Par55">To determine the utility of the binomialRF feature selection algorithm in translational bioinformatics, we conduct a validation study using data from the University of California – Irvine machine learning repository (UCI, hereinafter) and from The Cancer Genome Atlas (TCGA; Table <xref rid="Tab7" ref-type="table">7</xref>). The UCI machine learning repository contains over 480 datasets available as benchmarks for machine learning developers to test their algorithms. We present results for all techniques in the Madelon dataset and illustrate their performances using classification accuracy metrics (cases) presented above in Eqs. (<xref rid="Equ6" ref-type="">6-10</xref>). Since true variables are not known in these datasets, variable selection accuracies are not calculated. For the TCGA datasets, we only present results for a subset of the methods that did not encounter memory or computation issues.
<table-wrap id="Tab7"><label>Table 7</label><caption><p>TCGA validation study datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Description</th><th>Breast cancer</th><th>Kidney cancer</th></tr></thead><tbody><tr><td><bold>Cohort</bold></td><td><italic>194 matched tumor-normal samples</italic></td><td><italic>130 matched tumor-normal samples</italic></td></tr><tr><td><bold>Outcome prediction</bold></td><td><p>97 tumor,</p><p>97 normal samples</p></td><td><p>65 tumor,</p><p>65 normal samples</p></td></tr><tr><td><bold>Access</bold></td><td><italic>TCGASTAT;;getTCGA</italic></td><td><italic>TCGASTAT;;getTCGA</italic></td></tr></tbody></table></table-wrap></p>
      <p id="Par56">We selected the TCGA breast and kidney cancers as two representative datasets with at least 100 matched normal-tumor samples (Table <xref rid="Tab7" ref-type="table">7</xref>). The data were downloaded via the R package <italic>TCGA2STAT</italic> [<xref ref-type="bibr" rid="CR54">54</xref>], accessed 2020/01, using R.3.5.0. Both RNA sequencing datasets were normalized using RPKM [<xref ref-type="bibr" rid="CR55">55</xref>] and matched into tumor-normal samples. With many prior studies using the TCGA datasets, our goal was to conduct a binomialRF case study to i) confirm the clinical findings, ii) attain similar prediction performance, and iii) evaluate qualitatively the main effect features and their prioritized interactions. To validate the binomialRF interaction algorithm, we extend the validation of the TCGA datasets <italic>by proposing statistical gene-gene interaction discoveries</italic> and build a classifier from these interactions. We then evaluate their cancer relevance in two ways: (i) a review of literature by trained curators to identify the involvement of these transcripts in cancer pathophysiology, and (ii) a comparison of transcripts with the cancer-driving genes of the COSMIC knowledge-base [<xref ref-type="bibr" rid="CR56">56</xref>].</p>
    </sec>
    <sec id="Sec20">
      <title>binomialRF implemented as open source package</title>
      <p id="Par57">The binomialRF R package, wrapping around <italic>randomForest</italic> R package [<xref ref-type="bibr" rid="CR57">57</xref>], is freely available on on CRAN (stable release), with accompanying documentation and help files while experimental updates are released on the Github repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/SamirRachidZaim/binomialRF">https://github.com/SamirRachidZaim/binomialRF</ext-link>). The following repository contains all the code and results presented in this manuscript (<ext-link ext-link-type="uri" xlink:href="https://github.com/SamirRachidZaim/binomialRF_simulationStudy">https://github.com/SamirRachidZaim/binomialRF_simulationStudy</ext-link>).</p>
    </sec>
  </sec>
</body>
<back>
  <glossary>
    <title>Abbreviations</title>
    <def-list>
      <def-item>
        <term>⊗</term>
        <def>
          <p id="Par4">Symbol denoting interaction</p>
        </def>
      </def-item>
      <def-item>
        <term>BY</term>
        <def>
          <p id="Par5">Benjamini Yekutieli adjustment</p>
        </def>
      </def-item>
      <def-item>
        <term>cdf</term>
        <def>
          <p id="Par6">cumulative distribution function</p>
        </def>
      </def-item>
      <def-item>
        <term>GO</term>
        <def>
          <p id="Par7">Gene Ontology</p>
        </def>
      </def-item>
      <def-item>
        <term>RF</term>
        <def>
          <p id="Par8">Random forest</p>
        </def>
      </def-item>
      <def-item>
        <term>UCI</term>
        <def>
          <p id="Par9">University of California – Irvine</p>
        </def>
      </def-item>
      <def-item>
        <term>TCGA</term>
        <def>
          <p id="Par10">The Cancer Genome Atlas</p>
        </def>
      </def-item>
      <def-item>
        <term>iRF</term>
        <def>
          <p id="Par11">iterative Random Forests</p>
        </def>
      </def-item>
    </def-list>
  </glossary>
  <fn-group>
    <fn>
      <p>
        <bold>Publisher’s Note</bold>
      </p>
      <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <ack>
    <title>Acknowledgements</title>
    <p>The authors would like to acknowledge the University of Arizona’s High-Performance Computing (HPC) for providing the space and computing hours to conduct our simulation studies and analyses.</p>
    <sec id="FPar1">
      <title>Conflict of interest</title>
      <p id="Par58">None declared.</p>
    </sec>
  </ack>
  <notes notes-type="author-contribution">
    <title>Authors’ contributions</title>
    <p>SRZ conducted all the analyses in R; SRZ, HHZ and YAL contributed to the analytical framework and analyses; all authors contributed to the evaluation and interpretation of the study; SRZ, JB, WC, LW, and CK contributed to the figures; SRZ, JB, WC, LW, and CK contributed to the tables; JB, WC, LW contributed to the evaluation of the clinical studies; SRZ, HHZ, CK, and YAL contributed to the writing of the manuscript; all authors read and approved the final manuscript.</p>
  </notes>
  <notes notes-type="funding-information">
    <title>Funding</title>
    <p>This work was supported in part by The University of Arizona Health Sciences Center for Biomedical Informatics and Biostatistics, the BIO5 Institute, and the NIH (U01AI122275, NCI P30CA023074, 1UG3OD023171, NSF 1740858). UAHS supported the salaries of SRZ, and in part those of CK, YAL, JB, WC, LW; U01AI122275 and 1UG3OD023171 supported in part the salary of YAL, NSF 1740858 supported in part the salary of HHZ. This article did not receive sponsorship for publication.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Availability of data and materials</title>
    <p>The simulated datasets were generated dynamically in the numerical studies and are available in the R scripts in the Github repository, under the code subdirectory which can be accessed via the following link: <ext-link ext-link-type="uri" xlink:href="https://github.com/SamirRachidZaim/binomialRF_simulationStudy/code">https://github.com/SamirRachidZaim/binomialRF_simulationStudy/code</ext-link> (see R scripts titled “simulation_XXX.R”). The “Madelon” benchmark dataset was obtained from the UCI Machine Learning repository [<ext-link ext-link-type="uri" xlink:href="https://archive.ics.uci.edu/ml/datasets/Madelon">https://archive.ics.uci.edu/ml/datasets/Madelon</ext-link>], and the TCGA Breast and Renal Cancer datasets were obtained from the TCGA repository using the <italic>TCGA2STAT</italic> R library. We documented their download and access in our ‘accessTCGA.R’ R script in the Github repository under the TCGA_validation folder (binomialRF_simulationStudy/ code/TCGA_validation). Our open-source <italic>binomialRF</italic> R package is available for installation on CRAN.</p>
  </notes>
  <notes id="FPar2">
    <title>Ethics approval and consent to participate</title>
    <p id="Par59">This study used publicly available datasets and does not require ethics approval nor consent of participants.</p>
  </notes>
  <notes id="FPar3">
    <title>Consent for publication</title>
    <p id="Par60">All authors have read and consented to publish this material.</p>
  </notes>
  <notes id="FPar4" notes-type="COI-statement">
    <title>Competing interests</title>
    <p id="Par61">The authors have no financial competing interests nor non-financial competing interests.</p>
  </notes>
  <ref-list id="Bib1">
    <title>References</title>
    <ref id="CR1">
      <label>1.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Breiman</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Random forests</article-title>
        <source>Mach Learn</source>
        <year>2001</year>
        <volume>45</volume>
        <fpage>5</fpage>
        <lpage>32</lpage>
      </element-citation>
    </ref>
    <ref id="CR2">
      <label>2.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Chen</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Ishwaran</surname>
            <given-names>H</given-names>
          </name>
        </person-group>
        <article-title>Random forests for genomic data analysis</article-title>
        <source>Genomics</source>
        <year>2012</year>
        <volume>99</volume>
        <fpage>323</fpage>
        <lpage>329</lpage>
        <?supplied-pmid 22546560?>
        <pub-id pub-id-type="pmid">22546560</pub-id>
      </element-citation>
    </ref>
    <ref id="CR3">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bienkowska</surname>
            <given-names>JR</given-names>
          </name>
          <name>
            <surname>Dalgin</surname>
            <given-names>GS</given-names>
          </name>
          <name>
            <surname>Batliwalla</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Allaire</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Roubenoff</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Gregersen</surname>
            <given-names>PK</given-names>
          </name>
          <name>
            <surname>Carulli</surname>
            <given-names>JP</given-names>
          </name>
        </person-group>
        <article-title>Convergent random Forest predictor: methodology for predicting drug response from genome-scale data applied to anti-TNF response</article-title>
        <source>Genomics</source>
        <year>2009</year>
        <volume>94</volume>
        <fpage>423</fpage>
        <lpage>432</lpage>
        <?supplied-pmid 19699293?>
        <pub-id pub-id-type="pmid">19699293</pub-id>
      </element-citation>
    </ref>
    <ref id="CR4">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Boulesteix</surname>
            <given-names>AL</given-names>
          </name>
          <name>
            <surname>Janitza</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Kruppa</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>König</surname>
            <given-names>IR</given-names>
          </name>
        </person-group>
        <article-title>Overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics</article-title>
        <source>Wiley Interdiscip Rev Data Min Knowl Discov</source>
        <year>2012</year>
        <volume>2</volume>
        <fpage>493</fpage>
        <lpage>507</lpage>
      </element-citation>
    </ref>
    <ref id="CR5">
      <label>5.</label>
      <mixed-citation publication-type="other">Diaz-Uriarte R. GeneSrF and varSelRF: a web-based tool and R package for gene selection and classification using random forest. BMC Bioinformatics. 2007;8(1):328.</mixed-citation>
    </ref>
    <ref id="CR6">
      <label>6.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Díaz-Uriarte</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>De Andres</surname>
            <given-names>SA</given-names>
          </name>
        </person-group>
        <article-title>Gene selection and classification of microarray data using random forest</article-title>
        <source>BMC Bioinformatics</source>
        <year>2006</year>
        <volume>7</volume>
        <fpage>3</fpage>
        <?supplied-pmid 16398926?>
        <pub-id pub-id-type="pmid">16398926</pub-id>
      </element-citation>
    </ref>
    <ref id="CR7">
      <label>7.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Goldstein</surname>
            <given-names>BA</given-names>
          </name>
          <name>
            <surname>Hubbard</surname>
            <given-names>AE</given-names>
          </name>
          <name>
            <surname>Cutler</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Barcellos</surname>
            <given-names>LF</given-names>
          </name>
        </person-group>
        <article-title>An application of random forests to a genome-wide association dataset: methodological considerations &amp; new findings</article-title>
        <source>BMC Genet</source>
        <year>2010</year>
        <volume>11</volume>
        <fpage>49</fpage>
        <?supplied-pmid 20546594?>
        <pub-id pub-id-type="pmid">20546594</pub-id>
      </element-citation>
    </ref>
    <ref id="CR8">
      <label>8.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Izmirlian</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>Application of the random forest classification algorithm to a SELDI-TOF proteomics study in the setting of a cancer prevention trial</article-title>
        <source>Ann N Y Acad Sci</source>
        <year>2004</year>
        <volume>1020</volume>
        <fpage>154</fpage>
        <lpage>174</lpage>
        <?supplied-pmid 15208191?>
        <pub-id pub-id-type="pmid">15208191</pub-id>
      </element-citation>
    </ref>
    <ref id="CR9">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Jiang</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Wang</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Ma</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Sun</surname>
            <given-names>X</given-names>
          </name>
          <name>
            <surname>Lu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>MiPred: classification of real and pseudo microRNA precursors using random forest prediction model with combined features</article-title>
        <source>Nucleic Acids Res</source>
        <year>2007</year>
        <volume>35</volume>
        <fpage>W339</fpage>
        <lpage>W344</lpage>
        <?supplied-pmid 17553836?>
        <pub-id pub-id-type="pmid">17553836</pub-id>
      </element-citation>
    </ref>
    <ref id="CR10">
      <label>10.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Archer</surname>
            <given-names>KJ</given-names>
          </name>
          <name>
            <surname>Kimes</surname>
            <given-names>RV</given-names>
          </name>
        </person-group>
        <article-title>Empirical characterization of random forest variable importance measures</article-title>
        <source>Comput Stat Data Anal</source>
        <year>2008</year>
        <volume>52</volume>
        <fpage>2249</fpage>
        <lpage>2260</lpage>
      </element-citation>
    </ref>
    <ref id="CR11">
      <label>11.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Genuer</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Poggi</surname>
            <given-names>J-M</given-names>
          </name>
          <name>
            <surname>Tuleau-Malot</surname>
            <given-names>C</given-names>
          </name>
        </person-group>
        <article-title>VSURF: an R package for variable selection using random forests</article-title>
        <source>The R Journal</source>
        <year>2015</year>
        <volume>7</volume>
        <fpage>19</fpage>
        <lpage>33</lpage>
      </element-citation>
    </ref>
    <ref id="CR12">
      <label>12.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Szymczak</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Holzinger</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Dasgupta</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Malley</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Molloy</surname>
            <given-names>AM</given-names>
          </name>
          <name>
            <surname>Mills</surname>
            <given-names>JL</given-names>
          </name>
          <name>
            <surname>Brody</surname>
            <given-names>LC</given-names>
          </name>
          <name>
            <surname>Stambolian</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Bailey-Wilson</surname>
            <given-names>JE</given-names>
          </name>
        </person-group>
        <article-title>r2VIM: a new variable selection method for random forests in genome-wide association studies</article-title>
        <source>BioData Min</source>
        <year>2016</year>
        <volume>9</volume>
        <fpage>7</fpage>
        <?supplied-pmid 26839594?>
        <pub-id pub-id-type="pmid">26839594</pub-id>
      </element-citation>
    </ref>
    <ref id="CR13">
      <label>13.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kursa</surname>
            <given-names>MB</given-names>
          </name>
          <name>
            <surname>Rudnicki</surname>
            <given-names>WR</given-names>
          </name>
        </person-group>
        <article-title>Feature selection with the Boruta package</article-title>
        <source>J Stat Softw</source>
        <year>2010</year>
        <volume>36</volume>
        <fpage>1</fpage>
        <lpage>13</lpage>
      </element-citation>
    </ref>
    <ref id="CR14">
      <label>14.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Altmann</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Toloşi</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Sander</surname>
            <given-names>O</given-names>
          </name>
          <name>
            <surname>Lengauer</surname>
            <given-names>T</given-names>
          </name>
        </person-group>
        <article-title>Permutation importance: a corrected feature importance measure</article-title>
        <source>Bioinformatics</source>
        <year>2010</year>
        <volume>26</volume>
        <fpage>1340</fpage>
        <lpage>1347</lpage>
        <?supplied-pmid 20385727?>
        <pub-id pub-id-type="pmid">20385727</pub-id>
      </element-citation>
    </ref>
    <ref id="CR15">
      <label>15.</label>
      <mixed-citation publication-type="other">Zaim SR, Kenost C, Lussier YA, Zhang HH. binomialRF: scalable feature selection and screening for random forests to identify biomarkers and their interactions. bioRxiv. 2019:681973.</mixed-citation>
    </ref>
    <ref id="CR16">
      <label>16.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Neumann</surname>
            <given-names>U</given-names>
          </name>
          <name>
            <surname>Genze</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Heider</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>EFS: an ensemble feature selection tool implemented as R-package and web-application</article-title>
        <source>BioData Min</source>
        <year>2017</year>
        <volume>10</volume>
        <fpage>21</fpage>
        <?supplied-pmid 28674556?>
        <pub-id pub-id-type="pmid">28674556</pub-id>
      </element-citation>
    </ref>
    <ref id="CR17">
      <label>17.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Calle</surname>
            <given-names>ML</given-names>
          </name>
          <name>
            <surname>Urrea</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Boulesteix</surname>
            <given-names>A-L</given-names>
          </name>
          <name>
            <surname>Malats</surname>
            <given-names>N</given-names>
          </name>
        </person-group>
        <article-title>AUC-RF: a new strategy for genomic profiling with random forest</article-title>
        <source>Hum Hered</source>
        <year>2011</year>
        <volume>72</volume>
        <fpage>121</fpage>
        <lpage>132</lpage>
        <?supplied-pmid 21996641?>
        <pub-id pub-id-type="pmid">21996641</pub-id>
      </element-citation>
    </ref>
    <ref id="CR18">
      <label>18.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Nguyen</surname>
            <given-names>H-N</given-names>
          </name>
          <name>
            <surname>Ohn</surname>
            <given-names>S-Y</given-names>
          </name>
        </person-group>
        <article-title>Drfe: dynamic recursive feature elimination for gene identification based on random forest</article-title>
        <source>International conference on neural information processing</source>
        <year>2006</year>
        <publisher-loc>Berlin</publisher-loc>
        <publisher-name>Springer</publisher-name>
        <fpage>1</fpage>
        <lpage>10</lpage>
      </element-citation>
    </ref>
    <ref id="CR19">
      <label>19.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tuv</surname>
            <given-names>E</given-names>
          </name>
          <name>
            <surname>Borisov</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Runger</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Torkkola</surname>
            <given-names>K</given-names>
          </name>
        </person-group>
        <article-title>Feature selection with ensembles, artificial variables, and redundancy elimination</article-title>
        <source>J Mach Learn Res</source>
        <year>2009</year>
        <volume>10</volume>
        <fpage>1341</fpage>
        <lpage>1366</lpage>
      </element-citation>
    </ref>
    <ref id="CR20">
      <label>20.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Degenhardt</surname>
            <given-names>F</given-names>
          </name>
          <name>
            <surname>Seifert</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Szymczak</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Evaluation of variable selection methods for random forests and omics data sets</article-title>
        <source>Brief Bioinform</source>
        <year>2017</year>
        <volume>20</volume>
        <fpage>492</fpage>
        <lpage>503</lpage>
      </element-citation>
    </ref>
    <ref id="CR21">
      <label>21.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Char</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Shah</surname>
            <given-names>NH</given-names>
          </name>
          <name>
            <surname>Magnus</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>Implementing machine learning in health care—addressing ethical challenges</article-title>
        <source>N Engl J Med</source>
        <year>2018</year>
        <volume>378</volume>
        <fpage>981</fpage>
        <?supplied-pmid 29539284?>
        <pub-id pub-id-type="pmid">29539284</pub-id>
      </element-citation>
    </ref>
    <ref id="CR22">
      <label>22.</label>
      <mixed-citation publication-type="other">Rudin C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell. 2019;1(5)1:206–15.</mixed-citation>
    </ref>
    <ref id="CR23">
      <label>23.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Možina</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Žabkar</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bratko</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>Argument based machine learning</article-title>
        <source>Artif Intell</source>
        <year>2007</year>
        <volume>171</volume>
        <fpage>922</fpage>
        <lpage>937</lpage>
      </element-citation>
    </ref>
    <ref id="CR24">
      <label>24.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Watson</surname>
            <given-names>DS</given-names>
          </name>
          <name>
            <surname>Krutzinna</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Bruce</surname>
            <given-names>IN</given-names>
          </name>
          <name>
            <surname>Griffiths</surname>
            <given-names>CE</given-names>
          </name>
          <name>
            <surname>McInnes</surname>
            <given-names>IB</given-names>
          </name>
          <name>
            <surname>Barnes</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Floridi</surname>
            <given-names>L</given-names>
          </name>
        </person-group>
        <article-title>Clinical applications of machine learning algorithms: beyond the black box</article-title>
        <source>BMJ</source>
        <year>2019</year>
        <volume>364</volume>
        <fpage>l886</fpage>
        <?supplied-pmid 30862612?>
        <pub-id pub-id-type="pmid">30862612</pub-id>
      </element-citation>
    </ref>
    <ref id="CR25">
      <label>25.</label>
      <mixed-citation publication-type="other">Chipman HA, George EI, McCulloch RE. BART: Bayesian additive regression trees. Ann Appl Stat. 2010;4(1):266–98.</mixed-citation>
    </ref>
    <ref id="CR26">
      <label>26.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <article-title>Regression shrinkage and selection via the lasso</article-title>
        <source>J R Stat Soc Ser B Methodol</source>
        <year>1996</year>
        <volume>58</volume>
        <fpage>267</fpage>
        <lpage>288</lpage>
      </element-citation>
    </ref>
    <ref id="CR27">
      <label>27.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hao</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Feng</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>HH</given-names>
          </name>
        </person-group>
        <article-title>Model selection for high-dimensional quadratic regression via regularization</article-title>
        <source>J Am Stat Assoc</source>
        <year>2018</year>
        <volume>113</volume>
        <fpage>615</fpage>
        <lpage>625</lpage>
      </element-citation>
    </ref>
    <ref id="CR28">
      <label>28.</label>
      <mixed-citation publication-type="other">Kim AA, Zaim SR, Subbian V. Assessing reproducibility and veracity across machine learning techniques in biomedicine: a case study using TCGA data. Int J Med Inform. 2020:104148..</mixed-citation>
    </ref>
    <ref id="CR29">
      <label>29.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Zaim</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Schissler</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Lussier</surname>
            <given-names>YA</given-names>
          </name>
        </person-group>
        <article-title>Emergence of pathway-level composite biomarkers from converging gene set signals of heterogeneous transcriptomic responses</article-title>
        <source>Pac Symp Biocomput</source>
        <year>2018</year>
        <publisher-loc>Singapore</publisher-loc>
        <publisher-name>World Scientific</publisher-name>
        <fpage>484</fpage>
        <lpage>495</lpage>
      </element-citation>
    </ref>
    <ref id="CR30">
      <label>30.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Strobl</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Zeileis</surname>
            <given-names>A</given-names>
          </name>
        </person-group>
        <source>Danger: high power!–exploring the statistical properties of a test for random forest variable importance</source>
        <year>2008</year>
      </element-citation>
    </ref>
    <ref id="CR31">
      <label>31.</label>
      <mixed-citation publication-type="other">Li Q, Zaim SR, Aberasturi D, Berghout J, Li H, Vitali F, Kenost C, Zhang HH, Lussier YA. Interpretation of Omics dynamics in a single subject using local estimates of dispersion between two transcriptomes. bioRxiv. 2019:405332.</mixed-citation>
    </ref>
    <ref id="CR32">
      <label>32.</label>
      <mixed-citation publication-type="other">Wang Q, Nguyen T-T, Huang JZ, Nguyen TT. An efficient random forests algorithm for high dimensional data classification. ADAC. 2018;12(4):953–72.</mixed-citation>
    </ref>
    <ref id="CR33">
      <label>33.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Ye</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Zhang</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Ho</surname>
            <given-names>S-S</given-names>
          </name>
        </person-group>
        <article-title>ForesTexter: an efficient random forest algorithm for imbalanced text categorization</article-title>
        <source>Knowl-Based Syst</source>
        <year>2014</year>
        <volume>67</volume>
        <fpage>105</fpage>
        <lpage>116</lpage>
      </element-citation>
    </ref>
    <ref id="CR34">
      <label>34.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ye</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>JZ</given-names>
          </name>
          <name>
            <surname>Ng</surname>
            <given-names>MK</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>X</given-names>
          </name>
        </person-group>
        <article-title>Stratified sampling for feature subspace selection in random forests for high dimensional data</article-title>
        <source>Pattern Recogn</source>
        <year>2013</year>
        <volume>46</volume>
        <fpage>769</fpage>
        <lpage>787</lpage>
      </element-citation>
    </ref>
    <ref id="CR35">
      <label>35.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Sinha</surname>
            <given-names>VYKPK</given-names>
          </name>
          <name>
            <surname>Kulkarni</surname>
            <given-names>VY</given-names>
          </name>
        </person-group>
        <article-title>Efficient learning of random forest classifier using disjoint partitioning approach</article-title>
        <source>Proceedings of the World Congress on Engineering</source>
        <year>2013</year>
        <fpage>3</fpage>
        <lpage>5</lpage>
      </element-citation>
    </ref>
    <ref id="CR36">
      <label>36.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Lakshminarayanan</surname>
            <given-names>B</given-names>
          </name>
          <name>
            <surname>Roy</surname>
            <given-names>DM</given-names>
          </name>
          <name>
            <surname>Teh</surname>
            <given-names>YW</given-names>
          </name>
        </person-group>
        <article-title>Mondrian forests: efficient online random forests</article-title>
        <source>Advances in neural information processing systems</source>
        <year>2014</year>
        <fpage>3140</fpage>
        <lpage>3148</lpage>
      </element-citation>
    </ref>
    <ref id="CR37">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Malley</surname>
            <given-names>JD</given-names>
          </name>
          <name>
            <surname>Andrew</surname>
            <given-names>AS</given-names>
          </name>
          <name>
            <surname>Karagas</surname>
            <given-names>MR</given-names>
          </name>
          <name>
            <surname>Moore</surname>
            <given-names>JH</given-names>
          </name>
        </person-group>
        <article-title>Detecting gene-gene interactions using a permutation-based random forest method</article-title>
        <source>BioData Min</source>
        <year>2016</year>
        <volume>9</volume>
        <fpage>14</fpage>
        <?supplied-pmid 27053949?>
        <pub-id pub-id-type="pmid">27053949</pub-id>
      </element-citation>
    </ref>
    <ref id="CR38">
      <label>38.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Friedman</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Hastie</surname>
            <given-names>T</given-names>
          </name>
          <name>
            <surname>Tibshirani</surname>
            <given-names>R</given-names>
          </name>
        </person-group>
        <source>The elements of statistical learning</source>
        <year>2001</year>
        <publisher-loc>New York</publisher-loc>
        <publisher-name>Springer series in statistics</publisher-name>
      </element-citation>
    </ref>
    <ref id="CR39">
      <label>39.</label>
      <mixed-citation publication-type="other">Rudin C, Radin J. Why are we using black box models in AI when we don’t need to? A lesson from an explainable AI competition. Harvard Data Sci Rev. 2019;1..</mixed-citation>
    </ref>
    <ref id="CR40">
      <label>40.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Song</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Langfelder</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Horvath</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>Random generalized linear model: a highly accurate and interpretable ensemble predictor</article-title>
        <source>BMC Bioinformatics</source>
        <year>2013</year>
        <volume>14</volume>
        <fpage>5</fpage>
        <?supplied-pmid 23323760?>
        <pub-id pub-id-type="pmid">23323760</pub-id>
      </element-citation>
    </ref>
    <ref id="CR41">
      <label>41.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Molnar</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Casalicchio</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bischl</surname>
            <given-names>B</given-names>
          </name>
        </person-group>
        <article-title>iml: An R package for interpretable machine learning</article-title>
        <source>J Open Source Softw</source>
        <year>2018</year>
        <volume>3</volume>
        <fpage>786</fpage>
      </element-citation>
    </ref>
    <ref id="CR42">
      <label>42.</label>
      <mixed-citation publication-type="other">Ribeiro MT, Singh S, Guestrin C. Model-agnostic interpretability of machine learning. arXiv preprint arXiv:160605386 2016.</mixed-citation>
    </ref>
    <ref id="CR43">
      <label>43.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Samek</surname>
            <given-names>W</given-names>
          </name>
        </person-group>
        <article-title>Learning with explainable trees</article-title>
        <source>Nat Mach Intell</source>
        <year>2020</year>
        <volume>2</volume>
        <fpage>1</fpage>
        <lpage>2</lpage>
      </element-citation>
    </ref>
    <ref id="CR44">
      <label>44.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zaim</surname>
            <given-names>SR</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Schissler</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Lussier</surname>
            <given-names>YA</given-names>
          </name>
        </person-group>
        <article-title>Emergence of pathway-level composite biomarkers from converging gene set signals of heterogeneous transcriptomic responses</article-title>
        <source>Pac Symp Biocomput</source>
        <year>2018</year>
        <volume>23</volume>
        <fpage>484</fpage>
        <lpage>495</lpage>
        <?supplied-pmid 29218907?>
        <pub-id pub-id-type="pmid">29218907</pub-id>
      </element-citation>
    </ref>
    <ref id="CR45">
      <label>45.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gardeux</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Achour</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Maienschein-Cline</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Pesce</surname>
            <given-names>L</given-names>
          </name>
          <name>
            <surname>Parinandi</surname>
            <given-names>G</given-names>
          </name>
          <name>
            <surname>Bahroos</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Winn</surname>
            <given-names>R</given-names>
          </name>
          <name>
            <surname>Foster</surname>
            <given-names>I</given-names>
          </name>
        </person-group>
        <article-title>‘N-of-1-pathways’ unveils personal deregulated mechanisms from a single pair of RNA-Seq samples: towards precision medicine</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2014</year>
        <volume>21</volume>
        <fpage>1015</fpage>
        <lpage>1025</lpage>
        <?supplied-pmid 25301808?>
        <pub-id pub-id-type="pmid">25301808</pub-id>
      </element-citation>
    </ref>
    <ref id="CR46">
      <label>46.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Gardeux</surname>
            <given-names>V</given-names>
          </name>
          <name>
            <surname>Berghout</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Achour</surname>
            <given-names>I</given-names>
          </name>
          <name>
            <surname>Schissler</surname>
            <given-names>AG</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>Q</given-names>
          </name>
          <name>
            <surname>Kenost</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>J</given-names>
          </name>
          <name>
            <surname>Shang</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Bosco</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Saner</surname>
            <given-names>D</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>A genome-by-environment interaction classifier for precision medicine: personal transcriptome response to rhinovirus identifies children prone to asthma exacerbations</article-title>
        <source>J Am Med Inform Assoc</source>
        <year>2017</year>
        <volume>24</volume>
        <fpage>1116</fpage>
        <lpage>1126</lpage>
        <?supplied-pmid 29016970?>
        <pub-id pub-id-type="pmid">29016970</pub-id>
      </element-citation>
    </ref>
    <ref id="CR47">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Ashburner</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Ball</surname>
            <given-names>CA</given-names>
          </name>
          <name>
            <surname>Blake</surname>
            <given-names>JA</given-names>
          </name>
          <name>
            <surname>Botstein</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Butler</surname>
            <given-names>H</given-names>
          </name>
          <name>
            <surname>Cherry</surname>
            <given-names>JM</given-names>
          </name>
          <name>
            <surname>Davis</surname>
            <given-names>AP</given-names>
          </name>
          <name>
            <surname>Dolinski</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Dwight</surname>
            <given-names>SS</given-names>
          </name>
          <name>
            <surname>Eppig</surname>
            <given-names>JT</given-names>
          </name>
        </person-group>
        <article-title>Gene ontology: tool for the unification of biology</article-title>
        <source>Nat Genet</source>
        <year>2000</year>
        <volume>25</volume>
        <fpage>25</fpage>
        <?supplied-pmid 10802651?>
        <pub-id pub-id-type="pmid">10802651</pub-id>
      </element-citation>
    </ref>
    <ref id="CR48">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Hsueh</surname>
            <given-names>H-M</given-names>
          </name>
          <name>
            <surname>Zhou</surname>
            <given-names>D-W</given-names>
          </name>
          <name>
            <surname>Tsai</surname>
            <given-names>C-A</given-names>
          </name>
        </person-group>
        <article-title>Random forests-based differential analysis of gene sets for gene expression data</article-title>
        <source>Gene</source>
        <year>2013</year>
        <volume>518</volume>
        <fpage>179</fpage>
        <lpage>186</lpage>
        <?supplied-pmid 23219997?>
        <pub-id pub-id-type="pmid">23219997</pub-id>
      </element-citation>
    </ref>
    <ref id="CR49">
      <label>49.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Witt</surname>
            <given-names>G</given-names>
          </name>
        </person-group>
        <article-title>A simple distribution for the sum of correlated, exchangeable binary data</article-title>
        <source>Commun Stat Theory Methods</source>
        <year>2014</year>
        <volume>43</volume>
        <fpage>4265</fpage>
        <lpage>4280</lpage>
      </element-citation>
    </ref>
    <ref id="CR50">
      <label>50.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kuk</surname>
            <given-names>AY</given-names>
          </name>
        </person-group>
        <article-title>A litter-based approach to risk assessment in developmental toxicity studies via a power family of completely monotone functions</article-title>
        <source>J R Stat Soc: Ser C: Appl Stat</source>
        <year>2004</year>
        <volume>53</volume>
        <fpage>369</fpage>
        <lpage>386</lpage>
      </element-citation>
    </ref>
    <ref id="CR51">
      <label>51.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Benjamini</surname>
            <given-names>Y</given-names>
          </name>
          <name>
            <surname>Yekutieli</surname>
            <given-names>D</given-names>
          </name>
        </person-group>
        <article-title>The control of the false discovery rate in multiple testing under dependency</article-title>
        <source>Ann Stat</source>
        <year>2001</year>
        <volume>29</volume>
        <fpage>1165</fpage>
        <lpage>1188</lpage>
      </element-citation>
    </ref>
    <ref id="CR52">
      <label>52.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Nelder</surname>
            <given-names>JA</given-names>
          </name>
        </person-group>
        <article-title>The selection of terms in response-surface models—how strong is the weak-heredity principle?</article-title>
        <source>Am Stat</source>
        <year>1998</year>
        <volume>52</volume>
        <fpage>315</fpage>
        <lpage>318</lpage>
      </element-citation>
    </ref>
    <ref id="CR53">
      <label>53.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Choi</surname>
            <given-names>NH</given-names>
          </name>
          <name>
            <surname>Li</surname>
            <given-names>W</given-names>
          </name>
          <name>
            <surname>Zhu</surname>
            <given-names>J</given-names>
          </name>
        </person-group>
        <article-title>Variable selection with the strong heredity constraint and its oracle property</article-title>
        <source>J Am Stat Assoc</source>
        <year>2010</year>
        <volume>105</volume>
        <fpage>354</fpage>
        <lpage>364</lpage>
      </element-citation>
    </ref>
    <ref id="CR54">
      <label>54.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wan</surname>
            <given-names>Y-W</given-names>
          </name>
          <name>
            <surname>Allen</surname>
            <given-names>GI</given-names>
          </name>
          <name>
            <surname>Liu</surname>
            <given-names>Z</given-names>
          </name>
        </person-group>
        <article-title>TCGA2STAT: simple TCGA data access for integrated statistical analysis in R</article-title>
        <source>Bioinformatics</source>
        <year>2016</year>
        <volume>32</volume>
        <fpage>952</fpage>
        <lpage>954</lpage>
        <?supplied-pmid 26568634?>
        <pub-id pub-id-type="pmid">26568634</pub-id>
      </element-citation>
    </ref>
    <ref id="CR55">
      <label>55.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Wagner</surname>
            <given-names>GP</given-names>
          </name>
          <name>
            <surname>Kin</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Lynch</surname>
            <given-names>VJ</given-names>
          </name>
        </person-group>
        <article-title>Measurement of mRNA abundance using RNA-seq data: RPKM measure is inconsistent among samples</article-title>
        <source>Theory Biosci</source>
        <year>2012</year>
        <volume>131</volume>
        <fpage>281</fpage>
        <lpage>285</lpage>
        <?supplied-pmid 22872506?>
        <pub-id pub-id-type="pmid">22872506</pub-id>
      </element-citation>
    </ref>
    <ref id="CR56">
      <label>56.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Bindal</surname>
            <given-names>N</given-names>
          </name>
          <name>
            <surname>Forbes</surname>
            <given-names>SA</given-names>
          </name>
          <name>
            <surname>Beare</surname>
            <given-names>D</given-names>
          </name>
          <name>
            <surname>Gunasekaran</surname>
            <given-names>P</given-names>
          </name>
          <name>
            <surname>Leung</surname>
            <given-names>K</given-names>
          </name>
          <name>
            <surname>Kok</surname>
            <given-names>CY</given-names>
          </name>
          <name>
            <surname>Jia</surname>
            <given-names>M</given-names>
          </name>
          <name>
            <surname>Bamford</surname>
            <given-names>S</given-names>
          </name>
          <name>
            <surname>Cole</surname>
            <given-names>C</given-names>
          </name>
          <name>
            <surname>Ward</surname>
            <given-names>S</given-names>
          </name>
        </person-group>
        <article-title>COSMIC: the catalogue of somatic mutations in cancer</article-title>
        <source>Genome Biol</source>
        <year>2011</year>
        <volume>12</volume>
        <fpage>P3</fpage>
      </element-citation>
    </ref>
    <ref id="CR57">
      <label>57.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Liaw</surname>
            <given-names>A</given-names>
          </name>
          <name>
            <surname>Wiener</surname>
            <given-names>M</given-names>
          </name>
        </person-group>
        <article-title>Classification and regression by randomForest</article-title>
        <source>R news</source>
        <year>2002</year>
        <volume>2</volume>
        <fpage>18</fpage>
        <lpage>22</lpage>
      </element-citation>
    </ref>
  </ref-list>
</back>
