<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//ACS//DTD ACS Journal DTD v1.02 20061031//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName ACSJournal-v102.dtd?>
<?SourceDTD.Version 1.02?>
<?ConverterInfo.XSLTName acs2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">ACS Omega</journal-id>
    <journal-id journal-id-type="iso-abbrev">ACS Omega</journal-id>
    <journal-id journal-id-type="publisher-id">ao</journal-id>
    <journal-id journal-id-type="coden">acsodf</journal-id>
    <journal-title-group>
      <journal-title>ACS Omega</journal-title>
    </journal-title-group>
    <issn pub-type="epub">2470-1343</issn>
    <publisher>
      <publisher-name>American Chemical Society</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8529678</article-id>
    <article-id pub-id-type="doi">10.1021/acsomega.1c04017</article-id>
    <article-categories>
      <subj-group>
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DGL-LifeSci: An Open-Source Toolkit for Deep Learning
on Graphs in Life Science</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author" corresp="yes" id="ath1">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6123-2188</contrib-id>
        <name>
          <surname>Li</surname>
          <given-names>Mufei</given-names>
        </name>
        <xref rid="cor1" ref-type="other">*</xref>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath2">
        <name>
          <surname>Zhou</surname>
          <given-names>Jinjing</given-names>
        </name>
        <xref rid="aff1" ref-type="aff">†</xref>
      </contrib>
      <contrib contrib-type="author" id="ath3">
        <name>
          <surname>Hu</surname>
          <given-names>Jiajing</given-names>
        </name>
        <xref rid="aff2" ref-type="aff">‡</xref>
      </contrib>
      <contrib contrib-type="author" id="ath4">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3932-1808</contrib-id>
        <name>
          <surname>Fan</surname>
          <given-names>Wenxuan</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">§</xref>
      </contrib>
      <contrib contrib-type="author" id="ath5">
        <name>
          <surname>Zhang</surname>
          <given-names>Yangkang</given-names>
        </name>
        <xref rid="aff4" ref-type="aff">∥</xref>
      </contrib>
      <contrib contrib-type="author" id="ath6">
        <name>
          <surname>Gu</surname>
          <given-names>Yaxin</given-names>
        </name>
        <xref rid="aff3" ref-type="aff">§</xref>
      </contrib>
      <contrib contrib-type="author" id="ath7">
        <name>
          <surname>Karypis</surname>
          <given-names>George</given-names>
        </name>
        <xref rid="aff5" ref-type="aff">⊥</xref>
        <xref rid="aff6" ref-type="aff">#</xref>
      </contrib>
      <aff id="aff1"><label>†</label><institution>AWS
Shanghai AI Lab, 5F-102</institution>, 1901 Huashan Road, Shanghai200030, <country>P. R. China</country></aff>
      <aff id="aff2"><label>‡</label><institution>Maurice
Wohl Clinical Neuroscience Institute, King’s College London</institution>, 5 Cutcombe Road, London SE5 9RT, <country>U.K.</country></aff>
      <aff id="aff3"><label>§</label><institution>School
of Pharmacy, East China University of Science and Technology</institution>, 130 Meilong Road, Shanghai 200237, <country>P. R. China</country></aff>
      <aff id="aff4"><label>∥</label>College
of Computer Science and Technology, <institution>Zhejiang
University</institution>, 866 Yuhangtang Road, Hangzhou 310058, <country>P. R. China</country></aff>
      <aff id="aff5"><label>⊥</label><institution>AWS
AI</institution>, East Palo Alto, California 94303, <country>United States</country></aff>
      <aff id="aff6"><label>#</label>Department
of Computer Science and Engineering, <institution>University
of Minnesota</institution>, 4-192 KHKH,
200 Union St SE, Minnesota, Minneapolis55455, <country>United States</country></aff>
    </contrib-group>
    <author-notes>
      <corresp id="cor1"><label>*</label>Email: <email>limufe@amazon.com</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>05</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <day>19</day>
      <month>10</month>
      <year>2021</year>
    </pub-date>
    <volume>6</volume>
    <issue>41</issue>
    <fpage>27233</fpage>
    <lpage>27238</lpage>
    <history>
      <date date-type="received">
        <day>27</day>
        <month>07</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>24</day>
        <month>09</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 The Authors. Published by American Chemical Society</copyright-statement>
      <copyright-year>2021</copyright-year>
      <copyright-holder>The Authors</copyright-holder>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>Permits non-commercial access and re-use, provided that author attribution and integrity are maintained; but does not permit creation of adaptations or other derivative works (<ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">https://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p content-type="toc-graphic">
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ao1c04017_0004" id="ab-tgr1"/>
      </p>
      <p>Graph neural networks
(GNNs) constitute a class of deep learning
methods for graph data. They have wide applications in chemistry and
biology, such as molecular property prediction, reaction prediction,
and drug–target interaction prediction. Despite the interest,
GNN-based modeling is challenging as it requires graph data preprocessing
and modeling in addition to programming and deep learning. Here, we
present Deep Graph Library (DGL)-LifeSci, an open-source package for
deep learning on graphs in life science. Deep Graph Library (DGL)-LifeSci
is a python toolkit based on RDKit, PyTorch, and Deep Graph Library
(DGL). DGL-LifeSci allows GNN-based modeling on custom datasets for
molecular property prediction, reaction prediction, and molecule generation.
With its command-line interfaces, users can perform modeling without
any background in programming and deep learning. We test the command-line
interfaces using standard benchmarks MoleculeNet, USPTO, and ZINC.
Compared with previous implementations, DGL-LifeSci achieves a speed
up by up to 6×. For modeling flexibility, DGL-LifeSci provides
well-optimized modules for various stages of the modeling pipeline.
In addition, DGL-LifeSci provides pretrained models for reproducing
the test experiment results and applying models without training.
The code is distributed under an Apache-2.0 License and is freely
accessible at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/awslabs/dgl-lifesci">https://github.com/awslabs/dgl-lifesci</uri>.</p>
    </abstract>
    <custom-meta-group>
      <custom-meta>
        <meta-name>document-id-old-9</meta-name>
        <meta-value>ao1c04017</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>document-id-new-14</meta-name>
        <meta-value>ao1c04017</meta-value>
      </custom-meta>
      <custom-meta>
        <meta-name>ccc-price</meta-name>
        <meta-value/>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
</front>
<body>
  <sec id="sec1">
    <label>1</label>
    <title>Introduction</title>
    <p>A large amount of the
chemical and biological data corresponds
to attributed graphs, e.g., molecular graphs, interaction networks,
and biological pathways. Many of the machine learning (ML) tasks that
arise in this domain can be formulated as learning tasks on graphs.
For example, molecular property prediction can be formulated as learning
a mapping from molecular graphs to real numbers (regression) or discrete
values (classification);<sup><xref ref-type="bibr" rid="ref1">1</xref></sup> molecule generation
can be formulated as learning a distribution over molecular graphs;<sup><xref ref-type="bibr" rid="ref2">2</xref></sup> reaction prediction can be formulated as learning
a mapping from one set of graphs (reactants) to another set of graphs
(products).<sup><xref ref-type="bibr" rid="ref3">3</xref></sup> A representation is a vector
of a user-defined dimensionality. Graph neural networks (GNNs) combine
graph structures and features in representation learning and they
have been one of the most popular approaches for learning on graphs.<sup><xref ref-type="bibr" rid="ref4">4</xref>,<xref ref-type="bibr" rid="ref5">5</xref></sup> GNNs have also attracted considerable attention in life science
and researchers have applied them to many different tasks.<sup><xref ref-type="bibr" rid="ref1">1</xref>−<xref ref-type="bibr" rid="ref3">3</xref>,<xref ref-type="bibr" rid="ref6">6</xref>−<xref ref-type="bibr" rid="ref12">12</xref></sup></p>
    <p>Despite significant research, it is often challenging for
experts
in life science to use GNN-based approaches. To unlock the power of
GNNs requires clean interfaces for custom datasets and robust and
efficient pipelines. This is because developing GNN pipelines by oneself
requires a combined skill set of programming, machine learning, and
GNN modeling, which is time-consuming to obtain. This calls for a
set of ready-to-run programs, which should make little assumption
about users’ background.</p>
    <p>Prior efforts have greatly lowered
the bar for GNN-based modeling
in life science, but none of them fully addresses the problem. DeepChem<sup><xref ref-type="bibr" rid="ref13">13</xref></sup> is a package for deep learning in drug discovery,
materials science, quantum chemistry, and biology. While it implements
several GNN models, it still requires users to program. Chainer Chemistry<sup><xref ref-type="bibr" rid="ref14">14</xref></sup> is a package for deep learning in biology and
chemistry, based on Chainer.<sup><xref ref-type="bibr" rid="ref15">15</xref></sup> It only
provides a command-line interface for GNN-based regression on molecules
and requires users to write code for other tasks. PiNN<sup><xref ref-type="bibr" rid="ref16">16</xref></sup> implements a GNN variant for predicting potential
energy surfaces and physicochemical properties of molecules and materials.
It also requires users to program themselves.</p>
    <p>Here, we present
a python toolkit named Deep Graph Library (DGL)-LifeSci.
It provides high-quality and robust implementations of seven models
for molecular property prediction, one model for molecule generation,
and one model for chemical reaction prediction. For all of these models
and tasks, there is an associated command-line script for predictions
on custom datasets without writing a single line of code. DGL-LifeSci
also provides pretrained models for all experiments. Compared with
previous implementations, it achieves a speed up by up to 6×.</p>
    <p>In the following, we first provide a high-level overview of how
graph neural networks work over molecules. Then, we discuss the implementation
and package features of DGL-LifeSci. After that, we present the results
of evaluating DGL-LifeSci in terms of robustness and efficiency. Finally,
we conclude with a discussion on future work.</p>
  </sec>
  <sec id="sec2">
    <label>2</label>
    <title>Graph Neural
Networks over Molecules</title>
    <p>GNNs perform graph-based representation
learning by combining information
from the topology of a graph and the features associated with its
nodes and edges. They iteratively update the representation of a node
by aggregating representations of its neighbors. As the number of
iterations increases, the nodes gain information from an increasingly
larger local subgraph.</p>
    <p>When applying GNNs to molecules as in
molecular property prediction,
there are two phases—a message passing phase and a readout
phase. <xref rid="fig1" ref-type="fig">Figure <xref rid="fig1" ref-type="fig">1</xref></xref> is
an illustration of them.</p>
    <fig id="fig1" position="float">
      <label>Figure 1</label>
      <caption>
        <p>Illustration of the message passing phase (left)
and readout phase
(right).</p>
      </caption>
      <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ao1c04017_0002" id="gr1" position="float"/>
    </fig>
    <sec id="sec2.1">
      <label>2.1</label>
      <title>Message Passing Phase</title>
      <p>The message
passing phase updates node representations simultaneously across the
entire graph and consists of multiple rounds of message passing. In
a round of message passing, the representation of a node is updated
by applying learnable functions to its original representation, the
representations of its adjacent nodes, and the representations of
its incident edges. The operation is similar to gathering messages
from adjacent nodes. By performing <italic>k</italic> rounds of message
passing, we can aggregate information from all of the nodes/edges
that are within <italic>k</italic> hops from each node.</p>
    </sec>
    <sec id="sec2.2">
      <label>2.2</label>
      <title>Readout Phase</title>
      <p>The readout phase computes
a representation for the entire graph. This representation is computed
by applying a potentially learnable function to the representations
of all of the nodes in the graph, e.g., summation over them. Once
we obtain graph representations, we can pass them to a multilayer
perceptron (MLP) for final prediction.</p>
    </sec>
  </sec>
  <sec id="sec3">
    <label>3.</label>
    <title>Package
Features</title>
    <p>DGL-LifeSci contains four components: (i) a set
of ready-to-run
scripts for training and inference; (ii) programming APIs for allowing
researchers to develop their own custom pipelines and models; (iii)
a set of pretrained models that can either be fine-tuned or directly
used to perform inference; and (iv) a set of built-in datasets for
quick experimentation.</p>
    <p>It provides models that can be used to
solve three tasks. The first
task is molecular property prediction or quantitative structure–activity
relationship (QSAR) prediction. This can be formulated as a regression
or classification task for single molecules. The second task is molecule
generation. The third task is chemical reaction prediction.</p>
    <sec id="sec3.1">
      <label>3.1</label>
      <title>Usage</title>
      <p>DGL-LifeSci provides command-line
scripts for each task. They are responsible for invoking the modeling
pipeline, which handles model training and model evaluation. Users
need to prepare their data in a standard format. They can then use
the command-line interface by specifying the path to the data file
along with some additional arguments. For example, below is the command-line
interface for regression and classification problems in molecular
property prediction. Users need to prepare molecules in the form of
SMILES strings with their properties to predict in a CSV file.<graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ao1c04017_0005" id="fx1" position="float"/> DGL-LifeSci
also provides an optional support for a hyperparameter
search other than using the default ones. It uses Bayesian optimization
based on hyperopt<sup><xref ref-type="bibr" rid="ref17">17</xref></sup> for a hyperparameter
search.</p>
    </sec>
  </sec>
  <sec id="sec4">
    <label>4.</label>
    <title>Implementation</title>
    <sec id="sec4.1">
      <label>4.1</label>
      <title>Dependencies</title>
      <p>DGL-LifeSci
is developed
using PyTorch<sup><xref ref-type="bibr" rid="ref18">18</xref></sup> and Deep Graph Library
(DGL).<sup><xref ref-type="bibr" rid="ref19">19</xref></sup> PyTorch is a general-purpose deep
learning framework and DGL is a high-performant GNN library. In addition,
it uses RDKit<sup><xref ref-type="bibr" rid="ref20">20</xref></sup> for utilities related to
cheminformatics.</p>
    </sec>
    <sec id="sec4.2">
      <label>4.2</label>
      <title>Modeling Pipeline and Modules</title>
      <p>A general
GNN-based modeling pipeline consists of three stages: dataset preparation,
model initialization, and model training. The dataset preparation
stage involves data loading, graph construction, representation initialization
for nodes and edges (graph featurization), and dataset interface construction.
The model training stage involves model update, metric computation,
and early stopping. As presented in <xref rid="fig2" ref-type="fig">Figure <xref rid="fig2" ref-type="fig">2</xref></xref>, DGL-LifeSci is modularized for these various
stages and stage components so as to cater to the need of different
uses. While DGL-LifeSci allows users to perform GNN-based modeling
without programming, advanced users can also adapt these modules for
their own development.</p>
      <fig id="fig2" position="float">
        <label>Figure 2</label>
        <caption>
          <p>Overview of modules in DGL-LifeSci and their usage.</p>
        </caption>
        <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="ao1c04017_0003" id="gr2" position="float"/>
      </fig>
    </sec>
    <sec id="sec4.3">
      <label>4.3</label>
      <title>Dataset Preparation</title>
      <p>DGL-LifeSci provides
dataset interfaces for supporting both built-in datasets and custom
datasets. The interfaces are responsible for loading raw data files
and invoking graph construction and featurization.</p>
      <p>Graph construction
and featurization are two important steps for GNN-specific data preparation.
DGL-LifeSci provides built-in support for constructing three kinds
of graphs for molecules—molecular graphs, distance-based graphs,
and complete graphs. In all of these graphs, each node corresponds
to an atom in a molecule. In a molecular graph, the edges correspond
to chemical bonds in the molecule. The construction of a distance-based
graph requires a molecule conformation and there is an edge between
a pair of atoms if the distance between them is within a cutoff distance.
In a complete graph, every pair of atoms is connected. For graph featurization,
DGL-LifeSci allows initializing various node and edge features from
atom and bond descriptors. <xref rid="tbl1" ref-type="other">Table <xref rid="tbl1" ref-type="other">1</xref></xref> gives an overview of them.</p>
      <table-wrap id="tbl1" position="float">
        <label>Table 1</label>
        <caption>
          <title>Descriptors
for Feature Initialization<xref rid="t1fn1" ref-type="table-fn">a</xref></title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center">descriptors</th>
              <th style="border:none;" align="center">possible values</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">atom type</td>
              <td style="border:none;" align="left">C, N, O, etc.</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom degree excluding hydrogen atoms</td>
              <td style="border:none;" align="left">non-negative integers</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom degree including hydrogen atoms</td>
              <td style="border:none;" align="left">non-negative integers</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom explicit valence</td>
              <td style="border:none;" align="left">non-negative integers</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom implicit
valence</td>
              <td style="border:none;" align="left">non-negative integers</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom hybridization</td>
              <td style="border:none;" align="left">S, SP, SP2, SP3, SP3D, SP3D2</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">total number of hydrogen atoms attached</td>
              <td style="border:none;" align="left">non-negative integers</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom formal charge</td>
              <td style="border:none;" align="left">integers</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">number of radical electrons
of an atom</td>
              <td style="border:none;" align="left">non-negative integers</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">whether an atom is aromatic</td>
              <td style="border:none;" align="left">1 (true), 0 (false)</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">whether an atom is in a ring</td>
              <td style="border:none;" align="left">1 (true),
0 (false)</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom chiral tag</td>
              <td style="border:none;" align="left">CW,
CCW, unspecified, other</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom chirality type</td>
              <td style="border:none;" align="left">R, S</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom mass</td>
              <td style="border:none;" align="left">non-negative
real numbers</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">whether an atom is chiral center</td>
              <td style="border:none;" align="left">1 (true), 0 (false)</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">bond type</td>
              <td style="border:none;" align="left">single, double, triple, aromatic</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">whether
a bond is conjugated</td>
              <td style="border:none;" align="left">1 (true), 0 (false)</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">whether a bond is in a ring</td>
              <td style="border:none;" align="left">1 (true), 0
(false)</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">stereo configuration of a bond</td>
              <td style="border:none;" align="left">none, any, OZ, OE, CIS, TRANS</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">direction
of a bond</td>
              <td style="border:none;" align="left">none, end-up-right, end-down-right</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="t1fn1">
            <label>a</label>
            <p>For non-numeric discrete-valued
descriptors, one-hot encoding is used in featurization. For numeric
discrete-valued descriptors, either raw number or one-hot encoding
can be used in featurization.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>Users can split the dataset into training/validation/test subsets
or do so for <italic>k</italic>-fold cross-validation. DGL-LifeSci
provides built-in support for random split, scaffold split, weight
split, and stratified split.<sup><xref ref-type="bibr" rid="ref21">21</xref></sup> The random
split performs a pure random split of a dataset. The scaffold split
separates structurally different molecules into different subsets
based on their Bemis–Murcko scaffolds.<sup><xref ref-type="bibr" rid="ref22">22</xref></sup> The weight split sorts molecules based on their weight and then
splits them in order. The stratified split sorts molecules based on
their label and ensures that each subset contains nearly the full
range of provided labels.</p>
    </sec>
    <sec id="sec4.4">
      <label>4.4</label>
      <title>Models Included</title>
      <p><xref rid="tbl2" ref-type="other">Table <xref rid="tbl2" ref-type="other">2</xref></xref> lists the
models implemented.
Graph Convolutional Network (GCN)<sup><xref ref-type="bibr" rid="ref23">23</xref></sup> and
Graph Attention Network (GAT)<sup><xref ref-type="bibr" rid="ref24">24</xref></sup> are two
popular GNNs initially developed for node classification and we extend
them for graph regression/classification. Originally, GCN and GAT
output node representations. We compute graph-level representations
from node-level representations by two operations. The first operation
performs a weighted sum of the representations of nodes in a graph,
where the weights are determined by passing the node representations
to a linear layer followed by a sigmoid function. The second operation
takes the element-wise maximum of the representations of nodes in
a graph. We then concatenate the results of the two operations and
pass them to an MLP for final prediction. Neural Fingerprint (NF)<sup><xref ref-type="bibr" rid="ref1">1</xref></sup> and Weave<sup><xref ref-type="bibr" rid="ref25">25</xref></sup> are among
the earliest models that extend rule-based molecular fingerprints
with graph neural networks. Message Passing Neural Network (MPNN)<sup><xref ref-type="bibr" rid="ref9">9</xref></sup> unifies multiple GNNs for quantum chemistry.
AttentiveFP<sup><xref ref-type="bibr" rid="ref26">26</xref></sup> extends GAT with gated recurrent
units.<sup><xref ref-type="bibr" rid="ref27">27</xref></sup></p>
      <table-wrap id="tbl2" position="float">
        <label>Table 2</label>
        <caption>
          <title>Models Implemented</title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center">task</th>
              <th style="border:none;" align="center">model</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">molecular property prediction</td>
              <td style="border:none;" align="left">GCN,<sup><xref ref-type="bibr" rid="ref23">23</xref></sup> GAT,<sup><xref ref-type="bibr" rid="ref24">24</xref></sup> NF,<sup><xref ref-type="bibr" rid="ref1">1</xref></sup> Weave,<sup><xref ref-type="bibr" rid="ref25">25</xref></sup> MPNN,<sup><xref ref-type="bibr" rid="ref9">9</xref></sup> AttentiveFP<sup><xref ref-type="bibr" rid="ref26">26</xref></sup></td>
            </tr>
            <tr>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">GIN + context prediction/deep graph
infomax/</td>
            </tr>
            <tr>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">edge prediction/attribute
masking<sup><xref ref-type="bibr" rid="ref32">32</xref></sup></td>
            </tr>
            <tr>
              <td style="border:none;" align="left">molecule
generation</td>
              <td style="border:none;" align="left">JTVAE<sup><xref ref-type="bibr" rid="ref2">2</xref></sup></td>
            </tr>
            <tr>
              <td style="border:none;" align="left">reaction prediction</td>
              <td style="border:none;" align="left">WLN<sup><xref ref-type="bibr" rid="ref3">3</xref></sup></td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>One
difficulty in developing learning-based approaches for molecular
property prediction is the gap between an extremely large chemical
space and extremely limited labels for molecular properties. It is
estimated that the number of drug-like molecules is between 10<sup>23</sup> and 10<sup>60</sup> while most datasets have less than tens
of thousands of molecules in MoleculeNet.<sup><xref ref-type="bibr" rid="ref21">21</xref>,<xref ref-type="bibr" rid="ref28">28</xref>−<xref ref-type="bibr" rid="ref31">31</xref></sup> Hu et al.<sup><xref ref-type="bibr" rid="ref32">32</xref></sup> propose to approach this
problem by utilizing millions of unlabeled molecules in pretraining
the weights of a GIN model for general molecule representations. One
can then fine-tune the model weights for predicting particular properties.
We directly include four models pretrained from their work in DGL-LifeSci.
The details of the datasets used for pretraining can be found in <xref rid="sec5.1" ref-type="other">Section <xref rid="sec5.1" ref-type="other">5.1</xref></xref> of their paper.
The models were pretrained with a same strategy for supervised graph-level
property prediction as described in Section 3.2.1 of their paper.
Each model was pretrained with a different strategy for self-supervised
learning as described in Sections 3.1 and 5.2 of their paper. We distinguish
the models by the associated strategies for self-supervised learning,
which are context prediction, deep graph infomax, edge prediction,
and attribute masking.</p>
      <p>Junction Tree Variational Autoencoder
(JTVAE)<sup><xref ref-type="bibr" rid="ref2">2</xref></sup> is an autoencoder that utilizes
both a junction tree and
a molecular graph for the intermediate representation of a molecule.
Weisfeiler–Lehman Network (WLN)<sup><xref ref-type="bibr" rid="ref3">3</xref></sup> is a two-stage model for chemical reaction prediction. It first
identifies potential bond changes and then enumerates and ranks candidate
products.</p>
    </sec>
  </sec>
  <sec id="sec5">
    <label>5.</label>
    <title>Results and Discussion</title>
    <sec id="sec5.1">
      <label>5.1</label>
      <title>Molecular Property Prediction</title>
      <p>We
test against six binary classification datasets in MoleculeNet and
evaluate the model performance by ROC-AUC averaged over all tasks.<sup><xref ref-type="bibr" rid="ref21">21</xref></sup> To evaluate the model performance on unseen
structures, we employ the scaffold split and use 80, 10, and 10% of
the dataset for training, validation, and test, respectively. We train
six models (NF, GCN, GAT, Weave, MPNN, AttentiveFP) from scratch using
the featurization proposed in DeepChem, which is described in <xref rid="tbl3" ref-type="other">Table <xref rid="tbl3" ref-type="other">3</xref></xref>. GCN and GAT take
initial node features only and they do not take initial edge features.
We also fine-tuned the four pretrained GIN models. For non-GNN baseline
models, we train an MLP and a <italic>k</italic> nearest-neighbor
classifier (KNN, <italic>k</italic> = 1) taking Extended-Connectivity
Fingerprints (ECFPs).</p>
      <table-wrap id="tbl3" position="float">
        <label>Table 3</label>
        <caption>
          <title>Descriptors Considered
in DeepChem
Featurization</title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center">descriptors</th>
              <th style="border:none;" align="center">possible values</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">atom type (one-hot encoding)</td>
              <td style="border:none;" align="left">C, N, O, S, F, Si, P, Cl, Br, Mg, Na, Ca,</td>
            </tr>
            <tr>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">Fe, As, Al, I, B, V K, Tl, Yb, Sb, Sn,</td>
            </tr>
            <tr>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">Ag, Pd, Co, Se, Ti, Zn, H, Li, Ge,
Cu,</td>
            </tr>
            <tr>
              <td style="border:none;" align="left"> </td>
              <td style="border:none;" align="left">Au, Ni, Cd, In, Mn, Zr,
Cr, Pt, Hg, Pb</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom degree excluding hydrogen
atoms (one-hot encoding)</td>
              <td style="border:none;" align="left">0–10</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom implicit valence (one-hot encoding)</td>
              <td style="border:none;" align="left">0–6</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom formal charge</td>
              <td style="border:none;" align="left">integers</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">number of radical electrons of an atom</td>
              <td style="border:none;" align="left">non-negative integers</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">whether an atom is
aromatic</td>
              <td style="border:none;" align="left">1 (true), 0 (false)</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">atom hybridization (one-hot encoding)</td>
              <td style="border:none;" align="left">SP, SP2, SP3,
SP3D, SP3D2</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">total number of hydrogen atoms
attached (one-hot encoding)</td>
              <td style="border:none;" align="left">0 - 4</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">bond type (one-hot encoding)</td>
              <td style="border:none;" align="left">single, double,
triple, aromatic</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">whether a bond is conjugated</td>
              <td style="border:none;" align="left">1 (true), 0 (false)</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">whether a bond
is in a ring</td>
              <td style="border:none;" align="left">1 (true), 0 (false)</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">stereo configuration of a bond (one-hot encoding)</td>
              <td style="border:none;" align="left">none,
any, OZ, OE, CIS, TRANS</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>For all of the settings, we perform a hyperparameter
search for
32 trials and choose the best hyperparameters based on the performance
on the validation set. Within each trial, we train a randomly initialized
model and perform an early stopping if the performance on the validation
set no longer improves for 30 epochs. Finally, we evaluate the model
achieving the best validation performance on the test set. <xref rid="tbl4" ref-type="other">Table <xref rid="tbl4" ref-type="other">4</xref></xref> presents the summary
of the test performance. For reference, we also include the fine-tuning
performance reported previously<sup><xref ref-type="bibr" rid="ref32">32</xref></sup> and the
best performance achieved by GNNs reported in the MoleculeNet paper.<sup><xref ref-type="bibr" rid="ref21">21</xref></sup> For Tox21, ToxCast, and SIDER, the MoleculeNet
paper reported performance numbers for a random dataset split and
the dataset split cannot be reproduced.</p>
      <table-wrap id="tbl4" position="float">
        <label>Table 4</label>
        <caption>
          <title>Test ROC-AUC
on Six Datasets from
MoleculeNet<xref rid="t4fn1" ref-type="table-fn">a</xref></title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center">model</th>
              <th style="border:none;" align="center" char=".">BBBP</th>
              <th style="border:none;" align="center" char=".">Tox21</th>
              <th style="border:none;" align="center" char=".">ToxCast</th>
              <th style="border:none;" align="center" char=".">SIDER</th>
              <th style="border:none;" align="center" char=".">HIV</th>
              <th style="border:none;" align="center" char=".">BACE</th>
            </tr>
            <tr>
              <th colspan="7" align="center">Models
trained from scratch</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">GCN</td>
              <td style="border:none;" align="char" char=".">0.63</td>
              <td style="border:none;" align="char" char=".">0.77</td>
              <td style="border:none;" align="char" char=".">0.62</td>
              <td style="border:none;" align="char" char=".">0.58</td>
              <td style="border:none;" align="char" char=".">0.76</td>
              <td style="border:none;" align="char" char=".">0.84</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">GAT</td>
              <td style="border:none;" align="char" char=".">0.68</td>
              <td style="border:none;" align="char" char=".">0.71</td>
              <td style="border:none;" align="char" char=".">0.64</td>
              <td style="border:none;" align="char" char=".">0.52</td>
              <td style="border:none;" align="char" char=".">0.76</td>
              <td style="border:none;" align="char" char=".">0.84</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">NF</td>
              <td style="border:none;" align="char" char=".">0.66</td>
              <td style="border:none;" align="char" char=".">0.75</td>
              <td style="border:none;" align="char" char=".">0.60</td>
              <td style="border:none;" align="char" char=".">0.53</td>
              <td style="border:none;" align="char" char=".">0.74</td>
              <td style="border:none;" align="char" char=".">0.80</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Weave</td>
              <td style="border:none;" align="char" char=".">0.67</td>
              <td style="border:none;" align="char" char=".">0.56</td>
              <td style="border:none;" align="char" char=".">0.62</td>
              <td style="border:none;" align="char" char=".">0.58</td>
              <td style="border:none;" align="char" char=".">0.73</td>
              <td style="border:none;" align="char" char=".">0.79</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">MPNN</td>
              <td style="border:none;" align="char" char=".">0.65</td>
              <td style="border:none;" align="char" char=".">0.70</td>
              <td style="border:none;" align="char" char=".">0.59</td>
              <td style="border:none;" align="char" char=".">0.54</td>
              <td style="border:none;" align="char" char=".">0.74</td>
              <td style="border:none;" align="char" char=".">0.85</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">AttentiveFP</td>
              <td style="border:none;" align="char" char=".">0.71</td>
              <td style="border:none;" align="char" char=".">0.70</td>
              <td style="border:none;" align="char" char=".">0.57</td>
              <td style="border:none;" align="char" char=".">0.53</td>
              <td style="border:none;" align="char" char=".">0.75</td>
              <td style="border:none;" align="char" char=".">0.73</td>
            </tr>
          </tbody>
        </table>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
          </colgroup>
          <thead>
            <tr>
              <th colspan="7" align="center">Non-GNN
baseline</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">MLP + ECFP</td>
              <td style="border:none;" align="char" char=".">0.67</td>
              <td style="border:none;" align="char" char=".">0.70</td>
              <td style="border:none;" align="char" char=".">0.58</td>
              <td style="border:none;" align="char" char=".">0.63</td>
              <td style="border:none;" align="char" char=".">0.76</td>
              <td style="border:none;" align="char" char=".">0.80</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">KNN + ECFP</td>
              <td style="border:none;" align="char" char=".">0.57</td>
              <td style="border:none;" align="char" char=".">0.58</td>
              <td style="border:none;" align="char" char=".">0.52</td>
              <td style="border:none;" align="char" char=".">0.56</td>
              <td style="border:none;" align="char" char=".">0.57</td>
              <td style="border:none;" align="char" char=".">0.66</td>
            </tr>
          </tbody>
        </table>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
          </colgroup>
          <thead>
            <tr>
              <th colspan="7" align="center">Pretrained
models fine-tuned</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">GIN + context
prediction</td>
              <td style="border:none;" align="char" char=".">0.63</td>
              <td style="border:none;" align="char" char=".">0.75</td>
              <td style="border:none;" align="char" char=".">0.64</td>
              <td style="border:none;" align="char" char=".">0.61</td>
              <td style="border:none;" align="char" char=".">0.77</td>
              <td style="border:none;" align="char" char=".">
                <bold>0.86</bold>
              </td>
            </tr>
            <tr>
              <td style="border:none;" align="left">GIN + deep graph infomax</td>
              <td style="border:none;" align="char" char=".">
                <bold>0.72</bold>
              </td>
              <td style="border:none;" align="char" char=".">0.78</td>
              <td style="border:none;" align="char" char=".">0.59</td>
              <td style="border:none;" align="char" char=".">0.63</td>
              <td style="border:none;" align="char" char=".">0.76</td>
              <td style="border:none;" align="char" char=".">0.71</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">GIN + edge
prediction</td>
              <td style="border:none;" align="char" char=".">0.70</td>
              <td style="border:none;" align="char" char=".">
                <bold>0.80</bold>
              </td>
              <td style="border:none;" align="char" char=".">0.59</td>
              <td style="border:none;" align="char" char=".">
                <bold>0.66</bold>
              </td>
              <td style="border:none;" align="char" char=".">0.72</td>
              <td style="border:none;" align="char" char=".">
                <bold>0.86</bold>
              </td>
            </tr>
            <tr>
              <td style="border:none;" align="left">GIN + attribute
masking</td>
              <td style="border:none;" align="char" char=".">
                <bold>0.72</bold>
              </td>
              <td style="border:none;" align="char" char=".">0.75</td>
              <td style="border:none;" align="char" char=".">0.58</td>
              <td style="border:none;" align="char" char=".">0.58</td>
              <td style="border:none;" align="char" char=".">0.75</td>
              <td style="border:none;" align="char" char=".">0.74</td>
            </tr>
          </tbody>
        </table>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
          </colgroup>
          <thead>
            <tr>
              <th colspan="7" align="center">Previously
reported results</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">GIN + context
prediction<sup><xref ref-type="bibr" rid="ref32">32</xref></sup></td>
              <td style="border:none;" align="char" char=".">0.69</td>
              <td style="border:none;" align="char" char=".">0.78</td>
              <td style="border:none;" align="char" char=".">0.66</td>
              <td style="border:none;" align="char" char=".">0.63</td>
              <td style="border:none;" align="char" char=".">
                <bold>0.80</bold>
              </td>
              <td style="border:none;" align="char" char=".">0.85</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">GIN + deep
graph infomax<sup><xref ref-type="bibr" rid="ref32">32</xref></sup></td>
              <td style="border:none;" align="char" char=".">0.68</td>
              <td style="border:none;" align="char" char=".">0.78</td>
              <td style="border:none;" align="char" char=".">0.65</td>
              <td style="border:none;" align="char" char=".">0.61</td>
              <td style="border:none;" align="char" char=".">0.78</td>
              <td style="border:none;" align="char" char=".">0.80</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">GIN + edge prediction<sup><xref ref-type="bibr" rid="ref32">32</xref></sup></td>
              <td style="border:none;" align="char" char=".">0.67</td>
              <td style="border:none;" align="char" char=".">0.78</td>
              <td style="border:none;" align="char" char=".">
                <bold>0.67</bold>
              </td>
              <td style="border:none;" align="char" char=".">0.63</td>
              <td style="border:none;" align="char" char=".">0.78</td>
              <td style="border:none;" align="char" char=".">0.79</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">GIN + attribute masking<sup><xref ref-type="bibr" rid="ref32">32</xref></sup></td>
              <td style="border:none;" align="char" char=".">0.67</td>
              <td style="border:none;" align="char" char=".">0.78</td>
              <td style="border:none;" align="char" char=".">0.65</td>
              <td style="border:none;" align="char" char=".">0.64</td>
              <td style="border:none;" align="char" char=".">0.77</td>
              <td style="border:none;" align="char" char=".">0.80</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">Best GNN in MoleculeNet paper<sup><xref ref-type="bibr" rid="ref21">21</xref></sup></td>
              <td style="border:none;" align="char" char=".">0.69</td>
              <td style="border:none;" align="char" char="."> </td>
              <td style="border:none;" align="char" char="."> </td>
              <td style="border:none;" align="char" char="."> </td>
              <td style="border:none;" align="char" char=".">0.76</td>
              <td style="border:none;" align="char" char=".">0.81</td>
            </tr>
          </tbody>
        </table>
        <table-wrap-foot>
          <fn id="t4fn1">
            <label>a</label>
            <p>The best numbers are in bold.</p>
          </fn>
        </table-wrap-foot>
      </table-wrap>
      <p>For all datasets, the best ROC-AUC achieved by DGL-LifeSci models
is higher than the ROC-AUC numbers achieved by the two baseline models.
The best fine-tuned pretrained model either outperforms the best model
trained from scratch or achieves a comparable performance, which suggests
the effectiveness of pretraining on a large amount of unlabeled molecules.
The performance of the fine-tuned models is comparable with the previously
reported results. On BBBP, HIV, and BACE, the best ROC-AUC achieved
by DGL-LifeSci models is consistently higher than the best ROC-AUC
achieved by GNNs in the MoleculeNet paper.</p>
    </sec>
    <sec id="sec5.2">
      <label>5.2</label>
      <title>Reaction
Prediction</title>
      <p>We test WLN against
USPTO<sup><xref ref-type="bibr" rid="ref33">33</xref></sup> dataset following the setting in
the original work.<sup><xref ref-type="bibr" rid="ref3">3</xref></sup> WLN is a two-stage
model for reaction prediction. The first stage identifies candidate
reaction centers, i.e., pairs of atoms that lose or form a bond in
the reaction. The second stage enumerates candidate products from
the candidate reaction centers and ranks them. We achieve comparable
performance for both stages as in <xref rid="tbl5" ref-type="other">Table <xref rid="tbl5" ref-type="other">5</xref></xref>.</p>
      <table-wrap id="tbl5" position="float">
        <label>Table 5</label>
        <caption>
          <title>Test Top-<italic>k</italic> Accuracy
(%) of WLN on USPTO</title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
            <col align="char" char="."/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center"> </th>
              <th colspan="3" align="center" char=".">reaction
center prediction<hr/></th>
              <th colspan="4" align="center" char=".">candidate
ranking<hr/></th>
            </tr>
            <tr>
              <th style="border:none;" align="center">implementations</th>
              <th style="border:none;" align="center" char=".">top 6</th>
              <th style="border:none;" align="center" char=".">top 8</th>
              <th style="border:none;" align="center" char=".">top 10</th>
              <th style="border:none;" align="center" char=".">top 1</th>
              <th style="border:none;" align="center" char=".">top 2</th>
              <th style="border:none;" align="center" char=".">top 3</th>
              <th style="border:none;" align="center" char=".">top 5</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">original</td>
              <td style="border:none;" align="char" char=".">89.8</td>
              <td style="border:none;" align="char" char=".">92.0</td>
              <td style="border:none;" align="char" char=".">93.3</td>
              <td style="border:none;" align="char" char=".">85.6</td>
              <td style="border:none;" align="char" char=".">90.5</td>
              <td style="border:none;" align="char" char=".">92.8</td>
              <td style="border:none;" align="char" char=".">93.4</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">DGL-LifeSci</td>
              <td style="border:none;" align="char" char=".">91.2</td>
              <td style="border:none;" align="char" char=".">93.8</td>
              <td style="border:none;" align="char" char=".">95.0</td>
              <td style="border:none;" align="char" char=".">85.6</td>
              <td style="border:none;" align="char" char=".">90.0</td>
              <td style="border:none;" align="char" char=".">91.7</td>
              <td style="border:none;" align="char" char=".">92.9</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="sec5.3">
      <label>5.3</label>
      <title>Molecule Generation</title>
      <p>We test JTVAE
against a ZINC<sup><xref ref-type="bibr" rid="ref34">34</xref></sup> subset for reconstructing
input molecules.<sup><xref ref-type="bibr" rid="ref2">2</xref></sup> We achieve an accuracy
of 76.4% while the authors’ released code achieved an accuracy
of 74.4%.</p>
    </sec>
    <sec id="sec5.4">
      <label>5.4</label>
      <title>Training Speed</title>
      <p>We compare the modeling
efficiency of DGL-LifeSci against previous implementations, including
original implementations and DeepChem. All experiments record the
averaged training time of one epoch. The testbed is one AWS EC2 p3.2
× large instance (one NVidia V100 GPU with 16GB GPU RAM and 8
VCPUs). Due to the differences in the combinations of models and datasets
across implementations, we only evaluate on a subset of the experiments.
The preliminary results in <xref rid="tbl6" ref-type="other">Table <xref rid="tbl6" ref-type="other">6</xref></xref> show that DGL-LifeSci achieves a comparable or superior
training speed, up to 6×.</p>
      <table-wrap id="tbl6" position="float">
        <label>Table 6</label>
        <caption>
          <title>Epoch Training Time
in Seconds</title>
        </caption>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th style="border:none;" align="center">experiment</th>
              <th style="border:none;" align="center">dataset</th>
              <th style="border:none;" align="center">previous implementation</th>
              <th style="border:none;" align="center">DGL-LifeSci</th>
              <th style="border:none;" align="center">speed up</th>
            </tr>
            <tr>
              <th colspan="5" align="center">Molecular
property prediction</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">NF</td>
              <td style="border:none;" align="left">HIV</td>
              <td style="border:none;" align="left">5.8 (DeepChem 2.3.0)</td>
              <td style="border:none;" align="left">2.5</td>
              <td style="border:none;" align="left">2.3×</td>
            </tr>
            <tr>
              <td style="border:none;" align="left">attentiveFP</td>
              <td style="border:none;" align="left">aromaticity<sup><xref ref-type="bibr" rid="ref26">26</xref></sup></td>
              <td style="border:none;" align="left">6.0</td>
              <td style="border:none;" align="left">1.0</td>
              <td style="border:none;" align="left">6.0×</td>
            </tr>
          </tbody>
        </table>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th colspan="5" align="center">Reaction
prediction</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">WLN for reaction
center prediction</td>
              <td style="border:none;" align="left">USPTO</td>
              <td style="border:none;" align="left">11 657</td>
              <td style="border:none;" align="left">2315</td>
              <td style="border:none;" align="left">5.0×</td>
            </tr>
          </tbody>
        </table>
        <table frame="hsides" rules="groups" border="0">
          <colgroup>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
            <col align="left"/>
          </colgroup>
          <thead>
            <tr>
              <th colspan="5" align="center">Molecule
generation</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="border:none;" align="left">JTVAE</td>
              <td style="border:none;" align="left">ZINC subset</td>
              <td style="border:none;" align="left">44666</td>
              <td style="border:none;" align="left">44 843</td>
              <td style="border:none;" align="left">1.0×</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
  <sec id="sec6">
    <label>6</label>
    <title>Conclusions</title>
    <p>Here,
we present DGL-LifeSci, an open-source Python toolkit for
deep learning on graphs in life science. In the current version of
DGL-LifeSci, we support GNN-based modeling for molecular property
prediction, reaction prediction, and molecule generation.</p>
    <p>With
command-line interfaces, users can perform efficient modeling
on custom datasets without programming a single line of code. Advanced
users can also adapt highly modularized building blocks for their
own development.</p>
    <p>In the current implementations of DGL-LifeSci,
the primary focus
is on small molecules. In the future, we aim to extend the support
to other graphs in life science like proteins and biological networks.
This will open up a much richer set of tasks in life science.</p>
  </sec>
  <sec sec-type="data-availability" id="sec7">
    <label>7</label>
    <title>Data and Software Availability</title>
    <p>The datasets and models
are publicly available at <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/awslabs/dgl-lifesci">https://github.com/awslabs/dgl-lifesci</uri>. The scripts for reproducing the experiments are available in the
following examples.<list list-type="bullet"><list-item><p>Molecular
property prediction: examples/property_prediction/moleculenet.</p></list-item><list-item><p>Reaction prediction: examples/reaction_prediction/rexgen_direct.</p></list-item><list-item><p>Molecule generation: examples/generative_models/jtvae.</p></list-item></list>RDKit, PyTorch, and DGL are all open-source software.</p>
  </sec>
</body>
<back>
  <notes notes-type="COI-statement" id="NOTES-d7e1397-autogenerated">
    <p>The authors
declare no
competing financial interest.</p>
  </notes>
  <ack>
    <title>Acknowledgments</title>
    <p>Since its release, DGL-LifeSci has received
many valuable
suggestions and contributions from the community. The authors are
grateful to the support of our users and contributors. For details
on the community contributions not covered in this manuscript and
in the future, one may refer to <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/awslabs/dgl-lifesci/blob/master/CONTRIBUTORS.md">https://github.com/awslabs/dgl-lifesci/blob/master/CONTRIBUTORS.md</uri>.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="ref1">
      <mixed-citation publication-type="conf-proc" id="cit1"><person-group person-group-type="allauthors"><name><surname>Duvenaud</surname><given-names>D. K.</given-names></name>; <name><surname>Maclaurin</surname><given-names>D.</given-names></name>; <name><surname>Iparraguirre</surname><given-names>J.</given-names></name>; <name><surname>Bombarell</surname><given-names>R.</given-names></name>; <name><surname>Hirzel</surname><given-names>T.</given-names></name>; <name><surname>Aspuru-Guzik</surname><given-names>A.</given-names></name>; <name><surname>Adams</surname><given-names>R.
P.</given-names></name></person-group><source>Convolutional Networks
on Graphs for Learning Molecular Fingerprints</source>, Advances in
Neural Information Processing Systems: Proceedings of the 28th International
Conference on Neural Information Processing Systems, <publisher-name>ACM</publisher-name>, <year>2015</year>; pp <fpage>2224</fpage>–<lpage>2232</lpage>.</mixed-citation>
    </ref>
    <ref id="ref2">
      <mixed-citation publication-type="conf-proc" id="cit2"><person-group person-group-type="allauthors"><name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T.</given-names></name></person-group><source>Junction Tree Variational
Autoencoder for Molecular Graph Generation</source>, Proceedings of
the 35th International Conference on Machine Learning, <publisher-name>PMLR</publisher-name>, <publisher-loc>Stockholmsmässan, Stockholm</publisher-loc>, <year>2018</year>; pp <fpage>2323</fpage>–<lpage>2332</lpage>.</mixed-citation>
    </ref>
    <ref id="ref3">
      <mixed-citation publication-type="journal" id="cit3"><name><surname>Coley</surname><given-names>C.</given-names></name>; <name><surname>Jin</surname><given-names>W.</given-names></name>; <name><surname>Rogers</surname><given-names>L.</given-names></name>; <name><surname>Jamison</surname><given-names>T. F.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T. S.</given-names></name>; <name><surname>Green</surname><given-names>W. H.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Jensen</surname><given-names>K. F.</given-names></name><article-title>A graph-convolutional neural network
model for the prediction of chemical reactivity</article-title>. <source>Chem. Sci.</source><year>2019</year>, <volume>10</volume>, <fpage>370</fpage>–<lpage>377</lpage>. <pub-id pub-id-type="doi">10.1039/C8SC04228D</pub-id>.<pub-id pub-id-type="pmid">30746086</pub-id></mixed-citation>
    </ref>
    <ref id="ref4">
      <mixed-citation publication-type="journal" id="cit4"><name><surname>Wu</surname><given-names>Z.</given-names></name>; <name><surname>Pan</surname><given-names>S.</given-names></name>; <name><surname>Chen</surname><given-names>F.</given-names></name>; <name><surname>Long</surname><given-names>G.</given-names></name>; <name><surname>Zhang</surname><given-names>C.</given-names></name>; <name><surname>Yu</surname><given-names>P. S.</given-names></name><article-title>A comprehensive
survey on graph neural networks</article-title>. <source>IEEE Trans.
Neural Networks Learn. Syst.</source><year>2021</year>, <volume>32</volume>, <fpage>4</fpage>–<lpage>24</lpage>. <pub-id pub-id-type="doi">10.1109/TNNLS.2020.2978386</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref5">
      <mixed-citation publication-type="journal" id="cit5"><name><surname>Zhou</surname><given-names>J.</given-names></name>; <name><surname>Cui</surname><given-names>G.</given-names></name>; <name><surname>Hu</surname><given-names>S.</given-names></name>; <name><surname>Zhang</surname><given-names>Z.</given-names></name>; <name><surname>Yang</surname><given-names>C.</given-names></name>; <name><surname>Liu</surname><given-names>Z.</given-names></name>; <name><surname>Wang</surname><given-names>L.</given-names></name>; <name><surname>Li</surname><given-names>C.</given-names></name>; <name><surname>Sun</surname><given-names>M.</given-names></name><article-title>Graph neural networks:
A review of methods and applications</article-title>. <source>AI Open</source><year>2020</year>, <volume>1</volume>, <fpage>57</fpage>–<lpage>81</lpage>. <pub-id pub-id-type="doi">10.1016/j.aiopen.2021.01.001</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref6">
      <mixed-citation publication-type="journal" id="cit6"><name><surname>Sun</surname><given-names>M.</given-names></name>; <name><surname>Zhao</surname><given-names>S.</given-names></name>; <name><surname>Gilvary</surname><given-names>C.</given-names></name>; <name><surname>Elemento</surname><given-names>O.</given-names></name>; <name><surname>Zhou</surname><given-names>J.</given-names></name>; <name><surname>Wang</surname><given-names>F.</given-names></name><article-title>Graph convolutional
networks for computational drug development and
discovery</article-title>. <source>Briefings Bioinf.</source><year>2019</year>, <volume>21</volume>, <fpage>919</fpage>–<lpage>935</lpage>. <pub-id pub-id-type="doi">10.1093/bib/bbz042</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref7">
      <mixed-citation publication-type="journal" id="cit7"><name><surname>Zitnik</surname><given-names>M.</given-names></name>; <name><surname>Agrawal</surname><given-names>M.</given-names></name>; <name><surname>Leskovec</surname><given-names>J.</given-names></name><article-title>Modeling polypharmacy side effects
with graph convolutional networks</article-title>. <source>Bioinformatics</source><year>2018</year>, <volume>34</volume>, <fpage>i457</fpage>–<lpage>i466</lpage>. <pub-id pub-id-type="doi">10.1093/bioinformatics/bty294</pub-id>.<pub-id pub-id-type="pmid">29949996</pub-id></mixed-citation>
    </ref>
    <ref id="ref8">
      <mixed-citation publication-type="journal" id="cit8"><name><surname>Stokes</surname><given-names>J. M.</given-names></name>; et al. <article-title>A Deep Learning Approach
to Antibiotic Discovery</article-title>. <source>Cell</source><year>2020</year>, <volume>180</volume>, <fpage>688</fpage>–<lpage>702</lpage>. <pub-id pub-id-type="doi">10.1016/j.cell.2020.01.021</pub-id>.<pub-id pub-id-type="pmid">32084340</pub-id></mixed-citation>
    </ref>
    <ref id="ref9">
      <mixed-citation publication-type="conf-proc" id="cit9"><person-group person-group-type="allauthors"><name><surname>Gilmer</surname><given-names>J.</given-names></name>; <name><surname>Schoenholz</surname><given-names>S. S.</given-names></name>; <name><surname>Riley</surname><given-names>P. F.</given-names></name>; <name><surname>Vinyals</surname><given-names>O.</given-names></name>; <name><surname>Dahl</surname><given-names>G.
E.</given-names></name></person-group><source>Neural Message Passing
for Quantum Chemistry</source>, Proceedings of the 34th International
Conference on Machine Learning, <publisher-name>PMLR</publisher-name>, <year>2017</year>; pp <fpage>1263</fpage>–<lpage>1272</lpage>.</mixed-citation>
    </ref>
    <ref id="ref10">
      <mixed-citation publication-type="conf-proc" id="cit10"><person-group person-group-type="allauthors"><name><surname>Shui</surname><given-names>Z.</given-names></name>; <name><surname>Karypis</surname><given-names>G.</given-names></name></person-group><source>Heterogeneous
Molecular
Graph Neural Networks for Predicting Molecule Properties</source>,
2020 IEEE International Conference on Data Mining (ICDM), <publisher-name>IEEE</publisher-name>, <publisher-loc>Los Alamitos, CA</publisher-loc>, <year>2020</year>; pp <fpage>492</fpage>–<lpage>500</lpage>.</mixed-citation>
    </ref>
    <ref id="ref11">
      <mixed-citation publication-type="journal" id="cit11"><name><surname>Feinberg</surname><given-names>E. N.</given-names></name>; <name><surname>Sur</surname><given-names>D.</given-names></name>; <name><surname>Wu</surname><given-names>Z.</given-names></name>; <name><surname>Husic</surname><given-names>B. E.</given-names></name>; <name><surname>Mai</surname><given-names>H.</given-names></name>; <name><surname>Li</surname><given-names>Y.</given-names></name>; <name><surname>Sun</surname><given-names>S.</given-names></name>; <name><surname>Yang</surname><given-names>J.</given-names></name>; <name><surname>Ramsundar</surname><given-names>B.</given-names></name>; <name><surname>Pande</surname><given-names>V. S.</given-names></name><article-title>PotentialNet for
Molecular Property Prediction</article-title>. <source>ACS Cent. Sci.</source><year>2018</year>, <volume>4</volume>, <fpage>1520</fpage>–<lpage>1530</lpage>. <pub-id pub-id-type="doi">10.1021/acscentsci.8b00507</pub-id>.<pub-id pub-id-type="pmid">30555904</pub-id></mixed-citation>
    </ref>
    <ref id="ref12">
      <mixed-citation publication-type="conf-proc" id="cit12"><person-group person-group-type="allauthors"><name><surname>Ingraham</surname><given-names>J.</given-names></name>; <name><surname>Garg</surname><given-names>V.</given-names></name>; <name><surname>Barzilay</surname><given-names>R.</given-names></name>; <name><surname>Jaakkola</surname><given-names>T.</given-names></name></person-group><source>Generative
Models for Graph-Based
Protein Design</source>, Advances in Neural Information Processing
Systems 32, <publisher-name>PMLR</publisher-name>, <year>2019</year>; pp<fpage>15820</fpage>–<lpage>15831</lpage>.</mixed-citation>
    </ref>
    <ref id="ref13">
      <mixed-citation publication-type="book" id="cit13"><person-group person-group-type="allauthors"><name><surname>Ramsundar</surname><given-names>B.</given-names></name>; <name><surname>Eastman</surname><given-names>P.</given-names></name>; <name><surname>Walters</surname><given-names>P.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name></person-group><source>Deep Learning for the Life Sciences</source>, <publisher-name>O’Reilly Media, Inc.</publisher-name>, <year>2019</year>.</mixed-citation>
    </ref>
    <ref id="ref14">
      <mixed-citation publication-type="weblink" id="cit14">Chainer
Chemistry: A Library for Deep Learning in Biology and Chemistry. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://github.com/chainer/chainer-chemistry">https://github.com/chainer/chainer-chemistry</uri>, [Online; accessed 29-October-<year>2020</year>].</mixed-citation>
    </ref>
    <ref id="ref15">
      <mixed-citation publication-type="undeclared" id="cit15"><person-group person-group-type="allauthors"><name><surname>Tokui</surname><given-names>S.</given-names></name>; <name><surname>Oono</surname><given-names>K.</given-names></name>; <name><surname>Hido</surname><given-names>S.</given-names></name>; <name><surname>Clayton</surname><given-names>J.</given-names></name></person-group> Chainer: a Next-Generation Open Source Framework
for Deep Learning. Workshop on Systems for ML at NeurIPS. <year>2015</year>.</mixed-citation>
    </ref>
    <ref id="ref16">
      <mixed-citation publication-type="journal" id="cit16"><name><surname>Shao</surname><given-names>Y.</given-names></name>; <name><surname>Hellström</surname><given-names>M.</given-names></name>; <name><surname>Mitev</surname><given-names>P. D.</given-names></name>; <name><surname>Knijff</surname><given-names>L.</given-names></name>; <name><surname>Zhang</surname><given-names>C.</given-names></name><article-title>PiNN: A Python
Library for Building Atomic Neural Networks of Molecules and Materials</article-title>. <source>J. Chem. Inf. Model.</source><year>2020</year>, <volume>60</volume>, <fpage>1184</fpage>–<lpage>1193</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.9b00994</pub-id>.<pub-id pub-id-type="pmid">31935100</pub-id></mixed-citation>
    </ref>
    <ref id="ref17">
      <mixed-citation publication-type="conf-proc" id="cit17"><person-group person-group-type="allauthors"><name><surname>Bergstra</surname><given-names>J.</given-names></name>; <name><surname>Yamins</surname><given-names>D.</given-names></name>; <name><surname>Cox</surname><given-names>D.
D.</given-names></name></person-group><source>Making a Science of Model
Search: Hyperparameter Optimization in Hundreds of Dimensions for
Vision Architectures</source>, Proceedings of the 30th International
Conference on Machine Learning, <publisher-name>PMLR</publisher-name>, <year>2012</year>; pp <fpage>115</fpage>–<lpage>123</lpage>.</mixed-citation>
    </ref>
    <ref id="ref18">
      <mixed-citation publication-type="conf-proc" id="cit18"><person-group person-group-type="allauthors"><name><surname>Paszke</surname><given-names>A.</given-names></name></person-group><etal/><source>PyTorch:
An Imperative Style, High-Performance Deep Learning Library</source>, Advances in Neural Information Processing Systems 32, <year>2019</year>; pp <fpage>8026</fpage>–<lpage>8037</lpage>.</mixed-citation>
    </ref>
    <ref id="ref19">
      <mixed-citation publication-type="undeclared" id="cit19"><person-group person-group-type="allauthors"><name><surname>Wang</surname><given-names>M.</given-names></name>; <name><surname>Zheng</surname><given-names>D.</given-names></name>; <name><surname>Ye</surname><given-names>Z.</given-names></name>; <name><surname>Gan</surname><given-names>Q.</given-names></name>; <name><surname>Li</surname><given-names>M.</given-names></name>; <name><surname>Song</surname><given-names>X.</given-names></name>; <name><surname>Zhou</surname><given-names>J.</given-names></name>; <name><surname>Ma</surname><given-names>C.</given-names></name>; <name><surname>Yu</surname><given-names>L.</given-names></name>; <name><surname>Gai</surname><given-names>Y.</given-names></name>; <name><surname>Xiao</surname><given-names>T.</given-names></name>; <name><surname>He</surname><given-names>T.</given-names></name>; <name><surname>Karypis</surname><given-names>G.</given-names></name>; <name><surname>Li</surname><given-names>J.</given-names></name>; <name><surname>Zhang</surname><given-names>Z.</given-names></name></person-group><article-title>Deep
Graph Library: A Graph-Centric, Highly-Performant Package for Graph
Neural Networks</article-title>, arXiv.1909.01315, arXiv.org e-Print
archive, <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://arxiv.org/abs/1909.01315.">https://arxiv.org/abs/1909.01315.</uri><year>2020</year>.</mixed-citation>
    </ref>
    <ref id="ref20">
      <mixed-citation publication-type="weblink" id="cit20">RDKit:
Open-source cheminformatics. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="http://www.rdkit.org">http://www.rdkit.org</uri>, [Online; accessed 30-October-<year>2020</year>].</mixed-citation>
    </ref>
    <ref id="ref21">
      <mixed-citation publication-type="journal" id="cit21"><name><surname>Wu</surname><given-names>Z.</given-names></name>; <name><surname>Ramsundar</surname><given-names>B.</given-names></name>; <name><surname>Feinberg</surname><given-names>E. N.</given-names></name>; <name><surname>Gomes</surname><given-names>J.</given-names></name>; <name><surname>Geniesse</surname><given-names>C.</given-names></name>; <name><surname>Pappu</surname><given-names>A. S.</given-names></name>; <name><surname>Leswing</surname><given-names>K.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name><article-title>MoleculeNet: a benchmark
for molecular machine learning</article-title>. <source>Chem. Sci.</source><year>2018</year>, <volume>9</volume>, <fpage>513</fpage>–<lpage>530</lpage>. <pub-id pub-id-type="doi">10.1039/C7SC02664A</pub-id>.<pub-id pub-id-type="pmid">29629118</pub-id></mixed-citation>
    </ref>
    <ref id="ref22">
      <mixed-citation publication-type="journal" id="cit22"><name><surname>Bemis</surname><given-names>G. W.</given-names></name>; <name><surname>Murcko</surname><given-names>M. A.</given-names></name><article-title>The Properties of Known Drugs. 1. Molecular Frameworks</article-title>. <source>J. Med. Chem.</source><year>1996</year>, <volume>39</volume>, <fpage>2887</fpage>–<lpage>2893</lpage>. <pub-id pub-id-type="doi">10.1021/jm9602928</pub-id>.<pub-id pub-id-type="pmid">8709122</pub-id></mixed-citation>
    </ref>
    <ref id="ref23">
      <mixed-citation publication-type="conf-proc" id="cit23"><person-group person-group-type="allauthors"><name><surname>Kipf</surname><given-names>T. N.</given-names></name>; <name><surname>Welling</surname><given-names>M.</given-names></name></person-group><source>Semi-Supervised Classification
with Graph Convolutional Networks</source>, International Conference
on Learning Representations, <publisher-name>OpenReview.net</publisher-name>, <year>2017</year>.</mixed-citation>
    </ref>
    <ref id="ref24">
      <mixed-citation publication-type="conf-proc" id="cit24"><person-group person-group-type="allauthors"><name><surname>Veličković</surname><given-names>P.</given-names></name>; <name><surname>Cucurull</surname><given-names>G.</given-names></name>; <name><surname>Casanova</surname><given-names>A.</given-names></name>; <name><surname>Romero</surname><given-names>A.</given-names></name>; <name><surname>Lió</surname><given-names>P.</given-names></name>; <name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><source>Graph Attention
Networks</source>, International Conference on Learning Representations, <publisher-name>Apollo</publisher-name>, <year>2018</year>.</mixed-citation>
    </ref>
    <ref id="ref25">
      <mixed-citation publication-type="journal" id="cit25"><name><surname>Kearnes</surname><given-names>S.</given-names></name>; <name><surname>McCloskey</surname><given-names>K.</given-names></name>; <name><surname>Berndl</surname><given-names>M.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name>; <name><surname>Riley</surname><given-names>P.</given-names></name><article-title>Molecular
graph convolutions: moving beyond fingerprints</article-title>. <source>J. Comput.-Aided Mol. Des.</source><year>2016</year>, <volume>30</volume>, <fpage>595</fpage>–<lpage>608</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id>.<pub-id pub-id-type="pmid">27558503</pub-id></mixed-citation>
    </ref>
    <ref id="ref26">
      <mixed-citation publication-type="journal" id="cit26"><name><surname>Xiong</surname><given-names>Z.</given-names></name>; <name><surname>Wang</surname><given-names>D.</given-names></name>; <name><surname>Liu</surname><given-names>X.</given-names></name>; <name><surname>Zhong</surname><given-names>F.</given-names></name>; <name><surname>Wan</surname><given-names>X.</given-names></name>; <name><surname>Li</surname><given-names>X.</given-names></name>; <name><surname>Li</surname><given-names>Z.</given-names></name>; <name><surname>Luo</surname><given-names>X.</given-names></name>; <name><surname>Chen</surname><given-names>K.</given-names></name>; <name><surname>Jiang</surname><given-names>H.</given-names></name>; <name><surname>Zheng</surname><given-names>M.</given-names></name><source>J.
Med. Chem.</source><year>2020</year>, <volume>63</volume>, <fpage>8749</fpage>–<lpage>8760</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jmedchem.9b00959</pub-id>.<pub-id pub-id-type="pmid">31408336</pub-id></mixed-citation>
    </ref>
    <ref id="ref27">
      <mixed-citation publication-type="conf-proc" id="cit27"><person-group person-group-type="allauthors"><name><surname>Cho</surname><given-names>K.</given-names></name>; <name><surname>van
Merriënboer</surname><given-names>B.</given-names></name>; <name><surname>Gulcehre</surname><given-names>C.</given-names></name>; <name><surname>Bahdanau</surname><given-names>D.</given-names></name>; <name><surname>Bougares</surname><given-names>F.</given-names></name>; <name><surname>Schwenk</surname><given-names>H.</given-names></name>; <name><surname>Bengio</surname><given-names>Y.</given-names></name></person-group><source>Learning
Phrase Representations using RNN Encoder–Decoder for Statistical
Machine Translation</source>, Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing (EMNLP), <publisher-name>Association for Computational Linguistics</publisher-name>, <year>2014</year>; pp <fpage>1724</fpage>–<lpage>1734</lpage>.</mixed-citation>
    </ref>
    <ref id="ref28">
      <mixed-citation publication-type="journal" id="cit28"><name><surname>Polishchuk</surname><given-names>P. G.</given-names></name>; <name><surname>Madzhidov</surname><given-names>T. I.</given-names></name>; <name><surname>Varnek</surname><given-names>A.</given-names></name><article-title>Estimation of the size
of drug-like
chemical space based on GDB-17 data</article-title>. <source>J. Comput.-Aided
Mol. Des.</source><year>2013</year>, <volume>27</volume>, <fpage>675</fpage>–<lpage>679</lpage>. <pub-id pub-id-type="doi">10.1007/s10822-013-9672-4</pub-id>.<pub-id pub-id-type="pmid">23963658</pub-id></mixed-citation>
    </ref>
    <ref id="ref29">
      <mixed-citation publication-type="journal" id="cit29"><name><surname>Steve
O’Hagan</surname><given-names>D. B. K.</given-names></name><article-title>Analysing and Navigating Natural Products Space for
Generating Small, Diverse, But Representative Chemical Libraries</article-title>. <source>Biotechnol. J.</source><year>2018</year>, <volume>13</volume>, <elocation-id>1700503</elocation-id><pub-id pub-id-type="doi">10.1002/biot.201700503</pub-id>.</mixed-citation>
    </ref>
    <ref id="ref30">
      <mixed-citation publication-type="journal" id="cit30"><name><surname>Reymond</surname><given-names>J.-L.</given-names></name><article-title>The Chemical
Space Project</article-title>. <source>Acc. Chem. Res.</source><year>2015</year>, <volume>48</volume>, <fpage>722</fpage>–<lpage>730</lpage>. <pub-id pub-id-type="doi">10.1021/ar500432k</pub-id>.<pub-id pub-id-type="pmid">25687211</pub-id></mixed-citation>
    </ref>
    <ref id="ref31">
      <mixed-citation publication-type="journal" id="cit31"><name><surname>Dobson</surname><given-names>C. M.</given-names></name><article-title>Chemical
space and biology</article-title>. <source>Nature</source><year>2004</year>, <volume>432</volume>, <fpage>824</fpage>–<lpage>828</lpage>. <pub-id pub-id-type="doi">10.1038/nature03192</pub-id>.<pub-id pub-id-type="pmid">15602547</pub-id></mixed-citation>
    </ref>
    <ref id="ref32">
      <mixed-citation publication-type="conf-proc" id="cit32"><person-group person-group-type="allauthors"><name><surname>Hu</surname><given-names>W.</given-names></name>; <name><surname>Liu</surname><given-names>B.</given-names></name>; <name><surname>Gomes</surname><given-names>J.</given-names></name>; <name><surname>Zitnik</surname><given-names>M.</given-names></name>; <name><surname>Liang</surname><given-names>P.</given-names></name>; <name><surname>Pande</surname><given-names>V.</given-names></name>; <name><surname>Leskovec</surname><given-names>J.</given-names></name></person-group><source>Strategies for Pre-training Graph Neural Networks</source>, International Conference on Learning Representations, <publisher-name>OpenReview.net</publisher-name>, <year>2020</year>.</mixed-citation>
    </ref>
    <ref id="ref33">
      <mixed-citation publication-type="weblink" id="cit33">Patent
reaction extraction: downloads. <uri xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://bitbucket.org/dan2097/patent-reaction-extraction/downloads">https://bitbucket.org/dan2097/patent-reaction-extraction/downloads</uri>, <year>2014</year>.</mixed-citation>
    </ref>
    <ref id="ref34">
      <mixed-citation publication-type="journal" id="cit34"><name><surname>Sterling</surname><given-names>T.</given-names></name>; <name><surname>Irwin</surname><given-names>J. J.</given-names></name><article-title>ZINC 15 –
Ligand Discovery for Everyone</article-title>. <source>J. Chem. Inf.
Model.</source><year>2015</year>, <volume>55</volume>, <fpage>2324</fpage>–<lpage>2337</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.5b00559</pub-id>.<pub-id pub-id-type="pmid">26479676</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
