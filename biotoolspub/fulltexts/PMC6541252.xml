<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1d3 20150301//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 39.96?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">PLoS One</journal-id>
    <journal-id journal-id-type="iso-abbrev">PLoS ONE</journal-id>
    <journal-id journal-id-type="publisher-id">plos</journal-id>
    <journal-id journal-id-type="pmc">plosone</journal-id>
    <journal-title-group>
      <journal-title>PLoS ONE</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1932-6203</issn>
    <publisher>
      <publisher-name>Public Library of Science</publisher-name>
      <publisher-loc>San Francisco, CA USA</publisher-loc>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">6541252</article-id>
    <article-id pub-id-type="doi">10.1371/journal.pone.0216796</article-id>
    <article-id pub-id-type="publisher-id">PONE-D-18-33606</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Research Article</subject>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Imaging Techniques</subject>
          <subj-group>
            <subject>Neuroimaging</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Neuroscience</subject>
          <subj-group>
            <subject>Neuroimaging</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Software Tools</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Engineering and Technology</subject>
        <subj-group>
          <subject>Software Engineering</subject>
          <subj-group>
            <subject>Software Tools</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Biology and Life Sciences</subject>
        <subj-group>
          <subject>Anatomy</subject>
          <subj-group>
            <subject>Histology</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Anatomy</subject>
          <subj-group>
            <subject>Histology</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Animal Studies</subject>
          <subj-group>
            <subject>Experimental Organism Systems</subject>
            <subj-group>
              <subject>Model Organisms</subject>
              <subj-group>
                <subject>Mouse Models</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Model Organisms</subject>
          <subj-group>
            <subject>Mouse Models</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Animal Studies</subject>
          <subj-group>
            <subject>Experimental Organism Systems</subject>
            <subj-group>
              <subject>Animal Models</subject>
              <subj-group>
                <subject>Mouse Models</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Medicine and Health Sciences</subject>
        <subj-group>
          <subject>Neurology</subject>
          <subj-group>
            <subject>Brain Diseases</subject>
          </subj-group>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Computer and Information Sciences</subject>
        <subj-group>
          <subject>Computer Software</subject>
        </subj-group>
      </subj-group>
      <subj-group subj-group-type="Discipline-v3">
        <subject>Research and Analysis Methods</subject>
        <subj-group>
          <subject>Bioassays and Physiological Analysis</subject>
          <subj-group>
            <subject>Electrophysiological Techniques</subject>
            <subj-group>
              <subject>Membrane Electrophysiology</subject>
              <subj-group>
                <subject>Electrode Recording</subject>
              </subj-group>
            </subj-group>
          </subj-group>
        </subj-group>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>Spatial registration of serial microscopic brain images to three-dimensional reference atlases with the QuickNII tool</article-title>
      <alt-title alt-title-type="running-head">Spatial registration of serial microscopic brain images to three-dimensional reference atlases</alt-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-4536-8197</contrib-id>
        <name>
          <surname>Puchades</surname>
          <given-names>Maja A.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-2093-6274</contrib-id>
        <name>
          <surname>Csucs</surname>
          <given-names>Gergely</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Software</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Ledergerber</surname>
          <given-names>Debora</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Formal analysis</role>
        <role content-type="http://credit.casrai.org/">Resources</role>
        <xref ref-type="aff" rid="aff002">
          <sup>2</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Leergaard</surname>
          <given-names>Trygve B.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7899-906X</contrib-id>
        <name>
          <surname>Bjaalie</surname>
          <given-names>Jan G.</given-names>
        </name>
        <role content-type="http://credit.casrai.org/">Conceptualization</role>
        <role content-type="http://credit.casrai.org/">Funding acquisition</role>
        <role content-type="http://credit.casrai.org/">Project administration</role>
        <role content-type="http://credit.casrai.org/">Supervision</role>
        <role content-type="http://credit.casrai.org/">Writing – original draft</role>
        <xref ref-type="aff" rid="aff001">
          <sup>1</sup>
        </xref>
        <xref ref-type="corresp" rid="cor001">*</xref>
      </contrib>
    </contrib-group>
    <aff id="aff001">
      <label>1</label>
      <addr-line>Neural Systems Laboratory, Department of Molecular Medicine, Institute of Basic Medical Sciences, University of Oslo, Norway</addr-line>
    </aff>
    <aff id="aff002">
      <label>2</label>
      <addr-line>Kavli Institute for Systems Neuroscience, Norwegian University of Science and Technology, NTNU, Norway</addr-line>
    </aff>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Malmierca</surname>
          <given-names>Manuel S.</given-names>
        </name>
        <role>Editor</role>
        <xref ref-type="aff" rid="edit1"/>
      </contrib>
    </contrib-group>
    <aff id="edit1">
      <addr-line>Universidad de Salamanca, SPAIN</addr-line>
    </aff>
    <author-notes>
      <fn fn-type="COI-statement" id="coi001">
        <p><bold>Competing Interests: </bold>The authors have declared that no competing interests exist.</p>
      </fn>
      <corresp id="cor001">* E-mail: <email>j.g.bjaalie@medisin.uio.no</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>29</day>
      <month>5</month>
      <year>2019</year>
    </pub-date>
    <pub-date pub-type="collection">
      <year>2019</year>
    </pub-date>
    <volume>14</volume>
    <issue>5</issue>
    <elocation-id>e0216796</elocation-id>
    <history>
      <date date-type="received">
        <day>22</day>
        <month>11</month>
        <year>2018</year>
      </date>
      <date date-type="accepted">
        <day>29</day>
        <month>4</month>
        <year>2019</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2019 Puchades et al</copyright-statement>
      <copyright-year>2019</copyright-year>
      <copyright-holder>Puchades et al</copyright-holder>
      <license xlink:href="http://creativecommons.org/licenses/by/4.0/">
        <license-p>This is an open access article distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</license-p>
      </license>
    </permissions>
    <self-uri content-type="pdf" xlink:href="pone.0216796.pdf"/>
    <abstract>
      <p>Modern high throughput brain wide profiling techniques for cells and their morphology, connectivity, and other properties, make the use of reference atlases with 3D coordinate frameworks essential. However, anatomical location of observations made in microscopic sectional images from rodent brains is typically determined by comparison with 2D anatomical reference atlases. A major challenge in this regard is that microscopic sections often are cut with orientations deviating from the standard planes used in the reference atlases, resulting in inaccuracies and a need for tedious correction steps. Overall, efficient tools for registration of large series of section images to reference atlases are currently not widely available. Here we present QuickNII, a stand-alone software tool for semi-automated affine spatial registration of sectional image data to a 3D reference atlas coordinate framework. A key feature in the tool is the capability to generate user defined cut planes through the reference atlas, matching the orientation of the cut plane of the sectional image data. The reference atlas is transformed to match anatomical landmarks in the corresponding experimental images. In this way, the spatial relationship between experimental image and atlas is defined, without introducing distortions in the original experimental images. Following anchoring of a limited number of sections containing key landmarks, transformations are propagated across the entire series of sectional images to reduce the amount of manual steps required. By having coordinates assigned to the experimental images, further analysis of the distribution of features extracted from the images is greatly facilitated.</p>
    </abstract>
    <funding-group>
      <award-group id="award001">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100010664</institution-id>
            <institution>H2020 Future and Emerging Technologies</institution>
          </institution-wrap>
        </funding-source>
        <award-id>720270 (HBP SGA1)</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7899-906X</contrib-id>
          <name>
            <surname>Bjaalie</surname>
            <given-names>Jan G.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award002">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/100010664</institution-id>
            <institution>H2020 Future and Emerging Technologies</institution>
          </institution-wrap>
        </funding-source>
        <award-id>785907 (HBP SGA2)</award-id>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7899-906X</contrib-id>
          <name>
            <surname>Bjaalie</surname>
            <given-names>Jan G.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <award-group id="award003">
        <funding-source>
          <institution-wrap>
            <institution-id institution-id-type="funder-id">http://dx.doi.org/10.13039/501100005416</institution-id>
            <institution>Norges Forskningsråd</institution>
          </institution-wrap>
        </funding-source>
        <principal-award-recipient>
          <contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-7899-906X</contrib-id>
          <name>
            <surname>Bjaalie</surname>
            <given-names>Jan G.</given-names>
          </name>
        </principal-award-recipient>
      </award-group>
      <funding-statement>This work was supported by the European Union`s Horizon 2020 Research and Innovation Programme under Grant Agreement No. 720270 (HBP SGA1) and Grant Agreement No. 785907 (HBP SGA2), with additional support from The Research Council of Norway, Grant Agreement No. 269774 (INCF National Node) to JGB. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</funding-statement>
    </funding-group>
    <counts>
      <fig-count count="4"/>
      <table-count count="1"/>
      <page-count count="14"/>
    </counts>
    <custom-meta-group>
      <custom-meta id="data-availability">
        <meta-name>Data Availability</meta-name>
        <meta-value>The software described is available through the public repository provided by NITRC.ORG. The detailed descriptions can be found on the QuickNII page on <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/quicknii">https://www.nitrc.org/projects/quicknii</ext-link> (RRID:SCR_016854).</meta-value>
      </custom-meta>
    </custom-meta-group>
  </article-meta>
  <notes>
    <title>Data Availability</title>
    <p>The software described is available through the public repository provided by NITRC.ORG. The detailed descriptions can be found on the QuickNII page on <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/quicknii">https://www.nitrc.org/projects/quicknii</ext-link> (RRID:SCR_016854).</p>
  </notes>
</front>
<body>
  <sec sec-type="intro" id="sec001">
    <title>Introduction</title>
    <p>Interpretation of brain-related data requires precise information about the anatomical location from which the data are derived. Recent advancements in the availability and usability of digital three-dimensional (3D) atlases and associated tools have made it feasible to associate many different data types to these known standards [<xref rid="pone.0216796.ref001" ref-type="bibr">1</xref>–<xref rid="pone.0216796.ref003" ref-type="bibr">3</xref>]. This in turn has given the research community the ability to greatly expand the capability to perform powerful and unique analyses with the option of looking beyond single studies or data modality.</p>
    <p>While the ability to register single two-dimensional (2D) images to 3D atlas slices exists in a number of tools [<xref rid="pone.0216796.ref004" ref-type="bibr">4</xref>–<xref rid="pone.0216796.ref007" ref-type="bibr">7</xref>], many are coding based interfaces and therefore not user friendly to the wider neuroscience community. In addition, the ability to treat the atlas as a true 3D image is not always feasible. Most paper, and even 3D atlases, can only be viewed in the standard coronal, sagittal, and horizontal planes, with few tools allowing views in non-standard planes. This greatly limits the ability to match real life data to the high quality datasets in canonical atlases.</p>
    <p>The importance of being able to examine an atlas off-axis is illustrated in <xref ref-type="fig" rid="pone.0216796.g001">Fig 1</xref>. Here the angle is oblique; perpendicular to the long axis of the rat hippocampus about halfway between the coronal and sagittal planes (<xref ref-type="fig" rid="pone.0216796.g001">Fig 1A1–1A4</xref>). The section angle was chosen to identify the precise locations of a series of electrode tracks, but the anatomical features are very difficult to interpret, even for experts. Generating custom atlas plates that match the image orientation provide the foundation necessary to properly identify anatomy (<xref ref-type="fig" rid="pone.0216796.g001">Fig 1B1–1B4</xref>). This example demonstrates that the ability to treat an atlas as truly 3D and generate customized reference atlas plates is an important feature for a registration tool. Further, when section angles are aimed at the standard planes, the matching of section images to canonical atlases is challenged by the often-occurring small, but non-trivial, deviations from the intended angle of orientation.</p>
    <fig id="pone.0216796.g001" orientation="portrait" position="float">
      <object-id pub-id-type="doi">10.1371/journal.pone.0216796.g001</object-id>
      <label>Fig 1</label>
      <caption>
        <title>Mapping electrophysiological recording sites in the rat hippocampal formation.</title>
        <p>(A1-A4) Raw images of thionin-stained histological sections cut perpendicularly to the long axis of the hippocampus in order to visualize electrode positions (illustrated in E). (B1-B4) Corresponding customised atlas maps from the rat Waxholm Space atlas, obtained from the QuickNII tool after registration. (C1) Visualisation of overlayed images A1 and B1 in QuickNII, allowing to read and collect coordinates for electrode tracts positions. The name of the region appears when the mouse is pointed to that region. (D) The coordinates can be plotted and visualized in a 3D viewer (e.g. MeshView on <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/meshview">https://www.nitrc.org/projects/meshview</ext-link>) enabling comparisons between animals or different experiments. (E) 3D visualization of the rat hippocampal formation, which is a C-shaped structure curving obliquely from the midline towards the temporal lobe of the brain. (Scale bar, 1 mm).</p>
      </caption>
      <graphic xlink:href="pone.0216796.g001"/>
    </fig>
    <p>As the community moves towards analyzing brains in 3D, there has been a push towards collecting datasets that span the full brain, especially in the rodent [<xref rid="pone.0216796.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0216796.ref002" ref-type="bibr">2</xref>, <xref rid="pone.0216796.ref008" ref-type="bibr">8</xref>–<xref rid="pone.0216796.ref013" ref-type="bibr">13</xref>]. While some laboratories have developed tools and workflows to reconstruct image series covering the whole brain back into 3D and register them to a canonical atlas [<xref rid="pone.0216796.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0216796.ref011" ref-type="bibr">11</xref>, <xref rid="pone.0216796.ref014" ref-type="bibr">14</xref>–<xref rid="pone.0216796.ref016" ref-type="bibr">16</xref>]; the majority of investigators do not have access to these kinds of technical resources. Registering an individual 2D image to a 3D atlas is challenging and time-consuming, and registering a large series, especially when a number of slices lack anatomical information, is nearly impossible. However, if individual images in a series can be treated as pieces of a puzzle, key landmarks in some slices can be used to aid the placement of slices lacking such information.</p>
    <p>We have created a new neuroinformatics tool called QuickNII (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/quicknii">https://www.nitrc.org/projects/quicknii</ext-link>), specifically to aid the process of bringing a series of 2D images into 3D space in register with a reference atlas. QuickNII is a standalone, GUI-based, open access tool tailored for efficient assignation of spatial location to serial microscopic brain images.</p>
    <p>Two versions are available; one for the Waxholm Space rat brain atlas [<xref rid="pone.0216796.ref003" ref-type="bibr">3</xref>, <xref rid="pone.0216796.ref017" ref-type="bibr">17</xref>] (available from <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/quicknii">https://www.nitrc.org/projects/quicknii</ext-link> and one for the Allen Brain Mouse Atlas [<xref rid="pone.0216796.ref001" ref-type="bibr">1</xref>, <xref rid="pone.0216796.ref002" ref-type="bibr">2</xref>] (available from <ext-link ext-link-type="uri" xlink:href="http://www.brain-map.org/">http://www.brain-map.org/</ext-link>). Within QuickNII, these volumetric brain references are used to generate customized atlas plates that match the spatial orientation of any experimental sections.</p>
  </sec>
  <sec id="sec002">
    <title>Results and discussions</title>
    <sec id="sec003">
      <title>Registration of serial sections images to a reference atlas with QuickNII</title>
      <p>To avoid the technical and scientific challenges related to adjusting and distorting very large images, QuickNII linearly stretches the atlas plates to match down-sized experimental section images. This establishes the spatial relationship between the atlas and input image necessary to assign spatial coordinates to the image. Anatomical location is defined by superimposing customized atlas images onto section images in a process we call “anchoring.”</p>
      <p>The high-level workflow of how one uses QuickNII to register a series of images and obtain the spatial information for the images is shown in <xref ref-type="fig" rid="pone.0216796.g002">Fig 2</xref> and explained in detail in the User guide and on <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/quicknii">https://www.nitrc.org/projects/quicknii</ext-link>.</p>
      <fig id="pone.0216796.g002" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216796.g002</object-id>
        <label>Fig 2</label>
        <caption>
          <title>QuickNII workflow.</title>
          <p>Diagram showing key steps of the workflow used to anchor serial section images to the Allen Mouse CCF v.3 reference atlas space using QuickNII. Following initial preprocessing steps where the sequence and orientation of the serial images is validated and a configuration XML file is generated, images are imported to QuickNII. The user will use visible landmarks in the experimental image to manipulate the atlas, adjusting position, scale, and orientation (rotation and tilt) that best matches the selected image. QuickNII automatically propagates information about position, scale, and tilt to the entire series. By iterative anchoring of selected key sections, the user can optimize the automatically propagated parameters. A final validation for each image is strongly recommended. Output from QuickNII is a series of custom atlas maps matching each anchored experimental image, and an XML file describing a set of vectors (<italic>o</italic>, <italic>u</italic>, and <italic>v</italic>) that define the position of each image relative to the technical origin of the reference atlas used.</p>
        </caption>
        <graphic xlink:href="pone.0216796.g002"/>
      </fig>
      <p>The principal steps are:</p>
      <list list-type="simple">
        <list-item>
          <p>Step 1: Users start by sectioning and collecting contiguous brain tissue slices. They typically end up with a series, or multiple series of slices with different stains or types of expression on alternating slices.</p>
        </list-item>
        <list-item>
          <p>Step 2: The slices are digitized and the images organized sequentially before pre-processing to ensure proper orientation of all images, eventual renaming and downscaling (see <xref ref-type="sec" rid="sec009">Methods</xref>). Upon completion, the user should have a series of image files consistent with the original brain slices.</p>
        </list-item>
        <list-item>
          <p>Step 3: Using the program “FileBuilder”, provided together with QuickNII, the user generates an XML descriptor file (see <xref ref-type="sec" rid="sec009">Methods</xref>), and QuickNII uses this information to generate an initial distribution of the images in the chosen atlas (<xref ref-type="supplementary-material" rid="pone.0216796.s001">S1 Fig</xref>, situation 1). Thus, when the user begins, the slices are sorted in the correct order and may already be roughly located in the appropriate position, giving a first approximation that can be fine-tuned by the user. Working in QuickNII gives the ability to view an image superimposed on the atlas modality of choice (multiple data modality options for each atlas) with varying levels of opacity in relation to the atlas (<xref ref-type="fig" rid="pone.0216796.g003">Fig 3A</xref>).</p>
        </list-item>
        <list-item>
          <p>Step 4: The user can browse through the images and identify landmarks for accurate positioning of each slice by adjusting the dorso-ventral and the mediolateral angles (<xref ref-type="fig" rid="pone.0216796.g003">Fig 3B</xref>). The location of the slice can be viewed in all three commonly used planes. This along with the power to view and adjust off-axis cuts through the atlas enables a better fit. Once the slice is positioned in the atlas, the atlas image can be scaled horizontally or vertically to match the size of the image slice. When the user is satisfied with the match between an image and the reference atlas, they can anchor it to the atlas (<xref ref-type="fig" rid="pone.0216796.g003">Fig 3C</xref>). The approach used here differs from other published methods where the anteroposterior position of the section in the atlas is only approximated and non-linear deformations methods are applied directly, without taking into account the section angle of orientation [<xref rid="pone.0216796.ref006" ref-type="bibr">6</xref>, <xref rid="pone.0216796.ref018" ref-type="bibr">18</xref>].</p>
        </list-item>
        <list-item>
          <p>Step 5: Once an image in a series has been anchored to the atlas, a new spacing distribution is automatically calculated for the remaining images. The spacing continues to be recalculated as additional images are registered (<xref ref-type="supplementary-material" rid="pone.0216796.s001">S1 Fig</xref>, situation 3). A major advantage of this method is that users can select images with more identifiable anatomical landmarks to map to the atlas, and in turn, this information is used to place the other slices without obvious landmarks.</p>
        </list-item>
        <list-item>
          <p>Step 6: Once two images near the start and the end of the series have been anchored to the atlas by the user, the remaining sections should be in close proximity to their appropriate position in the atlas. At this point, the user can rapidly step through each image in the series and fine-tune its position. The result of this procedure is considered to be a global anchoring, aimed at the best possible matching of the atlas to the brain region present in the experimental section. A local anchoring approach can be chosen if the user wishes to achieve the best possible match to one or a few brain regions, disregarding neighboring regions.</p>
        </list-item>
        <list-item>
          <p>Step 7: The user saves a new XML file, in which the spatial coordinates of each 2D image in relation to the atlas is captured.</p>
        </list-item>
      </list>
      <fig id="pone.0216796.g003" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216796.g003</object-id>
        <label>Fig 3</label>
        <caption>
          <title>Registration of a brain section image to the Allen mouse reference atlas with QuickNII.</title>
          <p>(A) Before registration in the QuickNII interface, the section image and the reference atlas are shown in a default Bregma position with the dorsoventral (DV) and mediolateral (ML) angles at position 0. (B) Line drawing of the reference atlas plate matching the location and tilted to fit the DV and ML angles of orientation of the section shown in A. (C) After registration in QuickNII, Bregma position, DV angle (+13) and MV angle (-4) have been adjusted by the user. Registration of more than one section to atlas results in repositioning of the other sections in the series relative to the reference atlas. Iterative manual registration of sections is performed until a satisfactory result is reached across the entire series of images. (D) The section images anchored with QuickNII can be visualized in a 3D viewer tool (scalablebrainatlas.incf.org), where atlas meshes can be selected and viewed (here the anterior commissure in red and the somatosensory and somatomotor areas in green).</p>
        </caption>
        <graphic xlink:href="pone.0216796.g003"/>
      </fig>
    </sec>
    <sec id="sec004">
      <title>QuickNII output: Examples of use</title>
      <sec id="sec005">
        <title>Location and visualization of electrode tracts in the rat Waxholm Space Atlas</title>
        <p>In QuickNII, the coordinates of any location in a 2D image are displayed when pointing with the mouse cursor (See User guide). The coordinates can be manually collected and plotted in the 3D viewer, MeshView (<xref ref-type="fig" rid="pone.0216796.g001">Fig 1D</xref>), enabling analysis of spatial distributions and comparisons of the distribution of selected features across animals.</p>
        <p>Depending on the quality of the histological material and the type of analysis to be pursued, an additional non-linear adjustment of the registration between the atlas slice and section image may be required.</p>
      </sec>
      <sec id="sec006">
        <title>Visualization of mouse brain sections in the Allen mouse brain atlas space</title>
        <p>For image series anchored to the Allen mouse brain atlas, users can visualize their sections together with atlas meshes in the Scalable Brain Atlas Composer tool [<xref rid="pone.0216796.ref019" ref-type="bibr">19</xref>] in an interactive manner (<xref ref-type="fig" rid="pone.0216796.g003">Fig 3D</xref>). This viewer allows uploading directly the output from QuickNII, i.e. the output XML file and png images.</p>
      </sec>
    </sec>
    <sec id="sec007">
      <title>Opportunities for interoperable use of neuroscience data</title>
      <p>Recent international efforts in large scale integration of neuroscience data and cellular and cell type profiling, such as the European Union Human Brain Project (<ext-link ext-link-type="uri" xlink:href="http://www.humanbrainproject.eu/en/">http://www.humanbrainproject.eu/en/</ext-link>), Human Cell Atlas (<ext-link ext-link-type="uri" xlink:href="http://www.humancellatlas.org/">http://www.humancellatlas.org/</ext-link>), and NIH BRAIN Initiative Cell Census Network (<ext-link ext-link-type="uri" xlink:href="http://www.biccn.org">http://www.biccn.org</ext-link>), recognize the importance of using 3D common coordinate frameworks and the need for developing the relevant computational tools and infrastructures enabling heavy computing power; not available for single research groups.</p>
      <p>In QuickNII, while anchoring is performed on downsized images; the full size images can inherit atlas-related information about brain structure (see <xref ref-type="sec" rid="sec009">Methods</xref>). Exporting the coordinates generated by QuickNII in XML format allows the option for further analysis and viewing in other software tools. In this context, we have developed an image viewer tool embedded in a web application that reads QuickNII XML files, displays high-resolution microscopic images (Microsoft Deep Zoom file format) with atlas overlay images, and provides basic annotation functionalities. This tool, which requires the possibility of accessing pyramid versions of high-resolution images, is used to inspect microscopic details and record points of interest as X,Y,Z coordinates in reference atlas space. These derived coordinates can be visualized as geometric objects together with meshes representing anatomical structures derived from a reference atlas. In this way, datasets from different experiments can be integrated in the same reference space, allowing quantitative analyses in response to diverse neuroanatomic-related questions. Examples of applications include the ability to accurately map recording electrode locations in 3D space and identify the areas and regions in which they were located (<xref ref-type="fig" rid="pone.0216796.g001">Fig 1</xref>), as well as the ability to co-localize tracer injection sites, multiple categories of neurons, or other labeled features in the brain, as illustrated in <xref ref-type="fig" rid="pone.0216796.g004">Fig 4</xref>. By combining the atlas maps with segmented images (<xref ref-type="fig" rid="pone.0216796.g004">Fig 4C and 4G</xref>), the user will be able to extract spatial coordinates of objects and compare data across datasets as illustrated in <xref ref-type="fig" rid="pone.0216796.g004">Fig 4I–4N</xref>. This workflow, albeit not accessible to individual neuroscience laboratories is part of an infrastructure created in the Human Brain Project with the aim to make it available to the whole community [<xref rid="pone.0216796.ref020" ref-type="bibr">20</xref>].</p>
      <fig id="pone.0216796.g004" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216796.g004</object-id>
        <label>Fig 4</label>
        <caption>
          <title>Spatial integration of data from different experimental image series.</title>
          <p>The figure illustrates how histological features from differently oriented histological images can be extracted and compared after spatial registration to a common reference atlas (Allen Mouse Brain Atlas). (A) Shows one sagittal microscopic image from a series of a mouse brain, labelled by in-situ hybridization with an RNA probe against parvalbulmin (data from mouse.brain-map.org/, case 19056). (E) Shows one coronal microscopic image from a series of a transgenic mouse brain (the Tg2576 model for Alzheimer disease) in which amyloid plaque has been visualized using immunohistochemistry. (B, F) Using QuickNII, the two series of sections are registered to the Allen Mouse Brain Atlas. The previous sections are shown with their respective atlas maps superimposed. (C, G) Labelled features of interest are extracted using the machine learning software tool ilastik (ilastik.org). (D, H) 3D visualization of the extracted features which, represented by centroid point coordinates, are displayed in 3D atlas space. Data points in (D) are color coded by regions in the atlas. (I-I’) Stereo pair images illustrating how data points from the two experimental data sets (blue dots, parvalbumin positive cells; red dots, amyloid plaques) can be co-visualized in an atlas region of interest, in this example the piriform cortex. To see the stereo images the viewer must cross the eye axis to let the pair of images merge into a 3D image. (J-N) Images illustrating combined data points in the piriform cortex in views from lateral (J), anterior (K) or obliquely from dorsal, perpendicular to the cortical sheet (L-M), as indicated on the inset brain images. The red and blue data points are displayed together (L) and separately (M, N). Scale bar, 1 mm.</p>
        </caption>
        <graphic xlink:href="pone.0216796.g004"/>
      </fig>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec008">
    <title>Conclusion</title>
    <p>We have here presented a strategy, workflow, and user-friendly tool for rapid registration of many microscopic 2D images to 3D atlas space. QuickNII gives researchers a tool to fairly quickly and easily locate series of 2D image slices in the 3D space of canonical atlases. We have used QuickNII for mouse and rat datasets and developed well-defined procedures and tutorials for anchoring 2D image series. These procedures together with training material have been successfully used by several test site laboratories [<xref rid="pone.0216796.ref021" ref-type="bibr">21</xref>] and in several Human Brain Project training workshops. QuickNII with its tutorial is available via the NITRC homepage (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/quicknii">https://www.nitrc.org/projects/quicknii</ext-link>).</p>
  </sec>
  <sec sec-type="materials|methods" id="sec009">
    <title>Materials and methods</title>
    <sec id="sec010">
      <title>Histological material</title>
      <p>The material shown in <xref ref-type="fig" rid="pone.0216796.g001">Fig 1</xref> is from a 5 months male Long Evans rat, from which histological sections were stained with Nissl. The experiments related to this animal were performed in accordance with the Norwegian Animal Welfare Act and the European Convention for the Protection of Vertebrate Animals used for Experimental and Other Scientific Purposes, and in compliance with protocols approved by the Norwegian Animal Research Authorities, permit number 7163. High-resolution section images were acquired using an automated slide scanner system (Axio Scan, Carl Zeiss MicroImaging, Jena, Germany). The material shown in Figs <xref ref-type="fig" rid="pone.0216796.g002">2</xref>, <xref ref-type="fig" rid="pone.0216796.g003">3</xref> and <xref ref-type="fig" rid="pone.0216796.g004">4(E) and 4(F)</xref> is from an 18 months male mouse model (Tg2576 mouse) for Alzheimer disease, provided by M. Hartlage-Rübsamen and S. Rossner. Coronal sections were stained for amyloid plaques (4G8 antibody, Signet Lab Cat# 4G8, RRID:AB_2313891) and visualized by immunohistochemistry. The animal experiments were approved by Landesdirektion Sachsen, license no T28/16. High-resolution section images were acquired using an automated slide scanner system (Axio Scan, Carl Zeiss MicroImaging, Jena, Germany). The material shown in <xref ref-type="fig" rid="pone.0216796.g004">Fig 4(A, B)</xref> is from a 57BL/6J, P56 male mouse. It demonstrates distribution of parvalbumin mRNA in sagittal brain sections identified using in-situ hybridization. The section images were provided by the Allen Institute for Brain Science, Parvalbumin in situ hybridization, experiment no. 2007 75457579, 2007, available from: <ext-link ext-link-type="uri" xlink:href="http://mouse.brainmap.org/experiment/show/75457579">http://mouse.brainmap.org/experiment/show/75457579</ext-link> (accessed 28 November 2017).</p>
    </sec>
    <sec id="sec011">
      <title>Preparation of image data</title>
      <sec id="sec012">
        <title>Preprocessing of images</title>
        <p>QuickNII v2.2 supports standard web-compatible image formats, 24-bit PNG and JPEG. Images can be loaded up to the resolution of 16 megapixels (e.g. 4000x4000 or 5000x3000 pixels), however QuickNII does not benefit from image resolutions exceeding the resolution of the monitor in use. For a standard FullHD or WUXGA display (1920x1080 or 1920x1200 pixels) the useful image area is approximately 1500x1000 pixels, using a similar resolution ensures optimal image-loading performance.</p>
        <p>Preprocessing of images (downsampling, rotation, renaming) can be achieved with open access software tools (e.g. ImageMagick, Matlab scripts) or python scripts found in many open source libraries (e.g. PIL) to fulfill these requirements (converting to PNG or JPEG and downscaling to screen-like size), but QuickNII allows storage of original image dimensions as part of its series descriptor. Serial section images should be assigned consecutive serial numbers, preferably indicated by three-digit numbers at the end of the file name, e.g. Sample_ID_s001.tif. Section sampling is given by the serial numbers. The section images are collected in a folder.</p>
      </sec>
      <sec id="sec013">
        <title>Creation of the image series descriptor file</title>
        <p>The program “FileBuilder.bat” is provided with QuickNII. This program asks for the folder where your images are located, reviews the image files, and generates an xml file. This file can be saved and serves as input file for the given collection of images in QuickNII. FileBuilder uses numbers in the file names in order to generate the serial ordering. If section numbers are not recognized, the user will have the option to number the images in the FileBuilder program.</p>
      </sec>
    </sec>
    <sec id="sec014">
      <title>Software technical description</title>
      <p>The detailed descriptions can be found on the QuickNII page on <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/quicknii">https://www.nitrc.org/projects/quicknii</ext-link> (RRID:SCR_016854).</p>
    </sec>
    <sec id="sec015">
      <title>Coordinate system</title>
      <p>QuickNII uses the Neuroimaging Informatics Technology Initiative (NIfTI) coordinate system (<ext-link ext-link-type="uri" xlink:href="https://nifti.nimh.nih.gov/">https://nifti.nimh.nih.gov/</ext-link>) for the reference atlases. Coordinates in NIfTI-space are expressed in voxels. The origin is the bottom left corner of the most anterior coronal plane, x axis increases from left to right, y axis increases from posterior to anterior, and the z axis increases from inferior to superior. As 2D images do not contain spatial information in relation to real space, they are described in the reference atlas using “anchor vectors”. The 2D images are not modified during this process; instead, each image is described by <italic>the corresponding atlas slice</italic> in the reference atlas and specified with the following vectors:</p>
      <list list-type="bullet">
        <list-item>
          <p><underline><bold><italic toggle="yes">o</italic></bold></underline> points from the technical origin to the top left corner of the given atlas slice, essentially specifying an origin for that atlas slice.</p>
        </list-item>
        <list-item>
          <p><underline><bold><italic toggle="yes">u</italic></bold></underline> points from <underline><bold><italic toggle="yes">o</italic></bold></underline> to the top right corner of the atlas slice, specifying the horizontal edge of the atlas slice.</p>
        </list-item>
        <list-item>
          <p><underline><bold><italic toggle="yes">v</italic></bold></underline> points from <underline><bold><italic toggle="yes">o</italic></bold></underline> to the bottom left corner of the atlas slice, specifying the vertical edge of the atlas slice.</p>
        </list-item>
      </list>
    </sec>
    <sec id="sec016">
      <title>Correspondence between image and atlas coordinates</title>
      <p>As <underline><bold><italic toggle="yes">o</italic></bold></underline>, <underline><bold><italic toggle="yes">u</italic></bold></underline> and <underline><bold><italic toggle="yes">v</italic></bold></underline> vectors are expressed in voxels, they represent a simple and direct connection between image pixels and atlas voxels, a weighted sum of <underline><bold><italic toggle="yes">u</italic></bold></underline> and <underline><bold><italic toggle="yes">v</italic></bold></underline> vectors has to be added to the origin, <underline><bold><italic toggle="yes">o</italic></bold></underline>. <underline><bold><italic toggle="yes">o</italic></bold></underline> itself is the top-left corner (0,0 pixel coordinate) of the image, <underline><bold><italic toggle="yes">o</italic></bold></underline>+<underline><bold><italic toggle="yes">u</italic></bold></underline> is the top-right corner (width-in-pixels,0), <underline><bold><italic toggle="yes">o</italic></bold></underline>+<underline><bold><italic toggle="yes">v</italic></bold></underline> is the bottom-left corner (0,height-in-pixels), <underline><bold><italic toggle="yes">o</italic></bold></underline>+<underline><bold><italic toggle="yes">u</italic></bold></underline>+<underline><bold><italic toggle="yes">v</italic></bold></underline> is the bottom-right corner (width-in-pixels,height-in-pixels), and everything else is somewhere in between, weights of u and v running from 0 to 1 (See <xref rid="pone.0216796.t001" ref-type="table">Table 1</xref>). Practically x and y pixel coordinates have to be divided by width-in-pixels and height-in-pixels respectively.</p>
      <table-wrap id="pone.0216796.t001" orientation="portrait" position="float">
        <object-id pub-id-type="doi">10.1371/journal.pone.0216796.t001</object-id>
        <label>Table 1</label>
        <caption>
          <title>Table showing the corner cases and calculation for an arbitrary pixel (where x,y denote pixel coordinates and w,h abbreviates width-in-pixels and height-in-pixels).</title>
        </caption>
        <alternatives>
          <graphic id="pone.0216796.t001g" xlink:href="pone.0216796.t001"/>
          <table frame="hsides" rules="groups">
            <colgroup span="1">
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
              <col align="left" valign="middle" span="1"/>
            </colgroup>
            <thead>
              <tr>
                <th align="left" rowspan="1" colspan="1"/>
                <th align="center" rowspan="1" colspan="1">Vector</th>
                <th align="center" rowspan="1" colspan="1">Coordinates</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td align="left" rowspan="1" colspan="1">Top-left corner</td>
                <td align="left" rowspan="1" colspan="1">
                  <underline>
                    <bold>
                      <italic toggle="yes">o</italic>
                    </bold>
                  </underline>
                </td>
                <td align="left" rowspan="1" colspan="1">[o<sub>x</sub> o<sub>y</sub> o<sub>z</sub>]</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Top-right corner</td>
                <td align="left" rowspan="1" colspan="1"><underline><bold><italic toggle="yes">o</italic></bold></underline>+<underline><bold><italic toggle="yes">u</italic></bold></underline></td>
                <td align="left" rowspan="1" colspan="1">[o<sub>x</sub>+u<sub>x</sub> o<sub>y</sub>+u<sub>y</sub> o<sub>z</sub>+u<sub>z</sub>]</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Bottom-left corner</td>
                <td align="left" rowspan="1" colspan="1"><underline><bold><italic toggle="yes">o</italic></bold></underline>+<underline><bold><italic toggle="yes">v</italic></bold></underline></td>
                <td align="left" rowspan="1" colspan="1">[o<sub>x</sub>+v<sub>x</sub> o<sub>y</sub>+v<sub>y</sub> o<sub>z</sub>+v<sub>z</sub>]</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Bottom-right corner</td>
                <td align="left" rowspan="1" colspan="1"><underline><bold><italic toggle="yes">o</italic></bold></underline>+<underline><bold><italic toggle="yes">u</italic></bold></underline>+<underline><bold><italic toggle="yes">v</italic></bold></underline></td>
                <td align="left" rowspan="1" colspan="1">[o<sub>x</sub>+u<sub>x</sub>+v<sub>x</sub> o<sub>y</sub>+u<sub>y</sub>+v<sub>y</sub> o<sub>z</sub>+u<sub>z</sub>+v<sub>z</sub>]</td>
              </tr>
              <tr>
                <td align="left" rowspan="1" colspan="1">Any pixel</td>
                <td align="left" rowspan="1" colspan="1"><underline><bold><italic toggle="yes">o</italic></bold></underline>+x/w*<underline><bold><italic toggle="yes">u</italic></bold></underline>+y/h*<underline><bold><italic toggle="yes">v</italic></bold></underline></td>
                <td align="left" rowspan="1" colspan="1">[o<sub>x</sub>+x/w*u<sub>x</sub>+y/h*v<sub>x</sub> o<sub>y</sub>+x/w*u<sub>y</sub>+y/h*v<sub>y</sub> o<sub>z</sub>+x/w*u<sub>z</sub>+y/h*v<sub>z</sub>]</td>
              </tr>
            </tbody>
          </table>
        </alternatives>
      </table-wrap>
      <p>The vectors can also be used to build a transformation matrix:
<disp-formula id="pone.0216796.e001"><alternatives><graphic xlink:href="pone.0216796.e001.jpg" id="pone.0216796.e001g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M1"><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="2pt"/><mml:mo>=</mml:mo><mml:mspace width="2pt"/><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>w</mml:mi></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mi>h</mml:mi></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo><mml:mi mathvariant="normal">*</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:math></alternatives></disp-formula>
Where x<sub>v</sub>, y<sub>v</sub>, z<sub>v</sub> are coordinates in atlas voxels, for x<sub>p</sub>, y<sub>p</sub> image pixels. w and h again abbreviates image dimensions, width-in-pixels and height-in-pixels.</p>
    </sec>
    <sec id="sec017">
      <title>Correspondence between atlas coordinates in voxels and physical space</title>
      <sec id="sec018">
        <title>Transformation of mouse voxel coordinates to Allen CCFv3</title>
        <p>(<ext-link ext-link-type="uri" xlink:href="http://help.brain-map.org/display/mousebrain/API">http://help.brain-map.org/display/mousebrain/API</ext-link>):
<disp-formula id="pone.0216796.e002"><alternatives><graphic xlink:href="pone.0216796.e002.jpg" id="pone.0216796.e002g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M2"><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="2pt"/><mml:mo>=</mml:mo><mml:mspace width="2pt"/><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo><mml:mi mathvariant="normal">*</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>25</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>-</mml:mo><mml:mn>25</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>-</mml:mo><mml:mn>25</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>13175</mml:mn></mml:mtd><mml:mtd><mml:mn>7975</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:math></alternatives></disp-formula></p>
      </sec>
    </sec>
    <sec id="sec019">
      <title>Transformation of rat voxel coordinates to Waxholm space</title>
      <disp-formula id="pone.0216796.e003">
        <alternatives>
          <graphic xlink:href="pone.0216796.e003.jpg" id="pone.0216796.e003g" mimetype="image" position="anchor" orientation="portrait"/>
          <mml:math id="M3">
            <mml:mo>(</mml:mo>
            <mml:mrow>
              <mml:mtable>
                <mml:mtr>
                  <mml:mtd>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>w</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>w</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>w</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>1</mml:mn>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:mrow>
            <mml:mo>)</mml:mo>
            <mml:mspace width="2pt"/>
            <mml:mo>=</mml:mo>
            <mml:mspace width="2pt"/>
            <mml:mo>(</mml:mo>
            <mml:mrow>
              <mml:mtable>
                <mml:mtr>
                  <mml:mtd>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>x</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>v</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>v</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>v</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>1</mml:mn>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:mrow>
            <mml:mo>)</mml:mo>
            <mml:mi mathvariant="normal">*</mml:mi>
            <mml:mo>[</mml:mo>
            <mml:mrow>
              <mml:mtable>
                <mml:mtr>
                  <mml:mtd>
                    <mml:mn>0.0390625</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>0</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>0</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>0</mml:mn>
                  </mml:mtd>
                </mml:mtr>
                <mml:mtr>
                  <mml:mtd>
                    <mml:mn>0</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>0.0390625</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>0</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>0</mml:mn>
                  </mml:mtd>
                </mml:mtr>
                <mml:mtr>
                  <mml:mtd>
                    <mml:mn>0</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>0</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>0.0390625</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>0</mml:mn>
                  </mml:mtd>
                </mml:mtr>
                <mml:mtr>
                  <mml:mtd>
                    <mml:mo>-</mml:mo>
                    <mml:mn>9.53125</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mo>-</mml:mo>
                    <mml:mn>24.3359375</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mo>-</mml:mo>
                    <mml:mn>9.6875</mml:mn>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mn>1</mml:mn>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:mrow>
            <mml:mo>]</mml:mo>
          </mml:math>
        </alternatives>
      </disp-formula>
      <p>Where x<sub>v</sub>, y<sub>v</sub>, z<sub>v</sub> are coordinates in atlas voxels (RAS axis orientation and order), x<sub>a</sub>, y<sub>a</sub>, z<sub>a</sub> are coordinates in Allen CCFv3 (PIR axis orientation and order, values are expressed in μm-s) and x<sub>w</sub>, y<sub>w</sub>, z<sub>w</sub> are coordinates in Waxholm Space (RAS axis orientation and order, values are expressed in mm-s).</p>
    </sec>
    <sec id="sec020">
      <title>Output data</title>
      <p>After the anchoring procedure, the images are linked to the 3D reference atlas. The output consists of a new XML file where the vectors describing the corresponding atlas slice are stored for each image file in the series.</p>
    </sec>
    <sec id="sec021">
      <title>Example of XML descriptor extended with anchoring vectors</title>
      <p>
        <monospace>&lt;?xml version = '1.0' encoding = ’UTF-8'?&gt;</monospace>
      </p>
      <p>
        <monospace>&lt;series name = ’Test series'&gt;</monospace>
      </p>
      <p>
        <monospace>&lt;slice filename = ’sampleID_s002.png’ nr = '2' width = '24723' height = '18561'</monospace>
      </p>
      <p>
        <monospace>anchoring = ’ox = 312.2&amp;oy = 533.8&amp;oz = 218.4&amp;ux = -185.7&amp;uy = -35.5&amp;uz = 6.6&amp;vx = -4.6&amp;vy = -7.5&amp;vz = -171.4'/&gt;</monospace>
      </p>
      <p>
        <monospace>&lt;slice filename = sampleID _s008.png’ nr = '8' width = '24722' height = '17507'</monospace>
      </p>
      <p>
        <monospace>anchoring = ’ox = 334.82142136461607&amp;oy = 485.7990978550188&amp;oz = 251.62087421842926&amp;ux = -228.65532680537657&amp;uy = -13.31692466388239&amp;uz = -11.98107468791568&amp;vx = 11.021383786310935&amp;vy = -7.15410850678 6784&amp;vz = -202.38817266644594'/&gt;</monospace>
      </p>
      <p>
        <monospace>&lt;/series&gt;</monospace>
      </p>
      <p>slice.anchoring: x-y-z coordinates of <underline>o</underline>-<underline>u</underline>-<underline>v</underline> vectors in URL-encoded format.</p>
      <p>While anchoring vectors are available for all section images all the time (see section on ‘Propagation’ below), the XML descriptor contains only the ones approved by users via pressing the ‘Store’ button. This approach encapsulates the distinction between anchor vectors being ‘estimated’ (algorithmically) or ‘verified’ (by the user), and conserves storage space at the same time.</p>
    </sec>
    <sec id="sec022">
      <title>Same example as JSON descriptor</title>
      <p>
        <monospace>{"name":"Test series","slices":[</monospace>
      </p>
      <p>
        <monospace>{"nr":2,"filename":"sampleID_s002.png","width":24723,"height":18561,</monospace>
      </p>
      <p>
        <monospace>"anchoring":[312.2,533.8,218.4,-185.7,-35.5,6.6,-4.6,-7.5,-171.4]},</monospace>
      </p>
      <p>
        <monospace>{"nr":8,"filename":"sampleID_s008.png","width":24722,"height":17507,</monospace>
      </p>
      <p>
        <monospace>"anchoring":[334.82142136461607,485.7990978550188,251.62087421842932,-228.6553268,-13.316924663882391,-11.981074687915681,11.021383786310937,-7.15410850678,-202.3881726664459]}</monospace>
      </p>
      <p>
        <monospace>]}</monospace>
      </p>
      <p>The field “anchoring” in the JSON format is a compact list of the 9 anchor components, in o<sub>x</sub>, o<sub>y</sub>, o<sub>z</sub>, u<sub>x</sub>, u<sub>y</sub>, u<sub>z</sub>, v<sub>x</sub>, v<sub>y</sub>, v<sub>z</sub> order.</p>
    </sec>
    <sec id="sec023">
      <title>Export atlas slices</title>
      <p>"Export Slices" generates atlas slices for all section images in the series. The images are generated into a timestamped subfolder next to the series, "Slices-YYYYMMDDHHmmSS" format. All available templates (MRI, DTI, pMRI, Nissl) are sliced as well as the segmentation volume. The slices follow the resolution of the atlas and thus they are typically much smaller and have a different aspect ratio than the original images. QuickNII works with the exact same slices and stretches them over the actual images to achieve the view what the user sees: all four corners of the slice are positioned at the corners of the image. This can be reproduced with arbitrary image manipulation software. A progress bar is showing the generation of slices, QuickNII cannot be used for anything else until the process is finished.</p>
      <p>Custom atlas slices are generated in the native resolution of the atlas (i.e. 1 pixel is a 40x40 um2 square for WHS Rat and 25x25 um2 square for Allen Mouse), and thus they are independent from the resolution of the original images and usually much smaller.</p>
      <p>Scaling along horizontal and vertical axes is done separately, the scaling factors are expected to be close to each other, but they are not necessarily equal.</p>
      <p>Let w,h denote the width and height of the original image in pixels, and c<sub>w</sub>,c<sub>h</sub> denote the width and height of the custom atlas slice.</p>
      <p>In order to get corresponding c<sub>x</sub>,c<sub>y</sub> position in the customized atlas slice for any x,y position in the original image, the following calculation can be done:
<disp-formula id="pone.0216796.e004"><alternatives><graphic xlink:href="pone.0216796.e004.jpg" id="pone.0216796.e004g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M4"><mml:mrow><mml:msub><mml:mtext>c</mml:mtext><mml:mtext>x</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mtext>x</mml:mtext><mml:mo>*</mml:mo><mml:msub><mml:mtext>c</mml:mtext><mml:mtext>w</mml:mtext></mml:msub><mml:mo>/</mml:mo><mml:mtext>w</mml:mtext></mml:mrow></mml:math></alternatives></disp-formula>
<disp-formula id="pone.0216796.e005"><alternatives><graphic xlink:href="pone.0216796.e005.jpg" id="pone.0216796.e005g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M5"><mml:mrow><mml:msub><mml:mtext>c</mml:mtext><mml:mtext>y</mml:mtext></mml:msub><mml:msub><mml:mrow><mml:mo>=</mml:mo><mml:mtext>y</mml:mtext><mml:mspace width="2pt"/><mml:mtext>c</mml:mtext></mml:mrow><mml:mtext>h</mml:mtext></mml:msub><mml:mtext>/h</mml:mtext></mml:mrow></mml:math></alternatives></disp-formula>
where x = 0…(w-1), y = 0…(h-1) and the resulting c<sub>x</sub> = 0…(c<sub>w</sub>-1), c<sub>y</sub> = 0…(c<sub>h</sub>-1), bounds are inclusive.</p>
      <p>With matrix notation:
<disp-formula id="pone.0216796.e006"><alternatives><graphic xlink:href="pone.0216796.e006.jpg" id="pone.0216796.e006g" mimetype="image" position="anchor" orientation="portrait"/><mml:math id="M6"><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="2pt"/><mml:mo>=</mml:mo><mml:mspace width="2pt"/><mml:mo>(</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>x</mml:mi></mml:mtd><mml:mtd><mml:mi>y</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>)</mml:mo><mml:mi mathvariant="normal">*</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:math></alternatives></disp-formula></p>
    </sec>
    <sec id="sec024">
      <title>Export formats</title>
      <list list-type="bullet">
        <list-item>
          <p>for templates and segmentation: 24-bit (truecolor) PNG files, "&lt;original filename&gt;-&lt;template or segmentation name&gt;.png" naming pattern is used</p>
        </list-item>
        <list-item>
          <p>for segmentation slices a "&lt;original filename&gt;-&lt;segmentation name&gt;.flat" file is generated too, which is an uncompressed binary format containing atlas identifiers</p>
        </list-item>
        <list-item>
          <p>palette: "&lt;segmentation name&gt;.json" file contains colors and names for segmentation.</p>
        </list-item>
      </list>
      <p>".flat" format:</p>
      <p>Offset 0 [byte]: Bpp, Bytes per pixel (1 or 2 in current uses) Offset 1 [32-bit integer]: width (in pixels) Offset 5 [32-bit integer]: height (in pixels) Offset 9: Bpp*width*height bytes of pixel (atlas identifier) data.</p>
      <p>Quantities are stored in big-endian order.</p>
      <p>Palette format:</p>
      <p>JSON file containing an array of (index, red, green, blue, name) tuples, where name is string the rest are numbers. Index is deliberately redundant; it contains the actual array index.</p>
    </sec>
    <sec id="sec025">
      <title>System requirements and software availability</title>
      <p>System requirements for the QuickNII tool are</p>
      <list list-type="bullet">
        <list-item>
          <p>Microsoft Windows: 64-bit operating system, Windows 7 or later</p>
        </list-item>
        <list-item>
          <p>Apple macOS: OS X 10.9 (Mavericks) or later</p>
        </list-item>
        <list-item>
          <p>3 gigabytes RAM</p>
        </list-item>
        <list-item>
          <p>Display resolution minimum 1440 pixels wide and minimum 650 pixels high.</p>
        </list-item>
      </list>
      <p>The QuickNII software is available for download from <ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/quicknii">https://www.nitrc.org/projects/quicknii</ext-link>.</p>
    </sec>
  </sec>
  <sec sec-type="supplementary-material" id="sec026">
    <title>Supporting information</title>
    <supplementary-material content-type="local-data" id="pone.0216796.s001">
      <label>S1 Fig</label>
      <caption>
        <title>Illustration of DV and ML angle adjustments.</title>
        <p>(A) Line drawing of an experimental rat brain section cut at an angle slightly tilted compared to the standard coronal plane. The section is one in a series of sections through the brain. Through the registration process in QuickNII, correctly angled and positioned atlas plates from a 3D rat brain reference atlas (Waxholm Space rat brain atlas) will be superimposed onto each section in the series. (B) Line drawing of a coronal reference atlas plate, found to be close to the section shown in A. The slight difference in angle of orientation between the images in A and B is most easily observed in the hippocampal region, marked with an asterisk. (C) Line drawing of a reference atlas plate matching the location and tilted to fit the angle of orientation of the section shown in A. In the middle and bottom rows, the assumed position of the section shown as is indicated in situation 1 (solid line relative to sagittal and horizontal atlas diagrams, shown in the middle and lower rows, respectively), whereas the angle of orientation of the atlas plate shown in C is indicated in situation 2. The angle of orientation and the scaling (not shown) defined for the adjusted atlas plate is automatically propagated across the series, while distances among sections are automatically stretched or compressed. (D) Section from A (grey) and atlas plate from C (blue) superimposed. Registration of more than one section to atlas (situation 3, red dashed line) results in repositioning of the other sections in the series relative to the reference atlas. Iterative manual registration of sections is performed until a satisfactory result is reached across the series of images.</p>
        <p>(TIF)</p>
      </caption>
      <media xlink:href="pone.0216796.s001.tif">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
    <supplementary-material content-type="local-data" id="pone.0216796.s002">
      <label>S1 File</label>
      <caption>
        <title>QuickNII user guide.</title>
        <p>The user guide summarizes the main steps in the anchoring procedure. Detailed and updated information about the user interface and the software functionalities can be found on NITRC.org.</p>
        <p>(PDF)</p>
      </caption>
      <media xlink:href="pone.0216796.s002.pdf">
        <caption>
          <p>Click here for additional data file.</p>
        </caption>
      </media>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack>
    <p>We thank Dmitri Darine and Ivar A. Moene for technical inputs,Martin Øvsthus, Sveinung Lillehaug, and Martyna Checinska for testing the software and giving feedback to increase functionality, and Rembrandt Bakker for enabling the output from QuickNII to be visualized in the Scalable Brain Atlas Composer tool. We also thank Sharon C. Yates for contributions to the figures, Ingvild E. Bjerke, Camilla H. Blixhavn and Heidi Kleven for contributions to the tutorial guide and user manual, and Jyl Boline for comments to the manuscript.</p>
  </ack>
  <ref-list>
    <title>References</title>
    <ref id="pone.0216796.ref001">
      <label>1</label>
      <mixed-citation publication-type="journal"><name><surname>Lein</surname><given-names>ES</given-names></name>, <name><surname>Hawrylycz</surname><given-names>MJ</given-names></name>, <name><surname>Ao</surname><given-names>N</given-names></name>, <name><surname>Ayres</surname><given-names>M</given-names></name>, <name><surname>Bensinger</surname><given-names>A</given-names></name>, <name><surname>Bernard</surname><given-names>A</given-names></name>, <etal>et al</etal><article-title>Genome-wide atlas of gene expression in the adult mouse brain</article-title>. <source>Nature</source>. <year>2007</year>;<volume>445</volume>(<issue>7124</issue>):<fpage>168</fpage>–<lpage>76</lpage>. <pub-id pub-id-type="doi">10.1038/nature05453</pub-id> .<?supplied-pmid 17151600?><pub-id pub-id-type="pmid">17151600</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref002">
      <label>2</label>
      <mixed-citation publication-type="journal"><name><surname>Oh</surname><given-names>SW</given-names></name>, <name><surname>Harris</surname><given-names>JA</given-names></name>, <name><surname>Ng</surname><given-names>L</given-names></name>, <name><surname>Winslow</surname><given-names>B</given-names></name>, <name><surname>Cain</surname><given-names>N</given-names></name>, <name><surname>Mihalas</surname><given-names>S</given-names></name>, <etal>et al</etal><article-title>A mesoscale connectome of the mouse brain</article-title>. <source>Nature</source>. <year>2014</year>;<volume>508</volume>(<issue>7495</issue>):<fpage>207</fpage>–<lpage>14</lpage>. Epub 2014/04/04. <pub-id pub-id-type="doi">10.1038/nature13186</pub-id> .<?supplied-pmid 24695228?><pub-id pub-id-type="pmid">24695228</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref003">
      <label>3</label>
      <mixed-citation publication-type="journal"><name><surname>Papp</surname><given-names>EA</given-names></name>, <name><surname>Leergaard</surname><given-names>TB</given-names></name>, <name><surname>Calabrese</surname><given-names>E</given-names></name>, <name><surname>Johnson</surname><given-names>GA</given-names></name>, <name><surname>Bjaalie</surname><given-names>JG</given-names></name>. <article-title>Waxholm Space atlas of the Sprague Dawley rat brain</article-title>. <source>Neuroimage</source>. <year>2014</year>;<volume>97</volume>:<fpage>374</fpage>–<lpage>86</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.04.001</pub-id> .<?supplied-pmid 24726336?><pub-id pub-id-type="pmid">24726336</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref004">
      <label>4</label>
      <mixed-citation publication-type="journal"><name><surname>Campagnola</surname><given-names>L</given-names></name>, <name><surname>Manis</surname><given-names>PB</given-names></name>. <article-title>A map of functional synaptic connectivity in the mouse anteroventral cochlear nucleus</article-title>. <source>J Neurosci</source>. <year>2014</year>;<volume>34</volume>(<issue>6</issue>):<fpage>2214</fpage>–<lpage>30</lpage>. Epub 2014/02/07. <pub-id pub-id-type="doi">10.1523/JNEUROSCI.4669-13.2014</pub-id> .<?supplied-pmid 24501361?><pub-id pub-id-type="pmid">24501361</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref005">
      <label>5</label>
      <mixed-citation publication-type="journal"><name><surname>Vandenberghe</surname><given-names>ME</given-names></name>, <name><surname>Herard</surname><given-names>AS</given-names></name>, <name><surname>Souedet</surname><given-names>N</given-names></name>, <name><surname>Sadouni</surname><given-names>E</given-names></name>, <name><surname>Santin</surname><given-names>MD</given-names></name>, <name><surname>Briet</surname><given-names>D</given-names></name>, <etal>et al</etal><article-title>High-throughput 3D whole-brain quantitative histopathology in rodents</article-title>. <source>Sci Rep</source>. <year>2016</year>;<volume>6</volume>:<fpage>20958</fpage> Epub 2016/02/16. <pub-id pub-id-type="doi">10.1038/srep20958</pub-id> .<?supplied-pmid 26876372?><pub-id pub-id-type="pmid">26876372</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref006">
      <label>6</label>
      <mixed-citation publication-type="journal"><name><surname>Furth</surname><given-names>D</given-names></name>, <name><surname>Vaissiere</surname><given-names>T</given-names></name>, <name><surname>Tzortzi</surname><given-names>O</given-names></name>, <name><surname>Xuan</surname><given-names>Y</given-names></name>, <name><surname>Martin</surname><given-names>A</given-names></name>, <name><surname>Lazaridis</surname><given-names>I</given-names></name>, <etal>et al</etal><article-title>An interactive framework for whole-brain maps at cellular resolution</article-title>. <source>Nat Neurosci</source>. <year>2018</year>;<volume>21</volume>(<issue>1</issue>):<fpage>139</fpage>–<lpage>49</lpage>. Epub 2017/12/06. <pub-id pub-id-type="doi">10.1038/s41593-017-0027-7</pub-id> .<?supplied-pmid 29203898?><pub-id pub-id-type="pmid">29203898</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref007">
      <label>7</label>
      <mixed-citation publication-type="journal"><name><surname>Niedworok</surname><given-names>CJ</given-names></name>, <name><surname>Brown</surname><given-names>AP</given-names></name>, <name><surname>Jorge Cardoso</surname><given-names>M</given-names></name>, <name><surname>Osten</surname><given-names>P</given-names></name>, <name><surname>Ourselin</surname><given-names>S</given-names></name>, <name><surname>Modat</surname><given-names>M</given-names></name>, <etal>et al</etal><article-title>aMAP is a validated pipeline for registration and segmentation of high-resolution mouse brain data</article-title>. <source>Nat Commun</source>. <year>2016</year>;<volume>7</volume>:<fpage>11879</fpage> Epub 2016/07/08. <pub-id pub-id-type="doi">10.1038/ncomms11879</pub-id> .<?supplied-pmid 27384127?><pub-id pub-id-type="pmid">27384127</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref008">
      <label>8</label>
      <mixed-citation publication-type="journal"><name><surname>Bohland</surname><given-names>JW</given-names></name>, <name><surname>Wu</surname><given-names>C</given-names></name>, <name><surname>Barbas</surname><given-names>H</given-names></name>, <name><surname>Bokil</surname><given-names>H</given-names></name>, <name><surname>Bota</surname><given-names>M</given-names></name>, <name><surname>Breiter</surname><given-names>HC</given-names></name>, <etal>et al</etal><article-title>A proposal for a coordinated effort for the determination of brainwide neuroanatomical connectivity in model organisms at a mesoscopic scale</article-title>. <source>PLoS Comput Biol</source>. <year>2009</year>;<volume>5</volume>(<issue>3</issue>):<fpage>e1000334</fpage> Epub 2009/03/28. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1000334</pub-id> .<?supplied-pmid 19325892?><pub-id pub-id-type="pmid">19325892</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref009">
      <label>9</label>
      <mixed-citation publication-type="journal"><name><surname>Ragan</surname><given-names>T</given-names></name>, <name><surname>Kadiri</surname><given-names>LR</given-names></name>, <name><surname>Venkataraju</surname><given-names>KU</given-names></name>, <name><surname>Bahlmann</surname><given-names>K</given-names></name>, <name><surname>Sutin</surname><given-names>J</given-names></name>, <name><surname>Taranda</surname><given-names>J</given-names></name>, <etal>et al</etal><article-title>Serial two-photon tomography for automated ex vivo mouse brain imaging</article-title>. <source>Nat Methods</source>. <year>2012</year>;<volume>9</volume>(<issue>3</issue>):<fpage>255</fpage>–<lpage>8</lpage>. Epub 2012/01/17. <pub-id pub-id-type="doi">10.1038/nmeth.1854</pub-id> .<?supplied-pmid 22245809?><pub-id pub-id-type="pmid">22245809</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref010">
      <label>10</label>
      <mixed-citation publication-type="journal"><name><surname>Tiesinga</surname><given-names>P</given-names></name>, <name><surname>Bakker</surname><given-names>R</given-names></name>, <name><surname>Hill</surname><given-names>S</given-names></name>, <name><surname>Bjaalie</surname><given-names>JG</given-names></name>. <article-title>Feeding the human brain model</article-title>. <source>Curr Opin Neurobiol</source>. <year>2015</year>;<volume>32</volume>:<fpage>107</fpage>–<lpage>14</lpage>. Epub 2015/03/01. <pub-id pub-id-type="doi">10.1016/j.conb.2015.02.003</pub-id> .<?supplied-pmid 25725212?><pub-id pub-id-type="pmid">25725212</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref011">
      <label>11</label>
      <mixed-citation publication-type="journal"><name><surname>Zakiewicz</surname><given-names>IM</given-names></name>, <name><surname>Majka</surname><given-names>P</given-names></name>, <name><surname>Wojcik</surname><given-names>DK</given-names></name>, <name><surname>Bjaalie</surname><given-names>JG</given-names></name>, <name><surname>Leergaard</surname><given-names>TB</given-names></name>. <article-title>Three-Dimensional Histology Volume Reconstruction of Axonal Tract Tracing Data: Exploring Topographical Organization in Subcortical Projections from Rat Barrel Cortex</article-title>. <source>PLoS One</source>. <year>2015</year>;<volume>10</volume>(<issue>9</issue>):<fpage>e0137571</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0137571</pub-id> .<?supplied-pmid 26398192?><pub-id pub-id-type="pmid">26398192</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref012">
      <label>12</label>
      <mixed-citation publication-type="journal"><name><surname>Zakiewicz</surname><given-names>IM</given-names></name>, <name><surname>van Dongen</surname><given-names>YC</given-names></name>, <name><surname>Leergaard</surname><given-names>TB</given-names></name>, <name><surname>Bjaalie</surname><given-names>JG</given-names></name>. <article-title>Workflow and atlas system for brain-wide mapping of axonal connectivity in rat</article-title>. <source>PLoS One</source>. <year>2011</year>;<volume>6</volume>(<issue>8</issue>):<fpage>e22669</fpage> Epub 2011/08/11. <pub-id pub-id-type="doi">10.1371/journal.pone.0022669</pub-id> .<?supplied-pmid 21829640?><pub-id pub-id-type="pmid">21829640</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref013">
      <label>13</label>
      <mixed-citation publication-type="journal"><name><surname>Majka</surname><given-names>P</given-names></name>, <name><surname>Kowalski</surname><given-names>JM</given-names></name>, <name><surname>Chlodzinska</surname><given-names>N</given-names></name>, <name><surname>Wojcik</surname><given-names>DK</given-names></name>. <article-title>3D brain atlas reconstructor service--online repository of three-dimensional models of brain structures</article-title>. <source>Neuroinformatics</source>. <year>2013</year>;<volume>11</volume>(<issue>4</issue>):<fpage>507</fpage>–<lpage>18</lpage>. <pub-id pub-id-type="doi">10.1007/s12021-013-9199-9</pub-id><pub-id pub-id-type="pmid">23943281</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref014">
      <label>14</label>
      <mixed-citation publication-type="journal"><name><surname>Ju</surname><given-names>T</given-names></name>, <name><surname>Warren</surname><given-names>J</given-names></name>, <name><surname>Carson</surname><given-names>J</given-names></name>, <name><surname>Bello</surname><given-names>M</given-names></name>, <name><surname>Kakadiaris</surname><given-names>I</given-names></name>, <name><surname>Chiu</surname><given-names>W</given-names></name>, <etal>et al</etal><article-title>3D volume reconstruction of a mouse brain from histological sections using warp filtering</article-title>. <source>J Neurosci Methods</source>. <year>2006</year>;<volume>156</volume>(<issue>1–2</issue>):<fpage>84</fpage>–<lpage>100</lpage>. Epub 2006/04/04. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.02.020</pub-id> .<?supplied-pmid 16580732?><pub-id pub-id-type="pmid">16580732</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref015">
      <label>15</label>
      <mixed-citation publication-type="journal"><name><surname>Schubert</surname><given-names>N</given-names></name>, <name><surname>Axer</surname><given-names>M</given-names></name>, <name><surname>Schober</surname><given-names>M</given-names></name>, <name><surname>Huynh</surname><given-names>AM</given-names></name>, <name><surname>Huysegoms</surname><given-names>M</given-names></name>, <name><surname>Palomero-Gallagher</surname><given-names>N</given-names></name>, <etal>et al</etal><article-title>3D Reconstructed Cyto-, Muscarinic M2 Receptor, and Fiber Architecture of the Rat Brain Registered to the Waxholm Space Atlas</article-title>. <source>Front Neuroanat</source>. <year>2016</year>;<volume>10</volume>:<fpage>51</fpage> Epub 2016/05/21. <pub-id pub-id-type="doi">10.3389/fnana.2016.00051</pub-id> .<?supplied-pmid 27199682?><pub-id pub-id-type="pmid">27199682</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref016">
      <label>16</label>
      <mixed-citation publication-type="journal"><name><surname>Kim</surname><given-names>Y</given-names></name>, <name><surname>Yang</surname><given-names>GR</given-names></name>, <name><surname>Pradhan</surname><given-names>K</given-names></name>, <name><surname>Venkataraju</surname><given-names>KU</given-names></name>, <name><surname>Bota</surname><given-names>M</given-names></name>, <name><surname>Garcia Del Molino</surname><given-names>LC</given-names></name>, <etal>et al</etal><article-title>Brain-wide Maps Reveal Stereotyped Cell-Type-Based Cortical Architecture and Subcortical Sexual Dimorphism</article-title>. <source>Cell</source>. <year>2017</year>;<volume>171</volume>(<issue>2</issue>):<fpage>456</fpage>–<lpage>69 e22</lpage>. Epub 2017/10/07. <pub-id pub-id-type="doi">10.1016/j.cell.2017.09.020</pub-id> .<?supplied-pmid 28985566?><pub-id pub-id-type="pmid">28985566</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref017">
      <label>17</label>
      <mixed-citation publication-type="journal"><name><surname>Kjonigsen</surname><given-names>LJ</given-names></name>, <name><surname>Lillehaug</surname><given-names>S</given-names></name>, <name><surname>Bjaalie</surname><given-names>JG</given-names></name>, <name><surname>Witter</surname><given-names>MP</given-names></name>, <name><surname>Leergaard</surname><given-names>TB</given-names></name>. <article-title>Waxholm Space atlas of the rat brain hippocampal region: three-dimensional delineations based on magnetic resonance and diffusion tensor imaging</article-title>. <source>Neuroimage</source>. <year>2015</year>;<volume>108</volume>:<fpage>441</fpage>–<lpage>9</lpage>. <pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.12.080</pub-id> .<?supplied-pmid 25585022?><pub-id pub-id-type="pmid">25585022</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref018">
      <label>18</label>
      <mixed-citation publication-type="journal"><name><surname>Shiffman</surname><given-names>S</given-names></name>, <name><surname>Basak</surname><given-names>S</given-names></name>, <name><surname>Kozlowski</surname><given-names>C</given-names></name>, <name><surname>Fuji</surname><given-names>RN</given-names></name>. <article-title>An automated mapping method for Nissl-stained mouse brain histologic sections</article-title>. <source>J Neurosci Methods</source>. <year>2018</year>;<volume>308</volume>:<fpage>219</fpage>–<lpage>27</lpage>. Epub 2018/08/11. <pub-id pub-id-type="doi">10.1016/j.jneumeth.2018.08.005</pub-id> .<?supplied-pmid 30096343?><pub-id pub-id-type="pmid">30096343</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref019">
      <label>19</label>
      <mixed-citation publication-type="journal"><name><surname>Bakker</surname><given-names>R</given-names></name>, <name><surname>Tiesinga</surname><given-names>P</given-names></name>, <name><surname>Kotter</surname><given-names>R</given-names></name>. <article-title>The Scalable Brain Atlas: Instant Web-Based Access to Public Brain Atlases and Related Content</article-title>. <source>Neuroinformatics</source>. <year>2015</year>;<volume>13</volume>(<issue>3</issue>):<fpage>353</fpage>–<lpage>66</lpage>. Epub 2015/02/17. <pub-id pub-id-type="doi">10.1007/s12021-014-9258-x</pub-id> .<?supplied-pmid 25682754?><pub-id pub-id-type="pmid">25682754</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref020">
      <label>20</label>
      <mixed-citation publication-type="journal"><name><surname>Bjerke</surname><given-names>IE</given-names></name>, <name><surname>Ovsthus</surname><given-names>M</given-names></name>, <name><surname>Papp</surname><given-names>EA</given-names></name>, <name><surname>Yates</surname><given-names>SC</given-names></name>, <name><surname>Silvestri</surname><given-names>L</given-names></name>, <name><surname>Fiorilli</surname><given-names>J</given-names></name>, <etal>et al</etal><article-title>Data integration through brain atlasing: Human Brain Project tools and strategies</article-title>. <source>Eur Psychiatry</source>. <year>2018</year>;<volume>50</volume>:<fpage>70</fpage>–<lpage>6</lpage>. Epub 2018/03/10. <pub-id pub-id-type="doi">10.1016/j.eurpsy.2018.02.004</pub-id> .<?supplied-pmid 29519589?><pub-id pub-id-type="pmid">29519589</pub-id></mixed-citation>
    </ref>
    <ref id="pone.0216796.ref021">
      <label>21</label>
      <mixed-citation publication-type="journal"><name><surname>Dempsey</surname><given-names>B</given-names></name>, <name><surname>Le</surname><given-names>S</given-names></name>, <name><surname>Turner</surname><given-names>A</given-names></name>, <name><surname>Bokiniec</surname><given-names>P</given-names></name>, <name><surname>Ramadas</surname><given-names>R</given-names></name>, <name><surname>Bjaalie</surname><given-names>JG</given-names></name>, <etal>et al</etal><article-title>Mapping and Analysis of the Connectome of Sympathetic Premotor Neurons in the Rostral Ventrolateral Medulla of the Rat Using a Volumetric Brain Atlas</article-title>. <source>Front Neural Circuits</source>. <year>2017</year>;<volume>11</volume>:<fpage>9</fpage> Epub 2017/03/17. <pub-id pub-id-type="doi">10.3389/fncir.2017.00009</pub-id> .<?supplied-pmid 28298886?><pub-id pub-id-type="pmid">28298886</pub-id></mixed-citation>
    </ref>
  </ref-list>
</back>
