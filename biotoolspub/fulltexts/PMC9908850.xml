<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD with MathML3 v1.2 20190208//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1-mathml3.dtd?>
<?SourceDTD.Version 1.2?>
<?ConverterInfo.XSLTName jats2jats3.xsl?>
<?ConverterInfo.Version 1?>
<?all-math-mml yes?>
<?use-mml?>
<?properties open_access?>
<?properties manuscript?>
<?origin nihpa?>
<?iso-abbr Neuroimage?>
<?submitter-system nihms?>
<?submitter-userid 14367927?>
<?submitter-authority eRA?>
<?submitter-login mingxial?>
<?submitter-name Mingxia Liu?>
<?domain nihpa?>
<processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
  <restricted-by>pmc</restricted-by>
</processing-meta>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-journal-id">9215515</journal-id>
    <journal-id journal-id-type="pubmed-jr-id">20498</journal-id>
    <journal-id journal-id-type="nlm-ta">Neuroimage</journal-id>
    <journal-id journal-id-type="iso-abbrev">Neuroimage</journal-id>
    <journal-title-group>
      <journal-title>NeuroImage</journal-title>
    </journal-title-group>
    <issn pub-type="ppub">1053-8119</issn>
    <issn pub-type="epub">1095-9572</issn>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">9908850</article-id>
    <article-id pub-id-type="pmid">36610676</article-id>
    <article-id pub-id-type="doi">10.1016/j.neuroimage.2023.119863</article-id>
    <article-id pub-id-type="manuscript">nihpa1866657</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>DomainATM: Domain adaptation toolbox for medical data analysis</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <name>
          <surname>Guan</surname>
          <given-names>Hao</given-names>
        </name>
      </contrib>
      <contrib contrib-type="author">
        <name>
          <surname>Liu</surname>
          <given-names>Mingxia</given-names>
        </name>
        <xref rid="CR1" ref-type="corresp">*</xref>
      </contrib>
      <aff id="A1">The Department of Radiology and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, USA</aff>
    </contrib-group>
    <author-notes>
      <corresp id="CR1"><label>*</label>Corresponding author. <email>mxliu@med.unc.edu</email> (M. Liu).</corresp>
    </author-notes>
    <pub-date pub-type="nihms-submitted">
      <day>30</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="ppub">
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="epub">
      <day>05</day>
      <month>1</month>
      <year>2023</year>
    </pub-date>
    <pub-date pub-type="pmc-release">
      <day>01</day>
      <month>3</month>
      <year>2023</year>
    </pub-date>
    <volume>268</volume>
    <fpage>119863</fpage>
    <lpage>119863</lpage>
    <permissions>
      <license>
        <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref>
        <license-p>This is an open access article under the CC BY-NC-ND license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>)</license-p>
      </license>
    </permissions>
    <abstract id="ABS1">
      <p id="P1">Domain adaptation (DA) is an important technique for modern machine learning-based medical data analysis, which aims at reducing distribution differences between different medical datasets. A proper domain adaptation method can significantly enhance the statistical power by pooling data acquired from multiple sites/centers. To this end, we have developed the Domain Adaptation Toolbox for Medical data analysis (DomainATM) ‚Äì an open-source software package designed for fast facilitation and easy customization of domain adaptation methods for medical data analysis. The DomainATM is implemented in MATLAB with a user-friendly graphical interface, and it consists of a collection of popular data adaptation algorithms that have been extensively applied to medical image analysis and computer vision. With DomainATM, researchers are able to facilitate fast feature-level and image-level adaptation, visualization and performance evaluation of different adaptation methods for medical data analysis. More importantly, the DomainATM enables the users to develop and test their own adaptation methods through scripting, greatly enhancing its utility and extensibility. An overview characteristic and usage of DomainATM is presented and illustrated with three example experiments, demonstrating its effectiveness, simplicity, and flexibility. The software, source code, and manual are available online.</p>
    </abstract>
    <kwd-group>
      <kwd>Domain adaptation</kwd>
      <kwd>Medical image analysis</kwd>
      <kwd>Medical image processing toolbox</kwd>
      <kwd>Open source software</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec id="S1">
    <label>1.</label>
    <title>Introduction</title>
    <p id="P2">Medical data analysis is nowadays being boosted by modern statistical analysis tools, <italic toggle="yes">i.e.</italic>, machine learning (<xref rid="R1" ref-type="bibr">Barrag√°n-Montero et al., 2021</xref>; <xref rid="R4" ref-type="bibr">Deo, 2015</xref>; <xref rid="R6" ref-type="bibr">Erickson et al., 2017</xref>; <xref rid="R7" ref-type="bibr">Fatima et al., 2017</xref>; <xref rid="R27" ref-type="bibr">Rajkomar et al., 2019</xref>). Classic machine learning typically assumes that training dataset (source domain) and test dataset (target domain) follow an independent but identical distribution (<xref rid="R36" ref-type="bibr">Valiant, 1984</xref>). In real-world practice, however, this assumption can hardly hold due to the well-known ‚Äúdomain shift‚Äù problem (<xref rid="R16" ref-type="bibr">Kondrateva et al., 2021</xref>; <xref rid="R25" ref-type="bibr">Pooch et al., 2020</xref>; <xref rid="R26" ref-type="bibr">Qui√±onero-Candela et al., 2009</xref>). In medical imaging, domain shift or data heterogeneity is widespread and caused by different scanning parameters (<italic toggle="yes">i.e.</italic>, between-scanner variability) and subject populations in multiple imaging sites. It may increase the test error along with the distribution difference between training and test data (<xref rid="R2" ref-type="bibr">Ben-David et al., 2007</xref>; <xref rid="R34" ref-type="bibr">Torralba and Efros, 2011</xref>). Thus the domain shift/difference may greatly degrade statistical power of multi-site/multi-center studies and hinder the building of effective machine learning models.</p>
    <p id="P3">For handling the domain shift problem among datasets and enhancing the generalization ability of machine learning models, domain adaptation has gradually come under the spotlight of the research community (<xref rid="R3" ref-type="bibr">Csurka, 2017</xref>; <xref rid="R17" ref-type="bibr">Kouw and Loog, 2019</xref>; <xref rid="R24" ref-type="bibr">Patel et al., 2015</xref>; <xref rid="R39" ref-type="bibr">Wang and Deng, 2018</xref>; <xref rid="R41" ref-type="bibr">Wilson and Cook, 2020</xref>; <xref rid="R45" ref-type="bibr">Zhang et al., 2020</xref>; <xref rid="R46" ref-type="bibr">Zou et al., 2020</xref>). In the field of medical data analysis, domain adaptation has gained considerable attention and increasing interest recently (<xref rid="R11" ref-type="bibr">Guan and Liu, 2022</xref>; <xref rid="R37" ref-type="bibr">Valverde et al., 2021</xref>). Briefly, domain adaptation can be defined as follows. Let <inline-formula><mml:math id="M1" display="inline"><mml:mrow><mml:mi>ùí≥</mml:mi><mml:mo>√ó</mml:mo><mml:mi>ùí¥</mml:mi></mml:mrow></mml:math></inline-formula> represent the joint feature space of samples and their corresponding category labels. A source domain <inline-formula><mml:math id="M2" display="inline"><mml:mi>ùíÆ</mml:mi></mml:math></inline-formula> and a target domain <inline-formula><mml:math id="M3" display="inline"><mml:mi>ùíØ</mml:mi></mml:math></inline-formula> are defined on the joint feature space, with different distributions <bold>P</bold><sub><bold>S</bold></sub> and <bold>P</bold><sub><bold>T</bold></sub>, respectively. Suppose there are <italic toggle="yes">n</italic><sub><italic toggle="yes">s</italic></sub> samples (subjects) with or without category labels in the source domain, as well as <italic toggle="yes">n</italic><sub><italic toggle="yes">t</italic></sub> samples in the target domain without category labels. Then the problem is how to reduce the distribution differences/variability between source and target domains so as to increase the performance of down-streaming tasks such as classification or segmentation.</p>
    <p id="P4">Many domain adaptation methods have been proposed or utilized in the field of medical data analysis which shows tremendous applicability. Most solutions, however, are implemented independently for very specific scenarios or target applications. Researchers often need to re-implement an algorithm or do methodological tailoring. The differences in implementation will often cause inconsistent experiment and analysis results. There is a lack of a unified platform for extensive comparison of different domain adaptation methods, helping avoid hand-crafted re-implementation for specific medical data analysis research. Thus a software toolbox that provides a platform of different adaptation methods is quite beneficial and necessary for researchers to compare, evaluate and select the proper method for their research project.</p>
    <p id="P5">An important issue for medical imaging researchers is the fast facilitation of domain adaptation algorithms. Due to privacy protection issues, many real-world medical data sets are not accessible or with restrictions. Using synthetic data which is able to simulate the ‚Äúdomain shift‚Äù phenomenon in a machine learning setting will greatly boost the efficiency. Another limitation is the complexity of certain domain adaptation methods. Time-consuming model training and exhaustive parameter tuning will be rather inconvenient, especially for researchers without high-level programming skills. Thus, fast facilitation of domain adaptation methods with real-time visualization for performance check is beneficial for medical data analysis.</p>
    <p id="P6">We also observe that in medical imaging image-level domain adaptation is an important topic (<xref rid="R11" ref-type="bibr">Guan and Liu, 2022</xref>). For example, MRIs acquired from different scanners may negatively influence the analysis result (<xref rid="R19" ref-type="bibr">Lee et al., 2019</xref>; <xref rid="R42" ref-type="bibr">Wittens et al., 2021</xref>). This has become the concern of many radiologists and neuroscientists. Thus incorporating both feature-level and image-level adaptation methods into one platform is beneficial for related medical imaging research.</p>
    <p id="P7">In light of these motivations, we develop the Domain Adaptation Toolbox for Medical data analysis (DomainATM) ‚Äì a software package that offers a platform for simulating, evaluating and developing different domain adaptation algorithms for medical data analysis. The toolbox is designed with a major principle that it could help researchers do fast facilitation of adaptation methods. Besides real-world medical data, synthetic data with user-defined statistical properties can be generated quickly for real-time simulation. Both feature-level and image-level domain adaptation algorithms are included in the software package with a graphical-user-interface (GUI). The running results will be automatically saved which can be further analyzed by the evaluation module of the toolbox. All the algorithms have consistent input/output formats under which the users can define their own adaptation algorithms and add them to the DomainATM freely. Thus the toolbox has good flexibility and scalability.</p>
    <p id="P8">This paper is organized as follows. In <xref rid="S2" ref-type="sec">Section 2</xref>, we introduce the characteristics of DomainATM, including its overall structure, key features and functions. In <xref rid="S3" ref-type="sec">Section 3</xref>, the workflow of DomainATM for the facilitation of domain adaptation is described. In <xref rid="S9" ref-type="sec">Section 4</xref>, representative domain adaptation methods that have been included in the toolbox are presented. In <xref rid="S25" ref-type="sec">Sections 5</xref> and <xref rid="S40" ref-type="sec">6</xref>, experiments for both feature-level and image-level adaptation are conducted to illustrate the application of the toolbox. This paper is concluded in <xref rid="S45" ref-type="sec">Section 7</xref>.</p>
  </sec>
  <sec id="S2">
    <label>2.</label>
    <title>Toolbox overview/characteristics</title>
    <p id="P9">The main structure of the DomainATM is illustrated in <xref rid="F2" ref-type="fig">Fig. 2</xref>. Currently, the toolbox consists of three modules. 1) The <bold>data module</bold> is responsible for loading and generating datasets. It can directly load an existing medical dataset (in .<italic toggle="yes">mat</italic> data file) or create synthetic datasets with user-defined statistical properties that can simulate domain shift. A dataset is in the format of <italic toggle="yes">M</italic> √ó <italic toggle="yes">N</italic> matrix, where <italic toggle="yes">M</italic> denotes the number of samples while <italic toggle="yes">N</italic> represents the feature dimension. 2) The <bold>algorithm module</bold> contains the implementations of different domain adaptation methods. All these adaptation algorithms have uniform input/output parameter formats. Users can easily add their self-defined algorithms into the toolbox with the same input/output format. By default, several representative methods which have been widely used in medical data analysis are included in the DomainATM. These methods can be categorized into <italic toggle="yes">feature-level</italic> adaptation methods and <italic toggle="yes">image-level</italic> adaptation methods. Besides, inspired by the design philosophy of fast facilitation, most of the algorithms included in the toolbox can run in real time and output results in seconds. 3) The <bold>evaluation module</bold> assesses the performance of different adaptation methods. For feature-level adaptation methods, we employ two evaluation metrics, including: domain-level classification accuracy and domain distribution distance. For image-level adaptation methods, we use three evaluation metrics, including correlation coefficient (CC), peak signal-noise ratio (PSNR) and mean square error (MSE). The DomainATM provides visualization functions to visualize the data distribution (or images) before and after adaptation which helps investigate and understand the performance of different domain adaptation algorithms.</p>
    <p id="P10">The DomainATM is implemented in MATLAB (originally implemented in MATLAB 2021b on Windows 10, MATLAB 2019 or more advanced versions are all good for it). Through test, DomainATM can be run on Windows, Mac OS and Linux systems. It can be easily used with a graphical-user-interface (GUI), as shown in <xref rid="F3" ref-type="fig">Fig. 3</xref>. The hardware platform can be a CPU-based PC (originally developed on Intel i-7 PC with 16 GB memory), which does not require much computation or memory resources. For advanced users, DomainATM provides an interface for writing MATLAB scripts to implement self-defined domain adaptation methods. The software, manual and source code for DomainATM are accessible online<sup><xref rid="FN4" ref-type="fn">1</xref></sup>.</p>
  </sec>
  <sec id="S3">
    <label>3.</label>
    <title>Toolbox workflow</title>
    <sec id="S4">
      <label>3.1.</label>
      <title>Creating/loading data</title>
      <p id="P11">The DomainATM can work for both feature-level adaptation and image-level adaptation. These two key modules in the toolbox are independent of each other. With respect to the input of feature-level adaptation, the toolbox accepts data in standard MATLAB .<italic toggle="yes">mat</italic> file format. Each row represents an observation (subject or sample) while every column represents a feature. Existing real-world medical datasets (in .<italic toggle="yes">mat</italic> format) can be directly imported and loaded into the toolbox for processing. In addition, the users can create a synthetic dataset. After assigning the sample number, mean value and covariance matrix, the toolbox can automatically generate a synthetic dataset following a normal distribution. After loading the real/synthetic data, their distribution will be automatically displayed in the toolbox. Both the real-world and created datasets are stored in the ‚Äúdata‚Äù subfolder of the toolbox.</p>
      <p id="P12">For image-level adaptation, the toolbox currently accepts 3D volumetric data (in .<italic toggle="yes">nii</italic> format). All the data will be converted to inner-built data in MATLAB. After loading the volumetric data, a middle slice (in axial view) will be automatically shown. Note that the ‚ÄúCreate Dataset‚Äù module currently only generates data for feature-level domain adaptation.</p>
    </sec>
    <sec id="S5">
      <label>3.2.</label>
      <title>Selecting domain adaptation algorithms</title>
      <p id="P13">After loading the data, the following procedure is to select, configure, and run the domain adaptation methods. Most adaptation methods have several hyper-parameters to be set. Users can tune them according to the specific tasks. Otherwise, default settings of these methods will be used. After configuration, the users can run the algorithms. All the built-in methods provided by the toolbox are simple, easy to use, and can run in real time within 5 seconds (on a PC with an Intel i-7 CPU, 16 GB memory).</p>
      <p id="P14">After running the adaptation methods, the results will be automatically saved in the ‚Äúevaluation‚Äù subfolder of the toolbox. For feature-level adaptation, the original source/target data, and the adapted source/target data will be saved (in .<italic toggle="yes">mat</italic> data format). For image-level adaptation, the adapted source images (target image is used as the reference image and will not be changed) will be saved (in .<italic toggle="yes">nii</italic> format). All the files are named with the corresponding adaptation method with time stamp.</p>
    </sec>
    <sec id="S6">
      <label>3.3.</label>
      <title>Evaluating data adaptation performance</title>
      <p id="P15">After running the adaptation methods and getting the results, performance evaluation can be conducted for the methods. For feature-level adaptation, we use <italic toggle="yes">distribution difference</italic> and <italic toggle="yes">domain-level classification accuracy</italic> as two metrics to assess the adaptation performance. For image-level adaptation, we adopt <italic toggle="yes">correlation coefficient (CC), peak signal-to-noise ratio (PSNR)</italic> and <italic toggle="yes">mean-square error (MSE)</italic> to evaluate the adaptation result. More details about these evaluation metrics will be elaborated in the experiment section.</p>
    </sec>
    <sec id="S7">
      <label>3.4.</label>
      <title>Visualization of data adaptation results</title>
      <p id="P16">Besides quantitative evaluation, result visualization is useful for qualitative analysis. The DomainATM provides visualization functions that help users better understand domain adaptation for medical data. For feature-level adaption, the feature distribution (in 2D space) before and after adaptation can be visualized. High-dimensional features will be mapped to 2D feature space via t-SNE (<xref rid="R22" ref-type="bibr">Van der Maaten and Hinton, 2008</xref>). For image-level adaptation, the adapted source image, the original source and target images can be viewed using the toolbox. After the adapted images have been saved in the ‚Äúevaluation‚Äù subfolder, they can also be visually inspected by other medical imaging software.</p>
    </sec>
    <sec id="S8">
      <label>3.5.</label>
      <title>Extension: Adding self-defined data adaptation algorithm</title>
      <p id="P17">In some tasks of medical data analysis, users might need to develop their own domain adaptation methods. The DomainATM supports self-defined algorithms for task-specific usage. The users can write a MATLAB script to define and implement their algorithms. The input/output format of the self-defined functions has to be consistent with other built-in adaptation methods. When adding an new algorithm, the self-defined script should be put in the ‚Äúalgorithms_feat‚Äù (feature-level) or the ‚Äúalgorithms_img‚Äù (image-level) subfolders in the toolbox. One can simply run and analyze their methods like the other built-in ones through GUI.</p>
    </sec>
  </sec>
  <sec id="S9">
    <label>4.</label>
    <title>Algorithms</title>
    <p id="P18">In this section, we briefly introduce the algorithms for feature-level and image-level data adaptation in DomainATM. More details can be found in the online manual.</p>
    <sec id="S10">
      <label>4.1.</label>
      <title>Feature-level data adaptation algorithm</title>
      <sec id="S11">
        <label>4.1.1.</label>
        <title>Baseline</title>
        <p id="P19">No feature-level domain adaptation is utilized. Both source and target data are kept in their original distributions (in the feature space).</p>
      </sec>
      <sec id="S12">
        <label>4.1.2.</label>
        <title>Subspace Alignment (SA)</title>
        <p id="P20">In this algorithm (<xref rid="R8" ref-type="bibr">Fernando et al., 2013</xref>), the source and target medical data are represented by subspaces in terms of eigenvectors. The source data are projected to the target domain through a transformation matrix. No category labels of source domain are needed. The key hyper-parameter is the dimension of the shared subspace.</p>
      </sec>
      <sec id="S13">
        <label>4.1.3.</label>
        <title>Correlation Alignment (CORAL)</title>
        <p id="P21">In this algorithm (<xref rid="R32" ref-type="bibr">Sun et al., 2016</xref>), domain shift/difference is minimized by aligning the second-order statistics (<italic toggle="yes">e.g.</italic>, covariance) of source and target distributions. No category label information and hyper-parameters are required for this method.</p>
      </sec>
      <sec id="S14">
        <label>4.1.4.</label>
        <title>Transfer Component Analysis (TCA)</title>
        <p id="P22">In this algorithm (<xref rid="R23" ref-type="bibr">Pan et al., 2010</xref>), a subspace shared by the source and target domain is searched in a reproducing kernel Hilbert space by minimizing the maximum mean discrepancy (MMD) distance. No source category labels are demanded. The key hyper-parameters are the kernel type and subspace dimension.</p>
      </sec>
      <sec id="S15">
        <label>4.1.5.</label>
        <title>Optimal Transport (OT)</title>
        <p id="P23">In this algorithm (Guan et al., 2021b), the samples in the source domain are projected into the target domain while keeping their conditional distributions. The projection is facilitated through minimization of Wasserstein distance between the two distributions. No category labels of the source domain are used. The key hyper-parameter is the regularization coefficient.</p>
      </sec>
      <sec id="S16">
        <label>4.1.6.</label>
        <title>Joint Distribution Adaptation (JDA)</title>
        <p id="P24">In this algorithm (<xref rid="R20" ref-type="bibr">Long et al., 2013</xref>), maximum mean discrepancy (MMD) is adopted to measure domain distribution differences, and is integrated into Principal Component Analysis (PCA) to build a representation that is robust to domain shift. Source category labels are needed in this algorithm. The key hyper-parameters include kernel type, subspace dimension and regularization parameter.</p>
      </sec>
      <sec id="S17">
        <label>4.1.7.</label>
        <title>Transfer Joint Matching (TJM)</title>
        <p id="P25">In this algorithm (<xref rid="R21" ref-type="bibr">Long et al., 2014</xref>), feature matching and instance reweighting strategies are combined to reduce domain shift. Minimization of maximum mean discrepancy (MMD) and <italic toggle="yes">l</italic><sub>2,1</sub> norm sparsity penalty on source data are integrated into PCA to construct domain-invariant features. Category labels of source domain are required. The key hyper-parameters include kernel type, subspace dimension and regularization parameter.</p>
      </sec>
      <sec id="S18">
        <label>4.1.8.</label>
        <title>Geodesic Flow Kernel (GFK)</title>
        <p id="P26">In this algorithm (<xref rid="R10" ref-type="bibr">Gong et al., 2012</xref>), the source and target data are embedded into the Grassmann manifolds, and the geodesic flows between them are used to model domain shift. Domain adaptation is conducted by projecting the data into several domain-invariant subspaces on the geodesic flow. Source category labels can be either used or not. The key hyper-parameter is the subspace dimension.</p>
      </sec>
      <sec id="S19">
        <label>4.1.9.</label>
        <title>Scatter Component Analysis (SCA)</title>
        <p id="P27">In this algorithm (<xref rid="R9" ref-type="bibr">Ghifary et al., 2016</xref>), original features are firstly projected to a reproducing kernel Hilbert space. Domain adaptation is then conducted through an optimization formulation, including maximizing the class separability, maximizing the data separability, and minimizing domain mismatch. Category labels of the source domain are used during adaptation. The key parameter is the dimension of the transformed space.</p>
      </sec>
      <sec id="S20">
        <label>4.1.10.</label>
        <title>Information-Theoretical Learning (ITL)</title>
        <p id="P28">In this algorithm (<xref rid="R29" ref-type="bibr">Shi and Sha, 2012</xref>), an optimal feature space is learned through jointly maximizing domain similarity and minimizing the expected classification error on target samples. Source category labels are required. The key hyper-parameters include subspace dimension and regularization parameter.</p>
      </sec>
    </sec>
    <sec id="S21">
      <label>4.2.</label>
      <title>Image-level data adaptation algorithm</title>
      <sec id="S22">
        <label>4.2.1.</label>
        <title>Baseline</title>
        <p id="P29">For two medical images acquired by different scanners/sites, no domain adaptation is facilitated in this method. Instead, the homogeneity/heterogeneity of the paired original images is directly compared in terms of certain evaluation metrics.</p>
      </sec>
      <sec id="S23">
        <label>4.2.2.</label>
        <title>Histogram Matching (HM)</title>
        <p id="P30">This method transforms source image to make its histogram matches the histogram of the target image (<xref rid="R30" ref-type="bibr">Shinohara et al., 2014</xref>). After adaptation, the intensity distributions of the source and target images become closer.</p>
      </sec>
      <sec id="S24">
        <label>4.2.3.</label>
        <title>Spectrum Swapping-based Image-level MRI Harmonization (SSIMH)</title>
        <p id="P31">In this method (<xref rid="R12" ref-type="bibr">Guan et al., 2022</xref>), the source and target images are firstly transformed into the frequency domain (<italic toggle="yes">e.g.</italic>, through Discrete Cosine Transform). Then, part of the low-frequency region of source image is replaced by the corresponding low-frequency area of the target image. Finally, the source image in the revised frequency domain is inverted back to the spatial domain to get the adapted image. The key hyper-parameter of this method is the threshold which defines the low-frequency region that is swapped between source and target images. In the toolbox, the default value is set to 3.</p>
        <p id="P32">The image-level domain adaptation methods work well in two different settings. (1) One-to-one image harmonization: Given a source image and a target/reference image, one can select a specific algorithm to adapt the source image to the target image space. (2) Batch image harmonization: Given multiple source images and a target image, we can adapt all source images to target image space via batch harmonization.</p>
      </sec>
    </sec>
  </sec>
  <sec id="S25">
    <label>5.</label>
    <title>Empirical evaluation of feature-level data adaptation algorithms in DomainATM</title>
    <sec id="S26">
      <label>5.1.</label>
      <title>Evaluation metric</title>
      <p id="P33">For feature-level adaptation methods, we adopt the metrics that evaluate the distribution changes before and after the adaptation process. Specifically, we use the following three methods/metrics for adaptation performance evaluation.</p>
      <list list-type="bullet" id="L1">
        <list-item>
          <p id="P34"><bold>Distribution difference.</bold> We adopt maximum mean discrepancy (MMD) to measure the data distribution differences between the source and target domains before and after domain adaptation. As a popular metric, the maximum mean discrepancy (MMD) has been widely used in domain adaptation research (<xref rid="R18" ref-type="bibr">Kumagai and Iwata, 2019</xref>; <xref rid="R20" ref-type="bibr">Long et al., 2013</xref>; <xref rid="R21" ref-type="bibr">2014</xref>; <xref rid="R23" ref-type="bibr">Pan et al., 2010</xref>; <xref rid="R40" ref-type="bibr">Wang et al., 2021</xref>; <xref rid="R43" ref-type="bibr">Yan et al., 2017</xref>), defined as follows:
<disp-formula id="FD1"><label>(1)</label><mml:math id="M4" display="block"><mml:mrow><mml:mi mathvariant="bold">M</mml:mi><mml:mi mathvariant="bold">M</mml:mi><mml:msubsup><mml:mi mathvariant="bold">D</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mo>‚Äñ</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>œï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>‚àí</mml:mo><mml:msub><mml:mi mathvariant="bold">E</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>œï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>‚Äñ</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>‚Ñã</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="M5" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñã</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the Reproducing Kernel Hilbert Space endowed with a kernel function <italic toggle="yes">k</italic>, and <inline-formula><mml:math id="M6" display="inline"><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>s</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>‚å©</mml:mo><mml:mrow><mml:mi>œï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>s</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>œï</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold">x</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>‚å™</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. If the MMD distance of source and target domains gets lower after adaptation, it indicates the data distribution difference becomes smaller.</p>
        </list-item>
        <list-item>
          <p id="P35"><bold>Domain classification.</bold> Suppose an equal number of samples are sampled from the source and target domains, respectively. These samples are assigned with <italic toggle="yes">domain labels, i.e.</italic>, the source samples are labeled as ‚Äú1‚Äù while target samples are assigned with the label ‚Äú0‚Äù. A <italic toggle="yes">domain discriminator/classifier</italic> is applied to all samples for distinguishing which samples come from the source domain and which ones are from the target domain. The classification result is used to assess domain shift/difference. A high domain classification accuracy indicates that the source and target samples can be easily distinguished, which means the domain shift is large. In contrast, if the domain classification accuracy drops down after the adaptation processing, it indicates the domain adaptation algorithm works because it makes the two domains get closer and become more difficult to distinguish.</p>
        </list-item>
      </list>
    </sec>
    <sec id="S27">
      <label>5.2.</label>
      <title>Experiment 1: Adaptation on synthetic dataset</title>
      <p id="P36">We first conduct experiments on synthetic datasets using DomainATM. Users can set the statistical properties of the synthetic data freely using DomainATM, and thus, can conduct fast test of different domain adaptation methods, which is helpful for understanding the characteristics of different methods and avoiding the access restrictions of many real-world medical datasets. Specifically, we generate two domains by Gaussian distributions. Each domain has two classes, with 30 positive samples and 30 negative ones, respectively. For the source domain <inline-formula><mml:math id="M7" display="inline"><mml:mi>ùíÆ</mml:mi></mml:math></inline-formula>, the means of positive and negative samples are [0, 0] and [0, 1], while their covariance matrices are [0.2, 0; 0, 0.2] and [0.1, 0; 0, 0.1]. For the target domain <inline-formula><mml:math id="M8" display="inline"><mml:mi>ùíØ</mml:mi></mml:math></inline-formula>, the means of positive and negative samples are [1, ‚àí0.5] and [1, 0.2], while their covariance matrices are [0.2, 0; 0, 0.2] and [0.1, 0; 0, 0.1].</p>
      <sec id="S28">
        <label>5.2.1.</label>
        <title>Data distribution visualization</title>
        <p id="P37">The distributions of the original data and the adapted data by different methods are visualized in <xref rid="F4" ref-type="fig">Fig. 4</xref>. From the visualization result, different domain adaptation methods can reduce the distributions of source and target samples to certain extent. For example, the optimal transport adaptation (OT) can project the source data into the target domain, and make the source distribution quite similar to the target domain.</p>
      </sec>
      <sec id="S29">
        <label>5.2.2.</label>
        <title>Distribution difference</title>
        <p id="P38">The data distribution differences (in terms of maximum mean discrepancy) of the source and target domains after domain adaptation are shown in <xref rid="F5" ref-type="fig">Fig. 5</xref>. The result of the Baseline method shows the original distribution of the source and target domain without any adaptation processing. From <xref rid="F5" ref-type="fig">Fig. 5</xref>, we can observe that domain adaptation can reduce the distribution differences between the original source and target domains.</p>
      </sec>
      <sec id="S30">
        <label>5.2.3.</label>
        <title>Domain-level classification</title>
        <p id="P39">We conduct domain-level classification on the source and target data. A domain classifier (we use a k-nearest neighbors classifier) is trained with source data (with the label ‚Äú1‚Äù) and target data (with the label ‚Äú0‚Äù). Source and target data are combined together and shuffled. In the experiments, we use 60% of the entire data samples for training the domain classifier while 40% are for test. The result of domain classification accuracy is shown in <xref rid="F6" ref-type="fig">Fig. 6</xref>.</p>
        <p id="P40">We also use another two classifiers, <italic toggle="yes">i.e.</italic>, support vector machine (SVM) and random forest (RF) for domain-level classification. For the SVM, we use a linear kernel and the penalty parameter C is set to 1. For the RF, 50 decision trees are used for the ensemble classification. These settings are also used for the other experiments. Their domain-level classification results are shown in <xref rid="T1" ref-type="table">Table 1</xref>.</p>
        <p id="P41">From <xref rid="F6" ref-type="fig">Fig. 6</xref> and <xref rid="T1" ref-type="table">Table 1</xref>, it can be seen that the domain classification accuracy drops after domain adaptation even different classifiers are used. This implies that source and target data become more difficult to be distinguished, <italic toggle="yes">i.e.</italic>, domain adaptation makes their distributions become more similar than in the original space.</p>
      </sec>
    </sec>
    <sec id="S31">
      <label>5.3.</label>
      <title>Experiment 2: Adaptation for Alzheimer‚Äôs disease analysis on ADNI</title>
      <p id="P42">We conduct experiments on the Alzheimer‚Äôs Disease Neuroimaging Initiative (ADNI) dataset (<xref rid="R15" ref-type="bibr">Jack Jr et al., 2008</xref>). The dataset consists of T1-weighted MRI data for Alzheimer‚Äôs disease (AD) analysis. We use two subsets of ADNI, <italic toggle="yes">i.e.</italic>, ADNI-1 (100 subjects with 1.5T T1-weighted structural MRIs) and ADNI-2 (100 subjects with 3.0T T1-weighted structural MRIs) as the source and target domains, respectively, to test the domain adaptation algorithms using DomainATM. ADNI-1 contains 50 patients with Alzheimer‚Äôs disease (AD) (positive samples) and 50 normal control (NC) subjects (negative samples). ADNI-2 has 50 CE subjects and 50 NC subjects. All the MRIs have been processed through a standard pipeline, including skull stripping, intensity correction, registration and re-sampling. Regions-of-interest (ROIs) features which are defined on <bold>90</bold> regions in the Anatomical Automatic Labeling (AAL) atlas (<xref rid="R35" ref-type="bibr">Tzourio-Mazoyer et al., 2002</xref>) are used to represent each subject. The 90-dimensional features denote the gray matter volumes in each brain region.</p>
      <sec id="S32">
        <label>5.3.1.</label>
        <title>Distribution visualization</title>
        <p id="P43">The distributions of original ADNI-1 and ADNI-2 data (in feature space) and the adapted data by different methods are visualized in <xref rid="F7" ref-type="fig">Fig 7</xref>. From the visualization results, the original source and target data have a relatively clear boundary. After domain adaptation, the domain boundaries become blurred, and the distribution of source and target domains gets closer to each other.</p>
      </sec>
      <sec id="S33">
        <label>5.3.2.</label>
        <title>Distribution distance</title>
        <p id="P44">The distribution differences (in terms of maximum mean discrepancy) of the source data, <italic toggle="yes">i.e.</italic>, ADNI-1, and target data, <italic toggle="yes">i.e.</italic>, ADNI-2, after domain adaptation are shown in <xref rid="F8" ref-type="fig">Fig. 8</xref>. The baseline illustrates the original distribution of the source and target domain without any adaptation processing. From the result, it can be observed that domain adaptation is able to reduce the distribution differences between the original source and target domains.</p>
      </sec>
      <sec id="S34">
        <label>5.3.3.</label>
        <title>Domain-level classification</title>
        <p id="P45">We facilitate domain-level classification on the source data, <italic toggle="yes">i.e.</italic>, ADNI-1, and target data, <italic toggle="yes">i.e.</italic>, ADNI-2. A domain classifier (k-nearest neighbors classifier) is trained with source data (with the label ‚Äú1‚Äù) and target data (with the label ‚Äú0‚Äù). Source and target data are combined together and shuffled. 60% of the entire data are adopted for training while 40% for testing. The result of domain-level classification is illustrated in <xref rid="F9" ref-type="fig">Fig. 9</xref>. Another two classifiers, including support vector machine (SVM) and random forest (RF) are also adopted for domain-level classification, and the result is listed in <xref rid="T2" ref-type="table">Table 2</xref>. From <xref rid="F9" ref-type="fig">Fig. 9</xref> and <xref rid="T2" ref-type="table">Table 2</xref>, we can see that the domain classification accuracy drops after domain adaptation despite the different types of domain classifiers. This indicates that the adapted source and target data get more difficult to be correctly classified, <italic toggle="yes">i.e.</italic>, domain adaptation is effective in reducing their distribution differences.</p>
      </sec>
    </sec>
    <sec id="S35">
      <label>5.4.</label>
      <title>Experiment 3: Domain adaptation for autism analysis on ABIDE</title>
      <p id="P46">We conduct experiments on the Autism Brain Imaging Data Exchange (ABIDE) dataset (<xref rid="R5" ref-type="bibr">Di Martino et al., 2014</xref>). This database consists of resting-state functional MRI (fMRI) data for Autism analysis. We use two sites from the ABIDE project, <italic toggle="yes">i.e.</italic>, NYU (184 subjects) and UM (145 subjects) as the source and target domains, respectively, to test the domain adaptation algorithms using the DomainATM. The NYU site consists of 79 positive samples (autism patients) and 105 negative samples (normal controls). These fMRIs are acquired by a 3 Tesla Allegra scanner. The UM site includes 68 positive samples (autism patients) and 77 negative samples (normal controls). These fMRIs are acquired using a 3 Tesla GE scanner located at the UM Functional MRI Laboratory. All the fMRIs go through a standard pipeline, including slice-timing and motion correction, nuisance signal regression, temporal filtering, and registration. The mean time series of 116 regions-of-interest (ROIs) defined by the Anatomical Automatic Labeling (AAL) atlas (<xref rid="R35" ref-type="bibr">Tzourio-Mazoyer et al., 2002</xref>) are extracted. Then, a 116√ó116 symmetrical resting-state functional connectivity (FC) matrix is generated for each subject, with each element representing the Pearson correlation coefficient between a pair of ROI signals. We extract the node betweenness centrality (<xref rid="R28" ref-type="bibr">Rubinov and Sporns, 2010</xref>) based on the FC matrix to represent each subject/sample.</p>
      <sec id="S36">
        <label>5.4.1.</label>
        <title>Distribution visualization</title>
        <p id="P47">The original distributions of two sites in ABIDE (in feature space) and the adapted data by different methods are visualized in <xref rid="F10" ref-type="fig">Fig. 10</xref>. From the visualization result, it can be observed that the boundary between original source and target data is relatively clear. After the domain adaptation processing, the domain boundaries become blurred, and the distributions of source and target domain get similar to each other.</p>
      </sec>
      <sec id="S37">
        <label>5.4.2.</label>
        <title>Distribution distance</title>
        <p id="P48">The data distribution differences (in terms of MMD) of the source NYU domain and target UM domain after domain adaptation are shown in <xref rid="F11" ref-type="fig">Fig. 11</xref>. The baseline is the original distribution of the source and target domain without any adaptation processing. The result shows that the distribution differences become smaller after adaptation processing by different algorithms.</p>
      </sec>
      <sec id="S38">
        <label>5.4.3.</label>
        <title>Domain-level classification</title>
        <p id="P49">We facilitate domain-level classification on the source data, <italic toggle="yes">i.e.</italic>, NYU, and target data, <italic toggle="yes">i.e.</italic>, UM. A domain classifier (k-nearest neighbors classifier) is trained with source data (with the label ‚Äú1‚Äù) and target data (with the label ‚Äú0‚Äù). Source and target data are combined together and shuffled. 60% of the entire data are adopted for training while 40% for test. The result of domain-level classification accuracy is illustrated in <xref rid="F12" ref-type="fig">Fig. 12</xref>. We also use support vector machine (SVM) and random forest (RF) to conduct the domain-level classification, and the result is shown in <xref rid="T3" ref-type="table">Table 3</xref> From the results, the domain classification accuracy gets worse after domain adaptation processing regardless of what domain classifiers have been used. This indicates that the adapted source and target data become more difficult to be discriminated, <italic toggle="yes">i.e.</italic>, using domain adaptation has successfully reduced their distribution differences.</p>
      </sec>
    </sec>
    <sec id="S39">
      <label>5.5.</label>
      <title>Discussion</title>
      <p id="P50">In the above experiments, we use two quantitative metrics, <italic toggle="yes">i.e.</italic>, MMD and domain classification accuracy, to evaluate the performance of different domain adaptation methods in DomainATM. The MMD is a direct assessment metric because it is directly calculated based on the statistical properties of source and target domains (datasets). Generally, if method A achieves a smaller MMD than method B, then A is supposed to be better. Domain classification accuracy is an indirect metric because it relies on a specific domain classifier. But it can also reflect the adaptation performance since confusing a classifier is difficult. If method A achieves a smaller domain classification accuracy than method B, then A is supposed to be better. Based on the experimental results, we have the following empirical findings.</p>
      <list list-type="bullet" id="L2">
        <list-item>
          <p id="P51">The CORAL, TCA and SCA algorithms have relatively worse domain adaptation performance than the other methods. They get significantly higher MMD values and domain classification accuracy than the others.</p>
        </list-item>
        <list-item>
          <p id="P52">The OT algorithm achieves the overall best performance among these adaptation methods. It generally produces the smallest MMD value and domain classification accuracy in all these three experiments.</p>
        </list-item>
        <list-item>
          <p id="P53">On the ADNI dataset, the TJM, JDA, GFK and ITL have comparable performance. They get similar domain classification accuracy and low MMD. On the ABIDE dataset, the algorithm ITL is worse than the others.</p>
        </list-item>
        <list-item>
          <p id="P54">Most algorithms are effective in significantly reducing the MMD value. By contrast, the domain classification accuracy is more difficult to reduce. This implies that it is challenging to confuse or deceive a domain classifier with certain domain adaptation methods. Thus, domain classification accuracy is a rigorous metric to assess the robustness of an adaptation algorithm.</p>
        </list-item>
      </list>
      <p id="P55">We also conduct statistical testing for performance comparison in terms of domain classification accuracy. Specifically, we compute the <italic toggle="yes">p</italic>-values via paired sample <italic toggle="yes">t</italic>-test between each adaptation method and the baseline. The <italic toggle="yes">p</italic>-values are smaller than 0.05, indicating that their differences are significant. In addition, we calculate the running time of each domain adaptation algorithm for each dataset on a PC with an Intel i-7 CPU and 16 GB memory. The comparison result is listed in <xref rid="T4" ref-type="table">Table 4</xref>.</p>
    </sec>
  </sec>
  <sec id="S40">
    <label>6.</label>
    <title>Empirical evaluation of image-level data adaptation algorithms in DomainATM</title>
    <sec id="S41">
      <label>6.1.</label>
      <title>Evaluation metrics</title>
      <p id="P56">For image-level adaptation methods, we adopt the metrics that evaluate the image similarity/dissimilarity before and after adaptation. Specifically, we adopt the following three metrics for image-level adaptation performance evaluation.</p>
      <list list-type="bullet" id="L8">
        <list-item>
          <p id="P57"><bold>Correlation Coefficient (CC).</bold> Denote the source and target images as <inline-formula><mml:math id="M9" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M10" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. After adaptation, we get <inline-formula><mml:math id="M11" display="inline"><mml:mrow><mml:msubsup><mml:mi>‚Ñê</mml:mi><mml:mi>s</mml:mi><mml:mo>‚Ä≤</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>. For performance assessment, if the correlation coefficient of <inline-formula><mml:math id="M12" display="inline"><mml:mrow><mml:msubsup><mml:mi>‚Ñê</mml:mi><mml:mi>s</mml:mi><mml:mo>‚Ä≤</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M13" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is higher than <inline-formula><mml:math id="M14" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M15" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, it indicates the corresponding adaptation algorithm works.</p>
        </list-item>
        <list-item>
          <p id="P58"><bold>Peak Signal-to-Noise Ratio (PSNR).</bold> If the peak signal-to-noise ratio of <inline-formula><mml:math id="M16" display="inline"><mml:mrow><mml:msubsup><mml:mi>‚Ñê</mml:mi><mml:mi>s</mml:mi><mml:mo>‚Ä≤</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M17" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is higher than <inline-formula><mml:math id="M18" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M19" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, it indicates the adaptation algorithm works.</p>
        </list-item>
        <list-item>
          <p id="P59"><bold>Mean-Squared Error (MSE).</bold> If the mean-squared error of <inline-formula><mml:math id="M20" display="inline"><mml:mrow><mml:msubsup><mml:mi>‚Ñê</mml:mi><mml:mi>s</mml:mi><mml:mo>‚Ä≤</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M21" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is smaller than <inline-formula><mml:math id="M22" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M23" display="inline"><mml:mrow><mml:msub><mml:mi>‚Ñê</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, it indicates the adaptation algorithms are effective.</p>
        </list-item>
      </list>
    </sec>
    <sec id="S42">
      <label>6.2.</label>
      <title>Materials and settings</title>
      <p id="P60">Phantom data of five traveling subjects with T1-weighted (T1-w) structural MRIs from the ABCD dataset (<xref rid="R38" ref-type="bibr">Volkow et al., 2018</xref>) are used for performance evaluation. Phantom-1 is scanned by GE and Philips scanners, respectively. Phantom-2 and Phantom-3 are acquired by Siemens and GE scanners, respectively. Phantom-4 and Phantom-5 are scanned by Philips and Siemens scanners, respectively. The protocols of the GE, Philips and Siemens scanners are consistent. These phantoms are used to test the performance of image-level domain adaptation methods in handling domain shift caused by different scanners. All these 3D MRIs are raw data in the <italic toggle="yes">NIfTI</italic> file format. We do not perform any pre-processing such as skull-stripping, registration or segmentation before image-level adaptation. During adaptation, the intensity of each image is normalized to the range of [0, 1]. For these volumetric images which contain multiple slices, the adaptation is facilitated on each slice, then the performance is calculated as an average metric value for all the slices within an image (volume).</p>
    </sec>
    <sec id="S43">
      <label>6.3.</label>
      <title>Result</title>
      <p id="P61">We conduct image-level domain adaptation on these five phantom structural MRI data, and the adaptation results in terms of the three metrics are shown in <xref rid="T5" ref-type="table">Table 5</xref>. From the result, it can be observed that image-level domain adaptation methods can generally achieve higher scores of correlation coefficient (CC) and peak signal-to-noise ratio (PSNR) and smaller mean square error (MSE). In some cases (<italic toggle="yes">e.g.</italic>, GE ‚Üí Philips), the Histogram Matching (HM) does not perform very well in terms of PSNR and MSE. Overall, the result indicates that image-level adaptation methods are useful in reducing the distribution shift between images caused by different scanners.</p>
    </sec>
    <sec id="S44">
      <label>6.4.</label>
      <title>Visual inspection</title>
      <p id="P62">To further investigate the effectiveness of image-level domain adaptation, we do visual inspections of the MRIs that are adapted to different scanner styles. We divide the phantom MRIs into three groups in terms of the scanners. Then we adapt MRIs acquired by one scanner to the styles of MRIs scanned by other scanners. We use the SSIMH method (<xref rid="R12" ref-type="bibr">Guan et al., 2022</xref>) in DomainATM to perform image-level adaptation. <xref rid="F13" ref-type="fig">Fig. 13</xref> shows the results of three different MRIs and their corresponding adapted images to different scanner styles. From the result, we have the following two observations. 1) Different scanners (<italic toggle="yes">i.e.</italic>, Siemens, Philips and GE) have a significant impact on the MRIs, which can cause the domain shift. 2) The image-level domain adaptation method is effective in harmonizing the source image to the target image (reference image) and reducing the domain shift caused by different scanners.</p>
    </sec>
  </sec>
  <sec id="S45">
    <label>7.</label>
    <title>Conclusion and future work</title>
    <p id="P63">Domain adaptation has become an important topic in the field of medical data analysis. In this paper, we develop a Domain Adaptation Toolbox for Medical data analysis (DomainATM), aiming to help researchers facilitate fast domain adaptation for medical data acquired from different sites/scanners. The DomainATM is easy to use, efficient to run, and most importantly, it is able to do both feature-level and image-level adaptation. In addition, users can add their own domain adaptation algorithms into the toolbox, making it flexible and extensible. Experiments on both synthetic and real-world medical datasets have been conducted to show the usage and effectiveness of DomainATM. We hope the toolbox can provide more convenience and benefit for researchers to do domain adaptation research in medical data analysis.</p>
    <p id="P64">There are several potential future works to further enrich and extend the DomainATM. <italic toggle="yes">First</italic>, for the sake of fast and easy facilitation of domain adaptation in medical imaging data, we only include machine learning methods in the current version, without considering deep learning methods that often require large computation resources. In the future, we plan to develop another version of the toolbox to include deep learning methods (such as various GANs (<xref rid="R31" ref-type="bibr">Sinha et al., 2021</xref>; <xref rid="R44" ref-type="bibr">Yi et al., 2019</xref>) and CNNs (Guan et al., 2021a; <xref rid="R33" ref-type="bibr">Tibrewala et al., 2020</xref>)). <italic toggle="yes">Second</italic>, the current evaluation metrics merely reflect domain differences, lacking the ability to further analyze practical applications (<italic toggle="yes">e.g.</italic>, to what extent Dice scores in a segmentation application varies before and after domain adaptation). We will address this issue to enrich the toolbox in the future. <italic toggle="yes">Besides</italic>, we plan to further improve the graphic user interface to enable users to set and tune the hyper-parameters of each domain adaptation method in a more convenient manner.</p>
  </sec>
  <sec sec-type="supplementary-material" id="SM1">
    <title>Supplementary Material</title>
    <supplementary-material id="SD1" position="float" content-type="local-data">
      <label>Supporting Information</label>
      <media xlink:href="NIHMS1866657-supplement-Supporting_Information.pdf" id="d64e1130" position="anchor"/>
    </supplementary-material>
  </sec>
</body>
<back>
  <ack id="S46">
    <title>Acknowledgment</title>
    <p id="P65">This work was supported in part by NIH grants (Nos. AG073297 and AG041721).</p>
  </ack>
  <fn-group>
    <fn id="FN1">
      <p id="P69">Code Availability Statement</p>
      <p id="P70">The software and code of the DomainATM toolbox as well as the manual have been submitted at the time of paper submission. These materials can be also found at the following link: <ext-link xlink:href="https://www.mingxia.web.unc.edu/domainatm/" ext-link-type="uri">https://www.mingxia.web.unc.edu/domainatm/</ext-link></p>
    </fn>
    <fn id="FN2">
      <p id="P71">Credit authorship contribution statement</p>
      <p id="P72"><bold>Hao Guan:</bold> Conceptualization, Methodology, Software, Writing ‚Äì original draft. <bold>Mingxia Liu:</bold> Conceptualization, Validation, Writing ‚Äì review &amp; editing, Supervision.</p>
    </fn>
    <fn id="FN3">
      <p id="P73">Supplementary material</p>
      <p id="P74">Supplementary material associated with this article can be found, in the online version, at doi:<ext-link xlink:href="10.1016/j.neuroimage.2023.119863" ext-link-type="doi">10.1016/j.neuroimage.2023.119863</ext-link></p>
    </fn>
    <fn id="FN4">
      <label>1</label>
      <p id="P75">
        <ext-link xlink:href="https://www.mingxia.web.unc.edu/domainatm/" ext-link-type="uri">https://www.mingxia.web.unc.edu/domainatm/</ext-link>
      </p>
    </fn>
  </fn-group>
  <sec sec-type="data-availability" id="S47">
    <title>Data Availability</title>
    <p id="P66">The ADNI dataset used in this study can be accessed via the following link <ext-link xlink:href="http://www.ida.loni.usc.edu/login.jsp?project=ADNI&amp;page=HOME" ext-link-type="uri">http://www.ida.loni.usc.edu/login.jsp?project=ADNI&amp;page=HOME</ext-link>. An access application should be permitted firstly.</p>
    <p id="P67">The ABIDE dataset used in this study can be accessed via the following link http://www.fcon_1000.projects.nitrc.org/indi/abide/abide_I.html. An access application should be permitted firstly.</p>
    <p id="P68">For review convenience, we included the processed datasets (in terms of 2D features) in the submitted source code of this paper which can be found in the data folder.</p>
  </sec>
  <ref-list>
    <title>References</title>
    <ref id="R1">
      <mixed-citation publication-type="journal"><name><surname>Barrag√°n-Montero</surname></name>, <etal/>, <year>2021</year>. <article-title>Artificial intelligence and machine learning for medical imaging: a technology review</article-title>. <source>Physica Med</source>. <volume>83</volume>, <fpage>242</fpage>‚Äì<lpage>256</lpage>.</mixed-citation>
    </ref>
    <ref id="R2">
      <mixed-citation publication-type="journal"><name><surname>Ben-David</surname><given-names>S</given-names></name>, <name><surname>Blitzer</surname><given-names>J</given-names></name>, <name><surname>Crammer</surname><given-names>K</given-names></name>, <name><surname>Pereira</surname><given-names>F</given-names></name>, <etal/>, <year>2007</year>. <article-title>Analysis of representations for domain adaptation</article-title>. <source>MIT; 1998</source>, pp. <fpage>137</fpage>‚Äì<lpage>144</lpage>.</mixed-citation>
    </ref>
    <ref id="R3">
      <mixed-citation publication-type="book"><name><surname>Csurka</surname><given-names>G</given-names></name>, <year>2017</year>. <part-title>A comprehensive survey on domain adaptation for visual applications</part-title>. In: <source>Domain Adaptation in Computer Vision Applications</source>. <publisher-name>Springer</publisher-name>, pp. <fpage>1</fpage>‚Äì<lpage>35</lpage>.</mixed-citation>
    </ref>
    <ref id="R4">
      <mixed-citation publication-type="journal"><name><surname>Deo</surname><given-names>RC</given-names></name>, <year>2015</year>. <article-title>Machine learning in medicine</article-title>. <source>Circulation</source><volume>132</volume> (<issue>20</issue>), <fpage>1920</fpage>‚Äì<lpage>1930</lpage>.<pub-id pub-id-type="pmid">26572668</pub-id></mixed-citation>
    </ref>
    <ref id="R5">
      <mixed-citation publication-type="journal"><name><surname>Di Martino</surname><given-names>A</given-names></name>, <name><surname>Yan</surname><given-names>CG</given-names></name>, <name><surname>Li</surname><given-names>Q</given-names></name>, <name><surname>Denio</surname><given-names>E</given-names></name>, <etal/>, <year>2014</year>. <article-title>The autism brain imaging data exchange: towards a large-scale evaluation of the intrinsic brain architecture in autism</article-title>. <source>Mol. Psychiatry</source><volume>19</volume> (<issue>6</issue>), <fpage>659</fpage>‚Äì<lpage>667</lpage>.<pub-id pub-id-type="pmid">23774715</pub-id></mixed-citation>
    </ref>
    <ref id="R6">
      <mixed-citation publication-type="journal"><name><surname>Erickson</surname><given-names>BJ</given-names></name>, <name><surname>Korfiatis</surname><given-names>P</given-names></name>, <name><surname>Akkus</surname><given-names>Z</given-names></name>, <name><surname>Kline</surname><given-names>TL</given-names></name>, <year>2017</year>. <article-title>Machine learning for medical imaging</article-title>. <source>Radiographics</source><volume>37</volume> (<issue>2</issue>), <fpage>505</fpage>.<pub-id pub-id-type="pmid">28212054</pub-id></mixed-citation>
    </ref>
    <ref id="R7">
      <mixed-citation publication-type="journal"><name><surname>Fatima</surname><given-names>M</given-names></name>, <name><surname>Pasha</surname><given-names>M</given-names></name>, <etal/>, <year>2017</year>. <article-title>Survey of machine learning algorithms for disease diagnostic</article-title>. <source>Journal of Intelligent Learning Systems and Applications</source><volume>9</volume> (<issue>01</issue>), <fpage>1</fpage>.</mixed-citation>
    </ref>
    <ref id="R8">
      <mixed-citation publication-type="confproc"><name><surname>Fernando</surname><given-names>B</given-names></name>, <name><surname>Habrard</surname><given-names>A</given-names></name>, <name><surname>Sebban</surname><given-names>M</given-names></name>, <name><surname>Tuytelaars</surname><given-names>T</given-names></name>, <year>2013</year>. <source>Unsupervised visual domain adaptation using subspace alignment</source>. In: <conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name>, pp. <fpage>2960</fpage>‚Äì<lpage>2967</lpage>.</mixed-citation>
    </ref>
    <ref id="R9">
      <mixed-citation publication-type="journal"><name><surname>Ghifary</surname><given-names>M</given-names></name>, <name><surname>Balduzzi</surname><given-names>D</given-names></name>, <name><surname>Kleijn</surname><given-names>WB</given-names></name>, <name><surname>Zhang</surname><given-names>M</given-names></name>, <year>2016</year>. <article-title>Scatter component analysis: a unified framework for domain adaptation and domain generalization</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source><volume>39</volume> (<issue>7</issue>), <fpage>1414</fpage>‚Äì<lpage>1430</lpage>.<pub-id pub-id-type="pmid">28113617</pub-id></mixed-citation>
    </ref>
    <ref id="R10">
      <mixed-citation publication-type="book"><name><surname>Gong</surname><given-names>B</given-names></name>, <name><surname>Shi</surname><given-names>Y</given-names></name>, <name><surname>Sha</surname><given-names>F</given-names></name>, <name><surname>Grauman</surname><given-names>K</given-names></name>, <year>2012</year>. <part-title>Geodesic flow kernel for unsupervised domain adaptation</part-title>. In: <source>2012 IEEE Conference on Computer Vision and Pattern Recognition</source>. <publisher-name>IEEE</publisher-name>, pp. <fpage>2066</fpage>‚Äì<lpage>2073</lpage>.</mixed-citation>
    </ref>
    <ref id="R11">
      <mixed-citation publication-type="journal"><name><surname>Guan</surname><given-names>H</given-names></name>, <name><surname>Liu</surname><given-names>M</given-names></name>, <year>2022</year>. <article-title>Domain adaptation for medical image analysis: asurvey</article-title>. <source>IEEE Trans. Biomed. Eng</source>. <volume>69</volume> (<issue>3</issue>), <fpage>1173</fpage>‚Äì<lpage>1185</lpage>.<pub-id pub-id-type="pmid">34606445</pub-id></mixed-citation>
    </ref>
    <ref id="R12">
      <mixed-citation publication-type="book"><name><surname>Guan</surname><given-names>H</given-names></name>, <name><surname>Liu</surname><given-names>S</given-names></name>, <name><surname>Lin</surname><given-names>W</given-names></name>, <name><surname>Yap</surname><given-names>P-T</given-names></name>, <name><surname>Liu</surname><given-names>M</given-names></name>, <year>2022</year>. <part-title>Fast image-level MRI harmonization via spectrum analysis</part-title>. <source>International Workshop on Machine Learning in Medical Imaging</source>. <publisher-name>Springer</publisher-name>.</mixed-citation>
    </ref>
    <ref id="R13">
      <mixed-citation publication-type="journal"><name><surname>Guan</surname><given-names>H</given-names></name>, <name><surname>Liu</surname><given-names>Y</given-names></name>, <name><surname>Yang</surname><given-names>E</given-names></name>, <name><surname>Yap</surname><given-names>P-T</given-names></name>, <name><surname>Shen</surname><given-names>D</given-names></name>, <name><surname>Liu</surname><given-names>M</given-names></name>, <year>2021</year>. <article-title>Multi-site MRI harmonization via attention-guided deep domain adaptation for brain disorder identification</article-title>. <source>Med Image Anal</source><volume>71</volume>, <fpage>102076</fpage>.<pub-id pub-id-type="pmid">33930828</pub-id></mixed-citation>
    </ref>
    <ref id="R14">
      <mixed-citation publication-type="confproc"><name><surname>Guan</surname><given-names>H</given-names></name>, <name><surname>Wang</surname><given-names>L</given-names></name>, <name><surname>Liu</surname><given-names>M</given-names></name>, <year>2021</year>. <source>Multi-source domain adaptation via optimal transport for brain dementia identification</source>. In: <conf-name>2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</conf-name>. <collab>IEEE</collab>, pp. <fpage>1514</fpage>‚Äì<lpage>1517</lpage>.</mixed-citation>
    </ref>
    <ref id="R15">
      <mixed-citation publication-type="journal"><name><surname>Jack</surname><given-names>CR</given-names><suffix>Jr</suffix></name>, <name><surname>Bernstein</surname><given-names>MA</given-names></name>, <name><surname>Fox</surname><given-names>NC</given-names></name>, <etal/>, <year>2008</year>. <article-title>The Alzheimer‚Äôs Disease Neuroimaging Initiative (ADNI): MRI methods</article-title>. <source>J. Magn. Reson. Imaging</source><volume>27</volume> (<issue>4</issue>), <fpage>685</fpage>‚Äì<lpage>691</lpage>.<pub-id pub-id-type="pmid">18302232</pub-id></mixed-citation>
    </ref>
    <ref id="R16">
      <mixed-citation publication-type="journal"><name><surname>Kondrateva</surname><given-names>E</given-names></name>, <name><surname>Pominova</surname><given-names>M</given-names></name>, <name><surname>Popova</surname><given-names>E</given-names></name>, <name><surname>Sharaev</surname><given-names>M</given-names></name>, <name><surname>Bernstein</surname><given-names>A</given-names></name>, <name><surname>Burnaev</surname><given-names>E</given-names></name>, <year>2021</year>. <article-title>Domain shift in computer vision models for MRI data analysis: An overview</article-title>. In: <source>Thirteenth International Conference on Machine Vision</source>, Vol. <volume>11605</volume>. <comment>SPIE</comment>, pp. <fpage>126</fpage>‚Äì<lpage>133</lpage>.</mixed-citation>
    </ref>
    <ref id="R17">
      <mixed-citation publication-type="journal"><name><surname>Kouw</surname><given-names>WM</given-names></name>, <name><surname>Loog</surname><given-names>M</given-names></name>, <year>2019</year>. <article-title>A review of domain adaptation without target labels</article-title>. <source>IEEE Trans Pattern Anal Mach Intell</source><volume>43</volume> (<issue>3</issue>), <fpage>766</fpage>‚Äì<lpage>785</lpage>.</mixed-citation>
    </ref>
    <ref id="R18">
      <mixed-citation publication-type="journal"><name><surname>Kumagai</surname><given-names>A</given-names></name>, <name><surname>Iwata</surname><given-names>T</given-names></name>, <year>2019</year>. <article-title>Unsupervised domain adaptation by matching distributions based on the maximum mean discrepancy via unilateral transformations</article-title>. In: <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>, Vol. <volume>33</volume>, pp. <fpage>4106</fpage>‚Äì<lpage>4113</lpage>.</mixed-citation>
    </ref>
    <ref id="R19">
      <mixed-citation publication-type="journal"><name><surname>Lee</surname><given-names>H</given-names></name>, <name><surname>Nakamura</surname><given-names>K</given-names></name>, <name><surname>Narayanan</surname><given-names>S</given-names></name>, <name><surname>Brown</surname><given-names>RA</given-names></name>, <name><surname>Arnold</surname><given-names>DL</given-names></name>, <name><surname>Initiative</surname><given-names>ADN</given-names></name>, <etal/>, <year>2019</year>. <article-title>Estimating and accounting for the effect of MRI scanner changes on longitudinal whole-brain volume change measurements</article-title>. <source>Neuroimage</source><volume>184</volume>, <fpage>555</fpage>‚Äì<lpage>565</lpage>.<pub-id pub-id-type="pmid">30253207</pub-id></mixed-citation>
    </ref>
    <ref id="R20">
      <mixed-citation publication-type="confproc"><name><surname>Long</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Ding</surname><given-names>G</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>, <name><surname>Yu</surname><given-names>PS</given-names></name>, <year>2013</year>. <source>Transfer feature learning with joint distribution adaptation</source>. In: <conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name>, pp. <fpage>2200</fpage>‚Äì<lpage>2207</lpage>.</mixed-citation>
    </ref>
    <ref id="R21">
      <mixed-citation publication-type="confproc"><name><surname>Long</surname><given-names>M</given-names></name>, <name><surname>Wang</surname><given-names>J</given-names></name>, <name><surname>Ding</surname><given-names>G</given-names></name>, <name><surname>Sun</surname><given-names>J</given-names></name>, <name><surname>Yu</surname><given-names>PS</given-names></name>, <year>2014</year>. <source>Transfer joint matching for unsupervised domain adaptation</source>. In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>1410</fpage>‚Äì<lpage>1417</lpage>.</mixed-citation>
    </ref>
    <ref id="R22">
      <mixed-citation publication-type="journal"><name><surname>Van der Maaten</surname><given-names>L</given-names></name>, <name><surname>Hinton</surname><given-names>G</given-names></name>, <year>2008</year>. <article-title>Visualizing data using t-SNE</article-title>. <source>Journal of Machine Learning Research</source> 9 (11).</mixed-citation>
    </ref>
    <ref id="R23">
      <mixed-citation publication-type="journal"><name><surname>Pan</surname><given-names>SJ</given-names></name>, <name><surname>Tsang</surname><given-names>IW</given-names></name>, <name><surname>Kwok</surname><given-names>JT</given-names></name>, <name><surname>Yang</surname><given-names>Q</given-names></name>, <year>2010</year>. <article-title>Domain adaptation via transfer component analysis</article-title>. <source>IEEE Trans. Neural Networks</source><volume>22</volume> (<issue>2</issue>), <fpage>199</fpage>‚Äì<lpage>210</lpage>.<pub-id pub-id-type="pmid">21095864</pub-id></mixed-citation>
    </ref>
    <ref id="R24">
      <mixed-citation publication-type="journal"><name><surname>Patel</surname><given-names>VM</given-names></name>, <name><surname>Gopalan</surname><given-names>R</given-names></name>, <name><surname>Li</surname><given-names>R</given-names></name>, <name><surname>Chellappa</surname><given-names>R</given-names></name>, <year>2015</year>. <article-title>Visual domain adaptation: a survey of recent advances</article-title>. <source>IEEE Signal Process Mag</source><volume>32</volume> (<issue>3</issue>), <fpage>53</fpage>‚Äì<lpage>69</lpage>.</mixed-citation>
    </ref>
    <ref id="R25">
      <mixed-citation publication-type="book"><name><surname>Pooch</surname><given-names>EH</given-names></name>, <name><surname>Ballester</surname><given-names>P</given-names></name>, <name><surname>Barros</surname><given-names>RC</given-names></name>, <year>2020</year>. <part-title>Can we trust deep learning based diagnosis? The impact of domain shift in chest radiograph classification</part-title>. In: <source>International Workshop on Thoracic Image Analysis</source>. <publisher-name>Springer</publisher-name>, pp. <fpage>74</fpage>‚Äì<lpage>83</lpage>.</mixed-citation>
    </ref>
    <ref id="R26">
      <mixed-citation publication-type="book"><name><surname>Qui√±onero-Candela</surname><given-names>J</given-names></name>, <name><surname>Sugiyama</surname><given-names>M</given-names></name>, <name><surname>Lawrence</surname><given-names>ND</given-names></name>, <name><surname>Schwaighofer</surname><given-names>A</given-names></name>, <year>2009</year>. <source>Dataset Shift in Machine Learning</source>. <publisher-name>MIT Press</publisher-name>.</mixed-citation>
    </ref>
    <ref id="R27">
      <mixed-citation publication-type="journal"><name><surname>Rajkomar</surname><given-names>A</given-names></name>, <name><surname>Dean</surname><given-names>J</given-names></name>, <name><surname>Kohane</surname><given-names>I</given-names></name>, <year>2019</year>. <article-title>Machine learning in medicine. N top</article-title><source>N. Engl. J. Med</source>. <volume>380</volume> (<issue>14</issue>), <fpage>1347</fpage>‚Äì<lpage>1358</lpage>.<pub-id pub-id-type="pmid">30943338</pub-id></mixed-citation>
    </ref>
    <ref id="R28">
      <mixed-citation publication-type="journal"><name><surname>Rubinov</surname><given-names>M</given-names></name>, <name><surname>Sporns</surname><given-names>O</given-names></name>, <year>2010</year>. <article-title>Complex network measures of brain connectivity: uses and interpretations</article-title>. <source>Neuroimage</source><volume>52</volume> (<issue>3</issue>), <fpage>1059</fpage>‚Äì<lpage>1069</lpage>.<pub-id pub-id-type="pmid">19819337</pub-id></mixed-citation>
    </ref>
    <ref id="R29">
      <mixed-citation publication-type="confproc"><name><surname>Shi</surname><given-names>Y</given-names></name>, <name><surname>Sha</surname><given-names>F</given-names></name>, <year>2012</year>. <source>Information-theoretical learning of discriminative clusters for unsupervised domain adaptation</source>. In: <conf-name>Proceedings of the 29th International Conference on International Conference on Machine Learning</conf-name>, pp. <fpage>1275</fpage>‚Äì<lpage>1282</lpage>.</mixed-citation>
    </ref>
    <ref id="R30">
      <mixed-citation publication-type="journal"><name><surname>Shinohara</surname><given-names>RT</given-names></name>, <etal/>, <year>2014</year>. <article-title>Statistical normalization techniques for magnetic resonance imaging. NeuroImage</article-title>: <source>Clinical</source><volume>6</volume>, <fpage>9</fpage>‚Äì<lpage>19</lpage>.</mixed-citation>
    </ref>
    <ref id="R31">
      <mixed-citation publication-type="confproc"><name><surname>Sinha</surname><given-names>S</given-names></name>, <name><surname>Thomopoulos</surname><given-names>SI</given-names></name>, <name><surname>Lam</surname><given-names>P</given-names></name>, <name><surname>Muir</surname><given-names>A</given-names></name>, <name><surname>Thompson</surname><given-names>PM</given-names></name>, <year>2021</year>. <source>Alzheimer‚Äôs disease classification accuracy is improved by MRI harmonization based on attention-guided generative adversarial networks</source>. In: <conf-name>17th International Symposium on Medical Information Processing and Analysis</conf-name>, Vol. <volume>12088</volume>. <comment>SPIE</comment>, pp. <fpage>180</fpage>‚Äì<lpage>189</lpage>.</mixed-citation>
    </ref>
    <ref id="R32">
      <mixed-citation publication-type="journal"><name><surname>Sun</surname><given-names>B</given-names></name>, <name><surname>Feng</surname><given-names>J</given-names></name>, <name><surname>Saenko</surname><given-names>K</given-names></name>, <year>2016</year>. <article-title>Return of frustratingly easy domain adaptation</article-title>. In: <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>, Vol. <volume>30</volume>.</mixed-citation>
    </ref>
    <ref id="R33">
      <mixed-citation publication-type="journal"><name><surname>Tibrewala</surname><given-names>R</given-names></name>, <name><surname>Ozhinsky</surname><given-names>E</given-names></name>, <name><surname>Shah</surname><given-names>R</given-names></name>, <name><surname>Flament</surname><given-names>I</given-names></name>, <name><surname>Crossley</surname><given-names>K</given-names></name>, <name><surname>Srinivasan</surname><given-names>R</given-names></name>, <name><surname>Souza</surname><given-names>R</given-names></name>, <name><surname>Link</surname><given-names>TM</given-names></name>, <name><surname>Pedoia</surname><given-names>V</given-names></name>, <name><surname>Majumdar</surname><given-names>S</given-names></name>, <year>2020</year>. <article-title>Computer-aided detection AI reduces interreader variability in grading hip abnormalities with MRI</article-title>. <source>J. Magn. Reson. Imaging</source><volume>52</volume> (<issue>4</issue>), <fpage>1163</fpage>‚Äì<lpage>1172</lpage>.<pub-id pub-id-type="pmid">32293775</pub-id></mixed-citation>
    </ref>
    <ref id="R34">
      <mixed-citation publication-type="confproc"><name><surname>Torralba</surname><given-names>A</given-names></name>, <name><surname>Efros</surname><given-names>AA</given-names></name>, <year>2011</year>. <source>Unbiased look at dataset bias</source>. In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>. <collab>IEEE</collab>, pp. <fpage>1521</fpage>‚Äì<lpage>1528</lpage>.</mixed-citation>
    </ref>
    <ref id="R35">
      <mixed-citation publication-type="journal"><name><surname>Tzourio-Mazoyer</surname><given-names>N</given-names></name>, <etal/>, <year>2002</year>. <article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title>. <source>Neuroimage</source><volume>15</volume> (<issue>1</issue>), <fpage>273</fpage>‚Äì<lpage>289</lpage>.<pub-id pub-id-type="pmid">11771995</pub-id></mixed-citation>
    </ref>
    <ref id="R36">
      <mixed-citation publication-type="journal"><name><surname>Valiant</surname><given-names>LG</given-names></name>, <year>1984</year>. <article-title>A theory of the learnable</article-title>. <source>Commun ACM</source><volume>27</volume> (<issue>11</issue>), <fpage>1134</fpage>‚Äì<lpage>1142</lpage>.</mixed-citation>
    </ref>
    <ref id="R37">
      <mixed-citation publication-type="journal"><name><surname>Valverde</surname><given-names>JM</given-names></name>, <name><surname>Imani</surname><given-names>V</given-names></name>, <name><surname>Abdollahzadeh</surname><given-names>A</given-names></name>, <name><surname>De Feo</surname><given-names>R</given-names></name>, <name><surname>Prakash</surname><given-names>M</given-names></name>, <name><surname>Ciszek</surname><given-names>R</given-names></name>, <name><surname>Tohka</surname><given-names>J</given-names></name>, <year>2021</year>. <article-title>Transfer learning in magnetic resonance brain imaging: a systematic review</article-title>. <source>Journal of Imaging</source><volume>7</volume> (<issue>4</issue>), <fpage>66</fpage>.<pub-id pub-id-type="pmid">34460516</pub-id></mixed-citation>
    </ref>
    <ref id="R38">
      <mixed-citation publication-type="journal"><name><surname>Volkow</surname><given-names>ND</given-names></name>, <etal/>, <year>2018</year>. <article-title>The conception of the ABCD study: from substance use to a broad NIH collaboration</article-title>. <source>Dev Cogn Neurosci</source><volume>32</volume>, <fpage>4</fpage>‚Äì<lpage>7</lpage>.<pub-id pub-id-type="pmid">29051027</pub-id></mixed-citation>
    </ref>
    <ref id="R39">
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>M</given-names></name>, <name><surname>Deng</surname><given-names>W</given-names></name>, <year>2018</year>. <article-title>Deep visual domain adaptation: a survey</article-title>. <source>Neurocomputing</source><volume>312</volume>, <fpage>135</fpage>‚Äì<lpage>153</lpage>.</mixed-citation>
    </ref>
    <ref id="R40">
      <mixed-citation publication-type="journal"><name><surname>Wang</surname><given-names>W</given-names></name>, <name><surname>Li</surname><given-names>H</given-names></name>, <name><surname>Ding</surname><given-names>Z</given-names></name>, <name><surname>Nie</surname><given-names>F</given-names></name>, <name><surname>Chen</surname><given-names>J</given-names></name>, <name><surname>Dong</surname><given-names>X</given-names></name>, <name><surname>Wang</surname><given-names>Z</given-names></name>, <year>2021</year>. <article-title>Rethinking maximum mean discrepancy for visual domain adaptation</article-title>. <source>IEEE Trans Neural Netw Learn Syst</source>.</mixed-citation>
    </ref>
    <ref id="R41">
      <mixed-citation publication-type="journal"><name><surname>Wilson</surname><given-names>G</given-names></name>, <name><surname>Cook</surname><given-names>DJ</given-names></name>, <year>2020</year>. <article-title>A survey of unsupervised deep domain adaptation</article-title>. <source>ACM Transactions on Intelligent Systems and Technology (TIST)</source><volume>11</volume> (<issue>5</issue>), <fpage>1</fpage>‚Äì<lpage>46</lpage>.</mixed-citation>
    </ref>
    <ref id="R42">
      <mixed-citation publication-type="journal"><name><surname>Wittens</surname><given-names>MMJ</given-names></name>, <etal/>, <year>2021</year>. <article-title>Inter-and intra-scanner variability of automated brain volumetry on three magnetic resonance imaging systems in Alzheimer‚Äôs disease and controls</article-title>. <source>Front Aging Neurosci</source><volume>13</volume>.</mixed-citation>
    </ref>
    <ref id="R43">
      <mixed-citation publication-type="confproc"><name><surname>Yan</surname><given-names>H</given-names></name>, <name><surname>Ding</surname><given-names>Y</given-names></name>, <name><surname>Li</surname><given-names>P</given-names></name>, <name><surname>Wang</surname><given-names>Q</given-names></name>, <name><surname>Xu</surname><given-names>Y</given-names></name>, <name><surname>Zuo</surname><given-names>W</given-names></name>, <year>2017</year>. <source>Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation</source>. In: <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>, pp. <fpage>2272</fpage>‚Äì<lpage>2281</lpage>.</mixed-citation>
    </ref>
    <ref id="R44">
      <mixed-citation publication-type="journal"><name><surname>Yi</surname><given-names>X</given-names></name>, <name><surname>Walia</surname><given-names>E</given-names></name>, <name><surname>Babyn</surname><given-names>P</given-names></name>, <year>2019</year>. <article-title>Generative adversarial network in medical imaging: a review</article-title>. <source>Med Image Anal</source><volume>58</volume>, <fpage>101552</fpage>.<pub-id pub-id-type="pmid">31521965</pub-id></mixed-citation>
    </ref>
    <ref id="R45">
      <mixed-citation publication-type="journal"><name><surname>Zhang</surname><given-names>J</given-names></name>, <name><surname>Chao</surname><given-names>H</given-names></name>, <name><surname>Yan</surname><given-names>P</given-names></name>, <year>2020</year>. <article-title>Robustified domain adaptation</article-title>. <source>arXiv</source>: <volume>2011</volume>.<fpage>09563</fpage></mixed-citation>
    </ref>
    <ref id="R46">
      <mixed-citation publication-type="confproc"><name><surname>Zou</surname><given-names>D</given-names></name>, <name><surname>Zhu</surname><given-names>Q</given-names></name>, <name><surname>Yan</surname><given-names>P</given-names></name>, <year>2020</year>. <source>Unsupervised domain adaptation with dual-scheme fusion network for medical image segmentation</source>. In: <conf-name>International Joint Conference on Artificial Intelligence</conf-name>, pp. <fpage>3291</fpage>‚Äì<lpage>3298</lpage>.</mixed-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig position="float" id="F1">
    <label>Fig. 1.</label>
    <caption>
      <p id="P76">Illustration of the ‚Äúdomain shift‚Äù phenomenon (<xref rid="R26" ref-type="bibr">Qui√±onero-Candela et al., 2009</xref>) (top row) and the fundamental of domain adaptation (distribution of source and target samples before and after adaptation).</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0001" position="float"/>
  </fig>
  <fig position="float" id="F2">
    <label>Fig. 2.</label>
    <caption>
      <p id="P77">Illustration of workflow of the DomainATM software. The DomainATM consists of three major components: 1) the data module loads or creates the datasets; 2) the algorithm module conducts feature-level or image-level domain adaptation and saves the results; and 3) the evaluation module assesses the adaptation performance according to specific metrics. DA: Domain Adaptation.</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0002" position="float"/>
  </fig>
  <fig position="float" id="F3">
    <label>Fig. 3.</label>
    <caption>
      <p id="P78">Graphical-User-Interface (GUI) of DomainATM.</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0003" position="float"/>
  </fig>
  <fig position="float" id="F4">
    <label>Fig. 4.</label>
    <caption>
      <p id="P79">Distribution of the synthetic data (baseline) and adapted data by nine different domain adaptation methods in the DomainATM toolbox. (<bold>+</bold> positive source samples; <bold>+</bold> positive target samples; ‚Ä¢ negative source samples; ‚Ä¢ negative target samples).</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0004" position="float"/>
  </fig>
  <fig position="float" id="F5">
    <label>Fig. 5.</label>
    <caption>
      <p id="P80">Synthetic data distribution differences in terms of maximum mean discrepancy before (baseline) and after domain adaptation using nine feature-level adaptation methods.</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0005" position="float"/>
  </fig>
  <fig position="float" id="F6">
    <label>Fig. 6.</label>
    <caption>
      <p id="P81">Synthetic data distribution differences in terms of domain-level classification accuracy on the synthetic dataset before (baseline) and after domain adaptation using nine feature-level adaptation methods.</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0006" position="float"/>
  </fig>
  <fig position="float" id="F7">
    <label>Fig. 7.</label>
    <caption>
      <p id="P82">Distribution of the original ADNI data (baseline) and adapted data by nine feature-level domain adaptation methods in the DomainATM toolbox. (<bold>+</bold> positive source samples; <bold>+</bold> positive target samples; ‚Ä¢ negative source samples; ‚Ä¢ negative target samples).</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0007" position="float"/>
  </fig>
  <fig position="float" id="F8">
    <label>Fig. 8.</label>
    <caption>
      <p id="P83">Data distribution differences in terms of maximum mean discrepancy on ADNI-1 and ADNI-2 before (baseline) and after domain adaptation operations.</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0008" position="float"/>
  </fig>
  <fig position="float" id="F9">
    <label>Fig. 9.</label>
    <caption>
      <p id="P84">Data distribution differences in terms of domain-level classification accuracy on ADNI-1 and ADNI-2 before (baseline) and after domain adaptation operations.</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0009" position="float"/>
  </fig>
  <fig position="float" id="F10">
    <label>Fig. 10.</label>
    <caption>
      <p id="P85">Distribution of the original ABIDE data (baseline) and adapted data by nine feature-level domain adaptation methods in the proposed DomainATM toolbox. (<bold>+</bold> positive source samples; <bold>+</bold> positive target samples; ‚Ä¢ negative source samples; ‚Ä¢ negative target samples).</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0010" position="float"/>
  </fig>
  <fig position="float" id="F11">
    <label>Fig. 11.</label>
    <caption>
      <p id="P86">Data distribution differences of two sites of ABIDE in terms of maximum mean discrepancy before (baseline) and after domain adaptation using nine feature-level adaptation methods.</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0011" position="float"/>
  </fig>
  <fig position="float" id="F12">
    <label>Fig. 12.</label>
    <caption>
      <p id="P87">Data distribution differences in terms of domain-level classification accuracy on two sites of ABIDE before (baseline) and after domain adaptation using nine feature-level adaptation methods.</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0012" position="float"/>
  </fig>
  <fig position="float" id="F13">
    <label>Fig. 13.</label>
    <caption>
      <p id="P88">Image-level domain adaptation via the Spectrum Swapping-based Image-level MRI Harmonization (SSIMH) method (<xref rid="R12" ref-type="bibr">Guan et al., 2022</xref>) for T1-weighted (T1-w) MRIs acquired by different scanners. Domain shift caused by the use of different scanners can be partly reduced by image-level adaptation via SSIMH.</p>
    </caption>
    <graphic xlink:href="nihms-1866657-f0013" position="float"/>
  </fig>
  <table-wrap position="float" id="T1">
    <label>Table 1</label>
    <caption>
      <p id="P89">Domain classification accuracy (%) using different classifiers on the synthetic dataset. (SVM: support vector machine; RF: random forest).</p>
    </caption>
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Baseline</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">SA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">CORAL</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">OT</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">TCA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">TJM</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">JDA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">GFK</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">SCA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">ITL</th>
        </tr>
        <tr>
          <th colspan="11" align="left" valign="top" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SVM</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">85</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">47</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">80</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">35</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">77</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">40</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">41</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">39</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">60</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">41</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RF</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">85</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">51</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">82</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">24</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">60</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">47</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">37</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">41</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">78</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">59</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T2">
    <label>Table 2</label>
    <caption>
      <p id="P90">Domain classification accuracy (%) using different classifiers on the ADNI-1 and ADNI-2 datasets. (SVM: support vector machine; RF: random forest).</p>
    </caption>
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Baseline</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">SA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">CORAL</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">OT</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">TCA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">TJM</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">JDA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">GFK</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">SCA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">ITL</th>
        </tr>
        <tr>
          <th colspan="11" align="left" valign="top" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SVM</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">79</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">43</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">79</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">43</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">77</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">43</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">43</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">43</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">43</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">43</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RF</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">85</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">85</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">52</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">80</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">63</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">58</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">60</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">50</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">57</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T3">
    <label>Table 3</label>
    <caption>
      <p id="P91">Domain classification accuracy (%) using different classifiers on two sites of ABIDE dataset. (SVM: support vector machine; RF: random forest).</p>
    </caption>
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">Baseline</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">SA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">CORAL</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">OT</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">TCA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">TJM</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">JDA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">GFK</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">SCA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">ITL</th>
        </tr>
        <tr>
          <th colspan="11" align="left" valign="top" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">SVM</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">69</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">55</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">67</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">55</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">68</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">55</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">55</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">55</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">55</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">65</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">RF</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">66</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">55</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">64</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">31</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">66</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">48</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">43</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">50</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">65</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">63</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T4">
    <label>Table 4</label>
    <caption>
      <p id="P92">Running time (in terms of seconds) of nine domain adaptation algorithms in DomainATM on three datasets.</p>
    </caption>
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="middle" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">SA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">CORAL</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">OT</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">TCA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">TJM</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">JDA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">GFK</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">SCA</th>
          <th align="left" valign="middle" rowspan="1" colspan="1">ITL</th>
        </tr>
        <tr>
          <th colspan="10" align="left" valign="top" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">Synthetic</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.09</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.05</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.28</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.06</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.19</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.85</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.09</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.04</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.06</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ADNI</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.05</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">2.78</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.04</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.21</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.92</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.09</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.13</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.13</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">ABIDE</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.03</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.01</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">6.07</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.07</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.26</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.92</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.09</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">1.74</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">0.25</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap position="float" id="T5" orientation="landscape">
    <label>Table 5</label>
    <caption>
      <p id="P93">Results of three image-level domain adaptation methods on T1-weighted MRIs of five traveling phantom subjects acquired by three different scanners from the ABCD dataset.</p>
    </caption>
    <table frame="hsides" rules="none">
      <colgroup span="1">
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
        <col align="left" valign="middle" span="1"/>
      </colgroup>
      <thead>
        <tr>
          <th align="left" valign="top" rowspan="1" colspan="1">Source Domain‚ÜíTarget Domain (Subject ID)</th>
          <th align="left" valign="top" rowspan="1" colspan="1">Method</th>
          <th align="left" valign="top" rowspan="1" colspan="1">CC</th>
          <th align="left" valign="top" rowspan="1" colspan="1">PSNR</th>
          <th align="left" valign="top" rowspan="1" colspan="1">MSE</th>
        </tr>
        <tr>
          <th colspan="5" align="left" valign="top" rowspan="1">
            <hr/>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td rowspan="3" align="left" valign="top" colspan="1">G‚ÜíSiemens (Phantom-2, Phantom-3)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Baseline</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.4889¬±0.0081</td>
          <td align="left" valign="top" rowspan="1" colspan="1">21.4143¬± 2.8718</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.0080¬± 0.0049</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">HM</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.5642¬±0.0395</td>
          <td align="left" valign="top" rowspan="1" colspan="1">22.3131¬± 2.5975</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.0064¬± 0.0037</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">SSIMH</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.5935¬±0.0221</td>
          <td align="left" valign="top" rowspan="1" colspan="1">22.7624¬± 2.5310</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.0057¬± 0.0032</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">Philips‚Üíiemens</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Baseline</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.5408¬±0.0194</td>
          <td align="left" valign="top" rowspan="1" colspan="1">18.7578¬± 0.8847</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.0135¬± 0.0028</td>
        </tr>
        <tr>
          <td rowspan="2" align="left" valign="top" colspan="1">(Phantom-4, Phantom-5)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">HM</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.5495¬±0.0388</td>
          <td align="left" valign="top" rowspan="1" colspan="1">18.7477¬± 1.1303</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.0135¬± 0.0035</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">SSIMH</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.6098¬±0.0269</td>
          <td align="left" valign="top" rowspan="1" colspan="1">20.1269¬± 1.8421</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.0101¬± 0.0042</td>
        </tr>
        <tr>
          <td rowspan="3" align="left" valign="top" colspan="1">GE‚ÜíPhilips (Phantom-1)</td>
          <td align="left" valign="top" rowspan="1" colspan="1">Baseline</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.4682</td>
          <td align="left" valign="top" rowspan="1" colspan="1">21.3915</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.0073</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">HM</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.5108</td>
          <td align="left" valign="top" rowspan="1" colspan="1">21.2482</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.0075</td>
        </tr>
        <tr>
          <td align="left" valign="top" rowspan="1" colspan="1">SSIMH</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.5570</td>
          <td align="left" valign="top" rowspan="1" colspan="1">22.6421</td>
          <td align="left" valign="top" rowspan="1" colspan="1">0.0054</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
