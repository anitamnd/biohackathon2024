<?properties open_access?>
<?DTDIdentifier.IdentifierValue -//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.1 20151215//EN?>
<?DTDIdentifier.IdentifierType public?>
<?SourceDTD.DTDName JATS-journalpublishing1.dtd?>
<?SourceDTD.Version 1.1?>
<?ConverterInfo.XSLTName jp2nlmx2.xsl?>
<?ConverterInfo.Version 1?>
<front>
  <journal-meta>
    <journal-id journal-id-type="nlm-ta">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="iso-abbrev">Sensors (Basel)</journal-id>
    <journal-id journal-id-type="publisher-id">sensors</journal-id>
    <journal-title-group>
      <journal-title>Sensors (Basel, Switzerland)</journal-title>
    </journal-title-group>
    <issn pub-type="epub">1424-8220</issn>
    <publisher>
      <publisher-name>MDPI</publisher-name>
    </publisher>
  </journal-meta>
  <article-meta>
    <article-id pub-id-type="pmcid">8004990</article-id>
    <article-id pub-id-type="doi">10.3390/s21062234</article-id>
    <article-id pub-id-type="publisher-id">sensors-21-02234</article-id>
    <article-categories>
      <subj-group subj-group-type="heading">
        <subject>Article</subject>
      </subj-group>
    </article-categories>
    <title-group>
      <article-title>ARETT: Augmented Reality Eye Tracking Toolkit for Head Mounted Displays</article-title>
    </title-group>
    <contrib-group>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1052-8901</contrib-id>
        <name>
          <surname>Kapp</surname>
          <given-names>Sebastian</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-21-02234">1</xref>
        <xref rid="c1-sensors-21-02234" ref-type="corresp">*</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6730-2466</contrib-id>
        <name>
          <surname>Barz</surname>
          <given-names>Michael</given-names>
        </name>
        <xref ref-type="aff" rid="af2-sensors-21-02234">2</xref>
        <xref ref-type="aff" rid="af3-sensors-21-02234">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4544-1462</contrib-id>
        <name>
          <surname>Mukhametov</surname>
          <given-names>Sergey</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-21-02234">1</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8857-8709</contrib-id>
        <name>
          <surname>Sonntag</surname>
          <given-names>Daniel</given-names>
        </name>
        <xref ref-type="aff" rid="af2-sensors-21-02234">2</xref>
        <xref ref-type="aff" rid="af3-sensors-21-02234">3</xref>
      </contrib>
      <contrib contrib-type="author">
        <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6985-3218</contrib-id>
        <name>
          <surname>Kuhn</surname>
          <given-names>Jochen</given-names>
        </name>
        <xref ref-type="aff" rid="af1-sensors-21-02234">1</xref>
      </contrib>
    </contrib-group>
    <contrib-group>
      <contrib contrib-type="editor">
        <name>
          <surname>Ward</surname>
          <given-names>Jamie A</given-names>
        </name>
        <role>Academic Editor</role>
      </contrib>
    </contrib-group>
    <aff id="af1-sensors-21-02234"><label>1</label>Department of Physics, Technische Universität Kaiserslautern, Erwin-Schrödinger-Str. 46, 67663 Kaiserslautern, Germany; <email>mukhamet@physik.uni-kl.de</email> (S.M.); <email>kuhn@physik.uni-kl.de</email> (J.K.)</aff>
    <aff id="af2-sensors-21-02234"><label>2</label>German Research Center for Artificial Intelligence (DFKI), Interactive Machine Learning Department, Stuhlsatzenhausweg 3, Saarland Informatics Campus D3_2, 66123 Saarbrücken, Germany; <email>michael.barz@dfki.de</email> (M.B.); <email>daniel.sonntag@dfki.de</email> (D.S.)</aff>
    <aff id="af3-sensors-21-02234"><label>3</label>Applied Artificial Intelligence, Oldenburg University, Marie-Curie Str. 1, 26129 Oldenburg, Germany</aff>
    <author-notes>
      <corresp id="c1-sensors-21-02234"><label>*</label>Correspondence: <email>kapp@physik.uni-kl.de</email></corresp>
    </author-notes>
    <pub-date pub-type="epub">
      <day>23</day>
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <pub-date pub-type="collection">
      <month>3</month>
      <year>2021</year>
    </pub-date>
    <volume>21</volume>
    <issue>6</issue>
    <elocation-id>2234</elocation-id>
    <history>
      <date date-type="received">
        <day>25</day>
        <month>2</month>
        <year>2021</year>
      </date>
      <date date-type="accepted">
        <day>17</day>
        <month>3</month>
        <year>2021</year>
      </date>
    </history>
    <permissions>
      <copyright-statement>© 2021 by the authors.</copyright-statement>
      <copyright-year>2021</copyright-year>
      <license license-type="open-access">
        <license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p>
      </license>
    </permissions>
    <abstract>
      <p>Currently an increasing number of head mounted displays (HMD) for virtual and augmented reality (VR/AR) are equipped with integrated eye trackers. Use cases of these integrated eye trackers include rendering optimization and gaze-based user interaction. In addition, visual attention in VR and AR is interesting for applied research based on eye tracking in cognitive or educational sciences for example. While some research toolkits for VR already exist, only a few target AR scenarios. In this work, we present an open-source eye tracking toolkit for reliable gaze data acquisition in AR based on Unity 3D and the Microsoft HoloLens 2, as well as an R package for seamless data analysis. Furthermore, we evaluate the spatial accuracy and precision of the integrated eye tracker for fixation targets with different distances and angles to the user (<inline-formula><mml:math id="mm1"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). On average, we found that gaze estimates are reported with an angular accuracy of <inline-formula><mml:math id="mm2"><mml:mrow><mml:mrow><mml:mn>0.83</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees and a precision of <inline-formula><mml:math id="mm3"><mml:mrow><mml:mrow><mml:mn>0.27</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees while the user is resting, which is on par with state-of-the-art mobile eye trackers.</p>
    </abstract>
    <kwd-group>
      <kwd>augmented reality</kwd>
      <kwd>eye tracking</kwd>
      <kwd>toolkit</kwd>
      <kwd>accuracy</kwd>
      <kwd>precision</kwd>
    </kwd-group>
  </article-meta>
</front>
<body>
  <sec sec-type="intro" id="sec1-sensors-21-02234">
    <title>1. Introduction</title>
    <p>Head mounted displays (HMD) got more affordable and lightweight in the last few years facilitating a broader usage of virtual and augmented reality (VR/AR) applications. In addition, recent devices are equipped with integrated eye trackers which primarily target novel gaze-based interaction techniques [<xref rid="B1-sensors-21-02234" ref-type="bibr">1</xref>,<xref rid="B2-sensors-21-02234" ref-type="bibr">2</xref>] and optimizing the display quality, e.g., using foveated rendering [<xref rid="B3-sensors-21-02234" ref-type="bibr">3</xref>,<xref rid="B4-sensors-21-02234" ref-type="bibr">4</xref>]. This creates new opportunities for eye tracking research in mixed reality settings. However, the number and functionality of research tools for AR and VR eye tracking is still limited, e.g., compared to the well-established stationary eye trackers that are attached to a two-dimensional display. Available commercial solutions for HMD eye tracking are mostly limited to VR (see, e.g., References [<xref rid="B5-sensors-21-02234" ref-type="bibr">5</xref>,<xref rid="B6-sensors-21-02234" ref-type="bibr">6</xref>]). Pupil Labs [<xref rid="B6-sensors-21-02234" ref-type="bibr">6</xref>] offers an extension for AR eye tracking which consists of mobile eye tracking equipment attached to an HMD, but with only a loose integration into AR application development tools.</p>
    <p>In this work, we aim at closing the gap of research tools for AR eye tracking. We implement an open-source toolkit that facilitates eye tracking research in AR environments with the Microsoft HoloLens 2. Our toolkit includes a package for the Unity 3D game development engine which enables simple integration of reliable gaze and meta data recordings in AR applications, and an R package for seamless post-hoc processing and analysis of the data. In addition, we conduct a user study (<inline-formula><mml:math id="mm4"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) for evaluating the spatial accuracy and precision of the gaze signal retrieved from our toolkit. We discuss our results and compare them to results for state-of-the-art mobile eye trackers from the literature.</p>
  </sec>
  <sec id="sec2-sensors-21-02234">
    <title>2. Related Work</title>
    <p>Our work is related to other research-oriented toolkits and software solutions for head-mounted eye tracking systems, particularly to those targeting VR and AR environments, and to literature on measuring the gaze estimation error.</p>
    <sec id="sec2dot1-sensors-21-02234">
      <title>2.1. AR and VR Eye Tracking</title>
      <p>Some toolkits for eye tracking research in VR are available. Tobii offers a solution for eye tracking analysis in VR by providing tools for the integration of eye tracking hardware to HMDs and analysis software for eye tracking research [<xref rid="B7-sensors-21-02234" ref-type="bibr">7</xref>]. Another commercial eye tracking add-on is offered by Pupil Labs for the HTC Vive HMD together with open-source software for data analysis [<xref rid="B6-sensors-21-02234" ref-type="bibr">6</xref>]. Non-commercial frameworks for eye tracking in AR or VR exist, as well. Stratmann et al. [<xref rid="B8-sensors-21-02234" ref-type="bibr">8</xref>] presented EyeMR, a low-cost system for integrating eye tracking into VR based on the Pupil Capture software and a custom Unity 3D framework. Lee et al. [<xref rid="B9-sensors-21-02234" ref-type="bibr">9</xref>] also presented a method for low-cost gaze tracking and gaze point estimation in head-mounted devices. Mardanbegi and Pfeiffer [<xref rid="B10-sensors-21-02234" ref-type="bibr">10</xref>] presented the EyeMRTK toolkit to develop gaze-based interaction techniques in VR and AR; however, the current implementation is limited to specific VR headsets. Adhanom et al. [<xref rid="B11-sensors-21-02234" ref-type="bibr">11</xref>] presented the GazeMetrics tool which provides a standardized approach to measure accuracy and precision in VR settings.</p>
      <p>The range of AR eye tracking toolkits is more limited. Pupil Labs [<xref rid="B6-sensors-21-02234" ref-type="bibr">6</xref>] offers eye tracking add-ons for the Microsoft HoloLens 1 and the Epson Moverio BT-300, but the analysis software is tailored to mobile eye tracking without HMDs and their integration into the Unity 3D development environment is discontinued (<uri xlink:href="https://github.com/pupil-labs/hmd-eyes/issues/100#issuecomment-662362737">https://github.com/pupil-labs/hmd-eyes/issues/100#issuecomment-662362737</uri>, accessed on 20 November 2020). This limits the usefulness of the offered add-ons and restricts applications to use cases in which no AR integration is required. Recent HMDs, like the Magic Leap 1 [<xref rid="B12-sensors-21-02234" ref-type="bibr">12</xref>] and the Microsoft HoloLens 2 [<xref rid="B13-sensors-21-02234" ref-type="bibr">13</xref>], are equipped with integrated eye trackers. However, the toolkits and APIs provided by the manufacturers are targeted at gaze-based interaction and not at eye tracking research [<xref rid="B14-sensors-21-02234" ref-type="bibr">14</xref>,<xref rid="B15-sensors-21-02234" ref-type="bibr">15</xref>]. Still, this enables an easy integration of visual attention into AR applications: using the spatial awareness of the devices provides eye-in-world data which otherwise has to be integrated using additional sensors [<xref rid="B16-sensors-21-02234" ref-type="bibr">16</xref>]. We build our toolkit on top of the eye tracking APIs of the HoloLens 2 device [<xref rid="B13-sensors-21-02234" ref-type="bibr">13</xref>]. However, all device specific code is encapsulated in a data access layer which enables easy adaption of the toolkit to other eye tracking enabled AR devices.</p>
    </sec>
    <sec id="sec2dot2-sensors-21-02234">
      <title>2.2. Measuring the Gaze Estimation Error</title>
      <p>Eye tracking research studies investigate the impact of an intervention on the eye movements of a participant. Typically, the gaze samples or fixations, i.e., the periods for which the eye is relatively still, are used to approximate the human visual attention, and are mapped to areas of interest (AOIs) for analysis. High gaze estimation quality is essential for eye tracking research because errors can heavily undermine the results [<xref rid="B17-sensors-21-02234" ref-type="bibr">17</xref>]. However, a key problem in head-mounted eye tracking is that the gaze estimation error, i.e., the difference between the estimated and true gaze position, can be substantial, particularly if participants move and if fixation distances vary [<xref rid="B18-sensors-21-02234" ref-type="bibr">18</xref>,<xref rid="B19-sensors-21-02234" ref-type="bibr">19</xref>]. Besides user position and orientation, also factors specific to the eye tracker and display, e.g., parameters of the calibration routine and of the display detection algorithm, can have significant impact on the gaze estimation error [<xref rid="B20-sensors-21-02234" ref-type="bibr">20</xref>]. Typical metrics for the error of gaze estimation include spatial accuracy and spatial precision [<xref rid="B21-sensors-21-02234" ref-type="bibr">21</xref>]. Spatial accuracy is commonly computed as the mean angular deviation of fixations to the actual position, and spatial precision as the root mean square error or standard deviation of individual gaze samples from their centroid [<xref rid="B21-sensors-21-02234" ref-type="bibr">21</xref>,<xref rid="B22-sensors-21-02234" ref-type="bibr">22</xref>].</p>
    </sec>
  </sec>
  <sec id="sec3-sensors-21-02234">
    <title>3. Augmented Reality Eye Tracking Toolkit</title>
    <p>We develop an eye tracking toolkit for augmented reality applications using the Unity 3D game development engine [<xref rid="B23-sensors-21-02234" ref-type="bibr">23</xref>]. Our goal is to simplify the access to eye tracking data from the Microsoft HoloLens 2 for research purposes or advanced interaction techniques. We aim at providing raw gaze data robustly at a fixed data rate, without delay, and with highest possible spatial accuracy and precision. For this, we implement an easy-to-use interface to control recordings and enable a simple integration into applications and research studies. In addition, we implement a package for the statistical computing environment R for seamless data analysis [<xref rid="B24-sensors-21-02234" ref-type="bibr">24</xref>]. The toolkit, a detailed documentation, and an example project are available on GitHub (<uri xlink:href="https://github.com/AR-Eye-Tracking-Toolkit/ARETT">https://github.com/AR-Eye-Tracking-Toolkit/ARETT</uri>, accessed on 22 March 2021) under the MIT open-source license.</p>
    <sec id="sec3dot1-sensors-21-02234">
      <title>3.1. Overview of HoloLens 2 Technology</title>
      <p>We briefly summarize the underlying technology, i.e., the eye tracking hardware and software of the HoloLens 2 which we interface in our data access layer. Similar to other head-mounted eye trackers, the Microsoft HoloLens 2 uses two infrared cameras that yield a close-up view of the wearer’s eyes [<xref rid="B13-sensors-21-02234" ref-type="bibr">13</xref>]. After using the built-in 9-point calibration routine, a closed processing module provides real-time 3D gaze data to developers including a gaze origin and a direction vector. Gaze data can be accessed within Unity 3D via the Mixed Reality Toolkit (MRTK) [<xref rid="B14-sensors-21-02234" ref-type="bibr">14</xref>] and via the underlying API for the Universal Windows Platform (UWP) [<xref rid="B25-sensors-21-02234" ref-type="bibr">25</xref>]. The MRTK primarily focuses on enabling gaze-based interaction via an easy-to-use API for developers. It does not offer recordings for research purposes, nor does it guarantee a fixed sampling rate which is tied to the Unity3D update rate. Hence, gaze samples might be missed. Our system is based on the API for the UWP which provides unsmoothed data, more stable data rates, and a higher level of control. Further, a high precision timestamp in the system-relative QueryPerformanceCounter (QPC) time domain with a precision of 100 ns is provided for each data point. The manufacturer is vague in reporting specifications related to data quality: the data rate is “approximately 30 Hz” with a spatial accuracy that ranges “approximately within 1.5 degrees” [<xref rid="B26-sensors-21-02234" ref-type="bibr">26</xref>].</p>
    </sec>
    <sec id="sec3dot2-sensors-21-02234">
      <title>3.2. Architecture &amp; Components of the Recording Tool</title>
      <p>The recording tool of our toolkit is implemented as a package for the Unity 3D game development engine and includes four major components: the generic data provider with the HoloLens-specific data access layer that makes timestamped gaze data available in real-time, the data logger that is responsible for storing the data, the web-based control interface, and a set of utility tools for data visualization. An overview of our system’s architecture and the interplay of individual components is shown in <xref ref-type="fig" rid="sensors-21-02234-f001">Figure 1</xref>. In the following, we describe each component in detail and discuss the implementation of egocentric video capture.</p>
      <p>The data provider accesses raw eye tracking data using the data access layer, processes it and raises according gaze data events. The data access layer on the HoloLens 2 checks for new gaze samples in a separate thread every 10 ms to reliably obtain all gaze samples from the API with a supposed data rate of 30 Hz, i.e., we expect a new gaze sample every <inline-formula><mml:math id="mm5"><mml:mrow><mml:mrow><mml:mn>33.33</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> ms. This pulling is necessary as no new data event is provided by the API. Each gaze sample includes the origin of the gaze point, its direction vector, and a timestamp. All gaze samples received by the access layer are queued in the data provider and processed in the next frame update in the Unity 3D main thread. For each gaze sample, we cast a ray and check for hits with collider objects in the scene. If the option spatial mapping of the MRTK is enabled for the application, this includes the real environment that is scanned by the depth sensors of the HoloLens 2. If a collider is hit, we extend the gaze sample by the intersection coordinates in the world coordinate system, the object’s name, position, rotation and scale, the intersection point in the object’s local coordinate system, and the gaze point projection to the 2D eye displays. In addition, we support AOI colliders for real-time gaze-to-AOI mapping with support for dynamic AOIs. AOI collider objects can be placed at any position of a Unity 3D scene or attached to virtual objects in the scene. AOIs must be defined during the application development phase. Real-time and gaze-based adaptations can be realized using custom scripts. Synchronized recordings of the gaze signal and the front-facing camera can be used to define further AOIs post-hoc. We separately cast gaze rays to check for hits with AOI colliders. In addition, we offer an option to store the position, rotation and scaling of game objects in the scene in our gaze sample. This can be used to simulate or visualize sequences of interest post-hoc. For each processed sample, we raise an event that can be subscribed by other components, such as the data logger.</p>
      <p>The data logger component provides the option to record all gaze samples. An overview of all recorded data columns can be found in <xref rid="sensors-21-02234-t001" ref-type="table">Table 1</xref>. The files are named based on the participant’s pseudonym and a custom recording name. All recordings of a participant are stored in one folder. The gaze data samples are saved as comma separated values (CSV) with one sample per row and the columns as described in <xref rid="sensors-21-02234-t001" ref-type="table">Table 1</xref>. In addition, we store meta information of the recording, e.g., the start and end time of the recording, in a separate text file in the JSON format. After the recording is started, developers can log additional events in terms of an info string that is stored as part of the gaze sample and in the JSON file. This enables researchers to track custom interaction events, which are of interest to their research question, and session annotations. The recording can be started via function calls and is used in our web-based control interface.</p>
      <p>We integrate two utility tools that ease the development, debugging, and monitoring of study prototypes. This includes a tool for visualizing a grid of fixation targets, and one for highlighting AOIs. The grid of fixation targets enables easy collection of gaze samples and corresponding target positions for the evaluation of spatial accuracy and precision. We use this tool in our evaluations: we show nine fixation targets arranged in a <inline-formula><mml:math id="mm6"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> grid at multiple distances from the user. The AOI highlighting helps in debugging dynamic and interactive experiment scenes in which AOIs can move around, appear and disappear during the experiment session. For this, the developer can add custom visualizations which can be dynamically shown and hidden using the control interface.</p>
      <p>Our toolkit comes with a web-based control interface (see <xref ref-type="fig" rid="sensors-21-02234-f002">Figure 2</xref>). It enables the experimenter to easily set a participant acronym and a recording name, and to start and stop recordings from any computer in the local network. Further, it provides access to our utility tools and allows an experimenter to add custom annotations to the recording during the study.</p>
      <p>Typically, head-mounted eye trackers use a world camera to record the environment from an egocentric perspective and map the wearer’s pupil positions to the corresponding video frames. The integrated eye tracker of the Microsoft HoloLens 2, however, maps pupil positions to gaze rays in the 3D coordinate system of the device.</p>
      <p>Our toolkit adds a virtual camera to the 3D scene that matches the location, projection, and resolution of the integrated front-facing camera. This enables the projection of the 3D gaze position to the virtual 2D camera image and, hence, to the webcam image. The virtual camera is preconfigured to match the integrated, front-facing webcam of the HoloLens 2. We recommend to check the configuration per use case and to adapt it, if the camera specifications differ. The 2D gaze signal is reported via the gaze sample event of the data provider and recorded in the <italic>gazePointWebcam</italic> column.</p>
      <p>If video streaming or capturing for demonstration purposes is required only, the Mixed Reality Capture (MRC) module of HoloLens 2 can be used. It streams or records an egocentric video with an overlay showing the virtual content [<xref rid="B27-sensors-21-02234" ref-type="bibr">27</xref>]. Our toolkit supports gaze visualization in this module by attaching a small sphere to the current gaze position that is visible in the capture but not to the user. However, this method is computationally demanding which constrains the framerate for all applications to 30 frames per second and has a negative impact on real-time interactive applications which limits its use to demonstration purposes.</p>
    </sec>
    <sec id="sec3dot3-sensors-21-02234">
      <title>3.3. R Package for Data Analysis</title>
      <p>We implement an R package for seamless data analysis of recordings from our recording tool. Existing data analysis tools are primarily targeted at stationary eye trackers that yield a two-dimensional gaze signal or mobile eye trackers that report gaze with respect to an egocentric video feed [<xref rid="B5-sensors-21-02234" ref-type="bibr">5</xref>,<xref rid="B28-sensors-21-02234" ref-type="bibr">28</xref>,<xref rid="B29-sensors-21-02234" ref-type="bibr">29</xref>,<xref rid="B30-sensors-21-02234" ref-type="bibr">30</xref>]. Our toolkit reports three dimensional gaze data with a world-centered coordinate system. We provide a new R package that supports this data paradigm. It offers offline fixation detection with corresponding pre- and post-processing routines. The R package and detailed documentation is published on GitHub (<uri xlink:href="https://github.com/AR-Eye-Tracking-Toolkit/ARETT-R-Package">https://github.com/AR-Eye-Tracking-Toolkit/ARETT-R-Package</uri>, accessed on 22 March 2021) under the MIT open-source license.</p>
      <p>We implement two functions for pre-processing the raw gaze data, <italic>gap fill</italic> and <italic>noise reduction</italic>, similar to Reference [<xref rid="B31-sensors-21-02234" ref-type="bibr">31</xref>]. The <italic>gap fill</italic> function linearly interpolates between valid gaze points with small gaps in between, e.g., due to loss of tracking. The <italic>noise reduction</italic> function applies a mean or median filter to the gaze data with a given window size.</p>
      <p>Three methods from the literature for offline fixation detection are implemented. This includes I-VT using a velocity threshold similar to Reference [<xref rid="B31-sensors-21-02234" ref-type="bibr">31</xref>], I-DT for VR as described by Llanes-Jurado et al. [<xref rid="B32-sensors-21-02234" ref-type="bibr">32</xref>] using a dispersion threshold, and I-AOI proposed by Salvucci and Goldberg [<xref rid="B33-sensors-21-02234" ref-type="bibr">33</xref>] based on detected areas of interest. Our implementation of I-VT follows the description by Olsen [<xref rid="B31-sensors-21-02234" ref-type="bibr">31</xref>]. It reproduces a similar behavior based on the data recorded using our toolkit. We calculate a velocity for each gaze point over a specified duration and categorize the points by comparing the velocities to a specified threshold. I-DT follows the implementation by Llanes-Jurado et al. [<xref rid="B32-sensors-21-02234" ref-type="bibr">32</xref>]. It computes the angular dispersion distance over a window of a specific size in terms of its duration. If the initial window exceeds this threshold it is moved forward until it does not exceeded the threshold. Then, the window is extended to the right until the dispersion threshold is exceeded. All samples in the window, excluding the last sample, are classified as belonging to a fixation. Afterwards, a new window is initialized at the position of the last gaze sample. These steps are repeated until all samples are classified. The I-AOI method for fixation detection is based on Salvucci and Goldberg [<xref rid="B33-sensors-21-02234" ref-type="bibr">33</xref>]. It differs from the other methods as it classifies fixations based on predefined areas of interest. First, all gaze points within an AOI are classified as belonging to a fixation. Next, groups of fixation samples are identified as a fixation event using a minimum duration threshold. Short events are discarded.</p>
      <p>In addition, we provide two functions for post-processing of detected fixations: <italic>merging adjacent fixations</italic> and <italic>discarding short fixations</italic>. The <italic>merge adjacent fixations</italic> function merges subsequent fixations if the gap is smaller than a defined maximum duration and, depending on the detection algorithm used, a maximum angle between them (I-VT) or a maximum dispersion distance (I-DT). For I-AOI, the two fixations must belong to the same AOI. The <italic>discard short fixations</italic> function removes short fixations based on a minimum fixation duration and is mainly interesting for the I-VT method because both other methods inherently contain a minimum fixation duration.</p>
    </sec>
  </sec>
  <sec id="sec4-sensors-21-02234">
    <title>4. Evaluation of Accuracy and Precision</title>
    <p>High eye tracking data quality is important for eye tracking research because errors in the gaze estimation process can undermine the validity of reported results [<xref rid="B17-sensors-21-02234" ref-type="bibr">17</xref>]. However, for the integrated eye tracker of the Microsoft HoloLens 2 we only find limited information about spatial accuracy and no information about spatial precision [<xref rid="B26-sensors-21-02234" ref-type="bibr">26</xref>]. We conduct a user study to analyze the accuracy and precision of gaze data from the HoloLens 2 that is recorded using our toolkit. We ask participants to fixate a set of targets, which have a static position with respect to the participant’s head, at different distances. We record the gaze signal while the participants are seated (setting I) or walking (setting II). Further, we ask them to fixate a target with a static world position while moving around (setting III). The results can serve as a reference for researchers when designing eye tracking studies, e.g., to decide whether the accuracy is sufficient, or to influence the position and size of AOIs. In addition, our results can guide interaction designers that develop gaze-based AR applications, for example to improve gaze-based selection [<xref rid="B34-sensors-21-02234" ref-type="bibr">34</xref>].</p>
    <sec sec-type="subjects" id="sec4dot1-sensors-21-02234">
      <title>4.1. Participants</title>
      <p>In total, we recruited 21 participants (7 or <inline-formula><mml:math id="mm7"><mml:mrow><mml:mrow><mml:mn>33</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> female; mean age <inline-formula><mml:math id="mm8"><mml:mrow><mml:mrow><mml:mn>29.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm9"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>8.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) of which 15 participated in all three settings. Two participants skipped setting III and four participants finished setting III only. This totals to 17 participants for settings I and II (4 or <inline-formula><mml:math id="mm10"><mml:mrow><mml:mrow><mml:mn>24</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> female; mean age <inline-formula><mml:math id="mm11"><mml:mrow><mml:mrow><mml:mn>29.1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm12"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>8.6</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>), and 19 participants for setting III (7 or <inline-formula><mml:math id="mm13"><mml:mrow><mml:mrow><mml:mn>37</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> female; mean age 30, <inline-formula><mml:math id="mm14"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>8.8</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>). All participants had normal or corrected-to-normal vision with one participant wearing contact lenses and three participants wearing glasses.</p>
    </sec>
    <sec id="sec4dot2-sensors-21-02234">
      <title>4.2. Conditions &amp; Tasks</title>
      <p>In our study, we include three settings in which we record the participants’ gaze signal and the position of multiple fixation targets. In setting I and II, we show a planar 9-point grid of fixation targets (<inline-formula><mml:math id="mm15"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) that is centered in front of the participant’s head and orthogonal to the forward direction (<xref ref-type="fig" rid="sensors-21-02234-f003">Figure 3</xref>a). Participants are standing still in setting I, and walking forward and backward in setting II during the recording phase. For both settings, the grid size is aligned to the field of view of the device. The outer fixation targets are positioned at the border of the field of view such that both eyes can see them. The distances between the corner targets (upper left, upper right, lower left, lower right) and the center target are <inline-formula><mml:math id="mm16"><mml:mrow><mml:mrow><mml:mn>18.25</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees of visual angle. The distances for the edge targets (upper center, middle left, middle right, lower center) are <inline-formula><mml:math id="mm17"><mml:mrow><mml:mrow><mml:mn>12.13</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees of visual angle. In addition, we vary the distance <italic>d</italic> of the grid for both settings: we include <inline-formula><mml:math id="mm18"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0.5</mml:mn><mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">m</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">m</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">m</mml:mi><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mspace width="0.166667em"/><mml:mi mathvariant="normal">m</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For all distances, we ask the participants to fixate all targets for three seconds, starting on the upper left in a left-to-right and top-to-bottom direction. An example picture of the settings I and II can be found in <xref ref-type="fig" rid="sensors-21-02234-f004">Figure 4</xref>. For setting III, we place a single fixation target at a static position in the world coordinate system: we show a sphere with diameter of 1 cm 15 cm above the surface on a table with a height of 75 cm (<xref ref-type="fig" rid="sensors-21-02234-f003">Figure 3</xref>b). Participants are seated in front of the table and are asked to move their heads left and right while keeping up the fixation to the sphere. With this setting, we simulate vestibulo-ocular reflex movements that are common in natural experiment settings in which participants interact with stationary AR content.</p>
    </sec>
    <sec id="sec4dot3-sensors-21-02234">
      <title>4.3. Procedure</title>
      <p>All settings are recorded in one session, starting with setting I and immediately followed by setting II and III. The order of the settings was identical for all participants. In the beginning of a session, the participant puts on the device which is adjusted to the head by a supervisor. The device is fitted to a participant’s head such that it does not move during the experiment but is still comfortable to wear. If the participant feels that the device loosens, it is tightened by the supervisor. During the whole procedure, the device is not moved on or removed from the participant’s head. After fitting, the integrated eye tracker is calibrated using the built-in calibration routine. We record gaze data and reference target positions with our new toolkit. Each task is recorded separately, resulting in a recording per distance for setting I and II, and a single recording for setting III. For settings I and II, we perform a manual fixation detection and remove gaze samples that belong to a saccade event. We performed a manual annotation of the gaze signal to extract fixations more accurately than possible with automatic algorithms which have, in particular, problems with event detection in mobile eye tracking signals [<xref rid="B35-sensors-21-02234" ref-type="bibr">35</xref>]. Gaze samples are labeled as belonging to a fixation unless the gaze position moved away from the fixation center, i.e., when turning into a saccade which ends at the next fixation center. The labeling is based on visual inspections from one expert. For setting III, we remove gaze samples before the participant starts fixating the sphere and moving his/her head, and after the participant stops. The participant is asked by the supervisor to start the movement and, after four minutes, asked to stop moving and to return to the starting position.</p>
    </sec>
    <sec id="sec4dot4-sensors-21-02234">
      <title>4.4. Metrics</title>
      <p>We define spatial accuracy and precision according to the literature [<xref rid="B34-sensors-21-02234" ref-type="bibr">34</xref>,<xref rid="B36-sensors-21-02234" ref-type="bibr">36</xref>]. Per target, we compute spatial accuracy as the distance between the mean gaze sample and the target position. Spatial precision is computed as the standard deviation of the distances between each gaze sample and the mean position of all gaze samples. We report both measures in cm, as well as in degrees of visual angle. The distance in cm is calculated using the distance between the gaze point and the target based on their positions in the reference frame provided by Unity 3D. The visual angle is calculated as the angle between the reported 3D gaze ray from the gaze origin to the gaze point and the 3D ray from the gaze origin to the target position.</p>
    </sec>
    <sec id="sec4dot5-sensors-21-02234">
      <title>4.5. Hypotheses</title>
      <p>Previous research on the gaze estimation error in head-mounted eye tracking reported significant differences in the spatial accuracy for varying distances and when moving around versus resting [<xref rid="B18-sensors-21-02234" ref-type="bibr">18</xref>,<xref rid="B20-sensors-21-02234" ref-type="bibr">20</xref>]. We expect similar characteristics for the integrated eye tracker of the Microsoft HoloLens 2. Hence, we hypothesize that the spatial accuracy is dependent on the distance of the fixation target (H1). Further, we expect a lower accuracy for setting II in which participants move than for setting I in which they are resting (H2). Similar to H2, we expect that spatial precision is lower for setting II, i.e., when participants move (H3). For setting III, we exploratively investigate the spatial accuracy and precision for a realistic research setting from educational sciences.</p>
    </sec>
    <sec sec-type="results" id="sec4dot6-sensors-21-02234">
      <title>4.6. Results</title>
      <p>A total of 335,867 gaze points are recorded over all participants in all three settings and before filtering. Analyzing the relative timestamp provided by the device, the mean difference between timestamps is 33 ms (SD 1 ms). One hundred and seventy-one of these gaze points show a time difference to the previous gaze point larger than 34 ms, and 27 gaze points show a difference smaller than 32 ms. Those with a difference larger than 34 ms are multiples of the expected <inline-formula><mml:math id="mm19"><mml:mrow><mml:mrow><mml:mn>33.33</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> ms. All gaze points with a difference smaller than 32 ms have a difference of 0 ms. After removing the 198 gaze points with erroneous timing, we see a mean difference between timestamps of <inline-formula><mml:math id="mm20"><mml:mrow><mml:mrow><mml:mn>33.33</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> ms (SD <inline-formula><mml:math id="mm21"><mml:mrow><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> ms).</p>
      <p>For setting I, we report the metrics for all targets which include, on average, <inline-formula><mml:math id="mm22"><mml:mrow><mml:mrow><mml:mn>108.47</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm23"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>43.04</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) gaze points after saccade removal. <xref rid="sensors-21-02234-t002" ref-type="table">Table 2</xref> shows the spatial accuracy and precision per distance, averaged over all nine fixation targets and participants. The mean angular accuracy over all distances is <inline-formula><mml:math id="mm24"><mml:mrow><mml:mrow><mml:mn>0.83</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees with a precision of <inline-formula><mml:math id="mm25"><mml:mrow><mml:mrow><mml:mn>0.27</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees. <xref ref-type="fig" rid="sensors-21-02234-f005">Figure 5</xref> visualizes the error for individual targets per distance. A visualization of the analyzed gaze positions of one participant at the upper left target can be found in <xref ref-type="fig" rid="sensors-21-02234-f006">Figure 6</xref>. A Shapiro-Wilk test shows that the means of accuracies in degrees of visual angle over all targets is not distributed normally for all distances but <inline-formula><mml:math id="mm26"><mml:mrow><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m, <inline-formula><mml:math id="mm27"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm28"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm29"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.12</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm30"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>4.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. To evaluate the difference in spatial accuracy over all distances we conduct a Friedman test. It shows a significant difference in accuracy between the different distances, <inline-formula><mml:math id="mm31"><mml:mrow><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>20.15</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm32"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Post hoc analysis with Wilcoxon signed-rank tests is conducted with a Bonferroni correction applied, resulting in a significance level set at <inline-formula><mml:math id="mm33"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.008</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. It reveals a significant difference in the accuracy between the distance <inline-formula><mml:math id="mm34"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m and the distances <inline-formula><mml:math id="mm35"><mml:mrow><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m and <inline-formula><mml:math id="mm36"><mml:mrow><mml:mrow><mml:mn>3.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m (<xref rid="sensors-21-02234-t003" ref-type="table">Table 3</xref>).</p>
      <p>The recordings for setting II include an average of <inline-formula><mml:math id="mm37"><mml:mrow><mml:mrow><mml:mn>121.23</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm38"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>32.53</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) gaze samples per target. The mean spatial accuracy, averaged over participants and fixation targets per distance, is reported in <xref rid="sensors-21-02234-t004" ref-type="table">Table 4</xref>. The mean angular accuracy over all distances is <inline-formula><mml:math id="mm39"><mml:mrow><mml:mrow><mml:mn>1.77</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees with a precision of <inline-formula><mml:math id="mm40"><mml:mrow><mml:mrow><mml:mn>1.13</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees. The results per fixation target are visualized in <xref ref-type="fig" rid="sensors-21-02234-f007">Figure 7</xref>. A visualization of the analyzed gaze positions of one participant at the upper left target can be found in <xref ref-type="fig" rid="sensors-21-02234-f008">Figure 8</xref>. The mean accuracy in degrees of visual angle over all targets is distributed normally for the distances <inline-formula><mml:math id="mm41"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m and <inline-formula><mml:math id="mm42"><mml:mrow><mml:mrow><mml:mn>4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m, but not at <inline-formula><mml:math id="mm43"><mml:mrow><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m and <inline-formula><mml:math id="mm44"><mml:mrow><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m as assessed by a Shapiro-Wilk test, <inline-formula><mml:math id="mm45"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.44</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm46"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm47"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm48"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>4.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.35</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Analogue to setting I we conduct a Friedman test to evaluate the difference in spatial accuracy over all distances. It shows a significant difference in accuracy between the different distances, <inline-formula><mml:math id="mm49"><mml:mrow><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mn>3</mml:mn><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>37.02</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm50"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. The Bonferroni corrected post hoc analysis with Wilcoxon signed-rank tests results in a significance level set at <inline-formula><mml:math id="mm51"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.008</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. It reveals a significant difference in spatial accuracy for all paired comparisons except for the distances <inline-formula><mml:math id="mm52"><mml:mrow><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m and <inline-formula><mml:math id="mm53"><mml:mrow><mml:mrow><mml:mn>4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m (<xref rid="sensors-21-02234-t005" ref-type="table">Table 5</xref>).</p>
      <p>In addition, we compare the spatial accuracy and precision results between setting I (resting) and setting II (walking). The differences in accuracy are not distributed normally for the distances <inline-formula><mml:math id="mm54"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m and <inline-formula><mml:math id="mm55"><mml:mrow><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m as assessed by a Shapiro-Wilk test, <inline-formula><mml:math id="mm56"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm57"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.003</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm58"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.26</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm59"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>4.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.44</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. A Wilcoxon signed-rank test shows that the accuracy differs significantly between setting I and II for all distances (<xref rid="sensors-21-02234-t006" ref-type="table">Table 6</xref>). The difference in precision is distributed normally for the distance of <inline-formula><mml:math id="mm60"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m but not for the other distances as assessed by a Shapiro-Wilk test, <inline-formula><mml:math id="mm61"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.44</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm62"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0.001</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm63"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.046</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="mm64"><mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mn>4.0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.003</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. A Wilcoxon signed-rank test shows that the precision differs significantly between setting I and II for all distances (<xref rid="sensors-21-02234-t007" ref-type="table">Table 7</xref>).</p>
      <p>For setting III, we include a mean of <inline-formula><mml:math id="mm65"><mml:mrow><mml:mrow><mml:mn>641.79</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="mm66"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>262.10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) gaze samples per participant for our analysis. The resulting accuracy and precision values together with the mean distance of the participants from the target can be found in <xref rid="sensors-21-02234-t008" ref-type="table">Table 8</xref>. We approximate the spatial accuracy in degrees of visual angle as using the following formula: <inline-formula><mml:math id="mm67"><mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>O</mml:mi><mml:mo>/</mml:mo><mml:mi>d</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with the accuracy in cm as <italic>O</italic> and the mean distance to the participant <italic>d</italic>. The same formula is used to calculate the precision by using the precision in cm as <italic>O</italic>. A 3D visualization of the analyzed gaze positions of one participant can be found in <xref ref-type="fig" rid="sensors-21-02234-f009">Figure 9</xref>.</p>
    </sec>
  </sec>
  <sec sec-type="discussion" id="sec5-sensors-21-02234">
    <title>5. Discussion</title>
    <p>The major goal of developing the augmented reality eye tracking toolkit is to enable researchers to easily use eye tracking in AR settings with the HoloLens 2. It should allow an efficient integration to Unity 3D scenes, enable recordings of a comprehensive set of eye tracking signals (see <xref rid="sensors-21-02234-t001" ref-type="table">Table 1</xref>), and a seamless analysis of the data via our R package. This would simplify integration of eye tracking into existing AR research, like Strzys et al. [<xref rid="B37-sensors-21-02234" ref-type="bibr">37</xref>] and Kapp et al. [<xref rid="B38-sensors-21-02234" ref-type="bibr">38</xref>]. Independently from the study reported in this publication, our toolkit is currently being used in two ongoing research studies which provide first evidences in this direction. One study utilizes the Microsoft HoloLens 2 to display two dimensional plots at a fixed distance while the participant is moving while another study investigates stationary augmentations on a table. The initial feedback from the study organizers, the developers of the AR application, and the experimenters is positive. No major issues occurred during the recordings, which certifies a high robustness, and the ease-of-use of the web interface was, informally, rated high.</p>
    <p>Our toolkit can also be used for facilitating gaze-based interaction and real-time adaptive applications using the data provider module. For instance, prior research proposed to use eye tracking and HMDs to augment the episodic memory of dementia patients by storing artificial memory sequences and presenting them when needed [<xref rid="B39-sensors-21-02234" ref-type="bibr">39</xref>]. Other works include approaches for gaze-based analysis of the users’ attention engagement and cognitive states for proactive content visualization [<xref rid="B40-sensors-21-02234" ref-type="bibr">40</xref>], and multi-focal plane interaction, such as object selection and manipulation at multiple fixation distances [<xref rid="B41-sensors-21-02234" ref-type="bibr">41</xref>]. It can also be used in research regarding selection techniques in AR [<xref rid="B42-sensors-21-02234" ref-type="bibr">42</xref>,<xref rid="B43-sensors-21-02234" ref-type="bibr">43</xref>]. The utility of the toolkit for realizing real-time adaptive applications has been shown in Reference [<xref rid="B44-sensors-21-02234" ref-type="bibr">44</xref>]. The presented prototype uses video and gaze information via our toolkit to automatically recognize and augment attended objects in an uninstrumented environment.</p>
    <sec id="sec5dot1-sensors-21-02234">
      <title>5.1. Evaluation of Accuracy and Precision</title>
      <p>The results from our evaluation show significant differences in spatial accuracy for varying distances in setting I and II. This supports our hypothesis H1. However, for setting I, the pairwise comparisons reveal that only the results for the smallest distance <inline-formula><mml:math id="mm68"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m and the distances <inline-formula><mml:math id="mm69"><mml:mrow><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm70"><mml:mrow><mml:mrow><mml:mn>4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m differ significantly. For setting II, the results significantly differ for all pairs except for the two farthest distances of <inline-formula><mml:math id="mm71"><mml:mrow><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m and <inline-formula><mml:math id="mm72"><mml:mrow><mml:mrow><mml:mn>4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m. Further, our results confirm the hypothesis H2 and H3: the accuracies and precision for each distance differ significantly between setting I and setting II while the results for setting II are poorer.</p>
      <p>Our observations also show that the spatial accuracy in degrees of visual angle increases with increasing distance (see <xref rid="sensors-21-02234-t002" ref-type="table">Table 2</xref> and <xref rid="sensors-21-02234-t004" ref-type="table">Table 4</xref>). Findings from the literature suggest that the accuracy decreases with increasing deviation from the calibration distance, i.e., the distance at which the fixation targets of the calibration routine are shown [<xref rid="B18-sensors-21-02234" ref-type="bibr">18</xref>,<xref rid="B20-sensors-21-02234" ref-type="bibr">20</xref>,<xref rid="B45-sensors-21-02234" ref-type="bibr">45</xref>]. This leads to our assumption that the built-in calibration routine of HoloLens 2 is placed at 2 to 4 m from the user, which is supported by the fact that Microsoft recommends an interaction distance of 2 m [<xref rid="B46-sensors-21-02234" ref-type="bibr">46</xref>]. It is possible that this increase in angular accuracy is an effect of the vergence-accommodation conflict [<xref rid="B47-sensors-21-02234" ref-type="bibr">47</xref>] as only a combined gaze ray is made available by the device.</p>
      <p>The official HoloLens 2 documentation reports a vague range for the spatial accuracy of “approximately within 1.5 degrees” with “slight imperfections” to be expected [<xref rid="B26-sensors-21-02234" ref-type="bibr">26</xref>]. Basically, our results coincide with these specifications, but are much more fine-grained. For the resting setting (I), we observe better spatial accuracy values ranging from <inline-formula><mml:math id="mm73"><mml:mrow><mml:mrow><mml:mn>1.00</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees of visual angle for a <inline-formula><mml:math id="mm74"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m distance to <inline-formula><mml:math id="mm75"><mml:mrow><mml:mrow><mml:mn>0.68</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees for <inline-formula><mml:math id="mm76"><mml:mrow><mml:mrow><mml:mn>4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m. For the walking setting (II), which has a lower spatial accuracy overall, the results for <inline-formula><mml:math id="mm77"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m and <inline-formula><mml:math id="mm78"><mml:mrow><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m are outside the official range with <inline-formula><mml:math id="mm79"><mml:mrow><mml:mrow><mml:mn>2.52</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm80"><mml:mrow><mml:mrow><mml:mn>1.84</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees of visual angle, respectively. The two other conditions lie within the specified boundary of <inline-formula><mml:math id="mm81"><mml:mrow><mml:mrow><mml:mn>1.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> degrees. The documented sampling rate of “approximately 30 Hz” was also met with a new gaze sample being observed every <inline-formula><mml:math id="mm82"><mml:mrow><mml:mrow><mml:mn>33.33</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> ms.</p>
      <p>Based on our findings, we suggest minimum target sizes for eye tracking research and gaze-based interaction with the HoloLens 2. Similar to Feit et al. [<xref rid="B34-sensors-21-02234" ref-type="bibr">34</xref>], who investigate the gaze estimation error for remote eye tracking, we calculate the minimum size such that <inline-formula><mml:math id="mm83"><mml:mrow><mml:mrow><mml:mn>95</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of all gaze samples hit the target. We use their formula that computes the minimum size based on a 2-dimensional Gaussian function as <inline-formula><mml:math id="mm84"><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>(</mml:mo><mml:mi>O</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>σ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with the spatial accuracy of the eye tracker as offset <italic>O</italic> and the spatial precision of the gaze signal as <inline-formula><mml:math id="mm85"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula>. The resulting minimum target sizes for varying distances are listed in <xref rid="sensors-21-02234-t009" ref-type="table">Table 9</xref>. For a distance of <inline-formula><mml:math id="mm86"><mml:mrow><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m, Microsoft recommends a target size of 5–10 cm, which conforms with our findings for setting I: we suggest a target size of <inline-formula><mml:math id="mm87"><mml:mrow><mml:mrow><mml:mn>11.10</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> cm in this case. However, if the participant is meant to move around, the targets should be significantly larger.</p>
      <p>In setting III, we explore the characteristics of the gaze estimation error for stationary targets. The average distance to the stationary target of <inline-formula><mml:math id="mm88"><mml:mrow><mml:mrow><mml:mn>49.87</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> cm is comparable to the <inline-formula><mml:math id="mm89"><mml:mrow><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m distance in setting I. However, the mean spatial accuracy is better and the precision is lower. This better result for spatial accuracy could be explained by the longer fixation durations and the varying viewing angles in setting III: on average, the mean gaze positions seem to balance around the fixation target, while the dispersion stays high (see <xref ref-type="fig" rid="sensors-21-02234-f009">Figure 9</xref>). Based on Feit et al. [<xref rid="B34-sensors-21-02234" ref-type="bibr">34</xref>], we suggest a minimum target size of <inline-formula><mml:math id="mm90"><mml:mrow><mml:mrow><mml:mn>4.16</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> cm. This is <inline-formula><mml:math id="mm91"><mml:mrow><mml:mrow><mml:mn>22</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> larger than the recommendation for setting I, and <inline-formula><mml:math id="mm92"><mml:mrow><mml:mrow><mml:mn>34</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the recommended size for setting II. Altogether, the results suggest that the fixation duration and the user condition, i.e., walking versus not walking, influences the spatial accuracy and precision, which should be considered when designing interactive and, potentially, mobile research applications.</p>
      <p>Finally, we compare the results of the HoloLens 2 eye tracker to available head-mounted eye trackers without an HMD. Macinnes et al. [<xref rid="B48-sensors-21-02234" ref-type="bibr">48</xref>] evaluated the spatial accuracy and precision of three mobile eye trackers for multiple distances while participants were seated. They included (i) the Pupil Labs 120 Hz Binocular glasses with an accuracy of <inline-formula><mml:math id="mm93"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>84</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and a precision of <inline-formula><mml:math id="mm94"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>16</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, (ii) the SensoMotoric Instruments (SMI) Eye Tracking Glasses 2 with an accuracy of <inline-formula><mml:math id="mm95"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>21</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and a precision of <inline-formula><mml:math id="mm96"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>19</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, and (iii) the Tobii Pro Glasses 2 with an accuracy of <inline-formula><mml:math id="mm97"><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>42</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and a precision of <inline-formula><mml:math id="mm98"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>34</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. On average, our results for the HoloLens 2 in setting I, which is the closest match to the setting in Reference [<xref rid="B48-sensors-21-02234" ref-type="bibr">48</xref>], yield an accuracy of <inline-formula><mml:math id="mm99"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>83</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and a precision of <inline-formula><mml:math id="mm100"><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>.</mml:mo><mml:msup><mml:mn>27</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. This is similar to the results of the Pupil Labs glasses that ranged best in the experiment by Macinnes et al. [<xref rid="B48-sensors-21-02234" ref-type="bibr">48</xref>] and suggests that the eye tracking data from HoloLens 2 can effectively be used in research experiments. However, one drawback is that the sampling rate of 30 Hz is lower compared to the devices evaluated in their experiment.</p>
    </sec>
    <sec id="sec5dot2-sensors-21-02234">
      <title>5.2. Limitations</title>
      <p>Our toolkit enables access to raw gaze data and provides additional tools for processing them. However, it is limited to the data that is made available through APIs of the device. For instance, the API reports a joint gaze vector for both eyes, while many commercial binocular eye tracking glasses report separate gaze rays. This forces to intersect the gaze ray with the virtual environment to receive a point of gaze. Separate rays can be intersected to extract gaze points without intersecting any surface, and to infer the fixation depth. In addition, this gaze point can be used to find close-by AOIs. Our evaluation focuses on limited set of interaction settings that probably do not generalize to all possible settings in AR environments. However, with setting III, we include a more realistic setting that closer matches typical AR environments with a moving user and fixed visualizations. We cannot rule out effects due to the experiment order as it was identical for all participants.</p>
      <p>Currently, our toolkit is constrained to the Microsoft HoloLens 2 as eye tracking device. However, all device specific functionality is encapsulated in the data access layer. This makes it possible to adapt the toolkit to other eye tracking enabled AR devices in the future. However, the gaze estimation is device-specific: the results from our evaluation on spatial accuracy and spatial precision do not hold for other devices. In addition, the sampling rate might change which needs to be addressed by re-configuring the data pulling rate. The data access layer could also subscribe gaze events or connect to a signal stream, if this is supported by the new device.</p>
    </sec>
  </sec>
  <sec sec-type="conclusions" id="sec6-sensors-21-02234">
    <title>6. Conclusions</title>
    <p>In this work, we presented an open-source toolkit that enables eye tracking research in AR using the HoloLens 2 device. We addressed the gap of missing research tools by implementing a Unity 3D package for reliable gaze data acquisition and an R package for seamless data analysis. We received first positive feedback on our toolkit from two other research studies, proving its utility. We conducted a user study (<inline-formula><mml:math id="mm101"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) to investigate the spatial accuracy and spatial precision of gaze data from our toolkit. The results suggest that the spatial accuracy increases when increasing the distance of fixation targets. Further, we found evidence that spatial accuracy and precision drop when participants are walking compared to standing still. Overall, the gaze estimation error is similar to recent head-mounted eye trackers without HMDs which shows the suitability of our toolkit for research applications. In future updates we will address the limitations of our toolkit as follows. We plan to add fully integrated support for video recording of the integrated camera using the data logger, as well as real-time streaming of video and gaze data. We will also investigate the effectiveness of attaching virtual AOIs to real objects for real-time gaze-to-AOI mapping. Further, we want to extend the functionality of our R package and integrate interfaces to existing gaze data processing tools, as well as integrate data access layers for other devices.</p>
  </sec>
</body>
<back>
  <fn-group>
    <fn>
      <p><bold>Publisher’s Note:</bold> MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
    </fn>
  </fn-group>
  <notes>
    <title>Author Contributions</title>
    <p>Conceptualization, methodology, software, formal analysis, data curation, visualization, supervision, S.K.; investigation, S.K. and S.M.; writing—original draft preparation, S.K. and M.B.; writing—review and editing, S.K., M.B., S.M., D.S., J.K.; resources, J.K.; project administration, funding acquisition, D.S. and J.K. All authors have read and agreed to the published version of the manuscript.</p>
  </notes>
  <notes>
    <title>Funding</title>
    <p>This research was funded by the German Federal Ministry of Education and Research (Bundesministerium für Bildung und Forschung; BMBF) via the project “GeAR” (Grant No. 01JD1811B and 01JD1811C).</p>
  </notes>
  <notes>
    <title>Institutional Review Board Statement</title>
    <p>Not applicable.</p>
  </notes>
  <notes>
    <title>Informed Consent Statement</title>
    <p>Informed consent was obtained from all subjects involved in the study.</p>
  </notes>
  <notes notes-type="data-availability">
    <title>Data Availability Statement</title>
    <p>The data presented in this study are available on request from the corresponding author. The data are not publicly available due to data privacy.</p>
  </notes>
  <notes notes-type="COI-statement">
    <title>Conflicts of Interest</title>
    <p>The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.</p>
  </notes>
  <ref-list>
    <title>References</title>
    <ref id="B1-sensors-21-02234">
      <label>1.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Majaranta</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Bulling</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Eye Tracking and Eye-Based Human–Computer Interaction</article-title>
        <source>Advances in Physiological Computing</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Fairclough</surname>
            <given-names>S.H.</given-names>
          </name>
          <name>
            <surname>Gilleade</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <series>Human–Computer Interaction Series</series>
        <publisher-name>Springer</publisher-name>
        <publisher-loc>London, UK</publisher-loc>
        <year>2014</year>
        <fpage>39</fpage>
        <lpage>65</lpage>
        <pub-id pub-id-type="doi">10.1007/978-1-4471-6392-3_3</pub-id>
      </element-citation>
    </ref>
    <ref id="B2-sensors-21-02234">
      <label>2.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Blattgerste</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Renner</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Pfeiffer</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>Advantages of eye-gaze over head-gaze-based selection in virtual and augmented reality under varying field of views</article-title>
        <source>Proceedings of the Workshop on Communication by Gaze Interaction—COGAIN ’18</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Morimoto</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Pfeiffer</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <publisher-name>ACM Press</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2018</year>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1145/3206343.3206349</pub-id>
      </element-citation>
    </ref>
    <ref id="B3-sensors-21-02234">
      <label>3.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Guenter</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Finch</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Drucker</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Tan</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Snyder</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <article-title>Foveated 3D graphics</article-title>
        <source>ACM Trans. Graph.</source>
        <year>2012</year>
        <volume>31</volume>
        <fpage>1</fpage>
        <lpage>10</lpage>
        <pub-id pub-id-type="doi">10.1145/2366145.2366183</pub-id>
      </element-citation>
    </ref>
    <ref id="B4-sensors-21-02234">
      <label>4.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Patney</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Salvi</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kim</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kaplanyan</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Wyman</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Benty</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Luebke</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Lefohn</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Towards foveated rendering for gaze-tracked virtual reality</article-title>
        <source>ACM Trans. Graph.</source>
        <year>2016</year>
        <volume>35</volume>
        <fpage>1</fpage>
        <lpage>12</lpage>
        <pub-id pub-id-type="doi">10.1145/2980179.2980246</pub-id>
      </element-citation>
    </ref>
    <ref id="B5-sensors-21-02234">
      <label>5.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Tobii Pro AB</collab>
        </person-group>
        <article-title>Pro Lab User Manual</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.tobiipro.com/siteassets/tobii-pro/user-manuals/Tobii-Pro-Lab-User-Manual/?v=1.152">https://www.tobiipro.com/siteassets/tobii-pro/user-manuals/Tobii-Pro-Lab-User-Manual/?v=1.152</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-12">(accessed on 12 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B6-sensors-21-02234">
      <label>6.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Pupil Labs</collab>
        </person-group>
        <article-title>Add Awareness to Your VR/AR Experience: Integrate and React</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://pupil-labs.com/products/vr-ar/">https://pupil-labs.com/products/vr-ar/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-20">(accessed on 20 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B7-sensors-21-02234">
      <label>7.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Tobii VR</collab>
        </person-group>
        <article-title>Tobii VR: Discover New Possibilities with Eye Tracking in VR</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://vr.tobii.com/">https://vr.tobii.com/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-20">(accessed on 20 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B8-sensors-21-02234">
      <label>8.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Stratmann</surname>
            <given-names>T.C.</given-names>
          </name>
          <name>
            <surname>Gruenefeld</surname>
            <given-names>U.</given-names>
          </name>
          <name>
            <surname>Boll</surname>
            <given-names>S.</given-names>
          </name>
        </person-group>
        <article-title>EyeMR—Low-cost Eye-Tracking for Rapid-prototyping in Head-mounted Mixed Reality</article-title>
        <source>Proceedings of the 2018 ACM Symposium on Eye Tracking Research &amp; Applications</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Sharif</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Krejtz</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2018</year>
        <fpage>1</fpage>
        <lpage>2</lpage>
        <pub-id pub-id-type="doi">10.1145/3204493.3208336</pub-id>
      </element-citation>
    </ref>
    <ref id="B9-sensors-21-02234">
      <label>9.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Lee</surname>
            <given-names>K.F.</given-names>
          </name>
          <name>
            <surname>Chen</surname>
            <given-names>Y.L.</given-names>
          </name>
          <name>
            <surname>Yu</surname>
            <given-names>C.W.</given-names>
          </name>
          <name>
            <surname>Chin</surname>
            <given-names>K.Y.</given-names>
          </name>
          <name>
            <surname>Wu</surname>
            <given-names>C.H.</given-names>
          </name>
        </person-group>
        <article-title>Gaze Tracking and Point Estimation Using Low-Cost Head-Mounted Devices</article-title>
        <source>Sensors</source>
        <year>2020</year>
        <volume>20</volume>
        <elocation-id>1917</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s20071917</pub-id>
        <?supplied-pmid 32235523?>
        <pub-id pub-id-type="pmid">32235523</pub-id>
      </element-citation>
    </ref>
    <ref id="B10-sensors-21-02234">
      <label>10.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mardanbegi</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Pfeiffer</surname>
            <given-names>T.</given-names>
          </name>
        </person-group>
        <article-title>EyeMRTK: A Toolkit for Developing Eye Gaze Interactive Applications in Virtual and Augmented Reality</article-title>
        <source>Proceedings of the 11th ACM Symposium on Eye Tracking Research &amp; Applications</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Krejtz</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Sharif</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2019</year>
        <fpage>1</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1145/3317956.3318155</pub-id>
      </element-citation>
    </ref>
    <ref id="B11-sensors-21-02234">
      <label>11.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Adhanom</surname>
            <given-names>I.B.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>S.C.</given-names>
          </name>
          <name>
            <surname>Folmer</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>MacNeilage</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <article-title>GazeMetrics: An Open-Source Tool for Measuring the Data Quality of HMD-based Eye Trackers</article-title>
        <source>ACM Symposium on Eye Tracking Research and Applications</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Bulling</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Huckauf</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Jain</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Radach</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Weiskopf</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1145/3379156.3391374</pub-id>
      </element-citation>
    </ref>
    <ref id="B12-sensors-21-02234">
      <label>12.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Magic Leap</collab>
        </person-group>
        <article-title>Magic Leap 1: A Thousand Breakthroughs in One</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.magicleap.com/en-us/magic-leap-1">https://www.magicleap.com/en-us/magic-leap-1</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-20">(accessed on 20 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B13-sensors-21-02234">
      <label>13.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Microsoft</collab>
        </person-group>
        <article-title>HoloLens 2: A New Reality for Computing</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.microsoft.com/en-us/hololens">https://www.microsoft.com/en-us/hololens</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-20">(accessed on 20 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B14-sensors-21-02234">
      <label>14.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Microsoft</collab>
        </person-group>
        <article-title>Eye Tracking in the Mixed Reality Toolkit</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html">https://microsoft.github.io/MixedRealityToolkit-Unity/Documentation/EyeTracking/EyeTracking_Main.html</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-17">(accessed on 17 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B15-sensors-21-02234">
      <label>15.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Magic Leap</collab>
        </person-group>
        <article-title>Eye Gaze</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://developer.magicleap.com/en-us/learn/guides/design-eye-gaze">https://developer.magicleap.com/en-us/learn/guides/design-eye-gaze</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-20">(accessed on 20 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B16-sensors-21-02234">
      <label>16.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Hausamann</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Sinnott</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>MacNeilage</surname>
            <given-names>P.R.</given-names>
          </name>
        </person-group>
        <article-title>Positional head-eye tracking outside the lab: An open-source solution</article-title>
        <source>ACM Symposium on Eye Tracking Research and Applications</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Bulling</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Huckauf</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Jain</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Radach</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Weiskopf</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>5</lpage>
        <pub-id pub-id-type="doi">10.1145/3379156.3391365</pub-id>
      </element-citation>
    </ref>
    <ref id="B17-sensors-21-02234">
      <label>17.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Holmqvist</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Andersson</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <source>Eye Tracking: A Comprehensive Guide to Methods, Paradigms and Measures</source>
        <publisher-name>Lund Eye-Tracking Research Institute</publisher-name>
        <publisher-loc>Lund, Sweden</publisher-loc>
        <year>2011</year>
      </element-citation>
    </ref>
    <ref id="B18-sensors-21-02234">
      <label>18.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Mardanbegi</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Hansen</surname>
            <given-names>D.W.</given-names>
          </name>
        </person-group>
        <article-title>Parallax error in the monocular head-mounted eye trackers</article-title>
        <source>Proceedings of the 2012 ACM Conference on Ubiquitous Computing</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2012</year>
        <fpage>689</fpage>
        <lpage>694</lpage>
        <pub-id pub-id-type="doi">10.1145/2370216.2370366</pub-id>
      </element-citation>
    </ref>
    <ref id="B19-sensors-21-02234">
      <label>19.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Barz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Stauden</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Sonntag</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Visual Search Target Inference in Natural Interaction Settings with Machine Learning</article-title>
        <source>Proceedings of the 2020 ACM Symposium on Eye Tracking Research &amp; Applications</source>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2020</year>
        <fpage>1</fpage>
        <lpage>8</lpage>
        <pub-id pub-id-type="doi">10.1145/3379155.3391314</pub-id>
      </element-citation>
    </ref>
    <ref id="B20-sensors-21-02234">
      <label>20.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Barz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Daiber</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Bulling</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Prediction of Gaze Estimation Error for Error-Aware Gaze-Based Interfaces</article-title>
        <source>Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &amp; Applications</source>
        <publisher-name>ACM Press</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2016</year>
        <fpage>275</fpage>
        <lpage>278</lpage>
        <pub-id pub-id-type="doi">10.1145/2857491.2857493</pub-id>
      </element-citation>
    </ref>
    <ref id="B21-sensors-21-02234">
      <label>21.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Holmqvist</surname>
            <given-names>K.</given-names>
          </name>
          <name>
            <surname>Nyström</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Mulvey</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <article-title>Eye tracker data quality: What it is and how to measure it</article-title>
        <source>Proceedings of the Symposium on Eye Tracking Research and Applications</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2012</year>
        <fpage>45</fpage>
        <lpage>52</lpage>
        <pub-id pub-id-type="doi">10.1145/2168556.2168563</pub-id>
      </element-citation>
    </ref>
    <ref id="B22-sensors-21-02234">
      <label>22.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Barz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Bulling</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Daiber</surname>
            <given-names>F.</given-names>
          </name>
        </person-group>
        <source>Computational Modelling and Prediction of Gaze Estimation Error for Head-Mounted Eye Trackers</source>
        <comment>Technical Report</comment>
        <publisher-name>DFKI</publisher-name>
        <publisher-loc>Kaiserslautern, Germany</publisher-loc>
        <year>2015</year>
      </element-citation>
    </ref>
    <ref id="B23-sensors-21-02234">
      <label>23.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Unity Technologies</collab>
        </person-group>
        <article-title>Unity Real-Time Development Platform|3D, 2D VR &amp; AR Engine</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://unity.com/">https://unity.com/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-02-23">(accessed on 23 February 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B24-sensors-21-02234">
      <label>24.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>The R Foundation</collab>
        </person-group>
        <article-title>R: The R Project for Statistical Computing</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.r-project.org/">https://www.r-project.org/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2021-02-23">(accessed on 23 February 2021)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B25-sensors-21-02234">
      <label>25.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Microsoft</collab>
        </person-group>
        <article-title>EyesPose Class</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://docs.microsoft.com/de-de/uwp/api/windows.perception.people.eyespose?view=winrt-19041">https://docs.microsoft.com/de-de/uwp/api/windows.perception.people.eyespose?view=winrt-19041</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-17">(accessed on 17 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B26-sensors-21-02234">
      <label>26.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Microsoft</collab>
        </person-group>
        <article-title>Eye Tracking on HoloLens 2</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://docs.microsoft.com/en-us/windows/mixed-reality/design/eye-tracking">https://docs.microsoft.com/en-us/windows/mixed-reality/design/eye-tracking</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-12">(accessed on 12 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B27-sensors-21-02234">
      <label>27.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Microsoft</collab>
        </person-group>
        <article-title>Create Mixed Reality Photos and Videos</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://docs.microsoft.com/en-us/hololens/holographic-photos-and-videos">https://docs.microsoft.com/en-us/hololens/holographic-photos-and-videos</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-13">(accessed on 13 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B28-sensors-21-02234">
      <label>28.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kassner</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Patera</surname>
            <given-names>W.</given-names>
          </name>
          <name>
            <surname>Bulling</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction</article-title>
        <source>Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing Adjunct Publication—UbiComp ’14 Adjunct</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Brush</surname>
            <given-names>A.J.</given-names>
          </name>
          <name>
            <surname>Friday</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kientz</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Scott</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Song</surname>
            <given-names>J.</given-names>
          </name>
        </person-group>
        <publisher-name>ACM Press</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2014</year>
        <fpage>1151</fpage>
        <lpage>1160</lpage>
        <pub-id pub-id-type="doi">10.1145/2638728.2641695</pub-id>
      </element-citation>
    </ref>
    <ref id="B29-sensors-21-02234">
      <label>29.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <name>
            <surname>Dink</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Ferguson</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>eyetrackingR: An R Library for Eye-tracking Data Analysis</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="http://www.eyetracking-r.com/">http://www.eyetracking-r.com/</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-24">(accessed on 24 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B30-sensors-21-02234">
      <label>30.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Zhegallo</surname>
            <given-names>A.V.</given-names>
          </name>
          <name>
            <surname>Marmalyuk</surname>
            <given-names>P.A.</given-names>
          </name>
        </person-group>
        <article-title>ETRAN–R Extension Package for Eye Tracking Results Analysis</article-title>
        <source>Perception</source>
        <year>2015</year>
        <volume>44</volume>
        <fpage>1129</fpage>
        <lpage>1135</lpage>
        <pub-id pub-id-type="doi">10.1177/0301006615594944</pub-id>
        <?supplied-pmid 26562926?>
        <pub-id pub-id-type="pmid">26562926</pub-id>
      </element-citation>
    </ref>
    <ref id="B31-sensors-21-02234">
      <label>31.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <name>
            <surname>Olsen</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>The Tobii I-VT Fixation Filter: Algorithm description</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://www.tobiipro.com/siteassets/tobii-pro/learn-and-support/analyze/how-do-we-classify-eye-movements/tobii-pro-i-vt-fixation-filter.pdf/?v=2012">https://www.tobiipro.com/siteassets/tobii-pro/learn-and-support/analyze/how-do-we-classify-eye-movements/tobii-pro-i-vt-fixation-filter.pdf/?v=2012</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-12">(accessed on 12 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B32-sensors-21-02234">
      <label>32.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Llanes-Jurado</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Marín-Morales</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Guixeres</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Alcañiz</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Development and Calibration of an Eye-Tracking Fixation Identification Algorithm for Immersive Virtual Reality</article-title>
        <source>Sensors</source>
        <year>2020</year>
        <volume>20</volume>
        <elocation-id>4956</elocation-id>
        <pub-id pub-id-type="doi">10.3390/s20174956</pub-id>
        <?supplied-pmid 32883026?>
        <pub-id pub-id-type="pmid">32883026</pub-id>
      </element-citation>
    </ref>
    <ref id="B33-sensors-21-02234">
      <label>33.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Salvucci</surname>
            <given-names>D.D.</given-names>
          </name>
          <name>
            <surname>Goldberg</surname>
            <given-names>J.H.</given-names>
          </name>
        </person-group>
        <article-title>Identifying Fixations and Saccades in Eye-Tracking Protocols</article-title>
        <source>Proceedings of the Eye Tracking Research &amp; Applications Symposium 2000 Palm Beach Gardens, FL, November 6–8, 2000</source>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2000</year>
        <pub-id pub-id-type="doi">10.1145/355017.355028</pub-id>
      </element-citation>
    </ref>
    <ref id="B34-sensors-21-02234">
      <label>34.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Feit</surname>
            <given-names>A.M.</given-names>
          </name>
          <name>
            <surname>Williams</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Toledo</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Paradiso</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Kulkarni</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Kane</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Morris</surname>
            <given-names>M.R.</given-names>
          </name>
        </person-group>
        <article-title>Toward Everyday Gaze Input</article-title>
        <source>Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Mark</surname>
            <given-names>G.</given-names>
          </name>
          <name>
            <surname>Fussell</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Lampe</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Schraefel</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Hourcade</surname>
            <given-names>J.P.</given-names>
          </name>
          <name>
            <surname>Appert</surname>
            <given-names>C.</given-names>
          </name>
          <name>
            <surname>Wigdor</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2017</year>
        <fpage>1118</fpage>
        <lpage>1130</lpage>
        <pub-id pub-id-type="doi">10.1145/3025453.3025599</pub-id>
      </element-citation>
    </ref>
    <ref id="B35-sensors-21-02234">
      <label>35.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Steil</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Huang</surname>
            <given-names>M.X.</given-names>
          </name>
          <name>
            <surname>Bulling</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Fixation detection for head-mounted eye tracking based on visual similarity of gaze targets</article-title>
        <source>Eye Tracking Research and Applications Symposium (ETRA)</source>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2018</year>
        <fpage>1</fpage>
        <lpage>9</lpage>
        <pub-id pub-id-type="doi">10.1145/3204493.3204538</pub-id>
      </element-citation>
    </ref>
    <ref id="B36-sensors-21-02234">
      <label>36.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Duchowski</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Medlin</surname>
            <given-names>E.</given-names>
          </name>
          <name>
            <surname>Cournia</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Murphy</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Gramopadhye</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Nair</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Vorah</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Melloy</surname>
            <given-names>B.</given-names>
          </name>
        </person-group>
        <article-title>3-D eye movement analysis</article-title>
        <source>Behav. Res. Methods Instrum. Comput.</source>
        <year>2002</year>
        <volume>34</volume>
        <fpage>573</fpage>
        <lpage>591</lpage>
        <pub-id pub-id-type="doi">10.3758/BF03195486</pub-id>
        <?supplied-pmid 12564561?>
        <pub-id pub-id-type="pmid">12564561</pub-id>
      </element-citation>
    </ref>
    <ref id="B37-sensors-21-02234">
      <label>37.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Strzys</surname>
            <given-names>M.P.</given-names>
          </name>
          <name>
            <surname>Kapp</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Thees</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kuhn</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Lukowicz</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Knierim</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Schmidt</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <article-title>Augmenting the thermal flux experiment: A mixed reality approach with the HoloLens</article-title>
        <source>Phys. Teach.</source>
        <year>2017</year>
        <volume>55</volume>
        <fpage>376</fpage>
        <lpage>377</lpage>
        <pub-id pub-id-type="doi">10.1119/1.4999739</pub-id>
      </element-citation>
    </ref>
    <ref id="B38-sensors-21-02234">
      <label>38.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kapp</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Thees</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Strzys</surname>
            <given-names>M.P.</given-names>
          </name>
          <name>
            <surname>Beil</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Kuhn</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Amiraslanov</surname>
            <given-names>O.</given-names>
          </name>
          <name>
            <surname>Javaheri</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Lukowicz</surname>
            <given-names>P.</given-names>
          </name>
          <name>
            <surname>Lauer</surname>
            <given-names>F.</given-names>
          </name>
          <name>
            <surname>Rheinländer</surname>
            <given-names>C.</given-names>
          </name>
          <etal/>
        </person-group>
        <article-title>Augmenting Kirchhoff’s laws: Using augmented reality and smartglasses to enhance conceptual electrical experiments for high school students</article-title>
        <source>Phys. Teach.</source>
        <year>2019</year>
        <volume>57</volume>
        <fpage>52</fpage>
        <lpage>53</lpage>
        <pub-id pub-id-type="doi">10.1119/1.5084931</pub-id>
      </element-citation>
    </ref>
    <ref id="B39-sensors-21-02234">
      <label>39.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Orlosky</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Toyama</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sonntag</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Kiyokawa</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Using Eye-Gaze and Visualization to Augment Memory</article-title>
        <source>Distributed, Ambient, and Pervasive Interactions</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Streitz</surname>
            <given-names>N.</given-names>
          </name>
          <name>
            <surname>Markopoulos</surname>
            <given-names>P.</given-names>
          </name>
        </person-group>
        <publisher-name>Springer International Publishing</publisher-name>
        <publisher-loc>Cham, Switzerland</publisher-loc>
        <year>2014</year>
        <volume>Volume 8530 LNCS</volume>
        <fpage>282</fpage>
        <lpage>291</lpage>
        <pub-id pub-id-type="doi">10.1007/978-3-319-07788-8_27</pub-id>
      </element-citation>
    </ref>
    <ref id="B40-sensors-21-02234">
      <label>40.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Toyama</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Sonntag</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Orlosky</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Kiyokawa</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>Attention Engagement and Cognitive State Analysis for Augmented Reality Text Display Functions</article-title>
        <source>Proceedings of the 20th International Conference on Intelligent User Interfaces—IUI ’15</source>
        <publisher-name>ACM Press</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2015</year>
        <fpage>322</fpage>
        <lpage>332</lpage>
        <pub-id pub-id-type="doi">10.1145/2678025.2701384</pub-id>
      </element-citation>
    </ref>
    <ref id="B41-sensors-21-02234">
      <label>41.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Toyama</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Orlosky</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sonntag</surname>
            <given-names>D.</given-names>
          </name>
          <name>
            <surname>Kiyokawa</surname>
            <given-names>K.</given-names>
          </name>
        </person-group>
        <article-title>A Natural Interface for Multi-Focal Plane Head Mounted Displays Using 3D Gaze</article-title>
        <source>Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces</source>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2014</year>
        <fpage>25</fpage>
        <lpage>32</lpage>
        <pub-id pub-id-type="doi">10.1145/2598153.2598154</pub-id>
      </element-citation>
    </ref>
    <ref id="B42-sensors-21-02234">
      <label>42.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>van der Meulen</surname>
            <given-names>H.</given-names>
          </name>
          <name>
            <surname>Kun</surname>
            <given-names>A.L.</given-names>
          </name>
          <name>
            <surname>Shaer</surname>
            <given-names>O.</given-names>
          </name>
        </person-group>
        <article-title>What Are We Missing?</article-title>
        <source>ISS ’17: Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces</source>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2017</year>
        <fpage>396</fpage>
        <lpage>400</lpage>
        <pub-id pub-id-type="doi">10.1145/3132272.3132278</pub-id>
      </element-citation>
    </ref>
    <ref id="B43-sensors-21-02234">
      <label>43.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Kytö</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Ens</surname>
            <given-names>B.</given-names>
          </name>
          <name>
            <surname>Piumsomboon</surname>
            <given-names>T.</given-names>
          </name>
          <name>
            <surname>Lee</surname>
            <given-names>G.A.</given-names>
          </name>
          <name>
            <surname>Billinghurst</surname>
            <given-names>M.</given-names>
          </name>
        </person-group>
        <article-title>Pinpointing</article-title>
        <source>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems—CHI ’18</source>
        <person-group person-group-type="editor">
          <name>
            <surname>Mandryk</surname>
            <given-names>R.</given-names>
          </name>
          <name>
            <surname>Hancock</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Perry</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Cox</surname>
            <given-names>A.</given-names>
          </name>
        </person-group>
        <publisher-name>ACM Press</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2018</year>
        <fpage>1</fpage>
        <lpage>14</lpage>
        <pub-id pub-id-type="doi">10.1145/3173574.3173655</pub-id>
      </element-citation>
    </ref>
    <ref id="B44-sensors-21-02234">
      <label>44.</label>
      <element-citation publication-type="other">
        <person-group person-group-type="author">
          <name>
            <surname>Barz</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Kapp</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Kuhn</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Sonntag</surname>
            <given-names>D.</given-names>
          </name>
        </person-group>
        <article-title>Automatic Recognition and Augmentation of Attended Objects in Real-time using Eye Tracking and a Head-mounted Display</article-title>
        <comment>Manuscript submitted for publication</comment>
      </element-citation>
    </ref>
    <ref id="B45-sensors-21-02234">
      <label>45.</label>
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name>
            <surname>Cerrolaza</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Villanueva</surname>
            <given-names>A.</given-names>
          </name>
          <name>
            <surname>Villanueva</surname>
            <given-names>M.</given-names>
          </name>
          <name>
            <surname>Cabeza</surname>
            <given-names>R.</given-names>
          </name>
        </person-group>
        <article-title>Error characterization and compensation in eye tracking systems</article-title>
        <source>Proceedings of the Symposium on Eye Tracking Research and Applications</source>
        <publisher-name>ACM</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year>2012</year>
        <fpage>205</fpage>
        <lpage>208</lpage>
        <pub-id pub-id-type="doi">10.1145/2168556.2168595</pub-id>
      </element-citation>
    </ref>
    <ref id="B46-sensors-21-02234">
      <label>46.</label>
      <element-citation publication-type="web">
        <person-group person-group-type="author">
          <collab>Microsoft</collab>
        </person-group>
        <article-title>Comfort</article-title>
        <comment>Available online: <ext-link ext-link-type="uri" xlink:href="https://docs.microsoft.com/de-de/windows/mixed-reality/design/comfort">https://docs.microsoft.com/de-de/windows/mixed-reality/design/comfort</ext-link></comment>
        <date-in-citation content-type="access-date" iso-8601-date="2020-11-25">(accessed on 25 November 2020)</date-in-citation>
      </element-citation>
    </ref>
    <ref id="B47-sensors-21-02234">
      <label>47.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Kramida</surname>
            <given-names>G.</given-names>
          </name>
        </person-group>
        <article-title>Resolving the Vergence-Accommodation Conflict in Head-Mounted Displays</article-title>
        <source>IEEE Trans. Vis. Comput. Graph.</source>
        <year>2016</year>
        <volume>22</volume>
        <fpage>1912</fpage>
        <lpage>1931</lpage>
        <pub-id pub-id-type="doi">10.1109/TVCG.2015.2473855</pub-id>
        <?supplied-pmid 26336129?>
        <pub-id pub-id-type="pmid">26336129</pub-id>
      </element-citation>
    </ref>
    <ref id="B48-sensors-21-02234">
      <label>48.</label>
      <element-citation publication-type="journal">
        <person-group person-group-type="author">
          <name>
            <surname>Macinnes</surname>
            <given-names>J.J.</given-names>
          </name>
          <name>
            <surname>Iqbal</surname>
            <given-names>S.</given-names>
          </name>
          <name>
            <surname>Pearson</surname>
            <given-names>J.</given-names>
          </name>
          <name>
            <surname>Johnson</surname>
            <given-names>E.N.</given-names>
          </name>
        </person-group>
        <article-title>Wearable Eye-tracking for Research: Automated dynamic gaze mapping and accuracy/precision comparisons across devices</article-title>
        <source>bioRxiv</source>
        <year>2018</year>
        <pub-id pub-id-type="doi">10.1101/299925</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</back>
<floats-group>
  <fig id="sensors-21-02234-f001" orientation="portrait" position="float">
    <label>Figure 1</label>
    <caption>
      <p>A diagram visualizing the components of the toolkit and their interaction.</p>
    </caption>
    <graphic xlink:href="sensors-21-02234-g001"/>
  </fig>
  <fig id="sensors-21-02234-f002" orientation="portrait" position="float">
    <label>Figure 2</label>
    <caption>
      <p>Screenshot of the control interface accessible over the network.</p>
    </caption>
    <graphic xlink:href="sensors-21-02234-g002"/>
  </fig>
  <fig id="sensors-21-02234-f003" orientation="portrait" position="float">
    <label>Figure 3</label>
    <caption>
      <p>Mixed reality photo of our HoloLens 2 applications for all three settings which are presented to the participants. The fixation grid for settings I and II is displayed at a fixed distance from the user and resized such that the angular size is identical for all distances (<bold>a</bold>). The sphere in setting III is positioned 15 cm above the table and stays fixed on top of the visual marker when the participant moves (<bold>b</bold>). These screenshots are 2D projections which do not reflect the field-of-view and depth perception of a participant in augmented reality (AR).</p>
    </caption>
    <graphic xlink:href="sensors-21-02234-g003"/>
  </fig>
  <fig id="sensors-21-02234-f004" orientation="portrait" position="float">
    <label>Figure 4</label>
    <caption>
      <p>Example of setting I and II in our study with the participant wearing a Microsoft HoloLens 2 and the supervisor controlling the recording using our toolkit.</p>
    </caption>
    <graphic xlink:href="sensors-21-02234-g004"/>
  </fig>
  <fig id="sensors-21-02234-f005" orientation="portrait" position="float">
    <label>Figure 5</label>
    <caption>
      <p>Plot of the mean accuracy at each distance for each target in setting I—resting. The accuracy angle for all targets is smaller than 1.5 degrees.</p>
    </caption>
    <graphic xlink:href="sensors-21-02234-g005"/>
  </fig>
  <fig id="sensors-21-02234-f006" orientation="portrait" position="float">
    <label>Figure 6</label>
    <caption>
      <p>Recorded gaze point of one participant in relation to the upper left target in setting I—resting. The red dot represents the mean gaze position with each cross being one recorded gaze point.</p>
    </caption>
    <graphic xlink:href="sensors-21-02234-g006"/>
  </fig>
  <fig id="sensors-21-02234-f007" orientation="portrait" position="float">
    <label>Figure 7</label>
    <caption>
      <p>Plot of the mean accuracy at each distance for each target in setting II—walking.</p>
    </caption>
    <graphic xlink:href="sensors-21-02234-g007"/>
  </fig>
  <fig id="sensors-21-02234-f008" orientation="portrait" position="float">
    <label>Figure 8</label>
    <caption>
      <p>Recorded gaze point of one participant in relation to the upper left target in setting II—walking. The red dot represents the mean gaze position with each cross being one recorded gaze point.</p>
    </caption>
    <graphic xlink:href="sensors-21-02234-g008"/>
  </fig>
  <fig id="sensors-21-02234-f009" orientation="portrait" position="float">
    <label>Figure 9</label>
    <caption>
      <p>Recorded gaze point of one participant in setting III—stationary target. The distance angle for all gaze points is smaller than 3 degrees.</p>
    </caption>
    <graphic xlink:href="sensors-21-02234-g009"/>
  </fig>
  <table-wrap id="sensors-21-02234-t001" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-02234-t001_Table 1</object-id>
    <label>Table 1</label>
    <caption>
      <p>Overview of recorded data.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Data Column</th>
          <th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Description</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td colspan="2" align="left" valign="middle" style="border-bottom:solid thin" rowspan="1">Time data</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">eyeDataTimestamp</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Unix timestamp of the gaze data (in ms)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">eyeDataRelativeTimestamp</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Relative timestamp of the gaze data (in ms, 100 ns precision)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">frameTimestamp</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Unix timestamp of the frame in which the data was processed (in ms)</td>
        </tr>
        <tr>
          <td colspan="2" align="left" valign="middle" style="border-bottom:solid thin" rowspan="1">Gaze data</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">isCalibrationValid</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Flag if the calibration of the wearer is valid</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazeHasValue</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Flag if valid gaze data exists (origin/direction)</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazeOrigin_(x/y/z)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Gaze origin in the global reference frame</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazeDirection_(x/y/z)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Gaze direction in the global reference frame</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePointHit</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Flag if the raycast hit an object and a gaze position exists</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePoint_(x/y/z)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Position of the gaze point in the global reference frame</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePoint_target_name</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Name of the game object hit by the gaze ray</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePoint_target_(x/y/z)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Position of the gaze point in the local reference frame of the hit object</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePoint_target_(pos/rot/scale)_(x/y/z)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Position, rotation, and scale of the game object hit by the gaze ray</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePoint(Left/Right/Mono)Screen_(x,y,z)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Position of the gaze point on the left, right and virtual mono display</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">gazePointWebcam_(x,y,z)</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Position of the gaze point on the webcam image</td>
        </tr>
        <tr>
          <td colspan="2" align="left" valign="middle" style="border-bottom:solid thin" rowspan="1">AOI data</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePointAOIHit</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Flag if the gaze ray hit an AOI</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePointAOI_(x/y/z)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Position of the gaze point on the AOI in global coordinates</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePointAOI_target_name</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Name of the game object representing the AOI</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePointAOI_target_(x/y/z)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Position of the gaze point in the local reference frame of the AOI</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gazePointAOI_target_(pos/rot/scale)_(x/y/z)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Position, rotation, and scale of the game object hit by the AOI ray</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">gazePointAOIWebcam_(x,y,z)</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Position of the gaze point on the AOI on the webcam image</td>
        </tr>
        <tr>
          <td colspan="2" align="left" valign="middle" style="border-bottom:solid thin" rowspan="1">Additional information</td>
        </tr>
        <tr>
          <td align="left" valign="middle" rowspan="1" colspan="1">gameObject_<italic>objectName</italic>_(pos/rot/scale)_(x/y/z)</td>
          <td align="left" valign="middle" rowspan="1" colspan="1">Position, rotation, and scale of selected game objects</td>
        </tr>
        <tr>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">info</td>
          <td align="left" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Info string of a logged event</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-21-02234-t002" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-02234-t002_Table 2</object-id>
    <label>Table 2</label>
    <caption>
      <p>Accuracy and precision for setting I—resting.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Distance</th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Accuracy (SD)</th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Precision (SD)</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in cm</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in deg</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in cm</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in deg</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.5 m</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.91 (0.41)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.00 (0.44)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.40 (0.16)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.29 (0.13)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.0 m</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.56 (0.83)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.85 (0.46)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.67 (0.24)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.25 (0.11)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.0 m</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.85 (1.31)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.77 (0.35)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.35 (0.49)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.24 (0.10)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.0 m</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">5.03 (2.27)</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.68 (0.31)</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">3.12 (1.26)</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.28 (0.12)</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-21-02234-t003" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-02234-t003_Table 3</object-id>
    <label>Table 3</label>
    <caption>
      <p>Results of the post hoc Wilcoxon signed-rank tests for setting I—resting. * the Bonferroni corrected significane level is <inline-formula><mml:math id="mm102"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.008</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Comparison</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm103"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm104"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm105"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm106"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm107"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm108"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm109"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm110"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm111"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm112"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm113"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm114"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Z</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−2.63</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.57</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.43</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−1.68</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−2.06</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−1.44</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">p</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.009</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001 *</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.001 *</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.093</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.039</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.149</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-21-02234-t004" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-02234-t004_Table 4</object-id>
    <label>Table 4</label>
    <caption>
      <p>Accuracy and precision for setting II—walking.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Distance</th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Accuracy (SD)</th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Precision (SD)</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in cm</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in deg</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in cm</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in deg</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.5 m</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.29 (0.64)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.52 (0.69)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.89 (0.34)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.31 (0.25)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.0 m</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.35 (1.50)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.84 (0.81)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.33 (1.00)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.16 (0.47)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.0 m</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.07 (1.94)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.39 (0.53)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">6.32 (1.52)</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.03 (0.27)</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.0 m</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">9.75 (3.08)</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.33 (0.42)</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">12.58 (3.19)</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.03 (0.32)</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-21-02234-t005" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-02234-t005_Table 5</object-id>
    <label>Table 5</label>
    <caption>
      <p>Results of the post hoc Wilcoxon signed-rank tests for setting II—walking. * the Bonferroni corrected significance level is <inline-formula><mml:math id="mm115"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.008</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th rowspan="2" align="center" valign="middle" style="border-top:solid thin;border-bottom:solid thin" colspan="1">Comparison</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm116"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm117"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm118"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm119"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm120"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm121"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mrow><mml:mn>2.0</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> m</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm122"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">1.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm123"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm124"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm125"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">2.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm126"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1"><inline-formula><mml:math id="mm127"><mml:mrow><mml:mrow><mml:mo mathvariant="bold">−</mml:mo><mml:mn mathvariant="bold">4.0</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> m</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Z</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.432</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.621</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.621</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.574</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−2.817</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−0.686</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">p</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.001 *</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001 *</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001 *</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001 *</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.005 *</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.492</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-21-02234-t006" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-02234-t006_Table 6</object-id>
    <label>Table 6</label>
    <caption>
      <p>Results of the Wilcoxon signed-rank tests for the comparison of the accuracy between setting I and II.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distance</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">0.5 m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">1.0 m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">2.0 m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">4.0 m</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Z</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.57</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.53</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">p</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-21-02234-t007" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-02234-t007_Table 7</object-id>
    <label>Table 7</label>
    <caption>
      <p>Results of the Wilcoxon signed-rank tests for the comparison of the precision between setting I and II.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distance</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">0.5 m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">1.0 m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">2.0 m</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">4.0 m</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">Z</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.62</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">−3.62</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">p</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">&lt;0.001</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-21-02234-t008" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-02234-t008_Table 8</object-id>
    <label>Table 8</label>
    <caption>
      <p>Accuracy, precision, and mean distance for setting III—stationary target.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-top:solid thin" rowspan="1" colspan="1">Distance (SD)</th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Accuracy (SD)</th>
          <th colspan="2" align="center" valign="middle" style="border-top:solid thin" rowspan="1">Precision (SD)</th>
        </tr>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in cm</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in cm</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in deg</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in cm</th>
          <th align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">in deg</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.87 (13.53)</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.34 (0.27)</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.39 (0.31)</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.87 (0.35)</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">1.00 (0.40)</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap id="sensors-21-02234-t009" orientation="portrait" position="float">
    <object-id pub-id-type="pii">sensors-21-02234-t009_Table 9</object-id>
    <label>Table 9</label>
    <caption>
      <p>Recommended minimum target size in cm based on Feit et al. [<xref rid="B34-sensors-21-02234" ref-type="bibr">34</xref>] and the identified accuracy and precision.</p>
    </caption>
    <table frame="hsides" rules="groups">
      <thead>
        <tr>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Distance</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Setting I (Resting)</th>
          <th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Setting II (Walking)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">0.5 m</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">3.42 cm</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">12.14 cm</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">1.0 m</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">5.80 cm</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">20.02 cm</td>
        </tr>
        <tr>
          <td align="center" valign="middle" rowspan="1" colspan="1">2.0 m</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">11.10 cm</td>
          <td align="center" valign="middle" rowspan="1" colspan="1">35.42 cm</td>
        </tr>
        <tr>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4.0 m</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">22.54 cm</td>
          <td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">69.82 cm</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</floats-group>
